arxiv-9300-1 | Structured Prediction of Sequences and Trees using Infinite Contexts | http://arxiv.org/pdf/1503.02417v1.pdf | author:Ehsan Shareghi, Gholamreza Haffari, Trevor Cohn, Ann Nicholson category:cs.LG cs.CL published:2015-03-09 summary:Linguistic structures exhibit a rich array of global phenomena, howevercommonly used Markov models are unable to adequately describe these phenomenadue to their strong locality assumptions. We propose a novel hierarchical modelfor structured prediction over sequences and trees which exploits globalcontext by conditioning each generation decision on an unbounded context ofprior decisions. This builds on the success of Markov models but withoutimposing a fixed bound in order to better represent global phenomena. Tofacilitate learning of this large and unbounded model, we use a hierarchicalPitman-Yor process prior which provides a recursive form of smoothing. Wepropose prediction algorithms based on A* and Markov Chain Monte Carlosampling. Empirical results demonstrate the potential of our model compared tobaseline finite-context Markov models on part-of-speech tagging and syntacticparsing.
arxiv-9300-2 | Deep Learning and the Information Bottleneck Principle | http://arxiv.org/pdf/1503.02406v1.pdf | author:Naftali Tishby, Noga Zaslavsky category:cs.LG published:2015-03-09 summary:Deep Neural Networks (DNNs) are analyzed via the theoretical framework of theinformation bottleneck (IB) principle. We first show that any DNN can bequantified by the mutual information between the layers and the input andoutput variables. Using this representation we can calculate the optimalinformation theoretic limits of the DNN and obtain finite sample generalizationbounds. The advantage of getting closer to the theoretical limit isquantifiable both by the generalization bound and by the network's simplicity.We argue that both the optimal architecture, number of layers andfeatures/connections at each layer, are related to the bifurcation points ofthe information bottleneck tradeoff, namely, relevant compression of the inputlayer with respect to the output layer. The hierarchical representations at thelayered network naturally correspond to the structural phase transitions alongthe information curve. We believe that this new insight can lead to newoptimality bounds and deep learning algorithms.
arxiv-9300-3 | Deep Human Parsing with Active Template Regression | http://arxiv.org/pdf/1503.02391v1.pdf | author:Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao Yang, Luoqi Liu, Jian Dong, Liang Lin, Shuicheng Yan category:cs.CV published:2015-03-09 summary:In this work, the human parsing task, namely decomposing a human image intosemantic fashion/body regions, is formulated as an Active Template Regression(ATR) problem, where the normalized mask of each fashion/body item is expressedas the linear combination of the learned mask templates, and then morphed to amore precise mask with the active shape parameters, including position, scaleand visibility of each semantic region. The mask template coefficients and theactive shape parameters together can generate the human parsing results, andare thus called the structure outputs for human parsing. The deep ConvolutionalNeural Network (CNN) is utilized to build the end-to-end relation between theinput human image and the structure outputs for human parsing. Morespecifically, the structure outputs are predicted by two separate networks. Thefirst CNN network is with max-pooling, and designed to predict the templatecoefficients for each label mask, while the second CNN network is withoutmax-pooling to preserve sensitivity to label mask position and accuratelypredict the active shape parameters. For a new image, the structure outputs ofthe two networks are fused to generate the probability of each label for eachpixel, and super-pixel smoothing is finally used to refine the human parsingresult. Comprehensive evaluations on a large dataset well demonstrate thesignificant superiority of the ATR framework over other state-of-the-arts forhuman parsing. In particular, the F1-score reaches $64.38\%$ by our ATRframework, significantly higher than $44.76\%$ based on the state-of-the-artalgorithm.
arxiv-9300-4 | Persistent Homology in Sparse Regression and Its Application to Brain Morphometry | http://arxiv.org/pdf/1409.0177v2.pdf | author:Moo K. Chung, Jamie L. Hanson, Jieping Ye, Richard J. Davidson, Seth D. Pollak category:stat.ME cs.CV published:2014-08-31 summary:Sparse systems are usually parameterized by a tuning parameter thatdetermines the sparsity of the system. How to choose the right tuning parameteris a fundamental and difficult problem in learning the sparse system. In thispaper, by treating the the tuning parameter as an additional dimension,persistent homological structures over the parameter space is introduced andexplored. The structures are then further exploited in speeding up thecomputation using the proposed soft-thresholding technique. The topologicalstructures are further used as multivariate features in the tensor-basedmorphometry (TBM) in characterizing white matter alterations in children whohave experienced severe early life stress and maltreatment. These analysesreveal that stress-exposed children exhibit more diffuse anatomicalorganization across the whole white matter region.
arxiv-9300-5 | Mathematical understanding of detailed balance condition violation and its application to Langevin dynamics | http://arxiv.org/pdf/1503.02356v1.pdf | author:M. Ohzeki, A. Ichiki category:stat.ML published:2015-03-09 summary:We develop an efficient sampling method by simulating Langevin dynamics withan artificial force rather than a natural force by using the gradient of thepotential energy. The standard technique for sampling following thepredetermined distribution such as the Gibbs-Boltzmann one is performed underthe detailed balance condition. In the present study, we propose a modifiedLangevin dynamics violating the detailed balance condition on thetransition-probability formulation. We confirm that the numericalimplementation of the proposed method actually demonstrates two majorbeneficial improvements: acceleration of the relaxation to the predetermineddistribution and reduction of the correlation time between two differentrealizations in the steady state.
arxiv-9300-6 | Fully Connected Deep Structured Networks | http://arxiv.org/pdf/1503.02351v1.pdf | author:Alexander G. Schwing, Raquel Urtasun category:cs.CV cs.LG published:2015-03-09 summary:Convolutional neural networks with many layers have recently been shown toachieve excellent results on many high-level tasks such as imageclassification, object detection and more recently also semantic segmentation.Particularly for semantic segmentation, a two-stage procedure is oftenemployed. Hereby, convolutional networks are trained to provide good localpixel-wise features for the second step being traditionally a more globalgraphical model. In this work we unify this two-stage process into a singlejoint training algorithm. We demonstrate our method on the semantic imagesegmentation task and show encouraging results on the challenging PASCAL VOC2012 dataset.
arxiv-9300-7 | An Unsupervised Method for Uncovering Morphological Chains | http://arxiv.org/pdf/1503.02335v1.pdf | author:Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola category:cs.CL published:2015-03-08 summary:Most state-of-the-art systems today produce morphological analysis based onlyon orthographic patterns. In contrast, we propose a model for unsupervisedmorphological analysis that integrates orthographic and semantic views ofwords. We model word formation in terms of morphological chains, from basewords to the observed words, breaking the chains into parent-child relations.We use log-linear models with morpheme and word-level features to predictpossible parents, including their modifications, for each word. The limited setof candidate parents for each word render contrastive estimation feasible. Ourmodel consistently matches or outperforms five state-of-the-art systems onArabic, English and Turkish.
arxiv-9300-8 | Fully Convolutional Networks for Semantic Segmentation | http://arxiv.org/pdf/1411.4038v2.pdf | author:Jonathan Long, Evan Shelhamer, Trevor Darrell category:cs.CV published:2014-11-14 summary:Convolutional networks are powerful visual models that yield hierarchies offeatures. We show that convolutional networks by themselves, trainedend-to-end, pixels-to-pixels, exceed the state-of-the-art in semanticsegmentation. Our key insight is to build "fully convolutional" networks thattake input of arbitrary size and produce correspondingly-sized output withefficient inference and learning. We define and detail the space of fullyconvolutional networks, explain their application to spatially dense predictiontasks, and draw connections to prior models. We adapt contemporaryclassification networks (AlexNet, the VGG net, and GoogLeNet) into fullyconvolutional networks and transfer their learned representations byfine-tuning to the segmentation task. We then define a novel architecture thatcombines semantic information from a deep, coarse layer with appearanceinformation from a shallow, fine layer to produce accurate and detailedsegmentations. Our fully convolutional network achieves state-of-the-artsegmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012),NYUDv2, and SIFT Flow, while inference takes one third of a second for atypical image.
arxiv-9300-9 | TuPAQ: An Efficient Planner for Large-scale Predictive Analytic Queries | http://arxiv.org/pdf/1502.00068v2.pdf | author:Evan R. Sparks, Ameet Talwalkar, Michael J. Franklin, Michael I. Jordan, Tim Kraska category:cs.DB cs.DC cs.LG published:2015-01-31 summary:The proliferation of massive datasets combined with the development ofsophisticated analytical techniques have enabled a wide variety of novelapplications such as improved product recommendations, automatic image tagging,and improved speech-driven interfaces. These and many other applications can besupported by Predictive Analytic Queries (PAQs). A major obstacle to supportingPAQs is the challenging and expensive process of identifying and training anappropriate predictive model. Recent efforts aiming to automate this processhave focused on single node implementations and have assumed that modeltraining itself is a black box, thus limiting the effectiveness of suchapproaches on large-scale problems. In this work, we build upon these recentefforts and propose an integrated PAQ planning architecture that combinesadvanced model search techniques, bandit resource allocation via runtimealgorithm introspection, and physical optimization via batching. The result isTuPAQ, a component of the MLbase system, which solves the PAQ planning problemwith comparable quality to exhaustive strategies but an order of magnitude moreefficiently than the standard baseline approach, and can scale to modelstrained on terabytes of data across hundreds of machines.
arxiv-9300-10 | Fitting 3D Morphable Models using Local Features | http://arxiv.org/pdf/1503.02330v1.pdf | author:Patrik Huber, Zhen-Hua Feng, William Christmas, Josef Kittler, Matthias Rätsch category:cs.CV 68T45 I.4.8; I.2.10 published:2015-03-08 summary:In this paper, we propose a novel fitting method that uses local imagefeatures to fit a 3D Morphable Model to 2D images. To overcome the obstacle ofoptimising a cost function that contains a non-differentiable featureextraction operator, we use a learning-based cascaded regression method thatlearns the gradient direction from data. The method allows to simultaneouslysolve for shape and pose parameters. Our method is thoroughly evaluated onMorphable Model generated data and first results on real data are presented.Compared to traditional fitting methods, which use simple raw features likepixel colour or edge maps, local features have been shown to be much morerobust against variations in imaging conditions. Our approach is unique in thatwe are the first to use local features to fit a Morphable Model. Because of the speed of our method, it is applicable for realtimeapplications. Our cascaded regression framework is available as an open sourcelibrary (https://github.com/patrikhuber).
arxiv-9300-11 | Financial Market Prediction | http://arxiv.org/pdf/1503.02328v1.pdf | author:Mike Wu category:cs.CE cs.LG published:2015-03-08 summary:Given financial data from popular sites like Yahoo and the London Exchange,the presented paper attempts to model and predict stocks that can be considered"good investments". Stocks are characterized by 125 features ranging from grossdomestic product to EDIBTA, and are labeled by discrepancies between stock andmarket price returns. An artificial neural network (Self-Organizing Map) isfitted to train on more than a million data points to predict "goodinvestments" given testing stocks from 2013 and after.
arxiv-9300-12 | DESAT: an SSW tool for SDO/AIA image de-saturation | http://arxiv.org/pdf/1503.02302v1.pdf | author:Richard A Schwartz, Gabriele Torre, Anna Maria Massone, Michele Piana category:astro-ph.IM cs.CV 85-08, 68U10 published:2015-03-08 summary:Saturation affects a significant rate of images recorded by the AtmosphericImaging Assembly on the Solar Dynamics Observatory. This paper describes acomputational method and a technological pipeline for the de-saturation of suchimages, based on several mathematical ingredients like ExpectationMaximization, image correlation and interpolation. An analysis of thecomputational properties and demands of the pipeline, together with anassessment of its reliability are performed against a set of data recorded fromthe Feburary 25 2014 flaring event.
arxiv-9300-13 | The combinatorial structure of beta negative binomial processes | http://arxiv.org/pdf/1401.0062v3.pdf | author:Creighton Heaukulani, Daniel M. Roy category:math.ST math.PR stat.ML stat.TH published:2013-12-31 summary:We characterize the combinatorial structure of conditionally-i.i.d. sequencesof negative binomial processes with a common beta process base measure. InBayesian nonparametric applications, such processes have served as models forlatent multisets of features underlying data. Analogously, random subsets arisefrom conditionally-i.i.d. sequences of Bernoulli processes with a common betaprocess base measure, in which case the combinatorial structure is described bythe Indian buffet process. Our results give a count analogue of the Indianbuffet process, which we call a negative binomial Indian buffet process. As anintermediate step toward this goal, we provide a construction for the betanegative binomial process that avoids a representation of the underlying betaprocess base measure. We describe the key Markov kernels needed to use a NB-IBPrepresentation in a Markov Chain Monte Carlo algorithm targeting a posteriordistribution.
arxiv-9300-14 | Higher order Matching Pursuit for Low Rank Tensor Learning | http://arxiv.org/pdf/1503.02216v1.pdf | author:Yuning Yang, Siamak Mehrkanoon, Johan A. K. Suykens category:stat.ML cs.LG math.OC published:2015-03-07 summary:Low rank tensor learning, such as tensor completion and multilinear multitasklearning, has received much attention in recent years. In this paper, wepropose higher order matching pursuit for low rank tensor learning problemswith a convex or a nonconvex cost function, which is a generalization of thematching pursuit type methods. At each iteration, the main cost of the proposedmethods is only to compute a rank-one tensor, which can be done efficiently,making the proposed methods scalable to large scale problems. Moreover, storingthe resulting rank-one tensors is of low storage requirement, which can help tobreak the curse of dimensionality. The linear convergence rate of the proposedmethods is established in various circumstances. Along with the main methods,we also provide a method of low computational complexity for approximatelycomputing the rank-one tensors, with provable approximation ratio, which helpsto improve the efficiency of the main methods and to analyze the convergencerate. Experimental results on synthetic as well as real datasets verify theefficiency and effectiveness of the proposed methods.
arxiv-9300-15 | The Informed Sampler: A Discriminative Approach to Bayesian Inference in Generative Computer Vision Models | http://arxiv.org/pdf/1402.0859v3.pdf | author:Varun Jampani, Sebastian Nowozin, Matthew Loper, Peter V. Gehler category:cs.CV cs.LG stat.ML published:2014-02-04 summary:Computer vision is hard because of a large variability in lighting, shape,and texture; in addition the image signal is non-additive due to occlusion.Generative models promised to account for this variability by accuratelymodelling the image formation process as a function of latent variables withprior beliefs. Bayesian posterior inference could then, in principle, explainthe observation. While intuitively appealing, generative models for computervision have largely failed to deliver on that promise due to the difficulty ofposterior inference. As a result the community has favoured efficientdiscriminative approaches. We still believe in the usefulness of generativemodels in computer vision, but argue that we need to leverage existingdiscriminative or even heuristic computer vision methods. We implement thisidea in a principled way with an "informed sampler" and in careful experimentsdemonstrate it on challenging generative models which contain renderer programsas their components. We concentrate on the problem of inverting an existinggraphics rendering engine, an approach that can be understood as "InverseGraphics". The informed sampler, using simple discriminative proposals based onexisting computer vision technology, achieves significant improvements ofinference.
arxiv-9300-16 | Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data | http://arxiv.org/pdf/1503.02182v1.pdf | author:Yarin Gal, Yutian Chen, Zoubin Ghahramani category:stat.ML published:2015-03-07 summary:Multivariate categorical data occur in many applications of machine learning.One of the main difficulties with these vectors of categorical variables issparsity. The number of possible observations grows exponentially with vectorlength, but dataset diversity might be poor in comparison. Recent models havegained significant improvement in supervised tasks with this data. These modelsembed observations in a continuous space to capture similarities between them.Building on these ideas we propose a Bayesian model for the unsupervised taskof distribution estimation of multivariate categorical data. We model vectorsof categorical variables as generated from a non-linear transformation of acontinuous latent space. Non-linearity captures multi-modality in thedistribution. The continuous representation addresses sparsity. Our model tiestogether many existing models, linking the linear categorical latent Gaussianmodel, the Gaussian process latent variable model, and Gaussian processclassification. We derive inference for our model based on recent developmentsin sampling based variational inference. We show empirically that the modeloutperforms its linear and discrete counterparts in imputation tasks of sparsedata.
arxiv-9300-17 | A Nonconvex Approach for Structured Sparse Learning | http://arxiv.org/pdf/1503.02164v1.pdf | author:Shubao Zhang, Hui Qian, Zhihua Zhang category:cs.IT cs.LG math.IT published:2015-03-07 summary:Sparse learning is an important topic in many areas such as machine learning,statistical estimation, signal processing, etc. Recently, there emerges agrowing interest on structured sparse learning. In this paper we focus on the$\ell_q$-analysis optimization problem for structured sparse learning ($0< q\leq 1$). Compared to previous work, we establish weaker conditions for exactrecovery in noiseless case and a tighter non-asymptotic upper bound of estimateerror in noisy case. We further prove that the nonconvex $\ell_q$-analysisoptimization can do recovery with a lower sample complexity and in a widerrange of cosparsity than its convex counterpart. In addition, we develop aniteratively reweighted method to solve the optimization problem under thevariational framework. Theoretical analysis shows that our method is capable ofpursuing a local minima close to the global minima. Also, empirical results ofpreliminary computational experiments illustrate that our nonconvex methodoutperforms both its convex counterpart and other state-of-the-art methods.
arxiv-9300-18 | Sparse Bayesian Dictionary Learning with a Gaussian Hierarchical Model | http://arxiv.org/pdf/1503.02144v1.pdf | author:Linxiao Yang, Jun Fang, Hong Cheng, Hongbin Li category:cs.LG cs.IT math.IT published:2015-03-07 summary:We consider a dictionary learning problem whose objective is to design adictionary such that the signals admits a sparse or an approximate sparserepresentation over the learned dictionary. Such a problem finds a variety ofapplications such as image denoising, feature extraction, etc. In this paper,we propose a new hierarchical Bayesian model for dictionary learning, in whicha Gaussian-inverse Gamma hierarchical prior is used to promote the sparsity ofthe representation. Suitable priors are also placed on the dictionary and thenoise variance such that they can be reasonably inferred from the data. Basedon the hierarchical model, a variational Bayesian method and a Gibbs samplingmethod are developed for Bayesian inference. The proposed methods have theadvantage that they do not require the knowledge of the noise variance \emph{apriori}. Numerical results show that the proposed methods are able to learn thedictionary with an accuracy better than existing methods, particularly for thecase where there is a limited number of training signals.
arxiv-9300-19 | Model selection of polynomial kernel regression | http://arxiv.org/pdf/1503.02143v1.pdf | author:Shaobo Lin, Xingping Sun, Zongben Xu, Jinshan Zeng category:cs.LG F.2.2 published:2015-03-07 summary:Polynomial kernel regression is one of the standard and state-of-the-artlearning strategies. However, as is well known, the choices of the degree ofpolynomial kernel and the regularization parameter are still open in the realmof model selection. The first aim of this paper is to develop a strategy toselect these parameters. On one hand, based on the worst-case learning rateanalysis, we show that the regularization term in polynomial kernel regressionis not necessary. In other words, the regularization parameter can decreasearbitrarily fast when the degree of the polynomial kernel is suitable tuned. Onthe other hand,taking account of the implementation of the algorithm, theregularization term is required. Summarily, the effect of the regularizationterm in polynomial kernel regression is only to circumvent the " ill-condition"of the kernel matrix. Based on this, the second purpose of this paper is topropose a new model selection strategy, and then design an efficient learningalgorithm. Both theoretical and experimental analysis show that the newstrategy outperforms the previous one. Theoretically, we prove that the newlearning strategy is almost optimal if the regression function is smooth.Experimentally, it is shown that the new strategy can significantly reduce thecomputational burden without loss of generalization capability.
arxiv-9300-20 | An Improved Image Mosaicing Algorithm for Damaged Documents | http://arxiv.org/pdf/1503.02136v1.pdf | author:Waheeda Dhokley, Khan Munifa, Shaikh Nazia, Shaikh Saiqua category:cs.CV published:2015-03-07 summary:It is a common phenomenon in day to day life; where in some of the documentgets damaged. Out of several reasons, the main reason for documents gettingdamaged is shredding by hands. Recovery of such documents is essential. Manualrecovery of such damaged document is tedious and time consuming task. In thispaper, we are describing an algorithm which recovers the original document fromsuch shredded pieces of the same. In order to implement this, we are using asimple technique called Image Mosaicing. In this technique a complete new imageis developed using two or more torn fragments. For simplicity ofimplementation, we are considering only two torn pieces of a document that willbe mosaiced together. The successful implementation of this algorithm wouldlead to recovery of important information which in turn would be beneficial invarious fields such as forensic sciences, archival study, etc
arxiv-9300-21 | Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition | http://arxiv.org/pdf/1503.02101v1.pdf | author:Rong Ge, Furong Huang, Chi Jin, Yang Yuan category:cs.LG math.OC stat.ML published:2015-03-06 summary:We analyze stochastic gradient descent for optimizing non-convex functions.In many cases for non-convex functions the goal is to find a reasonable localminimum, and the main concern is that gradient updates are trapped in saddlepoints. In this paper we identify strict saddle property for non-convex problemthat allows for efficient optimization. Using this property we show thatstochastic gradient descent converges to a local minimum in a polynomial numberof iterations. To the best of our knowledge this is the first work that givesglobal convergence guarantees for stochastic gradient descent on non-convexfunctions with exponentially many local minima and saddle points. Our analysiscan be applied to orthogonal tensor decomposition, which is widely used inlearning a rich class of latent variable models. We propose a new optimizationformulation for the tensor decomposition problem that has strict saddleproperty. As a result we get the first online algorithm for orthogonal tensordecomposition with global convergence guarantee.
arxiv-9300-22 | Band selection in RKHS for fast nonlinear unmixing of hyperspectral images | http://arxiv.org/pdf/1503.02090v1.pdf | author:T. Imbiriba, J. C. M. Bermudez, C. Richard, J. -Y. Tourneret category:cs.CV published:2015-03-06 summary:The profusion of spectral bands generated by the acquisition process ofhyperspectral images generally leads to high computational costs. Suchdifficulties arise in particular with nonlinear unmixing methods, which arenaturally more complex than linear ones. This complexity, associated with thehigh redundancy of information within the complete set of bands, make thesearch of band selection algorithms relevant. With this work, we propose a bandselection strategy in reproducing kernel Hilbert spaces that allows todrastically reduce the processing time required by nonlinear unmixingtechniques. Simulation results show a complexity reduction of two orders ofmagnitude without compromising unmixing performance.
arxiv-9300-23 | To Drop or Not to Drop: Robustness, Consistency and Differential Privacy Properties of Dropout | http://arxiv.org/pdf/1503.02031v1.pdf | author:Prateek Jain, Vivek Kulkarni, Abhradeep Thakurta, Oliver Williams category:cs.LG cs.NE stat.ML published:2015-03-06 summary:Training deep belief networks (DBNs) requires optimizing a non-convexfunction with an extremely large number of parameters. Naturally, existinggradient descent (GD) based methods are prone to arbitrarily poor local minima.In this paper, we rigorously show that such local minima can be avoided (uptoan approximation error) by using the dropout technique, a widely used heuristicin this domain. In particular, we show that by randomly dropping a few nodes ofa one-hidden layer neural network, the training objective function, up to acertain approximation error, decreases by a multiplicative factor. On the flip side, we show that for training convex empirical risk minimizers(ERM), dropout in fact acts as a "stabilizer" or regularizer. That is, a simpledropout based GD method for convex ERMs is stable in the face of arbitrarychanges to any one of the training points. Using the above assertion, we showthat dropout provides fast rates for generalization error in learning (convex)generalized linear models (GLM). Moreover, using the above mentioned stabilityproperties of dropout, we design dropout based differentially privatealgorithms for solving ERMs. The learned GLM thus, preserves privacy of each ofthe individual training points while providing accurate predictions for newtest points. Finally, we empirically validate our stability assertions fordropout in the context of convex ERMs and show that surprisingly, dropoutsignificantly outperforms (in terms of prediction accuracy) the L2regularization based methods for several benchmark datasets.
arxiv-9300-24 | On the Bayes-optimality of F-measure maximizers | http://arxiv.org/pdf/1310.4849v3.pdf | author:Willem Waegeman, Krzysztof Dembczynski, Arkadiusz Jachnik, Weiwei Cheng, Eyke Hullermeier category:stat.ML cs.LG published:2013-10-17 summary:The F-measure, which has originally been introduced in information retrieval,is nowadays routinely used as a performance metric for problems such as binaryclassification, multi-label classification, and structured output prediction.Optimizing this measure is a statistically and computationally challengingproblem, since no closed-form solution exists. Adopting a decision-theoreticperspective, this article provides a formal and experimental analysis ofdifferent approaches for maximizing the F-measure. We start with a Bayes-riskanalysis of related loss functions, such as Hamming loss and subset zero-oneloss, showing that optimizing such losses as a surrogate of the F-measure leadsto a high worst-case regret. Subsequently, we perform a similar type ofanalysis for F-measure maximizing algorithms, showing that such algorithms areapproximate, while relying on additional assumptions regarding the statisticaldistribution of the binary response variables. Furthermore, we present a newalgorithm which is not only computationally efficient but also Bayes-optimal,regardless of the underlying distribution. To this end, the algorithm requiresonly a quadratic (with respect to the number of binary responses) number ofparameters of the joint distribution. We illustrate the practical performanceof all analyzed methods by means of experiments with multi-label classificationproblems.
arxiv-9300-25 | Modeling the average shortest path length in growth of word-adjacency networks | http://arxiv.org/pdf/1409.4714v2.pdf | author:Andrzej Kulig, Stanislaw Drozdz, Jaroslaw Kwapien, Pawel Oswiecimka category:cs.CL physics.soc-ph published:2014-09-16 summary:We investigate properties of evolving linguistic networks defined by theword-adjacency relation. Such networks belong to the category of networks withaccelerated growth but their shortest path length appears to reveal the networksize dependence of different functional form than the ones known so far. Wethus compare the networks created from literary texts with their artificialsubstitutes based on different variants of the Dorogovtsev-Mendes model andobserve that none of them is able to properly simulate the novel asymptotics ofthe shortest path length. Then, we identify the local chain-like linear growthinduced by grammar and style as a missing element in this model and extend itby incorporating such effects. It is in this way that a satisfactory agreementwith the empirical result is obtained.
arxiv-9300-26 | Unimodal Bandits without Smoothness | http://arxiv.org/pdf/1406.7447v2.pdf | author:Richard Combes, Alexandre Proutiere category:cs.LG published:2014-06-28 summary:We consider stochastic bandit problems with a continuous set of arms andwhere the expected reward is a continuous and unimodal function of the arm. Nofurther assumption is made regarding the smoothness and the structure of theexpected reward function. For these problems, we propose the StochasticPentachotomy (SP) algorithm, and derive finite-time upper bounds on its regretand optimization error. In particular, we show that, for any expected rewardfunction $\mu$ that behaves as $\mu(x)=\mu(x^\star)-Cx-x^\star^\xi$ locallyaround its maximizer $x^\star$ for some $\xi, C>0$, the SP algorithm isorder-optimal. Namely its regret and optimization error scale as$O(\sqrt{T\log(T)})$ and $O(\sqrt{\log(T)/T})$, respectively, when the timehorizon $T$ grows large. These scalings are achieved without the knowledge of$\xi$ and $C$. Our algorithm is based on asymptotically optimal sequentialstatistical tests used to successively trim an interval that contains the bestarm with high probability. To our knowledge, the SP algorithm constitutes thefirst sequential arm selection rule that achieves a regret and optimizationerror scaling as $O(\sqrt{T})$ and $O(1/\sqrt{T})$, respectively, up to alogarithmic factor for non-smooth expected reward functions, as well as forsmooth functions with unknown smoothness.
arxiv-9300-27 | A Preadapted Universal Switch Distribution for Testing Hilberg's Conjecture | http://arxiv.org/pdf/1310.8511v2.pdf | author:Łukasz Dębowski category:cs.IT cs.CL math.IT 68P30, 94A45 E.4 published:2013-10-31 summary:Hilberg's conjecture about natural language states that the mutualinformation between two adjacent long blocks of text grows like a power of theblock length. The exponent in this statement can be upper bounded using thepointwise mutual information estimate computed for a carefully chosen code. Thebound is the better, the lower the compression rate is but there is arequirement that the code be universal. So as to improve a received upper boundfor Hilberg's exponent, in this paper, we introduce two novel universal codes,called the plain switch distribution and the preadapted switch distribution.Generally speaking, switch distributions are certain mixtures of adaptiveMarkov chains of varying orders with some additional communication to avoid socalled catch-up phenomenon. The advantage of these distributions is that theyboth achieve a low compression rate and are guaranteed to be universal. Usingthe switch distributions we obtain that a sample of a text in English isnon-Markovian with Hilberg's exponent being $\le 0.83$, which improves over theprevious bound $\le 0.94$ obtained using the Lempel-Ziv code.
arxiv-9300-28 | Convolutional LSTM Networks for Subcellular Localization of Proteins | http://arxiv.org/pdf/1503.01919v1.pdf | author:Søren Kaae Sønderby, Casper Kaae Sønderby, Henrik Nielsen, Ole Winther category:q-bio.QM cs.NE published:2015-03-06 summary:Machine learning is widely used to analyze biological sequence data.Non-sequential models such as SVMs or feed-forward neural networks are oftenused although they have no natural way of handling sequences of varying length.Recurrent neural networks such as the long short term memory (LSTM) model onthe other hand are designed to handle sequences. In this study we demonstratethat LSTM networks predict the subcellular location of proteins given only theprotein sequence with high accuracy (0.902) outperforming current state of theart algorithms. We further improve the performance by introducing convolutionalfilters and experiment with an attention mechanism which lets the LSTM focus onspecific parts of the protein. Lastly we introduce new visualizations of boththe convolutional filters and the attention mechanisms and show how they can beused to extract biological relevant knowledge from the LSTM networks.
arxiv-9300-29 | Fast image-based obstacle detection from unmanned surface vehicles | http://arxiv.org/pdf/1503.01918v1.pdf | author:Matej Kristan, Vildana Sulic, Stanislav Kovacic, Janez Pers category:cs.CV published:2015-03-06 summary:Obstacle detection plays an important role in unmanned surface vehicles(USV). The USVs operate in highly diverse environments in which an obstacle maybe a floating piece of wood, a scuba diver, a pier, or a part of a shoreline,which presents a significant challenge to continuous detection from imagestaken onboard. This paper addresses the problem of online detection byconstrained unsupervised segmentation. To this end, a new graphical model isproposed that affords a fast and continuous obstacle image-map estimation froma single video stream captured onboard a USV. The model accounts for thesemantic structure of marine environment as observed from USV by imposing weakstructural constraints. A Markov random field framework is adopted and a highlyefficient algorithm for simultaneous optimization of model parameters andsegmentation mask estimation is derived. Our approach does not requirecomputationally intensive extraction of texture features and comfortably runsin real-time. The algorithm is tested on a new, challenging, dataset forsegmentation and obstacle detection in marine environments, which is thelargest annotated dataset of its kind. Results on this dataset show that ourmodel outperforms the related approaches, while requiring a fraction ofcomputational effort.
arxiv-9300-30 | Hamiltonian ABC | http://arxiv.org/pdf/1503.01916v1.pdf | author:Edward Meeds, Robert Leenders, Max Welling category:stat.ML cs.LG q-bio.QM published:2015-03-06 summary:Approximate Bayesian computation (ABC) is a powerful and elegant frameworkfor performing inference in simulation-based models. However, due to thedifficulty in scaling likelihood estimates, ABC remains useful for relativelylow-dimensional problems. We introduce Hamiltonian ABC (HABC), a set oflikelihood-free algorithms that apply recent advances in scaling Bayesianlearning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We findthat a small number forward simulations can effectively approximate the ABCgradient, allowing Hamiltonian dynamics to efficiently traverse parameterspaces. We also describe a new simple yet general approach of incorporatingrandom seeds into the state of the Markov chain, further reducing the randomwalk behavior of HABC. We demonstrate HABC on several typical ABC problems, andshow that HABC samples comparably to regular Bayesian inference using truegradients on a high-dimensional problem from machine learning.
arxiv-9300-31 | Sequential Relevance Maximization with Binary Feedback | http://arxiv.org/pdf/1503.01910v1.pdf | author:Vijay Kamble, Nadia Fawaz, Fernando Silveira category:cs.LG cs.AI published:2015-03-06 summary:Motivated by online settings where users can provide explicit feedback aboutthe relevance of products that are sequentially presented to them, we look atthe recommendation process as a problem of dynamically optimizing thisrelevance feedback. Such an algorithm optimizes the fine tradeoff betweenpresenting the products that are most likely to be relevant, and learning thepreferences of the user so that more relevant recommendations can be made inthe future. We assume a standard predictive model inspired by collaborative filtering, inwhich a user is sampled from a distribution over a set of possible types. Forevery product category, each type has an associated relevance feedback that isassumed to be binary: the category is either relevant or irrelevant. Assumingthat the user stays for each additional recommendation opportunity withprobability $\beta$ independent of the past, the problem is to find a policythat maximizes the expected number of recommendations that are deemed relevantin a session. We analyze this problem and prove key structural properties of the optimalpolicy. Based on these properties, we first present an algorithm that strikes abalance between recursion and dynamic programming to compute this policy. Wefurther propose and analyze two heuristic policies: a `farsighted' greedypolicy that attains at least $1-\beta$ factor of the optimal payoff, and anaive greedy policy that attains at least $\frac{1-\beta}{1+\beta}$ factor ofthe optimal payoff in the worst case. Extensive simulations show that theseheuristics are very close to optimal in practice.
arxiv-9300-32 | Partial light field tomographic reconstruction from a fixed-camera focal stack | http://arxiv.org/pdf/1503.01903v1.pdf | author:A. Mousnier, E. Vural, C. Guillemot category:cs.CV cs.GR published:2015-03-06 summary:This paper describes a novel approach to partially reconstructhigh-resolution 4D light fields from a stack of differently focused photographstaken with a fixed camera. First, a focus map is calculated from this stackusing a simple approach combining gradient detection and region expansion withgraph-cut. Then, this focus map is converted into a depth map thanks to thecalibration of the camera. We proceed after this with the tomographicreconstruction of the epipolar images by back-projecting the focused regions ofthe scene only. We call it masked back-projection. The angles ofback-projection are calculated from the depth map. Thanks to the high angularresolution we achieve by suitably exploiting the image content captured over alarge interval of focus distances, we are able to render puzzling perspectiveshifts although the original photographs were taken from a single fixed cameraat a fixed position.
arxiv-9300-33 | A Bayesian Model of node interaction in networks | http://arxiv.org/pdf/1402.4279v2.pdf | author:Ingmar Schuster category:cs.LG stat.ME stat.ML published:2014-02-18 summary:We are concerned with modeling the strength of links in networks by takinginto account how often those links are used. Link usage is a strong indicatorof how closely two nodes are related, but existing network models in BayesianStatistics and Machine Learning are able to predict only wether a link existsat all. As priors for latent attributes of network nodes we explore the ChineseRestaurant Process (CRP) and a multivariate Gaussian with fixed dimensionality.The model is applied to a social network dataset and a word coocurrencedataset.
arxiv-9300-34 | Ranking and significance of variable-length similarity-based time series motifs | http://arxiv.org/pdf/1503.01883v1.pdf | author:Joan Serrà, Isabel Serra, Álvaro Corral, Josep Lluis Arcos category:cs.LG published:2015-03-06 summary:The detection of very similar patterns in a time series, commonly calledmotifs, has received continuous and increasing attention from diversescientific communities. In particular, recent approaches for discoveringsimilar motifs of different lengths have been proposed. In this work, we showthat such variable-length similarity-based motifs cannot be directly compared,and hence ranked, by their normalized dissimilarities. Specifically, we findthat length-normalized motif dissimilarities still have intrinsic dependencieson the motif length, and that lowest dissimilarities are particularly affectedby this dependency. Moreover, we find that such dependencies are generallynon-linear and change with the considered data set and dissimilarity measure.Based on these findings, we propose a solution to rank those motifs and measuretheir significance. This solution relies on a compact but accurate model of thedissimilarity space, using a beta distribution with three parameters thatdepend on the motif length in a non-linear way. We believe the incomparabilityof variable-length dissimilarities could go beyond the field of time series,and that similar modeling strategies as the one used here could be of help in amore broad context.
arxiv-9300-35 | Estimation of the parameters of an infectious disease model using neural networks | http://arxiv.org/pdf/1503.01847v1.pdf | author:V. Sree Hari Rao, M. Naresh Kumar category:cs.NE published:2015-03-06 summary:In this paper, we propose a realistic mathematical model taking into accountthe mutual interference among the interacting populations. This model attemptsto describe the control (vaccination) function as a function of the number ofinfective individuals, which is an improvement over the existing susceptible?infective epidemic models. Regarding the growth of the epidemic as a nonlinearphenomenon we have developed a neural network architecture to estimate thevital parameters associated with this model. This architecture is based on arecently developed new class of neural networks known as co-operative andsupportive neural networks. The application of this architecture to the presentstudy involves preprocessing of the input data, and this renders an efficientestimation of the rate of spread of the epidemic. It is observed that theproposed new neural network outperforms a simple feed-forward neural networkand polynomial regression.
arxiv-9300-36 | An Incidence Geometry approach to Dictionary Learning | http://arxiv.org/pdf/1402.7344v2.pdf | author:Meera Sitharam, Mohamad Tarifi, Menghan Wang category:cs.LG stat.ML published:2014-02-28 summary:We study the Dictionary Learning (aka Sparse Coding) problem of obtaining asparse representation of data points, by learning \emph{dictionary vectors}upon which the data points can be written as sparse linear combinations. Weview this problem from a geometry perspective as the spanning set of a subspacearrangement, and focus on understanding the case when the underlying hypergraphof the subspace arrangement is specified. For this Fitted Dictionary Learningproblem, we completely characterize the combinatorics of the associatedsubspace arrangements (i.e.\ their underlying hypergraphs). Specifically, acombinatorial rigidity-type theorem is proven for a type of geometric incidencesystem. The theorem characterizes the hypergraphs of subspace arrangements thatgenerically yield (a) at least one dictionary (b) a locally unique dictionary(i.e.\ at most a finite number of isolated dictionaries) of the specified size.We are unaware of prior application of combinatorial rigidity techniques in thesetting of Dictionary Learning, or even in machine learning. We also provide asystematic classification of problems related to Dictionary Learning togetherwith various algorithms, their assumptions and performance.
arxiv-9300-37 | Deep Clustered Convolutional Kernels | http://arxiv.org/pdf/1503.01824v1.pdf | author:Minyoung Kim, Luca Rigazio category:cs.LG cs.NE published:2015-03-06 summary:Deep neural networks have recently achieved state of the art performancethanks to new training algorithms for rapid parameter estimation and newregularization methods to reduce overfitting. However, in practice the networkarchitecture has to be manually set by domain experts, generally by a costlytrial and error procedure, which often accounts for a large portion of thefinal system performance. We view this as a limitation and propose a noveltraining algorithm that automatically optimizes network architecture, byprogressively increasing model complexity and then eliminating model redundancyby selectively removing parameters at training time. For convolutional neuralnetworks, our method relies on iterative split/merge clustering ofconvolutional kernels interleaved by stochastic gradient descent. We present atraining algorithm and experimental results on three different vision tasks,showing improved performance compared to similarly sized hand-craftedarchitectures.
arxiv-9300-38 | Latent Hierarchical Model for Activity Recognition | http://arxiv.org/pdf/1503.01820v1.pdf | author:Ninghang Hu, Gwenn Englebienne, Zhongyu Lou, Ben Kröse category:cs.RO cs.AI cs.CV cs.LG published:2015-03-06 summary:We present a novel hierarchical model for human activity recognition. Incontrast to approaches that successively recognize actions and activities, ourapproach jointly models actions and activities in a unified framework, andtheir labels are simultaneously predicted. The model is embedded with a latentlayer that is able to capture a richer class of contextual information in bothstate-state and observation-state pairs. Although loops are present in themodel, the model has an overall linear-chain structure, where the exactinference is tractable. Therefore, the model is very efficient in bothinference and learning. The parameters of the graphical model are learned witha Structured Support Vector Machine (Structured-SVM). A data-driven approach isused to initialize the latent variables; therefore, no manual labeling for thelatent states is required. The experimental results from using two benchmarkdatasets show that our model outperforms the state-of-the-art approach, and ourmodel is computationally more efficient.
arxiv-9300-39 | A General Hybrid Clustering Technique | http://arxiv.org/pdf/1503.01183v2.pdf | author:Saeid Amiri, Bertrand Clarke, Jennifer Clarke, Hoyt A. Koepke category:stat.ML cs.LG published:2015-03-04 summary:Here, we propose a clustering technique for general clustering problemsincluding those that have non-convex clusters. For a given desired number ofclusters $K$, we use three stages to find a clustering. The first stage uses ahybrid clustering technique to produce a series of clusterings of various sizes(randomly selected). They key steps are to find a $K$-means clustering using$K_\ell$ clusters where $K_\ell \gg K$ and then joins these small clusters byusing single linkage clustering. The second stage stabilizes the result ofstage one by reclustering via the `membership matrix' under Hamming distance togenerate a dendrogram. The third stage is to cut the dendrogram to get $K^*$clusters where $K^* \geq K$ and then prune back to $K$ to give a finalclustering. A variant on our technique also gives a reasonable estimate for$K_T$, the true number of clusters. We provide a series of arguments to justify the steps in the stages of ourmethods and we provide numerous examples involving real and simulated data tocompare our technique with other related techniques.
arxiv-9300-40 | Frequency Domain TOF: Encoding Object Depth in Modulation Frequency | http://arxiv.org/pdf/1503.01804v1.pdf | author:Achuta Kadambi, Vage Taamazyan, Suren Jayasuriya, Ramesh Raskar category:cs.CV cs.GR published:2015-03-05 summary:Time of flight cameras may emerge as the 3-D sensor of choice. Today, time offlight sensors use phase-based sampling, where the phase delay between emittedand received, high-frequency signals encodes distance. In this paper, wepresent a new time of flight architecture that relies only on frequency---werefer to this technique as frequency-domain time of flight (FD-TOF). Inspiredby optical coherence tomography (OCT), FD-TOF excels when frequency bandwidthis high. With the increasing frequency of TOF sensors, new challenges to timeof flight sensing continue to emerge. At high frequencies, FD-TOF offersseveral potential benefits over phase-based time of flight methods.
arxiv-9300-41 | Efficient Estimation of Mutual Information for Strongly Dependent Variables | http://arxiv.org/pdf/1411.2003v3.pdf | author:Shuyang Gao, Greg Ver Steeg, Aram Galstyan category:cs.IT math.IT stat.ML published:2014-11-07 summary:We demonstrate that a popular class of nonparametric mutual information (MI)estimators based on k-nearest-neighbor graphs requires number of samples thatscales exponentially with the true MI. Consequently, accurate estimation of MIbetween two strongly dependent variables is possible only for prohibitivelylarge sample size. This important yet overlooked shortcoming of the existingestimators is due to their implicit reliance on local uniformity of theunderlying joint distribution. We introduce a new estimator that is robust tolocal non-uniformity, works well with limited data, and is able to capturerelationship strengths over many orders of magnitude. We demonstrate thesuperior performance of the proposed estimator on both synthetic and real-worlddata.
arxiv-9300-42 | Learning Stochastic Recurrent Networks | http://arxiv.org/pdf/1411.7610v3.pdf | author:Justin Bayer, Christian Osendorfer category:stat.ML cs.LG published:2014-11-27 summary:Leveraging advances in variational inference, we propose to enhance recurrentneural networks with latent variables, resulting in Stochastic RecurrentNetworks (STORNs). The model i) can be trained with stochastic gradientmethods, ii) allows structured and multi-modal conditionals at each time step,iii) features a reliable estimator of the marginal likelihood and iv) is ageneralisation of deterministic recurrent neural networks. We evaluate themethod on four polyphonic musical data sets and motion capture data.
arxiv-9300-43 | Correct-by-synthesis reinforcement learning with temporal logic constraints | http://arxiv.org/pdf/1503.01793v1.pdf | author:Min Wen, Ruediger Ehlers, Ufuk Topcu category:cs.LO cs.GT cs.LG cs.SY published:2015-03-05 summary:We consider a problem on the synthesis of reactive controllers that optimizesome a priori unknown performance criterion while interacting with anuncontrolled environment such that the system satisfies a given temporal logicspecification. We decouple the problem into two subproblems. First, we extracta (maximally) permissive strategy for the system, which encodes multiple(possibly all) ways in which the system can react to the adversarialenvironment and satisfy the specifications. Then, we quantify the a prioriunknown performance criterion as a (still unknown) reward function and computean optimal strategy for the system within the operating envelope allowed by thepermissive strategy by using the so-called maximin-Q learning algorithm. Weestablish both correctness (with respect to the temporal logic specifications)and optimality (with respect to the a priori unknown performance criterion) ofthis two-step technique for a fragment of temporal logic specifications. Forspecifications beyond this fragment, correctness can still be preserved, butthe learned strategy may be sub-optimal. We present an algorithm to the overallproblem, and demonstrate its use and computational requirements on a set ofrobot motion planning examples.
arxiv-9300-44 | Min-Max Kernels | http://arxiv.org/pdf/1503.01737v1.pdf | author:Ping Li category:stat.ML cs.LG stat.CO published:2015-03-05 summary:The min-max kernel is a generalization of the popular resemblance kernel(which is designed for binary data). In this paper, we demonstrate, through anextensive classification study using kernel machines, that the min-max kerneloften provides an effective measure of similarity for nonnegative data. As themin-max kernel is nonlinear and might be difficult to be used for industrialapplications with massive data, we show that the min-max kernel can belinearized via hashing techniques. This allows practitioners to apply min-maxkernel to large-scale applications using well matured linear algorithms such aslinear SVM or logistic regression. The previous remarkable work on consistent weighted sampling (CWS) producessamples in the form of ($i^*, t^*$) where the $i^*$ records the location (andin fact also the weights) information analogous to the samples produced byclassical minwise hashing on binary data. Because the $t^*$ is theoreticallyunbounded, it was not immediately clear how to effectively implement CWS forbuilding large-scale linear classifiers. In this paper, we provide a simplesolution by discarding $t^*$ (which we refer to as the "0-bit" scheme). Via anextensive empirical study, we show that this 0-bit scheme does not loseessential information. We then apply the "0-bit" CWS for building linearclassifiers to approximate min-max kernel classifiers, as extensively validatedon a wide range of publicly available classification datasets. We expect thiswork will generate interests among data mining practitioners who would like toefficiently utilize the nonlinear information of non-binary and nonnegativedata.
arxiv-9300-45 | Error-Correcting Factorization | http://arxiv.org/pdf/1502.07976v2.pdf | author:Miguel Angel Bautista, Oriol Pujol, Fernando de la Torre, Sergio Escalera category:cs.CV cs.LG published:2015-02-27 summary:Error Correcting Output Codes (ECOC) is a successful technique in multi-classclassification, which is a core problem in Pattern Recognition and MachineLearning. A major advantage of ECOC over other methods is that the multi- classproblem is decoupled into a set of binary problems that are solvedindependently. However, literature defines a general error-correctingcapability for ECOCs without analyzing how it distributes among classes,hindering a deeper analysis of pair-wise error-correction. To address theselimitations this paper proposes an Error-Correcting Factorization (ECF) method,our contribution is three fold: (I) We propose a novel representation of theerror-correction capability, called the design matrix, that enables us to buildan ECOC on the basis of allocating correction to pairs of classes. (II) Wederive the optimal code length of an ECOC using rank properties of the designmatrix. (III) ECF is formulated as a discrete optimization problem, and arelaxed solution is found using an efficient constrained block coordinatedescent approach. (IV) Enabled by the flexibility introduced with the designmatrix we propose to allocate the error-correction on classes that are prone toconfusion. Experimental results in several databases show that when allocatingthe error-correction to confusable classes ECF outperforms state-of-the-artapproaches.
arxiv-9300-46 | Color Image Classification via Quaternion Principal Component Analysis Network | http://arxiv.org/pdf/1503.01657v1.pdf | author:Rui Zeng, Jiasong Wu, Zhuhong Shao, Yang Chen, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2015-03-05 summary:The Principal Component Analysis Network (PCANet), which is one of therecently proposed deep learning architectures, achieves the state-of-the-artclassification accuracy in various databases. However, the performance ofPCANet may be degraded when dealing with color images. In this paper, aQuaternion Principal Component Analysis Network (QPCANet), which is anextension of PCANet, is proposed for color images classification. Compared toPCANet, the proposed QPCANet takes into account the spatial distributioninformation of color images and ensures larger amount of intra-class invarianceof color images. Experiments conducted on different color image datasets suchas Caltech-101, UC Merced Land Use, Georgia Tech face and CURet have revealedthat the proposed QPCANet achieves higher classification accuracy than PCANet.
arxiv-9300-47 | Video-Based Facial Expression Recognition Using Local Directional Binary Pattern | http://arxiv.org/pdf/1503.01646v1.pdf | author:Sahar Hooshmand, Ali Jamali Avilaq, Amir Hossein Rezaie category:cs.CV published:2015-03-05 summary:Automatic facial expression analysis is a challenging issue and influenced somany areas such as human computer interaction. Due to the uncertainties of thelight intensity and light direction, the face gray shades are uneven and theexpression recognition rate under simple Local Binary Pattern is not ideal andpromising. In this paper we propose two state-of-the-art descriptors forperson-independent facial expression recognition. First the face regions of thewhole images in a video sequence are modeled with Volume Local DirectionalBinary pattern (VLDBP), which is an extended version of the LDBP operator,incorporating movement and appearance together. To make the surveycomputationally simple and easy to expand, only the co-occurrences of the LocalDirectional Binary Pattern on three orthogonal planes (LDBP-TOP) are debated.After extracting the feature vectors the K-Nearest Neighbor classifier was usedto recognize the expressions. The proposed methods are applied to the videos ofthe Extended Cohn-Kanade database (CK+) and the experimental outcomesdemonstrate that the offered techniques achieve more accuracy in comparisonwith the classic and traditional algorithms.
arxiv-9300-48 | Sparsistency and agnostic inference in sparse PCA | http://arxiv.org/pdf/1401.6978v3.pdf | author:Jing Lei, Vincent Q. Vu category:math.ST stat.ML stat.TH published:2014-01-27 summary:The presence of a sparse "truth" has been a constant assumption in thetheoretical analysis of sparse PCA and is often implicit in its methodologicaldevelopment. This naturally raises questions about the properties of sparse PCAmethods and how they depend on the assumption of sparsity. Under whatconditions can the relevant variables be selected consistently if the truth isassumed to be sparse? What can be said about the results of sparse PCA withoutassuming a sparse and unique truth? We answer these questions by investigatingthe properties of the recently proposed Fantope projection and selection (FPS)method in the high-dimensional setting. Our results provide general sufficientconditions for sparsistency of the FPS estimator. These conditions are weak andcan hold in situations where other estimators are known to fail. On the otherhand, without assuming sparsity or identifiability, we show that FPS provides asparse, linear dimension-reducing transformation that is close to the bestpossible in terms of maximizing the predictive covariance.
arxiv-9300-49 | Convex Optimization for Parallel Energy Minimization | http://arxiv.org/pdf/1503.01563v1.pdf | author:K. S. Sesh Kumar, Alvaro Barbero, Stefanie Jegelka, Suvrit Sra, Francis Bach category:cs.CV math.OC published:2015-03-05 summary:Energy minimization has been an intensely studied core problem in computervision. With growing image sizes (2D and 3D), it is now highly desirable to runenergy minimization algorithms in parallel. But many existing algorithms, inparticular, some efficient combinatorial algorithms, are difficult topar-allelize. By exploiting results from convex and submodular theory, wereformulate the quadratic energy minimization problem as a total variationdenoising problem, which, when viewed geometrically, enables the use ofprojection and reflection based convex methods. The resulting min-cut algorithm(and code) is conceptually very simple, and solves a sequence of TV denoisingproblems. We perform an extensive empirical evaluation comparingstate-of-the-art combinatorial algorithms and convex optimization techniques.On small problems the iterative convex methods match the combinatorial max-flowalgorithms, while on larger problems they offer other flexibility and importantgains: (a) their memory footprint is small; (b) their straightforwardparallelizability fits multi-core platforms; (c) they can easily bewarm-started; and (d) they quickly reach approximately good solutions, therebyenabling faster "inexact" solutions. A key consequence of our approach based onsubmodularity and convexity is that it is allows to combine any arbitrarycombinatorial or convex methods as subroutines, which allows one to obtainhybrid combinatorial and convex optimization algorithms that benefit from thestrengths of both.
arxiv-9300-50 | Visualization of Clandestine Labs from Seizure Reports: Thematic Mapping and Data Mining Research Directions | http://arxiv.org/pdf/1503.01549v1.pdf | author:William Hsu, Mohammed Abduljabbar, Ryuichi Osuga, Max Lu, Wesam Elshamy category:cs.IR cs.CL published:2015-03-05 summary:The problem of spatiotemporal event visualization based on reports entailssubtasks ranging from named entity recognition to relationship extraction andmapping of events. We present an approach to event extraction that is driven bydata mining and visualization goals, particularly thematic mapping and trendanalysis. This paper focuses on bridging the information extraction andvisualization tasks and investigates topic modeling approaches. We develop astatic, finite topic model and examine the potential benefits and feasibilityof extending this to dynamic topic modeling with a large number of topics andcontinuous time. We describe an experimental test bed for event mapping thatuses this end-to-end information retrieval system, and report preliminaryresults on a geoinformatics problem: tracking of methamphetamine lab seizureevents across time and space.
arxiv-9300-51 | A Statistical Parsing Framework for Sentiment Classification | http://arxiv.org/pdf/1401.6330v2.pdf | author:Li Dong, Furu Wei, Shujie Liu, Ming Zhou, Ke Xu category:cs.CL published:2014-01-24 summary:We present a statistical parsing framework for sentence-level sentimentclassification in this article. Unlike previous works that employ syntacticparsing results for sentiment analysis, we develop a statistical parser todirectly analyze the sentiment structure of a sentence. We show thatcomplicated phenomena in sentiment analysis (e.g., negation, intensification,and contrast) can be handled the same as simple and straightforward sentimentexpressions in a unified and probabilistic way. We formulate the sentimentgrammar upon Context-Free Grammars (CFGs), and provide a formal description ofthe sentiment parsing framework. We develop the parsing model to obtainpossible sentiment parse trees for a sentence, from which the polarity model isproposed to derive the sentiment strength and polarity, and the ranking modelis dedicated to selecting the best sentiment tree. We train the parser directlyfrom examples of sentences annotated only with sentiment polarity labels butwithout any syntactic annotations or polarity annotations of constituentswithin sentences. Therefore we can obtain training data easily. In particular,we train a sentiment parser, s.parser, from a large amount of review sentenceswith users' ratings as rough sentiment polarity labels. Extensive experimentson existing benchmark datasets show significant improvements over baselinesentiment classification approaches.
arxiv-9300-52 | Learning to rank in person re-identification with metric ensembles | http://arxiv.org/pdf/1503.01543v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV published:2015-03-05 summary:We propose an effective structured learning based approach to the problem ofperson re-identification which outperforms the current state-of-the-art on mostbenchmark data sets evaluated. Our framework is built on the basis of multiplelow-level hand-crafted and high-level visual features. We then formulate twooptimization algorithms, which directly optimize evaluation measures commonlyused in person re-identification, also known as the Cumulative MatchingCharacteristic (CMC) curve. Our new approach is practical to many real-worldsurveillance applications as the re-identification performance can beconcentrated in the range of most practical importance. The combination ofthese factors leads to a person re-identification system which outperforms mostexisting algorithms. More importantly, we advance state-of-the-art results onperson re-identification by improving the rank-$1$ recognition rates from$40\%$ to $50\%$ on the iLIDS benchmark, $16\%$ to $18\%$ on the PRID2011benchmark, $43\%$ to $46\%$ on the VIPeR benchmark, $34\%$ to $53\%$ on theCUHK01 benchmark and $21\%$ to $62\%$ on the CUHK03 benchmark.
arxiv-9300-53 | Pyrcca: regularized kernel canonical correlation analysis in Python and its applications to neuroimaging | http://arxiv.org/pdf/1503.01538v1.pdf | author:Natalia Y. Bilenko, Jack L. Gallant category:q-bio.QM cs.CV stat.ML published:2015-03-05 summary:Canonical correlation analysis (CCA) is a valuable method for interpretingcross-covariance across related datasets of different dimensionality. There aremany potential applications of CCA to neuroimaging data analysis. For instance,CCA can be used for finding functional similarities across fMRI datasetscollected from multiple subjects without resampling individual datasets to atemplate anatomy. In this paper, we introduce Pyrcca, an open-source Pythonmodule for executing CCA between two or more datasets. Pyrcca can be used toimplement CCA with or without regularization, and with or without linear or aGaussian kernelization of the datasets. We demonstrate an application of CCAimplemented with Pyrcca to neuroimaging data analysis. We use CCA to find adata-driven set of functional response patterns that are similar acrossindividual subjects in a natural movie experiment. We then demonstrate how thisset of response patterns discovered by CCA can be used to accurately predictsubject responses to novel natural movie stimuli.
arxiv-9300-54 | Deep Temporal Appearance-Geometry Network for Facial Expression Recognition | http://arxiv.org/pdf/1503.01532v1.pdf | author:Heechul Jung, Sihaeng Lee, Sunjeong Park, Injae Lee, Chunghyun Ahn, Junmo Kim category:cs.CV published:2015-03-05 summary:Temporal information can provide useful features for recognizing facialexpressions. However, to manually design useful features requires a lot ofeffort. In this paper, to reduce this effort, a deep learning technique whichis regarded as a tool to automatically extract useful features from raw data,is adopted. Our deep network is based on two different models. The first deepnetwork extracts temporal geometry features from temporal facial landmarkpoints, while the other deep network extracts temporal appearance features fromimage sequences . These two models are combined in order to boost theperformance of the facial expression recognition. Through several experiments,we showed that the two models cooperate with each other. As a result, weachieved superior performance to other state-of-the-art methods in CK+ andOulu-CASIA databases. Furthermore, one of the main contributions of this paperis that our deep network catches the facial action points automatically.
arxiv-9300-55 | Spectral Clustering by Ellipsoid and Its Connection to Separable Nonnegative Matrix Factorization | http://arxiv.org/pdf/1503.01531v1.pdf | author:Tomohiko Mizutani category:cs.CV published:2015-03-05 summary:This paper proposes a variant of the normalized cut algorithm for spectralclustering. Although the normalized cut algorithm applies the K-means algorithmto the eigenvectors of a normalized graph Laplacian for finding clusters, ouralgorithm instead uses a minimum volume enclosing ellipsoid for them. We showthat the algorithm shares similarity with the ellipsoidal rounding algorithmfor separable nonnegative matrix factorization. Our theoretical insight impliesthat the algorithm can serve as a bridge between spectral clustering andseparable NMF. The K-means algorithm has the issues in that the choice ofinitial points affects the construction of clusters and certain choices resultin poor clustering performance. The normalized cut algorithm inherits theseissues since K-means is incorporated in it, whereas the algorithm proposed heredoes not. An empirical study is presented to examine the performance of thealgorithm.
arxiv-9300-56 | Genetic optimization of the Hyperloop route through the Grapevine | http://arxiv.org/pdf/1503.01524v1.pdf | author:Casey J. Handmer category:cs.NE published:2015-03-05 summary:We demonstrate a genetic algorithm that employs a versatile fitness functionto optimize route selection for the Hyperloop, a proposed high speed passengertransportation system.
arxiv-9300-57 | Private Empirical Risk Minimization Beyond the Worst Case: The Effect of the Constraint Set Geometry | http://arxiv.org/pdf/1411.5417v2.pdf | author:Kunal Talwar, Abhradeep Thakurta, Li Zhang category:cs.LG cs.CR stat.ML published:2014-11-20 summary:Empirical Risk Minimization (ERM) is a standard technique in machinelearning, where a model is selected by minimizing a loss function overconstraint set. When the training dataset consists of private information, itis natural to use a differentially private ERM algorithm, and this problem hasbeen the subject of a long line of work started with Chaudhuri and Monteleoni2008. A private ERM algorithm outputs an approximate minimizer of the lossfunction and its error can be measured as the difference from the optimal valueof the loss function. When the constraint set is arbitrary, the required errorbounds are fairly well understood~\cite{BassilyST14}. In this work, we showthat the geometric properties of the constraint set can be used to derivesignificantly better results. Specifically, we show that a differentiallyprivate version of Mirror Descent leads to error bounds of the form$\tilde{O}(G_{\mathcal{C}}/n)$ for a lipschitz loss function, improving on the$\tilde{O}(\sqrt{p}/n)$ bounds in Bassily, Smith and Thakurta 2014. Here $p$ isthe dimensionality of the problem, $n$ is the number of data points in thetraining set, and $G_{\mathcal{C}}$ denotes the Gaussian width of theconstraint set that we optimize over. We show similar improvements for stronglyconvex functions, and for smooth functions. In addition, we show that when theloss function is Lipschitz with respect to the $\ell_1$ norm and $\mathcal{C}$is $\ell_1$-bounded, a differentially private version of the Frank-Wolfealgorithm gives error bounds of the form $\tilde{O}(n^{-2/3})$. This capturesthe important and common case of sparse linear regression (LASSO), when thedata $x_i$ satisfies $x_i_{\infty} \leq 1$ and we optimize over the $\ell_1$ball. We show new lower bounds for this setting, that together with knownbounds, imply that all our upper bounds are tight.
arxiv-9300-58 | Do We Need More Training Data? | http://arxiv.org/pdf/1503.01508v1.pdf | author:Xiangxin Zhu, Carl Vondrick, Charless Fowlkes, Deva Ramanan category:cs.CV published:2015-03-05 summary:Datasets for training object recognition systems are steadily increasing insize. This paper investigates the question of whether existing detectors willcontinue to improve as data grows, or saturate in performance due to limitedmodel complexity and the Bayes risk associated with the feature spaces in whichthey operate. We focus on the popular paradigm of discriminatively trainedtemplates defined on oriented gradient features. We investigate the performanceof mixtures of templates as the number of mixture components and the amount oftraining data grows. Surprisingly, even with proper treatment of regularizationand "outliers", the performance of classic mixture models appears to saturatequickly ($\sim$10 templates and $\sim$100 positive training examples pertemplate). This is not a limitation of the feature space as compositionalmixtures that share template parameters via parts and that can synthesize newtemplates not encountered during training yield significantly betterperformance. Based on our analysis, we conjecture that the greatest gains indetection performance will continue to derive from improved representations andlearning algorithms that can make efficient use of large datasets.
arxiv-9300-59 | Efficient Contextual Semi-Bandit Learning | http://arxiv.org/pdf/1502.05890v2.pdf | author:Akshay Krishnamurthy, Alekh Agarwal, Miroslav Dudik category:cs.LG stat.ML published:2015-02-20 summary:We study a variant of the contextual bandit problem, where on each round, thelearner plays a sequence of actions, receives a feature for each individualaction, and reward that is linearly related to these features. This setting hasapplications to network routing, crowd-sourcing, personalized search, and manyother domains. If the linear transformation is known, we analyze an algorithmthat is structurally similar to the algorithm of Agarwal et a. [2014] and showthat it enjoys a regret bound between $\tilde{O}(\sqrt{KLT \ln N})$ and$\tilde{O}(L\sqrt{KT \ln N})$, where $K$ is the number of actions, $L$ is thelength of each action sequence, $T$ is the number of rounds, and $N$ is thenumber of policies. If the linear transformation is unknown, we show that analgorithm that first explores to learn the unknown weights via linearregression and thereafter uses the estimated weights can achieve$\tilde{O}(\w\_1(KT)^{3/4} \sqrt{\ln N})$ regret, where $w$ is the true(unknown) weight vector. Both algorithms use an optimization oracle to avoidexplicit enumeration of the policies and consequently are computationallyefficient whenever an efficient algorithm for the fully supervised setting isavailable.
arxiv-9300-60 | Local Expectation Gradients for Doubly Stochastic Variational Inference | http://arxiv.org/pdf/1503.01494v1.pdf | author:Michalis K. Titsias category:stat.ML published:2015-03-04 summary:We introduce local expectation gradients which is a general purposestochastic variational inference algorithm for constructing stochasticgradients through sampling from the variational distribution. This algorithmdivides the problem of estimating the stochastic gradients over multiplevariational parameters into smaller sub-tasks so that each sub-task exploitsintelligently the information coming from the most relevant part of thevariational distribution. This is achieved by performing an exact expectationover the single random variable that mostly correlates with the variationalparameter of interest resulting in a Rao-Blackwellized estimate that has lowvariance and can work efficiently for both continuous and discrete randomvariables. Furthermore, the proposed algorithm has interesting similaritieswith Gibbs sampling but at the same time, unlike Gibbs sampling, it can betrivially parallelized.
arxiv-9300-61 | An Entropy Search Portfolio for Bayesian Optimization | http://arxiv.org/pdf/1406.4625v4.pdf | author:Bobak Shahriari, Ziyu Wang, Matthew W. Hoffman, Alexandre Bouchard-Côté, Nando de Freitas category:stat.ML cs.LG published:2014-06-18 summary:Bayesian optimization is a sample-efficient method for black-box globaloptimization. How- ever, the performance of a Bayesian optimization method verymuch depends on its exploration strategy, i.e. the choice of acquisitionfunction, and it is not clear a priori which choice will result in superiorperformance. While portfolio methods provide an effective, principled way ofcombining a collection of acquisition functions, they are often based onmeasures of past performance which can be misleading. To address this issue, weintroduce the Entropy Search Portfolio (ESP): a novel approach to portfolioconstruction which is motivated by information theoretic considerations. Weshow that ESP outperforms existing portfolio methods on several real andsynthetic problems, including geostatistical datasets and simulated controltasks. We not only show that ESP is able to offer performance as good as thebest, but unknown, acquisition function, but surprisingly it often gives betterperformance. Finally, over a wide range of conditions we find that ESP isrobust to the inclusion of poor acquisition functions.
arxiv-9300-62 | Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-$1$ Updates | http://arxiv.org/pdf/1402.5180v4.pdf | author:Animashree Anandkumar, Rong Ge, Majid Janzamin category:cs.LG math.NA stat.ML published:2014-02-21 summary:In this paper, we provide local and global convergence guarantees forrecovering CP (Candecomp/Parafac) tensor decomposition. The main step of theproposed algorithm is a simple alternating rank-$1$ update which is thealternating version of the tensor power iteration adapted for asymmetrictensors. Local convergence guarantees are established for third order tensorsof rank $k$ in $d$ dimensions, when $k=o \bigl( d^{1.5} \bigr)$ and the tensorcomponents are incoherent. Thus, we can recover overcomplete tensordecomposition. We also strengthen the results to global convergence guaranteesunder stricter rank condition $k \le \beta d$ (for arbitrary constant $\beta >1$) through a simple initialization procedure where the algorithm isinitialized by top singular vectors of random tensor slices. Furthermore, theapproximate local convergence guarantees for $p$-th order tensors are alsoprovided under rank condition $k=o \bigl( d^{p/2} \bigr)$. The guarantees alsoinclude tight perturbation analysis given noisy tensor.
arxiv-9300-63 | Toxicity Prediction using Deep Learning | http://arxiv.org/pdf/1503.01445v1.pdf | author:Thomas Unterthiner, Andreas Mayr, Günter Klambauer, Sepp Hochreiter category:stat.ML cs.LG cs.NE q-bio.BM published:2015-03-04 summary:Everyday we are exposed to various chemicals via food additives, cleaning andcosmetic products and medicines -- and some of them might be toxic. Howevertesting the toxicity of all existing compounds by biological experiments isneither financially nor logistically feasible. Therefore the governmentagencies NIH, EPA and FDA launched the Tox21 Data Challenge within the"Toxicology in the 21st Century" (Tox21) initiative. The goal of this challengewas to assess the performance of computational methods in predicting thetoxicity of chemical compounds. State of the art toxicity prediction methodsbuild upon specifically-designed chemical descriptors developed over decades.Though Deep Learning is new to the field and was never applied to toxicityprediction before, it clearly outperformed all other participating methods. Inthis application paper we show that deep nets automatically learn featuresresembling well-established toxicophores. In total, our Deep Learning approachwon both of the panel-challenges (nuclear receptors and stress response) aswell as the overall Grand Challenge, and thereby sets a new standard in toxprediction.
arxiv-9300-64 | Heteroscedastic Treed Bayesian Optimisation | http://arxiv.org/pdf/1410.7172v2.pdf | author:John-Alexander M. Assael, Ziyu Wang, Bobak Shahriari, Nando de Freitas category:cs.LG math.OC stat.ML published:2014-10-27 summary:Optimising black-box functions is important in many disciplines, such astuning machine learning models, robotics, finance and mining exploration.Bayesian optimisation is a state-of-the-art technique for the globaloptimisation of black-box functions which are expensive to evaluate. At thecore of this approach is a Gaussian process prior that captures our beliefabout the distribution over functions. However, in many cases a single Gaussianprocess is not flexible enough to capture non-stationarity in the objectivefunction. Consequently, heteroscedasticity negatively affects performance oftraditional Bayesian methods. In this paper, we propose a novel prior modelwith hierarchical parameter learning that tackles the problem ofnon-stationarity in Bayesian optimisation. Our results demonstrate substantialimprovements in a wide range of applications, including automatic machinelearning and mining exploration.
arxiv-9300-65 | Quantifying Uncertainty in Stochastic Models with Parametric Variability | http://arxiv.org/pdf/1503.01401v1.pdf | author:Kyle S. Hickmann, James M. Hyman, Sara Y. Del Valle category:stat.ML stat.ME published:2015-03-04 summary:We present a method to quantify uncertainty in the predictions made bysimulations of mathematical models that can be applied to a broad class ofstochastic, discrete, and differential equation models. Quantifying uncertaintyis crucial for determining how accurate the model predictions are andidentifying which input parameters affect the outputs of interest. Most of theexisting methods for uncertainty quantification require many samples togenerate accurate results, are unable to differentiate where the uncertainty iscoming from (e.g., parameters or model assumptions), or require a lot ofcomputational resources. Our approach addresses these challenges andopportunities by allowing different types of uncertainty, that is, uncertaintyin input parameters as well as uncertainty created through stochastic modelcomponents. This is done by combining the Karhunen-Loeve decomposition,polynomial chaos expansion, and Bayesian Gaussian process regression to createa statistical surrogate for the stochastic model. The surrogate separates theanalysis of variation arising through stochastic simulation and variationarising through uncertainty in the model parameterization. We illustrate ourapproach by quantifying the uncertainty in a stochastic ordinary differentialequation epidemic model. Specifically, we estimate four quantities of interestfor the epidemic model and show agreement between the surrogate and the actualmodel results.
arxiv-9300-66 | A Hierarchical Approach for Joint Multi-view Object Pose Estimation and Categorization | http://arxiv.org/pdf/1503.01393v1.pdf | author:Mete Ozay, Krzysztof Walas, Ales Leonardis category:cs.CV cs.RO published:2015-03-04 summary:We propose a joint object pose estimation and categorization approach whichextracts information about object poses and categories from the object partsand compositions constructed at different layers of a hierarchical objectrepresentation algorithm, namely Learned Hierarchy of Parts (LHOP). In theproposed approach, we first employ the LHOP to learn hierarchical partlibraries which represent entity parts and compositions across different objectcategories and views. Then, we extract statistical and geometric features fromthe part realizations of the objects in the images in order to represent theinformation about object pose and category at each different layer of thehierarchy. Unlike the traditional approaches which consider specific layers ofthe hierarchies in order to extract information to perform specific tasks, wecombine the information extracted at different layers to solve a joint objectpose estimation and categorization problem using distributed optimizationalgorithms. We examine the proposed generative-discriminative learning approachand the algorithms on two benchmark 2-D multi-view image datasets. The proposedapproach and the algorithms outperform state-of-the-art classification,regression and feature extraction algorithms. In addition, the experimentalresults shed light on the relationship between object categorization, poseestimation and the part realizations observed at different layers of thehierarchy.
arxiv-9300-67 | Large Scale Purchase Prediction with Historical User Actions on B2C Online Retail Platform | http://arxiv.org/pdf/1408.6515v3.pdf | author:Yuyu Zhang, Liang Pang, Lei Shi, Bin Wang category:cs.LG published:2014-08-27 summary:This paper describes the solution of Bazinga Team for Tmall RecommendationPrize 2014. With real-world user action data provided by Tmall, one of thelargest B2C online retail platforms in China, this competition requires topredict future user purchases on Tmall website. Predictions are judged onF1Score, which considers both precision and recall for fair evaluation. Thedata set provided by Tmall contains more than half billion action records fromover ten million distinct users. Such massive data volume poses a bigchallenge, and drives competitors to write every single program in MapReducefashion and run it on distributed cluster. We model the purchase predictionproblem as standard machine learning problem, and mainly employ regression andclassification methods as single models. Individual models are then aggregatedin a two-stage approach, using linear regression for blending, and finally alinear ensemble of blended models. The competition is approaching the end butstill in running during writing this paper. In the end, our team achievesF1Score 6.11 and ranks 7th (out of 7,276 teams in total).
arxiv-9300-68 | Group-Sparse Model Selection: Hardness and Relaxations | http://arxiv.org/pdf/1303.3207v4.pdf | author:Luca Baldassarre, Nirav Bhan, Volkan Cevher, Anastasios Kyrillidis, Siddhartha Satpathi category:cs.LG cs.IT math.IT stat.ML published:2013-03-13 summary:Group-based sparsity models are proven instrumental in linear regressionproblems for recovering signals from much fewer measurements than standardcompressive sensing. The main promise of these models is the recovery of"interpretable" signals through the identification of their constituent groups.In this paper, we establish a combinatorial framework for group-model selectionproblems and highlight the underlying tractability issues. In particular, weshow that the group-model selection problem is equivalent to the well-knownNP-hard weighted maximum coverage problem (WMC). Leveraging a graph-basedunderstanding of group models, we describe group structures which enablecorrect model selection in polynomial time via dynamic programming.Furthermore, group structures that lead to totally unimodular constraints havetractable discrete as well as convex relaxations. We also present ageneralization of the group-model that allows for within group sparsity, whichcan be used to model hierarchical sparsity. Finally, we study the Paretofrontier of group-sparse approximations for two tractable models, among whichthe tree sparsity model, and illustrate selection and computation trade-offsbetween our framework and the existing convex relaxations.
arxiv-9300-69 | Zipf's law holds for phrases, not words | http://arxiv.org/pdf/1406.5181v2.pdf | author:Jake Ryland Williams, Paul R. Lessard, Suma Desu, Eric Clark, James P. Bagrow, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL physics.soc-ph published:2014-06-19 summary:With Zipf's law being originally and most famously observed for wordfrequency, it is surprisingly limited in its applicability to human language,holding over no more than three to four orders of magnitude before hitting aclear break in scaling. Here, building on the simple observation that phrasesof one or more words comprise the most coherent units of meaning in language,we show empirically that Zipf's law for phrases extends over as many as nineorders of rank magnitude. In doing so, we develop a principled and scalablestatistical mechanical method of random text partitioning, which opens up arich frontier of rigorous text analysis via a rank ordering of mixed lengthphrases.
arxiv-9300-70 | The concept "altruism" for sociological research: from conceptualization to operationalization | http://arxiv.org/pdf/1503.01258v1.pdf | author:Oleg V. Pavenkov, Vladimir G. Pavenkov, Mariia V. Rubtcova category:cs.CY cs.CL published:2015-03-04 summary:This article addresses the question of the relevant conceptualization of{\guillemotleft}altruism{\guillemotright} in Russian from the perspectivesociological research operationalization. It investigates the spheres of socialapplication of the word {\guillemotleft}altruism{\guillemotright}, includeRussian equivalent {\guillemotleft}vzaimopomoshh`{\guillemotright} (mutualhelp). The data for the study comes from Russian National Corpus (Russian). Thetheoretical framework consists of Paul F. Lazarsfeld`s Theory of SociologicalResearch Methodology and the Natural Semantic Metalanguage (NSM). Quantitativeanalysis shows features in the representation of altruism in Russian thatsociologists need to know in the preparation of questionnaires, interviewguides and analysis of transcripts.
arxiv-9300-71 | Novel Deviation Bounds for Mixture of Independent Bernoulli Variables with Application to the Missing Mass | http://arxiv.org/pdf/1402.6262v5.pdf | author:Bahman Yari Saeed Khanloo category:stat.ML published:2014-02-25 summary:In this paper, we are concerned with obtaining distribution-freeconcentration inequalities for mixture of independent Bernoulli variables thatincorporate a notion of variance. Missing mass is the total probability massassociated to the outcomes that have not been seen in a given sample which isan important quantity that connects density estimates obtained from a sample tothe population for discrete distributions. Therefore, we are specificallymotivated to apply our method to study the concentration of missing mass -which can be expressed as a mixture of Bernoulli - in a novel way. We not only derive - for the first time - Bernstein-like large deviationbounds for the missing mass whose exponents behave almost linearly with respectto deviation size, but also sharpen McAllester and Ortiz (2003) and Berend andKontorovich (2013) for large sample sizes in the case of small deviations whichis the most interesting case in learning theory. In the meantime, our approachshows that the heterogeneity issue introduced in McAllester and Ortiz (2003) isresolvable in the case of missing mass in the sense that one can use standardinequalities but it may not lead to strong results. Thus, we postulate that ourresults are general and can be applied to provide potentially sharpBernstein-like bounds under some constraints.
arxiv-9300-72 | On the Effects of Low-Quality Training Data on Information Extraction from Clinical Reports | http://arxiv.org/pdf/1502.05472v2.pdf | author:Diego Marcheggiani, Fabrizio Sebastiani category:cs.LG cs.CL cs.IR published:2015-02-19 summary:In the last five years there has been a flurry of work on informationextraction from clinical documents, i.e., on algorithms capable of extracting,from the informal and unstructured texts that are generated during everydayclinical practice, mentions of concepts relevant to such practice. Most of thisliterature is about methods based on supervised learning, i.e., methods fortraining an information extraction system from manually annotated examples.While a lot of work has been devoted to devising learning methods that generatemore and more accurate information extractors, no work has been devoted toinvestigating the effect of the quality of training data on the learningprocess. Low quality in training data often derives from the fact that theperson who has annotated the data is different from the one against whosejudgment the automatically annotated data must be evaluated. In this paper wetest the impact of such data quality issues on the accuracy of informationextraction systems as applied to the clinical domain. We do this by comparingthe accuracy deriving from training data annotated by the authoritative coder(i.e., the one who has also annotated the test data, and by whose judgment wemust abide), with the accuracy deriving from training data annotated by adifferent coder. The results indicate that, although the disagreement betweenthe two coders (as measured on the training set) is substantial, the differenceis (surprisingly enough) not always statistically significant.
arxiv-9300-73 | Large Dimensional Analysis of Robust M-Estimators of Covariance with Outliers | http://arxiv.org/pdf/1503.01245v1.pdf | author:David Morales-Jimenez, Romain Couillet, Matthew R. McKay category:math.ST cs.IT math.IT stat.ML stat.TH published:2015-03-04 summary:A large dimensional characterization of robust M-estimators of covariance (orscatter) is provided under the assumption that the dataset comprisesindependent (essentially Gaussian) legitimate samples as well as arbitrarydeterministic samples, referred to as outliers. Building upon recent randommatrix advances in the area of robust statistics, we specifically show that theso-called Maronna M-estimator of scatter asymptotically behaves similar towell-known random matrices when the population and sample sizes grow togetherto infinity. The introduction of outliers leads the robust estimator to behaveasymptotically as the weighted sum of the sample outer products, with aconstant weight for all legitimate samples and different weights for theoutliers. A fine analysis of this structure reveals importantly that thepropensity of the M-estimator to attenuate (or enhance) the impact of outliersis mostly dictated by the alignment of the outliers with the inverse populationcovariance matrix of the legitimate samples. Thus, robust M-estimators canbring substantial benefits over more simplistic estimators such as theper-sample normalized version of the sample covariance matrix, which is notcapable of differentiating the outlying samples. The analysis shows that,within the class of Maronna's estimators of scatter, the Huber estimator ismost favorable for rejecting outliers. On the contrary, estimators more similarto Tyler's scale invariant estimator (often preferred in the literature) runthe risk of inadvertently enhancing some outliers.
arxiv-9300-74 | Active Sample Learning and Feature Selection: A Unified Approach | http://arxiv.org/pdf/1503.01239v1.pdf | author:Changsheng Li, Xiangfeng Wang, Weishan Dong, Junchi Yan, Qingshan Liu, Hongyuan Zha category:cs.LG published:2015-03-04 summary:This paper focuses on the problem of simultaneous sample and featureselection for machine learning in a fully unsupervised setting. Though mostexisting works tackle these two problems separately that derives twowell-studied sub-areas namely active learning and feature selection, a unifiedapproach is inspirational since they are often interleaved with each other.Noisy and high-dimensional features will bring adverse effect on sampleselection, while `good' samples will be beneficial to feature selection. Wepresent a unified framework to conduct active learning and feature selectionsimultaneously. From the data reconstruction perspective, both the selectedsamples and features can best approximate the original dataset respectively,such that the selected samples characterized by the selected features are veryrepresentative. Additionally our method is one-shot without iterativelyselecting samples for progressive labeling. Thus our model is especiallysuitable when the initial labeled samples are scarce or totally absent, whichexisting works hardly address particularly for simultaneous feature selection.To alleviate the NP-hardness of the raw problem, the proposed formulationinvolves a convex but non-smooth optimization problem. We solve it efficientlyby an iterative algorithm, and prove its global convergence. Experiments onpublicly available datasets validate that our method is promising compared withthe state-of-the-arts.
arxiv-9300-75 | Bethe Learning of Conditional Random Fields via MAP Decoding | http://arxiv.org/pdf/1503.01228v1.pdf | author:Kui Tang, Nicholas Ruozzi, David Belanger, Tony Jebara category:cs.LG cs.CV stat.ML published:2015-03-04 summary:Many machine learning tasks can be formulated in terms of predictingstructured outputs. In frameworks such as the structured support vector machine(SVM-Struct) and the structured perceptron, discriminative functions arelearned by iteratively applying efficient maximum a posteriori (MAP) decoding.However, maximum likelihood estimation (MLE) of probabilistic models over thesesame structured spaces requires computing partition functions, which isgenerally intractable. This paper presents a method for learning discreteexponential family models using the Bethe approximation to the MLE. Remarkably,this problem also reduces to iterative (MAP) decoding. This connection emergesby combining the Bethe approximation with a Frank-Wolfe (FW) algorithm on aconvex dual objective which circumvents the intractable partition function. Theresult is a new single loop algorithm MLE-Struct, which is substantially moreefficient than previous double-loop methods for approximate maximum likelihoodestimation. Our algorithm outperforms existing methods in experiments involvingimage segmentation, matching problems from vision, and a new dataset ofuniversity roommate assignments.
arxiv-9300-76 | Low-dimensional Models in Spatio-Temporal Wind Speed Forecasting | http://arxiv.org/pdf/1503.01210v1.pdf | author:Borhan M. Sanandaji, Akin Tascikaraoglu, Kameshwar Poolla, Pravin Varaiya category:cs.SY stat.ML published:2015-03-04 summary:Integrating wind power into the grid is challenging because of its randomnature. Integration is facilitated with accurate short-term forecasts of windpower. The paper presents a spatio-temporal wind speed forecasting algorithmthat incorporates the time series data of a target station and data ofsurrounding stations. Inspired by Compressive Sensing (CS) andstructured-sparse recovery algorithms, we claim that there usually exists anintrinsic low-dimensional structure governing a large collection of stationsthat should be exploited. We cast the forecasting problem as recovery of ablock-sparse signal $\boldsymbol{x}$ from a set of linear equations$\boldsymbol{b} = A\boldsymbol{x}$ for which we propose novel structure-sparserecovery algorithms. Results of a case study in the east coast show that theproposed Compressive Spatio-Temporal Wind Speed Forecasting (CST-WSF) algorithmsignificantly improves the short-term forecasts compared to a set ofwidely-used benchmark models.
arxiv-9300-77 | An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks | http://arxiv.org/pdf/1312.6211v3.pdf | author:Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2013-12-21 summary:Catastrophic forgetting is a problem faced by many machine learning modelsand algorithms. When trained on one task, then trained on a second task, manymachine learning models "forget" how to perform the first task. This is widelybelieved to be a serious problem for neural networks. Here, we investigate theextent to which the catastrophic forgetting problem occurs for modern neuralnetworks, comparing both established and recent gradient-based trainingalgorithms and activation functions. We also examine the effect of therelationship between the first task and the second task on catastrophicforgetting. We find that it is always best to train using the dropoutalgorithm--the dropout algorithm is consistently best at adapting to the newtask, remembering the old task, and has the best tradeoff curve between thesetwo extremes. We find that different tasks and relationships between tasksresult in very different rankings of activation function performance. Thissuggests the choice of activation function should always be cross-validated.
arxiv-9300-78 | Statistical modality tagging from rule-based annotations and crowdsourcing | http://arxiv.org/pdf/1503.01190v1.pdf | author:Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen Rambow, Benjamin Van Durme category:cs.CL cs.LG stat.ML published:2015-03-04 summary:We explore training an automatic modality tagger. Modality is the attitudethat a speaker might have toward an event or state. One of the main hurdles fortraining a linguistic tagger is gathering training data. This is particularlyproblematic for training a tagger for modality because modality triggers aresparse for the overwhelming majority of sentences. We investigate an approachto automatically training a modality tagger where we first gathered sentencesbased on a high-recall simple rule-based modality tagger and then providedthese sentences to Mechanical Turk annotators for further annotation. We usedthe resulting set of training data to train a precise modality tagger using amulti-class SVM that delivers good performance.
arxiv-9300-79 | On Online Control of False Discovery Rate | http://arxiv.org/pdf/1502.06197v2.pdf | author:Adel Javanmard, Andrea Montanari category:stat.ME cs.LG math.ST stat.AP stat.TH published:2015-02-22 summary:Multiple hypotheses testing is a core problem in statistical inference andarises in almost every scientific field. Given a sequence of null hypotheses$\mathcal{H}(n) = (H_1,..., H_n)$, Benjamini and Hochberg\cite{benjamini1995controlling} introduced the false discovery rate (FDR)criterion, which is the expected proportion of false positives among rejectednull hypotheses, and proposed a testing procedure that controls FDR below apre-assigned significance level. They also proposed a different criterion,called mFDR, which does not control a property of the realized set of tests;rather it controls the ratio of expected number of false discoveries to theexpected number of discoveries. In this paper, we propose two procedures for multiple hypotheses testing thatwe will call "LOND" and "LORD". These procedures control FDR and mFDR in an\emph{online manner}. Concretely, we consider an ordered --possibly infinite--sequence of null hypotheses $\mathcal{H} = (H_1,H_2,H_3,...)$ where, at eachstep $i$, the statistician must decide whether to reject hypothesis $H_i$having access only to the previous decisions. To the best of our knowledge, ourwork is the first that controls FDR in this setting. This model was introducedby Foster and Stine \cite{alpha-investing} whose alpha-investing rule onlycontrols mFDR in online manner. In order to compare different procedures, we develop lower bounds on thetotal discovery rate under the mixture model and prove that both LOND and LORDhave nearly linear number of discoveries. We further propose adjustment to LONDto address arbitrary correlation among the $p$-values. Finally, we evaluate theperformance of our procedures on both synthetic and real data comparing themwith alpha-investing rule, Benjamin-Hochberg method and a Bonferroni procedure.
arxiv-9300-80 | The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification | http://arxiv.org/pdf/1503.01161v1.pdf | author:Been Kim, Cynthia Rudin, Julie Shah category:stat.ML cs.LG published:2015-03-03 summary:We present the Bayesian Case Model (BCM), a general framework for Bayesiancase-based reasoning (CBR) and prototype classification and clustering. BCMbrings the intuitive power of CBR to a Bayesian generative framework. The BCMlearns prototypes, the "quintessential" observations that best representclusters in a dataset, by performing joint inference on cluster labels,prototypes and important features. Simultaneously, BCM pursues sparsity bylearning subspaces, the sets of features that play important roles in thecharacterization of the prototypes. The prototype and subspace representationprovides quantitative benefits in interpretability while preservingclassification accuracy. Human subject experiments verify statisticallysignificant improvements to participants' understanding when using explanationsproduced by BCM, compared to those given by prior art.
arxiv-9300-81 | Systematic Construction of Anomaly Detection Benchmarks from Real Data | http://arxiv.org/pdf/1503.01158v1.pdf | author:Andrew Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern, Weng-Keen Wong category:cs.AI cs.LG stat.ML published:2015-03-03 summary:Research in anomaly detection suffers from a lack of realistic andpublicly-available data sets. Because of this, most published experiments inanomaly detection validate their algorithms with application-specific casestudies or benchmark datasets of the researchers' construction. This makes itdifficult to compare different methods or to measure progress in the field. Italso limits our ability to understand the factors that determine theperformance of anomaly detection algorithms. This article proposes a newmethodology for empirical analysis and evaluation of anomaly detectionalgorithms. It is based on generating thousands of benchmark datasets bytransforming existing supervised learning benchmark datasets and manipulatingproperties relevant to anomaly detection. The paper identifies and validatesfour important dimensions: (a) point difficulty, (b) relative frequency ofanomalies, (c) clusteredness of anomalies, and (d) relevance of features. Weapply our generated datasets to analyze several leading anomaly detectionalgorithms. The evaluation verifies the importance of these dimensions andshows that, while some algorithms are clearly superior to others, anomalydetection accuracy is determined more by variation in the four dimensions thanby the choice of algorithm.
arxiv-9300-82 | Comparison of Algorithms for Compressed Sensing of Magnetic Resonance Images | http://arxiv.org/pdf/1502.02182v3.pdf | author:Jelena Badnjar category:cs.CV published:2015-02-07 summary:Magnetic resonance imaging (MRI) is an essential medical tool with inherentlyslow data acquisition process. Slow acquisition process requires patient to belong time exposed to scanning apparatus. In recent years significant effortsare made towards the applying Compressive Sensing technique to the acquisitionprocess of MRI and biomedical images. Compressive Sensing is an emerging theoryin signal processing. It aims to reduce the amount of acquired data requiredfor successful signal reconstruction. Reducing the amount of acquired imagecoefficients leads to lower acquisition time, i.e. time of exposition to theMRI apparatus. Using optimization algorithms, satisfactory image quality can beobtained from the small set of acquired samples. A number of optimizationalgorithms for the reconstruction of the biomedical images is proposed in theliterature. In this paper, three commonly used optimization algorithms arecompared and results are presented on the several MRI images.
arxiv-9300-83 | Complexity and universality in the long-range order of words | http://arxiv.org/pdf/1503.01129v1.pdf | author:Marcelo A Montemurro, Damián H Zanette category:cs.CL physics.soc-ph published:2015-03-03 summary:As is the case of many signals produced by complex systems, language presentsa statistical structure that is balanced between order and disorder. Here wereview and extend recent results from quantitative characterisations of thedegree of order in linguistic sequences that give insights into two relevantaspects of language: the presence of statistical universals in word ordering,and the link between semantic information and the statistical linguisticstructure. We first analyse a measure of relative entropy that assesses howmuch the ordering of words contributes to the overall statistical structure oflanguage. This measure presents an almost constant value close to 3.5 bits/wordacross several linguistic families. Then, we show that a direct application ofinformation theory leads to an entropy measure that can quantify and extractsemantic structures from linguistic samples, even without prior knowledge ofthe underlying language.
arxiv-9300-84 | Short Term Memory Capacity in Networks via the Restricted Isometry Property | http://arxiv.org/pdf/1307.7970v4.pdf | author:Adam S. Charles, Han Lun Yap, Christopher J. Rozell category:cs.IT cs.NE math.IT published:2013-07-01 summary:Cortical networks are hypothesized to rely on transient network activity tosupport short term memory (STM). In this paper we study the capacity ofrandomly connected recurrent linear networks for performing STM when the inputsignals are approximately sparse in some basis. We leverage results fromcompressed sensing to provide rigorous non asymptotic recovery guarantees,quantifying the impact of the input sparsity level, the input sparsity basis,and the network characteristics on the system capacity. Our analysisdemonstrates that network memory capacities can scale superlinearly with thenumber of nodes, and in some situations can achieve STM capacities that aremuch larger than the network size. We provide perfect recovery guarantees forfinite sequences and recovery bounds for infinite sequences. The latteranalysis predicts that network STM systems may have an optimal recovery lengththat balances errors due to omission and recall mistakes. Furthermore, we showthat the conditions yielding optimal STM capacity can be embodied in severalnetwork topologies, including networks with sparse or dense connectivities.
arxiv-9300-85 | Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research | http://arxiv.org/pdf/1503.01070v1.pdf | author:Atousa Torabi, Christopher Pal, Hugo Larochelle, Aaron Courville category:cs.CV cs.AI published:2015-03-03 summary:In this work, we introduce a dataset of video annotated with high qualitynatural language phrases describing the visual content in a given segment oftime. Our dataset is based on the Descriptive Video Service (DVS) that is nowencoded on many digital media products such as DVDs. DVS is an audio narrationdescribing the visual elements and actions in a movie for the visuallyimpaired. It is temporally aligned with the movie and mixed with the originalmovie soundtrack. We describe an automatic DVS segmentation and alignmentmethod for movies, that enables us to scale up the collection of a DVS-deriveddataset with minimal human intervention. Using this method, we have collectedthe largest DVS-derived dataset for video description of which we are aware.Our dataset currently includes over 84.6 hours of paired video/sentences from92 DVDs and is growing.
arxiv-9300-86 | Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP) | http://arxiv.org/pdf/1503.01057v1.pdf | author:Andrew Gordon Wilson, Hannes Nickisch category:cs.LG stat.ML published:2015-03-03 summary:We introduce a new structured kernel interpolation (SKI) framework, whichgeneralises and unifies inducing point methods for scalable Gaussian processes(GPs). SKI methods produce kernel approximations for fast computations throughkernel interpolation. The SKI framework clarifies how the quality of aninducing point approach depends on the number of inducing (aka interpolation)points, interpolation strategy, and GP covariance kernel. SKI also provides amechanism to create new scalable kernel methods, through choosing differentkernel interpolation strategies. Using SKI, with local cubic kernelinterpolation, we introduce KISS-GP, which is 1) more scalable than inducingpoint alternatives, 2) naturally enables Kronecker and Toeplitz algebra forsubstantial additional gains in scalability, without requiring any grid data,and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n)time and storage for GP inference. We evaluate KISS-GP for kernel matrixapproximation, kernel learning, and natural sound modelling.
arxiv-9300-87 | Projection onto the capped simplex | http://arxiv.org/pdf/1503.01002v1.pdf | author:Weiran Wang, Canyi Lu category:cs.LG published:2015-03-03 summary:We provide a simple and efficient algorithm for computing the Euclideanprojection of a point onto the capped simplex---a simplex with an additionaluniform bound on each coordinate---together with an elementary proof. Both theMATLAB and C++ implementations of the proposed algorithm can be downloaded athttps://eng.ucmerced.edu/people/wwang5.
arxiv-9300-88 | Anisotropic Diffusion in ITK | http://arxiv.org/pdf/1503.00992v1.pdf | author:Jean-Marie Mirebeau, Jérôme Fehrenbach, Laurent Risser, Shaza Tobji category:cs.CV math.AP published:2015-03-03 summary:Anisotropic Non-Linear Diffusion is a powerful image processing technique,which allows to simultaneously remove the noise and enhance sharp features intwo or three dimensional images. Anisotropic Diffusion is understood here inthe sense of Weickert, meaning that diffusion tensors are anisotropic andreflect the local orientation of image features. This is in contrast with thenon-linear diffusion filter of Perona and Malik, which only involves scalardiffusion coefficients, in other words isotropic diffusion tensors. In thispaper, we present an anisotropic non-linear diffusion technique we implementedin ITK. This technique is based on a recent adaptive scheme making thediffusion stable and requiring limited numerical resources. (See supplementarydata.)
arxiv-9300-89 | A totally unimodular view of structured sparsity | http://arxiv.org/pdf/1411.1990v2.pdf | author:Marwa El Halabi, Volkan Cevher category:cs.LG stat.ML published:2014-11-07 summary:This paper describes a simple framework for structured sparse recovery basedon convex optimization. We show that many structured sparsity models can benaturally represented by linear matrix inequalities on the support of theunknown parameters, where the constraint matrix has a totally unimodular (TU)structure. For such structured models, tight convex relaxations can be obtainedin polynomial time via linear programming. Our modeling framework unifies theprevalent structured sparsity norms in the literature, introduces newinteresting ones, and renders their tightness and tractability argumentstransparent.
arxiv-9300-90 | Normalization based K means Clustering Algorithm | http://arxiv.org/pdf/1503.00900v1.pdf | author:Deepali Virmani, Shweta Taneja, Geetika Malhotra category:cs.LG cs.DB published:2015-03-03 summary:K-means is an effective clustering technique used to separate similar datainto groups based on initial centroids of clusters. In this paper,Normalization based K-means clustering algorithm(N-K means) is proposed.Proposed N-K means clustering algorithm applies normalization prior toclustering on the available data as well as the proposed approach calculatesinitial centroids based on weights. Experimental results prove the bettermentof proposed N-K means clustering algorithm over existing K-means clusteringalgorithm in terms of complexity and overall performance.
arxiv-9300-91 | A Primal-Dual Algorithmic Framework for Constrained Convex Minimization | http://arxiv.org/pdf/1406.5403v2.pdf | author:Quoc Tran-Dinh, Volkan Cevher category:math.OC stat.ML published:2014-06-20 summary:We present a primal-dual algorithmic framework to obtain approximatesolutions to a prototypical constrained convex optimization problem, andrigorously characterize how common structural assumptions affect the numericalefficiency. Our main analysis technique provides a fresh perspective onNesterov's excessive gap technique in a structured fashion and unifies it withsmoothing and primal-dual methods. For instance, through the choices of a dualsmoothing strategy and a center point, our framework subsumes decompositionalgorithms, augmented Lagrangian as well as the alternating directionmethod-of-multipliers methods as its special cases, and provides optimalconvergence rates on the primal objective residual as well as the primalfeasibility gap of the iterates for all.
arxiv-9300-92 | Self-Dictionary Sparse Regression for Hyperspectral Unmixing: Greedy Pursuit and Pure Pixel Search are Related | http://arxiv.org/pdf/1409.4320v2.pdf | author:Xiao Fu, Wing-Kin Ma, Tsung-Han Chan, José M. Bioucas-Dias category:stat.ML cs.IT math.IT math.OC published:2014-09-15 summary:This paper considers a recently emerged hyperspectral unmixing formulationbased on sparse regression of a self-dictionary multiple measurement vector(SD-MMV) model, wherein the measured hyperspectral pixels are used as thedictionary. Operating under the pure pixel assumption, this SD-MMV formalism isspecial in that it allows simultaneous identification of the endmember spectralsignatures and the number of endmembers. Previous SD-MMV studies mainly focuson convex relaxations. In this study, we explore the alternative of greedypursuit, which generally provides efficient and simple algorithms. Inparticular, we design a greedy SD-MMV algorithm using simultaneous orthogonalmatching pursuit. Intriguingly, the proposed greedy algorithm is shown to beclosely related to some existing pure pixel search algorithms, especially, thesuccessive projection algorithm (SPA). Thus, a link between SD-MMV and purepixel search is revealed. We then perform exact recovery analyses, and provethat the proposed greedy algorithm is robust to noise---including itsidentification of the (unknown) number of endmembers---under a sufficiently lownoise level. The identification performance of the proposed greedy algorithm isdemonstrated through both synthetic and real-data experiments.
arxiv-9300-93 | A Survey On Video Forgery Detection | http://arxiv.org/pdf/1503.00843v1.pdf | author:Sowmya K. N., H. R. Chennamma category:cs.MM cs.CV published:2015-03-03 summary:The Digital Forgeries though not visibly identifiable to human perception itmay alter or meddle with underlying natural statistics of digital content.Tampering involves fiddling with video content in order to cause damage or makeunauthorized alteration/modification. Tampering detection in video iscumbersome compared to image when considering the properties of the video.Tampering impacts need to be studied and the applied technique/method is usedto establish the factual information for legal course in judiciary. In thispaper we give an overview of the prior literature and challenges involved invideo forgery detection where passive approach is found.
arxiv-9300-94 | Robustly Leveraging Prior Knowledge in Text Classification | http://arxiv.org/pdf/1503.00841v1.pdf | author:Biao Liu, Minlie Huang category:cs.CL cs.AI cs.IR cs.LG published:2015-03-03 summary:Prior knowledge has been shown very useful to address many natural languageprocessing tasks. Many approaches have been proposed to formalise a variety ofknowledge, however, whether the proposed approach is robust or sensitive to theknowledge supplied to the model has rarely been discussed. In this paper, wepropose three regularization terms on top of generalized expectation criteria,and conduct extensive experiments to justify the robustness of the proposedmethods. Experimental results demonstrate that our proposed methods obtainremarkable improvements and are much more robust than baselines.
arxiv-9300-95 | Transductive Multi-view Zero-Shot Learning | http://arxiv.org/pdf/1501.04560v2.pdf | author:Yanwei Fu, Timothy M. Hospedales, Tao Xiang, Shaogang Gong category:cs.CV cs.DS cs.MM published:2015-01-19 summary:Most existing zero-shot learning approaches exploit transfer learning via anintermediate-level semantic representation shared between an annotatedauxiliary dataset and a target dataset with different classes and noannotation. A projection from a low-level feature space to the semanticrepresentation space is learned from the auxiliary dataset and is appliedwithout adaptation to the target dataset. In this paper we identify twoinherent limitations with these approaches. First, due to having disjoint andpotentially unrelated classes, the projection functions learned from theauxiliary dataset/domain are biased when applied directly to the targetdataset/domain. We call this problem the projection domain shift problem andpropose a novel framework, transductive multi-view embedding, to solve it. Thesecond limitation is the prototype sparsity problem which refers to the factthat for each target class, only a single prototype is available for zero-shotlearning given a semantic representation. To overcome this problem, a novelheterogeneous multi-view hypergraph label propagation method is formulated forzero-shot learning in the transductive embedding space. It effectively exploitsthe complementary information offered by different semantic representations andtakes advantage of the manifold structures of multiple representation spaces ina coherent manner. We demonstrate through extensive experiments that theproposed approach (1) rectifies the projection shift between the auxiliary andtarget domains, (2) exploits the complementarity of multiple semanticrepresentations, (3) significantly outperforms existing methods for bothzero-shot and N-shot recognition on three image and video benchmark datasets,and (4) enables novel cross-view annotation tasks.
arxiv-9300-96 | $\ell_1$-K-SVD: A Robust Dictionary Learning Algorithm With Simultaneous Update | http://arxiv.org/pdf/1410.0311v2.pdf | author:Subhadip Mukherjee, Rupam Basu, Chandra Sekhar Seelamantula category:cs.CV cs.LG published:2014-08-26 summary:We develop a dictionary learning algorithm by minimizing the $\ell_1$distortion metric on the data term, which is known to be robust fornon-Gaussian noise contamination. The proposed algorithm exploits the idea ofiterative minimization of weighted $\ell_2$ error. We refer to this algorithmas $\ell_1$-K-SVD, where the dictionary atoms and the corresponding sparsecoefficients are simultaneously updated to minimize the $\ell_1$ objective,resulting in noise-robustness. We demonstrate through experiments that the$\ell_1$-K-SVD algorithm results in higher atom recovery rate compared with theK-SVD and the robust dictionary learning (RDL) algorithm proposed by Lu et al.,both in Gaussian and non-Gaussian noise conditions. We also show that, forfixed values of sparsity, number of dictionary atoms, and data-dimension, the$\ell_1$-K-SVD algorithm outperforms the K-SVD and RDL algorithms when thetraining set available is small. We apply the proposed algorithm for denoisingnatural images corrupted by additive Gaussian and Laplacian noise. The imagesdenoised using $\ell_1$-K-SVD are observed to have slightly higher peaksignal-to-noise ratio (PSNR) over K-SVD for Laplacian noise, but theimprovement in structural similarity index (SSIM) is significant (approximately$0.1$) for lower values of input PSNR, indicating the efficacy of the $\ell_1$metric.
arxiv-9300-97 | Context Forest for efficient object detection with large mixture models | http://arxiv.org/pdf/1503.00787v1.pdf | author:Davide Modolo, Alexander Vezhnevets, Vittorio Ferrari category:cs.CV published:2015-03-03 summary:We present Context Forest (ConF), a technique for predicting properties ofthe objects in an image based on its global appearance. Compared to standardnearest-neighbour techniques, ConF is more accurate, fast and memory efficient.We train ConF to predict which aspects of an object class are likely to appearin a given image (e.g. which viewpoint). This enables to speed-upmulti-component object detectors, by automatically selecting the most relevantcomponents to run on that image. This is particularly useful for detectorstrained from large datasets, which typically need many components to fullyabsorb the data and reach their peak performance. ConF provides a speed-up of2x for the DPM detector [1] and of 10x for the EE-SVM detector [2]. To showConF's generality, we also train it to predict at which locations objects arelikely to appear in an image. Incorporating this information in the detectorscore improves mAP performance by about 2% by removing false positivedetections in unlikely locations.
arxiv-9300-98 | Simple, Efficient, and Neural Algorithms for Sparse Coding | http://arxiv.org/pdf/1503.00778v1.pdf | author:Sanjeev Arora, Rong Ge, Tengyu Ma, Ankur Moitra category:cs.LG cs.DS cs.NE stat.ML published:2015-03-02 summary:Sparse coding is a basic task in many fields including signal processing,neuroscience and machine learning where the goal is to learn a basis thatenables a sparse representation of a given set of data, if one exists. Itsstandard formulation is as a non-convex optimization problem which is solved inpractice by heuristics based on alternating minimization. Re- cent work hasresulted in several algorithms for sparse coding with provable guarantees, butsomewhat surprisingly these are outperformed by the simple alternatingminimization heuristics. Here we give a general framework for understandingalternating minimization which we leverage to analyze existing heuristics andto design new ones also with provable guarantees. Some of these algorithms seemimplementable on simple neural architectures, which was the original motivationof Olshausen and Field (1997a) in introducing sparse coding. We also give thefirst efficient algorithm for sparse coding that works almost up to theinformation theoretic limit for sparse recovery on incoherent dictionaries. Allprevious algorithms that approached or surpassed this limit run in timeexponential in some natural parameter. Finally, our algorithms improve upon thesample complexity of existing approaches. We believe that our analysisframework will have applications in other settings where simple iterativealgorithms are used.
arxiv-9300-99 | Grouping and Recognition of Dot Patterns with Straight Offset Polygons | http://arxiv.org/pdf/1503.00769v1.pdf | author:Toshiro Kubota category:cs.CV published:2015-03-02 summary:When the boundary of a familiar object is shown by a series of isolated dots,humans can often recognize the object with ease. This ability can be sustainedwith addition of distracting dots around the object. However, such capabilityhas not been reproduced algorithmically on computers. We introduce a newalgorithm that groups a set of dots into multiple non-disjoint subsets. Itconnects the dots into a spanning tree using the proximity cue. It then appliesthe straight polygon transformation to an initial polygon derived from thespanning tree. The straight polygon divides the space into polygons recursivelyand each polygon can be viewed as grouping of a subset of the dots. The numberof polygons generated is O($n$). We also introduce simple shape selection andrecognition algorithms that can be applied to the grouping result. We used bothnatural and synthetic images to show effectiveness of these algorithms.
arxiv-9300-100 | Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift | http://arxiv.org/pdf/1502.03167v3.pdf | author:Sergey Ioffe, Christian Szegedy category:cs.LG published:2015-02-11 summary:Training Deep Neural Networks is complicated by the fact that thedistribution of each layer's inputs changes during training, as the parametersof the previous layers change. This slows down the training by requiring lowerlearning rates and careful parameter initialization, and makes it notoriouslyhard to train models with saturating nonlinearities. We refer to thisphenomenon as internal covariate shift, and address the problem by normalizinglayer inputs. Our method draws its strength from making normalization a part ofthe model architecture and performing the normalization for each trainingmini-batch. Batch Normalization allows us to use much higher learning rates andbe less careful about initialization. It also acts as a regularizer, in somecases eliminating the need for Dropout. Applied to a state-of-the-art imageclassification model, Batch Normalization achieves the same accuracy with 14times fewer training steps, and beats the original model by a significantmargin. Using an ensemble of batch-normalized networks, we improve upon thebest published result on ImageNet classification: reaching 4.9% top-5validation error (and 4.8% test error), exceeding the accuracy of human raters.
arxiv-9300-101 | Bayesian Optimization of Text Representations | http://arxiv.org/pdf/1503.00693v1.pdf | author:Dani Yogatama, Noah A. Smith category:cs.CL cs.LG stat.ML published:2015-03-02 summary:When applying machine learning to problems in NLP, there are many choices tomake about how to represent input texts. These choices can have a big effect onperformance, but they are often uninteresting to researchers or practitionerswho simply need a module that performs well. We propose an approach tooptimizing over this space of choices, formulating the problem as globaloptimization. We apply a sequential model-based optimization technique and showthat our method makes standard linear models competitive with moresophisticated, expensive state-of-the-art methods based on latent variablemodels or neural networks on various topic classification and sentimentanalysis problems. Our approach is a first step towards black-box NLP systemsthat work with raw text and do not require manual tuning.
arxiv-9300-102 | A review of mean-shift algorithms for clustering | http://arxiv.org/pdf/1503.00687v1.pdf | author:Miguel Á. Carreira-Perpiñán category:cs.LG cs.CV stat.ML published:2015-03-02 summary:A natural way to characterize the cluster structure of a dataset is byfinding regions containing a high density of data. This can be done in anonparametric way with a kernel density estimate, whose modes and henceclusters can be found using mean-shift algorithms. We describe the theory andpractice behind clustering based on kernel density estimates and mean-shiftalgorithms. We discuss the blurring and non-blurring versions of mean-shift;theoretical results about mean-shift algorithms and Gaussian mixtures;relations with scale-space theory, spectral clustering and other algorithms;extensions to tracking, to manifold and graph data, and to manifold denoising;K-modes and Laplacian K-modes algorithms; acceleration strategies for largedatasets; and applications to image segmentation, manifold denoising andmultivalued regression.
arxiv-9300-103 | A Hebbian/Anti-Hebbian Network Derived from Online Non-Negative Matrix Factorization Can Cluster and Discover Sparse Features | http://arxiv.org/pdf/1503.00680v1.pdf | author:Cengiz Pehlevan, Dmitri B. Chklovskii category:q-bio.NC cs.NE stat.ML published:2015-03-02 summary:Despite our extensive knowledge of biophysical properties of neurons, thereis no commonly accepted algorithmic theory of neuronal function. Here weexplore the hypothesis that single-layer neuronal networks perform onlinesymmetric nonnegative matrix factorization (SNMF) of the similarity matrix ofthe streamed data. By starting with the SNMF cost function we derive an onlinealgorithm, which can be implemented by a biologically plausible network withlocal learning rules. We demonstrate that such network performs soft clusteringof the data as well as sparse feature discovery. The derived algorithmreplicates many known aspects of sensory anatomy and biophysical properties ofneurons including unipolar nature of neuronal activity and synaptic weights,local synaptic plasticity rules and the dependence of learning rate oncumulative neuronal activity. Thus, we make a step towards an algorithmictheory of neuronal function, which should facilitate large-scale neural circuitsimulations and biologically inspired artificial intelligence.
arxiv-9300-104 | A Hebbian/Anti-Hebbian Neural Network for Linear Subspace Learning: A Derivation from Multidimensional Scaling of Streaming Data | http://arxiv.org/pdf/1503.00669v1.pdf | author:Cengiz Pehlevan, Tao Hu, Dmitri B. Chklovskii category:q-bio.NC cs.NE stat.ML published:2015-03-02 summary:Neural network models of early sensory processing typically reduce thedimensionality of streaming input data. Such networks learn the principalsubspace, in the sense of principal component analysis (PCA), by adjustingsynaptic weights according to activity-dependent learning rules. When derivedfrom a principled cost function these rules are nonlocal and hence biologicallyimplausible. At the same time, biologically plausible local rules have beenpostulated rather than derived from a principled cost function. Here, to bridgethis gap, we derive a biologically plausible network for subspace learning onstreaming data by minimizing a principled cost function. In a departure fromprevious work, where cost was quantified by the representation, orreconstruction, error, we adopt a multidimensional scaling (MDS) cost functionfor streaming data. The resulting algorithm relies only on biologicallyplausible Hebbian and anti-Hebbian local learning rules. In a stochasticsetting, synaptic weights converge to a stationary state which projects theinput data onto the principal subspace. If the data are generated by anonstationary distribution, the network can track the principal subspace. Thus,our result makes a step towards an algorithmic theory of neural computation.
arxiv-9300-105 | An $\mathcal{O}(n\log n)$ projection operator for weighted $\ell_1$-norm regularization with sum constraint | http://arxiv.org/pdf/1503.00600v1.pdf | author:Weiran Wang category:cs.LG published:2015-03-02 summary:We provide a simple and efficient algorithm for the projection operator forweighted $\ell_1$-norm regularization subject to a sum constraint, togetherwith an elementary proof. The implementation of the proposed algorithm can bedownloaded from the author's homepage.
arxiv-9300-106 | Deep Transfer Network: Unsupervised Domain Adaptation | http://arxiv.org/pdf/1503.00591v1.pdf | author:Xu Zhang, Felix Xinnan Yu, Shih-Fu Chang, Shengjin Wang category:cs.CV published:2015-03-02 summary:Domain adaptation aims at training a classifier in one dataset and applyingit to a related but not identical dataset. One successfully used framework ofdomain adaptation is to learn a transformation to match both the distributionof the features (marginal distribution), and the distribution of the labelsgiven features (conditional distribution). In this paper, we propose a newdomain adaptation framework named Deep Transfer Network (DTN), where the highlyflexible deep neural networks are used to implement such a distributionmatching process. This is achieved by two types of layers in DTN: the shared feature extractionlayers which learn a shared feature subspace in which the marginaldistributions of the source and the target samples are drawn close, and thediscrimination layers which match conditional distributions by classifiertransduction. We also show that DTN has a computation complexity linear to thenumber of training samples, making it suitable to large-scale problems. Bycombining the best paradigms in both worlds (deep neural networks inrecognition, and matching marginal and conditional distributions in domainadaptation), we demonstrate by extensive experiments that DTN improvessignificantly over former methods in both execution time and classificationaccuracy.
arxiv-9300-107 | Recovering PCA from Hybrid-$(\ell_1,\ell_2)$ Sparse Sampling of Data Elements | http://arxiv.org/pdf/1503.00547v1.pdf | author:Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail category:cs.IT cs.LG math.IT stat.ML published:2015-03-02 summary:This paper addresses how well we can recover a data matrix when only given afew of its elements. We present a randomized algorithm that element-wisesparsifies the data, retaining only a few its elements. Our new algorithmindependently samples the data using sampling probabilities that depend on boththe squares ($\ell_2$ sampling) and absolute values ($\ell_1$ sampling) of theentries. We prove that the hybrid algorithm recovers a near-PCA reconstructionof the data from a sublinear sample-size: hybrid-($\ell_1,\ell_2$) inherits the$\ell_2$-ability to sample the important elements as well as the regularizationproperties of $\ell_1$ sampling, and gives strictly better performance thaneither $\ell_1$ or $\ell_2$ on their own. We also give a one-pass version ofour algorithm and show experiments to corroborate the theory.
arxiv-9300-108 | A neuromorphic hardware framework based on population coding | http://arxiv.org/pdf/1503.00505v1.pdf | author:Chetan Singh Thakur, Tara Julia Hamilton, Runchun Wang, Jonathan Tapson, André van Schaik category:cs.NE published:2015-03-02 summary:In the biological nervous system, large neuronal populations workcollaboratively to encode sensory stimuli. These neuronal populations arecharacterised by a diverse distribution of tuning curves, ensuring that theentire range of input stimuli is encoded. Based on these principles, we havedesigned a neuromorphic system called a Trainable Analogue Block (TAB), whichencodes given input stimuli using a large population of neurons with aheterogeneous tuning curve profile. Heterogeneity of tuning curves is achievedusing random device mismatches in VLSI (Very Large Scale Integration) processand by adding a systematic offset to each hidden neuron. Here, we presentmeasurement results of a single test cell fabricated in a 65nm technology toverify the TAB framework. We have mimicked a large population of neurons byre-using measurement results from the test cell by varying offset. We thusdemonstrate the learning capability of the system for various regression tasks.The TAB system may pave the way to improve the design of analogue circuits forcommercial applications, by rendering circuits insensitive to random mismatchthat arises due to the manufacturing process.
arxiv-9300-109 | FPGA Implementation of the CAR Model of the Cochlea | http://arxiv.org/pdf/1503.00504v1.pdf | author:Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Richard F. Lyon, André van Schaik category:cs.NE cs.AR published:2015-03-02 summary:The front end of the human auditory system, the cochlea, converts soundsignals from the outside world into neural impulses transmitted along theauditory pathway for further processing. The cochlea senses and separates soundin a nonlinear active fashion, exhibiting remarkable sensitivity and frequencydiscrimination. Although several electronic models of the cochlea have beenproposed and implemented, none of these are able to reproduce all thecharacteristics of the cochlea, including large dynamic range, large gain andsharp tuning at low sound levels, and low gain and broad tuning at intensesound levels. Here, we implement the Cascade of Asymmetric Resonators (CAR)model of the cochlea on an FPGA. CAR represents the basilar membrane filter inthe Cascade of Asymmetric Resonators with Fast-Acting Compression (CAR-FAC)cochlear model. CAR-FAC is a neuromorphic model of hearing based on a pole-zerofilter cascade model of auditory filtering. It uses simple nonlinear extensionsof conventional digital filter stages that are well suited to FPGAimplementations, so that we are able to implement up to 1224 cochlear sectionson Virtex-6 FPGA to process sound data in real time. The FPGA implementation ofthe electronic cochlea described here may be used as a front-end sound analyserfor various machine-hearing applications.
arxiv-9300-110 | Utility-Theoretic Ranking for Semi-Automated Text Classification | http://arxiv.org/pdf/1503.00491v1.pdf | author:Giacomo Berardi, Andrea Esuli, Fabrizio Sebastiani category:cs.LG published:2015-03-02 summary:\emph{Semi-Automated Text Classification} (SATC) may be defined as the taskof ranking a set $\mathcal{D}$ of automatically labelled textual documents insuch a way that, if a human annotator validates (i.e., inspects and correctswhere appropriate) the documents in a top-ranked portion of $\mathcal{D}$ withthe goal of increasing the overall labelling accuracy of $\mathcal{D}$, theexpected increase is maximized. An obvious SATC strategy is to rank$\mathcal{D}$ so that the documents that the classifier has labelled with thelowest confidence are top-ranked. In this work we show that this strategy issuboptimal. We develop new utility-theoretic ranking methods based on thenotion of \emph{validation gain}, defined as the improvement in classificationeffectiveness that would derive by validating a given automatically labelleddocument. We also propose a new effectiveness measure for SATC-oriented rankingmethods, based on the expected reduction in classification error brought aboutby partially validating a list generated by a given ranking method. We reportthe results of experiments showing that, with respect to the baseline methodabove, and according to the proposed measure, our utility-theoretic rankingmethods can achieve substantially higher expected reductions in classificationerror.
arxiv-9300-111 | Optimality of Poisson processes intensity learning with Gaussian processes | http://arxiv.org/pdf/1409.5103v2.pdf | author:Alisa Kirichenko, Harry van Zanten category:math.ST stat.ML stat.TH published:2014-09-17 summary:In this paper we provide theoretical support for the so-called "SigmoidalGaussian Cox Process" approach to learning the intensity of an inhomogeneousPoisson process on a $d$-dimensional domain. This method was proposed by Adams,Murray and MacKay (ICML, 2009), who developed a tractable computationalapproach and showed in simulation and real data experiments that it can workquite satisfactorily. The results presented in the present paper providetheoretical underpinning of the method. In particular, we show how to tune thepriors on the hyper parameters of the model in order for the procedure toautomatically adapt to the degree of smoothness of the unknown intensity and toachieve optimal convergence rates.
arxiv-9300-112 | Signal inference with unknown response: Calibration-uncertainty renormalized estimator | http://arxiv.org/pdf/1410.6289v2.pdf | author:Sebastian Dorn, Torsten A. Enßlin, Maksim Greiner, Marco Selig, Vanessa Boehm category:astro-ph.IM cs.IT math.IT stat.ML published:2014-10-23 summary:The calibration of a measurement device is crucial for every scientificexperiment, where a signal has to be inferred from data. We present CURE, thecalibration uncertainty renormalized estimator, to reconstruct a signal andsimultaneously the instrument's calibration from the same data without knowingthe exact calibration, but its covariance structure. The idea of CURE,developed in the framework of information field theory, is starting with anassumed calibration to successively include more and more portions ofcalibration uncertainty into the signal inference equations and to absorb theresulting corrections into renormalized signal (and calibration) solutions.Thereby, the signal inference and calibration problem turns into solving asingle system of ordinary differential equations and can be identified withcommon resummation techniques used in field theories. We verify CURE byapplying it to a simplistic toy example and compare it against existentself-calibration schemes, Wiener filter solutions, and Markov Chain Monte Carlosampling. We conclude that the method is able to keep up in accuracy with thebest self-calibration methods and serves as a non-iterative alternative to it.
arxiv-9300-113 | On Parametric Modelling and Inference for Complex-Valued Time Series | http://arxiv.org/pdf/1306.5993v2.pdf | author:Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Jeffrey J. Early category:stat.ME stat.AP stat.CO stat.ML published:2013-06-25 summary:This paper introduces new parametric models for the autocovariance functionsand spectra of complex-valued time series, together with novel modifications offrequency-domain parameter inference methods appropriate for such time series.In particular, we introduce a version of the frequency-domain Whittlelikelihood for complex-valued processes. This represents a nontrivial extensionof the Whittle likelihood for bivariate real-valued processes, ascomplex-valued models can capture structure that is only evident by separatingnegative and positive frequency behaviour. Flexible inference methods for suchparametric models are proposed, and the properties of such methods are derived.The methods are applied to oceanographic and seismic time series, as examplesof naturally occurring sampled complex-valued time processes. We demonstratehow to reduce estimation bias of the Whittle likelihood caused by leakage andaliasing, improving parameter estimation of both real-valued andbivariate/complex-valued processes. We also provide techniques for testing theseries propriety or isotropy, as well as procedures for model choice andsemi-parametric inference.
arxiv-9300-114 | A Generalization of the Borkar-Meyn Theorem for Stochastic Recursive Inclusions | http://arxiv.org/pdf/1502.01953v2.pdf | author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY math.DS stat.ML published:2015-02-06 summary:In this paper the stability theorem of Borkar and Meyn is extended to includethe case when the mean field is a differential inclusion. Two different sets ofsufficient conditions are presented that guarantee the stability andconvergence of stochastic recursive inclusions. Our work builds on the works ofBenaim, Hofbauer and Sorin as well as Borkar and Meyn. As a corollary to one ofthe main theorems, a natural generalization of the Borkar and Meyn Theoremfollows. In addition, the original theorem of Borkar and Meyn is shown to holdunder slightly relaxed assumptions. Finally, as an application to one of themain theorems we discuss a solution to the approximate drift problem.
arxiv-9300-115 | Block stochastic gradient iteration for convex and nonconvex optimization | http://arxiv.org/pdf/1408.2597v3.pdf | author:Yangyang Xu, Wotao Yin category:math.OC cs.LG cs.NA math.NA stat.ML 90C06 published:2014-08-12 summary:The stochastic gradient (SG) method can minimize an objective functioncomposed of a large number of differentiable functions, or solve a stochasticoptimization problem, to a moderate accuracy. The block coordinatedescent/update (BCD) method, on the other hand, handles problems with multipleblocks of variables by updating them one at a time; when the blocks ofvariables are easier to update individually than together, BCD has a lowerper-iteration cost. This paper introduces a method that combines the featuresof SG and BCD for problems with many components in the objective and withmultiple (blocks of) variables. Specifically, a block stochastic gradient (BSG) method is proposed forsolving both convex and nonconvex programs. At each iteration, BSG approximatesthe gradient of the differentiable part of the objective by randomly sampling asmall set of data or sampling a few functions from the sum term in theobjective, and then, using those samples, it updates all the blocks ofvariables in either a deterministic or a randomly shuffled order. Itsconvergence for both convex and nonconvex cases are established in differentsenses. In the convex case, the proposed method has the same order ofconvergence rate as the SG method. In the nonconvex case, its convergence isestablished in terms of the expected violation of a first-order optimalitycondition. The proposed method was numerically tested on problems includingstochastic least squares and logistic regression, which are convex, as well aslow-rank tensor recovery and bilinear logistic regression, which are nonconvex.
arxiv-9300-116 | Phase Transitions in Sparse PCA | http://arxiv.org/pdf/1503.00338v1.pdf | author:Thibault Lesieur, Florent Krzakala, Lenka Zdeborova category:cs.IT math.IT stat.ML published:2015-03-01 summary:We study optimal estimation for sparse principal component analysis when thenumber of non-zero elements is small but on the same order as the dimension ofthe data. We employ approximate message passing (AMP) algorithm and its stateevolution to analyze what is the information theoretically minimal mean-squarederror and the one achieved by AMP in the limit of large sizes. For a specialcase of rank one and large enough density of non-zeros Deshpande and Montanari[1] proved that AMP is asymptotically optimal. We show that both for lowdensity and for large rank the problem undergoes a series of phase transitionssuggesting existence of a region of parameters where estimation is informationtheoretically possible, but AMP (and presumably every other polynomialalgorithm) fails. The analysis of the large rank limit is particularlyinstructive.
arxiv-9300-117 | Sparse Approximation of a Kernel Mean | http://arxiv.org/pdf/1503.00323v1.pdf | author:E. Cruz Cortés, C. Scott category:stat.ML cs.LG published:2015-03-01 summary:Kernel means are frequently used to represent probability distributions inmachine learning problems. In particular, the well known kernel densityestimator and the kernel mean embedding both have the form of a kernel mean.Unfortunately, kernel means are faced with scalability issues. A single pointevaluation of the kernel density estimator, for example, requires a computationtime linear in the training sample size. To address this challenge, we presenta method to efficiently construct a sparse approximation of a kernel mean. Wedo so by first establishing an incoherence-based bound on the approximationerror, and then noticing that, for the case of radial kernels, the bound can beminimized by solving the $k$-center problem. The outcome is a linear timeconstruction of a sparse kernel mean, which also lends itself naturally to anautomatic sparsity selection scheme. We show the computational gains of ourmethod by looking at three problems involving kernel means: Euclidean embeddingof distributions, class proportion estimation, and clustering using themean-shift algorithm.
arxiv-9300-118 | Constructive sparse trigonometric approximation for functions with small mixed smoothness | http://arxiv.org/pdf/1503.00282v1.pdf | author:V. N. Temlyakov category:math.NA stat.ML published:2015-03-01 summary:The paper gives a constructive method, based on greedy algorithms, thatprovides for the classes of functions with small mixed smoothness the bestpossible in the sense of order approximation error for the $m$-termapproximation with respect to the trigonometric system.
arxiv-9300-119 | An Online Convex Optimization Approach to Blackwell's Approachability | http://arxiv.org/pdf/1503.00255v1.pdf | author:Nahum Shimkin category:cs.GT cs.LG published:2015-03-01 summary:The notion of approachability in repeated games with vector payoffs wasintroduced by Blackwell in the 1950s, along with geometric conditions forapproachability and corresponding strategies that rely on computing {\emsteering directions} as projections from the current average payoff vector tothe (convex) target set. Recently, Abernethy, Batlett and Hazan (2011) proposeda class of approachability algorithms that rely on the no-regret properties ofOnline Linear Programming for computing a suitable sequence of steeringdirections. This is first carried out for target sets that are convex cones,and then generalized to any convex set by embedding it in a higher-dimensionalconvex cone. In this paper we present a more direct formulation that relies onthe support function of the set, along with suitable Online Convex Optimizationalgorithms, which leads to a general class of approachability algorithms. Wefurther show that Blackwell's original algorithm and its convergence follow asa special case.
arxiv-9300-120 | Matrix Completion with Noisy Entries and Outliers | http://arxiv.org/pdf/1503.00214v1.pdf | author:Raymond K. W. Wong, Thomas C. M. Lee category:stat.ML published:2015-03-01 summary:This paper considers the problem of matrix completion when the observedentries are noisy and contain outliers. It begins with introducing a newoptimization criterion for which the recovered matrix is defined as itssolution. This criterion uses the celebrated Huber function from the robuststatistics literature to downweigh the effects of outliers. A practicalalgorithm is developed to solve the optimization involved. This algorithm isfast, straightforward to implement, and monotonic convergent. Furthermore, theproposed methodology is theoretically shown to be stable in a well definedsense. Its promising empirical performance is demonstrated via a sequence ofsimulation experiments, including image inpainting.
arxiv-9300-121 | Extraction of Salient Sentences from Labelled Documents | http://arxiv.org/pdf/1412.6815v2.pdf | author:Misha Denil, Alban Demiraj, Nando de Freitas category:cs.CL cs.IR cs.LG published:2014-12-21 summary:We present a hierarchical convolutional document model with an architecturedesigned to support introspection of the document structure. Using this model,we show how to use visualisation techniques from the computer vision literatureto identify and extract topic-relevant sentences. We also introduce a new scalable evaluation technique for automatic sentenceextraction systems that avoids the need for time consuming human annotation ofvalidation data.
arxiv-9300-122 | The NLP Engine: A Universal Turing Machine for NLP | http://arxiv.org/pdf/1503.00168v1.pdf | author:Jiwei Li, Eduard Hovy category:cs.CL published:2015-02-28 summary:It is commonly accepted that machine translation is a more complex task thanpart of speech tagging. But how much more complex? In this paper we make anattempt to develop a general framework and methodology for computing theinformational and/or processing complexity of NLP applications and tasks. Wedefine a universal framework akin to a Turning Machine that attempts to fit(most) NLP tasks into one paradigm. We calculate the complexities of variousNLP tasks using measures of Shannon Entropy, and compare `simple' ones such aspart of speech tagging to `complex' ones such as machine translation. Thispaper provides a first, though far from perfect, attempt to quantify NLP tasksunder a uniform paradigm. We point out current deficiencies and suggest someavenues for fruitful research.
arxiv-9300-123 | Supervised learning sets benchmark for robust spike detection from calcium imaging signals | http://arxiv.org/pdf/1503.00135v1.pdf | author:Lucas Theis, Philipp Berens, Emmanouil Froudarakis, Jacob Reimer, Miroslav Román Rosón, Tom Baden, Thomas Euler, Andreas Tolias, Matthias Bethge category:stat.ML stat.AP published:2015-02-28 summary:A fundamental challenge in calcium imaging has been to infer the timing ofaction potentials from the measured noisy calcium fluorescence traces. Wesystematically evaluate a range of spike inference algorithms on a largebenchmark dataset recorded from varying neural tissue (V1 and retina) usingdifferent calcium indicators (OGB-1 and GCamp6). We show that a new algorithmbased on supervised learning in flexible probabilistic models outperforms allpreviously published techniques, setting a new standard for spike inferencefrom calcium signals. Importantly, it performs better than other algorithmseven on datasets not seen during training. Future data acquired in newexperimental conditions can easily be used to further improve its spikeprediction accuracy and generalization performance. Finally, we show thatcomparing algorithms on artificial data is not informative about performance onreal population imaging data, suggesting that a benchmark dataset may greatlyfacilitate future algorithmic developments.
arxiv-9300-124 | Non-linear Learning for Statistical Machine Translation | http://arxiv.org/pdf/1503.00107v1.pdf | author:Shujian Huang, Huadong Chen, Xinyu Dai, Jiajun Chen category:cs.CL cs.NE published:2015-02-28 summary:Modern statistical machine translation (SMT) systems usually use a linearcombination of features to model the quality of each translation hypothesis.The linear combination assumes that all the features are in a linearrelationship and constrains that each feature interacts with the rest featuresin an linear manner, which might limit the expressive power of the model andlead to a under-fit model on the current data. In this paper, we propose anon-linear modeling for the quality of translation hypotheses based on neuralnetworks, which allows more complex interaction between features. A learningframework is presented for training the non-linear models. We also discusspossible heuristics in designing the network structure which may improve thenon-linear learning performance. Experimental results show that with the basicfeatures of a hierarchical phrase-based machine translation system, our methodproduce translations that are better than a linear model.
arxiv-9300-125 | Improved Image Deblurring based on Salient-region Segmentation | http://arxiv.org/pdf/1503.00090v1.pdf | author:Chongyang Zhang, Weiyao Lin, Wei Li, Bing Zhou, Jun Xie, Jijia Li category:cs.CV published:2015-02-28 summary:Image deblurring techniques play important roles in many image processingapplications. As the blur varies spatially across the image plane, it calls forrobust and effective methods to deal with the spatially-variant blur problem.In this paper, a Saliency-based Deblurring (SD) approach is proposed based onthe saliency detection for salient-region segmentation and a correspondingcompensate method for image deblurring. We also propose a PDE-based deblurringmethod which introduces an anisotropic Partial Differential Equation (PDE)model for latent image prediction and employs an adaptive optimization model inthe kernel estimation and deconvolution steps. Experimental results demonstratethe effectiveness of the proposed algorithm.
arxiv-9300-126 | Why does Deep Learning work? - A perspective from Group Theory | http://arxiv.org/pdf/1412.6621v3.pdf | author:Arnab Paul, Suresh Venkatasubramanian category:cs.LG cs.NE stat.ML published:2014-12-20 summary:Why does Deep Learning work? What representations does it capture? How dohigher-order representations emerge? We study these questions from theperspective of group theory, thereby opening a new approach towards a theory ofDeep learning. One factor behind the recent resurgence of the subject is a key algorithmicstep called pre-training: first search for a good generative model for theinput samples, and repeat the process one layer at a time. We show deeperimplications of this simple principle, by establishing a connection with theinterplay of orbits and stabilizers of group actions. Although the neuralnetworks themselves may not form groups, we show the existence of {\em shadow}groups whose elements serve as close approximations. Over the shadow groups, the pre-training step, originally introduced as amechanism to better initialize a network, becomes equivalent to a search forfeatures with minimal orbits. Intuitively, these features are in a way the {\emsimplest}. Which explains why a deep learning network learns simple featuresfirst. Next, we show how the same principle, when repeated in the deeperlayers, can capture higher order representations, and why representationcomplexity increases as the layers get deeper.
arxiv-9300-127 | Macroblock Classification Method for Video Applications Involving Motions | http://arxiv.org/pdf/1503.00087v1.pdf | author:Weiyao Lin, Ming-Ting Sun, Hongxiang Li, Zhenzhong Chen, Wei Li, Bing Zhou category:cs.MM cs.CV published:2015-02-28 summary:In this paper, a macroblock classification method is proposed for variousvideo processing applications involving motions. Based on the analysis of theMotion Vector field in the compressed video, we propose to classify Macroblocksof each video frame into different classes and use this class information todescribe the frame content. We demonstrate that this low-computation-complexitymethod can efficiently catch the characteristics of the frame. Based on theproposed macroblock classification, we further propose algorithms for differentvideo processing applications, including shot change detection, motiondiscontinuity detection, and outlier rejection for global motion estimation.Experimental results demonstrate that the methods based on the proposedapproach can work effectively on these applications.
arxiv-9300-128 | Group Event Detection with a Varying Number of Group Members for Video Surveillance | http://arxiv.org/pdf/1503.00082v1.pdf | author:Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang category:cs.CV cs.AI cs.MM published:2015-02-28 summary:This paper presents a novel approach for automatic recognition of groupactivities for video surveillance applications. We propose to use a grouprepresentative to handle the recognition with a varying number of groupmembers, and use an Asynchronous Hidden Markov Model (AHMM) to model therelationship between people. Furthermore, we propose a group activity detectionalgorithm which can handle both symmetric and asymmetric group activities, anddemonstrate that this approach enables the detection of hierarchicalinteractions between people. Experimental results show the effectiveness of ourapproach.
arxiv-9300-129 | Activity Recognition Using A Combination of Category Components And Local Models for Video Surveillance | http://arxiv.org/pdf/1503.00081v1.pdf | author:Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang category:cs.CV cs.MM published:2015-02-28 summary:This paper presents a novel approach for automatic recognition of humanactivities for video surveillance applications. We propose to represent anactivity by a combination of category components, and demonstrate that thisapproach offers flexibility to add new activities to the system and an abilityto deal with the problem of building models for activities lacking trainingdata. For improving the recognition accuracy, a Confident-Frame- basedRecognition algorithm is also proposed, where the video frames with highconfidence for recognizing an activity are used as a specialized local model tohelp classify the remainder of the video frames. Experimental results show theeffectiveness of the proposed approach.
arxiv-9300-130 | Real-Time Grasp Detection Using Convolutional Neural Networks | http://arxiv.org/pdf/1412.3128v2.pdf | author:Joseph Redmon, Anelia Angelova category:cs.RO cs.CV published:2014-12-09 summary:We present an accurate, real-time approach to robotic grasp detection basedon convolutional neural networks. Our network performs single-stage regressionto graspable bounding boxes without using standard sliding window or regionproposal techniques. The model outperforms state-of-the-art approaches by 14percentage points and runs at 13 frames per second on a GPU. Our network cansimultaneously perform classification so that in a single step it recognizesthe object and finds a good grasp rectangle. A modification to this modelpredicts multiple grasps per object by using a locally constrained predictionmechanism. The locally constrained model performs significantly better,especially on objects that can be grasped in a variety of ways.
arxiv-9300-131 | DeepTrack: Learning Discriminative Feature Representations Online for Robust Visual Tracking | http://arxiv.org/pdf/1503.00072v1.pdf | author:Hanxi Li, Yi Li, Fatih Porikli category:cs.CV published:2015-02-28 summary:Deep neural networks, albeit their great success on feature learning invarious computer vision tasks, are usually considered as impractical for onlinevisual tracking because they require very long training time and a large numberof training samples. In this work, we present an efficient and very robusttracking algorithm using a single Convolutional Neural Network (CNN) forlearning effective feature representations of the target object, in a purelyonline manner. Our contributions are multifold: First, we introduce a noveltruncated structural loss function that maintains as many training samples aspossible and reduces the risk of tracking error accumulation. Second, weenhance the ordinary Stochastic Gradient Descent approach in CNN training witha robust sample selection mechanism. The sampling mechanism randomly generatespositive and negative samples from different temporal distributions, which aregenerated by taking the temporal relations and label noise into account.Finally, a lazy yet effective updating scheme is designed for CNN training.Equipped with this novel updating algorithm, the CNN model is robust to somelong-existing difficulties in visual tracking such as occlusion or incorrectdetections, without loss of the effective adaption for significant appearancechanges. In the experiment, our CNN tracker outperforms all comparedstate-of-the-art methods on two recently proposed benchmarks which in totalinvolve over 60 video sequences. The remarkable performance improvement overthe existing trackers illustrates the superiority of the featurerepresentations which are learned
arxiv-9300-132 | Convolutional Neural Networks for joint object detection and pose estimation: A comparative study | http://arxiv.org/pdf/1412.7190v4.pdf | author:Francisco Massa, Mathieu Aubry, Renaud Marlet category:cs.CV cs.LG cs.NE published:2014-12-22 summary:In this paper we study the application of convolutional neural networks forjointly detecting objects depicted in still images and estimating their 3Dpose. We identify different feature representations of oriented objects, andenergies that lead a network to learn this representations. The choice of therepresentation is crucial since the pose of an object has a natural, continuousstructure while its category is a discrete variable. We evaluate the differentapproaches on the joint object detection and pose estimation task of thePascal3D+ benchmark using Average Viewpoint Precision. We show that aclassification approach on discretized viewpoints achieves state-of-the-artperformance for joint object detection and pose estimation, and significantlyoutperforms existing baselines on this benchmark.
arxiv-9300-133 | Generating Multi-Sentence Lingual Descriptions of Indoor Scenes | http://arxiv.org/pdf/1503.00064v1.pdf | author:Dahua Lin, Chen Kong, Sanja Fidler, Raquel Urtasun category:cs.CV cs.CL published:2015-02-28 summary:This paper proposes a novel framework for generating lingual descriptions ofindoor scenes. Whereas substantial efforts have been made to tackle thisproblem, previous approaches focusing primarily on generating a single sentencefor each image, which is not sufficient for describing complex scenes. Weattempt to go beyond this, by generating coherent descriptions with multiplesentences. Our approach is distinguished from conventional ones in severalaspects: (1) a 3D visual parsing system that jointly infers objects,attributes, and relations; (2) a generative grammar learned automatically fromtraining text; and (3) a text generation algorithm that takes into account thecoherence among sentences. Experiments on the augmented NYU-v2 dataset showthat our framework can generate natural descriptions with substantially higherROGUE scores compared to those produced by the baseline.
arxiv-9300-134 | Efficient Upsampling of Natural Images | http://arxiv.org/pdf/1503.00040v1.pdf | author:Chinmay Hegde, Oncel Tuzel, Fatih Porikli category:cs.CV cs.GR published:2015-02-28 summary:We propose a novel method of efficient upsampling of a single natural image.Current methods for image upsampling tend to produce high-resolution imageswith either blurry salient edges, or loss of fine textural detail, or spuriousnoise artifacts. In our method, we mitigate these effects by modeling the input image as a sumof edge and detail layers, operating upon these layers separately, and mergingthe upscaled results in an automatic fashion. We formulate the upsampled outputimage as the solution to a non-convex energy minimization problem, and proposean algorithm to obtain a tractable approximate solution. Our algorithmcomprises two main stages. 1) For the edge layer, we use a nonparametricapproach by constructing a dictionary of patches from a given image, andsynthesize edge regions in a higher-resolution version of the image. 2) For thedetail layer, we use a global parametric texture enhancement approach tosynthesize detail regions across the image. We demonstrate that our method is able to accurately reproduce sharp edges aswell as synthesize photorealistic textures, while avoiding common artifactssuch as ringing and haloing. In addition, our method involves no training phaseor estimation of model parameters, and is easily parallelizable. We demonstratethe utility of our method on a number of challenging standard test photos.
arxiv-9300-135 | Sequential Feature Explanations for Anomaly Detection | http://arxiv.org/pdf/1503.00038v1.pdf | author:Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, Weng-Keen Wong category:cs.AI cs.LG stat.ML published:2015-02-28 summary:In many applications, an anomaly detection system presents the most anomalousdata instance to a human analyst, who then must determine whether the instanceis truly of interest (e.g. a threat in a security setting). Unfortunately, mostanomaly detectors provide no explanation about why an instance was consideredanomalous, leaving the analyst with no guidance about where to begin theinvestigation. To address this issue, we study the problems of computing andevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFEof an anomaly is a sequence of features, which are presented to the analyst oneat a time (in order) until the information contained in the highlightedfeatures is enough for the analyst to make a confident judgement about theanomaly. Since analyst effort is related to the amount of information that theyconsider in an investigation, an explanation's quality is related to the numberof features that must be revealed to attain confidence. One of our maincontributions is to present a novel framework for large scale quantitativeevaluations of SFEs, where the quality measure is based on analyst effort. Todo this we construct anomaly detection benchmarks from real data sets alongwith artificial experts that can be simulated for evaluation. Our secondcontribution is to evaluate several novel explanation approaches within theframework and on traditional anomaly detection benchmarks, offering severalinsights into the approaches.
arxiv-9300-136 | Parsing as Reduction | http://arxiv.org/pdf/1503.00030v1.pdf | author:Daniel Fernández-González, André F. T. Martins category:cs.CL published:2015-02-27 summary:We reduce phrase-representation parsing to dependency parsing. Our reductionis grounded on a new intermediate representation, "head-ordered dependencytrees", shown to be isomorphic to constituent trees. By encoding orderinformation in the dependency labels, we show that any off-the-shelf, trainabledependency parser can be used to produce constituents. When this parser isnon-projective, we can perform discontinuous parsing in a very natural manner.Despite the simplicity of our approach, experiments show that the resultingparsers are on par with strong baselines, such as the Berkeley parser forEnglish and the best single system in the SPMRL-2014 shared task. Results areparticularly striking for discontinuous parsing of German, where we surpass thecurrent state of the art by a wide margin.
arxiv-9300-137 | Random Walk Initialization for Training Very Deep Feedforward Networks | http://arxiv.org/pdf/1412.6558v3.pdf | author:David Sussillo, L. F. Abbott category:cs.NE cs.LG stat.ML published:2014-12-19 summary:Training very deep networks is an important open problem in machine learning.One of many difficulties is that the norm of the back-propagated error gradientcan grow or decay exponentially. Here we show that training very deepfeed-forward networks (FFNs) is not as difficult as previously thought. Unlikewhen back-propagation is applied to a recurrent network, application to an FFNamounts to multiplying the error gradient by a different random matrix at eachlayer. We show that the successive application of correctly scaled randommatrices to an initial vector results in a random walk of the log of the normof the resulting vectors, and we compute the scaling that makes this walkunbiased. The variance of the random walk grows only linearly with networkdepth and is inversely proportional to the size of each layer. Practically,this implies a gradient whose log-norm scales with the square root of thenetwork depth and shows that the vanishing gradient problem can be mitigated byincreasing the width of the layers. Mathematical analyses and experimentalresults using stochastic gradient descent to optimize tasks related to theMNIST and TIMIT datasets are provided to support these claims. Equations forthe optimal matrix scaling are provided for the linear and ReLU cases.
arxiv-9300-138 | Stochastic Dual Coordinate Ascent with Adaptive Probabilities | http://arxiv.org/pdf/1502.08053v1.pdf | author:Dominik Csiba, Zheng Qu, Peter Richtárik category:math.OC cs.LG stat.ML published:2015-02-27 summary:This paper introduces AdaSDCA: an adaptive variant of stochastic dualcoordinate ascent (SDCA) for solving the regularized empirical riskminimization problems. Our modification consists in allowing the methodadaptively change the probability distribution over the dual variablesthroughout the iterative process. AdaSDCA achieves provably better complexitybound than SDCA with the best fixed probability distribution, known asimportance sampling. However, it is of a theoretical character as it isexpensive to implement. We also propose AdaSDCA+: a practical variant which inour experiments outperforms existing non-adaptive methods.
arxiv-9300-139 | Image Segmentation in Liquid Argon Time Projection Chamber Detector | http://arxiv.org/pdf/1502.08046v1.pdf | author:Piotr Płoński, Dorota Stefan, Robert Sulej, Krzysztof Zaremba category:cs.CV hep-ex published:2015-02-27 summary:The Liquid Argon Time Projection Chamber (LAr-TPC) detectors provideexcellent imaging and particle identification ability for studying neutrinos.An efficient and automatic reconstruction procedures are required to exploitpotential of this imaging technology. Herein, a novel method for segmentationof images from LAr-TPC detectors is presented. The proposed approach computes afeature descriptor for each pixel in the image, which characterizes amplitudedistribution in pixel and its neighbourhood. The supervised classifier isemployed to distinguish between pixels representing particle's track and noise.The classifier is trained and evaluated on the hand-labeled dataset. Theproposed approach can be a preprocessing step for reconstructing algorithmsworking directly on detector images.
arxiv-9300-140 | A Dictionary Approach to EBSD Indexing | http://arxiv.org/pdf/1502.07436v2.pdf | author:Yu-Hui Chen, Se Un Park, Dennis Wei, Gregory Newstadt, Michael Jackson, Jeff P. Simmons, Marc De Graef, Alfred O. Hero category:cs.CV stat.AP published:2015-02-26 summary:We propose a framework for indexing of grain and sub-grain structures inelectron backscatter diffraction (EBSD) images of polycrystalline materials.The framework is based on a previously introduced physics-based forward modelby Callahan and De Graef (2013) relating measured patterns to grainorientations (Euler angle). The forward model is tuned to the microscope andthe sample symmetry group. We discretize the domain of the forward model onto adense grid of Euler angles and for each measured pattern we identify the mostsimilar patterns in the dictionary. These patterns are used to identifyboundaries, detect anomalies, and index crystal orientations. The statisticaldistribution of these closest matches is used in an unsupervised binarydecision tree (DT) classifier to identify grain boundaries and anomalousregions. The DT classifies a pattern as an anomaly if it has an abnormally lowsimilarity to any pattern in the dictionary. It classifies a pixel as beingnear a grain boundary if the highly ranked patterns in the dictionary differsignificantly over the pixels 3x3 neighborhood. Indexing is accomplished bycomputing the mean orientation of the closest dictionary matches to eachpattern. The mean orientation is estimated using a maximum likelihood approachthat models the orientation distribution as a mixture of Von Mises-Fisherdistributions over the quaternionic 3-sphere. The proposed dictionary matchingapproach permits segmentation, anomaly detection, and indexing to be performedin a unified manner with the additional benefit of uncertainty quantification.We demonstrate the proposed dictionary-based approach on a Ni-base IN100 alloy.
arxiv-9300-141 | Probabilistic Zero-shot Classification with Semantic Rankings | http://arxiv.org/pdf/1502.08039v1.pdf | author:Jihun Hamm, Mikhail Belkin category:cs.LG cs.AI cs.CV published:2015-02-27 summary:In this paper we propose a non-metric ranking-based representation ofsemantic similarity that allows natural aggregation of semantic informationfrom multiple heterogeneous sources. We apply the ranking-based representationto zero-shot learning problems, and present deterministic and probabilisticzero-shot classifiers which can be built from pre-trained classifiers withoutretraining. We demonstrate their the advantages on two large real-world imagedatasets. In particular, we show that aggregating different sources of semanticinformation, including crowd-sourcing, leads to more accurate classification.
arxiv-9300-142 | SciRecSys: A Recommendation System for Scientific Publication by Discovering Keyword Relationships | http://arxiv.org/pdf/1502.08033v1.pdf | author:Vu Le Anh, Vo Hoang Hai, Hung Nghiep Tran, Jason J. Jung category:cs.DL cs.CL cs.IR published:2015-02-27 summary:In this work, we propose a new approach for discovering various relationshipsamong keywords over the scientific publications based on a Markov Chain model.It is an important problem since keywords are the basic elements forrepresenting abstract objects such as documents, user profiles, topics and manythings else. Our model is very effective since it combines four importantfactors in scientific publications: content, publicity, impact and randomness.Particularly, a recommendation system (called SciRecSys) has been presented tosupport users to efficiently find out relevant articles.
arxiv-9300-143 | Author Name Disambiguation by Using Deep Neural Network | http://arxiv.org/pdf/1502.08030v1.pdf | author:Hung Nghiep Tran, Tin Huynh, Tien Do category:cs.DL cs.CL cs.LG published:2015-02-27 summary:Author name ambiguity decreases the quality and reliability of informationretrieved from digital libraries. Existing methods have tried to solve thisproblem by predefining a feature set based on expert's knowledge for a specificdataset. In this paper, we propose a new approach which uses deep neuralnetwork to learn features automatically from data. Additionally, we propose thegeneral system architecture for author name disambiguation on any dataset. Inthis research, we evaluate the proposed method on a dataset containingVietnamese author names. The results show that this method significantlyoutperforms other methods that use predefined feature set. The proposed methodachieves 99.31% in terms of accuracy. Prediction error rate decreases from1.83% to 0.69%, i.e., it decreases by 1.14%, or 62.3% relatively compared withother methods that use predefined feature set (Table 3).
arxiv-9300-144 | Exact tensor completion using t-SVD | http://arxiv.org/pdf/1502.04689v2.pdf | author:Zemin Zhang, Shuchin Aeron category:cs.LG cs.NA stat.ML published:2015-02-16 summary:In this paper we focus on the problem of completion of multidimensionalarrays (also referred to as tensors) from limited sampling. Our approach isbased on a recently proposed tensor-Singular Value Decomposition (t-SVD) [1].Using this factorization one can derive notion of tensor rank, referred to asthe tensor tubal rank, which has optimality properties similar to that ofmatrix rank derived from SVD. As shown in [2] some multidimensional data, suchas panning video sequences exhibit low tensor tubal rank and we look at theproblem of completing such data under random sampling of the data cube. We showthat by solving a convex optimization problem, which minimizes the tensornuclear norm obtained as the convex relaxation of tensor tubal rank, one canguarantee recovery with overwhelming probability as long as samples inproportion to the degrees of freedom in t-SVD are observed. In this sense ourresults are order-wise optimal. The conditions under which this result holdsare very similar to the incoherency conditions for the matrix completion,albeit we define incoherency under the algebraic set-up of t-SVD. We show theperformance of the algorithm on some real data sets and compare it with otherexisting approaches based on tensor flattening and Tucker decomposition.
arxiv-9300-145 | Second-order Quantile Methods for Experts and Combinatorial Games | http://arxiv.org/pdf/1502.08009v1.pdf | author:Wouter M. Koolen, Tim van Erven category:cs.LG stat.ML published:2015-02-27 summary:We aim to design strategies for sequential decision making that adjust to thedifficulty of the learning problem. We study this question both in the settingof prediction with expert advice, and for more general combinatorial decisiontasks. We are not satisfied with just guaranteeing minimax regret rates, but wewant our algorithms to perform significantly better on easy data. Two popularways to formalize such adaptivity are second-order regret bounds and quantilebounds. The underlying notions of 'easy data', which may be paraphrased as "thelearning problem has small variance" and "multiple decisions are useful", aresynergetic. But even though there are sophisticated algorithms that exploit oneof the two, no existing algorithm is able to adapt to both. In this paper we outline a new method for obtaining such adaptive algorithms,based on a potential function that aggregates a range of learning rates (whichare essential tuning parameters). By choosing the right prior we constructefficient algorithms and show that they reap both benefits by proving the firstbounds that are both second-order and incorporate quantiles.
arxiv-9300-146 | Non-stochastic Best Arm Identification and Hyperparameter Optimization | http://arxiv.org/pdf/1502.07943v1.pdf | author:Kevin Jamieson, Ameet Talwalkar category:cs.LG stat.ML published:2015-02-27 summary:Motivated by the task of hyperparameter optimization, we introduce thenon-stochastic best-arm identification problem. Within the multi-armed banditliterature, the cumulative regret objective enjoys algorithms and analyses forboth the non-stochastic and stochastic settings while to the best of ourknowledge, the best-arm identification framework has only been considered inthe stochastic setting. We introduce the non-stochastic setting under thisframework, identify a known algorithm that is well-suited for this setting, andanalyze its behavior. Next, by leveraging the iterative nature of standardmachine learning algorithms, we cast hyperparameter optimization as an instanceof non-stochastic best-arm identification, and empirically evaluate ourproposed algorithm on this task. Our empirical results show that, by allocatingmore resources to promising hyperparameter settings, we typically achievecomparable test accuracies an order of magnitude faster than baseline methods.
arxiv-9300-147 | Generative Modeling of Hidden Functional Brain Networks | http://arxiv.org/pdf/1412.6602v2.pdf | author:Shaurabh Nandy, Richard M. Golden category:stat.ML q-bio.NC published:2014-12-20 summary:Functional connectivity refers to the temporal statistical relationshipbetween spatially distinct brain regions and is usually inferred from the timeseries coherence/correlation in brain activity between regions of interest. Inhuman functional brain networks, the network structure is often inferred fromfunctional magnetic resonance imaging (fMRI) blood oxygen level dependent(BOLD) signal. Since the BOLD signal is a proxy for neuronal activity, it is ofinterest to learn the latent functional network structure. Additionally,despite a core set of observations about functional networks such assmall-worldness, modularity, exponentially truncated degree distributions, andpresence of various types of hubs, very little is known about the computationalprinciples which can give rise to these observations. This paper introduces aHidden Markov Random Field framework for the purpose of representing,estimating, and evaluating latent neuronal functional relationships betweendifferent brain regions using fMRI data.
arxiv-9300-148 | Local Translation Prediction with Global Sentence Representation | http://arxiv.org/pdf/1502.07920v1.pdf | author:Jiajun Zhang category:cs.CL published:2015-02-27 summary:Statistical machine translation models have made great progress in improvingthe translation quality. However, the existing models predict the targettranslation with only the source- and target-side local context information. Inpractice, distinguishing good translations from bad ones does not only dependon the local features, but also rely on the global sentence-level information.In this paper, we explore the source-side global sentence-level features fortarget-side local translation prediction. We propose a novelbilingually-constrained chunk-based convolutional neural network to learnsentence semantic representations. With the sentence-level featurerepresentation, we further design a feed-forward neural network to betterpredict translations using both local and global information. The large-scaleexperiments show that our method can obtain substantial improvements intranslation quality over the strong baseline: the hierarchical phrase-basedtranslation model augmented with the neural network joint model.
arxiv-9300-149 | Unsupervised Domain Adaptation by Backpropagation | http://arxiv.org/pdf/1409.7495v2.pdf | author:Yaroslav Ganin, Victor Lempitsky category:stat.ML cs.LG cs.NE published:2014-09-26 summary:Top-performing deep architectures are trained on massive amounts of labeleddata. In the absence of labeled data for a certain task, domain adaptationoften provides an attractive option given that labeled data of similar naturebut from a different domain (e.g. synthetic images) are available. Here, wepropose a new approach to domain adaptation in deep architectures that can betrained on large amount of labeled data from the source domain and large amountof unlabeled data from the target domain (no labeled target-domain data isnecessary). As the training progresses, the approach promotes the emergence of "deep"features that are (i) discriminative for the main learning task on the sourcedomain and (ii) invariant with respect to the shift between the domains. Weshow that this adaptation behaviour can be achieved in almost any feed-forwardmodel by augmenting it with few standard layers and a simple new gradientreversal layer. The resulting augmented architecture can be trained usingstandard backpropagation. Overall, the approach can be implemented with little effort using any of thedeep-learning packages. The method performs very well in a series of imageclassification experiments, achieving adaptation effect in the presence of bigdomain shifts and outperforming previous state-of-the-art on Office datasets.
arxiv-9300-150 | Study on Sparse Representation based Classification for Biometric Verification | http://arxiv.org/pdf/1502.06073v2.pdf | author:Zengxi Huang, Yiguang Liu, Xiaoming Wang, Jinrong Hu category:cs.CV published:2015-02-21 summary:In this paper, we propose a multimodal verification system integrating faceand ear based on sparse representation based classification (SRC). The face andear query samples are first encoded separately to derive sparsity-based matchscores, and which are then combined with sum-rule fusion for verification.Apart from validating the encouraging performance of SRC-based multimodalverification, this paper also dedicates to provide a clear understanding aboutthe characteristics of SRC-based biometric verification. To this end, twosparsity-based metrics, i.e. spare coding error (SCE) and sparse contributionrate (SCR), are involved, together with face and ear unimodal SRC-basedverification. As for the issue that SRC-based biometric verification may sufferfrom heavy computational burden and verification accuracy degradation withincrease of enrolled subjects, we argue that it could be properly resolved byexploiting small random dictionary for sparsity-based score computation, whichconsists of training samples from a limited number of randomly selectedsubjects. Experimental results demonstrate the superiority of SRC-basedmultimodal verification compared to the state-of-the-art multimodal methodslike likelihood ratio (LLR), support vector machine (SVM), and the sum-rulefusion methods using cosine similarity, meanwhile the idea of using smallrandom dictionary is feasible in both effectiveness and efficiency.
arxiv-9300-151 | Hybrid coding of visual content and local image features | http://arxiv.org/pdf/1502.07828v1.pdf | author:Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi, Stefano Tubaro category:cs.MM cs.CV published:2015-02-27 summary:Distributed visual analysis applications, such as mobile visual search orVisual Sensor Networks (VSNs) require the transmission of visual content on abandwidth-limited network, from a peripheral node to a processing unit.Traditionally, a Compress-Then-Analyze approach has been pursued, in whichsensing nodes acquire and encode the pixel-level representation of the visualcontent, that is subsequently transmitted to a sink node in order to beprocessed. This approach might not represent the most effective solution, sinceseveral analysis applications leverage a compact representation of the content,thus resulting in an inefficient usage of network resources. Furthermore,coding artifacts might significantly impact the accuracy of the visual task athand. To tackle such limitations, an orthogonal approach namedAnalyze-Then-Compress has been proposed. According to such a paradigm, sensingnodes are responsible for the extraction of visual features, that are encodedand transmitted to a sink node for further processing. In spite of improvedtask efficiency, such paradigm implies the central processing node not beingable to reconstruct a pixel-level representation of the visual content. In thispaper we propose an effective compromise between the two paradigms, namelyHybrid-Analyze-Then-Compress (HATC) that aims at jointly encoding visualcontent and local image features. Furthermore, we show how a target tradeoffbetween image quality and task accuracy might be achieved by accuratelyallocating the bitrate to either visual content or local features.
arxiv-9300-152 | Language Recognition using Random Indexing | http://arxiv.org/pdf/1412.7026v2.pdf | author:Aditya Joshi, Johan Halseth, Pentti Kanerva category:cs.CL cs.LG published:2014-12-22 summary:Random Indexing is a simple implementation of Random Projections with a widerange of applications. It can solve a variety of problems with good accuracywithout introducing much complexity. Here we use it for identifying thelanguage of text samples. We present a novel method of generating languagerepresentation vectors using letter blocks. Further, we show that the method iseasily implemented and requires little computational power and space.Experiments on a number of model parameters illustrate certain properties abouthigh dimensional sparse vector representations of data. Proof of statisticallyrelevant language vectors are shown through the extremely high success ofvarious language recognition tasks. On a difficult data set of 21,000 shortsentences from 21 different languages, our model performs a languagerecognition task and achieves 97.8% accuracy, comparable to state-of-the-artmethods.
arxiv-9300-153 | Minimum message length estimation of mixtures of multivariate Gaussian and von Mises-Fisher distributions | http://arxiv.org/pdf/1502.07813v1.pdf | author:Parthan Kasarapu, Lloyd Allison category:cs.LG stat.ML published:2015-02-27 summary:Mixture modelling involves explaining some observed evidence using acombination of probability distributions. The crux of the problem is theinference of an optimal number of mixture components and their correspondingparameters. This paper discusses unsupervised learning of mixture models usingthe Bayesian Minimum Message Length (MML) criterion. To demonstrate theeffectiveness of search and inference of mixture parameters using the proposedapproach, we select two key probability distributions, each handlingfundamentally different types of data: the multivariate Gaussian distributionto address mixture modelling of data distributed in Euclidean space, and themultivariate von Mises-Fisher (vMF) distribution to address mixture modellingof directional data distributed on a unit hypersphere. The key contributions ofthis paper, in addition to the general search and inference methodology,include the derivation of MML expressions for encoding the data usingmultivariate Gaussian and von Mises-Fisher distributions, and the analyticalderivation of the MML estimates of the parameters of the two distributions. Ourapproach is tested on simulated and real world data sets. For instance, weinfer vMF mixtures that concisely explain experimentally determinedthree-dimensional protein conformations, providing an effective null modeldescription of protein structures that is central to many inference problems instructural bioinformatics. The experimental results demonstrate that theperformance of our proposed search and inference method along with the encodingschemes improve on the state of the art mixture modelling techniques.
arxiv-9300-154 | Modelling Local Deep Convolutional Neural Network Features to Improve Fine-Grained Image Classification | http://arxiv.org/pdf/1502.07802v1.pdf | author:ZongYuan Ge, Chris McCool, Conrad Sanderson, Peter Corke category:cs.CV published:2015-02-27 summary:We propose a local modelling approach using deep convolutional neuralnetworks (CNNs) for fine-grained image classification. Recently, deep CNNstrained from large datasets have considerably improved the performance ofobject recognition. However, to date there has been limited work using thesedeep CNNs as local feature extractors. This partly stems from CNNs havinginternal representations which are high dimensional, thereby making suchrepresentations difficult to model using stochastic models. To overcome thisissue, we propose to reduce the dimensionality of one of the internal fullyconnected layers, in conjunction with layer-restricted retraining to avoidretraining the entire network. The distribution of low-dimensional featuresobtained from the modified layer is then modelled using a Gaussian mixturemodel. Comparative experiments show that considerable performance improvementscan be achieved on the challenging Fish and UEC FOOD-100 datasets.
arxiv-9300-155 | The conjugated null space method of blind PSF estimation and deconvolution optimization | http://arxiv.org/pdf/1502.07781v1.pdf | author:Yuriy A. Bunyak, Roman N. Kvetnyy, Olga Yu. Sofina category:cs.CV published:2015-02-26 summary:We have shown that the vector of the point spread function (PSF)lexicographical presentation belongs to the left side conjugated null space(NS) of the autoregression (AR) matrix operator on condition the AR parametersare common for original and blurred images. The method of the PSF and inversePSF (IPSF) evaluation in the basis of the NS eigenfunctions is offered. Theoptimization of the PSF and IPSF shape with the aim of fluctuation eliminationis considered in NS spectral domain and image space domain. The function ofsurface area was used as the regularization functional. Two methods of originalimage estimate optimization were designed basing on maximum entropygeneralization of sought and blurred images conditional probability density andregularization. The first method uses balanced variations of convolutions withthe PSF and IPSF to obtaining iterative schema of image optimization. Thevariations balance is providing by dynamic regularization basing on conditionof the iteration process convergence. The regularization has dynamic characterbecause depends on current and previous image estimate variations. The secondmethod implements the regularization of the deconvolution optimization incurved space with metric defined on image estimate surface. The given iterativeschemas have fast convergence and therefore can be used for reconstruction ofhigh resolution images series in real time. The NS can be used for design ofdenoising bilateral linear filter which does not introduce image smoothing.
arxiv-9300-156 | Efficient Geometric-based Computation of the String Subsequence Kernel | http://arxiv.org/pdf/1502.07776v1.pdf | author:Slimane Bellaouar, Hadda Cherroun, Djelloul Ziadi category:cs.LG cs.CG published:2015-02-26 summary:Kernel methods are powerful tools in machine learning. They have to becomputationally efficient. In this paper, we present a novel Geometric-basedapproach to compute efficiently the string subsequence kernel (SSK). Our mainidea is that the SSK computation reduces to range query problem. We started bythe construction of a match list $L(s,t)=\{(i,j):s_{i}=t_{j}\}$ where $s$ and$t$ are the strings to be compared; such match list contains only the requireddata that contribute to the result. To compute efficiently the SSK, we extendedthe layered range tree data structure to a layered range sum tree, arange-aggregation data structure. The whole process takes $ O(pL\logL)$time and $O(L\logL)$ space, where $L$ is the size of the match list and$p$ is the length of the SSK. We present empiric evaluations of our approachagainst the dynamic and the sparse programming approaches both on syntheticallygenerated data and on newswire article data. Such experiments show theefficiency of our approach for large alphabet size except for very shortstrings. Moreover, compared to the sparse dynamic approach, the proposedapproach outperforms absolutely for long strings.
arxiv-9300-157 | Learning computationally efficient dictionaries and their implementation as fast transforms | http://arxiv.org/pdf/1406.5388v3.pdf | author:Luc Le Magoarou, Rémi Gribonval category:cs.LG published:2014-06-20 summary:Dictionary learning is a branch of signal processing and machine learningthat aims at finding a frame (called dictionary) in which some training dataadmits a sparse representation. The sparser the representation, the better thedictionary. The resulting dictionary is in general a dense matrix, and itsmanipulation can be computationally costly both at the learning stage and laterin the usage of this dictionary, for tasks such as sparse coding. Dictionarylearning is thus limited to relatively small-scale problems. In this paper,inspired by usual fast transforms, we consider a general dictionary structurethat allows cheaper manipulation, and propose an algorithm to learn suchdictionaries --and their fast implementation-- over training data. The approachis demonstrated experimentally with the factorization of the Hadamard matrixand with synthetic dictionary learning experiments.
arxiv-9300-158 | Exploiting a comparability mapping to improve bi-lingual data categorization: a three-mode data analysis perspective | http://arxiv.org/pdf/1502.07157v2.pdf | author:Pierre-François Marteau, Guiyao Ke category:cs.IR cs.CL published:2015-02-25 summary:We address in this paper the co-clustering and co-classification of bilingualdata laying in two linguistic similarity spaces when a comparability measuredefining a mapping between these two spaces is available. A new approach thatwe can characterized as a three-mode analysis scheme, is proposed to mix thecomparability measure with the two similarity measures. Our aim is to improvejointly the accuracy of classification and clustering tasks performed in eachof the two linguistic spaces, as well as the quality of the final alignment ofcomparable clusters that can be obtained. We used first some purely syntheticrandom data sets to assess our formal similarity-comparability mixing model. Wethen propose two variants of the comparability measure that has been defined by(Li and Gaussier 2010) in the context of bilingual lexicon extraction to adaptit to clustering or categorizing tasks. These two variant measures aresubsequently used to evaluate our similarity-comparability mixing model in thecontext of the co-classification and co-clustering of comparable textual datasets collected from Wikipedia categories for the English and French languages.Our experiments show clear improvements in clustering and classificationaccuracies when mixing comparability with similarity measures, with, asexpected, a higher robustness obtained when the two comparability variantmeasures that we propose are used. We believe that this approach isparticularly well suited for the construction of thematic comparable corpora ofcontrollable quality.
arxiv-9300-159 | Covariance Matrices and Influence Scores for Mean Field Variational Bayes | http://arxiv.org/pdf/1502.07685v1.pdf | author:Ryan Giordano, Tamara Broderick category:stat.ML stat.ME published:2015-02-26 summary:Mean field variational Bayes (MFVB) is a popular posterior approximationmethod due to its fast runtime on large-scale data sets. However, it is wellknown that a major failing of MFVB is that it underestimates the uncertainty ofmodel variables (sometimes severely) and provides no information about modelvariable covariance. We develop a fast, general methodology for exponentialfamilies that augments MFVB to deliver accurate uncertainty estimates for modelvariables -- both for individual variables and coherently across variables.MFVB for exponential families defines a fixed-point equation in the means ofthe approximating posterior, and our approach yields a covariance estimate byperturbing this fixed point. Inspired by linear response theory, we call ourmethod linear response variational Bayes (LRVB). We also show how LRVB can beused to quickly calculate a measure of the influence of individual data pointson parameter point estimates. We demonstrate the accuracy and scalability ofour method by learning Gaussian mixture models for both simulated and realdata.
arxiv-9300-160 | Introducing SLAMBench, a performance and accuracy benchmarking methodology for SLAM | http://arxiv.org/pdf/1410.2167v2.pdf | author:Luigi Nardi, Bruno Bodin, M. Zeeshan Zia, John Mawer, Andy Nisbet, Paul H. J. Kelly, Andrew J. Davison, Mikel Luján, Michael F. P. O'Boyle, Graham Riley, Nigel Topham, Steve Furber category:cs.RO cs.CV cs.DC cs.PF published:2014-10-08 summary:Real-time dense computer vision and SLAM offer great potential for a newlevel of scene modelling, tracking and real environmental interaction for manytypes of robot, but their high computational requirements mean that use on massmarket embedded platforms is challenging. Meanwhile, trends in low-cost,low-power processing are towards massive parallelism and heterogeneity, makingit difficult for robotics and vision researchers to implement their algorithmsin a performance-portable way. In this paper we introduce SLAMBench, apublicly-available software framework which represents a starting point forquantitative, comparable and validatable experimental research to investigatetrade-offs in performance, accuracy and energy consumption of a dense RGB-DSLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP,OpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-Dsequences with trajectory and scene ground truth for reliable accuracycomparison of different implementation and algorithms. We present an analysisand breakdown of the constituent algorithmic elements of KinectFusion, andexperimentally investigate their execution time on a variety of multicore andGPUaccelerated platforms. For a popular embedded platform, we also present ananalysis of energy efficiency for different configuration alternatives.
arxiv-9300-161 | Online Learning with Feedback Graphs: Beyond Bandits | http://arxiv.org/pdf/1502.07617v1.pdf | author:Noga Alon, Nicolò Cesa-Bianchi, Ofer Dekel, Tomer Koren category:cs.LG published:2015-02-26 summary:We study a general class of online learning problems where the feedback isspecified by a graph. This class includes online prediction with expert adviceand the multi-armed bandit problem, but also several learning problems wherethe online player does not necessarily observe his own loss. We analyze how thestructure of the feedback graph controls the inherent difficulty of the induced$T$-round learning problem. Specifically, we show that any feedback graphbelongs to one of three classes: strongly observable graphs, weakly observablegraphs, and unobservable graphs. We prove that the first class induces learningproblems with $\widetilde\Theta(\alpha^{1/2} T^{1/2})$ minimax regret, where$\alpha$ is the independence number of the underlying graph; the second classinduces problems with $\widetilde\Theta(\delta^{1/3}T^{2/3})$ minimax regret,where $\delta$ is the domination number of a certain portion of the graph; andthe third class induces problems with linear minimax regret. Our resultssubsume much of the previous work on learning with feedback graphs and revealnew connections to partial monitoring games. We also show how the regret isaffected if the graphs are allowed to vary with time.
arxiv-9300-162 | Coding local and global binary visual features extracted from video sequences | http://arxiv.org/pdf/1502.07939v1.pdf | author:Luca Baroffio, Antonio Canclini, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi, Stefano Tubaro category:cs.MM cs.CV published:2015-02-26 summary:Binary local features represent an effective alternative to real-valueddescriptors, leading to comparable results for many visual analysis tasks,while being characterized by significantly lower computational complexity andmemory requirements. When dealing with large collections, a more compactrepresentation based on global features is often preferred, which can beobtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)model. Several applications, including for example visual sensor networks andmobile augmented reality, require visual features to be transmitted over abandwidth-limited network, thus calling for coding techniques that aim atreducing the required bit budget, while attaining a target level of efficiency.In this paper we investigate a coding scheme tailored to both local and globalbinary features, which aims at exploiting both spatial and temporal redundancyby means of intra- and inter-frame coding. In this respect, the proposed codingscheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)paradigm. That is, visual features are extracted from the acquired content,encoded at remote nodes, and finally transmitted to a central controller thatperforms visual analysis. This is in contrast with the traditional approach, inwhich visual content is acquired at a node, compressed and then sent to acentral unit for further processing, according to the Compress-Then-Analyze(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means ofrate-efficiency curves in the context of two different visual analysis tasks:homography estimation and content-based retrieval. Our results show that thenovel ATC paradigm based on the proposed coding primitives can be competitivewith CTA, especially in bandwidth limited scenarios.
arxiv-9300-163 | A hypothesize-and-verify framework for Text Recognition using Deep Recurrent Neural Networks | http://arxiv.org/pdf/1502.07540v1.pdf | author:Anupama Ray, Sai Rajeswar, Santanu Chaudhury category:cs.CV published:2015-02-26 summary:Deep LSTM is an ideal candidate for text recognition. However textrecognition involves some initial image processing steps like segmentation oflines and words which can induce error to the recognition system. Withoutsegmentation, learning very long range context is difficult and becomescomputationally intractable. Therefore, alternative soft decisions are neededat the pre-processing level. This paper proposes a hybrid text recognizer usinga deep recurrent neural network with multiple layers of abstraction and longrange context along with a language model to verify the performance of the deepneural network. In this paper we construct a multi-hypotheses tree architecturewith candidate segments of line sequences from different segmentationalgorithms at its different branches. The deep neural network is trained onperfectly segmented data and tests each of the candidate segments, generatingunicode sequences. In the verification step, these unicode sequences arevalidated using a sub-string match with the language model and best firstsearch is used to find the best possible combination of alternative hypothesisfrom the tree structure. Thus the verification framework using language modelseliminates wrong segmentation outputs and filters recognition errors.
arxiv-9300-164 | Rational Kernels for Arabic Stemming and Text Classification | http://arxiv.org/pdf/1502.07504v1.pdf | author:Attia Nehar, Djelloul Ziadi, Hadda Cherroun category:cs.CL published:2015-02-26 summary:In this paper, we address the problems of Arabic Text Classification andstemming using Transducers and Rational Kernels. We introduce a new stemmingtechnique based on the use of Arabic patterns (Pattern Based Stemmer). Patternsare modelled using transducers and stemming is done without depending on anydictionary. Using transducers for stemming, documents are transformed intofinite state transducers. This document representation allows us to use andexplore rational kernels as a framework for Arabic Text Classification.Stemming experiments are conducted on three word collections and classificationexperiments are done on the Saudi Press Agency dataset. Results show that ourapproach, when compared with other approaches, is promising specially in termsof Accuracy, Recall and F1.
arxiv-9300-165 | Identifiability of the Simplex Volume Minimization Criterion for Blind Hyperspectral Unmixing: The No Pure-Pixel Case | http://arxiv.org/pdf/1406.5273v2.pdf | author:Chia-Hsiang Lin, Wing-Kin Ma, Wei-Chiang Li, Chong-Yung Chi, ArulMurugan Ambikapathi category:stat.ML cs.IT math.IT math.OC published:2014-06-20 summary:In blind hyperspectral unmixing (HU), the pure-pixel assumption is well-knownto be powerful in enabling simple and effective blind HU solutions. However,the pure-pixel assumption is not always satisfied in an exact sense, especiallyfor scenarios where pixels are heavily mixed. In the no pure-pixel case, a goodblind HU approach to consider is the minimum volume enclosing simplex (MVES).Empirical experience has suggested that MVES algorithms can perform wellwithout pure pixels, although it was not totally clear why this is true from atheoretical viewpoint. This paper aims to address the latter issue. We developan analysis framework wherein the perfect endmember identifiability of MVES isstudied under the noiseless case. We prove that MVES is indeed robust againstlack of pure pixels, as long as the pixels do not get too heavily mixed and tooasymmetrically spread. The theoretical results are verified by numericalsimulations.
arxiv-9300-166 | Application and Verification of Algorithm Learning Based Neural Network | http://arxiv.org/pdf/1406.2614v4.pdf | author:Rizwana Kalsoom, Moomal Qureshi category:cs.NE published:2014-06-07 summary:This paper has been withdrawn by the author due to a crucial accuracy errorin Fig. 5. For precise performance of ALBNN please refer to Yoon et al.'s workin the following article. Yoon, H., Park, C. S., Kim, J. S., & Baek, J. G.(2013). Algorithm learning based neural network integrating feature selectionand classification. Expert Systems with Applications, 40(1), 231-241.http://www.sciencedirect.com/science/article/pii/S0957417412008731
arxiv-9300-167 | A Holistic Approach for Modeling and Synthesis of Image Processing Applications for Heterogeneous Computing Architectures | http://arxiv.org/pdf/1502.07453v1.pdf | author:Christian Hartmann, Anna Yupatova, Marc Reichenbach, Dietmar Fey, Reinhard German category:cs.CV published:2015-02-26 summary:Image processing applications are common in every field of our daily life.However, most of them are very complex and contain several tasks with differentcomplexities which result in varying requirements for computing architectures.Nevertheless, a general processing scheme in every image processing applicationhas a similar structure, called image processing pipeline: (1) capturing animage, (2) pre-processing using local operators, (3) processing with globaloperators and (4) post-processing using complex operations. Therefore,application-specialized hardware solutions based on heterogeneous architecturesare used for image processing. Unfortunately the development of applicationsfor heterogeneous hardware architectures is challenging due to the distributionof computational tasks among processors and programmable logic units. Nowadays,image processing systems are started from scratch which is time-consuming,error-prone and inflexible. A new methodology for modeling and implementing isneeded in order to reduce the development time of heterogenous image processingsystems. This paper introduces a new holistic top down approach for imageprocessing systems. Two challenges have to be investigated. First, designersought to be able to model their complete image processing pipeline on anabstract layer using UML. Second, we want to close the gap between the abstractsystem and the system architecture.
arxiv-9300-168 | Concept for a CMOS Image Sensor Suited for Analog Image Pre-Processing | http://arxiv.org/pdf/1502.07449v1.pdf | author:Lan Shi, Christopher Soell, Andreas Baenisch, Robert Weigel, Jürgen Seiler, Thomas Ussmueller category:cs.ET cs.AR cs.CV published:2015-02-26 summary:A concept for a novel CMOS image sensor suited for analog imagepre-processing is presented in this paper. As an example, an image restorationalgorithm for reducing image noise is applied as image pre-processing in theanalog domain. To supply low-latency data input for analog image preprocessing,the proposed concept for a CMOS image sensor offers a new sensor signalacquisition method in 2D. In comparison to image pre-processing in the digitaldomain, the proposed analog image pre-processing promises an improved imagequality. Furthermore, the image noise at the stage of analog sensor signalacquisition can be used to select the most effective restoration algorithmapplied to the analog circuit due to image processing prior to the A/Dconverter.
arxiv-9300-169 | Automatic Optimization of Hardware Accelerators for Image Processing | http://arxiv.org/pdf/1502.07448v1.pdf | author:Oliver Reiche, Konrad Häublein, Marc Reichenbach, Frank Hannig, Jürgen Teich, Dietmar Fey category:cs.PL cs.CV published:2015-02-26 summary:In the domain of image processing, often real-time constraints are required.In particular, in safety-critical applications, such as X-ray computedtomography in medical imaging or advanced driver assistance systems in theautomotive domain, timing is of utmost importance. A common approach tomaintain real-time capabilities of compute-intensive applications is to offloadthose computations to dedicated accelerator hardware, such as FieldProgrammable Gate Arrays (FPGAs). Programming such architectures is achallenging task, with respect to the typical FPGA-specific design criteria:Achievable overall algorithm latency and resource usage of FPGA primitives(BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifiesthis task by enabling the description of algorithms in well-known higherlanguages (C/C++) and its automatic synthesis that can be accomplished by HLStools. However, algorithm developers still need expert knowledge about thetarget architecture, in order to achieve satisfying results. Therefore, inprevious work, we have shown that elevating the description of image algorithmsto an even higher abstraction level, by using a Domain-Specific Language (DSL),can significantly cut down the complexity for designing such algorithms forFPGAs. To give the developer even more control over the common trade-off,latency vs. resource usage, we will present an automatic optimization processwhere these criteria are analyzed and fed back to the DSL compiler, in order togenerate code that is closer to the desired design specifications. Finally, wegenerate code for stereo block matching algorithms and compare it withhandwritten implementations to quantify the quality of our results.
arxiv-9300-170 | Estimating the Potential Speedup of Computer Vision Applications on Embedded Multiprocessors | http://arxiv.org/pdf/1502.07446v1.pdf | author:Vítor Schwambach, Sébastien Cleyet-Merle, Alain Issard, Stéphane Mancini category:cs.CV cs.DC cs.PF published:2015-02-26 summary:Computer vision applications constitute one of the key drivers for embeddedmulticore architectures. Although the number of available cores is increasingin new architectures, designing an application to maximize the utilization ofthe platform is still a challenge. In this sense, parallel performanceprediction tools can aid developers in understanding the characteristics of anapplication and finding the most adequate parallelization strategy. In thiswork, we present a method for early parallel performance estimation on embeddedmultiprocessors from sequential application traces. We describe itsimplementation in Parana, a fast trace-driven simulator targeting OpenMPapplications on the STMicroelectronics' STxP70 Application-SpecificMultiprocessor (ASMP). Results for the FAST key point detector application showan error margin of less than 10% compared to the reference cycle-approximatesimulator, with lower modeling effort and up to 20x faster execution time.
arxiv-9300-171 | Proceedings of the DATE Friday Workshop on Heterogeneous Architectures and Design Methods for Embedded Image Systems (HIS 2015) | http://arxiv.org/pdf/1502.07241v2.pdf | author:Frank Hannig, Dietmar Fey, Anton Lokhmotov category:cs.AR cs.CV cs.DC published:2015-02-25 summary:This volume contains the papers accepted at the DATE Friday Workshop onHeterogeneous Architectures and Design Methods for Embedded Image Systems (HIS2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located withthe Conference on Design, Automation and Test in Europe (DATE).
arxiv-9300-172 | Connections Between Nuclear Norm and Frobenius Norm Based Representation | http://arxiv.org/pdf/1502.07423v1.pdf | author:Xi Peng, Canyi Lu, Zhang Yi, Huajin Tang category:cs.CV published:2015-02-26 summary:Several recent works have shown that Frobenius-Norm based Representation(FNR) is comparable with Sparse Representation (SR) and Nuclear-Norm basedRepresentation (NNR) in face recognition and subspace clustering. Despite thesuccess of FNR in experimental studies, less theoretical analysis is providedto understand its working mechanism. In this paper, we fill this gap bybridging FNR and NNR. More specially, we prove that: 1) when the dictionary canprovide enough representative capacity, FNR is exactly the NNR; 2) Otherwise,FNR and NNR are two solutions on the column space of the dictionary. The firstresult provides a novel theoretical explanation towards some existing FNR basedmethods by crediting their success to low rank property. The second resultprovides a new insight to understand FNR and NNR under a unified framework.
arxiv-9300-173 | Semi-supervised Segmentation Fusion of Multi-spectral and Aerial Images | http://arxiv.org/pdf/1502.04981v2.pdf | author:Mete Ozay category:cs.CV published:2015-02-17 summary:A Semi-supervised Segmentation Fusion algorithm is proposed using consensusand distributed learning. The aim of Unsupervised Segmentation Fusion (USF) isto achieve a consensus among different segmentation outputs obtained fromdifferent segmentation algorithms by computing an approximate solution to theNP problem with less computational complexity. Semi-supervision is incorporatedin USF using a new algorithm called Semi-supervised Segmentation Fusion (SSSF).In SSSF, side information about the co-occurrence of pixels in the same ordifferent segments is formulated as the constraints of a convex optimizationproblem. The results of the experiments employed on artificial and real-worldbenchmark multi-spectral and aerial images show that the proposed algorithmsperform better than the individual state-of-the art segmentation algorithms.
arxiv-9300-174 | Fracking Deep Convolutional Image Descriptors | http://arxiv.org/pdf/1412.6537v2.pdf | author:Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Francesc Moreno-Noguer category:cs.CV published:2014-12-19 summary:In this paper we propose a novel framework for learning local imagedescriptors in a discriminative manner. For this purpose we explore a siamesearchitecture of Deep Convolutional Neural Networks (CNN), with a Hingeembedding loss on the L2 distance between descriptors. Since a siamesearchitecture uses pairs rather than single image patches to train, there exista large number of positive samples and an exponential number of negativesamples. We propose to explore this space with a stochastic sampling of thetraining set, in combination with an aggressive mining strategy over both thepositive and negative samples which we denote as "fracking". We perform athorough evaluation of the architecture hyper-parameters, and demonstrate largeperformance gains compared to both standard CNN learning strategies,hand-crafted image descriptors like SIFT, and the state-of-the-art on learneddescriptors: up to 2.5x vs SIFT and 1.5x vs the state-of-the-art in terms ofthe area under the curve (AUC) of the Precision-Recall curve.
arxiv-9300-175 | Highly corrupted image inpainting through hypoelliptic diffusion | http://arxiv.org/pdf/1502.07331v1.pdf | author:Dario Prandi, Alexey Remizov, Roman Chertovskih, Ugo Boscain, Jean-Paul Gauthier category:cs.CV math.AP published:2015-02-25 summary:We present a new image inpainting algorithm, the Averaging and HypoellipticEvolution (AHE) algorithm, inspired by the one presented in [1] and based upona (semi-discrete) variation of the Citti--Petitot--Sarti model of the primaryvisual cortex V1. In particular, we focus on reconstructing highly corruptedimages (i.e. where more than the 80% of the image is missing). [1] U. Boscain, R. A. Chertovskih, J. P. Gauthier, and A. O. Remizov,Hypoelliptic diffusion and human vision: a semidiscrete new twist, SIAM J.Imaging Sci., vol. 7, no. 2, pp. 669--695, 2014.
arxiv-9300-176 | Competing with the Empirical Risk Minimizer in a Single Pass | http://arxiv.org/pdf/1412.6606v2.pdf | author:Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford category:stat.ML cs.LG published:2014-12-20 summary:In many estimation problems, e.g. linear and logistic regression, we wish tominimize an unknown objective given only unbiased samples of the objectivefunction. Furthermore, we aim to achieve this using as few samples as possible.In the absence of computational constraints, the minimizer of a sample averageof observed data -- commonly referred to as either the empirical risk minimizer(ERM) or the $M$-estimator -- is widely regarded as the estimation strategy ofchoice due to its desirable statistical convergence properties. Our goal inthis work is to perform as well as the ERM, on every problem, while minimizingthe use of computational resources such as running time and space usage. We provide a simple streaming algorithm which, under standard regularityassumptions on the underlying problem, enjoys the following properties: * The algorithm can be implemented in linear time with a single pass of theobserved data, using space linear in the size of a single sample. * The algorithm achieves the same statistical rate of convergence as theempirical risk minimizer on every problem, even considering constant factors. * The algorithm's performance depends on the initial error at a rate thatdecreases super-polynomially. * The algorithm is easily parallelizable. Moreover, we quantify the (finite-sample) rate at which the algorithm becomescompetitive with the ERM.
arxiv-9300-177 | Online Pairwise Learning Algorithms with Kernels | http://arxiv.org/pdf/1502.07229v1.pdf | author:Yiming Ying, Ding-Xuan Zhou category:stat.ML cs.LG published:2015-02-25 summary:Pairwise learning usually refers to a learning task which involves a lossfunction depending on pairs of examples, among which most notable ones includeranking, metric learning and AUC maximization. In this paper, we study anonline algorithm for pairwise learning with a least-square loss function in anunconstrained setting of a reproducing kernel Hilbert space (RKHS), which werefer to as the Online Pairwise lEaRning Algorithm (OPERA). In contrast toexisting works \cite{Kar,Wang} which require that the iterates are restrictedto a bounded domain or the loss function is strongly-convex, OPERA isassociated with a non-strongly convex objective function and learns the targetfunction in an unconstrained RKHS. Specifically, we establish a general theoremwhich guarantees the almost surely convergence for the last iterate of OPERAwithout any assumptions on the underlying distribution. Explicit convergencerates are derived under the condition of polynomially decaying step sizes. Wealso establish an interesting property for a family of widely-used kernels inthe setting of pairwise learning and illustrate the above convergence resultsusing such kernels. Our methodology mainly depends on the characterization ofRKHSs using its associated integral operators and probability inequalities forrandom variables with values in a Hilbert space.
arxiv-9300-178 | Exploiting Feature and Class Relationships in Video Categorization with Regularized Deep Neural Networks | http://arxiv.org/pdf/1502.07209v1.pdf | author:Yu-Gang Jiang, Zuxuan Wu, Jun Wang, Xiangyang Xue, Shih-Fu Chang category:cs.CV cs.MM published:2015-02-25 summary:In this paper, we study the challenging problem of categorizing videosaccording to high-level semantics such as the existence of a particular humanaction or a complex event. Although extensive efforts have been devoted inrecent years, most existing works combined multiple video features using simplefusion strategies and neglected the utilization of inter-class semanticrelationships. This paper proposes a novel unified framework that jointlyexploits the feature relationships and the class relationships for improvedcategorization performance. Specifically, these two types of relationships areestimated and utilized by rigorously imposing regularizations in the learningprocess of a deep neural network (DNN). Such a regularized DNN (rDNN) can beefficiently realized using a GPU-based implementation with an affordabletraining cost. Through arming the DNN with better capability of harnessing boththe feature and the class relationships, the proposed rDNN is more suitable formodeling video semantics. With extensive experimental evaluations, we show thatrDNN produces superior performance over several state-of-the-art approaches. Onthe well-known Hollywood2 and Columbia Consumer Video benchmarks, we obtainvery competitive results: 66.9\% and 73.5\% respectively in terms of meanaverage precision. In addition, to substantially evaluate our rDNN andstimulate future research on large scale video categorization, we collect andrelease a new benchmark dataset, called FCVID, which contains 91,223 Internetvideos and 239 manually annotated categories.
arxiv-9300-179 | Toggling a Genetic Switch Using Reinforcement Learning | http://arxiv.org/pdf/1303.3183v2.pdf | author:Aivar Sootla, Natalja Strelkowa, Damien Ernst, Mauricio Barahona, Guy-Bart Stan category:cs.SY cs.CE cs.LG q-bio.MN published:2013-03-12 summary:In this paper, we consider the problem of optimal exogenous control of generegulatory networks. Our approach consists in adapting an establishedreinforcement learning algorithm called the fitted Q iteration. This algorithminfers the control law directly from the measurements of the system's responseto external control inputs without the use of a mathematical model of thesystem. The measurement data set can either be collected from wet-labexperiments or artificially created by computer simulations of dynamical modelsof the system. The algorithm is applicable to a wide range of biologicalsystems due to its ability to deal with nonlinear and stochastic systemdynamics. To illustrate the application of the algorithm to a gene regulatorynetwork, the regulation of the toggle switch system is considered. The controlobjective of this problem is to drive the concentrations of two specificproteins to a target region in the state space.
arxiv-9300-180 | The VC-Dimension of Similarity Hypotheses Spaces | http://arxiv.org/pdf/1502.07143v1.pdf | author:Mark Herbster, Paul Rubenstein, James Townsend category:cs.LG published:2015-02-25 summary:Given a set $X$ and a function $h:X\longrightarrow\{0,1\}$ which labels eachelement of $X$ with either $0$ or $1$, we may define a function $h^{(s)}$ tomeasure the similarity of pairs of points in $X$ according to $h$.Specifically, for $h\in \{0,1\}^X$ we define $h^{(s)}\in \{0,1\}^{X\times X}$by $h^{(s)}(w,x):= \mathbb{1}[h(w) = h(x)]$. This idea can be extended to a setof functions, or hypothesis space $\mathcal{H} \subseteq \{0,1\}^X$ by defininga similarity hypothesis space $\mathcal{H}^{(s)}:=\{h^{(s)}:h\in\mathcal{H}\}$.We show that ${{vc-dimension}}(\mathcal{H}^{(s)}) \in\Theta({{vc-dimension}}(\mathcal{H}))$.
arxiv-9300-181 | Cross-Modality Hashing with Partial Correspondence | http://arxiv.org/pdf/1502.05224v2.pdf | author:Yun Gu, Haoyang Xue, Jie Yang category:cs.CV published:2015-02-18 summary:Learning a hashing function for cross-media search is very desirable due toits low storage cost and fast query speed. However, the data crawled fromInternet cannot always guarantee good correspondence among different modalitieswhich affects the learning for hashing function. In this paper, we focus oncross-modal hashing with partially corresponded data. The data without fullcorrespondence are made in use to enhance the hashing performance. Theexperiments on Wiki and NUS-WIDE datasets demonstrates that the proposed methodoutperforms some state-of-the-art hashing approaches with fewer correspondenceinformation.
arxiv-9300-182 | A Note on the Kullback-Leibler Divergence for the von Mises-Fisher distribution | http://arxiv.org/pdf/1502.07104v1.pdf | author:Tom Diethe category:stat.ML published:2015-02-25 summary:We present a derivation of the Kullback Leibler (KL)-Divergence (also knownas Relative Entropy) for the von Mises Fisher (VMF) Distribution in$d$-dimensions.
arxiv-9300-183 | On aggregation for heavy-tailed classes | http://arxiv.org/pdf/1502.07097v1.pdf | author:Shahar Mendelson category:math.ST stat.ML stat.TH I.2.6 published:2015-02-25 summary:We introduce an alternative to the notion of `fast rate' in Learning Theory,which coincides with the optimal error rate when the given class happens to beconvex and regular in some sense. While it is well known that such a ratecannot always be attained by a learning procedure (i.e., a procedure thatselects a function in the given class), we introduce an aggregation procedurethat attains that rate under rather minimal assumptions -- for example, thatthe $L_q$ and $L_2$ norms are equivalent on the linear span of the class forsome $q>2$, and the target random variable is square-integrable.
arxiv-9300-184 | Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval | http://arxiv.org/pdf/1502.07058v1.pdf | author:Adam W. Harley, Alex Ufkes, Konstantinos G. Derpanis category:cs.CV cs.IR cs.LG cs.NE published:2015-02-25 summary:This paper presents a new state-of-the-art for document image classificationand retrieval, using features learned by deep convolutional neural networks(CNNs). In object and scene analysis, deep neural nets are capable of learninga hierarchical chain of abstraction from pixel inputs to concise anddescriptive representations. The current work explores this capacity in therealm of document analysis, and confirms that this representation strategy issuperior to a variety of popular hand-crafted alternatives. Experiments alsoshow that (i) features extracted from CNNs are robust to compression, (ii) CNNstrained on non-document images transfer well to document analysis tasks, and(iii) enforcing region-specific feature-learning is unnecessary givensufficient training data. This work also makes available a new labelled subsetof the IIT-CDIP collection, containing 400,000 document images across 16categories, useful for training new CNNs for document analysis.
arxiv-9300-185 | Describing Colors, Textures and Shapes for Content Based Image Retrieval - A Survey | http://arxiv.org/pdf/1502.07041v1.pdf | author:Jamil Ahmad, Muhammad Sajjad, Irfan Mehmood, Seungmin Rho, Sung Wook Baik category:cs.IR cs.CV published:2015-02-25 summary:Visual media has always been the most enjoyed way of communication. From theadvent of television to the modern day hand held computers, we have witnessedthe exponential growth of images around us. Undoubtedly it's a fact that theycarry a lot of information in them which needs be utilized in an effectivemanner. Hence intense need has been felt to efficiently index and store largeimage collections for effective and on- demand retrieval. For this purposelow-level features extracted from the image contents like color, texture andshape has been used. Content based image retrieval systems employing thesefeatures has proven very successful. Image retrieval has promising applicationsin numerous fields and hence has motivated researchers all over the world. Newand improved ways to represent visual content are being developed each day.Tremendous amount of research has been carried out in the last decade. In thispaper we will present a detailed overview of some of the powerful color,texture and shape descriptors for content based image retrieval. A comparativeanalysis will also be carried out for providing an insight into outstandingchallenges in this field.
arxiv-9300-186 | Web-scale Surface and Syntactic n-gram Features for Dependency Parsing | http://arxiv.org/pdf/1502.07038v1.pdf | author:Dominick Ng, Mohit Bansal, James R. Curran category:cs.CL published:2015-02-25 summary:We develop novel first- and second-order features for dependency parsingbased on the Google Syntactic Ngrams corpus, a collection of subtree counts ofparsed sentences from scanned books. We also extend previous work on surface$n$-gram features from Web1T to the Google Books corpus and from first-order tosecond-order, comparing and analysing performance over newswire and webtreebanks. Surface and syntactic $n$-grams both produce substantial and complementarygains in parsing accuracy across domains. Our best system combines the twofeature sets, achieving up to 0.8% absolute UAS improvements on newswire and1.4% on web text.
arxiv-9300-187 | Building with Drones: Accurate 3D Facade Reconstruction using MAVs | http://arxiv.org/pdf/1502.07019v1.pdf | author:Shreyansh Daftry, Christof Hoppe, Horst Bischof category:cs.RO cs.AI cs.CV published:2015-02-25 summary:Automatic reconstruction of 3D models from images using multi-viewStructure-from-Motion methods has been one of the most fruitful outcomes ofcomputer vision. These advances combined with the growing popularity of MicroAerial Vehicles as an autonomous imaging platform, have made 3D vision toolsubiquitous for large number of Architecture, Engineering and Constructionapplications among audiences, mostly unskilled in computer vision. However, toobtain high-resolution and accurate reconstructions from a large-scale objectusing SfM, there are many critical constraints on the quality of image data,which often become sources of inaccuracy as the current 3D reconstructionpipelines do not facilitate the users to determine the fidelity of input dataduring the image acquisition. In this paper, we present and advocate aclosed-loop interactive approach that performs incremental reconstruction inreal-time and gives users an online feedback about the quality parameters likeGround Sampling Distance (GSD), image redundancy, etc on a surface mesh. Wealso propose a novel multi-scale camera network design to prevent scene driftcaused by incremental map building, and release the first multi-scale imagesequence dataset as a benchmark. Further, we evaluate our system on realoutdoor scenes, and show that our interactive pipeline combined with amulti-scale camera network approach provides compelling accuracy in multi-viewreconstruction tasks when compared against the state-of-the-art methods.
arxiv-9300-188 | On Convolutional Approximations to Linear Dimensionality Reduction Operators for Large Scale Data Processing | http://arxiv.org/pdf/1502.07017v1.pdf | author:Swayambhoo Jain, Jarvis Haupt category:stat.ML published:2015-02-25 summary:In this paper, we examine the problem of approximating a general lineardimensionality reduction (LDR) operator, represented as a matrix $A \in\mathbb{R}^{m \times n}$ with $m < n$, by a partial circulant matrix with rowsrelated by circular shifts. Partial circulant matrices admit fastimplementations via Fourier transform methods and subsampling operations; ourinvestigation here is motivated by a desire to leverage these potentialcomputational improvements in large-scale data processing tasks. We establish afundamental result, that most large LDR matrices (whose row spaces areuniformly distributed) in fact cannot be well approximated by partial circulantmatrices. Then, we propose a natural generalization of the partial circulantapproximation framework that entails approximating the range space of a givenLDR operator $A$ over a restricted domain of inputs, using a matrix formed as aproduct of a partial circulant matrix having $m '> m$ rows and a $m \times k$'post processing' matrix. We introduce a novel algorithmic technique, based onsparse matrix factorization, for identifying the factors comprising suchapproximations, and provide preliminary evidence to demonstrate the potentialof this approach.
arxiv-9300-189 | Classification approach based on association rules mining for unbalanced data | http://arxiv.org/pdf/1202.5514v2.pdf | author:Cheikh Ndour, Aliou Diop, Simplice Dossou-Gbété category:stat.ML cs.LG published:2012-02-24 summary:This paper deals with the binary classification task when the target classhas the lower probability of occurrence. In such situation, it is not possibleto build a powerful classifier by using standard methods such as logisticregression, classification tree, discriminant analysis, etc. To overcome thisshort-coming of these methods which yield classifiers with low sensibility, wetackled the classification problem here through an approach based on theassociation rules learning. This approach has the advantage of allowing theidentification of the patterns that are well correlated with the target class.Association rules learning is a well known method in the area of data-mining.It is used when dealing with large database for unsupervised discovery of localpatterns that expresses hidden relationships between input variables. Inconsidering association rules from a supervised learning point of view, arelevant set of weak classifiers is obtained from which one derives aclassifier that performs well.
arxiv-9300-190 | Personalising Mobile Advertising Based on Users Installed Apps | http://arxiv.org/pdf/1503.00587v1.pdf | author:Jenna Reps, Uwe Aickelin, Jonathan Garibaldi, Chris Damski category:cs.CY cs.LG published:2015-02-24 summary:Mobile advertising is a billion pound industry that is rapidly expanding. Thesuccess of an advert is measured based on how users interact with it. In thispaper we investigate whether the application of unsupervised learning andassociation rule mining could be used to enable personalised targeting ofmobile adverts with the aim of increasing the interaction rate. Over May andJune 2014 we recorded advert interactions such as tapping the advert orwatching the whole advert video along with the set of apps a user has installedat the time of the interaction. Based on the apps that the users have installedwe applied k-means clustering to profile the users into one of ten classes. Dueto the large number of apps considered we implemented dimension reduction toreduced the app feature space by mapping the apps to their iTunes category andclustered users based on the percentage of their apps that correspond to eachiTunes app category. The clustering was externally validated by investigatingdifferences between the way the ten profiles interact with the various advertsgenres (lifestyle, finance and entertainment adverts). In addition associationrule mining was performed to find whether the time of the day that the advertis served and the number of apps a user has installed makes certain profilesmore likely to interact with the advert genres. The results showed there wereclear differences in the way the profiles interact with the different advertgenres and the results of this paper suggest that mobile advert targeting wouldimprove the frequency that users interact with an advert.
arxiv-9300-191 | Scalable Variational Inference in Log-supermodular Models | http://arxiv.org/pdf/1502.06531v2.pdf | author:Josip Djolonga, Andreas Krause category:cs.LG stat.ML published:2015-02-23 summary:We consider the problem of approximate Bayesian inference in log-supermodularmodels. These models encompass regular pairwise MRFs with binary variables, butallow to capture high-order interactions, which are intractable for existingapproximate inference techniques such as belief propagation, mean field, andvariants. We show that a recently proposed variational approach to inference inlog-supermodular models -L-FIELD- reduces to the widely-studied minimum normproblem for submodular minimization. This insight allows to leverage powerfulexisting tools, and hence to solve the variational problem orders of magnitudemore efficiently than previously possible. We then provide another naturalinterpretation of L-FIELD, demonstrating that it exactly minimizes a specifictype of R\'enyi divergence measure. This insight sheds light on the nature ofthe variational approximations produced by L-FIELD. Furthermore, we show how toperform parallel inference as message passing in a suitable factor graph at alinear convergence rate, without having to sum up over all the configurationsof the factor. Finally, we apply our approach to a challenging imagesegmentation task. Our experiments confirm scalability of our approach, highquality of the marginals, and the benefit of incorporating higher-orderpotentials.
arxiv-9300-192 | A Review of Audio Features and Statistical Models Exploited for Voice Pattern Design | http://arxiv.org/pdf/1502.06811v1.pdf | author:Ngoc Q. K. Duong, Hien-Thanh Duong category:cs.SD stat.ML published:2015-02-24 summary:Audio fingerprinting, also named as audio hashing, has been well-known as apowerful technique to perform audio identification and synchronization. Itbasically involves two major steps: fingerprint (voice pattern) design andmatching search. While the first step concerns the derivation of a robust andcompact audio signature, the second step usually requires knowledge aboutdatabase and quick-search algorithms. Though this technique offers a wide rangeof real-world applications, to the best of the authors' knowledge, acomprehensive survey of existing algorithms appeared more than eight years ago.Thus, in this paper, we present a more up-to-date review and, for emphasizingon the audio signal processing aspect, we focus our state-of-the-art survey onthe fingerprint design step for which various audio features and theirtractable statistical models are discussed.
arxiv-9300-193 | Hands Deep in Deep Learning for Hand Pose Estimation | http://arxiv.org/pdf/1502.06807v1.pdf | author:Markus Oberweger, Paul Wohlhart, Vincent Lepetit category:cs.CV published:2015-02-24 summary:We introduce and evaluate several architectures for Convolutional NeuralNetworks to predict the 3D joint locations of a hand given a depth map. Wefirst show that a prior on the 3D pose can be easily introduced andsignificantly improves the accuracy and reliability of the predictions. We alsoshow how to use context efficiently to deal with ambiguities between fingers.These two contributions allow us to significantly outperform thestate-of-the-art on several challenging benchmarks, both in terms of accuracyand computation times.
arxiv-9300-194 | New HSL Distance Based Colour Clustering Algorithm | http://arxiv.org/pdf/1505.05819v1.pdf | author:Vasile Patrascu category:cs.CV published:2015-02-24 summary:In this paper, we define a distance for the HSL colour system. Next, theproposed distance is used for a fuzzy colour clustering algorithm construction.The presented algorithm is related to the well-known fuzzy c-means algorithm.Finally, the clustering algorithm is used as colour reduction method. Theobtained experimental results are presented to demonstrate the effectiveness ofour approach.
arxiv-9300-195 | Online Tracking by Learning Discriminative Saliency Map with Convolutional Neural Network | http://arxiv.org/pdf/1502.06796v1.pdf | author:Seunghoon Hong, Tackgeun You, Suha Kwak, Bohyung Han category:cs.CV published:2015-02-24 summary:We propose an online visual tracking algorithm by learning discriminativesaliency map using Convolutional Neural Network (CNN). Given a CNN pre-trainedon a large-scale image repository in offline, our algorithm takes outputs fromhidden layers of the network as feature descriptors since they show excellentrepresentation performance in various general visual recognition problems. Thefeatures are used to learn discriminative target appearance models using anonline Support Vector Machine (SVM). In addition, we construct target-specificsaliency map by backpropagating CNN features with guidance of the SVM, andobtain the final tracking result in each frame based on the appearance modelgeneratively constructed with the saliency map. Since the saliency mapvisualizes spatial configuration of target effectively, it improves targetlocalization accuracy and enable us to achieve pixel-level target segmentation.We verify the effectiveness of our tracking algorithm through extensiveexperiment on a challenging benchmark, where our method illustrates outstandingperformance compared to the state-of-the-art tracking algorithms.
arxiv-9300-196 | Using Riemannian geometry for SSVEP-based Brain Computer Interface | http://arxiv.org/pdf/1501.03227v3.pdf | author:Emmanuel K. Kalunga, Sylvain Chevallier, Quentin Barthelemy category:cs.LG stat.ML published:2015-01-14 summary:Riemannian geometry has been applied to Brain Computer Interface (BCI) forbrain signals classification yielding promising results. Studyingelectroencephalographic (EEG) signals from their associated covariance matricesallows a mitigation of common sources of variability (electronic, electrical,biological) by constructing a representation which is invariant to theseperturbations. While working in Euclidean space with covariance matrices isknown to be error-prone, one might take advantage of algorithmic advances ininformation geometry and matrix manifold to implement methods for SymmetricPositive-Definite (SPD) matrices. This paper proposes a comprehensive review ofthe actual tools of information geometry and how they could be applied oncovariance matrices of EEG. In practice, covariance matrices should beestimated, thus a thorough study of all estimators is conducted on real EEGdataset. As a main contribution, this paper proposes an online implementationof a classifier in the Riemannian space and its subsequent assessment inSteady-State Visually Evoked Potential (SSVEP) experimentations.
arxiv-9300-197 | On the Complexity of A/B Testing | http://arxiv.org/pdf/1405.3224v2.pdf | author:Emilie Kaufmann, Olivier Cappé, Aurélien Garivier category:math.ST cs.LG stat.ML stat.TH published:2014-05-13 summary:A/B testing refers to the task of determining the best option among twoalternatives that yield random outcomes. We provide distribution-dependentlower bounds for the performance of A/B testing that improve over the resultscurrently available both in the fixed-confidence (or delta-PAC) andfixed-budget settings. When the distribution of the outcomes are Gaussian, weprove that the complexity of the fixed-confidence and fixed-budget settings areequivalent, and that uniform sampling of both alternatives is optimal only inthe case of equal variances. In the common variance case, we also provide astopping rule that terminates faster than existing fixed-confidence algorithms.In the case of Bernoulli distributions, we show that the complexity offixed-budget setting is smaller than that of fixed-confidence setting and thatuniform sampling of both alternatives -though not optimal- is advisable inpractice when combined with an appropriate stopping criterion.
arxiv-9300-198 | Discrete Wavelet Transform and Gradient Difference based approach for text localization in videos | http://arxiv.org/pdf/1502.06703v1.pdf | author:B. H. Shekar, Smitha M. L., P. Shivakumara category:cs.CV published:2015-02-24 summary:The text detection and localization is important for video analysis andunderstanding. The scene text in video contains semantic information and thuscan contribute significantly to video retrieval and understanding. However,most of the approaches detect scene text in still images or single video frame.Videos differ from images in temporal redundancy. This paper proposes a novelhybrid method to robustly localize the texts in natural scene images and videosbased on fusion of discrete wavelet transform and gradient difference. A set ofrules and geometric properties have been devised to localize the actual textregions. Then, morphological operation is performed to generate the textregions and finally the connected component analysis is employed to localizethe text in a video frame. The experimental results obtained on publiclyavailable standard ICDAR 2003 and Hua dataset illustrate that the proposedmethod can accurately detect and localize texts of various sizes, fonts andcolors. The experimentation on huge collection of video databases reveal thesuitability of the proposed method to video databases.
arxiv-9300-199 | 1-Bit Matrix Completion under Exact Low-Rank Constraint | http://arxiv.org/pdf/1502.06689v1.pdf | author:Sonia Bhaskar, Adel Javanmard category:stat.ML published:2015-02-24 summary:We consider the problem of noisy 1-bit matrix completion under an exact rankconstraint on the true underlying matrix $M^*$. Instead of observing a subsetof the noisy continuous-valued entries of a matrix $M^*$, we observe a subsetof noisy 1-bit (or binary) measurements generated according to a probabilisticmodel. We consider constrained maximum likelihood estimation of $M^*$, under aconstraint on the entry-wise infinity-norm of $M^*$ and an exact rankconstraint. This is in contrast to previous work which has used convexrelaxations for the rank. We provide an upper bound on the matrix estimationerror under this model. Compared to the existing results, our bound has fasterconvergence rate with matrix dimensions when the fraction of revealed 1-bitobservations is fixed, independent of the matrix dimensions. We also propose aniterative algorithm for solving our nonconvex optimization with a certificateof global optimality of the limiting point. This algorithm is based on low rankfactorization of $M^*$. We validate the method on synthetic and real data withimproved performance over existing methods.
arxiv-9300-200 | Learning Fast-Mixing Models for Structured Prediction | http://arxiv.org/pdf/1502.06668v1.pdf | author:Jacob Steinhardt, Percy Liang category:cs.LG published:2015-02-24 summary:Markov Chain Monte Carlo (MCMC) algorithms are often used for approximateinference inside learning, but their slow mixing can be difficult to diagnoseand the approximations can seriously degrade learning. To alleviate theseissues, we define a new model family using strong Doeblin Markov chains, whosemixing times can be precisely controlled by a parameter. We also develop analgorithm to learn such models, which involves maximizing the data likelihoodunder the induced stationary distribution of these chains. We show empiricalimprovements on two challenging inference tasks.
arxiv-9300-201 | Reified Context Models | http://arxiv.org/pdf/1502.06665v1.pdf | author:Jacob Steinhardt, Percy Liang category:cs.LG published:2015-02-24 summary:A classic tension exists between exact inference in a simple model andapproximate inference in a complex model. The latter offers expressivity andthus accuracy, but the former provides coverage of the space, an importantproperty for confidence estimation and learning with indirect supervision. Inthis work, we introduce a new approach, reified context models, to reconcilethis tension. Specifically, we let the amount of context (the arity of thefactors in a graphical model) be chosen "at run-time" by reifying it---that is,letting this choice itself be a random variable inside the model. Empirically,we show that our approach obtains expressivity and coverage on three naturallanguage tasks.
arxiv-9300-202 | On The Identifiability of Mixture Models from Grouped Samples | http://arxiv.org/pdf/1502.06644v1.pdf | author:Robert A. Vandermeulen, Clayton D. Scott category:stat.ML cs.LG math.ST stat.TH published:2015-02-23 summary:Finite mixture models are statistical models which appear in many problems instatistics and machine learning. In such models it is assumed that data aredrawn from random probability measures, called mixture components, which arethemselves drawn from a probability measure P over probability measures. Whenestimating mixture models, it is common to make assumptions on the mixturecomponents, such as parametric assumptions. In this paper, we make noassumption on the mixture components, and instead assume that observations fromthe mixture model are grouped, such that observations in the same group areknown to be drawn from the same component. We show that any mixture of mprobability measures can be uniquely identified provided there are 2m-1observations per group. Moreover we show that, for any m, there exists amixture of m probability measures that cannot be uniquely identified whengroups have 2m-2 observations. Our results hold for any sample space with morethan one element.
arxiv-9300-203 | Optimal Sparse Linear Auto-Encoders and Sparse PCA | http://arxiv.org/pdf/1502.06626v1.pdf | author:Malik Magdon-Ismail, Christos Boutsidis category:cs.LG cs.AI cs.IT math.IT stat.CO stat.ML published:2015-02-23 summary:Principal components analysis (PCA) is the optimal linear auto-encoder ofdata, and it is often used to construct features. Enforcing sparsity on theprincipal components can promote better generalization, while improving theinterpretability of the features. We study the problem of constructing optimalsparse linear auto-encoders. Two natural questions in such a setting are: i)Given a level of sparsity, what is the best approximation to PCA that can beachieved? ii) Are there low-order polynomial-time algorithms which canasymptotically achieve this optimal tradeoff between the sparsity and theapproximation quality? In this work, we answer both questions by giving efficient low-orderpolynomial-time algorithms for constructing asymptotically \emph{optimal}linear auto-encoders (in particular, sparse features with near-PCAreconstruction error) and demonstrate the performance of our algorithms on realdata.
arxiv-9300-204 | Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems | http://arxiv.org/pdf/1502.06590v1.pdf | author:Yash Deshpande, Andrea Montanari category:cs.CC cs.IT math.IT math.ST stat.ML stat.TH published:2015-02-23 summary:Given a large data matrix $A\in\mathbb{R}^{n\times n}$, we consider theproblem of determining whether its entries are i.i.d. with some known marginaldistribution $A_{ij}\sim P_0$, or instead $A$ contains a principal submatrix$A_{{\sf Q},{\sf Q}}$ whose entries have marginal distribution $A_{ij}\simP_1\neq P_0$. As a special case, the hidden (or planted) clique problemrequires to find a planted clique in an otherwise uniformly random graph. Assuming unbounded computational resources, this hypothesis testing problemis statistically solvable provided ${\sf Q}\ge C \log n$ for a suitableconstant $C$. However, despite substantial effort, no polynomial time algorithmis known that succeeds with high probability when ${\sf Q} = o(\sqrt{n})$.Recently Meka and Wigderson \cite{meka2013association}, proposed a method toestablish lower bounds within the Sum of Squares (SOS) semidefinite hierarchy. Here we consider the degree-$4$ SOS relaxation, and study the construction of\cite{meka2013association} to prove that SOS fails unless $k\ge C\,n^{1/3}/\log n$. An argument presented by Barak implies that this lower boundcannot be substantially improved unless the witness construction is changed inthe proof. Our proof uses the moments method to bound the spectrum of a certainrandom association scheme, i.e. a symmetric random matrix whose rows andcolumns are indexed by the edges of an Erd\"os-Renyi random graph.
arxiv-9300-205 | Shannon, Tsallis and Kaniadakis entropies in bi-level image thresholding | http://arxiv.org/pdf/1502.06556v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2015-02-23 summary:The maximum entropy principle is often used for bi-level or multi-levelthresholding of images. For this purpose, some methods are available based onShannon and Tsallis entropies. In this paper, we discuss them and propose amethod based on Kaniadakis entropy.
arxiv-9300-206 | An Algorithm for Online K-Means Clustering | http://arxiv.org/pdf/1412.5721v2.pdf | author:Edo Liberty, Ram Sriharsha, Maxim Sviridenko category:cs.DS cs.LG published:2014-12-18 summary:This paper shows that one can be competitive with the k-means objective whileoperating online. In this model, the algorithm receives vectors v_1,...,v_n oneby one in an arbitrary order. For each vector the algorithm outputs a clusteridentifier before receiving the next one. Our online algorithm generates ~O(k)clusters whose k-means cost is ~O(W*). Here, W* is the optimal k-means costusing k clusters and ~O suppresses poly-logarithmic factors. We also show that,experimentally, it is not much worse than k-means++ while operating in astrictly more constrained computational model.
arxiv-9300-207 | Bandit Convex Optimization: sqrt{T} Regret in One Dimension | http://arxiv.org/pdf/1502.06398v1.pdf | author:Sébastien Bubeck, Ofer Dekel, Tomer Koren, Yuval Peres category:cs.LG math.OC published:2015-02-23 summary:We analyze the minimax regret of the adversarial bandit convex optimizationproblem. Focusing on the one-dimensional case, we prove that the minimax regretis $\widetilde\Theta(\sqrt{T})$ and partially resolve a decade-old openproblem. Our analysis is non-constructive, as we do not present a concretealgorithm that attains this regret rate. Instead, we use minimax duality toreduce the problem to a Bayesian setting, where the convex loss functions aredrawn from a worst-case distribution, and then we solve the Bayesian version ofthe problem with a variant of Thompson Sampling. Our analysis features a noveluse of convexity, formalized as a "local-to-global" property of convexfunctions, that may be of independent interest.
arxiv-9300-208 | Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding | http://arxiv.org/pdf/1502.06344v1.pdf | author:Clemens-Alexander Brust, Sven Sickert, Marcel Simon, Erik Rodner, Joachim Denzler category:cs.CV published:2015-02-23 summary:Classifying single image patches is important in many different applications,such as road detection or scene understanding. In this paper, we presentconvolutional patch networks, which are convolutional networks learned todistinguish different image patches and which can be used for pixel-wiselabeling. We also show how to incorporate spatial information of the patch asan input to the network, which allows for learning spatial priors for certaincategories jointly with an appearance model. In particular, we focus on roaddetection and urban scene understanding, two application areas where we areable to achieve state-of-the-art results on the KITTI as well as on theLabelMeFacade dataset. Furthermore, our paper offers a guideline for people working in the area anddesperately wandering through all the painstaking details that render trainingCNs on image patches extremely difficult.
arxiv-9300-209 | Differentially Private Bayesian Optimization | http://arxiv.org/pdf/1501.04080v2.pdf | author:Matt J. Kusner, Jacob R. Gardner, Roman Garnett, Kilian Q. Weinberger category:stat.ML published:2015-01-16 summary:Bayesian optimization is a powerful tool for fine-tuning the hyper-parametersof a wide variety of machine learning models. The success of machine learninghas led practitioners in diverse real-world settings to learn classifiers forpractical problems. As machine learning becomes commonplace, Bayesianoptimization becomes an attractive method for practitioners to automate theprocess of classifier hyper-parameter tuning. A key observation is that thedata used for tuning models in these settings is often sensitive. Certain datasuch as genetic predisposition, personal email statistics, and car accidenthistory, if not properly private, may be at risk of being inferred fromBayesian optimization outputs. To address this, we introduce methods forreleasing the best hyper-parameters and classifier accuracy privately.Leveraging the strong theoretical guarantees of differential privacy and knownBayesian optimization convergence bounds, we prove that under a GP assumptionthese private quantities are also near-optimal. Finally, even if thisassumption is not satisfied, we can use different smoothness guarantees toprotect privacy.
arxiv-9300-210 | Compressive Hyperspectral Imaging with Side Information | http://arxiv.org/pdf/1502.06260v1.pdf | author:Xin Yuan, Tsung-Han Tsai, Ruoyu Zhu, Patrick Llull, David Brady, Lawrence Carin category:cs.CV published:2015-02-22 summary:A blind compressive sensing algorithm is proposed to reconstructhyperspectral images from spectrally-compressed measurements.Thewavelength-dependent data are coded and then superposed, mapping thethree-dimensional hyperspectral datacube to a two-dimensional image. Theinversion algorithm learns a dictionary {\em in situ} from the measurements viaglobal-local shrinkage priors. By using RGB images as side information of thecompressive sensing system, the proposed approach is extended to learn acoupled dictionary from the joint dataset of the compressed measurements andthe corresponding RGB images, to improve reconstruction quality. A prototypecamera is built using a liquid-crystal-on-silicon modulator. Experimentalreconstructions of hyperspectral datacubes from both simulated and realcompressed measurements demonstrate the efficacy of the proposed inversionalgorithm, the feasibility of the camera and the benefit of side information.
arxiv-9300-211 | Generative Deep Deconvolutional Learning | http://arxiv.org/pdf/1412.6039v3.pdf | author:Yunchen Pu, Xin Yuan, Lawrence Carin category:stat.ML cs.LG published:2014-12-18 summary:A generative Bayesian model is developed for deep (multi-layer) convolutionaldictionary learning. A novel probabilistic pooling operation is integrated intothe deep model, yielding efficient bottom-up and top-down probabilisticlearning. After learning the deep convolutional dictionary, testing isimplemented via deconvolutional inference. To speed up this inference, a newstatistical approach is proposed to project the top-layer dictionary elementsto the data level. Following this, only one layer of deconvolution is requiredduring testing. Experimental results demonstrate powerful capabilities of themodel to learn multi-layer features from images. Excellent classificationresults are obtained on both the MNIST and Caltech 101 datasets.
arxiv-9300-212 | Some enumerations of binary digital images | http://arxiv.org/pdf/1502.06236v1.pdf | author:P. Christopher Staecker category:math.CO cs.CV math.GN I.4.m published:2015-02-22 summary:The topology of digital images has been studied much in recent years, but noattempt has been made to exhaustively catalog the structure of binary images ofsmall numbers of points. We produce enumerations of several classes of digitalimages up to isomorphism and decide which among them are homotopy equivalent toone another. Noting some patterns in the results, we make some conjecturesabout digital images which are irreducible but not rigid.
arxiv-9300-213 | Spatio-temporal Video Parsing for Abnormality Detection | http://arxiv.org/pdf/1502.06235v1.pdf | author:Borislav Antić, Björn Ommer category:cs.CV published:2015-02-22 summary:Abnormality detection in video poses particular challenges due to theinfinite size of the class of all irregular objects and behaviors. Thus no (orby far not enough) abnormal training samples are available and we need to findabnormalities in test data without actually knowing what they are.Nevertheless, the prevailing concept of the field is to directly search forindividual abnormal local patches or image regions independent of another. Toaddress this problem, we propose a method for joint detection of abnormalitiesin videos by spatio-temporal video parsing. The goal of video parsing is tofind a set of indispensable normal spatio-temporal object hypotheses thatjointly explain all the foreground of a video, while, at the same time, beingsupported by normal training samples. Consequently, we avoid a direct detectionof abnormalities and discover them indirectly as those hypotheses which areneeded for covering the foreground without finding an explanation forthemselves by normal samples. Abnormalities are localized by MAP inference in agraphical model and we solve it efficiently by formulating it as a convexoptimization problem. We experimentally evaluate our approach on severalchallenging benchmark sets, improving over the state-of-the-art on all standardbenchmarks both in terms of abnormality classification and localization.
arxiv-9300-214 | Video Text Localization with an emphasis on Edge Features | http://arxiv.org/pdf/1502.06219v1.pdf | author:B. H. Shekar, Smitha M. L. category:cs.CV published:2015-02-22 summary:The text detection and localization plays a major role in video analysis andunderstanding. The scene text embedded in video consist of high-level semanticsand hence contributes significantly to visual content analysis and retrieval.This paper proposes a novel method to robustly localize the texts in naturalscene images and videos based on sobel edge emphasizing approach. The inputimage is preprocessed and edge emphasis is done to detect the text clusters.Further, a set of rules have been devised using morphological operators forfalse positive elimination and connected component analysis is performed todetect the text regions and hence text localization is performed. Theexperimental results obtained on publicly available standard datasetsillustrate that the proposed method can detect and localize the texts ofvarious sizes, fonts and colors.
arxiv-9300-215 | Bi-Level Image Thresholding obtained by means of Kaniadakis Entropy | http://arxiv.org/pdf/1502.04500v3.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2015-02-16 summary:In this paper we are proposing the use of Kaniadakis entropy in the bi-levelthresholding of images, in the framework of a maximum entropy principle. Wediscuss the role of its entropic index in determining the threshold and indriving an "image transition", that is, an abrupt transition in the appearanceof the corresponding bi-level image. Some examples are proposed to illustratethe method and for comparing it to the approach which is using the Tsallisentropy.
arxiv-9300-216 | Nearly optimal classification for semimetrics | http://arxiv.org/pdf/1502.06208v1.pdf | author:Lee-Ad Gottlieb, Aryeh Kontorovich category:cs.LG cs.CC cs.DS published:2015-02-22 summary:We initiate the rigorous study of classification in semimetric spaces, whichare point sets with a distance function that is non-negative and symmetric, butneed not satisfy the triangle inequality. For metric spaces, the doublingdimension essentially characterizes both the runtime and sample complexity ofclassification algorithms --- yet we show that this is not the case forsemimetrics. Instead, we define the {\em density dimension} and discover thatit plays a central role in the statistical and algorithmic feasibility oflearning in semimetric spaces. We present nearly optimal sample compressionalgorithms and use these to obtain generalization guarantees, including fastrates. The latter hold for general sample compression schemes and may be ofindependent interest.
arxiv-9300-217 | Two-stage Sampling, Prediction and Adaptive Regression via Correlation Screening (SPARCS) | http://arxiv.org/pdf/1502.06189v1.pdf | author:Hamed Firouzi, Bala Rajaratnam, Alfred Hero category:stat.ML cs.LG published:2015-02-22 summary:This paper proposes a general adaptive procedure for budget-limited predictordesign in high dimensions called two-stage Sampling, Prediction and AdaptiveRegression via Correlation Screening (SPARCS). SPARCS can be applied to highdimensional prediction problems in experimental science, medicine, finance, andengineering, as illustrated by the following. Suppose one wishes to run asequence of experiments to learn a sparse multivariate predictor of a dependentvariable $Y$ (disease prognosis for instance) based on a $p$ dimensional set ofindependent variables $\mathbf X=[X_1,\ldots, X_p]^T$ (assayed biomarkers).Assume that the cost of acquiring the full set of variables $\mathbf X$increases linearly in its dimension. SPARCS breaks the data collection into twostages in order to achieve an optimal tradeoff between sampling cost andpredictor performance. In the first stage we collect a few ($n$) expensivesamples $\{y_i,\mathbf x_i\}_{i=1}^n$, at the full dimension $p\gg n$ of$\mathbf X$, winnowing the number of variables down to a smaller dimension $l <p$ using a type of cross-correlation or regression coefficient screening. Inthe second stage we collect a larger number $(t-n)$ of cheaper samples of the$l$ variables that passed the screening of the first stage. At the secondstage, a low dimensional predictor is constructed by solving the standardregression problem using all $t$ samples of the selected variables. SPARCS isan adaptive online algorithm that implements false positive control on theselected variables, is well suited to small sample sizes, and is scalable tohigh dimensions. We establish asymptotic bounds for the Familywise Error Rate(FWER), specify high dimensional convergence rates for support recovery, andestablish optimal sample allocation rules to the first and second stages.
arxiv-9300-218 | Teaching and compressing for low VC-dimension | http://arxiv.org/pdf/1502.06187v1.pdf | author:Shay Moran, Amir Shpilka, Avi Wigderson, Amir Yehudayoff category:cs.LG published:2015-02-22 summary:In this work we study the quantitative relation between VC-dimension and twoother basic parameters related to learning and teaching. We present relativelyefficient constructions of {\em sample compression schemes} and {\em teachingsets} for classes of low VC-dimension. Let $C$ be a finite boolean conceptclass of VC-dimension $d$. Set $k = O(d 2^d \log \log C)$. We construct sample compression schemes of size $k$ for $C$, with additionalinformation of $k \log(k)$ bits. Roughly speaking, given any list of$C$-labelled examples of arbitrary length, we can retain only $k$ labeledexamples in a way that allows to recover the labels of all others examples inthe list. We also prove that there always exists a concept $c$ in $C$ with a teachingset (i.e. a list of $c$-labelled examples uniquely identifying $c$) of size$k$. Equivalently, we prove that the recursive teaching dimension of $C$ is atmost $k$. The question of constructing sample compression schemes for classes of smallVC-dimension was suggested by Littlestone and Warmuth (1986), and the problemof constructing teaching sets for classes of small VC-dimension was suggestedby Kuhlmann (1999). Previous constructions for general concept classes yieldedsize $O(\log C)$ for both questions, even when the VC-dimension is constant.
arxiv-9300-219 | SDCA without Duality | http://arxiv.org/pdf/1502.06177v1.pdf | author:Shai Shalev-Shwartz category:cs.LG published:2015-02-22 summary:Stochastic Dual Coordinate Ascent is a popular method for solving regularizedloss minimization for the case of convex losses. In this paper we show how avariant of SDCA can be applied for non-convex losses. We prove linearconvergence rate even if individual loss functions are non-convex as long asthe expected loss is convex.
arxiv-9300-220 | Clustering multi-way data: a novel algebraic approach | http://arxiv.org/pdf/1412.7056v2.pdf | author:Eric Kernfeld, Shuchin Aeron, Misha Kilmer category:cs.LG cs.CV cs.IT math.IT stat.ML published:2014-12-22 summary:In this paper, we develop a method for unsupervised clustering of two-way(matrix) data by combining two recent innovations from different fields: theSparse Subspace Clustering (SSC) algorithm [10], which groups points comingfrom a union of subspaces into their respective subspaces, and the t-product[18], which was introduced to provide a matrix-like multiplication for thirdorder tensors. Our algorithm is analogous to SSC in that an "affinity" betweendifferent data points is built using a sparse self-representation of the data.Unlike SSC, we employ the t-product in the self-representation. This allows usmore flexibility in modeling; infact, SSC is a special case of our method. Whenusing the t-product, three-way arrays are treated as matrices whose elements(scalars) are n-tuples or tubes. Convolutions take the place of scalarmultiplication. This framework allows us to embed the 2-D data into avector-space-like structure called a free module over a commutative ring. Thesefree modules retain many properties of complex inner-product spaces, and weleverage that to provide theoretical guarantees on our algorithm. We show thatcompared to vector-space counterparts, SSmC achieves higher accuracy and betterable to cluster data with less preprocessing in some image clustering problems.In particular we show the performance of the proposed method on Weizmann facedatabase, the Extended Yale B Face database and the MNIST handwritten digitsdatabase.
arxiv-9300-221 | Using NLP to measure democracy | http://arxiv.org/pdf/1502.06161v1.pdf | author:Thiago Marzagão category:cs.CL cs.IR cs.LG stat.ML published:2015-02-22 summary:This paper uses natural language processing to create the first machine-codeddemocracy index, which I call Automated Democracy Scores (ADS). The ADS arebased on 42 million news articles from 6,043 different sources and cover allindependent countries in the 1993-2012 period. Unlike the democracy indices wehave today the ADS are replicable and have standard errors small enough toactually distinguish between cases. The ADS are produced with supervised learning. Three approaches are tried: a)a combination of Latent Semantic Analysis and tree-based regression methods; b)a combination of Latent Dirichlet Allocation and tree-based regression methods;and c) the Wordscores algorithm. The Wordscores algorithm outperforms thealternatives, so it is the one on which the ADS are based. There is a web application where anyone can change the training set and seehow the results change: democracy-scores.org
arxiv-9300-222 | Learning Generative Models with Visual Attention | http://arxiv.org/pdf/1312.6110v3.pdf | author:Yichuan Tang, Nitish Srivastava, Ruslan Salakhutdinov category:cs.CV published:2013-12-20 summary:Attention has long been proposed by psychologists as important foreffectively dealing with the enormous sensory stimulus available in theneocortex. Inspired by the visual attention models in computationalneuroscience and the need of object-centric data for generative models, wedescribe for generative learning framework using attentional mechanisms.Attentional mechanisms can propagate signals from region of interest in a sceneto an aligned canonical representation, where generative modeling takes place.By ignoring background clutter, generative models can concentrate theirresources on the object of interest. Our model is a proper graphical modelwhere the 2D Similarity transformation is a part of the top-down process. AConvNet is employed to provide good initializations during posterior inferencewhich is based on Hamiltonian Monte Carlo. Upon learning images of faces, ourmodel can robustly attend to face regions of novel test subjects. Moreimportantly, our model can learn generative models of new faces from a noveldataset of large images where the face locations are not known.
arxiv-9300-223 | Detection of Planted Solutions for Flat Satisfiability Problems | http://arxiv.org/pdf/1502.06144v1.pdf | author:Quentin Berthet, Jordan S. Ellenberg category:math.ST cs.CC cs.LG stat.TH published:2015-02-21 summary:We study the detection problem of finding planted solutions in randominstances of flat satisfiability problems, a generalization of booleansatisfiability formulas. We describe the properties of random instances of flatsatisfiability, as well of the optimal rates of detection of the associatedhypothesis testing problem. We also study the performance of an algorithmicallyefficient testing procedure. We introduce a modification of our model, thelight planting of solutions, and show that it is as hard as the problem oflearning parity with noise. This hints strongly at the difficulty of detectingplanted flat satisfiability for a wide class of tests.
arxiv-9300-224 | Universal Memory Architectures for Autonomous Machines | http://arxiv.org/pdf/1502.06132v1.pdf | author:Dan P. Guralnik, Daniel E. Koditschek category:cs.AI cs.LG cs.RO math.MG published:2015-02-21 summary:We propose a self-organizing memory architecture for perceptual experience,capable of supporting autonomous learning and goal-directed problem solving inthe absence of any prior information about the agent's environment. Thearchitecture is simple enough to ensure (1) a quadratic bound (in the number ofavailable sensors) on space requirements, and (2) a quadratic bound on thetime-complexity of the update-execute cycle. At the same time, it issufficiently complex to provide the agent with an internal representation whichis (3) minimal among all representations of its class which account for everysensory equivalence class subject to the agent's belief state; (4) capable, inprinciple, of recovering the homotopy type of the system's state space; (5)learnable with arbitrary precision through a random application of theavailable actions. The provable properties of an effectively trained memorystructure exploit a duality between weak poc sets -- a symbolic (discrete)representation of subset nesting relations -- and non-positively curved cubicalcomplexes, whose rich convexity theory underlies the planning cycle of theproposed architecture.
arxiv-9300-225 | Sensitivity Analysis for Computationally Expensive Models using Optimization and Objective-oriented Surrogate Approximations | http://arxiv.org/pdf/1410.7291v2.pdf | author:Yilun Wang, Christine A. Shoemaker category:stat.ML 62K05 G.3 published:2014-10-27 summary:In this paper, we focus on developing efficient sensitivity analysis methodsfor a computationally expensive objective function $f(x)$ in the case that theminimization of it has just been performed. Here "computationally expensive"means that each of its evaluation takes significant amount of time, andtherefore our main goal to use a small number of function evaluations of $f(x)$to further infer the sensitivity information of these different parameters.Correspondingly, we consider the optimization procedure as an adaptiveexperimental design and re-use its available function evaluations as theinitial design points to establish a surrogate model $s(x)$ (or called responsesurface). The sensitivity analysis is performed on $s(x)$, which is an lieu of$f(x)$. Furthermore, we propose a new local multivariate sensitivity measure,for example, around the optimal solution, for high dimensional problems. Then acorresponding "objective-oriented experimental design" is proposed in order tomake the generated surrogate $s(x)$ better suitable for the accuratecalculation of the proposed specific local sensitivity quantities. In addition,we demonstrate the better performance of the Gaussian radial basis functioninterpolator over Kriging in our cases, which are of relatively highdimensionality and few experimental design points. Numerical experimentsdemonstrate that the optimization procedure and the "objective-orientedexperimental design" behavior much better than the classical Latin HypercubeDesign. In addition, the performance of Kriging is not as good as Gaussian RBF,especially in the case of high dimensional problems.
arxiv-9300-226 | Deep Learning using Linear Support Vector Machines | http://arxiv.org/pdf/1306.0239v4.pdf | author:Yichuan Tang category:cs.LG stat.ML published:2013-06-02 summary:Recently, fully-connected and convolutional neural networks have been trainedto achieve state-of-the-art performance on a wide variety of tasks such asspeech recognition, image classification, natural language processing, andbioinformatics. For classification tasks, most of these "deep learning" modelsemploy the softmax activation function for prediction and minimizecross-entropy loss. In this paper, we demonstrate a small but consistentadvantage of replacing the softmax layer with a linear support vector machine.Learning minimizes a margin-based loss instead of the cross-entropy loss. Whilethere have been various combinations of neural nets and SVMs in prior art, ourresults using L2-SVMs show that by simply replacing softmax with linear SVMsgives significant gains on popular deep learning datasets MNIST, CIFAR-10, andthe ICML 2013 Representation Learning Workshop's face expression recognitionchallenge.
arxiv-9300-227 | Reinforcement Learning in a Neurally Controlled Robot Using Dopamine Modulated STDP | http://arxiv.org/pdf/1502.06096v1.pdf | author:Richard Evans category:cs.NE cs.RO published:2015-02-21 summary:Recent work has shown that dopamine-modulated STDP can solve many of theissues associated with reinforcement learning, such as the distal rewardproblem. Spiking neural networks provide a useful technique in implementingreinforcement learning in an embodied context as they can deal with continuousparameter spaces and as such are better at generalizing the correct behaviourto perform in a given context. In this project we implement a version of DA-modulated STDP in an embodiedrobot on a food foraging task. Through simulated dopaminergic neurons we showhow the robot is able to learn a sequence of behaviours in order to achieve afood reward. In tests the robot was able to learn food-attraction behaviour,and subsequently unlearn this behaviour when the environment changed, in all 50trials. Moreover we show that the robot is able to operate in an environmentwhereby the optimal behaviour changes rapidly and so the agent must constantlyrelearn. In a more complex environment, consisting of food-containers, therobot was able to learn food-container attraction in 95% of trials, despite thelarge temporal distance between the correct behaviour and the reward. This isachieved by shifting the dopamine response from the primary stimulus (food) tothe secondary stimulus (food-container). Our work provides insights into the reasons behind some observed biologicalphenomena, such as the bursting behaviour observed in dopaminergic neurons. Aswell as demonstrating how spiking neural network controlled robots are able tosolve a range of reinforcement learning tasks.
arxiv-9300-228 | Study of a Robust Algorithm Applied in the Optimal Position Tuning for the Camera Lens in Automated Visual Inspection Systems | http://arxiv.org/pdf/1502.06081v1.pdf | author:Radu Arsinte category:cs.CV published:2015-02-21 summary:This paper present the mathematical fundaments and experimental study of analgorithm used to find the optimal position for the camera lens to obtain amaximum of details. This information can be further applied to a appropriatesystem to automatically correct this position. The algorithm is based on theevaluation of a so called resolution function who calculates the maximum ofgradient in a certain zone of the image. The paper also presents alternativeforms of the function, results of measurements and set up a set of practicalrules for the right application of the algorithm.
arxiv-9300-229 | Intra-and-Inter-Constraint-based Video Enhancement based on Piecewise Tone Mapping | http://arxiv.org/pdf/1502.06080v1.pdf | author:Yuanzhe Chen, Weiyao Lin, Chongyang Zhang, Zhenzhong Chen, Ning Xu, Jun Xie category:cs.CV cs.MM published:2015-02-21 summary:Video enhancement plays an important role in various video applications. Inthis paper, we propose a new intra-and-inter-constraint-based video enhancementapproach aiming to 1) achieve high intra-frame quality of the entire picturewhere multiple region-of-interests (ROIs) can be adaptively and simultaneouslyenhanced, and 2) guarantee the inter-frame quality consistencies among videoframes. We first analyze features from different ROIs and create a piecewisetone mapping curve for the entire frame such that the intra-frame quality of aframe can be enhanced. We further introduce new inter-frame constraints toimprove the temporal quality consistency. Experimental results show that theproposed algorithm obviously outperforms the state-of-the-art algorithms.
arxiv-9300-230 | A Heat-Map-based Algorithm for Recognizing Group Activities in Videos | http://arxiv.org/pdf/1502.06076v1.pdf | author:Weiyao Lin, Hang Chu, Jianxin Wu, Bin Sheng, Zhenzhong Chen category:cs.CV published:2015-02-21 summary:In this paper, a new heat-map-based (HMB) algorithm is proposed for groupactivity recognition. The proposed algorithm first models human trajectories asseries of "heat sources" and then applies a thermal diffusion process to createa heat map (HM) for representing the group activities. Based on this heat map,a new key-point based (KPB) method is used for handling the alignments amongheat maps with different scales and rotations. And a surface-fitting (SF)method is also proposed for recognizing group activities. Our proposed HMfeature can efficiently embed the temporal motion information of the groupactivities while the proposed KPB and SF methods can effectively utilize thecharacteristics of the heat map for activity recognition. Experimental resultsdemonstrate the effectiveness of our proposed algorithms.
arxiv-9300-231 | A new network-based algorithm for human activity recognition in video | http://arxiv.org/pdf/1502.06075v1.pdf | author:Weiyao Lin, Yuanzhe Chen, Jianxin Wu, Hanli Wang, Bin Sheng, Hongxiang Li category:cs.CV published:2015-02-21 summary:In this paper, a new network-transmission-based (NTB) algorithm is proposedfor human activity recognition in videos. The proposed NTB algorithm models theentire scene as an error-free network. In this network, each node correspondsto a patch of the scene and each edge represents the activity correlationbetween the corresponding patches. Based on this network, we further modelpeople in the scene as packages while human activities can be modeled as theprocess of package transmission in the network. By analyzing these specific"package transmission" processes, various activities can be effectivelydetected. The implementation of our NTB algorithm into abnormal activitydetection and group activity recognition are described in detail in the paper.Experimental results demonstrate the effectiveness of our proposed algorithm.
arxiv-9300-232 | MILJS : Brand New JavaScript Libraries for Matrix Calculation and Machine Learning | http://arxiv.org/pdf/1502.06064v1.pdf | author:Ken Miura, Tetsuaki Mano, Atsushi Kanehira, Yuichiro Tsuchiya, Tatsuya Harada category:stat.ML cs.LG cs.MS published:2015-02-21 summary:MILJS is a collection of state-of-the-art, platform-independent, scalable,fast JavaScript libraries for matrix calculation and machine learning. Our corelibrary offering a matrix calculation is called Sushi, which exhibits farbetter performance than any other leading machine learning libraries written inJavaScript. Especially, our matrix multiplication is 177 times faster than thefastest JavaScript benchmark. Based on Sushi, a machine learning library calledTempura is provided, which supports various algorithms widely used in machinelearning research. We also provide Soba as a visualization library. Theimplementations of our libraries are clearly written, properly documented andthus can are easy to get started with, as long as there is a web browser. Theselibraries are available from http://mil-tokyo.github.io/ under the MIT license.
arxiv-9300-233 | Adaptation Algorithm and Theory Based on Generalized Discrepancy | http://arxiv.org/pdf/1405.1503v3.pdf | author:Corinna Cortes, Mehryar Mohri, Andres Muñoz Medina category:cs.LG published:2014-05-07 summary:We present a new algorithm for domain adaptation improving upon a discrepancyminimization algorithm previously shown to outperform a number of algorithmsfor this task. Unlike many previous algorithms for domain adaptation, ouralgorithm does not consist of a fixed reweighting of the losses over thetraining sample. We show that our algorithm benefits from a solid theoreticalfoundation and more favorable learning bounds than discrepancy minimization. Wepresent a detailed description of our algorithm and give several efficientsolutions for solving its optimization problem. We also report the results ofseveral experiments showing that it outperforms discrepancy minimization.
arxiv-9300-234 | Microsoft COCO: Common Objects in Context | http://arxiv.org/pdf/1405.0312v3.pdf | author:Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Dollár category:cs.CV published:2014-05-01 summary:We present a new dataset with the goal of advancing the state-of-the-art inobject recognition by placing the question of object recognition in the contextof the broader question of scene understanding. This is achieved by gatheringimages of complex everyday scenes containing common objects in their naturalcontext. Objects are labeled using per-instance segmentations to aid in preciseobject localization. Our dataset contains photos of 91 objects types that wouldbe easily recognizable by a 4 year old. With a total of 2.5 million labeledinstances in 328k images, the creation of our dataset drew upon extensive crowdworker involvement via novel user interfaces for category detection, instancespotting and instance segmentation. We present a detailed statistical analysisof the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we providebaseline performance analysis for bounding box and segmentation detectionresults using a Deformable Parts Model.
arxiv-9300-235 | Interactive Fingerprinting Codes and the Hardness of Preventing False Discovery | http://arxiv.org/pdf/1410.1228v2.pdf | author:Thomas Steinke, Jonathan Ullman category:cs.CR cs.DS cs.LG published:2014-10-05 summary:We show an essentially tight bound on the number of adaptively chosenstatistical queries that a computationally efficient algorithm can answeraccurately given $n$ samples from an unknown distribution. A statistical queryasks for the expectation of a predicate over the underlying distribution, andan answer to a statistical query is accurate if it is "close" to the correctexpectation over the distribution. This question was recently studied by Dworket al., who showed how to answer $\tilde{\Omega}(n^2)$ queries efficiently, andalso by Hardt and Ullman, who showed that answering $\tilde{O}(n^3)$ queries ishard. We close the gap between the two bounds and show that, under a standardhardness assumption, there is no computationally efficient algorithm that,given $n$ samples from an unknown distribution, can give valid answers to$O(n^2)$ adaptively chosen statistical queries. An implication of our resultsis that computationally efficient algorithms for answering arbitrary,adaptively chosen statistical queries may as well be differentially private. We obtain our results using a new connection between the problem of answeringadaptively chosen statistical queries and a combinatorial object called aninteractive fingerprinting code. In order to optimize our hardness result, wegive a new Fourier-analytic approach to analyzing fingerprinting codes that issimpler, more flexible, and yields better parameters than previousconstructions.
arxiv-9300-236 | Web Similarity | http://arxiv.org/pdf/1502.05957v1.pdf | author:Andrew R. Cohen, Paul M. B. Vitanyi category:cs.IR cs.CL cs.CV published:2015-02-20 summary:Normalized web distance (NWD) is a similarity or normalized semantic distancebased on the World Wide Web or any other large electronic database, forinstance Wikipedia, and a search engine that returns reliable aggregate pagecounts. For sets of search terms the NWD gives a similarity on a scale from 0(identical) to 1 (completely different). The NWD approximates the similarityaccording to all (upper semi)computable properties. We develop the theory andgive applications. The derivation of the NWD method is based on Kolmogorovcomplexity.
arxiv-9300-237 | Refining Adverse Drug Reactions using Association Rule Mining for Electronic Healthcare Data | http://arxiv.org/pdf/1502.05943v1.pdf | author:Jenna M. Reps, Uwe Aickelin, Jiangang Ma, Yanchun Zhang category:cs.DB cs.CE cs.LG published:2015-02-20 summary:Side effects of prescribed medications are a common occurrence. Electronichealthcare databases present the opportunity to identify new side effectsefficiently but currently the methods are limited due to confounding (i.e. whenan association between two variables is identified due to them both beingassociated to a third variable). In this paper we propose a proof of concept method that learns commonassociations and uses this knowledge to automatically refine side effectsignals (i.e. exposure-outcome associations) by removing instances of theexposure-outcome associations that are caused by confounding. This leaves thesignal instances that are most likely to correspond to true side effectoccurrences. We then calculate a novel measure termed the confounding-adjustedrisk value, a more accurate absolute risk value of a patient experiencing theoutcome within 60 days of the exposure. Tentative results suggest that the method works. For the four signals (i.e.exposure-outcome associations) investigated we are able to correctly filter themajority of exposure-outcome instances that were unlikely to correspond to trueside effects. The method is likely to improve when tuning the association rulemining parameters for specific health outcomes. This paper shows that it may be possible to filter signals at a patient levelbased on association rules learned from considering patients' medicalhistories. However, additional work is required to develop a way to automatethe tuning of the method's parameters.
arxiv-9300-238 | Achieving All with No Parameters: Adaptive NormalHedge | http://arxiv.org/pdf/1502.05934v1.pdf | author:Haipeng Luo, Robert E. Schapire category:cs.LG published:2015-02-20 summary:We study the classic online learning problem of predicting with expertadvice, and propose a truly parameter-free and adaptive algorithm that achievesseveral objectives simultaneously without using any prior information. The maincomponent of this work is an improved version of the NormalHedge.DT algorithm(Luo and Schapire, 2014), called AdaNormalHedge. On one hand, this newalgorithm ensures small regret when the competitor has small loss and almostconstant regret when the losses are stochastic. On the other hand, thealgorithm is able to compete with any convex combination of the expertssimultaneously, with a regret in terms of the relative entropy of the prior andthe competitor. This resolves an open problem proposed by Chaudhuri et al.(2009) and Chernov and Vovk (2010). Moreover, we extend the results to thesleeping expert setting and provide two applications to illustrate the power ofAdaNormalHedge: 1) competing with time-varying unknown competitors and 2)predicting almost as well as the best pruning tree. Our results on theseapplications significantly improve previous work from different aspects, and aspecial case of the first application resolves another open problem proposed byWarmuth and Koolen (2014) on whether one can simultaneously achieve optimalshifting regret for both adversarial and stochastic losses.
arxiv-9300-239 | Supervised Dictionary Learning and Sparse Representation-A Review | http://arxiv.org/pdf/1502.05928v1.pdf | author:Mehrdad J. Gangeh, Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel category:cs.CV published:2015-02-20 summary:Dictionary learning and sparse representation (DLSR) is a recent andsuccessful mathematical model for data representation that achievesstate-of-the-art performance in various fields such as pattern recognition,machine learning, computer vision, and medical imaging. The originalformulation for DLSR is based on the minimization of the reconstruction errorbetween the original signal and its sparse representation in the space of thelearned dictionary. Although this formulation is optimal for solving problemssuch as denoising, inpainting, and coding, it may not lead to optimal solutionin classification tasks, where the ultimate goal is to make the learneddictionary and corresponding sparse representation as discriminative aspossible. This motivated the emergence of a new category of techniques, whichis appropriately called supervised dictionary learning and sparserepresentation (S-DLSR), leading to more optimal dictionary and sparserepresentation in classification tasks. Despite many research efforts forS-DLSR, the literature lacks a comprehensive view of these techniques, theirconnections, advantages and shortcomings. In this paper, we address this gapand provide a review of the recently proposed algorithms for S-DLSR. We firstpresent a taxonomy of these algorithms into six categories based on theapproach taken to include label information into the learning of the dictionaryand/or sparse representation. For each category, we draw connections betweenthe algorithms in this category and present a unified framework for them. Wethen provide guidelines for applied researchers on how to represent and learnthe building blocks of an S-DLSR solution based on the problem at hand. Thisreview provides a broad, yet deep, view of the state-of-the-art methods forS-DLSR and allows for the advancement of research and development in thisemerging area of research.
arxiv-9300-240 | Feature-Budgeted Random Forest | http://arxiv.org/pdf/1502.05925v1.pdf | author:Feng Nan, Joseph Wang, Venkatesh Saligrama category:stat.ML cs.LG published:2015-02-20 summary:We seek decision rules for prediction-time cost reduction, where completedata is available for training, but during prediction-time, each feature canonly be acquired for an additional cost. We propose a novel random forestalgorithm to minimize prediction error for a user-specified {\it average}feature acquisition budget. While random forests yield strong generalizationperformance, they do not explicitly account for feature costs and furthermorerequire low correlation among trees, which amplifies costs. Our random forestgrows trees with low acquisition cost and high strength based on greedy minimaxcost-weighted-impurity splits. Theoretically, we establish near-optimalacquisition cost guarantees for our algorithm. Empirically, on a number ofbenchmark datasets we demonstrate superior accuracy-cost curves againststate-of-the-art prediction-time algorithms.
arxiv-9300-241 | Building pattern recognition applications with the SPARE library | http://arxiv.org/pdf/1410.5263v2.pdf | author:Lorenzo Livi, Guido Del Vescovo, Antonello Rizzi, Fabio Massimo Frattale Mascioli category:cs.CV cs.MS D.2.2 published:2014-10-20 summary:This paper presents the SPARE C++ library, an open source software toolconceived to build pattern recognition and soft computing systems. The libraryfollows the requirement of the generality: most of the implemented algorithmsare able to process user-defined input data types transparently, such aslabeled graphs and sequences of objects, as well as standard numeric vectors.Here we present a high-level picture of the SPARE library characteristics,focusing instead on the specific practical possibility of constructing patternrecognition systems for different input data types. In particular, as a proofof concept, we discuss two application instances involving clustering ofreal-valued multidimensional sequences and classification of labeled graphs.
arxiv-9300-242 | A Data Mining framework to model Consumer Indebtedness with Psychological Factors | http://arxiv.org/pdf/1502.05911v1.pdf | author:Alexandros Ladas, Eamonn Ferguson, Uwe Aickelin, Jon Garibaldi category:cs.LG cs.CE published:2015-02-20 summary:Modelling Consumer Indebtedness has proven to be a problem of complex nature.In this work we utilise Data Mining techniques and methods to explore themultifaceted aspect of Consumer Indebtedness by examining the contribution ofPsychological Factors, like Impulsivity to the analysis of Consumer Debt. Ourresults confirm the beneficial impact of Psychological Factors in modellingConsumer Indebtedness and suggest a new approach in analysing Consumer Debt,that would take into consideration more Psychological characteristics ofconsumers and adopt techniques and practices from Data Mining.
arxiv-9300-243 | On predictability of rare events leveraging social media: a machine learning perspective | http://arxiv.org/pdf/1502.05886v1.pdf | author:Lei Le, Emilio Ferrara, Alessandro Flammini category:cs.SI cs.LG physics.soc-ph published:2015-02-20 summary:Information extracted from social media streams has been leveraged toforecast the outcome of a large number of real-world events, from politicalelections to stock market fluctuations. An increasing amount of studiesdemonstrates how the analysis of social media conversations provides cheapaccess to the wisdom of the crowd. However, extents and contexts in which suchforecasting power can be effectively leveraged are still unverified at least ina systematic way. It is also unclear how social-media-based predictions compareto those based on alternative information sources. To address these issues,here we develop a machine learning framework that leverages social mediastreams to automatically identify and predict the outcomes of soccer matches.We focus in particular on matches in which at least one of the possibleoutcomes is deemed as highly unlikely by professional bookmakers. We argue thatsport events offer a systematic approach for testing the predictive power ofsocial media, and allow to compare such power against the rigorous baselinesset by external sources. Despite such strict baselines, our framework yieldsabove 8% marginal profit when used to inform simple betting strategies. Thesystem is based on real-time sentiment analysis and exploits data collectedimmediately before the games, allowing for informed bets. We discuss therationale behind our approach, describe the learning framework, its predictionperformance and the return it provides as compared to a set of bettingstrategies. To test our framework we use both historical Twitter data from the2014 FIFA World Cup games, and real-time Twitter data collected by monitoringthe conversations about all soccer matches of four major European tournaments(FA Premier League, Serie A, La Liga, and Bundesliga), and the 2014 UEFAChampions League, during the period between Oct. 25th 2014 and Nov. 26th 2014.
arxiv-9300-244 | NP-Hardness and Inapproximability of Sparse PCA | http://arxiv.org/pdf/1502.05675v2.pdf | author:Malik Magdon-Ismail category:cs.LG cs.CC cs.DS math.CO stat.ML published:2015-02-19 summary:We give a reduction from {\sc clique} to establish that sparse PCA isNP-hard. The reduction has a gap which we use to exclude an FPTAS for sparsePCA (unless P=NP). Under weaker complexity assumptions, we also excludepolynomial constant-factor approximation algorithms.
arxiv-9300-245 | A General Multi-Graph Matching Approach via Graduated Consistency-regularized Boosting | http://arxiv.org/pdf/1502.05840v1.pdf | author:Junchi Yan, Minsu Cho, Hongyuan Zha, Xiaokang Yang, Stephen Chu category:cs.CV published:2015-02-20 summary:This paper addresses the problem of matching $N$ weighted graphs referring toan identical object or category. More specifically, matching the common nodecorrespondences among graphs. This multi-graph matching problem involves twoingredients affecting the overall accuracy: i) the local pairwise matchingaffinity score among graphs; ii) the global matching consistency that measuresthe uniqueness of the pairwise matching results by different chaining orders.Previous studies typically either enforce the matching consistency constraintsin the beginning of iterative optimization, which may propagate matching errorboth over iterations and across graph pairs; or separate affinity optimizingand consistency regularization in two steps. This paper is motivated by theobservation that matching consistency can serve as a regularizer in theaffinity objective function when the function is biased due to noises orinappropriate modeling. We propose multi-graph matching methods to incorporatethe two aspects by boosting the affinity score, meanwhile gradually infusingthe consistency as a regularizer. Furthermore, we propose a node-wiseconsistency/affinity-driven mechanism to elicit the common inlier nodes out ofthe irrelevant outliers. Extensive results on both synthetic and public imagedatasets demonstrate the competency of the proposed algorithms.
arxiv-9300-246 | A provably convergent alternating minimization method for mean field inference | http://arxiv.org/pdf/1502.05832v1.pdf | author:Pierre Baqué, Jean-Hubert Hours, François Fleuret, Pascal Fua category:cs.LG math.OC published:2015-02-20 summary:Mean-Field is an efficient way to approximate a posterior distribution incomplex graphical models and constitutes the most popular class of Bayesianvariational approximation methods. In most applications, the mean fielddistribution parameters are computed using an alternate coordinateminimization. However, the convergence properties of this algorithm remainunclear. In this paper, we show how, by adding an appropriate penalizationterm, we can guarantee convergence to a critical point, while keeping a closedform update at each step. A convergence rate estimate can also be derived basedon recent results in non-convex optimization.
arxiv-9300-247 | Spike Event Based Learning in Neural Networks | http://arxiv.org/pdf/1502.05777v1.pdf | author:James A. Henderson, TingTing A. Gibson, Janet Wiles category:cs.NE cs.LG published:2015-02-20 summary:A scheme is derived for learning connectivity in spiking neural networks. Thescheme learns instantaneous firing rates that are conditional on the activityin other parts of the network. The scheme is independent of the choice ofneuron dynamics or activation function, and network architecture. It involvestwo simple, online, local learning rules that are applied only in response tooccurrences of spike events. This scheme provides a direct method fortransferring ideas between the fields of deep learning and computationalneuroscience. This learning scheme is demonstrated using a layered feedforwardspiking neural network trained self-supervised on a prediction andclassification task for moving MNIST images collected using a Dynamic VisionSensor.
arxiv-9300-248 | Pairwise Constraint Propagation: A Survey | http://arxiv.org/pdf/1502.05752v1.pdf | author:Zhenyong Fu, Zhiwu Lu category:cs.CV cs.LG stat.ML published:2015-02-19 summary:As one of the most important types of (weaker) supervised information inmachine learning and pattern recognition, pairwise constraint, which specifieswhether a pair of data points occur together, has recently received significantattention, especially the problem of pairwise constraint propagation. At leasttwo reasons account for this trend: the first is that compared to the datalabel, pairwise constraints are more general and easily to collect, and thesecond is that since the available pairwise constraints are usually limited,the constraint propagation problem is thus important. This paper provides an up-to-date critical survey of pairwise constraintpropagation research. There are two underlying motivations for us to write thissurvey paper: the first is to provide an up-to-date review of the existingliterature, and the second is to offer some insights into the studies ofpairwise constraint propagation. To provide a comprehensive survey, we not onlycategorize existing propagation techniques but also present detaileddescriptions of representative methods within each category.
arxiv-9300-249 | A New Sampling Technique for Tensors | http://arxiv.org/pdf/1502.05023v2.pdf | author:Srinadh Bhojanapalli, Sujay Sanghavi category:stat.ML cs.DS cs.IT cs.LG math.IT published:2015-02-17 summary:In this paper we propose new techniques to sample arbitrary third-ordertensors, with an objective of speeding up tensor algorithms that have recentlygained popularity in machine learning. Our main contribution is a new way toselect, in a biased random way, only $O(n^{1.5}/\epsilon^2)$ of the possible$n^3$ elements while still achieving each of the three goals: \\ {\em (a)tensor sparsification}: for a tensor that has to be formed from arbitrarysamples, compute very few elements to get a good spectral approximation, andfor arbitrary orthogonal tensors {\em (b) tensor completion:} recover anexactly low-rank tensor from a small number of samples via alternating leastsquares, or {\em (c) tensor factorization:} approximating factors of a low-ranktensor corrupted by noise. \\ Our sampling can be used along with existingtensor-based algorithms to speed them up, removing the computational bottleneckin these methods.
arxiv-9300-250 | Gray-Level Image Transitions Driven by Tsallis Entropic Index | http://arxiv.org/pdf/1502.04204v2.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2015-02-14 summary:The maximum entropy principle is largely used in thresholding andsegmentation of images. Among the several formulations of this principle, themost effectively applied is that based on Tsallis non-extensive entropy. Here,we discuss the role of its entropic index in determining the thresholds. Whenthis index is spanning the interval (0,1), for some images, the values ofthresholds can have large leaps. In this manner, we observe abrupt transitionsin the appearance of corresponding bi-level or multi-level images. Thesegray-level image transitions are analogous to order or texture transitionsobserved in physical systems, transitions which are driven by the temperatureor by other physical quantities.
arxiv-9300-251 | Learning to Execute | http://arxiv.org/pdf/1410.4615v3.pdf | author:Wojciech Zaremba, Ilya Sutskever category:cs.NE cs.AI cs.LG published:2014-10-17 summary:Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) arewidely used because they are expressive and are easy to train. Our interestlies in empirically evaluating the expressiveness and the learnability of LSTMsin the sequence-to-sequence regime by training them to evaluate short computerprograms, a domain that has traditionally been seen as too complex for neuralnetworks. We consider a simple class of programs that can be evaluated with asingle left-to-right pass using constant memory. Our main result is that LSTMscan learn to map the character-level representations of such programs to theircorrect outputs. Notably, it was necessary to use curriculum learning, andwhile conventional curriculum learning proved ineffective, we developed a newvariant of curriculum learning that improved our networks' performance in allexperimental conditions. The improved curriculum had a dramatic impact on anaddition problem, making it possible to train an LSTM to add two 9-digitnumbers with 99% accuracy.
arxiv-9300-252 | Recurrent Neural Network Regularization | http://arxiv.org/pdf/1409.2329v5.pdf | author:Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals category:cs.NE published:2014-09-08 summary:We present a simple regularization technique for Recurrent Neural Networks(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successfultechnique for regularizing neural networks, does not work well with RNNs andLSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and showthat it substantially reduces overfitting on a variety of tasks. These tasksinclude language modeling, speech recognition, image caption generation, andmachine translation.
arxiv-9300-253 | Finding Dantzig selectors with a proximity operator based fixed-point algorithm | http://arxiv.org/pdf/1502.05571v1.pdf | author:Ashley Prater, Lixin Shen, Bruce W. Suter category:math.NA stat.ML published:2015-02-19 summary:In this paper, we study a simple iterative method for finding the Dantzigselector, which was designed for linear regression problems. The methodconsists of two main stages. The first stage is to approximate the Dantzigselector through a fixed-point formulation of solutions to the Dantzig selectorproblem. The second stage is to construct a new estimator by regressing dataonto the support of the approximated Dantzig selector. We compare our method toan alternating direction method, and present the results of numericalsimulations using both the proposed method and the alternating direction methodon synthetic and real data sets. The numerical simulations demonstrate that thetwo methods produce results of similar quality, however the proposed methodtends to be significantly faster.
arxiv-9300-254 | Multi-valued Color Representation Based on Frank t-norm Properties | http://arxiv.org/pdf/1502.05565v1.pdf | author:Vasile Patrascu category:cs.CV published:2015-02-19 summary:In this paper two knowledge representation models are proposed, FP4 and FP6.Both combine ideas from fuzzy sets and four-valued and hexa-valued logics. Bothrepresent imprecise properties whose accomplished degree is unknown orcontradictory for some objects. A possible application in the color analysisand color image processing is discussed.
arxiv-9300-255 | Robust Active Ranking from Sparse Noisy Comparisons | http://arxiv.org/pdf/1502.05556v1.pdf | author:Lucas Maystre, Matthias Grossglauser category:stat.ML cs.LG published:2015-02-19 summary:From sporting events to sociological surveys, ranking from pairwisecomparisons is a tool of choice for many applications. When certain pairs ofitems are difficult to compare, outcomes can be noisy, and it is necessary todevelop robust strategies. In this work, we show how a simple active samplingscheme that uses a standard black box sorting algorithm enables the efficientrecovery of the ranking, achieving low error with sparse samples. Both intheory and practice, this active strategy performs systematically better thanselecting comparisons at random. As a detour, we show a link between RankCentrality, a recently proposed algorithm for rank aggregation, and the MLestimator for the Bradley-Terry model. This enables us to develop a new,provably convergent iterative algorithm for computing the ML estimate.
arxiv-9300-256 | NeuroSVM: A Graphical User Interface for Identification of Liver Patients | http://arxiv.org/pdf/1502.05534v1.pdf | author:Kalyan Nagaraj, Amulyashree Sridhar category:cs.LG cs.HC published:2015-02-19 summary:Diagnosis of liver infection at preliminary stage is important for bettertreatment. In todays scenario devices like sensors are used for detection ofinfections. Accurate classification techniques are required for automaticidentification of disease samples. In this context, this study utilizes datamining approaches for classification of liver patients from healthyindividuals. Four algorithms (Naive Bayes, Bagging, Random forest and SVM) wereimplemented for classification using R platform. Further to improve theaccuracy of classification a hybrid NeuroSVM model was developed using SVM andfeed-forward artificial neural network (ANN). The hybrid model was tested forits performance using statistical parameters like root mean square error (RMSE)and mean absolute percentage error (MAPE). The model resulted in a predictionaccuracy of 98.83%. The results suggested that development of hybrid modelimproved the accuracy of prediction. To serve the medicinal community forprediction of liver disease among patients, a graphical user interface (GUI)has been developed using R. The GUI is deployed as a package in localrepository of R platform for users to perform prediction.
arxiv-9300-257 | Classification and Bayesian Optimization for Likelihood-Free Inference | http://arxiv.org/pdf/1502.05503v1.pdf | author:Michael U. Gutmann, Jukka Corander, Ritabrata Dutta, Samuel Kaski category:stat.CO stat.ME stat.ML published:2015-02-19 summary:Some statistical models are specified via a data generating process for whichthe likelihood function cannot be computed in closed form. Standardlikelihood-based inference is then not feasible but the model parameters can beinferred by finding the values which yield simulated data that resemble theobserved data. This approach faces at least two major difficulties: The firstdifficulty is the choice of the discrepancy measure which is used to judgewhether the simulated data resemble the observed data. The second difficulty isthe computationally efficient identification of regions in the parameter spacewhere the discrepancy is low. We give here an introduction to our recent workwhere we tackle the two difficulties through classification and Bayesianoptimization.
arxiv-9300-258 | Screen Content Image Segmentation Using Least Absolute Deviation Fitting | http://arxiv.org/pdf/1501.03755v2.pdf | author:Shervin Minaee, Yao Wang category:cs.CV published:2015-01-15 summary:We propose an algorithm for separating the foreground (mainly text and linegraphics) from the smoothly varying background in screen content images. Theproposed method is designed based on the assumption that the background part ofthe image is smoothly varying and can be represented by a linear combination ofa few smoothly varying basis functions, while the foreground text and graphicscreate sharp discontinuity and cannot be modeled by this smooth representation.The algorithm separates the background and foreground using a least absolutedeviation method to fit the smooth model to the image pixels. This algorithmhas been tested on several images from HEVC standard test sequences for screencontent coding, and is shown to have superior performance over other popularmethods, such as k-means clustering based segmentation in DjVu and shapeprimitive extraction and coding (SPEC) algorithm. Such background/foregroundsegmentation are important pre-processing steps for text extraction andseparate coding of background and foreground for compression of screen contentimages.
arxiv-9300-259 | Visualizing Object Detection Features | http://arxiv.org/pdf/1502.05461v1.pdf | author:Carl Vondrick, Aditya Khosla, Hamed Pirsiavash, Tomasz Malisiewicz, Antonio Torralba category:cs.CV published:2015-02-19 summary:We introduce algorithms to visualize feature spaces used by object detectors.Our method works by inverting a visual feature back to multiple natural images.We found that these visualizations allow us to analyze object detection systemsin new ways and gain new insight into the detector's failures. For example,when we visualize the features for high scoring false alarms, we discoveredthat, although they are clearly wrong in image space, they do look deceptivelysimilar to true positives in feature space. This result suggests that many ofthese false alarms are caused by our choice of feature space, and supports thatcreating a better learning algorithm or building bigger datasets is unlikely tocorrect these errors. By visualizing feature spaces, we can gain a moreintuitive understanding of recognition systems.
arxiv-9300-260 | Rule-and Dictionary-based Solution for Variations in Written Arabic Names in Social Networks, Big Data, Accounting Systems and Large Databases | http://arxiv.org/pdf/1502.05441v1.pdf | author:Ahmad B. A. Hassanat, Ghada Awad Altarawneh category:cs.DB cs.CL cs.IR published:2015-02-18 summary:This paper investigates the problem that some Arabic names can be written inmultiple ways. When someone searches for only one form of a name, neither exactnor approximate matching is appropriate for returning the multiple variants ofthe name. Exact matching requires the user to enter all forms of the name forthe search, and approximate matching yields names not among the variations ofthe one being sought. In this paper, we attempt to solve the problem with adictionary of all Arabic names mapped to their different (alternative) writingforms. We generated alternatives based on rules we derived from reviewing thefirst names of 9.9 million citizens and former citizens of Jordan. Thisdictionary can be used for both standardizing the written form when inserting anew name into a database and for searching for the name and all its alternativewritten forms. Creating the dictionary automatically based on rules resulted inat least 7% erroneous acceptance errors and 7.9% erroneous rejection errors. Weaddressed the errors by manually editing the dictionary. The dictionary can beof help to real world-databases, with the qualification that manual editingdoes not guarantee 100% correctness.
arxiv-9300-261 | Fusion of Image Segmentation Algorithms using Consensus Clustering | http://arxiv.org/pdf/1502.05435v1.pdf | author:Mete Ozay, Fatos T. Yarman Vural, Sanjeev R. Kulkarni, H. Vincent Poor category:cs.CV published:2015-02-18 summary:A new segmentation fusion method is proposed that ensembles the output ofseveral segmentation algorithms applied on a remotely sensed image. Thecandidate segmentation sets are processed to achieve a consensus segmentationusing a stochastic optimization algorithm based on the Filtered Stochastic BOEM(Best One Element Move) method. For this purpose, Filtered Stochastic BOEM isreformulated as a segmentation fusion problem by designing a new distancelearning approach. The proposed algorithm also embeds the computation of theoptimum number of clusters into the segmentation fusion problem.
arxiv-9300-262 | On learning k-parities with and without noise | http://arxiv.org/pdf/1502.05375v1.pdf | author:Arnab Bhattacharyya, Ameet Gadekar, Ninad Rajgopal category:cs.DS cs.DM cs.LG published:2015-02-18 summary:We first consider the problem of learning $k$-parities in the on-linemistake-bound model: given a hidden vector $x \in \{0,1\}^n$ with $x=k$ and asequence of "questions" $a_1, a_2, ...\in \{0,1\}^n$, where the algorithm mustreply to each question with $< a_i, x> \pmod 2$, what is the best tradeoffbetween the number of mistakes made by the algorithm and its time complexity?We improve the previous best result of Buhrman et al. by an $\exp(k)$ factor inthe time complexity. Second, we consider the problem of learning $k$-parities in the presence ofclassification noise of rate $\eta \in (0,1/2)$. A polynomial time algorithmfor this problem (when $\eta > 0$ and $k = \omega(1)$) is a longstandingchallenge in learning theory. Grigorescu et al. showed an algorithm running intime ${n \choose k/2}^{1 + 4\eta^2 +o(1)}$. Note that this algorithm inherentlyrequires time ${n \choose k/2}$ even when the noise rate $\eta$ is polynomiallysmall. We observe that for sufficiently small noise rate, it is possible tobreak the $n \choose k/2$ barrier. In particular, if for some function $f(n) =\omega(1)$ and $\alpha \in [1/2, 1)$, $k = n/f(n)$ and $\eta = o(f(n)^{-\alpha}/\log n)$, then there is an algorithm for the problem with running time$poly(n)\cdot {n \choose k}^{1-\alpha} \cdot e^{-k/4.01}$.
arxiv-9300-263 | A Clustering Analysis of Tweet Length and its Relation to Sentiment | http://arxiv.org/pdf/1406.3287v3.pdf | author:Matthew Mayo category:cs.CL cs.IR cs.SI published:2014-06-12 summary:Sentiment analysis of Twitter data is performed. The researcher has made thefollowing contributions via this paper: (1) an innovative method for derivingsentiment score dictionaries using an existing sentiment dictionary as seedwords is explored, and (2) an analysis of clustered tweet sentiment scoresbased on tweet length is performed.
arxiv-9300-264 | NEFI: Network Extraction From Images | http://arxiv.org/pdf/1502.05241v1.pdf | author:Michael Dirnberger, Adrian Neumann, Tim Kehl category:cs.CV cs.SE published:2015-02-18 summary:Networks and network-like structures are amongst the central building blocksof many technological and biological systems. Given a mathematical graphrepresentation of a network, methods from graph theory enable a preciseinvestigation of its properties. Software for the analysis of graphs is widelyavailable and has been applied to graphs describing large scale networks suchas social networks, protein-interaction networks, etc. In these applications,graph acquisition, i.e., the extraction of a mathematical graph from a network,is relatively simple. However, for many network-like structures, e.g. leafvenations, slime molds and mud cracks, data collection relies on images wheregraph extraction requires domain-specific solutions or even manual. Here weintroduce Network Extraction From Images, NEFI, a software tool thatautomatically extracts accurate graphs from images of a wide range of networksoriginating in various domains. While there is previous work on graphextraction from images, theoretical results are fully accessible only to anexpert audience and ready-to-use implementations for non-experts are rarelyavailable or insufficiently documented. NEFI provides a novel platform allowingpractitioners from many disciplines to easily extract graph representationsfrom images by supplying flexible tools from image processing, computer visionand graph theory bundled in a convenient package. Thus, NEFI constitutes ascalable alternative to tedious and error-prone manual graph extraction andspecial purpose tools. We anticipate NEFI to enable the collection of largerdatasets by reducing the time spent on graph extraction. The analysis of thesenew datasets may open up the possibility to gain new insights into thestructure and function of various types of networks. NEFI is open source andavailable http://nefi.mpi-inf.mpg.de.
arxiv-9300-265 | F0 Modeling In Hmm-Based Speech Synthesis System Using Deep Belief Network | http://arxiv.org/pdf/1502.05213v1.pdf | author:Sankar Mukherjee, Shyamal Kumar Das Mandal category:cs.LG cs.NE published:2015-02-18 summary:In recent years multilayer perceptrons (MLPs) with many hid- den layers DeepNeural Network (DNN) has performed sur- prisingly well in many speech tasks,i.e. speech recognition, speaker verification, speech synthesis etc. Althoughin the context of F0 modeling these techniques has not been ex- ploitedproperly. In this paper, Deep Belief Network (DBN), a class of DNN family hasbeen employed and applied to model the F0 contour of synthesized speech whichwas generated by HMM-based speech synthesis system. The experiment was done onBengali language. Several DBN-DNN architectures ranging from four to sevenhidden layers and up to 200 hid- den units per hidden layer was presented andevaluated. The results were compared against clustering tree techniques pop-ularly found in statistical parametric speech synthesis. We show that fromtextual inputs DBN-DNN learns a high level structure which in turn improves F0contour in terms of ob- jective and subjective tests.
arxiv-9300-266 | IAT - Image Annotation Tool: Manual | http://arxiv.org/pdf/1502.05212v1.pdf | author:Gianluigi Ciocca, Paolo Napoletano, Raimondo Schettini category:cs.CV published:2015-02-18 summary:The annotation of image and video data of large datasets is a fundamentaltask in multimedia information retrieval and computer vision applications. Inorder to support the users during the image and video annotation process,several software tools have been developed to provide them with a graphicalenvironment which helps drawing object contours, handling tracking informationand specifying object metadata. Here we introduce a preliminary version of theimage annotation tools developed at the Imaging and Vision Laboratory.
arxiv-9300-267 | A Stochastic Quasi-Newton Method for Large-Scale Optimization | http://arxiv.org/pdf/1401.7020v2.pdf | author:R. H. Byrd, S. L. Hansen, J. Nocedal, Y. Singer category:math.OC cs.LG stat.ML published:2014-01-27 summary:The question of how to incorporate curvature information in stochasticapproximation methods is challenging. The direct application of classicalquasi- Newton updating techniques for deterministic optimization leads to noisycurvature estimates that have harmful effects on the robustness of theiteration. In this paper, we propose a stochastic quasi-Newton method that isefficient, robust and scalable. It employs the classical BFGS update formula inits limited memory form, and is based on the observation that it is beneficialto collect curvature information pointwise, and at regular intervals, through(sub-sampled) Hessian-vector products. This technique differs from theclassical approach that would compute differences of gradients, and wherecontrolling the quality of the curvature estimates can be difficult. We presentnumerical results on problems arising in machine learning that suggest that theproposed method shows much promise.
arxiv-9300-268 | Dengue disease prediction using weka data mining tool | http://arxiv.org/pdf/1502.05167v1.pdf | author:Kashish Ara Shakil, Shadma Anis, Mansaf Alam category:cs.CY cs.LG published:2015-02-18 summary:Dengue is a life threatening disease prevalent in several developed as wellas developing countries like India.In this paper we discuss various algorithmapproaches of data mining that have been utilized for dengue diseaseprediction. Data mining is a well known technique used by health organizationsfor classification of diseases such as dengue, diabetes and cancer inbioinformatics research. In the proposed approach we have used WEKA with 10cross validation to evaluate data and compare results. Weka has an extensivecollection of different machine learning and data mining algorithms. In thispaper we have firstly classified the dengue data set and then compared thedifferent data mining techniques in weka through Explorer, knowledge flow andExperimenter interfaces. Furthermore in order to validate our approach we haveused a dengue dataset with 108 instances but weka used 99 rows and 18attributes to determine the prediction of disease and their accuracy usingclassifications of different algorithms to find out the best performance. Themain objective of this paper is to classify data and assist the users inextracting useful information from data and easily identify a suitablealgorithm for accurate predictive model from it. From the findings of thispaper it can be concluded that Na\"ive Bayes and J48 are the best performancealgorithms for classified accuracy because they achieved maximum accuracy= 100%with 99 correctly classified instances, maximum ROC = 1, had least meanabsolute error and it took minimum time for building this model throughExplorer and Knowledge flow results
arxiv-9300-269 | Proper Complex Gaussian Processes for Regression | http://arxiv.org/pdf/1502.04868v2.pdf | author:Rafael Boloix-Tortosa, F. Javier Payán-Somet, Eva Arias-de-Reyna, Juan José Murillo-Fuentes category:cs.LG stat.ML published:2015-02-17 summary:Complex-valued signals are used in the modeling of many systems inengineering and science, hence being of fundamental interest. Often, randomcomplex-valued signals are considered to be proper. A proper complex randomvariable or process is uncorrelated with its complex conjugate. This assumptionis a good model of the underlying physics in many problems, and simplifies thecomputations. While linear processing and neural networks have been widelystudied for these signals, the development of complex-valued nonlinear kernelapproaches remains an open problem. In this paper we propose Gaussian processesfor regression as a framework to develop 1) a solution for propercomplex-valued kernel regression and 2) the design of the reproducing kernelfor complex-valued inputs, using the convolutional approach forcross-covariances. In this design we pay attention to preserve, in the complexdomain, the measure of similarity between near inputs. The hyperparameters ofthe kernel are learned maximizing the marginal likelihood using Wirtingerderivatives. Besides, the approach is connected to the multiple output learningscenario. In the experiments included, we first solve a proper complex Gaussianprocess where the cross-covariance does not cancel, a challenging scenario whendealing with proper complex signals. Then we successfully use these novelresults to solve some problems previously proposed in the literature asbenchmarks, reporting a remarkable improvement in the estimation error.
arxiv-9300-270 | Phrase Based Language Model for Statistical Machine Translation: Empirical Study | http://arxiv.org/pdf/1501.05203v3.pdf | author:Geliang Chen category:cs.CL published:2015-01-21 summary:Reordering is a challenge to machine translation (MT) systems. In MT, thewidely used approach is to apply word based language model (LM) which considersthe constituent units of a sentence as words. In speech recognition (SR), somephrase based LM have been proposed. However, those LMs are not necessarilysuitable or optimal for reordering. We propose two phrase based LMs whichconsiders the constituent units of a sentence as phrases. Experiments show thatour phrase based LMs outperform the word based LM with the respect ofperplexity and n-best list re-ranking.
arxiv-9300-271 | Temporal Embedding in Convolutional Neural Networks for Robust Learning of Abstract Snippets | http://arxiv.org/pdf/1502.05113v1.pdf | author:Jiajun Liu, Kun Zhao, Brano Kusy, Ji-rong Wen, Raja Jurdak category:cs.LG cs.NE published:2015-02-18 summary:The prediction of periodical time-series remains challenging due to varioustypes of data distortions and misalignments. Here, we propose a novel modelcalled Temporal embedding-enhanced convolutional neural Network (TeNet) tolearn repeatedly-occurring-yet-hidden structural elements in periodicaltime-series, called abstract snippets, for predicting future changes. Our modeluses convolutional neural networks and embeds a time-series with its potentialneighbors in the temporal domain for aligning it to the dominant patterns inthe dataset. The model is robust to distortions and misalignments in thetemporal domain and demonstrates strong prediction power for periodicaltime-series. We conduct extensive experiments and discover that the proposed model showssignificant and consistent advantages over existing methods on a variety ofdata modalities ranging from human mobility to household power consumptionrecords. Empirical results indicate that the model is robust to various factorssuch as number of samples, variance of data, numerical ranges of data etc. Theexperiments also verify that the intuition behind the model can be generalizedto multiple data types and applications and promises significant improvement inprediction performances across the datasets studied.
arxiv-9300-272 | CSAL: Self-adaptive Labeling based Clustering Integrating Supervised Learning on Unlabeled Data | http://arxiv.org/pdf/1502.05111v1.pdf | author:Fangfang Li, Guandong Xu, Longbing Cao category:cs.LG published:2015-02-18 summary:Supervised classification approaches can predict labels for unknown databecause of the supervised training process. The success of classification isheavily dependent on the labeled training data. Differently, clustering iseffective in revealing the aggregation property of unlabeled data, but theperformance of most clustering methods is limited by the absence of labeleddata. In real applications, however, it is time-consuming and sometimesimpossible to obtain labeled data. The combination of clustering andclassification is a promising and active approach which can largely improve theperformance. In this paper, we propose an innovative and effective clusteringframework based on self-adaptive labeling (CSAL) which integrates clusteringand classification on unlabeled data. Clustering is first employed to partitiondata and a certain proportion of clustered data are selected by our proposedlabeling approach for training classifiers. In order to refine the trainedclassifiers, an iterative process of Expectation-Maximization algorithm isdevised into the proposed clustering framework CSAL. Experiments are conductedon publicly data sets to test different combinations of clustering algorithmsand classification models as well as various training data labeling methods.The experimental results show that our approach along with the self-adaptivemethod outperforms other methods.
arxiv-9300-273 | Real time clustering of time series using triangular potentials | http://arxiv.org/pdf/1502.05090v1.pdf | author:Aldo Pacchiano, Oliver Williams category:cs.LG published:2015-02-18 summary:Motivated by the problem of computing investment portfolio weightings weinvestigate various methods of clustering as alternatives to traditionalmean-variance approaches. Such methods can have significant benefits from apractical point of view since they remove the need to invert a samplecovariance matrix, which can suffer from estimation error and will almostcertainly be non-stationary. The general idea is to find groups of assets whichshare similar return characteristics over time and treat each group as a singlecomposite asset. We then apply inverse volatility weightings to these newcomposite assets. In the course of our investigation we devise a method ofclustering based on triangular potentials and we present associated theoreticalresults as well as various examples based on synthetic data.
arxiv-9300-274 | Long-term Recurrent Convolutional Networks for Visual Recognition and Description | http://arxiv.org/pdf/1411.4389v3.pdf | author:Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell category:cs.CV published:2014-11-17 summary:Models based on deep convolutional networks have dominated recent imageinterpretation tasks; we investigate whether models which are also recurrent,or "temporally deep", are effective for tasks involving sequences, visual andotherwise. We develop a novel recurrent convolutional architecture suitable forlarge-scale visual learning which is end-to-end trainable, and demonstrate thevalue of these models on benchmark video recognition tasks, image descriptionand retrieval problems, and video narration challenges. In contrast to currentmodels which assume a fixed spatio-temporal receptive field or simple temporalaveraging for sequential processing, recurrent convolutional models are "doublydeep"' in that they can be compositional in spatial and temporal "layers". Suchmodels may have advantages when target concepts are complex and/or trainingdata are limited. Learning long-term dependencies is possible whennonlinearities are incorporated into the network state updates. Long-term RNNmodels are appealing in that they directly can map variable-length inputs(e.g., video frames) to variable length outputs (e.g., natural language text)and can model complex temporal dynamics; yet they can be optimized withbackpropagation. Our recurrent long-term models are directly connected tomodern visual convnet models and can be jointly trained to simultaneously learntemporal dynamics and convolutional perceptual representations. Our resultsshow such models have distinct advantages over state-of-the-art models forrecognition or generation which are separately defined and/or optimized.
arxiv-9300-275 | On Sex, Evolution, and the Multiplicative Weights Update Algorithm | http://arxiv.org/pdf/1502.05056v1.pdf | author:Reshef Meir, David Parkes category:cs.LG cs.GT published:2015-02-17 summary:We consider a recent innovative theory by Chastain et al. on the role of sexin evolution [PNAS'14]. In short, the theory suggests that the evolutionaryprocess of gene recombination implements the celebrated multiplicative weightsupdates algorithm (MWUA). They prove that the population dynamics induced bysexual reproduction can be precisely modeled by genes that use MWUA as theirlearning strategy in a particular coordination game. The result holds in theenvironments of \emph{weak selection}, under the assumption that the populationfrequencies remain a product distribution. We revisit the theory, eliminating both the requirement of weak selection andany assumption on the distribution of the population. Removing the assumptionof product distributions is crucial, since as we show, this assumption isinconsistent with the population dynamics. We show that the marginal alleledistributions induced by the population dynamics precisely match the marginalsinduced by a multiplicative weights update algorithm in this general setting,thereby affirming and substantially generalizing these earlier results. We further revise the implications for convergence and utility or fitnessguarantees in coordination games. In contrast to the claim of Chastain etal.[PNAS'14], we conclude that the sexual evolutionary dynamics does not entailany property of the population distribution, beyond those already implied byconvergence.
arxiv-9300-276 | On the Inductive Bias of Dropout | http://arxiv.org/pdf/1412.4736v4.pdf | author:David P. Helmbold, Philip M. Long category:cs.LG cs.AI cs.NE math.ST stat.ML stat.TH published:2014-12-15 summary:Dropout is a simple but effective technique for learning in neural networksand other settings. A sound theoretical understanding of dropout is needed todetermine when dropout should be applied and how to use it most effectively. Inthis paper we continue the exploration of dropout as a regularizer pioneered byWager, et.al. We focus on linear classification where a convex proxy to themisclassification loss (i.e. the logistic loss used in logistic regression) isminimized. We show: (a) when the dropout-regularized criterion has a uniqueminimizer, (b) when the dropout-regularization penalty goes to infinity withthe weights, and when it remains bounded, (c) that the dropout regularizationcan be non-monotonic as individual weights increase from 0, and (d) that thedropout regularization penalty may not be convex. This last point isparticularly surprising because the combination of dropout regularization withany convex loss proxy is always a convex function. In order to contrast dropout regularization with $L_2$ regularization, weformalize the notion of when different sources are more compatible withdifferent regularizers. We then exhibit distributions that are provably morecompatible with dropout regularization than $L_2$ regularization, and viceversa. These sources provide additional insight into how the inductive biasesof dropout and $L_2$ regularization differ. We provide some similar results for$L_1$ regularization.
arxiv-9300-277 | Context Tricks for Cheap Semantic Segmentation | http://arxiv.org/pdf/1502.04983v1.pdf | author:Thanapong Intharah, Gabriel J. Brostow category:cs.CV published:2015-02-17 summary:Accurate semantic labeling of image pixels is difficult because intra-classvariability is often greater than inter-class variability. In turn, fastsemantic segmentation is hard because accurate models are usually toocomplicated to also run quickly at test-time. Our experience with building andrunning semantic segmentation systems has also shown a reasonably obviousbottleneck on model complexity, imposed by small training datasets. Wetherefore propose two simple complementary strategies that leverage context togive better semantic segmentation, while scaling up or down to train ondifferent-sized datasets. As easy modifications for existing semantic segmentation algorithms, weintroduce Decorrelated Semantic Texton Forests, and the Context Sensitive ImageLevel Prior. The proposed modifications are tested using a Semantic TextonForest (STF) system, and the modifications are validated on two standardbenchmark datasets, MSRC-21 and PascalVOC-2010. In Python based comparisons,our system is insignificantly slower than STF at test-time, yet producessuperior semantic segmentations overall, with just push-button training.
arxiv-9300-278 | Measuring and Understanding Sensory Representations within Deep Networks Using a Numerical Optimization Framework | http://arxiv.org/pdf/1502.04972v1.pdf | author:Chuan-Yung Tsai, David D. Cox category:cs.NE q-bio.NC published:2015-02-17 summary:A central challenge in sensory neuroscience is describing how the activity ofpopulations of neurons can represent useful features of the externalenvironment. However, while neurophysiologists have long been able to recordthe responses of neurons in awake, behaving animals, it is another matterentirely to say what a given neuron does. A key problem is that in many sensorydomains, the space of all possible stimuli that one might encounter iseffectively infinite; in vision, for instance, natural scenes arecombinatorially complex, and an organism will only encounter a tiny fraction ofpossible stimuli. As a result, even describing the response properties ofsensory neurons is difficult, and investigations of neuronal functions arealmost always critically limited by the number of stimuli that can beconsidered. In this paper, we propose a closed-loop, optimization-basedexperimental framework for characterizing the response properties of sensoryneurons, building on past efforts in closed-loop experimental methods, andleveraging recent advances in artificial neural networks to serve as as aproving ground for our techniques. Specifically, using deep convolutionalneural networks, we asked whether modern black-box optimization techniques canbe used to interrogate the "tuning landscape" of an artificial neuron in adeep, nonlinear system, without imposing significant constraints on the spaceof stimuli under consideration. We introduce a series of measures to quantifythe tuning landscapes, and show how these relate to the performances of thenetworks in an object recognition task. To the extent that deep convolutionalneural networks increasingly serve as de facto working hypotheses forbiological vision, we argue that developing a unified approach for studyingboth artificial and biological systems holds great potential to advance bothfields together.
arxiv-9300-279 | Outperforming Word2Vec on Analogy Tasks with Random Projections | http://arxiv.org/pdf/1412.6616v2.pdf | author:Abram Demski, Volkan Ustun, Paul Rosenbloom, Cody Kommers category:cs.CL cs.LG published:2014-12-20 summary:We present a distributed vector representation based on a simplification ofthe BEAGLE system, designed in the context of the Sigma cognitive architecture.Our method does not require gradient-based training of neural networks, matrixdecompositions as with LSA, or convolutions as with BEAGLE. All that isinvolved is a sum of random vectors and their pointwise products. Despite thesimplicity of this technique, it gives state-of-the-art results on analogyproblems, in most cases better than Word2Vec. To explain this success, weinterpret it as a dimension reduction via random projection.
arxiv-9300-280 | The Linearization of Pairwise Markov Networks | http://arxiv.org/pdf/1502.04956v1.pdf | author:Wolfgang Gatterbauer category:cs.AI cs.LG cs.SI published:2015-02-17 summary:Belief Propagation (BP) allows to approximate exact probabilistic inferencein graphical models, such as Markov networks (also called Markov random fields,or undirected graphical models). However, no exact convergence guarantees forBP are known, in general. Recent work has proposed to approximate BP bylinearizing the update equations around default values for the special casewhen all edges in the Markov network carry the same symmetric, doublystochastic potential. This linearization has led to exact convergenceguarantees, considerable speed-up, while maintaining high quality results innetwork-based classification (i.e. when we only care about the most likelylabel or class for each node and not the exact probabilities). The presentpaper generalizes our prior work on Linearized Belief Propagation (LinBP) withan approach that approximates Loopy Belief Propagation on any pairwise Markovnetwork with the problem of solving a linear equation system.
arxiv-9300-281 | Spectrum Bandit Optimization | http://arxiv.org/pdf/1302.6974v4.pdf | author:Marc Lelarge, Alexandre Proutiere, M. Sadegh Talebi category:cs.LG cs.NI math.OC published:2013-02-27 summary:We consider the problem of allocating radio channels to links in a wirelessnetwork. Links interact through interference, modelled as a conflict graph(i.e., two interfering links cannot be simultaneously active on the samechannel). We aim at identifying the channel allocation maximizing the totalnetwork throughput over a finite time horizon. Should we know the average radioconditions on each channel and on each link, an optimal allocation would beobtained by solving an Integer Linear Program (ILP). When radio conditions areunknown a priori, we look for a sequential channel allocation policy thatconverges to the optimal allocation while minimizing on the way the throughputloss or {\it regret} due to the need for exploring sub-optimal allocations. Weformulate this problem as a generic linear bandit problem, and analyze it firstin a stochastic setting where radio conditions are driven by a stationarystochastic process, and then in an adversarial setting where radio conditionscan evolve arbitrarily. We provide new algorithms in both settings and deriveupper bounds on their regrets.
arxiv-9300-282 | Unified Heat Kernel Regression for Diffusion, Kernel Smoothing and Wavelets on Manifolds and Its Application to Mandible Growth Modeling in CT Images | http://arxiv.org/pdf/1409.6498v2.pdf | author:Moo K. Chung, Anqi Qiu, Seongho Seo, Houri K. Vorperian category:cs.CV stat.ME published:2014-09-23 summary:We present a novel kernel regression framework for smoothing scalar surfacedata using the Laplace-Beltrami eigenfunctions. Starting with the heat kernelconstructed from the eigenfunctions, we formulate a new bivariate kernelregression framework as a weighted eigenfunction expansion with the heat kernelas the weights. The new kernel regression is mathematically equivalent toisotropic heat diffusion, kernel smoothing and recently popular diffusionwavelets. Unlike many previous partial differential equation based approachesinvolving diffusion, our approach represents the solution of diffusionanalytically, reducing numerical inaccuracy and slow convergence. The numericalimplementation is validated on a unit sphere using spherical harmonics. As anillustration, we have applied the method in characterizing the localized growthpattern of mandible surfaces obtained in CT images from subjects between ages 0and 20 years by regressing the length of displacement vectors with respect tothe template surface.
arxiv-9300-283 | Randomized LU decomposition: An Algorithm for Dictionaries Construction | http://arxiv.org/pdf/1502.04824v1.pdf | author:Aviv Rotbart, Gil Shabat, Yaniv Shmueli, Amir Averbuch category:cs.CV I.5 published:2015-02-17 summary:In recent years, distinctive-dictionary construction has gained importancedue to his usefulness in data processing. Usually, one or more dictionaries areconstructed from a training data and then they are used to classify signalsthat did not participate in the training process. A new dictionary constructionalgorithm is introduced. It is based on a low-rank matrix factorization beingachieved by the application of the randomized LU decomposition to a trainingdata. This method is fast, scalable, parallelizable, consumes low memory,outperforms SVD in these categories and works also extremely well on largesparse matrices. In contrast to existing methods, the randomized LUdecomposition constructs an under-complete dictionary, which simplifies boththe construction and the classification processes of newly arrived signals. Thedictionary construction is generic and general that fits differentapplications. We demonstrate the capabilities of this algorithm for file typeidentification, which is a fundamental task in digital security arena,performed nowadays for example by sandboxing mechanism, deep packet inspection,firewalls and anti-virus systems. We propose a content-based method thatdetects file types that neither depend on file extension nor on metadata. Suchapproach is harder to deceive and we show that only a few file fragments from awhole file are needed for a successful classification. Based on the constructeddictionaries, we show that the proposed method can effectively identifyexecution code fragments in PDF files. $\textbf{Keywords.}$ Dictionary construction, classification, LUdecomposition, randomized LU decomposition, content-based file detection,computer security.
arxiv-9300-284 | Learning Poisson Binomial Distributions | http://arxiv.org/pdf/1107.2702v4.pdf | author:Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio category:cs.DS cs.LG math.ST stat.TH published:2011-07-13 summary:We consider a basic problem in unsupervised learning: learning an unknown\emph{Poisson Binomial Distribution}. A Poisson Binomial Distribution (PBD)over $\{0,1,\dots,n\}$ is the distribution of a sum of $n$ independentBernoulli random variables which may have arbitrary, potentially non-equal,expectations. These distributions were first studied by S. Poisson in 1837\cite{Poisson:37} and are a natural $n$-parameter generalization of thefamiliar Binomial Distribution. Surprisingly, prior to our work this basiclearning problem was poorly understood, and known results for it were far fromoptimal. We essentially settle the complexity of the learning problem for this basicclass of distributions. As our first main result we give a highly efficientalgorithm which learns to $\eps$-accuracy (with respect to the total variationdistance) using $\tilde{O}(1/\eps^3)$ samples \emph{independent of $n$}. Therunning time of the algorithm is \emph{quasilinear} in the size of its inputdata, i.e., $\tilde{O}(\log(n)/\eps^3)$ bit-operations. (Observe that each drawfrom the distribution is a $\log(n)$-bit string.) Our second main result is a{\em proper} learning algorithm that learns to $\eps$-accuracy using$\tilde{O}(1/\eps^2)$ samples, and runs in time $(1/\eps)^{\poly (\log(1/\eps))} \cdot \log n$. This is nearly optimal, since any algorithm {for thisproblem} must use $\Omega(1/\eps^2)$ samples. We also give positive andnegative results for some extensions of this learning problem to weighted sumsof independent Bernoulli random variables.
arxiv-9300-285 | Model Selection for Topic Models via Spectral Decomposition | http://arxiv.org/pdf/1410.6466v2.pdf | author:Dehua Cheng, Xinran He, Yan Liu category:stat.ML cs.IR cs.LG stat.CO 62H30 H.3.3 published:2014-10-23 summary:Topic models have achieved significant successes in analyzing large-scaletext corpus. In practical applications, we are always confronted with thechallenge of model selection, i.e., how to appropriately set the number oftopics. Following recent advances in topic model inference via tensordecomposition, we make a first attempt to provide theoretical analysis on modelselection in latent Dirichlet allocation. Under mild conditions, we derive theupper bound and lower bound on the number of topics given a text collection offinite size. Experimental results demonstrate that our bounds are accurate andtight. Furthermore, using Gaussian mixture model as an example, we show thatour methodology can be easily generalized to model selection analysis for otherlatent models.
arxiv-9300-286 | On the Predictive Properties of Binary Link Functions | http://arxiv.org/pdf/1502.04742v1.pdf | author:Necla Gunduz, Ernest Fokoue category:stat.ML stat.ME published:2015-02-16 summary:This paper provides a theoretical and computational justification of the longheld claim that of the similarity of the probit and logit link functions oftenused in binary classification. Despite this widespread recognition of thestrong similarities between these two link functions, very few (if any)researchers have dedicated time to carry out a formal study aimed atestablishing and characterizing firmly all the aspects of the similarities anddifferences. This paper proposes a definition of both structural and predictiveequivalence of link functions-based binary regression models, and explores thevarious ways in which they are either similar or dissimilar. From a predictiveanalytics perspective, it turns out that not only are probit and logitperfectly predictively concordant, but the other link functions like cauchitand complementary log log enjoy very high percentage of predictive equivalence.Throughout this paper, simulated and real life examples demonstrate all theequivalence results that we prove theoretically.
arxiv-9300-287 | ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike and Slab Priors | http://arxiv.org/pdf/1502.04726v1.pdf | author:Hojjat S. Mousavi, Vishal Monga, Trac D. Tran category:stat.ML cs.CV math.OC published:2015-02-16 summary:In this letter, we address sparse signal recovery using spike and slabpriors. In particular, we focus on a Bayesian framework where sparsity isenforced on reconstruction coefficients via probabilistic priors. Theoptimization resulting from spike and slab prior maximization is known to be ahard non-convex problem, and existing solutions involve simplifying assumptionsand/or relaxations. We propose an approach called Iterative Convex Refinement(ICR) that aims to solve the aforementioned optimization problem directlyallowing for greater generality in the sparse structure. Essentially, ICRsolves a sequence of convex optimization problems such that sequence ofsolutions converges to a sub-optimal solution of the original hard optimizationproblem. We propose two versions of our algorithm: a.) an unconstrainedversion, and b.) with a non-negativity constraint on sparse coefficients, whichmay be required in some real-world problems. Experimental validation isperformed on both synthetic data and for a real-world image recovery problem,which illustrates merits of ICR over state of the art alternatives.
arxiv-9300-288 | HEp-2 Cell Classification via Fusing Texture and Shape Information | http://arxiv.org/pdf/1502.04658v1.pdf | author:Xianbiao Qi, Guoying Zhao, Chun-Guang Li, Jun Guo, Matti Pietikäinen category:cs.CV published:2015-02-16 summary:Indirect Immunofluorescence (IIF) HEp-2 cell image is an effective evidencefor diagnosis of autoimmune diseases. Recently computer-aided diagnosis ofautoimmune diseases by IIF HEp-2 cell classification has attracted greatattention. However the HEp-2 cell classification task is quite challenging dueto large intra-class variation and small between-class variation. In this paperwe propose an effective and efficient approach for the automatic classificationof IIF HEp-2 cell image by fusing multi-resolution texture information andricher shape information. To be specific, we propose to: a) capture themulti-resolution texture information by a novel Pairwise Rotation InvariantCo-occurrence of Local Gabor Binary Pattern (PRICoLGBP) descriptor, b) depictthe richer shape information by using an Improved Fisher Vector (IFV) modelwith RootSIFT features which are sampled from large image patches in multiplescales, and c) combine them properly. We evaluate systematically the proposedapproach on the IEEE International Conference on Pattern Recognition (ICPR)2012, IEEE International Conference on Image Processing (ICIP) 2013 and ICPR2014 contest data sets. The experimental results for the proposed methodssignificantly outperform the winners of ICPR 2012 and ICIP 2013 contest, andachieve comparable performance with the winner of the newly released ICPR 2014contest.
arxiv-9300-289 | Inferring 3D Object Pose in RGB-D Images | http://arxiv.org/pdf/1502.04652v1.pdf | author:Saurabh Gupta, Pablo Arbeláez, Ross Girshick, Jitendra Malik category:cs.CV published:2015-02-16 summary:The goal of this work is to replace objects in an RGB-D scene withcorresponding 3D models from a library. We approach this problem by firstdetecting and segmenting object instances in the scene using the approach fromGupta et al. [13]. We use a convolutional neural network (CNN) to predict thepose of the object. This CNN is trained using pixel normals in imagescontaining rendered synthetic objects. When tested on real data, it outperformsalternative algorithms trained on real data. We then use this coarse poseestimate along with the inferred pixel support to align a small number ofprototypical models to the data, and place the model that fits the best intothe scene. We observe a 48% relative improvement in performance at the task of3D detection over the current state-of-the-art [33], while being an order ofmagnitude faster at the same time.
arxiv-9300-290 | Large-scale optimization with the primal-dual column generation method | http://arxiv.org/pdf/1309.2168v2.pdf | author:Jacek Gondzio, Pablo González-Brevis, Pedro Munari category:math.OC cs.LG cs.NA published:2013-09-09 summary:The primal-dual column generation method (PDCGM) is a general-purpose columngeneration technique that relies on the primal-dual interior point method tosolve the restricted master problems. The use of this interior point methodvariant allows to obtain suboptimal and well-centered dual solutions whichnaturally stabilizes the column generation. As recently presented in theliterature, reductions in the number of calls to the oracle and in the CPUtimes are typically observed when compared to the standard column generation,which relies on extreme optimal dual solutions. However, these results arebased on relatively small problems obtained from linear relaxations ofcombinatorial applications. In this paper, we investigate the behaviour of thePDCGM in a broader context, namely when solving large-scale convex optimizationproblems. We have selected applications that arise in important real-lifecontexts such as data analysis (multiple kernel learning problem),decision-making under uncertainty (two-stage stochastic programming problems)and telecommunication and transportation networks (multicommodity network flowproblem). In the numerical experiments, we use publicly available benchmarkinstances to compare the performance of the PDCGM against recent results fordifferent methods presented in the literature, which were the best availableresults to date. The analysis of these results suggests that the PDCGM offersan attractive alternative over specialized methods since it remains competitivein terms of number of iterations and CPU times even for large-scaleoptimization problems.
arxiv-9300-291 | Particle Gibbs for Bayesian Additive Regression Trees | http://arxiv.org/pdf/1502.04622v1.pdf | author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG stat.CO published:2015-02-16 summary:Additive regression trees are flexible non-parametric models and popularoff-the-shelf tools for real-world non-linear regression. In applicationdomains, such as bioinformatics, where there is also demand for probabilisticpredictions with measures of uncertainty, the Bayesian additive regressiontrees (BART) model, introduced by Chipman et al. (2010), is increasinglypopular. As data sets have grown in size, however, the standardMetropolis-Hastings algorithms used to perform inference in BART are provinginadequate. In particular, these Markov chains make local changes to the treesand suffer from slow mixing when the data are high-dimensional or the bestfitting trees are more than a few layers deep. We present a novel sampler forBART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and atop-down particle filtering algorithm for Bayesian decision trees(Lakshminarayanan et al., 2013). Rather than making local changes to individualtrees, the PG sampler proposes a complete tree to fit the residual. Experimentsshow that the PG sampler outperforms existing samplers in many settings.
arxiv-9300-292 | Deep Transform: Error Correction via Probabilistic Re-Synthesis | http://arxiv.org/pdf/1502.04617v1.pdf | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-02-16 summary:Errors in data are usually unwelcome and so some means to correct them isuseful. However, it is difficult to define, detect or correct errors in anunsupervised way. Here, we train a deep neural network to re-synthesize itsinputs at its output layer for a given class of data. We then exploit the factthat this abstract transformation, which we call a deep transform (DT),inherently rejects information (errors) existing outside of the abstractfeature space. Using the DT to perform probabilistic re-synthesis, wedemonstrate the recovery of data that has been subject to extreme degradation.
arxiv-9300-293 | The Ladder: A Reliable Leaderboard for Machine Learning Competitions | http://arxiv.org/pdf/1502.04585v1.pdf | author:Avrim Blum, Moritz Hardt category:cs.LG published:2015-02-16 summary:The organizer of a machine learning competition faces the problem ofmaintaining an accurate leaderboard that faithfully represents the quality ofthe best submission of each competing team. What makes this estimation problemparticularly challenging is its sequential and adaptive nature. As participantsare allowed to repeatedly evaluate their submissions on the leaderboard, theymay begin to overfit to the holdout data that supports the leaderboard. Fewtheoretical results give actionable advice on how to design a reliableleaderboard. Existing approaches therefore often resort to poorly understoodheuristics such as limiting the bit precision of answers and the rate ofre-submission. In this work, we introduce a notion of "leaderboard accuracy" tailored to theformat of a competition. We introduce a natural algorithm called "the Ladder"and demonstrate that it simultaneously supports strong theoretical guaranteesin a fully adaptive model of estimation, withstands practical adversarialattacks, and achieves high utility on real submission files from an actualcompetition hosted by Kaggle. Notably, we are able to sidestep a powerful recent hardness result foradaptive risk estimation that rules out algorithms such as ours under aseemingly very similar notion of accuracy. On a practical note, we provide acompletely parameter-free variant of our algorithm that can be deployed in areal competition with no tuning required whatsoever.
arxiv-9300-294 | Mondrian Forests: Efficient Online Random Forests | http://arxiv.org/pdf/1406.2673v2.pdf | author:Balaji Lakshminarayanan, Daniel M. Roy, Yee Whye Teh category:stat.ML cs.LG published:2014-06-10 summary:Ensembles of randomized decision trees, usually referred to as randomforests, are widely used for classification and regression tasks in machinelearning and statistics. Random forests achieve competitive predictiveperformance and are computationally efficient to train and test, making themexcellent candidates for real-world prediction tasks. The most popular randomforest variants (such as Breiman's random forest and extremely randomizedtrees) operate on batches of training data. Online methods are now in greaterdemand. Existing online random forests, however, require more training datathan their batch counterpart to achieve comparable predictive performance. Inthis work, we use Mondrian processes (Roy and Teh, 2009) to construct ensemblesof random decision trees we call Mondrian forests. Mondrian forests can begrown in an incremental/online fashion and remarkably, the distribution ofonline Mondrian forests is the same as that of batch Mondrian forests. Mondrianforests achieve competitive predictive performance comparable with existingonline random forests and periodically re-trained batch random forests, whilebeing more than an order of magnitude faster, thus representing a bettercomputation vs accuracy tradeoff.
arxiv-9300-295 | Spatial Diffuseness Features for DNN-Based Speech Recognition in Noisy and Reverberant Environments | http://arxiv.org/pdf/1410.2479v2.pdf | author:Andreas Schwarz, Christian Huemmer, Roland Maas, Walter Kellermann category:cs.CL cs.NE cs.SD stat.ML published:2014-10-09 summary:We propose a spatial diffuseness feature for deep neural network (DNN)-basedautomatic speech recognition to improve recognition accuracy in reverberant andnoisy environments. The feature is computed in real-time from multiplemicrophone signals without requiring knowledge or estimation of the directionof arrival, and represents the relative amount of diffuse noise in each timeand frequency bin. It is shown that using the diffuseness feature as anadditional input to a DNN-based acoustic model leads to a reduced word errorrate for the REVERB challenge corpus, both compared to logmelspec featuresextracted from noisy signals, and features enhanced by spectral subtraction.
arxiv-9300-296 | The Responsibility Weighted Mahalanobis Kernel for Semi-Supervised Training of Support Vector Machines for Classification | http://arxiv.org/pdf/1502.04033v2.pdf | author:Tobias Reitmaier, Bernhard Sick category:cs.LG stat.ML published:2015-02-13 summary:Kernel functions in support vector machines (SVM) are needed to assess thesimilarity of input samples in order to classify these samples, for instance.Besides standard kernels such as Gaussian (i.e., radial basis function, RBF) orpolynomial kernels, there are also specific kernels tailored to considerstructure in the data for similarity assessment. In this article, we willcapture structure in data by means of probabilistic mixture density models, forexample Gaussian mixtures in the case of real-valued input spaces. From thedistance measures that are inherently contained in these models, e.g.,Mahalanobis distances in the case of Gaussian mixtures, we derive a new kernel,the responsibility weighted Mahalanobis (RWM) kernel. Basically, this kernelemphasizes the influence of model components from which any two samples thatare compared are assumed to originate (that is, the "responsible" modelcomponents). We will see that this kernel outperforms the RBF kernel and otherkernels capturing structure in data (such as the LAP kernel in Laplacian SVM)in many applications where partially labeled data are available, i.e., forsemi-supervised training of SVM. Other key advantages are that the RWM kernelcan easily be used with standard SVM implementations and training algorithmssuch as sequential minimal optimization, and heuristics known for theparametrization of RBF kernels in a C-SVM can easily be transferred to this newkernel. Properties of the RWM kernel are demonstrated with 20 benchmark datasets and an increasing percentage of labeled samples in the training data.
arxiv-9300-297 | Clustering by Descending to the Nearest Neighbor in the Delaunay Graph Space | http://arxiv.org/pdf/1502.04502v1.pdf | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG published:2015-02-16 summary:In our previous works, we proposed a physically-inspired rule to organize thedata points into an in-tree (IT) structure, in which some undesired edges areallowed to occur. By removing those undesired or redundant edges, this ITstructure is divided into several separate parts, each representing onecluster. In this work, we seek to prevent the undesired edges from arising atthe source. Before using the physically-inspired rule, data points are at firstorganized into a proximity graph which restricts each point to select theoptimal directed neighbor just among its neighbors. Consequently, separatedin-trees or clusters automatically arise, without redundant edges requiring tobe removed.
arxiv-9300-298 | Color Image Enhancement Using the lrgb Coordinates in the Context of Support Fuzzification | http://arxiv.org/pdf/1502.04499v1.pdf | author:Vasile Patrascu category:cs.CV published:2015-02-16 summary:Image enhancement is an important stage in the image-processing domain. Themost known image enhancement method is the histogram equalization. This methodis an automated one, and realizes a simultaneous modification for brightnessand contrast in the case of monochrome images and for brightness, contrast,saturation and hue in the case of color images. Simple and efficient methodscan be obtained if affine transforms within logarithmic models are used. A veryimportant thing in the affine transform determination for color images is thecoordinate system that is used for color space representation. Thus, the usingof the RGB coordinates leads to a simultaneous modification of luminosity andsaturation. In this paper using the lrgb perceptual coordinates one can defineaffine transforms, which allow a separated modification of luminosity l andsaturation s (saturation being calculated with the component rgb in thechromatic plane). Better results can be obtained if partitions are defined onthe image support and then the pixels are separately processed in each windowbelonging to the defined partition. Classical partitions frequently lead to theappearance of some discontinuities at the boundaries between these windows. Inorder to avoid all these drawbacks the classical partitions may be replaced byfuzzy partitions. Their elements will be fuzzy windows and in each of themthere will be defined an affine transform induced by parameters using the fuzzymean, fuzzy variance and fuzzy saturation computed for the pixels that belongto the analyzed window. The final image is obtained by summing up in a weightway the images of every fuzzy window.
arxiv-9300-299 | Towards Building Deep Networks with Bayesian Factor Graphs | http://arxiv.org/pdf/1502.04492v1.pdf | author:Amedeo Buonanno, Francesco A. N. Palmieri category:cs.CV cs.LG published:2015-02-16 summary:We propose a Multi-Layer Network based on the Bayesian framework of theFactor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensionallattice. The Latent Variable Model (LVM) is the basic building block of aquadtree hierarchy built on top of a bottom layer of random variables thatrepresent pixels of an image, a feature map, or more generally a collection ofspatially distributed discrete variables. The multi-layer architectureimplements a hierarchical data representation that, via belief propagation, canbe used for learning and inference. Typical uses are pattern completion,correction and classification. The FGrn paradigm provides great flexibility andmodularity and appears as a promising candidate for building deep networks: thesystem can be easily extended by introducing new and different (in cardinalityand in type) variables. Prior knowledge, or supervised information, can beintroduced at different scales. The FGrn paradigm provides a handy way forbuilding all kinds of architectures by interconnecting only three types ofunits: Single Input Single Output (SISO) blocks, Sources and Replicators. Thenetwork is designed like a circuit diagram and the belief messages flowbidirectionally in the whole system. The learning algorithms operate onlylocally within each block. The framework is demonstrated in this paper in athree-layer structure applied to images extracted from a standard data set.
arxiv-9300-300 | Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks | http://arxiv.org/pdf/1312.5845v7.pdf | author:Takashi Shinozaki, Yasushi Naruse category:cs.NE cs.CV cs.LG stat.ML published:2013-12-20 summary:We propose a novel learning method for multilayered neural networks whichuses feedforward supervisory signal and associates classification of a newinput with that of pre-trained input. The proposed method effectively uses richinput information in the earlier layer for robust leaning and revising internalrepresentation in a multilayer neural network.
