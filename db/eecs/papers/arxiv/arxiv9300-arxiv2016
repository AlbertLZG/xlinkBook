arxiv-1412-6502 | Detecting Epileptic Seizures from EEG Data using Neural Networks |  http://arxiv.org/abs/1412.6502  | author:Siddharth Pramod, Adam Page, Tinoosh Mohsenin, Tim Oates category:cs.LG cs.NE q-bio.NC published:2014-12-19 summary:We explore the use of neural networks trained with dropout in predictingepileptic seizures from electroencephalographic data (scalp EEG). The input tothe neural network is a 126 feature vector containing 9 features for each ofthe 14 EEG channels obtained over 1-second, non-overlapping windows. The modelsin our experiments achieved high sensitivity and specificity on patient recordsnot used in the training process. This is demonstrated usingleave-one-out-cross-validation across patient records, where we hold out onepatient's record as the test set and use all other patients' records fortraining; repeating this procedure for all patients in the database.
arxiv-1412-6249 | Purine: A bi-graph based deep learning framework |  http://arxiv.org/abs/1412.6249  | author:Min Lin, Shuo Li, Xuan Luo, Shuicheng Yan category:cs.NE cs.LG published:2014-12-19 summary:In this paper, we introduce a novel deep learning framework, termed Purine.In Purine, a deep network is expressed as a bipartite graph (bi-graph), whichis composed of interconnected operators and data tensors. With the bi-graphabstraction, networks are easily solvable with event-driven task dispatcher. Wethen demonstrate that different parallelism schemes over GPUs and/or CPUs onsingle or multiple PCs can be universally implemented by graph composition.This eases researchers from coding for various parallelization schemes, and thesame dispatcher can be used for solving variant graphs. Scheduled by the taskdispatcher, memory transfers are fully overlapped with other computations,which greatly reduce the communication overhead and help us achieve approximatelinear acceleration.
arxiv-1412-6219 | Information-Theoretic Methods for Identifying Relationships among Climate Variables |  http://arxiv.org/abs/1412.6219  | author:Kevin H. Knuth, Deniz Gençağa, William B. Rossow category:physics.ao-ph stat.ML published:2014-12-19 summary:Information-theoretic quantities, such as entropy, are used to quantify theamount of information a given variable provides. Entropies can be used togetherto compute the mutual information, which quantifies the amount of informationtwo variables share. However, accurately estimating these quantities from datais extremely challenging. We have developed a set of computational techniquesthat allow one to accurately compute marginal and joint entropies. Thesealgorithms are probabilistic in nature and thus provide information on theuncertainty in our estimates, which enable us to establish statisticalsignificance of our findings. We demonstrate these methods by identifyingrelations between cloud data from the International Satellite Cloud ClimatologyProject (ISCCP) and data from other sources, such as equatorial pacific seasurface temperatures (SST).
arxiv-1412-6505 | Pooled Motion Features for First-Person Videos |  http://arxiv.org/abs/1412.6505  | author:M. S. Ryoo, Brandon Rothrock, Larry Matthies category:cs.CV published:2014-12-19 summary:In this paper, we present a new feature representation for first-personvideos. In first-person video understanding (e.g., activity recognition), it isvery important to capture both entire scene dynamics (i.e., egomotion) andsalient local motion observed in videos. We describe a representation frameworkbased on time series pooling, which is designed to abstractshort-term/long-term changes in feature descriptor elements. The idea is tokeep track of how descriptor values are changing over time and summarize themto represent motion in the activity video. The framework is general, handlingany types of per-frame feature descriptors including conventional motiondescriptors like histogram of optical flows (HOF) as well as appearancedescriptors from more recent convolutional neural networks (CNN). Weexperimentally confirm that our approach clearly outperforms previous featurerepresentations including bag-of-visual-words and improved Fisher vector (IFV)when using identical underlying feature descriptors. We also confirm that ourfeature representation has superior performance to existing state-of-the-artfeatures like local spatio-temporal features and Improved Trajectory Features(originally developed for 3rd-person videos) when handling first-person videos.Multiple first-person activity datasets were tested under various settings toconfirm these findings.
arxiv-1412-6448 | Embedding Word Similarity with Neural Machine Translation |  http://arxiv.org/abs/1412.6448  | author:Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline Devin, Yoshua Bengio category:cs.CL published:2014-12-19 summary:Neural language models learn word representations, or embeddings, thatcapture rich linguistic and conceptual information. Here we investigate theembeddings learned by neural machine translation models, a recently-developedclass of neural language model. We show that embeddings from translation modelsoutperform those learned by monolingual models at tasks that require knowledgeof both conceptual similarity and lexical-syntactic role. We further show thatthese effects hold when translating from both English to French and English toGerman, and argue that the desirable properties of translation embeddingsshould emerge largely independently of the source and target languages.Finally, we apply a new method for training neural translation models with verylarge vocabularies, and show that this vocabulary expansion algorithm resultsin minimal degradation of embedding quality. Our embedding spaces can bequeried in an online demo and downloaded from our web page. Overall, ouranalyses indicate that translation-based embeddings should be used inapplications that require concepts to be organised according to similarityand/or lexical function, while monolingual embeddings are better suited tomodelling (nonspecific) inter-word relatedness.
arxiv-1412-6553 | Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition |  http://arxiv.org/abs/1412.6553  | author:Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, Victor Lempitsky category:cs.CV cs.LG published:2014-12-19 summary:We propose a simple two-step approach for speeding up convolution layerswithin large convolutional neural networks based on tensor decomposition anddiscriminative fine-tuning. Given a layer, we use non-linear least squares tocompute a low-rank CP-decomposition of the 4D convolution kernel tensor into asum of a small number of rank-one tensors. At the second step, thisdecomposition is used to replace the original convolutional layer with asequence of four convolutional layers with small kernels. After suchreplacement, the entire network is fine-tuned on the training data usingstandard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive withprevious approaches, leading to higher obtained CPU speedups at the cost oflower accuracy drops for the smaller of the two networks. Thus, for the36-class character classification CNN, our approach obtains a 8.5x CPU speedupof the whole network with only minor accuracy drop (1% from 91% to 90%). Forthe standard ImageNet architecture (AlexNet), the approach speeds up the secondconvolution layer by a factor of 4x at the cost of $1\%$ increase of theoverall top-5 classification error.
arxiv-1412-6550 | FitNets: Hints for Thin Deep Nets |  http://arxiv.org/abs/1412.6550  | author:Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio category:cs.LG cs.NE published:2014-12-19 summary:While depth tends to improve network performances, it also makesgradient-based training more difficult since deeper networks tend to be morenon-linear. The recently proposed knowledge distillation approach is aimed atobtaining small and fast-to-execute models, and it has shown that a studentnetwork could imitate the soft output of a larger teacher network or ensembleof networks. In this paper, we extend this idea to allow the training of astudent that is deeper and thinner than the teacher, using not only the outputsbut also the intermediate representations learned by the teacher as hints toimprove the training process and final performance of the student. Because thestudent intermediate hidden layer will generally be smaller than the teacher'sintermediate hidden layer, additional parameters are introduced to map thestudent hidden layer to the prediction of the teacher hidden layer. This allowsone to train deeper students that can generalize better or run faster, atrade-off that is controlled by the chosen student capacity. For example, onCIFAR-10, a deep student network with almost 10.4 times less parametersoutperforms a larger, state-of-the-art teacher network.
arxiv-1412-6286 | Regression with Linear Factored Functions |  http://arxiv.org/abs/1412.6286  | author:Wendelin Böhmer, Klaus Obermayer category:cs.LG stat.ML published:2014-12-19 summary:Many applications that use empirically estimated functions face a curse ofdimensionality, because the integrals over most function classes must beapproximated by sampling. This paper introduces a novel regression-algorithmthat learns linear factored functions (LFF). This class of functions hasstructural properties that allow to analytically solve certain integrals and tocalculate point-wise products. Applications like belief propagation andreinforcement learning can exploit these properties to break the curse andspeed up computation. We derive a regularized greedy optimization scheme, thatlearns factored basis functions during training. The novel regression algorithmperforms competitively to Gaussian processes on benchmark tasks, and thelearned LFF functions are with 4-9 factored basis functions on average verycompact.
arxiv-1412-6418 | Inducing Semantic Representation from Text by Jointly Predicting and Factorizing Relations |  http://arxiv.org/abs/1412.6418  | author:Ivan Titov, Ehsan Khoddam category:cs.CL cs.LG stat.ML published:2014-12-19 summary:In this work, we propose a new method to integrate two recent lines of work:unsupervised induction of shallow semantics (e.g., semantic roles) andfactorization of relations in text and knowledge bases. Our model consists oftwo components: (1) an encoding component: a semantic role labeling model whichpredicts roles given a rich set of syntactic and lexical features; (2) areconstruction component: a tensor factorization model which relies on roles topredict argument fillers. When the components are estimated jointly to minimizeerrors in argument reconstruction, the induced roles largely correspond toroles defined in annotated resources. Our method performs on par with mostaccurate role induction methods on English, even though, unlike these previousapproaches, we do not incorporate any prior linguistic knowledge about thelanguage.
arxiv-1412-6514 | Score Function Features for Discriminative Learning |  http://arxiv.org/abs/1412.6514  | author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG stat.ML published:2014-12-19 summary:Feature learning forms the cornerstone for tackling challenging learningproblems in domains such as speech, computer vision and natural languageprocessing. In this paper, we consider a novel class of matrix andtensor-valued features, which can be pre-trained using unlabeled samples. Wepresent efficient algorithms for extracting discriminative information, giventhese pre-trained features and labeled samples for any related task. Our classof features are based on higher-order score functions, which capture localvariations in the probability density function of the input. We establish atheoretical framework to characterize the nature of discriminative informationthat can be extracted from score-function features, when used in conjunctionwith labeled samples. We employ efficient spectral decomposition algorithms (onmatrices and tensors) for extracting discriminative components. The advantageof employing tensor-valued features is that we can extract richerdiscriminative information in the form of an overcomplete representations.Thus, we present a novel framework for employing generative models of the inputfor discriminative learning.
arxiv-1412-6506 | Cauchy Principal Component Analysis |  http://arxiv.org/abs/1412.6506  | author:Pengtao Xie, Eric Xing category:cs.LG stat.ML published:2014-12-19 summary:Principal Component Analysis (PCA) has wide applications in machine learning,text mining and computer vision. Classical PCA based on a Gaussian noise modelis fragile to noise of large magnitude. Laplace noise assumption based PCAmethods cannot deal with dense noise effectively. In this paper, we proposeCauchy Principal Component Analysis (Cauchy PCA), a very simple yet effectivePCA method which is robust to various types of noise. We utilize Cauchydistribution to model noise and derive Cauchy PCA under the maximum likelihoodestimation (MLE) framework with low rank constraint. Our method can robustlyestimate the low rank matrix regardless of whether noise is large or small,dense or sparse. We analyze the robustness of Cauchy PCA from a robuststatistics view and present an efficient singular value projection optimizationmethod. Experimental results on both simulated data and real applicationsdemonstrate the robustness of Cauchy PCA to various noise patterns.
arxiv-1412-6493 | A la Carte - Learning Fast Kernels |  http://arxiv.org/abs/1412.6493  | author:Zichao Yang, Alexander J. Smola, Le Song, Andrew Gordon Wilson category:cs.LG stat.ML published:2014-12-19 summary:Kernel methods have great promise for learning rich statisticalrepresentations of large modern datasets. However, compared to neural networks,kernel methods have been perceived as lacking in scalability and flexibility.We introduce a family of fast, flexible, lightly parametrized and generalpurpose kernel learning methods, derived from Fastfood basis functionexpansions. We provide mechanisms to learn the properties of groups of spectralfrequencies in these expansions, which require only O(mlogd) time and O(m)memory, for m basis functions and d input dimensions. We show that the proposedmethods can learn a wide class of kernels, outperforming the alternatives inaccuracy, speed, and memory consumption.
arxiv-1412-6277 | N-gram-Based Low-Dimensional Representation for Document Classification |  http://arxiv.org/abs/1412.6277  | author:Rémi Lebret, Ronan Collobert category:cs.CL published:2014-12-19 summary:The bag-of-words (BOW) model is the common approach for classifyingdocuments, where words are used as feature for training a classifier. Thisgenerally involves a huge number of features. Some techniques, such as LatentSemantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have beendesigned to summarize documents in a lower dimension with the least semanticinformation loss. Some semantic information is nevertheless always lost, sinceonly words are considered. Instead, we aim at using information coming fromn-grams to overcome this limitation, while remaining in a low-dimension space.Many approaches, such as the Skip-gram model, provide good word vectorrepresentations very quickly. We propose to average these representations toobtain representations of n-grams. All n-grams are thus embedded in a samesemantic space. A K-means clustering can then group them into semanticconcepts. The number of features is therefore dramatically reduced anddocuments can be represented as bag of semantic concepts. We show that thismodel outperforms LSA and LDA on a sentiment classification task, and yieldssimilar results than a traditional BOW-model with far less features.
arxiv-1412-6279 | Non-parametric PSF estimation from celestial transit solar images using blind deconvolution |  http://arxiv.org/abs/1412.6279  | author:Adriana Gonzalez, Véronique Delouille, Laurent Jacques category:cs.CV astro-ph.SR published:2014-12-19 summary:Context: Characterization of instrumental effects in astronomical imaging isimportant in order to extract accurate physical information from theobservations. The measured image in a real optical instrument is usuallyrepresented by the convolution of an ideal image with a Point Spread Function(PSF). Additionally, the image acquisition process is also contaminated byother sources of noise (read-out, photon-counting). The problem of estimatingboth the PSF and a denoised image is called blind deconvolution and isill-posed. Aims: We propose a blind deconvolution scheme that relies on imageregularization. Contrarily to most methods presented in the literature, ourmethod does not assume a parametric model of the PSF and can thus be applied toany telescope. Methods: Our scheme uses a wavelet analysis prior model on the image and weakassumptions on the PSF. We use observations from a celestial transit, where theocculting body can be assumed to be a black disk. These constraints allow us toretain meaningful solutions for the filter and the image, eliminating trivial,translated and interchanged solutions. Under an additive Gaussian noiseassumption, they also enforce noise canceling and avoid reconstructionartifacts by promoting the whiteness of the residual between the blurredobservations and the cleaned data. Results: Our method is applied to synthetic and experimental data. The PSF isestimated for the SECCHI/EUVI instrument using the 2007 Lunar transit, and forSDO/AIA using the 2012 Venus transit. Results show that the proposednon-parametric blind deconvolution method is able to estimate the core of thePSF with a similar quality to parametric methods proposed in the literature. Wealso show that, if these parametric estimations are incorporated in theacquisition model, the resulting PSF outperforms both the parametric andnon-parametric methods.
arxiv-1412-6504 | Learning to Segment Moving Objects in Videos |  http://arxiv.org/abs/1412.6504  | author:Katerina Fragkiadaki, Pablo Arbelaez, Panna Felsen, Jitendra Malik category:cs.CV published:2014-12-19 summary:We segment moving objects in videos by ranking spatio-temporal segmentproposals according to "moving objectness": how likely they are to contain amoving object. In each video frame, we compute segment proposals using multiplefigure-ground segmentations on per frame motion boundaries. We rank them with aMoving Objectness Detector trained on image and motion fields to detect movingobjects and discard over/under segmentations or background parts of the scene.We extend the top ranked segments into spatio-temporal tubes using randomwalkers on motion affinities of dense point trajectories. Our final tuberanking consistently outperforms previous segmentation methods in the twolargest video segmentation benchmarks currently available, for any number ofproposals. Further, our per frame moving object proposals increase thedetection rate up to 7\% over previous state-of-the-art static proposalmethods.
arxiv-1412-6211 | Multiple Authors Detection: A Quantitative Analysis of Dream of the Red Chamber |  http://arxiv.org/abs/1412.6211  | author:Xianfeng Hu, Yang Wang, Qiang Wu category:cs.LG cs.CL published:2014-12-19 summary:Inspired by the authorship controversy of Dream of the Red Chamber and theapplication of machine learning in the study of literary stylometry, we developa rigorous new method for the mathematical analysis of authorship by testingfor a so-called chrono-divide in writing styles. Our method incorporates someof the latest advances in the study of authorship attribution, particularlytechniques from support vector machines. By introducing the notion of relativefrequency as a feature ranking metric our method proves to be highly effectiveand robust. Applying our method to the Cheng-Gao version of Dream of the Red Chamber hasled to convincing if not irrefutable evidence that the first $80$ chapters andthe last $40$ chapters of the book were written by two different authors.Furthermore, our analysis has unexpectedly provided strong support to thehypothesis that Chapter 67 was not the work of Cao Xueqin either. We have also tested our method to the other three Great Classical Novels inChinese. As expected no chrono-divides have been found. This provides furtherevidence of the robustness of our method.
arxiv-1412-6257 | Gradual training of deep denoising auto encoders |  http://arxiv.org/abs/1412.6257  | author:Alexander Kalmanovich, Gal Chechik category:cs.LG cs.NE published:2014-12-19 summary:Stacked denoising auto encoders (DAEs) are well known to learn useful deeprepresentations, which can be used to improve supervised training byinitializing a deep network. We investigate a training scheme of a deep DAE,where DAE layers are gradually added and keep adapting as additional layers areadded. We show that in the regime of mid-sized datasets, this gradual trainingprovides a small but consistent improvement over stacked training in bothreconstruction quality and classification error over stacked training on MNISTand CIFAR datasets.
arxiv-1412-6534 | Empirically Estimable Classification Bounds Based on a New Divergence Measure |  http://arxiv.org/abs/1412.6534  | author:Visar Berisha, Alan Wisler, Alfred O. Hero, Andreas Spanias category:cs.IT math.IT stat.ML published:2014-12-19 summary:Information divergence functions play a critical role in statistics andinformation theory. In this paper we show that a non-parametric f-divergencemeasure can be used to provide improved bounds on the minimum binaryclassification probability of error for the case when the training and testdata are drawn from the same distribution and for the case where there existssome mismatch between training and test distributions. We confirm thetheoretical results by designing feature selection algorithms using thecriteria from these bounds and by evaluating the algorithms on a series ofpathological speech classification tasks.
arxiv-1412-6464 | Simplified firefly algorithm for 2D image key-points search |  http://arxiv.org/abs/1412.6464  | author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana, Zbigniew Marszałek, Dawid Połap, Marcin Woźniak category:cs.NE cs.AI cs.CV published:2014-12-19 summary:In order to identify an object, human eyes firstly search the field of viewfor points or areas which have particular properties. These properties are usedto recognise an image or an object. Then this process could be taken as a modelto develop computer algorithms for images identification. This paper proposesthe idea of applying the simplified firefly algorithm to search for key-areasin 2D images. For a set of input test images the proposed version of fireflyalgorithm has been examined. Research results are presented and discussed toshow the efficiency of this evolutionary computation method.
arxiv-1412-6451 | Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer |  http://arxiv.org/abs/1412.6451  | author:Mark Wernsdorfer, Ute Schmid category:cs.LG cs.AI cs.RO published:2014-12-19 summary:Methods of deep machine learning enable to to reuse low-level representationsefficiently for generating more abstract high-level representations.Originally, deep learning has been applied passively (e.g., for classificationpurposes). Recently, it has been extended to estimate the value of actions forautonomous agents within the framework of reinforcement learning (RL). Explicitmodels of the environment can be learned to augment such a value function.Although "flat" connectionist methods have already been used for model-basedRL, up to now, only model-free variants of RL have been equipped with methodsfrom deep learning. We propose a variant of deep model-based RL that enables anagent to learn arbitrarily abstract hierarchical representations of itsenvironment. In this paper, we present research on how such hierarchicalrepresentations can be grounded in sensorimotor interaction between an agentand its environment.
arxiv-1412-6391 | Py3DFreeHandUS: a library for voxel-array reconstruction using Ultrasonography and attitude sensors |  http://arxiv.org/abs/1412.6391  | author:Davide Monari, Francesco Cenni, Erwin Aertbeliën, Kaat Desloovere category:cs.CV cs.CE published:2014-12-19 summary:In medical imaging, there is a growing interest to provide real-time imageswith good quality for large anatomical structures. To cope with this issue, wedeveloped a library that allows to replace, for some specific clinicalapplications, more robust systems such as Computer Tomography (CT) and MagneticResonance Imaging (MRI). Our python library Py3DFreeHandUS is a package forprocessing data acquired simultaneously by ultra-sonographic systems (US) andmarker-based optoelectronic systems. In particular, US data enables tovisualize subcutaneous body structures, whereas the optoelectronic system isable to collect the 3D position in space for reflective objects, that arecalled markers. By combining these two measurement devices, it is possible toreconstruct the real 3D morphology of body structures such as muscles, forrelevant clinical implications. In the present research work, the differentsteps which allow to obtain a relevant 3D data set as well as the proceduresfor calibrating the systems and for determining the quality of thereconstruction.
arxiv-1412-6388 | Distributed Decision Trees |  http://arxiv.org/abs/1412.6388  | author:Ozan İrsoy, Ethem Alpaydın category:cs.LG stat.ML published:2014-12-19 summary:Recently proposed budding tree is a decision tree algorithm in which everynode is part internal node and part leaf. This allows representing everydecision tree in a continuous parameter space, and therefore a budding tree canbe jointly trained with backpropagation, like a neural network. Even thoughthis continuity allows it to be used in hierarchical representation learning,the learned representations are local: Activation makes a soft selection amongall root-to-leaf paths in a tree. In this work we extend the budding tree andpropose the distributed tree where the children use different and independentsplits and hence multiple paths in a tree can be traversed at the same time.This ability to combine multiple paths gives the power of a distributedrepresentation, as in a traditional perceptron layer. We show that distributedtrees perform comparably or better than budding and traditional hard trees onclassification and regression tasks.
arxiv-1412-6558 | Random Walk Initialization for Training Very Deep Feedforward Networks |  http://arxiv.org/abs/1412.6558  | author:David Sussillo, L. F. Abbott category:cs.NE cs.LG stat.ML published:2014-12-19 summary:Training very deep networks is an important open problem in machine learning.One of many difficulties is that the norm of the back-propagated error gradientcan grow or decay exponentially. Here we show that training very deepfeed-forward networks (FFNs) is not as difficult as previously thought. Unlikewhen back-propagation is applied to a recurrent network, application to an FFNamounts to multiplying the error gradient by a different random matrix at eachlayer. We show that the successive application of correctly scaled randommatrices to an initial vector results in a random walk of the log of the normof the resulting vectors, and we compute the scaling that makes this walkunbiased. The variance of the random walk grows only linearly with networkdepth and is inversely proportional to the size of each layer. Practically,this implies a gradient whose log-norm scales with the square root of thenetwork depth and shows that the vanishing gradient problem can be mitigated byincreasing the width of the layers. Mathematical analyses and experimentalresults using stochastic gradient descent to optimize tasks related to theMNIST and TIMIT datasets are provided to support these claims. Equations forthe optimal matrix scaling are provided for the linear and ReLU cases.
arxiv-1412-6264 | Supertagging: Introduction, learning, and application |  http://arxiv.org/abs/1412.6264  | author:Taraka Rama K category:cs.CL published:2014-12-19 summary:Supertagging is an approach originally developed by Bangalore and Joshi(1999) to improve the parsing efficiency. In the beginning, the scholars usedsmall training datasets and somewhat na\"ive smoothing techniques to learn theprobability distributions of supertags. Since its inception, the applicabilityof Supertags has been explored for TAG (tree-adjoining grammar) formalism aswell as other related yet, different formalisms such as CCG. This article willtry to summarize the various chapters, relevant to statistical parsing, fromthe most recent edited book volume (Bangalore and Joshi, 2010). The chapterswere selected so as to blend the learning of supertags, its integration intofull-scale parsing, and in semantic parsing.
arxiv-1412-6296 | Generative Modeling of Convolutional Neural Networks |  http://arxiv.org/abs/1412.6296  | author:Jifeng Dai, Yang Lu, Ying-Nian Wu category:cs.CV cs.LG cs.NE published:2014-12-19 summary:The convolutional neural networks (CNNs) have proven to be a powerful toolfor discriminative learning. Recently researchers have also started to showinterest in the generative aspects of CNNs in order to gain a deeperunderstanding of what they have learned and how to further improve them. Thispaper investigates generative modeling of CNNs. The main contributions include:(1) We construct a generative model for the CNN in the form of exponentialtilting of a reference distribution. (2) We propose a generative gradient forpre-training CNNs by a non-parametric importance sampling scheme, which isfundamentally different from the commonly used discriminative gradient, and yethas the same computational architecture and cost as the latter. (3) We proposea generative visualization method for the CNNs by sampling from an explicitparametric image distribution. The proposed visualization method can directlydraw synthetic samples for any given node in a trained CNN by the HamiltonianMonte Carlo (HMC) algorithm, without resorting to any extra hold-out images.Experiments on the challenging ImageNet benchmark show that the proposedgenerative gradient pre-training consistently helps improve the performances ofCNNs, and the proposed generative visualization method generates meaningful andvaried samples of synthetic images from a large-scale deep CNN.
arxiv-1412-6285 | From dependency to causality: a machine learning approach |  http://arxiv.org/abs/1412.6285  | author:Gianluca Bontempi, Maxime Flauder category:cs.LG cs.AI stat.ML published:2014-12-19 summary:The relationship between statistical dependency and causality lies at theheart of all statistical approaches to causal inference. Recent results in theChaLearn cause-effect pair challenge have shown that causal directionality canbe inferred with good accuracy also in Markov indistinguishable configurationsthanks to data driven approaches. This paper proposes a supervised machinelearning approach to infer the existence of a directed causal link between twovariables in multivariate settings with $n>2$ variables. The approach relies onthe asymmetry of some conditional (in)dependence relations between the membersof the Markov blankets of two variables causally connected. Our results showthat supervised learning methods may be successfully used to extract causalinformation on the basis of asymmetric statistical descriptors also for $n>2$variate distributions.
arxiv-1412-6346 | Reverse Engineering Chemical Reaction Networks from Time Series Data |  http://arxiv.org/abs/1412.6346  | author:Dominic P. Searson, Mark J. Willis, Allen Wright category:cs.NE q-bio.MN published:2014-12-19 summary:The automated inference of physically interpretable (bio)chemical reactionnetwork models from measured experimental data is a challenging problem whosesolution has significant commercial and academic ramifications. It isdemonstrated, using simulations, how sets of elementary reactions comprisingchemical reaction networks, as well as their rate coefficients, may beaccurately recovered from non-equilibrium time series concentration data, suchas that obtained from laboratory scale reactors. A variant of an evolutionaryalgorithm called differential evolution in conjunction with least squarestechniques is used to search the space of reaction networks in order to inferboth the reaction network topology and its rate parameters. Properties of thestoichiometric matrices of trial networks are used to bias the search towardsphysically realisable solutions. No other information, such as chemicalcharacterisation of the reactive species is required, although where availableit may be used to improve the search process.
arxiv-1412-6537 | Fracking Deep Convolutional Image Descriptors |  http://arxiv.org/abs/1412.6537  | author:Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Francesc Moreno-Noguer category:cs.CV published:2014-12-19 summary:In this paper we propose a novel framework for learning local imagedescriptors in a discriminative manner. For this purpose we explore a siamesearchitecture of Deep Convolutional Neural Networks (CNN), with a Hingeembedding loss on the L2 distance between descriptors. Since a siamesearchitecture uses pairs rather than single image patches to train, there exista large number of positive samples and an exponential number of negativesamples. We propose to explore this space with a stochastic sampling of thetraining set, in combination with an aggressive mining strategy over both thepositive and negative samples which we denote as "fracking". We perform athorough evaluation of the architecture hyper-parameters, and demonstrate largeperformance gains compared to both standard CNN learning strategies,hand-crafted image descriptors like SIFT, and the state-of-the-art on learneddescriptors: up to 2.5x vs SIFT and 1.5x vs the state-of-the-art in terms ofthe area under the curve (AUC) of the Precision-Recall curve.
arxiv-1412-6515 | On distinguishability criteria for estimating generative models |  http://arxiv.org/abs/1412.6515  | author:Ian J. Goodfellow category:stat.ML published:2014-12-19 summary:Two recently introduced criteria for estimation of generative models are bothbased on a reduction to binary classification. Noise-contrastive estimation(NCE) is an estimation procedure in which a generative model is trained to beable to distinguish data samples from noise samples. Generative adversarialnetworks (GANs) are pairs of generator and discriminator networks, with thegenerator network learning to generate samples by attempting to fool thediscriminator network into believing its samples are real data. Both estimationprocedures use the same function to drive learning, which naturally raisesquestions about how they are related to each other, as well as whether thisfunction is related to maximum likelihood estimation (MLE). NCE corresponds totraining an internal data model belonging to the {\em discriminator} networkbut using a fixed generator network. We show that a variant of NCE, with adynamic generator network, is equivalent to maximum likelihood estimation.Since pairing a learned discriminator with an appropriate dynamically selectedgenerator recovers MLE, one might expect the reverse to hold for pairing alearned generator with a certain discriminator. However, we show thatrecovering MLE for a learned generator requires departing from thedistinguishability game. Specifically: (i) The expected gradient of the NCE discriminator can be made to match theexpected gradient of MLE, if one is allowed to use a non-stationary noise distribution for NCE, (ii) No choice of discriminator network can make the expected gradient forthe GAN generator match that of MLE, and (iii) The existing theory does not guarantee that GANs will converge in thenon-convex case. This suggests that the key next step in GAN research is to determine whetherGANs converge, and if not, to modify their training algorithm to forceconvergence.
arxiv-1412-6039 | Generative Deep Deconvolutional Learning |  http://arxiv.org/abs/1412.6039  | author:Yunchen Pu, Xin Yuan, Lawrence Carin category:stat.ML cs.LG published:2014-12-18 summary:A generative Bayesian model is developed for deep (multi-layer) convolutionaldictionary learning. A novel probabilistic pooling operation is integrated intothe deep model, yielding efficient bottom-up and top-down probabilisticlearning. After learning the deep convolutional dictionary, testing isimplemented via deconvolutional inference. To speed up this inference, a newstatistical approach is proposed to project the top-layer dictionary elementsto the data level. Following this, only one layer of deconvolution is requiredduring testing. Experimental results demonstrate powerful capabilities of themodel to learn multi-layer features from images. Excellent classificationresults are obtained on both the MNIST and Caltech 101 datasets.
arxiv-1412-5802 | Contour Detection Using Contrast Formulas in the Framework of Logarithmic Models |  http://arxiv.org/abs/1412.5802  | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:In this paper we use a new logarithmic model of image representation,developed in [1,2], for edge detection. In fact, in the framework of the newmodel we obtain the formulas for computing the "contrast of a pixel" and the"contrast" image is just the "contour" or edge image. In our setting the rangeof values is preserved and the quality of the contour is good for high as wellas for low luminosity regions. We present the comparison of our results withthe results using classical edge detection operators.
arxiv-1412-6071 | Fractional Max-Pooling |  http://arxiv.org/abs/1412.6071  | author:Benjamin Graham category:cs.CV published:2014-12-18 summary:Convolutional networks almost always incorporate some form of spatialpooling, and very often it is alpha times alpha max-pooling with alpha=2.Max-pooling act on the hidden layers of the network, reducing their size by aninteger multiplicative factor alpha. The amazing by-product of discarding 75%of your data is that you build into the network a degree of invariance withrespect to translations and elastic distortions. However, if you simplyalternate convolutional layers with max-pooling layers, performance is limiteddue to the rapid reduction in spatial size, and the disjoint nature of thepooling regions. We have formulated a fractional version of max-pooling wherealpha is allowed to take non-integer values. Our version of max-pooling isstochastic as there are lots of different ways of constructing suitable poolingregions. We find that our form of fractional max-pooling reduces overfitting ona variety of datasets: for instance, we improve on the state-of-the art forCIFAR-100 without even using dropout.
arxiv-1412-5796 | Image Enhancement Using a Generalization of Homographic Function |  http://arxiv.org/abs/1412.5796  | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:This paper presents a new method of gray level image enhancement, based onpoint transforms. In order to define the transform function, it was used ageneralization of the homographic function.
arxiv-1412-5862 | A theoretical basis for efficient computations with noisy spiking neurons |  http://arxiv.org/abs/1412.5862  | author:Zeno Jonke, Stefan Habenschuss, Wolfgang Maass category:cs.NE q-bio.NC 68Q10 published:2014-12-18 summary:Network of neurons in the brain apply - unlike processors in our currentgeneration of computer hardware - an event-based processing strategy, whereshort pulses (spikes) are emitted sparsely by neurons to signal the occurrenceof an event at a particular point in time. Such spike-based computationspromise to be substantially more power-efficient than traditional clockedprocessing schemes. However it turned out to be surprisingly difficult todesign networks of spiking neurons that are able to carry out demandingcomputations. We present here a new theoretical framework for organizingcomputations of networks of spiking neurons. In particular, we show that asuitable design enables them to solve hard constraint satisfaction problemsfrom the domains of planning - optimization and verification - logicalinference. The underlying design principles employ noise as a computationalresource. Nevertheless the timing of spikes (rather than just spike rates)plays an essential role in the resulting computations. Furthermore, one candemonstrate for the Traveling Salesman Problem a surprising computationaladvantage of networks of spiking neurons compared with traditional artificialneural networks and Gibbs sampling. The identification of such advantage hasbeen a well-known open problem.
arxiv-1412-5836 | Incorporating Both Distributional and Relational Semantics in Word Representations |  http://arxiv.org/abs/1412.5836  | author:Daniel Fried, Kevin Duh category:cs.CL published:2014-12-18 summary:We investigate the hypothesis that word representations ought to incorporateboth distributional and relational semantics. To this end, we employ theAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes adistributional objective on raw text and a relational objective on WordNet.Preliminary results on knowledge base completion, analogy tests, and parsingshow that word representations trained on both objectives can give improvementsin some cases.
arxiv-1412-5744 | Stochastic Descent Analysis of Representation Learning Algorithms |  http://arxiv.org/abs/1412.5744  | author:Richard M. Golden category:stat.ML cs.LG published:2014-12-18 summary:Although stochastic approximation learning methods have been widely used inthe machine learning literature for over 50 years, formal theoretical analysesof specific machine learning algorithms are less common because stochasticapproximation theorems typically possess assumptions which are difficult tocommunicate and verify. This paper presents a new stochastic approximationtheorem for state-dependent noise with easily verifiable assumptions applicableto the analysis and design of important deep learning algorithms including:adaptive learning, contrastive divergence learning, stochastic descentexpectation maximization, and active learning.
arxiv-1412-6092 | Image enhancement using the mean dynamic range maximization with logarithmic operations |  http://arxiv.org/abs/1412.6092  | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:In this paper we use a logarithmic model for gray level image enhancement. Webegin with a short presentation of the model and then, we propose a new formulafor the mean dynamic range. After that we present two image transforms: oneperforms an optimal enhancement of the mean dynamic range using the logarithmicaddition, and the other does the same for positive and negative values usingthe logarithmic scalar multiplication. We present the comparison of the resultsobtained by dynamic ranges optimization with the results obtained usingclassical image enhancement methods like gamma correction and histogramequalization.
arxiv-1412-5949 | Large Scale Distributed Distance Metric Learning |  http://arxiv.org/abs/1412.5949  | author:Pengtao Xie, Eric Xing category:cs.LG published:2014-12-18 summary:In large scale machine learning and data mining problems with high featuredimensionality, the Euclidean distance between data points can beuninformative, and Distance Metric Learning (DML) is often desired to learn aproper similarity measure (using side information such as example data pairsbeing similar or dissimilar). However, high dimensionality and large volume ofpairwise constraints in modern big data can lead to prohibitive computationalcost for both the original DML formulation in Xing et al. (2002) and laterextensions. In this paper, we present a distributed algorithm for DML, and alarge-scale implementation on a parameter server architecture. Our approachbuilds on a parallelizable reformulation of Xing et al. (2002), and anasynchronous stochastic gradient descent optimization procedure. To ourknowledge, this is the first distributed solution to DML, and we show that, ona system with 256 CPU cores, our program is able to complete a DML task on adataset with 1 million data points, 22-thousand features, and 200 millionlabeled data pairs, in 15 hours; and the learned metric shows greateffectiveness in properly measuring distances.
arxiv-1412-5787 | Gray Level Image Enhancement Using Polygonal Functions |  http://arxiv.org/abs/1412.5787  | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:This paper presents a method for enhancing the gray level images. This methodtakes part from the category of point transforms and it is based oninterpolation functions. The latter have a graphic represented by polygonallines. The interpolation nodes of these functions are calculated taking intoaccount the statistics of gray levels belonging to the image.
arxiv-1412-6163 | Automated Objective Surgical Skill Assessment in the Operating Room Using Unstructured Tool Motion |  http://arxiv.org/abs/1412.6163  | author:Piyush Poddar, Narges Ahmidi, S. Swaroop Vedula, Lisa Ishii, Gregory D. Hager, Masaru Ishii category:cs.CV published:2014-12-18 summary:Previous work on surgical skill assessment using intraoperative tool motionin the operating room (OR) has focused on highly-structured surgical tasks suchas cholecystectomy. Further, these methods only considered generic motionmetrics such as time and number of movements, which are of limited instructivevalue. In this paper, we developed and evaluated an automated approach to thesurgical skill assessment of nasal septoplasty in the OR. The obstructed fieldof view and highly unstructured nature of septoplasty precludes trainees fromefficiently learning the procedure. We propose a descriptive structure ofseptoplasty consisting of two types of activity: (1) brushing activity directedaway from the septum plane characterizing the consistency of the surgeon'swrist motion and (2) activity along the septal plane characterizing thesurgeon's coverage pattern. We derived features related to these two activitytypes that classify a surgeon's level of training with an average accuracy ofabout 72%. The features we developed provide surgeons with personalized,actionable feedback regarding their tool motion.
arxiv-1412-5903 | Deep Structured Output Learning for Unconstrained Text Recognition |  http://arxiv.org/abs/1412.5903  | author:Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-12-18 summary:We develop a representation suitable for the unconstrained recognition ofwords in natural images: the general case of no fixed lexicon and unknownlength. To this end we propose a convolutional neural network (CNN) basedarchitecture which incorporates a Conditional Random Field (CRF) graphicalmodel, taking the whole word image as a single input. The unaries of the CRFare provided by a CNN that predicts characters at each position of the output,while higher order terms are provided by another CNN that detects the presenceof N-grams. We show that this entire model (CRF, character predictor, N-grampredictor) can be jointly optimised by back-propagating the structured outputloss, essentially requiring the system to perform multi-task learning, andtraining uses purely synthetically generated data. The resulting model is amore accurate system on standard real-world text recognition benchmarks thancharacter prediction alone, setting a benchmark for systems that have not beentrained on a particular lexicon. In addition, our model achievesstate-of-the-art accuracy in lexicon-constrained scenarios, without beingspecifically modelled for constrained recognition. To test the generalisationof our model, we also perform experiments with random alpha-numeric strings toevaluate the method when no visual language model is applicable.
arxiv-1412-5769 | Gray level image enhancement using the Bernstein polynomials |  http://arxiv.org/abs/1412.5769  | author:Vasile Patrascu category:cs.CV published:2014-12-18 summary:This paper presents a method for enhancing the gray level images. Thispresented method takes part from the category of point operations and it isbased on piecewise linear functions. The interpolation nodes of these functionsare calculated using the Bernstein polynomials.
arxiv-1412-6134 | Data Representation using the Weyl Transform |  http://arxiv.org/abs/1412.6134  | author:Qiang Qiu, Andrew Thompson, Robert Calderbank, Guillermo Sapiro category:cs.CV stat.ML published:2014-12-18 summary:The Weyl transform is introduced as a rich framework for data representation.Transform coefficients are connected to the Walsh-Hadamard transform ofmultiscale autocorrelations, and different forms of dyadic periodicity in asignal are shown to appear as different features in its Weyl coefficients. TheWeyl transform has a high degree of symmetry with respect to a large group ofmultiscale transformations, which allows compact yet discriminativerepresentations to be obtained by pooling coefficients. The effectiveness ofthe Weyl transform is demonstrated through the example of textured imageclassification.
arxiv-1412-5764 | Image Dynamic Range Enhancement in the Context of Logarithmic Models |  http://arxiv.org/abs/1412.5764  | author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV published:2014-12-18 summary:Images of a scene observed under a variable illumination or with a variableoptical aperture are not identical. Does a privileged representant exist? Inwhich mathematical context? How to obtain it? The authors answer to suchquestions in the context of logarithmic models for images. After a shortpresentation of the model, the paper presents two image transforms: oneperforms an optimal enhancement of the dynamic range, and the other does thesame for the mean dynamic range. Experimental results are shown.
arxiv-1412-5732 | Dynamic Structure Embedded Online Multiple-Output Regression for Stream Data |  http://arxiv.org/abs/1412.5732  | author:Changsheng Li, Fan Wei, Weishan Dong, Qingshan Liu, Xiangfeng Wang, Xin Zhang category:cs.LG published:2014-12-18 summary:Online multiple-output regression is an important machine learning techniquefor modeling, predicting, and compressing multi-dimensional correlated datastreams. In this paper, we propose a novel online multiple-output regressionmethod, called MORES, for stream data. MORES can \emph{dynamically} learn thestructure of the coefficients change in each update step to facilitate themodel's continuous refinement. We observe that limited expressive ability ofthe regression model, especially in the preliminary stage of online update,often leads to the variables in the residual errors being dependent. In lightof this point, MORES intends to \emph{dynamically} learn and leverage thestructure of the residual errors to improve the prediction accuracy. Moreover,we define three statistical variables to \emph{exactly} represent all the seensamples for \emph{incrementally} calculating prediction loss in each onlineupdate round, which can avoid loading all the training data into memory forupdating model, and also effectively prevent drastic fluctuation of the modelin the presence of noise. Furthermore, we introduce a forgetting factor to setdifferent weights on samples so as to track the data streams' evolvingcharacteristics quickly from the latest samples. Experiments on one syntheticdataset and three real-world datasets validate the effectiveness of theproposed method. In addition, the update speed of MORES is at least 2000samples processed per second on the three real-world datasets, more than 15times faster than the state-of-the-art online learning algorithm.
arxiv-1412-6095 | Theoretical and Numerical Analysis of Approximate Dynamic Programming with Approximation Errors |  http://arxiv.org/abs/1412.6095  | author:Ali Heydari category:cs.SY cs.LG math.OC stat.ML published:2014-12-18 summary:This study is aimed at answering the famous question of how the approximationerrors at each iteration of Approximate Dynamic Programming (ADP) affect thequality of the final results considering the fact that errors at each iterationaffect the next iteration. To this goal, convergence of Value Iteration schemeof ADP for deterministic nonlinear optimal control problems with undiscountedcost functions is investigated while considering the errors existing inapproximating respective functions. The boundedness of the results around theoptimal solution is obtained based on quantities which are known in a generaloptimal control problem and assumptions which are verifiable. Moreover, sincethe presence of the approximation errors leads to the deviation of the resultsfrom optimality, sufficient conditions for stability of the system operated bythe result obtained after a finite number of value iterations, along with anestimation of its region of attraction, are derived in terms of a calculableupper bound of the control approximation error. Finally, the process ofimplementation of the method on an orbital maneuver problem is investigatedthrough which the assumptions made in the theoretical developments are verifiedand the sufficient conditions are applied for guaranteeing stability and nearoptimality.
arxiv-1412-6045 | A Simple and Efficient Method To Generate Word Sense Representations |  http://arxiv.org/abs/1412.6045  | author:Luis Nieto Piña, Richard Johansson category:cs.CL published:2014-12-18 summary:Distributed representations of words have boosted the performance of manyNatural Language Processing tasks. However, usually only one representation perword is obtained, not acknowledging the fact that some words have multiplemeanings. This has a negative effect on the individual word representations andthe language model as a whole. In this paper we present a simple model thatenables recent techniques for building word vectors to represent distinctsenses of polysemic words. In our assessment of this model we show that it isable to effectively discriminate between words' senses and to do so in acomputationally efficient manner.
arxiv-1412-5687 | Towards Open World Recognition |  http://arxiv.org/abs/1412.5687  | author:Abhijit Bendale, Terrance Boult category:cs.CV published:2014-12-18 summary:With the of advent rich classification models and high computational powervisual recognition systems have found many operational applications.Recognition in the real world poses multiple challenges that are not apparentin controlled lab environments. The datasets are dynamic and novel categoriesmust be continuously detected and then added. At prediction time, a trainedsystem has to deal with myriad unseen categories. Operational systems requireminimum down time, even to learn. To handle these operational issues, wepresent the problem of Open World recognition and formally define it. We provethat thresholding sums of monotonically decreasing functions of distances inlinearly transformed feature space can balance "open space risk" and empiricalrisk. Our theory extends existing algorithms for open world recognition. Wepresent a protocol for evaluation of open world recognition systems. We presentthe Nearest Non-Outlier (NNO) algorithm which evolves model efficiently, addingobject categories incrementally while detecting outliers and managing openspace risk. We perform experiments on the ImageNet dataset with 1.2M+ images tovalidate the effectiveness of our method on large scale visual recognitiontasks. NNO consistently yields superior results on open world recognition.
arxiv-1412-5967 | Tag-Aware Ordinal Sparse Factor Analysis for Learning and Content Analytics |  http://arxiv.org/abs/1412.5967  | author:Andrew S. Lan, Christoph Studer, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.LG published:2014-12-18 summary:Machine learning offers novel ways and means to design personalized learningsystems wherein each student's educational experience is customized in realtime depending on their background, learning goals, and performance to date.SPARse Factor Analysis (SPARFA) is a novel framework for machine learning-basedlearning analytics, which estimates a learner's knowledge of the conceptsunderlying a domain, and content analytics, which estimates the relationshipsamong a collection of questions and those concepts. SPARFA jointly learns theassociations among the questions and the concepts, learner concept knowledgeprofiles, and the underlying question difficulties, solely based on thecorrect/incorrect graded responses of a population of learners to a collectionof questions. In this paper, we extend the SPARFA framework significantly toenable: (i) the analysis of graded responses on an ordinal scale (partialcredit) rather than a binary scale (correct/incorrect); (ii) the exploitationof tags/labels for questions that partially describe the question{conceptassociations. The resulting Ordinal SPARFA-Tag framework greatly enhances theinterpretability of the estimated concepts. We demonstrate using realeducational data that Ordinal SPARFA-Tag outperforms both SPARFA and existingcollaborative filtering techniques in predicting missing learner responses.
arxiv-1412-5968 | Quantized Matrix Completion for Personalized Learning |  http://arxiv.org/abs/1412.5968  | author:Andrew S. Lan, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG published:2014-12-18 summary:The recently proposed SPARse Factor Analysis (SPARFA) framework forpersonalized learning performs factor analysis on ordinal or binary-valued(e.g., correct/incorrect) graded learner responses to questions. The underlyingfactors are termed "concepts" (or knowledge components) and are used forlearning analytics (LA), the estimation of learner concept-knowledge profiles,and for content analytics (CA), the estimation of question-concept associationsand question difficulties. While SPARFA is a powerful tool for LA and CA, itrequires a number of algorithm parameters (including the number of concepts),which are difficult to determine in practice. In this paper, we proposeSPARFA-Lite, a convex optimization-based method for LA that builds on matrixcompletion, which only requires a single algorithm parameter and enables us toautomatically identify the required number of concepts. Using a variety ofeducational datasets, we demonstrate that SPARFALite (i) achieves comparableperformance in predicting unobserved learner responses to existing methods,including item response theory (IRT) and SPARFA, and (ii) is computationallymore efficient.
arxiv-1412-5721 | An Algorithm for Online K-Means Clustering |  http://arxiv.org/abs/1412.5721  | author:Edo Liberty, Ram Sriharsha, Maxim Sviridenko category:cs.DS cs.LG published:2014-12-18 summary:This paper shows that one can be competitive with the k-means objective whileoperating online. In this model, the algorithm receives vectors v_1,...,v_n oneby one in an arbitrary order. For each vector the algorithm outputs a clusteridentifier before receiving the next one. Our online algorithm generates ~O(k)clusters whose k-means cost is ~O(W*). Here, W* is the optimal k-means costusing k clusters and ~O suppresses poly-logarithmic factors. We also show that,experimentally, it is not much worse than k-means++ while operating in astrictly more constrained computational model.
arxiv-1412-6056 | Unsupervised Learning of Spatiotemporally Coherent Metrics |  http://arxiv.org/abs/1412.6056  | author:Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun category:cs.CV published:2014-12-18 summary:Current state-of-the-art classification and detection algorithms rely onsupervised training. In this work we study unsupervised feature learning in thecontext of temporally coherent video data. We focus on feature learning fromunlabeled video data, using the assumption that adjacent video frames containsemantically similar information. This assumption is exploited to train aconvolutional pooling auto-encoder regularized by slowness and sparsity. Weestablish a connection between slow feature learning to metric learning andshow that the trained encoder can be used to define a more temporally andsemantically coherent metric.
arxiv-1412-6124 | Semantic Part Segmentation using Compositional Model combining Shape and Appearance |  http://arxiv.org/abs/1412.6124  | author:Jianyu Wang, Alan Yuille category:cs.CV published:2014-12-18 summary:In this paper, we study the problem of semantic part segmentation foranimals. This is more challenging than standard object detection, objectsegmentation and pose estimation tasks because semantic parts of animals oftenhave similar appearance and highly varying shapes. To tackle these challenges,we build a mixture of compositional models to represent the object boundary andthe boundaries of semantic parts. And we incorporate edge, appearance, andsemantic part cues into the compositional model. Given part-level segmentationannotation, we develop a novel algorithm to learn a mixture of compositionalmodels under various poses and viewpoints for certain animal classes.Furthermore, a linear complexity algorithm is offered for efficient inferenceof the compositional model using dynamic programming. We evaluate our methodfor horse and cow using a newly annotated dataset on Pascal VOC 2010 which haspixelwise part labels. Experimental results demonstrate the effectiveness ofour method.
arxiv-1412-6115 | Compressing Deep Convolutional Networks using Vector Quantization |  http://arxiv.org/abs/1412.6115  | author:Yunchao Gong, Liu Liu, Ming Yang, Lubomir Bourdev category:cs.CV cs.LG cs.NE published:2014-12-18 summary:Deep convolutional neural networks (CNN) has become the most promising methodfor object recognition, repeatedly demonstrating record breaking results forimage classification and object detection in recent years. However, a very deepCNN generally involves many layers with millions of parameters, making thestorage of the network model to be extremely large. This prohibits the usage ofdeep CNNs on resource limited hardware, especially cell phones or otherembedded devices. In this paper, we tackle this model storage issue byinvestigating information theoretical vector quantization methods forcompressing the parameters of CNNs. In particular, we have found in terms ofcompressing the most storage demanding dense connected layers, vectorquantization methods have a clear gain over existing matrix factorizationmethods. Simply applying k-means clustering to the weights or conductingproduct quantization can lead to a very good balance between model size andrecognition accuracy. For the 1000-category classification task in the ImageNetchallenge, we are able to achieve 16-24 times compression of the network withonly 1% loss of classification accuracy using the state-of-the-art CNN.
arxiv-1412-5710 | Multiobjective Optimization of Classifiers by Means of 3-D Convex Hull Based Evolutionary Algorithm |  http://arxiv.org/abs/1412.5710  | author:Jiaqi Zhao, Vitor Basto Fernandes, Licheng Jiao, Iryna Yevseyeva, Asep Maulana, Rui Li, Thomas Bäck, Michael T. M. Emmerich category:cs.NE cs.LG published:2014-12-18 summary:Finding a good classifier is a multiobjective optimization problem withdifferent error rates and the costs to be minimized. The receiver operatingcharacteristic is widely used in the machine learning community to analyze theperformance of parametric classifiers or sets of Pareto optimal classifiers. Inorder to directly compare two sets of classifiers the area (or volume) underthe convex hull can be used as a scalar indicator for the performance of a setof classifiers in receiver operating characteristic space. Recently, the convex hull based multiobjective genetic programming algorithmwas proposed and successfully applied to maximize the convex hull area forbinary classification problems. The contribution of this paper is to extendthis algorithm for dealing with higher dimensional problem formulations. Inparticular, we discuss problems where parsimony (or classifier complexity) isstated as a third objective and multi-class classification with three differenttrue classification rates to be maximized. The design of the algorithm proposed in this paper is inspired byindicator-based evolutionary algorithms, where first a performance indicatorfor a solution set is established and then a selection operator is designedthat complies with the performance indicator. In this case, the performanceindicator will be the volume under the convex hull. The algorithm is tested andanalyzed in a proof of concept study on different benchmarks that are designedfor measuring its capability to capture relevant parts of a convex hull. Further benchmark and application studies on email classification and featureselection round up the analysis and assess robustness and usefulness of the newalgorithm in real world settings.
arxiv-1412-5808 | Minimizing the Number of Matching Queries for Object Retrieval |  http://arxiv.org/abs/1412.5808  | author:Johannes Niedermayer, Peer Kröger category:cs.CV published:2014-12-18 summary:To increase the computational efficiency of interest-point based objectretrieval, researchers have put remarkable research efforts into improving theefficiency of kNN-based feature matching, pursuing to match thousands offeatures against a database within fractions of a second. However, due to thehigh-dimensional nature of image features that reduces the effectivity of indexstructures (curse of dimensionality), due to the vast amount of features storedin image databases (images are often represented by up to several thousandfeatures), this ultimate goal demanded to trade query runtimes for queryprecision. In this paper we address an approach complementary to indexing inorder to improve the runtimes of retrieval by querying only the most promisingkeypoint descriptors, as this affects matching runtimes linearly and cantherefore lead to increased efficiency. As this reduction of kNN queriesreduces the number of tentative correspondences, a loss of query precision isminimized by an additional image-level correspondence generation stage with acomputational performance independent of the underlying indexing structure. Weevaluate such an adaption of the standard recognition pipeline on a variety ofdatasets using both SIFT and state-of-the-art binary descriptors. Our resultssuggest that decreasing the number of queried descriptors does not necessarilyimply a reduction in the result quality as long as alternative ways ofincreasing query recall (by thoroughly selecting k) and MAP (using image-levelcorrespondence generation) are considered.
arxiv-1412-6093 | Learning Temporal Dependencies in Data Using a DBN-BLSTM |  http://arxiv.org/abs/1412.6093  | author:Kratarth Goel, Raunaq Vohra category:cs.LG cs.NE published:2014-12-18 summary:Since the advent of deep learning, it has been used to solve various problemsusing many different architectures. The application of such deep architecturesto auditory data is also not uncommon. However, these architectures do notalways adequately consider the temporal dependencies in data. We thus propose anew generic architecture called the Deep Belief Network - Bidirectional LongShort-Term Memory (DBN-BLSTM) network that models sequences by keeping track ofthe temporal information while enabling deep representations in the data. Wedemonstrate this new architecture by applying it to the task of musicgeneration and obtain state-of-the-art results.
arxiv-1412-6181 | Crypto-Nets: Neural Networks over Encrypted Data |  http://arxiv.org/abs/1412.6181  | author:Pengtao Xie, Misha Bilenko, Tom Finley, Ran Gilad-Bachrach, Kristin Lauter, Michael Naehrig category:cs.LG cs.CR cs.NE published:2014-12-18 summary:The problem we address is the following: how can a user employ a predictivemodel that is held by a third party, without compromising private information.For example, a hospital may wish to use a cloud service to predict thereadmission risk of a patient. However, due to regulations, the patient'smedical files cannot be revealed. The goal is to make an inference using themodel, without jeopardizing the accuracy of the prediction or the privacy ofthe data. To achieve high accuracy, we use neural networks, which have been shown tooutperform other learning models for many tasks. To achieve the privacyrequirements, we use homomorphic encryption in the following protocol: the dataowner encrypts the data and sends the ciphertexts to the third party to obtaina prediction from a trained model. The model operates on these ciphertexts andsends back the encrypted prediction. In this protocol, not only the dataremains private, even the values predicted are available only to the dataowner. Using homomorphic encryption and modifications to the activation functionsand training algorithms of neural networks, we show that it is protocol ispossible and may be feasible. This method paves the way to build a securecloud-based neural network prediction services without invading users' privacy.
arxiv-1412-5896 | On the Stability of Deep Networks |  http://arxiv.org/abs/1412.5896  | author:Raja Giryes, Guillermo Sapiro, Alex M. Bronstein category:stat.ML cs.IT cs.LG cs.NE math.IT math.MG published:2014-12-18 summary:In this work we study the properties of deep neural networks (DNN) withrandom weights. We formally prove that these networks perform adistance-preserving embedding of the data. Based on this we then drawconclusions on the size of the training data and the networks' structure. Alonger version of this paper with more results and details can be found in(Giryes et al., 2015). In particular, we formally prove in the longer versionthat DNN with random Gaussian weights perform a distance-preserving embeddingof the data, with a special treatment for in-class and out-of-class data.
arxiv-1412-6177 | Example Selection For Dictionary Learning |  http://arxiv.org/abs/1412.6177  | author:Tomoki Tsuchida, Garrison W. Cottrell category:cs.LG cs.AI stat.ML published:2014-12-18 summary:In unsupervised learning, an unbiased uniform sampling strategy is typicallyused, in order that the learned features faithfully encode the statisticalstructure of the training data. In this work, we explore whether active exampleselection strategies - algorithms that select which examples to use, based onthe current estimate of the features - can accelerate learning. Specifically,we investigate effects of heuristic and saliency-inspired selection algorithmson the dictionary learning task with sparse activations. We show that someselection algorithms do improve the speed of learning, and we speculate on whythey might work.
arxiv-1412-5758 | Decomposition-Based Domain Adaptation for Real-World Font Recognition |  http://arxiv.org/abs/1412.5758  | author:Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang category:cs.CV published:2014-12-18 summary:We present a domain adaption framework to address a domain mismatch betweensynthetic training and real-world testing data. We demonstrate our method on achallenging fine-grain classification problem: recognizing a font style from animage of text. In this task, it is very easy to generate lots of rendered fontexamples but very hard to obtain real-world labeled images. Thisreal-to-synthetic domain gap caused poor generalization to new real data inprevious font recognition methods (Chen et al. (2014)). In this paper, weintroduce a Convolutional Neural Network decomposition approach, leveraging alarge training corpus of synthetic data to obtain effective features forclassification. This is done using an adaptation technique based on a StackedConvolutional Auto-Encoder that exploits a large collection of unlabeledreal-world text images combined with synthetic data preprocessed in a specificway. The proposed DeepFont method achieves an accuracy of higher than 80%(top-5) on a new large labeled real-world dataset we collected.
arxiv-1412-5675 | Stabilizing Value Iteration with and without Approximation Errors |  http://arxiv.org/abs/1412.5675  | author:Ali Heydari category:cs.SY math.OC stat.ML published:2014-12-17 summary:Adaptive optimal control using value iteration (VI) initiated from astabilizing policy is theoretically analyzed in various aspects including thecontinuity of the result, the stability of the system operated using anysingle/constant resulting control policy, the stability of the system operatedusing the evolving/time-varying control policy, the convergence of thealgorithm, and the optimality of the limit function. Afterwards, the effect ofpresence of approximation errors in the involved function approximationprocesses is incorporated and another set of results for boundedness of theapproximate VI as well as stability of the system operated under the resultsfor both cases of applying a single policy or an evolving policy are derived. Afeature of the presented results is providing estimations of the region ofattraction so that if the initial condition is within the region, the wholetrajectory will remain inside it and hence, the function approximation resultswill be reliable.
arxiv-1412-5676 | Optimal Triggering of Networked Control Systems |  http://arxiv.org/abs/1412.5676  | author:Ali Heydari category:cs.SY math.OC stat.ML published:2014-12-17 summary:The problem of resource allocation of nonlinear networked control systems isinvestigated, where, unlike the well discussed case of triggering forstability, the objective is optimal triggering. An approximate dynamicprogramming approach is developed for solving problems with fixed final timesinitially and then it is extended to infinite horizon problems. Different casesincluding Zero-Order-Hold, Generalized Zero-Order-Hold, and stochastic networksare investigated. Afterwards, the developments are extended to the case ofproblems with unknown dynamics and a model-free scheme is presented forlearning the (approximate) optimal solution. After detailed analyses ofconvergence, optimality, and stability of the results, the performance of themethod is demonstrated through different numerical examples.
arxiv-1412-5384 | Representation of Evolutionary Algorithms in FPGA Cluster for Project of Large-Scale Networks |  http://arxiv.org/abs/1412.5384  | author:Andre B. Perina, Marcilyanne M. Gois, Paulo Matias, Joao M. P. Cardoso, Alexandre C. B. Delbem, Vanderlei Bonato category:cs.DC cs.NE published:2014-12-17 summary:Many problems are related to network projects, such as electric distribution,telecommunication and others. Most of them can be represented by graphs, whichmanipulate thousands or millions of nodes, becoming almost an impossible taskto obtain real-time solutions. Many efficient solutions use EvolutionaryAlgorithms (EA), where researches show that performance of EAs can besubstantially raised by using an appropriate representation, such as theNode-Depth Encoding (NDE). The objective of this work was to partition animplementation on single-FPGA (Field-Programmable Gate Array) based on NDE from512 nodes to a multi-FPGAs approach, expanding the system to 4096 nodes.
arxiv-1412-5404 | Word Network Topic Model: A Simple but General Solution for Short and Imbalanced Texts |  http://arxiv.org/abs/1412.5404  | author:Yuan Zuo, Jichang Zhao, Ke Xu category:cs.CL cs.IR published:2014-12-17 summary:The short text has been the prevalent format for information of Internet inrecent decades, especially with the development of online social media, whosemillions of users generate a vast number of short messages everyday. Althoughsophisticated signals delivered by the short text make it a promising sourcefor topic modeling, its extreme sparsity and imbalance brings unprecedentedchallenges to conventional topic models like LDA and its variants. Aiming atpresenting a simple but general solution for topic modeling in short texts, wepresent a word co-occurrence network based model named WNTM to tackle thesparsity and imbalance simultaneously. Different from previous approaches, WNTMmodels the distribution over topics for each word instead of learning topicsfor each document, which successfully enhance the semantic density of dataspace without importing too much time or space complexity. Meanwhile, the richcontextual information preserved in the word-word space also guarantees itssensitivity in identifying rare topics with convincing quality. Furthermore,employing the same Gibbs sampling with LDA makes WNTM easily to be extended tovarious application scenarios. Extensive validations on both short and normaltexts testify the outperformance of WNTM as compared to baseline methods. Andfinally we also demonstrate its potential in precisely discovering newlyemerging topics or unexpected events in Weibo at pretty early stages.
arxiv-1412-5448 | Extended Recommendation Framework: Generating the Text of a User Review as a Personalized Summary |  http://arxiv.org/abs/1412.5448  | author:Mickaël Poussevin, Vincent Guigue, Patrick Gallinari category:cs.IR cs.CL published:2014-12-17 summary:We propose to augment rating based recommender systems by providing the userwith additional information which might help him in his choice or in theunderstanding of the recommendation. We consider here as a new task, thegeneration of personalized reviews associated to items. We use an extractivesummary formulation for generating these reviews. We also show that the twoinformation sources, ratings and items could be used both for estimatingratings and for generating summaries, leading to improved performance for eachsystem compared to the use of a single source. Besides these two contributions,we show how a personalized polarity classifier can integrate the rating andtextual aspects. Overall, the proposed system offers the user threepersonalized hints for a recommendation: rating, text and polarity. We evaluatethese three components on two datasets using appropriate measures for eachtask.
arxiv-1412-5477 | Computational Model to Generate Case-Inflected Forms of Masculine Nouns for Word Search in Sanskrit E-Text |  http://arxiv.org/abs/1412.5477  | author:S V Kasmir Raja, V Rajitha, Lakshmanan Meenakshi category:cs.CL published:2014-12-17 summary:The problem of word search in Sanskrit is inseparable from complexities thatinclude those caused by euphonic conjunctions and case-inflections. Thecase-inflectional forms of a noun normally number 24 owing to the fact that inSanskrit there are eight cases and three numbers-singular, dual and plural. Thetraditional method of generating these inflectional forms is rather elaborateowing to the fact that there are differences in the forms generated betweeneven very similar words and there are subtle nuances involved. Further, itwould be a cumbersome exercise to generate and search for 24 forms of a wordduring a word search in a large text, using the currently availablecase-inflectional form generators. This study presents a new approach togenerating case-inflectional forms that is simpler to compute. Further, anoptimized model that is sufficient for generating only those word forms thatare required in a word search and is more than 80% efficient compared to thecomplete case-inflectional forms generator, is presented in this study for thefirst time.
arxiv-1412-5488 | Full-reference image quality assessment by combining global and local distortion measures |  http://arxiv.org/abs/1412.5488  | author:Ashirbani Saha, Q. M. Jonathan Wu category:cs.CV published:2014-12-17 summary:Full-reference image quality assessment (FR-IQA) techniques compare areference and a distorted/test image and predict the perceptual quality of thetest image in terms of a scalar value representing an objective score. Theevaluation of FR-IQA techniques is carried out by comparing the objectivescores from the techniques with the subjective scores (obtained from humanobservers) provided in the image databases used for the IQA. Hence, wereasonably assume that the goal of a human observer is to rate the distortionpresent in the test image. The goal oriented tasks are processed by the humanvisual system (HVS) through top-down processing which actively searches forlocal distortions driven by the goal. Therefore local distortion measures in animage are important for the top-down processing. At the same time, bottom-upprocessing also takes place signifying spontaneous visual functions in the HVS.To account for this, global perceptual features can be used. Therefore, wehypothesize that the resulting objective score for an image can be derived fromthe combination of local and global distortion measures calculated from thereference and test images. We calculate the local distortion by measuring thelocal correlation differences from the gradient and contrast information. Forglobal distortion, dissimilarity of the saliency maps computed from a bottom-upmodel of saliency is used. The motivation behind the proposed approach has beenthoroughly discussed, accompanied by an intuitive analysis. Finally,experiments are conducted in six benchmark databases suggesting theeffectiveness of the proposed approach that achieves competitive performancewith the state-of-the-art methods providing an improvement in the overallperformance.
arxiv-1412-5513 | Towards a constructive multilayer perceptron for regression task using non-parametric clustering. A case study of Photo-Z redshift reconstruction |  http://arxiv.org/abs/1412.5513  | author:Cyrine Arouri, Engelbert Mephu Nguifo, Sabeur Aridhi, Cécile Roucelle, Gaelle Bonnet-Loosli, Norbert Tsopzé category:cs.NE cs.AI published:2014-12-17 summary:The choice of architecture of artificial neuron network (ANN) is still achallenging task that users face every time. It greatly affects the accuracy ofthe built network. In fact there is no optimal method that is applicable tovarious implementations at the same time. In this paper we propose a method toconstruct ANN based on clustering, that resolves the problems of random and adhoc approaches for multilayer ANN architecture. Our method can be applied toregression problems. Experimental results obtained with different datasets,reveals the efficiency of our method.
arxiv-1502-05988 | Deep Learning for Multi-label Classification |  http://arxiv.org/abs/1502.05988  | author:Jesse Read, Fernando Perez-Cruz category:cs.LG cs.AI published:2014-12-17 summary:In multi-label classification, the main focus has been to develop ways oflearning the underlying dependencies between labels, and to take advantage ofthis at classification time. Developing better feature-space representationshas been predominantly employed to reduce complexity, e.g., by eliminatingnon-helpful feature attributes from the input space prior to (or during)training. This is an important task, since many multi-label methods typicallycreate many different copies or views of the same input data as they transformit, and considerable memory can be saved by taking advantage of redundancy. Inthis paper, we show that a proper development of the feature space can makelabels less interdependent and easier to model and predict at inference time.For this task we use a deep learning approach with restricted Boltzmannmachines. We present a deep network that, in an empirical evaluation,outperforms a number of competitive methods from the literature
arxiv-1412-5323 | Gene Similarity-based Approaches for Determining Core-Genes of Chloroplasts |  http://arxiv.org/abs/1412.5323  | author:Bassam AlKindy, Christophe Guyeux, Jean-François Couchot, Michel Salomon, Jacques M. Bahi category:cs.NE q-bio.GN published:2014-12-17 summary:In computational biology and bioinformatics, the manner to understandevolution processes within various related organisms paid a lot of attentionthese last decades. However, accurate methodologies are still needed todiscover genes content evolution. In a previous work, two novel approachesbased on sequence similarities and genes features have been proposed. Moreprecisely, we proposed to use genes names, sequence similarities, or both,insured either from NCBI or from DOGMA annotation tools. Dogma has theadvantage to be an up-to-date accurate automatic tool specifically designed forchloroplasts, whereas NCBI possesses high quality human curated genes (togetherwith wrongly annotated ones). The key idea of the former proposal was to takethe best from these two tools. However, the first proposal was limited by namevariations and spelling errors on the NCBI side, leading to core trees of lowquality. In this paper, these flaws are fixed by improving the comparison ofNCBI and DOGMA results, and by relaxing constraints on gene names while addinga stage of post-validation on gene sequences. The two stages of similaritymeasures, on names and sequences, are thus proposed for sequence clustering.This improves results that can be obtained using either NCBI or DOGMA alone.Results obtained with this quality control test are further investigated andcompared with previously released ones, on both computational and biologicalaspects, considering a set of 99 chloroplastic genomes.
arxiv-1412-5322 | An Algebraical Model for Gray Level Images |  http://arxiv.org/abs/1412.5322  | author:Vasile Patrascu category:cs.CV published:2014-12-17 summary:In this paper we propose a new algebraical model for the gray level images.It can be used for digital image processing. The model adresses to those imageswhich are generated in improper light conditions (very low or high level). Thevector space structure is able to illustrate some features into the image usingmodified level of contrast and luminosity. Also, the defined structure could beused in image enhancement. The general approach is presented with experimentalresults to demonstrate image enhancement.
arxiv-1412-5474 | Flattened Convolutional Neural Networks for Feedforward Acceleration |  http://arxiv.org/abs/1412.5474  | author:Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello category:cs.NE cs.LG published:2014-12-17 summary:We present flattened convolutional neural networks that are designed for fastfeedforward execution. The redundancy of the parameters, especially weights ofthe convolutional filters in convolutional neural networks has been extensivelystudied and different heuristics have been proposed to construct a low rankbasis of the filters after training. In this work, we train flattened networksthat consist of consecutive sequence of one-dimensional filters across alldirections in 3D space to obtain comparable performance as conventionalconvolutional networks. We tested flattened model on different datasets andfound that the flattened layer can effectively substitute for the 3D filterswithout loss of accuracy. The flattened convolution pipelines provide aroundtwo times speed-up during feedforward pass compared to the baseline model dueto the significant reduction of learning parameters. Furthermore, the proposedmethod does not require efforts in manual tuning or post processing once themodel is trained.
arxiv-1412-5617 | Learning from Data with Heterogeneous Noise using SGD |  http://arxiv.org/abs/1412.5617  | author:Shuang Song, Kamalika Chaudhuri, Anand D. Sarwate category:cs.LG published:2014-12-17 summary:We consider learning from data of variable quality that may be obtained fromdifferent heterogeneous sources. Addressing learning from heterogeneous data inits full generality is a challenging problem. In this paper, we adopt instead amodel in which data is observed through heterogeneous noise, where the noiselevel reflects the quality of the data source. We study how to use stochasticgradient algorithms to learn in this model. Our study is motivated by twoconcrete examples where this problem arises naturally: learning with localdifferential privacy based on data from multiple sources with different privacyrequirements, and learning from data with labels of variable quality. The main contribution of this paper is to identify how heterogeneous noiseimpacts performance. We show that given two datasets with heterogeneous noise,the order in which to use them in standard SGD depends on the learning rate. Wepropose a method for changing the learning rate as a function of theheterogeneity, and prove new regret bounds for our method in two cases ofinterest. Experiments on real data show that our method performs better thanusing a single learning rate and using only the less noisy of the two datasetswhen the noise level is low to moderate.
arxiv-1412-5627 | Feature extraction from complex networks: A case of study in genomic sequences classification |  http://arxiv.org/abs/1412.5627  | author:Bruno Mendes Moro Conque, André Yoshiaki Kashiwabara, Fabrício Martins Lopes category:cs.CE cs.LG q-bio.QM published:2014-12-17 summary:This work presents a new approach for classification of genomic sequencesfrom measurements of complex networks and information theory. For this, it isconsidered the nucleotides, dinucleotides and trinucleotides of a genomicsequence. For each of them, the entropy, sum entropy and maximum entropy valuesare calculated.For each of them is also generated a network, in which the nodesare the nucleotides, dinucleotides or trinucleotides and its edges areestimated by observing the respective adjacency among them in the genomicsequence. In this way, it is generated three networks, for which measures ofcomplex networks are extracted.These measures together with measures ofinformation theory comprise a feature vector representing a genomic sequence.Thus, the feature vector is used for classification by methods such as SVM,MultiLayer Perceptron, J48, IBK, Naive Bayes and Random Forest in order toevaluate the proposed approach.It was adopted coding sequences, intergenicsequences and TSS (Transcriptional Starter Sites) as datasets, for which thebetter results were obtained by the Random Forest with 91.2%, followed by J48with 89.1% and SVM with 84.8% of accuracy. These results indicate that the newapproach of feature extraction has its value, reaching good levels ofclassification even considering only the genomic sequences, i.e., no other apriori knowledge about them is considered.
arxiv-1412-5632 | Support recovery without incoherence: A case for nonconvex regularization |  http://arxiv.org/abs/1412.5632  | author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH 62F12 published:2014-12-17 summary:We demonstrate that the primal-dual witness proof method may be used toestablish variable selection consistency and $\ell_\infty$-bounds for sparseregression problems, even when the loss function and/or regularizer arenonconvex. Using this method, we derive two theorems concerning supportrecovery and $\ell_\infty$-guarantees for the regression estimator in a generalsetting. Our results provide rigorous theoretical justification for the use ofnonconvex regularization: For certain nonconvex regularizers with vanishingderivative away from the origin, support recovery consistency may be guaranteedwithout requiring the typical incoherence conditions present in $\ell_1$-basedmethods. We then derive several corollaries that illustrate the wideapplicability of our method to analyzing composite objective functionsinvolving losses such as least squares, nonconvex modified least squares forerrors-in variables linear regression, the negative log likelihood forgeneralized linear models, and the graphical Lasso. We conclude with empiricalstudies to corroborate our theoretical predictions.
arxiv-1412-5334 | The Affine Transforms for Image Enhancement in the Context of Logarithmic Models |  http://arxiv.org/abs/1412.5334  | author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV published:2014-12-17 summary:The logarithmic model offers new tools for image processing. An efficientmethod for image enhancement is to use an affine transformation with thelogarithmic operations: addition and scalar multiplication. We define somecriteria for automatically determining the parameters of the processing andthis is done via mean and variance computed by logarithmic operations.
arxiv-1412-5328 | A Mathematical Model for Logarithmic Image Processing |  http://arxiv.org/abs/1412.5328  | author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV published:2014-12-17 summary:In this paper, we propose a new mathematical model for image processing. Itis a logarithmical one. We consider the bounded interval (-1, 1) as the set ofgray levels. Firstly, we define two operations: addition <+> and real scalarmultiplication <x>. With these operations, the set of gray levels becomes areal vector space. Then, defining the scalar product (..) and the norm ., we obtain an Euclidean space of the gray levels. Secondly, we extend theseoperations and functions for color images. We finally show the effect ofvarious simple operations on an image.
arxiv-1412-5325 | Color Image Enhancement In the Framework of Logarithmic Models |  http://arxiv.org/abs/1412.5325  | author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV published:2014-12-17 summary:In this paper, we propose a mathematical model for color image processing. Itis a logarithmical one. We consider the cube (-1,1)x(-1,1)x(-1,1) as the set ofvalues for the color space. We define two operations: addition <+> and realscalar multiplication <x>. With these operations the space of colors becomes areal vector space. Then, defining the scalar product (..) and the norm ., we obtain a (logarithmic) Euclidean space. We show how we can use thismodel for color image enhancement and we present some experimental results.
arxiv-1412-5659 | Effective sampling for large-scale automated writing evaluation systems |  http://arxiv.org/abs/1412.5659  | author:Nicholas Dronen, Peter W. Foltz, Kyle Habermehl category:cs.CL cs.LG published:2014-12-17 summary:Automated writing evaluation (AWE) has been shown to be an effectivemechanism for quickly providing feedback to students. It has already seen wideadoption in enterprise-scale applications and is starting to be adopted inlarge-scale contexts. Training an AWE model has historically required a singlebatch of several hundred writing examples and human scores for each of them.This requirement limits large-scale adoption of AWE since human-scoring essaysis costly. Here we evaluate algorithms for ensuring that AWE models areconsistently trained using the most informative essays. Our results show how tominimize training set sizes while maximizing predictive performance, therebyreducing cost without unduly sacrificing accuracy. We conclude with adiscussion of how to integrate this approach into large-scale AWE systems.
arxiv-1412-5567 | Deep Speech: Scaling up end-to-end speech recognition |  http://arxiv.org/abs/1412.5567  | author:Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, Andrew Y. Ng category:cs.CL cs.LG cs.NE published:2014-12-17 summary:We present a state-of-the-art speech recognition system developed usingend-to-end deep learning. Our architecture is significantly simpler thantraditional speech systems, which rely on laboriously engineered processingpipelines; these traditional systems also tend to perform poorly when used innoisy environments. In contrast, our system does not need hand-designedcomponents to model background noise, reverberation, or speaker variation, butinstead directly learns a function that is robust to such effects. We do notneed a phoneme dictionary, nor even the concept of a "phoneme." Key to ourapproach is a well-optimized RNN training system that uses multiple GPUs, aswell as a set of novel data synthesis techniques that allow us to efficientlyobtain a large amount of varied data for training. Our system, called DeepSpeech, outperforms previously published results on the widely studiedSwitchboard Hub5'00, achieving 16.0% error on the full test set. Deep Speechalso handles challenging noisy environments better than widely used,state-of-the-art commercial speech systems.
arxiv-1412-5490 | High Frequency Content based Stimulus for Perceptual Sharpness Assessment in Natural Images |  http://arxiv.org/abs/1412.5490  | author:Ashirbani Saha, Q. M. Jonathan Wu category:cs.CV published:2014-12-17 summary:A blind approach to evaluate the perceptual sharpness present in a naturalimage is proposed. Though the literature demonstrates a set of variegatedvisual cues to detect or evaluate the absence or presence of sharpness, weemphasize in the current work that high frequency content and local standarddeviation can form strong features to compute perceived sharpness in anynatural image, and can be considered an able alternative for the existing cues.Unsharp areas in a natural image happen to exhibit uniform intensity or lack ofsharp changes between regions. Sharp region transitions in an image are causedby the presence of spatial high frequency content. Therefore, in the proposedapproach, we hypothesize that using the high frequency content as the principalstimulus, the perceived sharpness can be quantified in an image. When an imageis convolved with a high pass filter, higher values at any pixel locationsignify the presence of high frequency content at those locations. Consideringthese values as the stimulus, the exponent of the stimulus is weighted by localstandard deviation to impart the contribution of the local contrast within theformation of the sharpness map. The sharpness map highlights the relativelysharper regions in the image and is used to calculate the perceived sharpnessscore of the image. The advantages of the proposed method lie in its use ofsimple visual cues of high frequency content and local contrast to arrive atthe perceptual score, and requiring no training with the images. The promise ofthe proposed method is demonstrated by its ability to compute perceivedsharpness for within image and across image sharpness changes and for blindevaluation of perceptual degradation resulting due to presence of blur.Experiments conducted on several databases demonstrate improved performance ofthe proposed method over that of the state-of-the-art techniques.
arxiv-1412-5673 | Entity-Augmented Distributional Semantics for Discourse Relations |  http://arxiv.org/abs/1412.5673  | author:Yangfeng Ji, Jacob Eisenstein category:cs.CL cs.LG published:2014-12-17 summary:Discourse relations bind smaller linguistic elements into coherent texts.However, automatically identifying discourse relations is difficult, because itrequires understanding the semantics of the linked sentences. A more subtlechallenge is that it is not enough to represent the meaning of each sentence ofa discourse relation, because the relation may depend on links betweenlower-level elements, such as entity mentions. Our solution computesdistributional meaning representations by composition up the syntactic parsetree. A key difference from previous work on compositional distributionalsemantics is that we also compute representations for entity mentions, using anovel downward compositional pass. Discourse relations are predicted not onlyfrom the distributional representations of the sentences, but also of theircoreferent entity mentions. The resulting system obtains substantialimprovements over the previous state-of-the-art in predicting implicitdiscourse relations in the Penn Discourse Treebank.
arxiv-1412-5335 | Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews |  http://arxiv.org/abs/1412.5335  | author:Grégoire Mesnil, Tomas Mikolov, Marc'Aurelio Ranzato, Yoshua Bengio category:cs.CL cs.IR cs.LG cs.NE published:2014-12-17 summary:Sentiment analysis is a common task in natural language processing that aimsto detect polarity of a text document (typically a consumer review). In thesimplest settings, we discriminate only between positive and negativesentiment, turning the task into a standard binary classification problem. Wecompare several ma- chine learning approaches to this problem, and combine themto achieve the best possible results. We show how to use for this task thestandard generative lan- guage models, which are slightly complementary to thestate of the art techniques. We achieve strong results on a well-known datasetof IMDB movie reviews. Our results are easily reproducible, as we publish alsothe code needed to repeat the experiments. This should simplify further advanceof the state of the art, as other researchers can combine their techniques withours with little effort.
arxiv-1412-5236 | The supervised hierarchical Dirichlet process |  http://arxiv.org/abs/1412.5236  | author:Andrew M. Dai, Amos J. Storkey category:stat.ML cs.LG published:2014-12-17 summary:We propose the supervised hierarchical Dirichlet process (sHDP), anonparametric generative model for the joint distribution of a group ofobservations and a response variable directly associated with that whole group.We compare the sHDP with another leading method for regression on grouped data,the supervised latent Dirichlet allocation (sLDA) model. We evaluate our methodon two real-world classification problems and two real-world regressionproblems. Bayesian nonparametric regression models based on the Dirichletprocess, such as the Dirichlet process-generalised linear models (DP-GLM) havepreviously been explored; these models allow flexibility in modelling nonlinearrelationships. However, until now, Hierarchical Dirichlet Process (HDP)mixtures have not seen significant use in supervised problems with grouped datasince a straightforward application of the HDP on the grouped data results inlearnt clusters that are not predictive of the responses. The sHDP solves thisproblem by allowing for clusters to be learnt jointly from the group structureand from the label assigned to each group.
arxiv-1412-5275 | Iranian cashes recognition using mobile |  http://arxiv.org/abs/1412.5275  | author:Ismail Nojavani, Azade Rezaeezade, Amirhassan Monadjemi category:cs.CV published:2014-12-17 summary:In economical societies of today, using cash is an inseparable aspect ofhuman life. People use cashes for marketing, services, entertainments, bankoperations and so on. This huge amount of contact with cash and the necessityof knowing the monetary value of it caused one of the most challenging problemsfor visually impaired people. In this paper we propose a mobile phone basedapproach to identify monetary value of a picture taken from cashes using someimage processing and machine vision techniques. While the developed approach isvery fast, it can recognize the value of cash by average accuracy of about 95%and can overcome different challenges like rotation, scaling, collision,illumination changes, perspective, and some others.
arxiv-1412-5250 | Hierarchical Vector Autoregression |  http://arxiv.org/abs/1412.5250  | author:William B. Nicholson, Jacob Bien, David S. Matteson category:stat.ME stat.CO stat.ML published:2014-12-17 summary:Vector autoregression (VAR) is a fundamental tool for modeling the jointdynamics of multivariate time series. However, as the number of componentseries is increased, the VAR model quickly becomes overparameterized, makingreliable estimation difficult and impeding its adoption as a forecasting toolin high dimensional settings. A number of authors have sought to address thisissue by incorporating regularized approaches, such as the lasso, that imposesparse or low-rank structures on the estimated coefficient parameters of theVAR. More traditional approaches attempt to address overparameterization byselecting a low lag order, based on the assumption that dynamic dependenceamong components is short-range. However, these methods typically assume asingle, universal lag order that applies across all components, unnecessarilyconstraining the dynamic relationship between the components and impedingforecast performance. The lasso-based approaches are more flexible but do notincorporate the notion of lag order selection. We propose a new class of regularized VAR models, called hierarchical vectorautoregression (HVAR), that embed the notion of lag selection into a convexregularizer. The key convex modeling tool is a group lasso with nested groupswhich ensure the sparsity pattern of autoregressive lag coefficients honors theordered structure inherent to VAR. We provide computationally efficientalgorithms for solving HVAR problems that can be parallelized across thecomponents. A simulation study shows the improved performance in forecastingand lag order selection over previous approaches, and a macroeconomicapplication further highlights forecasting improvements as well as theconvenient, interpretable output of a HVAR model.
arxiv-1412-5244 | Learning unbiased features |  http://arxiv.org/abs/1412.5244  | author:Yujia Li, Kevin Swersky, Richard Zemel category:cs.LG cs.AI cs.NE stat.ML published:2014-12-17 summary:A key element in transfer learning is representation learning; ifrepresentations can be developed that expose the relevant factors underlyingthe data, then new tasks and domains can be learned readily based on mappingsof these salient factors. We propose that an important aim for theserepresentations are to be unbiased. Different forms of representation learningcan be derived from alternative definitions of unwanted bias, e.g., bias toparticular tasks, domains, or irrelevant underlying data dimensions. One veryuseful approach to estimating the amount of bias in a representation comes frommaximum mean discrepancy (MMD) [5], a measure of distance between probabilitydistributions. We are not the first to suggest that MMD can be a usefulcriterion in developing representations that apply across multiple domains ortasks [1]. However, in this paper we describe a number of novel applications ofthis criterion that we have devised, all based on the idea of developingunbiased representations. These formulations include: a standard domainadaptation framework; a method of learning invariant representations; anapproach based on noise-insensitive autoencoders; and a novel form ofgenerative model.
arxiv-1502-06434 | ANN Model to Predict Stock Prices at Stock Exchange Markets |  http://arxiv.org/abs/1502.06434  | author:B. W. Wanjawa, L. Muchemi category:q-fin.ST cs.CE cs.LG cs.NE published:2014-12-17 summary:Stock exchanges are considered major players in financial sectors of manycountries. Most Stockbrokers, who execute stock trade, use technical,fundamental or time series analysis in trying to predict stock prices, so as toadvise clients. However, these strategies do not usually guarantee good returnsbecause they guide on trends and not the most likely price. It is thereforenecessary to explore improved methods of prediction. The research proposes the use of Artificial Neural Network that isfeedforward multi-layer perceptron with error backpropagation and develops amodel of configuration 5:21:21:1 with 80% training data in 130,000 cycles. Theresearch develops a prototype and tests it on 2008-2012 data from stock marketse.g. Nairobi Securities Exchange and New York Stock Exchange, where predictionresults show MAPE of between 0.71% and 2.77%. Validation done with Encog andNeuroph realized comparable results. The model is thus capable of prediction ontypical stock markets.
arxiv-1412-5661 | DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection |  http://arxiv.org/abs/1412.5661  | author:Wanli Ouyang, Xiaogang Wang, Xingyu Zeng, Shi Qiu, Ping Luo, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Chen-Change Loy, Xiaoou Tang category:cs.CV cs.NE published:2014-12-17 summary:In this paper, we propose deformable deep convolutional neural networks forgeneric object detection. This new deep learning object detection framework hasinnovations in multiple aspects. In the proposed new deep architecture, a newdeformation constrained pooling (def-pooling) layer models the deformation ofobject parts with geometric constraint and penalty. A new pre-training strategyis proposed to learn feature representations more suitable for the objectdetection task and with good generalization capability. By changing the netstructures, training strategies, adding and removing some key components in thedetection pipeline, a set of models with large diversity are obtained, whichsignificantly improves the effectiveness of model averaging. The proposedapproach improves the mean averaged precision obtained by RCNN\cite{girshick2014rich}, which was the state-of-the-art, from 31\% to 50.3\% onthe ILSVRC2014 detection test set. It also outperforms the winner ofILSVRC2014, GoogLeNet, by 6.1\%. Detailed component-wise analysis is alsoprovided through extensive experimental evaluation, which provide a global viewfor people to understand the deep learning object detection pipeline.
arxiv-1412-5272 | Consistency Analysis of an Empirical Minimum Error Entropy Algorithm |  http://arxiv.org/abs/1412.5272  | author:Jun Fan, Ting Hu, Qiang Wu, Ding-Xuan Zhou category:cs.LG stat.ML published:2014-12-17 summary:In this paper we study the consistency of an empirical minimum error entropy(MEE) algorithm in a regression setting. We introduce two types of consistency.The error entropy consistency, which requires the error entropy of the learnedfunction to approximate the minimum error entropy, is shown to be always trueif the bandwidth parameter tends to 0 at an appropriate rate. The regressionconsistency, which requires the learned function to approximate the regressionfunction, however, is a complicated issue. We prove that the error entropyconsistency implies the regression consistency for homoskedastic models wherethe noise is independent of the input variable. But for heteroskedastic models,a counterexample is used to show that the two types of consistency do notcoincide. A surprising result is that the regression consistency is alwaystrue, provided that the bandwidth parameter tends to infinity at an appropriaterate. Regression consistency of two classes of special models is shown to holdwith fixed bandwidth parameter, which further illustrates the complexity ofregression consistency of MEE. Fourier transform plays crucial roles in ouranalysis.
arxiv-1412-4944 | Efficient GPU Implementation for Single Block Orthogonal Dictionary Learning |  http://arxiv.org/abs/1412.4944  | author:Paul Irofti category:cs.CV cs.DC published:2014-12-16 summary:Dictionary training for sparse representations involves dealing with largechunks of data and complex algorithms that determine time consumingimplementations. SBO is an iterative dictionary learning algorithm based onconstructing unions of orthonormal bases via singular value decomposition, thatrepresents each data item through a single best fit orthobase. In this paper wepresent a GPGPU approach of implementing SBO in OpenCL. We provide a lock-freesolution that ensures full-occupancy of the GPU by following the map-reducemodel for the sparse-coding stage and by making use of the Partitioned GlobalAddress Space (PGAS) model for developing parallel dictionary updates. Theresulting implementation achieves a favourable trade-off between algorithmcomplexity and data representation quality compared to PAK-SVD which is thestandard overcomplete dictionary learning approach. We present and discussnumerical results showing a significant acceleration of the execution time forthe dictionary learning process.
arxiv-1412-5104 | Locally Scale-Invariant Convolutional Neural Networks |  http://arxiv.org/abs/1412.5104  | author:Angjoo Kanazawa, Abhishek Sharma, David Jacobs category:cs.CV cs.LG cs.NE published:2014-12-16 summary:Convolutional Neural Networks (ConvNets) have shown excellent results on manyvisual classification tasks. With the exception of ImageNet, these datasets arecarefully crafted such that objects are well-aligned at similar scales.Naturally, the feature learning problem gets more challenging as the amount ofvariation in the data increases, as the models have to learn to be invariant tocertain changes in appearance. Recent results on the ImageNet dataset show thatgiven enough data, ConvNets can learn such invariances producing verydiscriminative features [1]. But could we do more: use less parameters, lessdata, learn more discriminative features, if certain invariances were builtinto the learning process? In this paper we present a simple model that allowsConvNets to learn features in a locally scale-invariant manner withoutincreasing the number of model parameters. We show on a modified MNIST datasetthat when faced with scale variation, building in scale-invariance allowsConvNets to learn more discriminative features with reduced chances ofover-fitting.
arxiv-1412-4967 | Sparse, guided feature connections in an Abstract Deep Network |  http://arxiv.org/abs/1412.4967  | author:Anthony Knittel, Alan Blair category:cs.NE published:2014-12-16 summary:We present a technique for developing a network of re-used features, wherethe topology is formed using a coarse learning method, that allowsgradient-descent fine tuning, known as an Abstract Deep Network (ADN). Newfeatures are built based on observed co-occurrences, and the network ismaintained using a selection process related to evolutionary algorithms. Thisallows coarse ex- ploration of the problem space, effective for irregulardomains, while gradient descent allows pre- cise solutions. Accuracy onstandard UCI and Protein-Structure Prediction problems is comparable withbenchmark SVM and optimized GBML approaches, and shows scalability foraddressing large problems. The discrete implementation is symbolic, allowinginterpretability, while the continuous method using fine-tuning shows improvedaccuracy. The binary multiplexer problem is explored, as an irregular domainthat does not support gradient descent learning, showing solution to the bench-mark 135-bit problem. A convolutional implementation is demonstrated on imageclassification, showing an error-rate of 0.79% on the MNIST problem, without apre-defined topology. The ADN system provides a method for developing a verysparse, deep feature topology, based on observed relationships betweenfeatures, that is able to find solutions in irregular domains, and initialize anetwork prior to gradient descent learning.
arxiv-1412-4986 | A Scalable Asynchronous Distributed Algorithm for Topic Modeling |  http://arxiv.org/abs/1412.4986  | author:Hsiang-Fu Yu, Cho-Jui Hsieh, Hyokun Yun, S. V. N Vishwanathan, Inderjit S. Dhillon category:cs.DC cs.IR cs.LG published:2014-12-16 summary:Learning meaningful topic models with massive document collections whichcontain millions of documents and billions of tokens is challenging because oftwo reasons: First, one needs to deal with a large number of topics (typicallyin the order of thousands). Second, one needs a scalable and efficient way ofdistributing the computation across multiple machines. In this paper we presenta novel algorithm F+Nomad LDA which simultaneously tackles both these problems.In order to handle large number of topics we use an appropriately modifiedFenwick tree. This data structure allows us to sample from a multinomialdistribution over $T$ items in $O(\log T)$ time. Moreover, when topic countschange the data structure can be updated in $O(\log T)$ time. In order todistribute the computation across multiple processor we present a novelasynchronous framework inspired by the Nomad algorithm of\cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperformstate-of-the-art on massive problems which involve millions of documents,billions of words, and thousands of topics.
arxiv-1412-5067 | Analysis of Optimal Recombination in Genetic Algorithm for a Scheduling Problem with Setups |  http://arxiv.org/abs/1412.5067  | author:A. V. Eremeev, Ju. V. Kovalenko category:cs.NE published:2014-12-16 summary:In this paper, we perform an experimental study of optimal recombinationoperator for makespan minimization problem on single machine withsequence-dependent setup times ($1s_{vu}C_{\max}$). The computationalexperiment on benchmark problems from TSPLIB library indicates practicalapplicability of optimal recombination in crossover operator of geneticalgorithm for $1s_{vu}C_{\max}$.
arxiv-1412-4940 | Discovering beautiful attributes for aesthetic image analysis |  http://arxiv.org/abs/1412.4940  | author:Luca Marchesotti, Naila Murray, Florent Perronnin category:cs.CV published:2014-12-16 summary:Aesthetic image analysis is the study and assessment of the aestheticproperties of images. Current computational approaches to aesthetic imageanalysis either provide accurate or interpretable results. To obtain bothaccuracy and interpretability by humans, we advocate the use of learned andnameable visual attributes as mid-level features. For this purpose, we proposeto discover and learn the visual appearance of attributes automatically, usinga recently introduced database, called AVA, which contains more than 250,000images together with their aesthetic scores and textual comments given byphotography enthusiasts. We provide a detailed analysis of these annotations aswell as the context in which they were given. We then describe how these threekey components of AVA - images, scores, and comments - can be effectivelyleveraged to learn visual attributes. Lastly, we show that these learnedattributes can be successfully used in three applications: aesthetic qualityprediction, image tagging and retrieval.
arxiv-1412-5158 | Testing and Confidence Intervals for High Dimensional Proportional Hazards Model |  http://arxiv.org/abs/1412.5158  | author:Ethan X. Fang, Yang Ning, Han Liu category:stat.ML math.ST stat.TH published:2014-12-16 summary:This paper proposes a decorrelation-based approach to test hypotheses andconstruct confidence intervals for the low dimensional component of highdimensional proportional hazards models. Motivated by the geometric projectionprinciple, we propose new decorrelated score, Wald and partial likelihood ratiostatistics. Without assuming model selection consistency, we prove theasymptotic normality of these test statistics, establish their semiparametricoptimality. We also develop new procedures for constructing pointwiseconfidence intervals for the baseline hazard function and baseline survivalfunction. Thorough numerical results are provided to back up our theory.
arxiv-1412-5126 | A Robust Regression Approach for Background/Foreground Segmentation |  http://arxiv.org/abs/1412.5126  | author:Shervin Minaee, Haoping Yu, Yao Wang category:cs.CV published:2014-12-16 summary:Background/foreground segmentation has a lot of applications in image andvideo processing. In this paper, a segmentation algorithm is proposed which ismainly designed for text and line extraction in screen content. The proposedmethod makes use of the fact that the background in each block is usuallysmoothly varying and can be modeled well by a linear combination of a fewsmoothly varying basis functions, while the foreground text and graphics createsharp discontinuity. The algorithm separates the background and foregroundpixels by trying to fit pixel values in the block into a smooth function usinga robust regression method. The inlier pixels that can fit well will beconsidered as background, while remaining outlier pixels will be consideredforeground. This algorithm has been extensively tested on several images fromHEVC standard test sequences for screen content coding, and is shown to havesuperior performance over other methods, such as the k-means clustering basedsegmentation algorithm in DjVu. This background/foreground segmentation can beused in different applications such as: text extraction, separate coding ofbackground and foreground for compression of screen content and mixed contentdocuments, principle line extraction from palmprint and crease detection infingerprint images.
arxiv-1412-5059 | Estimation of Large Covariance and Precision Matrices from Temporally Dependent Observations |  http://arxiv.org/abs/1412.5059  | author:Hai Shu, Bin Nan category:math.ST stat.ML stat.TH published:2014-12-16 summary:We consider the estimation of large covariance and precision matrices fromhigh-dimensional sub-Gaussian observations with slowly decaying temporaldependence that is bounded by certain polynomial decay rate. The temporaldependence is allowed to be long-range so with longer memory than thoseconsidered in the current literature. The rates of convergence are obtained forthe generalized thresholding estimation of covariance and correlation matrices,and for the constrained $\ell_1$ minimization and the $\ell_1$ penalizedlikelihood estimation of precision matrix. Properties of sparsistency andsign-consistency are also established. A gap-block cross-validation method isproposed for the tuning parameter selection, which performs well insimulations. As our motivating example, we study the brain functionalconnectivity using resting-state fMRI time series data with long-range temporaldependence.
arxiv-1412-5212 | Application of Topic Models to Judgments from Public Procurement Domain |  http://arxiv.org/abs/1412.5212  | author:Michał Łopuszyński category:cs.CL published:2014-12-16 summary:In this work, automatic analysis of themes contained in a large corpora ofjudgments from public procurement domain is performed. The employed techniqueis unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed,to use LDA in conjunction with recently developed method of unsupervisedkeyword extraction. Such an approach improves the interpretability of theautomatically obtained topics and allows for better computational performance.The described analysis illustrates a potential of the method in detectingrecurring themes and discovering temporal trends in lodged contract appeals.These results may be in future applied to improve information retrieval fromrepositories of legal texts or as auxiliary material for legal analyses carriedout by human experts.
arxiv-1412-4864 | Learning with Pseudo-Ensembles |  http://arxiv.org/abs/1412.4864  | author:Philip Bachman, Ouais Alsharif, Doina Precup category:stat.ML cs.LG cs.NE published:2014-12-16 summary:We formalize the notion of a pseudo-ensemble, a (possibly infinite)collection of child models spawned from a parent model by perturbing itaccording to some noise process. E.g., dropout (Hinton et. al, 2012) in a deepneural network trains a pseudo-ensemble of child subnetworks generated byrandomly masking nodes in the parent network. We present a novel regularizerbased on making the behavior of a pseudo-ensemble robust with respect to thenoise process generating it. In the fully-supervised setting, our regularizermatches the performance of dropout. But, unlike dropout, our regularizernaturally extends to the semi-supervised setting, where it producesstate-of-the-art results. We provide a case study in which we transform theRecursive Neural Tensor Network of (Socher et. al, 2013) into apseudo-ensemble, which significantly improves its performance on a real-worldsentiment analysis benchmark.
arxiv-1412-5083 | Random Forests Can Hash |  http://arxiv.org/abs/1412.5083  | author:Qiang Qiu, Guillermo Sapiro, Alex Bronstein category:cs.CV cs.IR cs.LG stat.ML published:2014-12-16 summary:Hash codes are a very efficient data representation needed to be able to copewith the ever growing amounts of data. We introduce a random forest semantichashing scheme with information-theoretic code aggregation, showing for thefirst time how random forest, a technique that together with deep learning haveshown spectacular results in classification, can also be extended tolarge-scale retrieval. Traditional random forest fails to enforce theconsistency of hashes generated from each tree for the same class data, i.e.,to preserve the underlying similarity, and it also lacks a principled way forcode aggregation across trees. We start with a simple hashing scheme, whereindependently trained random trees in a forest are acting as hashing functions.We the propose a subspace model as the splitting function, and show that itenforces the hash consistency in a tree for data from the same class. We alsointroduce an information-theoretic approach for aggregating codes of individualtrees into a single hash code, producing a near-optimal unique hash for eachclass. Experiments on large-scale public datasets are presented, showing thatthe proposed approach significantly outperforms state-of-the-art hashingmethods for retrieval tasks.
arxiv-1412-4863 | Max-Margin based Discriminative Feature Learning |  http://arxiv.org/abs/1412.4863  | author:Changsheng Li, Qingshan Liu, Weishan Dong, Xin Zhang, Lin Yang category:cs.LG published:2014-12-16 summary:In this paper, we propose a new max-margin based discriminative featurelearning method. Specifically, we aim at learning a low-dimensional featurerepresentation, so as to maximize the global margin of the data and make thesamples from the same class as close as possible. In order to enhance therobustness to noise, a $l_{2,1}$ norm constraint is introduced to make thetransformation matrix in group sparsity. In addition, for multi-classclassification tasks, we further intend to learn and leverage the correlationrelationships among multiple class tasks for assisting in learningdiscriminative features. The experimental results demonstrate the power of theproposed method against the related state-of-the-art methods.
arxiv-1412-4930 | Rehabilitation of Count-based Models for Word Vector Representations |  http://arxiv.org/abs/1412.4930  | author:Rémi Lebret, Ronan Collobert category:cs.CL published:2014-12-16 summary:Recent works on word representations mostly rely on predictive models.Distributed word representations (aka word embeddings) are trained to optimallypredict the contexts in which the corresponding words tend to appear. Suchmodels have succeeded in capturing word similarties as well as semantic andsyntactic regularities. Instead, we aim at reviving interest in a model basedon counts. We present a systematic study of the use of the Hellinger distanceto extract semantic representations from the word co-occurence statistics oflarge text corpora. We show that this distance gives good performance on wordsimilarity and analogy tasks, with a proper type and size of context, and adimensionality reduction based on a stochastic low-rank approximation. Besidesbeing both simple and intuitive, this method also provides an encoding functionwhich can be used to infer unseen words or phrases. This becomes a clearadvantage compared to predictive models which must train these new words.
arxiv-1412-7012 | Boltzmann-machine learning of prior distributions of binarized natural images |  http://arxiv.org/abs/1412.7012  | author:Tomoyuki Obuchi, Hirokazu Koma, Muneki Yasuda category:stat.ML cs.CV published:2014-12-16 summary:Prior distributions of binarized natural images are learned by usingBoltzmann machine. We find that there emerges a structure with two sublatticesin the interactions, and the nearest-neighbor and next-nearest-neighborinteractions correspondingly take two discriminative values, which reflectsindividual characteristics of three sets of pictures we treat. On the otherhand, in a longer spacial scale, a longer-range (though still rapidly-decaying)ferromagnetic interaction commonly appear in all the cases. The characteristiclength scale of the interactions is universally about up to four latticespacing $\xi \approx 4$. These results are derived by using the mean-fieldmethod which effectively reduces the computational time required in Boltzmannmachine. An improved mean-field method called the Bethe approximation alsogives the same result, which reinforces the validity of our analysis andfindings. Relations to criticality, frustration, and simple-cell receptivefields are also discussed.
arxiv-1412-5218 | Testing MCMC code |  http://arxiv.org/abs/1412.5218  | author:Roger B. Grosse, David K. Duvenaud category:cs.SE cs.LG stat.ML published:2014-12-16 summary:Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilisticmodeling and inference, but are difficult to debug, and are prone to silentfailure if implemented naively. We outline several strategies for testing thecorrectness of MCMC algorithms. Specifically, we advocate writing code in amodular way, where conditional probability calculations are kept separate fromthe logic of the sampler. We discuss strategies for both unit testing andintegration testing. As a running example, we show how a Python implementationof Gibbs sampling for a mixture of Gaussians model can be tested.
arxiv-1412-4846 | Scaling laws in human speech, decreasing emergence of new words and a generalized model |  http://arxiv.org/abs/1412.4846  | author:Ruokuang Lin, Qianli D. Y. Ma, Chunhua Bian category:cs.CL published:2014-12-16 summary:Human language, as a typical complex system, its organization and evolutionis an attractive topic for both physical and cultural researchers. In thispaper, we present the first exhaustive analysis of the text organization ofhuman speech. Two important results are that: (i) the construction andorganization of spoken language can be characterized as Zipf's law and Heaps'law, as observed in written texts; (ii) word frequency vs. rank distributionand the growth of distinct words with the increase of text length showssignificant differences between book and speech. In speech word frequencydistribution are more concentrated on higher frequency words, and the emergenceof new words decreases much rapidly when the content length grows. Based onthese observations, a new generalized model is proposed to explain thesecomplex dynamical behaviors and the differences between speech and book.
arxiv-1412-4682 | Rule-based Emotion Detection on Social Media: Putting Tweets on Plutchik's Wheel |  http://arxiv.org/abs/1412.4682  | author:Erik Tromp, Mykola Pechenizkiy category:cs.CL published:2014-12-15 summary:We study sentiment analysis beyond the typical granularity of polarity andinstead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as anextension to the Rule-Based Emission Model algorithm to deduce such emotionsfrom human-written messages. We evaluate our approach on two different datasetsand compare its performance with the current state-of-the-art techniques foremotion detection, including a recursive auto-encoder. The results of theexperimental study suggest that RBEM-Emo is a promising approach advancing thecurrent state-of-the-art in emotion detection.
arxiv-1412-4616 | A Broadcast News Corpus for Evaluation and Tuning of German LVCSR Systems |  http://arxiv.org/abs/1412.4616  | author:Felix Weninger, Björn Schuller, Florian Eyben, Martin Wöllmer, Gerhard Rigoll category:cs.CL cs.SD published:2014-12-15 summary:Transcription of broadcast news is an interesting and challenging applicationfor large-vocabulary continuous speech recognition (LVCSR). We present indetail the structure of a manually segmented and annotated corpus includingover 160 hours of German broadcast news, and propose it as an evaluationframework of LVCSR systems. We show our own experimental results on the corpus,achieved with a state-of-the-art LVCSR decoder, measuring the effect ofdifferent feature sets and decoding parameters, and thereby demonstrate thatreal-time decoding of our test set is feasible on a desktop PC at 9.2% worderror rate.
arxiv-1412-6061 | CITlab ARGUS for Arabic Handwriting |  http://arxiv.org/abs/1412.6061  | author:Gundram Leifert, Roger Labahn, Tobias Strauß category:cs.CV cs.NE 68T10, 68T05 published:2014-12-15 summary:In the recent years it turned out that multidimensional recurrent neuralnetworks (MDRNN) perform very well for offline handwriting recognition taskslike the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing anddictionary lookup, our ARGUS software completed this task with an error rate of26.27% in its primary setup.
arxiv-1412-6012 | CITlab ARGUS for historical data tables |  http://arxiv.org/abs/1412.6012  | author:Gundram Leifert, Tobias Grüning, Tobias Strauß, Roger Labahn, for the University o category:cs.CV cs.NE 68T05, 68T10 published:2014-12-15 summary:We describe CITlab's recognition system for the ANWRESH-2014 competitionattached to the 14. International Conference on Frontiers in HandwritingRecognition, ICFHR 2014. The task comprises word recognition from segmentedhistorical documents. The core components of our system are based onmulti-dimensional recurrent neural networks (MDRNN) and connectionist temporalclassification (CTC). The software modules behind that as well as the basicutility technologies are essentially powered by PLANET's ARGUS framework forintelligent text recognition and image processing.
arxiv-1412-4526 | Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification |  http://arxiv.org/abs/1412.4526  | author:Hongsheng Li, Rui Zhao, Xiaogang Wang category:cs.CV published:2014-12-15 summary:We present highly efficient algorithms for performing forward and backwardpropagation of Convolutional Neural Network (CNN) for pixelwise classificationon images. For pixelwise classification tasks, such as image segmentation andobject detection, surrounding image patches are fed into CNN for predicting theclasses of centered pixels via forward propagation and for updating CNNparameters via backward propagation. However, forward and backward propagationwas originally designed for whole-image classification. Directly applying it topixelwise classification in a patch-by-patch scanning manner is extremelyinefficient, because surrounding patches of pixels have large overlaps, whichlead to a lot of redundant computation. The proposed algorithms eliminate all the redundant computation inconvolution and pooling on images by introducing novel d-regularly sparsekernels. It generates exactly the same results as those by patch-by-patchscanning. Convolution and pooling operations with such kernels are able tocontinuously access memory and can run efficiently on GPUs. A fraction ofpatches of interest can be chosen from each training image for backwardpropagation by applying a mask to the error map at the last CNN layer. Itscomputation complexity is constant with respect to the number of patchessampled from the image. Experiments have shown that our proposed algorithmsspeed up commonly used patch-by-patch scanning over 1500 times in both forwardand backward propagation. The speedup increases with the sizes of images andpatches.
arxiv-1412-4470 | Automatic video scene segmentation based on spatial-temporal clues and rhythm |  http://arxiv.org/abs/1412.4470  | author:Walid Mahdi, Liming Chen, Mohsen Ardebilian category:cs.CV published:2014-12-15 summary:With ever increasing computing power and data storage capacity, the potentialfor large digital video libraries is growing rapidly.However, the massive useof video for the moment is limited by its opaque characteristics. Indeed, auser who has to handle and retrieve sequentially needs too much time in orderto find out segments of interest within a video. Therefore, providing anenvironment both convenient and efficient for video storing and retrieval,especially for content-based searching as this exists in traditional textbaseddatabase systems, has been the focus of recent and important efforts of a largeresearch community In this paper, we propose a new automatic video scene segmentation methodthat explores two main video features; these are spatial-temporal relationshipand rhythm of shots. The experimental evidence we obtained from a 80minutevideo showed that our prototype provides very high accuracy for videosegmentation.
arxiv-1412-4564 | MatConvNet - Convolutional Neural Networks for MATLAB |  http://arxiv.org/abs/1412.4564  | author:Andrea Vedaldi, Karel Lenc category:cs.CV cs.LG cs.MS cs.NE published:2014-12-15 summary:MatConvNet is an implementation of Convolutional Neural Networks (CNNs) forMATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.It exposes the building blocks of CNNs as easy-to-use MATLAB functions,providing routines for computing linear convolutions with filter banks, featurepooling, and many more. In this manner, MatConvNet allows fast prototyping ofnew CNN architectures; at the same time, it supports efficient computation onCPU and GPU allowing to train complex models on large datasets such as ImageNetILSVRC. This document provides an overview of CNNs and how they are implementedin MatConvNet and gives the technical details of each computational block inthe toolbox.
arxiv-1412-4736 | On the Inductive Bias of Dropout |  http://arxiv.org/abs/1412.4736  | author:David P. Helmbold, Philip M. Long category:cs.LG cs.AI cs.NE math.ST stat.ML stat.TH published:2014-12-15 summary:Dropout is a simple but effective technique for learning in neural networksand other settings. A sound theoretical understanding of dropout is needed todetermine when dropout should be applied and how to use it most effectively. Inthis paper we continue the exploration of dropout as a regularizer pioneered byWager, et.al. We focus on linear classification where a convex proxy to themisclassification loss (i.e. the logistic loss used in logistic regression) isminimized. We show: (a) when the dropout-regularized criterion has a uniqueminimizer, (b) when the dropout-regularization penalty goes to infinity withthe weights, and when it remains bounded, (c) that the dropout regularizationcan be non-monotonic as individual weights increase from 0, and (d) that thedropout regularization penalty may not be convex. This last point isparticularly surprising because the combination of dropout regularization withany convex loss proxy is always a convex function. In order to contrast dropout regularization with $L_2$ regularization, weformalize the notion of when different sources are more compatible withdifferent regularizers. We then exhibit distributions that are provably morecompatible with dropout regularization than $L_2$ regularization, and viceversa. These sources provide additional insight into how the inductive biasesof dropout and $L_2$ regularization differ. We provide some similar results for$L_1$ regularization.
arxiv-1412-4679 | Bayesian multi-tensor factorization |  http://arxiv.org/abs/1412.4679  | author:Suleiman A. Khan, Eemeli Leppäaho, Samuel Kaski category:stat.ML published:2014-12-15 summary:We introduce Bayesian multi-tensor factorization, a model that is the firstBayesian formulation for joint factorization of multiple matrices and tensors.The research problem generalizes the joint matrix-tensor factorization problemto arbitrary sets of tensors of any depth, including matrices, can beinterpreted as unsupervised multi-view learning from multiple data tensors, andcan be generalized to relax the usual trilinear tensor factorizationassumptions. The result is a factorization of the set of tensors into factorsshared by any subsets of the tensors, and factors private to individualtensors. We demonstrate the performance against existing baselines in multipletensor factorization tasks in structural toxicogenomics and functionalneuroimaging.
arxiv-1412-4438 | Fixed Point Algorithm Based on Quasi-Newton Method for Convex Minimization Problem with Application to Image Deblurring |  http://arxiv.org/abs/1412.4438  | author:Dai-Qiang Chen category:cs.CV published:2014-12-15 summary:Solving an optimization problem whose objective function is the sum of twoconvex functions has received considerable interests in the context of imageprocessing recently. In particular, we are interested in the scenario when anon-differentiable convex function such as the total variation (TV) norm isincluded in the objective function due to many variational models establishedin image processing have this nature. In this paper, we propose a fast fixedpoint algorithm based on the quasi-Newton method for solving this class ofproblem, and apply it in the field of TV-based image deblurring. The novelmethod is derived from the idea of the quasi-Newton method, and the fixed-pointalgorithms based on the proximity operator, which were widely investigated veryrecently. Utilizing the non-expansion property of the proximity operator wefurther investigate the global convergence of the proposed algorithm. Numericalexperiments on image deblurring problem with additive or multiplicative noiseare presented to demonstrate that the proposed algorithm is superior to therecently developed fixed-point algorithm in the computational efficiency.
arxiv-1412-4659 | Finding a sparse vector in a subspace: Linear sparsity using alternating directions |  http://arxiv.org/abs/1412.4659  | author:Qing Qu, Ju Sun, John Wright category:cs.IT cs.CV cs.LG math.IT math.OC stat.ML published:2014-12-15 summary:We consider the problem of recovering the sparsest vector in a genericsubspace $\mathcal{S} \subseteq \mathbb{R}^p$ with $\mathrm{dim}(\mathcal{S})=n < p$. This problem can be considered a homogeneous variant of the sparserecovery problem, and finds applications in sparse dictionary learning, sparsePCA, and many other problems in signal processing and machine learning. Simpleconvex heuristics for this problem provably break down when the fraction ofnonzero entries in the target sparse vector substantially exceeds$O(1/\sqrt{n})$. In contrast, we exhibit a relatively simple nonconvex approachbased on alternating directions, which provably succeeds even when the fractionof nonzero entries is $\Omega(1)$. To the best of our knowledge, this is thefirst practical algorithm to achieve this linear scaling. This result assumes aplanted sparse model for the subspace, in which the target sparse vector isembedded in an otherwise random subspace. Empirically, our proposed algorithmalso succeeds in more challenging data models, e.g., sparse dictionarylearning.
arxiv-1412-4729 | Translating Videos to Natural Language Using Deep Recurrent Neural Networks |  http://arxiv.org/abs/1412.4729  | author:Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko category:cs.CV cs.CL published:2014-12-15 summary:Solving the visual symbol grounding problem has long been a goal ofartificial intelligence. The field appears to be advancing closer to this goalwith recent breakthroughs in deep learning for natural language grounding instatic images. In this paper, we propose to translate videos directly tosentences using a unified deep neural network with both convolutional andrecurrent structure. Described video datasets are scarce, and most existingmethods have been applied to toy domains with a small vocabulary of possiblewords. By transferring knowledge from 1.2M+ images with category labels and100,000+ images with captions, our method is able to create sentencedescriptions of open-domain videos with large vocabularies. We compare ourapproach with recent work using language generation metrics, subject, verb, andobject prediction accuracy, and a human evaluation.
arxiv-1412-4433 | Inexact Alternating Direction Method Based on Newton descent algorithm with Application to Poisson Image Deblurring |  http://arxiv.org/abs/1412.4433  | author:Dai-Qiang Chen category:cs.CV published:2014-12-15 summary:The recovery of images from the observations that are degraded by a linearoperator and further corrupted by Poisson noise is an important task in modernimaging applications such as astronomical and biomedical ones. Gradient-basedregularizers involve the popular total variation semi-norm have become standardtechniques for Poisson image restoration due to its edge-preserving ability.Various efficient algorithms have been developed for solving the correspondingminimization problem with non-smooth regularization terms. In this paper,motivated by the idea of the alternating direction minimization algorithm andthe Newton's method with upper convergent rate, we further propose inexactalternating direction methods utilizing the proximal Hessian matrix informationof the objective function, in a way reminiscent of Newton descent methods.Besides, we also investigate the global convergence of the proposed algorithmsunder certain conditions. Finally, we illustrate that the proposed algorithmsoutperform the current state-of-the-art algorithms through numericalexperiments on Poisson image deblurring.
arxiv-1412-4446 | Domain-Adversarial Neural Networks |  http://arxiv.org/abs/1412.4446  | author:Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand category:stat.ML cs.LG cs.NE published:2014-12-15 summary:We introduce a new representation learning algorithm suited to the context ofdomain adaptation, in which data at training and test time come from similarbut different distributions. Our algorithm is directly inspired by theory ondomain adaptation suggesting that, for effective domain transfer to beachieved, predictions must be made based on a data representation that cannotdiscriminate between the training (source) and test (target) domains. Wepropose a training objective that implements this idea in the context of aneural network, whose hidden layer is trained to be predictive of theclassification task, but uninformative as to the domain of the input. Ourexperiments on a sentiment analysis classification benchmark, where the targetdomain data available at training time is unlabeled, show that our neuralnetwork for domain adaption algorithm has better performance than either astandard neural network or an SVM, even if trained on input features extractedwith the state-of-the-art marginalized stacked denoising autoencoders of Chenet al. (2012).
arxiv-1412-4690 | GPTIPS 2: an open-source software platform for symbolic data mining |  http://arxiv.org/abs/1412.4690  | author:Dominic P. Searson category:cs.MS cs.NE published:2014-12-15 summary:GPTIPS is a free, open source MATLAB based software platform for symbolicdata mining (SDM). It uses a multigene variant of the biologically inspiredmachine learning method of genetic programming (MGGP) as the engine that drivesthe automatic model discovery process. Symbolic data mining is the process ofextracting hidden, meaningful relationships from data in the form of symbolicequations. In contrast to other data-mining methods, the structuraltransparency of the generated predictive equations can give new insights intothe physical systems or processes that generated the data. Furthermore, thistransparency makes the models very easy to deploy outside of MATLAB. Therationale behind GPTIPS is to reduce the technical barriers to using,understanding, visualising and deploying GP based symbolic models of data,whilst at the same time remaining highly customisable and delivering robustnumerical performance for power users. In this chapter, notable new features ofthe latest version of the software are discussed with these aims in mind.Additionally, a simplified variant of the MGGP high level gene crossovermechanism is proposed. It is demonstrated that the new functionality of GPTIPS2 (a) facilitates the discovery of compact symbolic relationships from datausing multiple approaches, e.g. using novel gene-centric visualisation analysisto mitigate horizontal bloat and reduce complexity in multigene symbolicregression models (b) provides numerous methods for visualising the propertiesof symbolic models (c) emphasises the generation of graphically navigablelibraries of models that are optimal in terms of the Pareto trade off surfaceof model performance and complexity and (d) expedites real world applicationsby the simple, rapid and robust deployment of symbolic models outside thesoftware environment they were developed in.
arxiv-1412-4401 | Tools for Terminology Processing |  http://arxiv.org/abs/1412.4401  | author:C. Enguehard, B. Daille, E. Morin category:cs.CY cs.CL published:2014-12-14 summary:Automatic terminology processing appeared 10 years ago when electroniccorpora became widely available. Such processing may be statistically orlinguistically based and produces terminology resources that can be used in anumber of applications : indexing, information retrieval, technology watch,etc. We present the tools that have been developed in the IRIN Institute. Theyall take as input texts (or collection of texts) and reflect different statesof terminology processing: term acquisition, term recognition and termstructuring.
arxiv-1412-6144 | The Computational Theory of Intelligence: Applications to Genetic Programming and Turing Machines |  http://arxiv.org/abs/1412.6144  | author:Daniel Kovach category:cs.NE published:2014-12-14 summary:In this paper, we continue the efforts of the Computational Theory ofIntelligence (CTI) by extending concepts to include computational processes interms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive,and Hybrid Computational Intelligence processes are also introduced anddiscussed. We consider the ramifications of the assumptions of CTI with regardto the qualities of reproduction and virility. Applications to Biology,Computer Science and Cyber Security are also discussed.
arxiv-1412-4369 | Incorporating Both Distributional and Relational Semantics in Word Representations |  http://arxiv.org/abs/1412.4369  | author:Daniel Fried, Kevin Duh category:cs.CL published:2014-12-14 summary:We investigate the hypothesis that word representations ought to incorporateboth distributional and relational semantics. To this end, we employ theAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes adistributional objective on raw text and a relational objective on WordNet.Preliminary results on knowledge base completion, analogy tests, and parsingshow that word representations trained on both objectives can give improvementsin some cases.
arxiv-1412-4385 | Unsupervised Domain Adaptation with Feature Embeddings |  http://arxiv.org/abs/1412.4385  | author:Yi Yang, Jacob Eisenstein category:cs.CL cs.LG published:2014-12-14 summary:Representation learning is the dominant technique for unsupervised domainadaptation, but existing approaches often require the specification of "pivotfeatures" that generalize across domains, which are selected by task-specificheuristics. We show that a novel but simple feature embedding approach providesbetter performance, by exploiting the feature template structure common in NLPproblems.
arxiv-1412-4314 | Recurrent-Neural-Network for Language Detection on Twitter Code-Switching Corpus |  http://arxiv.org/abs/1412.4314  | author:Joseph Chee Chang, Chu-Cheng Lin category:cs.NE cs.CL published:2014-12-14 summary:Mixed language data is one of the difficult yet less explored domains ofnatural language processing. Most research in fields like machine translationor sentiment analysis assume monolingual input. However, people who are capableof using more than one language often communicate using multiple languages atthe same time. Sociolinguists believe this "code-switching" phenomenon to besocially motivated. For example, to express solidarity or to establishauthority. Most past work depend on external tools or resources, such aspart-of-speech tagging, dictionary look-up, or named-entity recognizers toextract rich features for training machine learning models. In this paper, wetrain recurrent neural networks with only raw features, and use word embeddingto automatically learn meaningful representations. Using the samemixed-language Twitter corpus, our system is able to outperform the bestSVM-based systems reported in the EMNLP'14 Code-Switching Workshop by 1% inaccuracy, or by 17% in error rate reduction.
arxiv-1412-4313 | Combining the Best of Graphical Models and ConvNets for Semantic Segmentation |  http://arxiv.org/abs/1412.4313  | author:Michael Cogswell, Xiao Lin, Senthil Purushwalkam, Dhruv Batra category:cs.CV published:2014-12-14 summary:We present a two-module approach to semantic segmentation that incorporatesConvolutional Networks (CNNs) and Graphical Models. Graphical models are usedto generate a small (5-30) set of diverse segmentations proposals, such thatthis set has high recall. Since the number of required proposals is so low, wecan extract fairly complex features to rank them. Our complex feature of choiceis a novel CNN called SegNet, which directly outputs a (coarse) semanticsegmentation. Importantly, SegNet is specifically trained to optimize thecorpus-level PASCAL IOU loss function. To the best of our knowledge, this isthe first CNN specifically designed for semantic segmentation. This two-moduleapproach achieves $52.5\%$ on the PASCAL 2012 segmentation challenge.
arxiv-1412-4186 | An Evaluation of Support Vector Machines as a Pattern Recognition Tool |  http://arxiv.org/abs/1412.4186  | author:Eugene Borovikov category:cs.LG 62-07 published:2014-12-13 summary:The purpose of this report is in examining the generalization performance ofSupport Vector Machines (SVM) as a tool for pattern recognition and objectclassification. The work is motivated by the growing popularity of the methodthat is claimed to guarantee a good generalization performance for the task inhand. The method is implemented in MATLAB. SVMs based on various kernels aretested for classifying data from various domains.
arxiv-1412-4196 | Descriptor Ensemble: An Unsupervised Approach to Descriptor Fusion in the Homography Space |  http://arxiv.org/abs/1412.4196  | author:Yuan-Ting Hu, Yen-Yu Lin, Hsin-Yi Chen, Kuang-Jui Hsu, Bing-Yu Chen category:cs.CV published:2014-12-13 summary:With the aim to improve the performance of feature matching, we present anunsupervised approach to fuse various local descriptors in the space ofhomographies. Inspired by the observation that the homographies of correctfeature correspondences vary smoothly along the spatial domain, our approachstands on the unsupervised nature of feature matching, and can select a gooddescriptor for matching each feature point. Specifically, the homography spaceserves as the common domain, in which a correspondence obtained by anydescriptor is considered as a point, for integrating various heterogeneousdescriptors. Both geometric coherence and spatial continuity amongcorrespondences are considered via computing their geodesic distances in thespace. In this way, mutual verification across different descriptors isallowed, and correct correspondences will be highlighted with a high degree ofconsistency (i.e., short geodesic distances here). It follows that one-classSVM can be applied to identifying these correct correspondences, and boosts theperformance of feature matching. The proposed approach is comprehensivelycompared with the state-of-the-art approaches, and evaluated on four benchmarksof image matching. The promising results manifest its effectiveness.
arxiv-1412-4183 | A survey of modern optical character recognition techniques |  http://arxiv.org/abs/1412.4183  | author:Eugene Borovikov category:cs.CV 62-04 published:2014-12-13 summary:This report explores the latest advances in the field of digital documentrecognition. With the focus on printed document imagery, we discuss the majordevelopments in optical character recognition (OCR) and document imageenhancement/restoration in application to Latin and non-Latin scripts. Inaddition, we review and discuss the available technologies for hand-writtendocument recognition. In this report, we also provide some company-accumulatedbenchmark results on available OCR engines.
arxiv-1412-4217 | A Study of Sindhi Related and Arabic Script Adapted languages Recognition |  http://arxiv.org/abs/1412.4217  | author:Dil Nawaz Hakro, A. Z. Talib, Zeeshan Bhatti, G. N. Moja category:cs.CV published:2014-12-13 summary:A large number of publications are available for the Optical CharacterRecognition (OCR). Significant researches, as well as articles are present forthe Latin, Chinese and Japanese scripts. Arabic script is also one of maturescript from OCR perspective. The adaptive languages which share Arabic scriptor its extended characters; still lacking the OCRs for their language. In thispaper we present the efforts of researchers on Arabic and its related andadapted languages. This survey is organized in different sections, in whichintroduction is followed by properties of Sindhi Language. OCR processtechniques and methods used by various researchers are presented. The lastsection is dedicated for future work and conclusion is also discussed.
arxiv-1412-4182 | The Statistics of Streaming Sparse Regression |  http://arxiv.org/abs/1412.4182  | author:Jacob Steinhardt, Stefan Wager, Percy Liang category:math.ST cs.LG stat.ML stat.TH published:2014-12-13 summary:We present a sparse analogue to stochastic gradient descent that isguaranteed to perform well under similar conditions to the lasso. In the linearregression setup with irrepresentable noise features, our algorithm recoversthe support set of the optimal parameter vector with high probability, andachieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T),where k is the sparsity of the solution, d is the number of features, and T isthe number of training examples. Meanwhile, our algorithm does not require anymore computational resources than stochastic gradient descent. In ourexperiments, we find that our method substantially out-performs existingstreaming algorithms on both real and simulated data.
arxiv-1412-4271 | Multi-Context Models for Reasoning under Partial Knowledge: Generative Process and Inference Grammar |  http://arxiv.org/abs/1412.4271  | author:Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos category:cs.AI math.LO math.PR stat.ML published:2014-12-13 summary:Arriving at the complete probabilistic knowledge of a domain, i.e., learninghow all variables interact, is indeed a demanding task. In reality, settingsoften arise for which an individual merely possesses partial knowledge of thedomain, and yet, is expected to give adequate answers to a variety of posedqueries. That is, although precise answers to some queries, in principle,cannot be achieved, a range of plausible answers is attainable for each querygiven the available partial knowledge. In this paper, we propose theMulti-Context Model (MCM), a new graphical model to represent the state ofpartial knowledge as to a domain. MCM is a middle ground between ProbabilisticLogic, Bayesian Logic, and Probabilistic Graphical Models. For this model wediscuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., toform partial beliefs regarding a domain in a gradual and probabilisticallyconsistent way, and (ii) how to perform inference, i.e., to evaluate aprobability of interest involving some variables of the domain.
arxiv-1412-4205 | The application of the Bayes Ying Yang harmony based GMMs in on-line signature verification |  http://arxiv.org/abs/1412.4205  | author:Xiaosha Zhao, Mandan Liu category:cs.CV published:2014-12-13 summary:In this contribution, a Bayes Ying Yang(BYY) harmony based approach foron-line signature verification is presented. In the proposed method, a simplebut effective Gaussian Mixture Models(GMMs) is used to represent for eachuser's signature model based on the prior information collected. Different fromthe early works, in this paper, we use the Bayes Ying Yang machine combinedwith the harmony function to achieve Automatic Model Selection(AMS) during theparameter learning for the GMMs, so that a better approximation of the usermodel is assured. Experiments on a database from the First InternationalSignature Verification Competition(SVC 2004) confirm that this combinedalgorithm yields quite satisfactory results.
arxiv-1412-4175 | Optimizing Over Radial Kernels on Compact Manifolds |  http://arxiv.org/abs/1412.4175  | author:Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV published:2014-12-13 summary:We tackle the problem of optimizing over all possible positive definiteradial kernels on Riemannian manifolds for classification. Kernel methods onRiemannian manifolds have recently become increasingly popular in computervision. However, the number of known positive definite kernels on manifoldsremain very limited. Furthermore, most kernels typically depend on at least oneparameter that needs to be tuned for the problem at hand. A poor choice ofkernel, or of parameter value, may yield significant performance drop-off.Here, we show that positive definite radial kernels on the unit n-sphere, theGrassmann manifold and Kendall's shape manifold can be expressed in a simpleform whose parameters can be automatically optimized within a support vectormachine framework. We demonstrate the benefits of our kernel learning algorithmon object, face, action and shape recognition.
arxiv-1412-4174 | A Framework for Shape Analysis via Hilbert Space Embedding |  http://arxiv.org/abs/1412.4174  | author:Sadeep Jayasumana, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV published:2014-12-13 summary:We propose a framework for 2D shape analysis using positive definite kernelsdefined on Kendall's shape manifold. Different representations of 2D shapes areknown to generate different nonlinear spaces. Due to the nonlinearity of thesespaces, most existing shape classification algorithms resort to nearestneighbor methods and to learning distances on shape spaces. Here, we propose tomap shapes on Kendall's shape manifold to a high dimensional Hilbert spacewhere Euclidean geometry applies. To this end, we introduce a kernel on thismanifold that permits such a mapping, and prove its positive definiteness. Thiskernel lets us extend kernel-based algorithms developed for Euclidean spaces,such as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate thebenefits of our approach over the state-of-the-art methods on shapeclassification, clustering and retrieval.
arxiv-1412-4172 | Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices |  http://arxiv.org/abs/1412.4172  | author:Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV published:2014-12-13 summary:Symmetric Positive Definite (SPD) matrices have become popular to encodeimage information. Accounting for the geometry of the Riemannian manifold ofSPD matrices has proven key to the success of many algorithms. However, mostexisting methods only approximate the true shape of the manifold locally by itstangent plane. In this paper, inspired by kernel methods, we propose to map SPDmatrices to a high dimensional Hilbert space where Euclidean geometry applies.To encode the geometry of the manifold in the mapping, we introduce a family ofprovably positive definite kernels on the Riemannian manifold of SPD matrices.These kernels are derived from the Gaussian ker- nel, but exploit differentmetrics on the manifold. This lets us extend kernel-based algorithms developedfor Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold ofSPD matrices. We demonstrate the benefits of our approach on the problems ofpedestrian detection, ob- ject categorization, texture analysis, 2D motionsegmentation and Diffusion Tensor Imaging (DTI) segmentation.
arxiv-1412-4210 | Learning Precise Spike Train to Spike Train Transformations in Multilayer Feedforward Neuronal Networks |  http://arxiv.org/abs/1412.4210  | author:Arunava Banerjee category:cs.NE published:2014-12-13 summary:We derive a synaptic weight update rule for learning temporally precise spiketrain to spike train transformations in multilayer feedforward networks ofspiking neurons. The framework, aimed at seamlessly generalizing errorbackpropagation to the deterministic spiking neuron setting, is based strictlyon spike timing and avoids invoking concepts pertaining to spike rates orprobabilistic models of spiking. The derivation is founded on two innovations.First, an error functional is proposed that compares the spike train emitted bythe output neuron of the network to the desired spike train by way of theirputative impact on a virtual postsynaptic neuron. This formulation sidestepsthe need for spike alignment and leads to closed form solutions for allquantities of interest. Second, virtual assignment of weights to spikes ratherthan synapses enables a perturbation analysis of individual spike times andsynaptic weights of the output as well as all intermediate neurons in thenetwork, which yields the gradients of the error functional with respect to thesaid entities. Learning proceeds via a gradient descent mechanism thatleverages these quantities. Simulation experiments demonstrate the efficacy ofthe proposed learning framework. The experiments also highlight asymmetriesbetween synapses on excitatory and inhibitory neurons.
arxiv-1412-4181 | Oriented Edge Forests for Boundary Detection |  http://arxiv.org/abs/1412.4181  | author:Sam Hallman, Charless C. Fowlkes category:cs.CV published:2014-12-13 summary:We present a simple, efficient model for learning boundary detection based ona random forest classifier. Our approach combines (1) efficient clustering oftraining examples based on simple partitioning of the space of local edgeorientations and (2) scale-dependent calibration of individual tree outputprobabilities prior to multiscale combination. The resulting model outperformspublished results on the challenging BSDS500 boundary detection benchmark.Further, on large datasets our model requires substantially less memory fortraining and speeds up training time by a factor of 10 over the structuredforest model.
arxiv-1412-4218 | Optimization of Reliability of Network of Given Connectivity using Genetic Algorithm |  http://arxiv.org/abs/1412.4218  | author:Ho Tat Lam, Kwok Yip Szeto category:physics.soc-ph cs.NE cs.SI published:2014-12-13 summary:Reliability is one of the important measures of how well the system meets itsdesign objective, and mathematically is the probability that a system willperform satisfactorily for at least a given period of time. When the system isdescribed by a connected network of N components (nodes) and their L connection(links), the reliability of the system becomes a difficult network designproblem which solutions are of great practical interest in science andengineering. This paper discusses the numerical method of finding the mostreliable network for a given N and L using genetic algorithm. For a giventopology of the network, the reliability is numerically computed usingadjacency matrix. For a search in the space of all possible topologies of theconnected network with N nodes and L links, genetic operators such as mutationand crossover are applied to the adjacency matrix through a stringrepresentation. In the context of graphs, the mutation of strings in geneticalgorithm corresponds to the rewiring of graphs, while crossover corresponds tothe interchange of the sub-graphs. For small networks where the most reliablenetwork can be found by exhaustive search, genetic algorithm is very efficient.For larger networks, our results not only demonstrate the efficiency of ouralgorithm, but also suggest that the most reliable network will have highsymmetry.
arxiv-1412-4237 | First order algorithms in variational image processing |  http://arxiv.org/abs/1412.4237  | author:Martin Burger, Alex Sawatzky, Gabriele Steidl category:math.OC cs.CV stat.ML published:2014-12-13 summary:Variational methods in imaging are nowadays developing towards a quiteuniversal and flexible tool, allowing for highly successful approaches on taskslike denoising, deblurring, inpainting, segmentation, super-resolution,disparity, and optical flow estimation. The overall structure of suchapproaches is of the form ${\cal D}(Ku) + \alpha {\cal R} (u) \rightarrow\min_u$ ; where the functional ${\cal D}$ is a data fidelity term alsodepending on some input data $f$ and measuring the deviation of $Ku$ from suchand ${\cal R}$ is a regularization functional. Moreover $K$ is a (often linear)forward operator modeling the dependence of data on an underlying image, and$\alpha$ is a positive regularization parameter. While ${\cal D}$ is oftensmooth and (strictly) convex, the current practice almost exclusively usesnonsmooth regularization functionals. The majority of successful techniques isusing nonsmooth and convex functionals like the total variation andgeneralizations thereof or $\ell_1$-norms of coefficients arising from scalarproducts with some frame system. The efficient solution of such variationalproblems in imaging demands for appropriate algorithms. Taking into account thespecific structure as a sum of two very different terms to be minimized,splitting algorithms are a quite canonical choice. Consequently this field hasrevived the interest in techniques like operator splittings or augmentedLagrangians. Here we shall provide an overview of methods currently developedand recent results as well as some computational studies providing a comparisonof different methods and also illustrating their success in applications.
arxiv-1412-4102 | Representing Data by a Mixture of Activated Simplices |  http://arxiv.org/abs/1412.4102  | author:Chunyu Wang, John Flynn, Yizhou Wang, Alan L. Yuille category:cs.CV published:2014-12-12 summary:We present a new model which represents data as a mixture of simplices.Simplices are geometric structures that generalize triangles. We give a simplegeometric understanding that allows us to learn a simplicial structureefficiently. Our method requires that the data are unit normalized (and thuslie on the unit sphere). We show that under this restriction, building a modelwith simplices amounts to constructing a convex hull inside the sphere whoseboundary facets is close to the data. We call the boundary facets of the convexhull that are close to the data Activated Simplices. While the total number ofbases used to build the simplices is a parameter of the model, the dimensionsof the individual activated simplices are learned from the data. Simplices canhave different dimensions, which facilitates modeling of inhomogeneous datasources. The simplicial structure is bounded --- this is appropriate formodeling data with constraints, such as human elbows can not bend more than 180degrees. The simplices are easy to interpret and extremes within the data canbe discovered among the vertices. The method provides good reconstruction andregularization. It supports good nearest neighbor classification and it allowsrealistic generative models to be constructed. It achieves state-of-the-artresults on benchmark datasets, including 3D poses and digits.
arxiv-1412-4056 | Blind system identification using kernel-based methods |  http://arxiv.org/abs/1412.4056  | author:Giulio Bottegal, Riccardo S. Risuleo, Håkan Hjalmarsson category:cs.SY stat.ML published:2014-12-12 summary:We propose a new method for blind system identification. Resorting to aGaussian regression framework, we model the impulse response of the unknownlinear system as a realization of a Gaussian process. The structure of thecovariance matrix (or kernel) of such a process is given by the stable splinekernel, which has been recently introduced for system identification purposesand depends on an unknown hyperparameter. We assume that the input can belinearly described by few parameters. We estimate these parameters, togetherwith the kernel hyperparameter and the noise variance, using an empirical Bayesapproach. The related optimization problem is efficiently solved with a noveliterative scheme based on the Expectation-Maximization method. In particular,we show that each iteration consists of a set of simple update rules. We show,through some numerical experiments, very promising performance of the proposedmethod.
arxiv-1412-4098 | Manifold Matching using Shortest-Path Distance and Joint Neighborhood Selection |  http://arxiv.org/abs/1412.4098  | author:Cencheng Shen, Joshua T. Vogelstein, Carey E. Priebe category:stat.ML published:2014-12-12 summary:We propose a nonlinear manifold matching algorithm to match multiple datasets using shortest-path distance and joint neighborhood selection. Based onthe correspondence information, a neighborhood graph is jointly constructed;then the shortest-path distance within each data set is computed from the jointneighborhood graph, followed by embedding into and matching in a commonlow-dimensional Euclidean space. Our approach exhibits superior and robustperformance for matching data from disparate sources, compared to algorithmsthat do not use shortest-path distance or joint neighborhood selection.
arxiv-1412-4080 | Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso |  http://arxiv.org/abs/1412.4080  | author:Antoine Bonnefoy, Valentin Emiya, Liva Ralaivola, Rémi Gribonval category:stat.ML cs.LG published:2014-12-12 summary:Recent computational strategies based on screening tests have been proposedto accelerate algorithms addressing penalized sparse regression problems suchas the Lasso. Such approaches build upon the idea that it is worth dedicatingsome small computational effort to locate inactive atoms and remove them fromthe dictionary in a preprocessing stage so that the regression algorithmworking with a smaller dictionary will then converge faster to the solution ofthe initial problem. We believe that there is an even more efficient way toscreen the dictionary and obtain a greater acceleration: inside each iterationof the regression algorithm, one may take advantage of the algorithmcomputations to obtain a new screening test for free with increasing screeningeffects along the iterations. The dictionary is henceforth dynamically screenedinstead of being screened statically, once and for all, before the firstiteration. We formalize this dynamic screening principle in a generalalgorithmic scheme and apply it by embedding inside a number of first-orderalgorithms adapted existing screening tests to solve the Lasso or new screeningtests to solve the Group-Lasso. Computational gains are assessed in a large setof experiments on synthetic data as well as real-world sounds and images. Theyshow both the screening efficiency and the gain in terms running times.
arxiv-1412-3958 | An Automatic Seeded Region Growing for 2D Biomedical Image Segmentation |  http://arxiv.org/abs/1412.3958  | author:Mohammed M. Abdelsamea category:cs.CV published:2014-12-12 summary:In this paper, an automatic seeded region growing algorithm is proposed forcellular image segmentation. First, the regions of interest (ROIs) extractedfrom the preprocessed image. Second, the initial seeds are automaticallyselected based on ROIs extracted from the image. Third, the most reprehensiveseeds are selected using a machine learning algorithm. Finally, the cellularimage is segmented into regions where each region corresponds to a seed. Theaim of the proposed is to automatically extract the Region of Interests (ROI)from the cellular images in terms of overcoming the explosion, undersegmentation and over segmentation problems. Experimental results show that theproposed algorithm can improve the segmented image and the segmented resultsare less noisy as compared to some existing algorithms.
arxiv-1412-3949 | CITlab ARGUS for historical handwritten documents |  http://arxiv.org/abs/1412.3949  | author:Tobias Strauß, Tobias Grüning, Gundram Leifert, Roger Labahn, for the University o category:cs.CV cs.NE 68T05, 68T10 published:2014-12-12 summary:We describe CITlab's recognition system for the HTRtS competition attached tothe 14. International Conference on Frontiers in Handwriting Recognition, ICFHR2014. The task comprises the recognition of historical handwritten documents.The core algorithms of our system are based on multi-dimensional recurrentneural networks (MDRNN) and connectionist temporal classification (CTC). Thesoftware modules behind that as well as the basic utility technologies areessentially powered by PLANET's ARGUS framework for intelligent textrecognition and image processing.
arxiv-1412-3925 | Region segmentation for sparse decompositions: better brain parcellations from rest fMRI |  http://arxiv.org/abs/1412.3925  | author:Alexandre Abraham, Elvis Dohmatob, Bertrand Thirion, Dimitris Samaras, Gael Varoquaux category:q-bio.NC cs.CV published:2014-12-12 summary:Functional Magnetic Resonance Images acquired during resting-state provideinformation about the functional organization of the brain through measuringcorrelations between brain areas. Independent components analysis is thereference approach to estimate spatial components from weakly structured datasuch as brain signal time courses; each of these components may be referred toas a brain network and the whole set of components can be conceptualized as abrain functional atlas. Recently, new methods using a sparsity prior haveemerged to deal with low signal-to-noise ratio data. However, even when usingsophisticated priors, the results may not be very sparse and most often do notseparate the spatial components into brain regions. This work presentspost-processing techniques that automatically sparsify brain maps and separateregions properly using geometric operations, and compares these techniquesaccording to faithfulness to data and stability metrics. In particular, amongthreshold-based approaches, hysteresis thresholding and random walkersegmentation, the latter improves significantly the stability of both dense andsparse models.
arxiv-1412-4128 | Expanded Alternating Optimization of Nonconvex Functions with Applications to Matrix Factorization and Penalized Regression |  http://arxiv.org/abs/1412.4128  | author:W. James Murdoch, Mu Zhu category:stat.CO stat.ML published:2014-12-12 summary:We propose a general technique for improving alternating optimization (AO) ofnonconvex functions. Starting from the solution given by AO, we conduct anothersequence of searches over subspaces that are both meaningful to theoptimization problem at hand and different from those used by AO. Todemonstrate the utility of our approach, we apply it to the matrixfactorization (MF) algorithm for recommender systems and the coordinate descentalgorithm for penalized regression (PR), and show meaningful improvements usingboth real-world (for MF) and simulated (for PR) data sets. Moreover, wedemonstrate for MF that, by constructing search spaces customized to the givendata set, we can significantly increase the convergence rate of our technique.
arxiv-1412-4021 | A Robust Transformation-Based Learning Approach Using Ripple Down Rules for Part-of-Speech Tagging |  http://arxiv.org/abs/1412.4021  | author:Dat Quoc Nguyen, Dai Quoc Nguyen, Dang Duc Pham, Son Bao Pham category:cs.CL published:2014-12-12 summary:In this paper, we propose a new approach to construct a system oftransformation rules for the Part-of-Speech (POS) tagging task. Our approach isbased on an incremental knowledge acquisition method where rules are stored inan exception structure and new rules are only added to correct the errors ofexisting rules; thus allowing systematic control of the interaction between therules. Experimental results on 13 languages show that our approach is fast interms of training time and tagging speed. Furthermore, our approach obtainsvery competitive accuracy in comparison to state-of-the-art POS andmorphological taggers.
arxiv-1412-3922 | Size sensitive packing number for Hamming cube and its consequences |  http://arxiv.org/abs/1412.3922  | author:Kunal Dutta, Arijit Ghosh category:cs.DM cs.CG cs.LG math.CO published:2014-12-12 summary:We prove a size-sensitive version of Haussler's Packinglemma~\cite{Haussler92spherepacking} for set-systems with bounded primalshatter dimension, which have an additional {\em size-sensitive property}. Thisanswers a question asked by Ezra~\cite{Ezra-sizesendisc-soda-14}. We alsopartially address another point raised by Ezra regarding overcounting of setsin her chaining procedure. As a consequence of these improvements, we get animprovement on the size-sensitive discrepancy bounds for set systems with theabove property. Improved bounds on the discrepancy for these special setsystems also imply an improvement in the sizes of {\em relative $(\varepsilon,\delta)$-approximations} and $(\nu, \alpha)$-samples.
arxiv-1412-3919 | Machine Learning for Neuroimaging with Scikit-Learn |  http://arxiv.org/abs/1412.3919  | author:Alexandre Abraham, Fabian Pedregosa, Michael Eickenberg, Philippe Gervais, Andreas Muller, Jean Kossaifi, Alexandre Gramfort, Bertrand Thirion, Gäel Varoquaux category:cs.LG cs.CV stat.ML published:2014-12-12 summary:Statistical machine learning methods are increasingly used for neuroimagingdata analysis. Their main virtue is their ability to model high-dimensionaldatasets, e.g. multivariate analysis of activation images or resting-state timeseries. Supervised learning is typically used in decoding or encoding settingsto relate brain images to behavioral or clinical observations, whileunsupervised learning can uncover hidden structures in sets of images (e.g.resting state functional MRI) or find sub-populations in large cohorts. Byconsidering different functional neuroimaging applications, we illustrate howscikit-learn, a Python machine learning library, can be used to perform somekey analysis steps. Scikit-learn contains a very large set of statisticallearning algorithms, both supervised and unsupervised, and its application toneuroimaging data provides a versatile tool to study the brain.
arxiv-1412-4044 | Adaptive Stochastic Gradient Descent on the Grassmannian for Robust Low-Rank Subspace Recovery and Clustering |  http://arxiv.org/abs/1412.4044  | author:Jun He, Yue Zhang category:stat.ML cs.CV cs.NA math.OC published:2014-12-12 summary:In this paper, we present GASG21 (Grassmannian Adaptive Stochastic Gradientfor $L_{2,1}$ norm minimization), an adaptive stochastic gradient algorithm torobustly recover the low-rank subspace from a large matrix. In the presence ofcolumn outliers, we reformulate the batch mode matrix $L_{2,1}$ normminimization with rank constraint problem as a stochastic optimization approachconstrained on Grassmann manifold. For each observed data vector, the low-ranksubspace $\mathcal{S}$ is updated by taking a gradient step along the geodesicof Grassmannian. In order to accelerate the convergence rate of the stochasticgradient method, we choose to adaptively tune the constant step-size byleveraging the consecutive gradients. Furthermore, we demonstrate that withproper initialization, the K-subspaces extension, K-GASG21, can robustlycluster a large number of corrupted data vectors into a union of subspaces.Numerical experiments on synthetic and real data demonstrate the efficiency andaccuracy of the proposed algorithms even with heavy column outliers corruption.
arxiv-1412-3914 | Edge Preserving Multi-Modal Registration Based On Gradient Intensity Self-Similarity |  http://arxiv.org/abs/1412.3914  | author:Tamar Rott, Dorin Shriki, Tamir Bendory category:cs.CV published:2014-12-12 summary:Image registration is a challenging task in the world of medical imaging.Particularly, accurate edge registration plays a central role in a variety ofclinical conditions. The Modality Independent Neighbourhood Descriptor (MIND)demonstrates state of the art alignment, based on the image self-similarity.However, this method appears to be less accurate regarding edge registration.In this work, we propose a new registration method, incorporating gradientintensity and MIND self-similarity metric. Experimental results show thesuperiority of this method in edge registration tasks, while preserving theoriginal MIND performance for other image features and textures.
arxiv-1412-4160 | Ripple Down Rules for Question Answering |  http://arxiv.org/abs/1412.4160  | author:Dat Quoc Nguyen, Dai Quoc Nguyen, Son Bao Pham category:cs.CL cs.IR published:2014-12-12 summary:Recent years have witnessed a new trend of building ontology-based questionanswering systems. These systems use semantic web information to produce moreprecise answers to users' queries. However, these systems are mostly designedfor English. In this paper, we introduce an ontology-based question answeringsystem named KbQAS which, to the best of our knowledge, is the first one madefor Vietnamese. KbQAS employs our question analysis approach thatsystematically constructs a knowledge base of grammar rules to convert eachinput question into an intermediate representation element. KbQAS then takesthe intermediate representation element with respect to a target ontology andapplies concept-matching techniques to return an answer. On a wide range ofVietnamese questions, experimental results show that the performance of KbQASis promising with accuracies of 84.1% and 82.4% for analyzing input questionsand retrieving output answers, respectively. Furthermore, our question analysisapproach can easily be applied to new domains and new languages, thus savingtime and human effort.
arxiv-1412-3635 | Simulating a perceptron on a quantum computer |  http://arxiv.org/abs/1412.3635  | author:Maria Schuld, Ilya Sinayskiy, Francesco Petruccione category:quant-ph cs.LG cs.NE published:2014-12-11 summary:Perceptrons are the basic computational unit of artificial neural networks,as they model the activation mechanism of an output neuron due to incomingsignals from its neighbours. As linear classifiers, they play an important rolein the foundations of machine learning. In the context of the emerging field ofquantum machine learning, several attempts have been made to develop acorresponding unit using quantum information theory. Based on the quantum phaseestimation algorithm, this paper introduces a quantum perceptron modelimitating the step-activation function of a classical perceptron. This schemerequires resources in $\mathcal{O}(n)$ (where $n$ is the size of the input) andpromises efficient applications for more complex structures such as trainablequantum neural networks.
arxiv-1412-3756 | Certifying and removing disparate impact |  http://arxiv.org/abs/1412.3756  | author:Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, Suresh Venkatasubramanian category:stat.ML cs.CY published:2014-12-11 summary:What does it mean for an algorithm to be biased? In U.S. law, unintentionalbias is encoded via disparate impact, which occurs when a selection process haswidely different outcomes for different groups, even as it appears to beneutral. This legal determination hinges on a definition of a protected class(ethnicity, gender, religious practice) and an explicit description of theprocess. When the process is implemented using computers, determining disparate impact(and hence bias) is harder. It might not be possible to disclose the process.In addition, even if the process is open, it might be hard to elucidate in alegal setting how the algorithm makes its decisions. Instead of requiringaccess to the algorithm, we propose making inferences based on the data thealgorithm uses. We make four contributions to this problem. First, we link the legal notionof disparate impact to a measure of classification accuracy that while known,has received relatively little attention. Second, we propose a test fordisparate impact based on analyzing the information leakage of the protectedclass from the other data attributes. Third, we describe methods by which datamight be made unbiased. Finally, we present empirical evidence supporting theeffectiveness of our test for disparate impact and our approach for bothmasking bias and preserving relevant information in the data. Interestingly,our approach resembles some actual selection practices that have recentlyreceived legal scrutiny.
arxiv-1412-3714 | Feature Weight Tuning for Recursive Neural Networks |  http://arxiv.org/abs/1412.3714  | author:Jiwei Li category:cs.NE cs.AI cs.CL cs.LG published:2014-12-11 summary:This paper addresses how a recursive neural network model can automaticallyleave out useless information and emphasize important evidence, in other words,to perform "weight tuning" for higher-level representation acquisition. Wepropose two models, Weighted Neural Network (WNN) and Binary-Expectation NeuralNetwork (BENN), which automatically control how much one specific unitcontributes to the higher-level representation. The proposed model can beviewed as incorporating a more powerful compositional function for embeddingacquisition in recursive neural networks. Experimental results demonstrate thesignificant improvement over standard neural models.
arxiv-1412-3506 | Road Detection by One-Class Color Classification: Dataset and Experiments |  http://arxiv.org/abs/1412.3506  | author:Jose M. Alvarez, Theo Gevers, Antonio M. Lopez category:cs.CV published:2014-12-11 summary:Detecting traversable road areas ahead a moving vehicle is a key process formodern autonomous driving systems. A common approach to road detection consistsof exploiting color features to classify pixels as road or background. Thesealgorithms reduce the effect of lighting variations and weather conditions byexploiting the discriminant/invariant properties of different colorrepresentations. Furthermore, the lack of labeled datasets has motivated thedevelopment of algorithms performing on single images based on the assumptionthat the bottom part of the image belongs to the road surface. In this paper, we first introduce a dataset of road images taken at differenttimes and in different scenarios using an onboard camera. Then, we devise asimple online algorithm and conduct an exhaustive evaluation of differentclassifiers and the effect of using different color representation tocharacterize pixels.
arxiv-1412-3773 | Distinguishing cause from effect using observational data: methods and benchmarks |  http://arxiv.org/abs/1412.3773  | author:Joris M. Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, Bernhard Schölkopf category:cs.LG cs.AI stat.ML stat.OT published:2014-12-11 summary:The discovery of causal relationships from purely observational data is afundamental problem in science. The most elementary form of such a causaldiscovery problem is to decide whether X causes Y or, alternatively, Y causesX, given joint observations of two variables X, Y. An example is to decidewhether altitude causes temperature, or vice versa, given only jointmeasurements of both variables. Even under the simplifying assumptions of noconfounding, no feedback loops, and no selection bias, such bivariate causaldiscovery problems are challenging. Nevertheless, several approaches foraddressing those problems have been proposed in recent years. We review twofamilies of such methods: Additive Noise Methods (ANM) and InformationGeometric Causal Inference (IGCI). We present the benchmark CauseEffectPairsthat consists of data for 100 different cause-effect pairs selected from 37datasets from various domains (e.g., meteorology, biology, medicine,engineering, economy, etc.) and motivate our decisions regarding the "groundtruth" causal directions of all pairs. We evaluate the performance of severalbivariate causal discovery methods on these real-world benchmark data and inaddition on artificially simulated data. Our empirical results on real-worlddata indicate that certain methods are indeed able to distinguish cause fromeffect using only purely observational data, although more benchmark data wouldbe needed to obtain statistically significant conclusions. One of the bestperforming methods overall is the additive-noise method originally proposed byHoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of0.74+-0.05 on the real-world benchmark. As the main theoretical contribution ofthis work we prove the consistency of that method.
arxiv-1412-3709 | An active search strategy for efficient object class detection |  http://arxiv.org/abs/1412.3709  | author:Abel Gonzalez-Garcia, Alexander Vezhnevets, Vittorio Ferrari category:cs.CV published:2014-12-11 summary:Object class detectors typically apply a window classifier to all the windowsin a large set, either in a sliding window manner or using object proposals. Inthis paper, we develop an active search strategy that sequentially chooses thenext window to evaluate based on all the information gathered before. Thisresults in a substantial reduction in the number of classifier evaluations andin a more elegant approach in general. Our search strategy is guided by twoforces. First, we exploit context as the statistical relation between theappearance of a window and its location relative to the object, as observed inthe training set. This enables to jump across distant regions in the image(e.g. observing a sky region suggests that cars might be far below) and is doneefficiently in a Random Forest framework. Second, we exploit the score of theclassifier to attract the search to promising areas surrounding a highly scoredwindow, and to keep away from areas near low scored ones. Our search strategycan be applied on top of any classifier as it treats it as a black-box. Inexperiments with R-CNN on the challenging SUN2012 dataset, our method matchesthe detection accuracy of evaluating all windows independently, whileevaluating 9x fewer windows.
arxiv-1412-3613 | A Novel Adaptive Possibilistic Clustering Algorithm |  http://arxiv.org/abs/1412.3613  | author:Spyridoula D. Xenaki, Konstantinos D. Koutroumbas, Athanasios A. Rontogiannis category:cs.CV published:2014-12-11 summary:In this paper a novel possibilistic c-means clustering algorithm, calledAdaptive Possibilistic c-means, is presented. Its main feature is that {\itall} its parameters, after their initialization, are properly adapted duringits execution. Provided that the algorithm starts with a reasonableoverestimate of the number of physical clusters formed by the data, it iscapable, in principle, to unravel them (a long-standing issue in the clusteringliterature). This is due to the fully adaptive nature of the proposed algorithmthat enables the removal of the clusters that gradually become obsolete. Inaddition, the adaptation of all its parameters increases the flexibility of thealgorithm in following the variations in the formation of the clusters thatoccur from iteration to iteration. Theoretical results that are indicative ofthe convergence behavior of the algorithm are also provided. Finally, extensivesimulation results on both synthetic and real data highlight the effectivenessof the proposed algorithm.
arxiv-1412-3596 | EgoSampling: Fast-Forward and Stereo for Egocentric Videos |  http://arxiv.org/abs/1412.3596  | author:Yair Poleg, Tavi Halperin, Chetan Arora, Shmuel Peleg category:cs.CV cs.MM published:2014-12-11 summary:While egocentric cameras like GoPro are gaining popularity, the videos theycapture are long, boring, and difficult to watch from start to end. Fastforwarding (i.e. frame sampling) is a natural choice for faster video browsing.However, this accentuates the shake caused by natural head motion, making thefast forwarded video useless. We propose EgoSampling, an adaptive frame sampling that gives more stablefast forwarded videos. Adaptive frame sampling is formulated as energyminimization, whose optimal solution can be found in polynomial time. In addition, egocentric video taken while walking suffers from the left-rightmovement of the head as the body weight shifts from one leg to another. We turnthis drawback into a feature: Stereo video can be created by sampling theframes from the left most and right most head positions of each step, formingapproximate stereo-pairs.
arxiv-1412-3617 | Efficient penalty search for multiple changepoint problems |  http://arxiv.org/abs/1412.3617  | author:Kaylea Haynes, Idris A. Eckley, Paul Fearnhead category:stat.CO stat.ML published:2014-12-11 summary:In the multiple changepoint setting, various search methods have beenproposed which involve optimising either a constrained or penalised costfunction over possible numbers and locations of changepoints using dynamicprogramming. Such methods are typically computationally intensive. Recent workin the penalised optimisation setting has focussed on developing apruning-based approach which gives an improved computational cost that, undercertain conditions, is linear in the number of data points. Such an approachnaturally requires the specification of a penalty to avoid under/over-fitting.Work has been undertaken to identify the appropriate penalty choice for datagenerating processes with known distributional form, but in many applicationsthe model assumed for the data is not correct and these penalty choices are notalways appropriate. Consequently it is desirable to have an approach thatenables us to compare segmentations for different choices of penalty. To thisend we present a method to obtain optimal changepoint segmentations of datasequences for all penalty values across a continuous range. This permits anevaluation of the various segmentations to identify a suitably parsimoniouspenalty choice. The computational complexity of this approach can be linear inthe number of data points and linear in the difference between the number ofchangepoints in the optimal segmentations for the smallest and largest penaltyvalues. This can be orders of magnitude faster than alternative approaches thatfind optimal segmentations for a range of the number of changepoints.
arxiv-1412-3708 | Compact Part-Based Image Representations |  http://arxiv.org/abs/1412.3708  | author:Marc Goessling, Yali Amit category:cs.CV cs.LG stat.ML published:2014-12-11 summary:Learning compact, interpretable image representations is a very natural taskwhich has not been solved satisfactorily even for simple classes of binaryimages. In this paper, we review various ways of composing parts (or experts)for binary data and argue that competitive forms of interaction are best suitedto learn low-dimensional representations. We propose a new composition rulewhich discourages parts from focusing on similar structures and which penalizesopposing votes strongly so that abstaining from voting becomes more attractive.We also introduce a novel sequential initialization procedure based on aprocess of oversimplification and correction. Experiments show that with ourapproach very intuitive models can be learned.
arxiv-1412-4031 | High-level numerical simulations of noise in CCD and CMOS photosensors: review and tutorial |  http://arxiv.org/abs/1412.4031  | author:Mikhail Konnik, James Welsh category:astro-ph.IM cs.CV published:2014-12-11 summary:In many applications, such as development and testing of image processingalgorithms, it is often necessary to simulate images containing realistic noisefrom solid-state photosensors. A high-level model of CCD and CMOS photosensorsbased on a literature review is formulated in this paper. The model includesphoto-response non-uniformity, photon shot noise, dark current Fixed PatternNoise, dark current shot noise, offset Fixed Pattern Noise, source followernoise, sense node reset noise, and quantisation noise. The model also includesvoltage-to-voltage, voltage-to-electrons, and analogue-to-digital converternon-linearities. The formulated model can be used to create synthetic imagesfor testing and validation of image processing algorithms in the presence ofrealistic images noise. An example of the simulated CMOS photosensor and acomparison with a custom-made CMOS hardware sensor is presented. Procedures forcharacterisation from both light and dark noises are described. Experimentalresults that confirm the validity of the numerical model are provided. Thepaper addresses the issue of the lack of comprehensive high-level photosensormodels that enable engineers to simulate realistic effects of noise on theimages obtained from solid-state photosensors.
arxiv-1412-3555 | Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling |  http://arxiv.org/abs/1412.3555  | author:Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio category:cs.NE cs.LG published:2014-12-11 summary:In this paper we compare different types of recurrent units in recurrentneural networks (RNNs). Especially, we focus on more sophisticated units thatimplement a gating mechanism, such as a long short-term memory (LSTM) unit anda recently proposed gated recurrent unit (GRU). We evaluate these recurrentunits on the tasks of polyphonic music modeling and speech signal modeling. Ourexperiments revealed that these advanced recurrent units are indeed better thanmore traditional recurrent units such as tanh units. Also, we found GRU to becomparable to LSTM.
arxiv-1501-01209 | Reinforcement Learning and Nonparametric Detection of Game-Theoretic Equilibrium Play in Social Networks |  http://arxiv.org/abs/1501.01209  | author:Omid Namvar Gharehshiran, William Hoiles, Vikram Krishnamurthy category:cs.GT cs.LG cs.SI stat.ML published:2014-12-11 summary:This paper studies two important signal processing aspects of equilibriumbehavior in non-cooperative games arising in social networks, namely,reinforcement learning and detection of equilibrium play. The first part of thepaper presents a reinforcement learning (adaptive filtering) algorithm thatfacilitates learning an equilibrium by resorting to diffusion cooperationstrategies in a social network. Agents form homophilic social groups, withinwhich they exchange past experiences over an undirected graph. It is shownthat, if all agents follow the proposed algorithm, their global behavior isattracted to the correlated equilibria set of the game. The second part of thepaper provides a test to detect if the actions of agents are consistent withplay from the equilibrium of a concave potential game. The theory of revealedpreference from microeconomics is used to construct a non-parametric decisiontest and statistical test which only require the probe and associated actionsof agents. A stochastic gradient algorithm is given to optimize the probe inreal time to minimize the Type-II error probabilities of the detection testsubject to specified Type-I error probability. We provide a real-world exampleusing the energy market, and a numerical example to detect malicious agents inan online social network.
arxiv-1412-5068 | Towards Deep Neural Network Architectures Robust to Adversarial Examples |  http://arxiv.org/abs/1412.5068  | author:Shixiang Gu, Luca Rigazio category:cs.LG cs.CV cs.NE published:2014-12-11 summary:Recent work has shown deep neural networks (DNNs) to be highly susceptible towell-designed, small perturbations at the input layer, or so-called adversarialexamples. Taking images as an example, such distortions are oftenimperceptible, but can result in 100% mis-classification for a state of the artDNN. We study the structure of adversarial examples and explore networktopology, pre-processing and training strategies to improve the robustness ofDNNs. We perform various experiments to assess the removability of adversarialexamples by corrupting with additional noise and pre-processing with denoisingautoencoders (DAEs). We find that DAEs can remove substantial amounts of theadversarial noise. How- ever, when stacking the DAE with the original DNN, theresulting network can again be attacked by new adversarial examples with evensmaller distortion. As a solution, we propose Deep Contractive Network, a modelwith a new end-to-end training procedure that includes a smoothness penaltyinspired by the contractive autoencoder (CAE). This increases the networkrobustness to adversarial examples, without a significant performance penalty.
arxiv-1412-3705 | A Topic Modeling Approach to Ranking |  http://arxiv.org/abs/1412.3705  | author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG stat.ML published:2014-12-11 summary:We propose a topic modeling approach to the prediction of preferences inpairwise comparisons. We develop a new generative model for pairwisecomparisons that accounts for multiple shared latent rankings that areprevalent in a population of users. This new model also captures inconsistentuser behavior in a natural way. We show how the estimation of latent rankingsin the new generative model can be formally reduced to the estimation of topicsin a statistically equivalent topic modeling problem. We leverage recentadvances in the topic modeling literature to develop an algorithm that canlearn shared latent rankings with provable consistency as well as sample andcomputational complexity guarantees. We demonstrate that the new approach isempirically competitive with the current state-of-the-art approaches inpredicting preferences on some semi-synthetic and real world datasets.
arxiv-1412-3421 | Multi-Atlas Segmentation of Biomedical Images: A Survey |  http://arxiv.org/abs/1412.3421  | author:Juan Eugenio Iglesias, Mert Rory Sabuncu category:cs.CV published:2014-12-10 summary:Multi-atlas segmentation (MAS), first introduced and popularized by thepioneering work of Rohlfing, Brandt, Menzel and Maurer Jr (2004), Klein, Mensh,Ghosh, Tourville and Hirsch (2005), and Heckemann, Hajnal, Aljabar, Rueckertand Hammers (2006), is becoming one of the most widely-used and successfulimage segmentation techniques in biomedical applications. By manipulating andutilizing the entire dataset of "atlases" (training images that have beenpreviously labeled, e.g., manually by an expert), rather than some model-basedaverage representation, MAS has the flexibility to better capture anatomicalvariation, thus offering superior segmentation accuracy. This benefit, however,typically comes at a high computational cost. Recent advancements in computerhardware and image processing software have been instrumental in addressingthis challenge and facilitated the wide adoption of MAS. Today, MAS has come along way and the approach includes a wide array of sophisticated algorithmsthat employ ideas from machine learning, probabilistic modeling, optimization,and computer vision, among other fields. This paper presents a survey ofpublished MAS algorithms and studies that have applied these methods to variousbiomedical problems. In writing this survey, we have three distinct aims. Ourprimary goal is to document how MAS was originally conceived, later evolved,and now relates to alternative methods. Second, this paper is intended to be adetailed reference of past research activity in MAS, which now spans over adecade (2003 - 2014) and entails novel methodological developments andapplication-specific solutions. Finally, our goal is to also present aperspective on the future of MAS, which, we believe, will be one of thedominant approaches in biomedical image segmentation.
arxiv-1412-3397 | Sequential Labeling with online Deep Learning |  http://arxiv.org/abs/1412.3397  | author:Gang Chen, Ran Xu, Sargur Srihari category:cs.LG 68T10 I.2.6 published:2014-12-10 summary:Deep learning has attracted great attention recently and yielded the state ofthe art performance in dimension reduction and classification problems.However, it cannot effectively handle the structured output prediction, e.g.sequential labeling. In this paper, we propose a deep learning structure, whichcan learn discriminative features for sequential labeling problems. Morespecifically, we add the inter-relationship between labels in our deep learningstructure, in order to incorporate the context information from the sequentialdata. Thus, our model is more powerful than linear Conditional Random Fields(CRFs) because the objective function learns latent non-linear features so thattarget labeling can be better predicted. We pretrain the deep structure withstacked restricted Boltzmann machines (RBMs) for feature learning and optimizeour objective function with online learning algorithm, a mixture of perceptrontraining and stochastic gradient descent. We test our model on differentchallenge tasks, and show that our model outperforms significantly over thecompletive baselines.
arxiv-1412-3328 | Memory vectors for similarity search in high-dimensional spaces |  http://arxiv.org/abs/1412.3328  | author:Ahmet Iscen, Teddy Furon, Vincent Gripon, Michael Rabbat, Hervé Jégou category:cs.CV cs.DB published:2014-12-10 summary:We study an indexing architecture to store and search in a database ofhigh-dimensional vectors. This architecture is composed of several memoryunits, each of which summarizes a fraction of the database by a singlerepresentative vector.The potential similarity of the query to one of thevectors stored in the memory unit is gauged by a simple correlation with thememory unit's representative vector. This representative optimizes the test ofthe following hypothesis: the query is independent from any vector in thememory unit vs. the query is a simple perturbation of one of the storedvectors. Compared to exhaustive search, our approach finds the most similardatabase vectors significantly faster without a noticeable reduction in searchquality. Interestingly, the reduction of complexity is provably better inhigh-dimensional spaces. We empirically demonstrate its practical interest in alarge-scale image search scenario with off-the-shelf state-of-the-artdescriptors.
arxiv-1412-3191 | Bach in 2014: Music Composition with Recurrent Neural Network |  http://arxiv.org/abs/1412.3191  | author:I-Ting Liu, Bhiksha Ramakrishnan category:cs.AI cs.NE published:2014-12-10 summary:We propose a framework for computer music composition that uses resilientpropagation (RProp) and long short term memory (LSTM) recurrent neural network.In this paper, we show that LSTM network learns the structure andcharacteristics of music pieces properly by demonstrating its ability torecreate music. We also show that predicting existing music using RPropoutperforms Back propagation through time (BPTT).
arxiv-1412-3474 | Deep Domain Confusion: Maximizing for Domain Invariance |  http://arxiv.org/abs/1412.3474  | author:Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell category:cs.CV published:2014-12-10 summary:Recent reports suggest that a generic supervised deep CNN model trained on alarge-scale dataset reduces, but does not remove, dataset bias on a standardbenchmark. Fine-tuning deep models in a new domain can require a significantamount of data, which for many applications is simply not available. We proposea new CNN architecture which introduces an adaptation layer and an additionaldomain confusion loss, to learn a representation that is both semanticallymeaningful and domain invariant. We additionally show that a domain confusionmetric can be used for model selection to determine the dimension of anadaptation layer and the best position for the layer in the CNN architecture.Our proposed adaptation method offers empirical performance which exceedspreviously published results on a standard benchmark visual domain adaptationtask.
arxiv-1412-8109 | Complex support vector machines regression for robust channel estimation in LTE downlink system |  http://arxiv.org/abs/1412.8109  | author:Anis Charrada, Abdelaziz Samet category:cs.IT cs.LG math.IT published:2014-12-10 summary:In this paper, the problem of channel estimation for LTE Downlink system inthe environment of high mobility presenting non-Gaussian impulse noiseinterfering with reference signals is faced. The estimation of the frequencyselective time varying multipath fading channel is performed by using a channelestimator based on a nonlinear complex Support Vector Machine Regression (SVR)which is applied to Long Term Evolution (LTE) downlink. The estimationalgorithm makes use of the pilot signals to estimate the total frequencyresponse of the highly selective fading multipath channel. Thus, the algorithmmaps trained data into a high dimensional feature space and uses the structuralrisk minimization principle to carry out the regression estimation for thefrequency response function of the fading channel. The obtained results showthe effectiveness of the proposed method which has better performance than theconventional Least Squares (LS) and Decision Feedback methods to track thevariations of the fading multipath channel.
arxiv-1412-3411 | GP-select: Accelerating EM using adaptive subspace preselection |  http://arxiv.org/abs/1412.3411  | author:Jacquelyn A. Shelton, Jan Gasthaus, Zhenwen Dai, Joerg Luecke, Arthur Gretton category:stat.ML cs.LG published:2014-12-10 summary:We propose a nonparametric procedure to achieve fast inference in generativegraphical models when the number of latent states is very large. The approachis based on iterative latent variable preselection, where we alternate betweenlearning a 'selection function' to reveal the relevant latent variables, anduse this to obtain a compact approximation of the posterior distribution forEM; this can make inference possible where the number of possible latent statesis e.g. exponential in the number of latent variables, whereas an exactapproach would be computationally unfeasible. We learn the selection functionentirely from the observed data and current EM state via Gaussian processregression: this is by contrast with earlier approaches, where selections werehand-designed for each problem setting. We show our approach to perform as wellas these bespoke selection functions on a wide variety of inference problems:in particular, for the challenging case of a hierarchical model for objectlocalization with occlusion, we achieve results that match a customizedstate-of-the-art selection method, at a far lower computational cost.
arxiv-1412-3684 | Object Recognition Using Deep Neural Networks: A Survey |  http://arxiv.org/abs/1412.3684  | author:Soren Goyal, Paul Benjamin category:cs.CV cs.LG cs.NE published:2014-12-10 summary:Recognition of objects using Deep Neural Networks is an active area ofresearch and many breakthroughs have been made in the last few years. The paperattempts to indicate how far this field has progressed. The paper brieflydescribes the history of research in Neural Networks and describe several ofthe recent advances in this field. The performances of recently developedNeural Network Algorithm over benchmark datasets have been tabulated. Finally,some the applications of this field have been provided.
arxiv-1412-3369 | Candidate Constrained CRFs for Loss-Aware Structured Prediction |  http://arxiv.org/abs/1412.3369  | author:Faruk Ahmed, Daniel Tarlow, Dhruv Batra category:cs.CV published:2014-12-10 summary:When evaluating computer vision systems, we are often concerned withperformance on a task-specific evaluation measure such as theIntersection-Over-Union score used in the PASCAL VOC image segmentationchallenge. Ideally, our systems would be tuned specifically to these evaluationmeasures. However, despite much work on loss-aware structured prediction, topperforming systems do not use these techniques. In this work, we seek toaddress this problem, incorporating loss-aware prediction in a manner that isamenable to the approaches taken by top performing systems. Our main idea is tosimultaneously leverage two systems: a highly tuned pipeline system as is foundon top of leaderboards, and a traditional CRF. We show how to combine highquality candidate solutions from the pipeline with the probabilistic approachof the CRF that is amenable to loss-aware prediction. The result is that we canuse loss-aware prediction methodology to improve performance of the highlytuned pipeline system.
arxiv-1412-3276 | Generalised Entropy MDPs and Minimax Regret |  http://arxiv.org/abs/1412.3276  | author:Emmanouil G. Androulakis, Christos Dimitrakakis category:cs.LG stat.ML published:2014-12-10 summary:Bayesian methods suffer from the problem of how to specify prior beliefs. Oneinteresting idea is to consider worst-case priors. This requires solving astochastic zero-sum game. In this paper, we extend well-known results frombandit theory in order to discover minimax-Bayes policies and discuss when theyare practical.
arxiv-1412-3489 | Quantum Deep Learning |  http://arxiv.org/abs/1412.3489  | author:Nathan Wiebe, Ashish Kapoor, Krysta M. Svore category:quant-ph cs.LG cs.NE published:2014-12-10 summary:In recent years, deep learning has had a profound impact on machine learningand artificial intelligence. At the same time, algorithms for quantum computershave been shown to efficiently solve some problems that are intractable onconventional, classical computers. We show that quantum computing not onlyreduces the time required to train a deep restricted Boltzmann machine, butalso provides a richer and more comprehensive framework for deep learning thanclassical computing and leads to significant improvements in the optimizationof the underlying objective function. Our quantum methods also permit efficienttraining of full Boltzmann machines and multi-layer, fully connected models anddo not have well known classical counterparts.
arxiv-1412-3432 | Detecting Overlapping Communities in Networks Using Spectral Methods |  http://arxiv.org/abs/1412.3432  | author:Yuan Zhang, Elizaveta Levina, Ji Zhu category:stat.ML published:2014-12-10 summary:Community detection is a fundamental problem in network analysis which ismade more challenging by overlaps between communities which often occur inpractice. Here we propose a general, flexible, and interpretable generativemodel for overlapping communities, which can be thought of as a generalizationof the degree-corrected stochastic block model. We develop an efficientspectral algorithm for estimating the community memberships, which deals withthe overlaps by employing the K-medians algorithm rather than the usual K-meansfor clustering in the spectral domain. We show that the algorithm isasymptotically consistent when networks are not too sparse and the overlapsbetween communities not too large. Numerical experiments on both simulatednetworks and many real social networks demonstrate that our method performsvery well compared to a number of benchmark methods for overlapping communitydetection.
arxiv-1412-3161 | Object-centric Sampling for Fine-grained Image Classification |  http://arxiv.org/abs/1412.3161  | author:Xiaoyu Wang, Tianbao Yang, Guobin Chen, Yuanqing Lin category:cs.CV published:2014-12-10 summary:This paper proposes to go beyond the state-of-the-art deep convolutionalneural network (CNN) by incorporating the information from object detection,focusing on dealing with fine-grained image classification. Unfortunately, CNNsuffers from over-fiting when it is trained on existing fine-grained imageclassification benchmarks, which typically only consist of less than a few tensof thousands training images. Therefore, we first construct a large-scalefine-grained car recognition dataset that consists of 333 car classes with morethan 150 thousand training images. With this large-scale dataset, we are ableto build a strong baseline for CNN with top-1 classification accuracy of 81.6%.One major challenge in fine-grained image classification is that many classesare very similar to each other while having large within-class variation. Onecontributing factor to the within-class variation is cluttered imagebackground. However, the existing CNN training takes uniform window samplingover the image, acting as blind on the location of the object of interest. Incontrast, this paper proposes an \emph{object-centric sampling} (OCS) schemethat samples image windows based on the object location information. Thechallenge in using the location information lies in how to design powerfulobject detector and how to handle the imperfectness of detection results. Tothat end, we design a saliency-aware object detection approach specific for thesetting of fine-grained image classification, and the uncertainty of detectionresults are naturally handled in our OCS scheme. Our framework is demonstratedto be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on thelarge-scale fine-grained car classification dataset.
arxiv-1412-3409 | Teaching Deep Convolutional Neural Networks to Play Go |  http://arxiv.org/abs/1412.3409  | author:Christopher Clark, Amos Storkey category:cs.AI cs.LG cs.NE published:2014-12-10 summary:Mastering the game of Go has remained a long standing challenge to the fieldof AI. Modern computer Go systems rely on processing millions of possiblefuture positions to play well, but intuitively a stronger and more 'humanlike'way to play the game would be to rely on pattern recognition abilities ratherthen brute force computation. Following this sentiment, we train deepconvolutional neural networks to play Go by training them to predict the movesmade by expert Go players. To solve this problem we introduce a number of noveltechniques, including a method of tying weights in the network to 'hard code'symmetries that are expect to exist in the target function, and demonstrate inan ablation study they considerably improve performance. Our final networks areable to achieve move prediction accuracies of 41.1% and 44.4% on two differentGo datasets, surpassing previous state of the art on this task by significantmargins. Additionally, while previous move prediction programs have not yieldedstrong Go playing programs, we show that the networks trained in this workacquired high levels of skill. Our convolutional neural networks canconsistently defeat the well known Go program GNU Go, indicating it is state ofthe art among programs that do not use Monte Carlo Tree Search. It is also ableto win some games against state of the art Go playing program Fuego while usinga fraction of the play time. This success at playing Go indicates high levelprinciples of the game were learned.
arxiv-1412-3297 | Convergence and rate of convergence of some greedy algorithms in convex optimization |  http://arxiv.org/abs/1412.3297  | author:Vladimir Temlyakov category:stat.ML math.NA published:2014-12-10 summary:The paper gives a systematic study of the approximate versions of threegreedy-type algorithms that are widely used in convex optimization. Byapproximate version we mean the one where some of evaluations are made with anerror. Importance of such versions of greedy-type algorithms in convexoptimization and in approximation theory was emphasized in previous literature.
arxiv-1412-3159 | Road Detection via On--line Label Transfer |  http://arxiv.org/abs/1412.3159  | author:José M. Álvarez, Ferran Diego, Joan Serrat, Antonio M. López category:cs.CV published:2014-12-10 summary:Vision-based road detection is an essential functionality for supportingadvanced driver assistance systems (ADAS) such as road following and vehicleand pedestrian detection. The major challenges of road detection are dealingwith shadows and lighting variations and the presence of other objects in thescene. Current road detection algorithms characterize road areas at pixel leveland group pixels accordingly. However, these algorithms fail in presence ofstrong shadows and lighting variations. Therefore, we propose a road detectionalgorithm based on video alignment. The key idea of the algorithm is to exploitthe similarities occurred when a vehicle follows the same trajectory more thanonce. In this way, road areas are learned in a first ride and then, this roadknowledge is used to infer areas depicting drivable road surfaces in subsequentrides. Two different experiments are conducted to validate the proposal ondifferent video sequences taken at different scenarios and different daytime.The former aims to perform on-line road detection. The latter aims to performoff-line road detection and is applied to automatically generate theground-truth necessary to validate road detection algorithms. Qualitative andquantitative evaluations prove that the proposed algorithm is a valid roaddetection approach.
arxiv-1412-3336 | Statistical Patterns in Written Language |  http://arxiv.org/abs/1412.3336  | author:Damián H. Zanette category:cs.CL published:2014-12-10 summary:Quantitative linguistics has been allowed, in the last few decades, withinthe admittedly blurry boundaries of the field of complex systems. A growinghost of applied mathematicians and statistical physicists devote their effortsto disclose regularities, correlations, patterns, and structural properties oflanguage streams, using techniques borrowed from statistics and informationtheory. Overall, results can still be categorized as modest, but the prospectsare promising: medium- and long-range features in the organization of humanlanguage -which are beyond the scope of traditional linguistics- have alreadyemerged from this kind of analysis and continue to be reported, contributing anew perspective to our understanding of this most complex communication system.This short book is intended to review some of these recent contributions.
arxiv-1412-3009 | Brain Tumor Detection Based on Bilateral Symmetry Information |  http://arxiv.org/abs/1412.3009  | author:Narkhede Sachin, Deven Shah, Vaishali Khairnar, Sujata Kadu category:cs.CV published:2014-12-09 summary:Advances in computing technology have allowed researchers across many fieldsof endeavor to collect and maintain vast amounts of observational statisticaldata such as clinical data,biological patient data,data regarding access of websites,financial data,and the like.Brain Magnetic ResonanceImaging(MRI)segmentation is a complex problem in the field of medical imagingdespite various presented methods.MR image of human brain can be divided intoseveral sub regions especially soft tissues such as gray matter,white matterand cerebrospinal fluid.Although edge information is the main clue in imagesegmentation,it can not get a better result in analysis the content of imageswithout combining other information.The segmentation of brain tissue in themagnetic resonance imaging(MRI)is very important for detecting the existenceand outlines of tumors.In this paper,an algorithm about segmentation based onthe symmetry character of brain MRI image is presented.Our goal is to detectthe position and boundary of tumors automatically.Experiments were conducted onreal pictures,and the results show that the algorithm is flexible andconvenient.
arxiv-1412-3128 | Real-Time Grasp Detection Using Convolutional Neural Networks |  http://arxiv.org/abs/1412.3128  | author:Joseph Redmon, Anelia Angelova category:cs.RO cs.CV published:2014-12-09 summary:We present an accurate, real-time approach to robotic grasp detection basedon convolutional neural networks. Our network performs single-stage regressionto graspable bounding boxes without using standard sliding window or regionproposal techniques. The model outperforms state-of-the-art approaches by 14percentage points and runs at 13 frames per second on a GPU. Our network cansimultaneously perform classification so that in a single step it recognizesthe object and finds a good grasp rectangle. A modification to this modelpredicts multiple grasps per object by using a locally constrained predictionmechanism. The locally constrained model performs significantly better,especially on objects that can be grasped in a variety of ways.
arxiv-1412-3051 | POPE: Post Optimization Posterior Evaluation of Likelihood Free Models |  http://arxiv.org/abs/1412.3051  | author:Edward Meeds, Michael Chiang, Mary Lee, Olivier Cinquin, John Lowengrub, Max Welling category:stat.ML q-bio.QM published:2014-12-09 summary:In many domains, scientists build complex simulators of natural phenomenathat encode their hypotheses about the underlying processes. These simulatorscan be deterministic or stochastic, fast or slow, constrained or unconstrained,and so on. Optimizing the simulators with respect to a set of parameter valuesis common practice, resulting in a single parameter setting that minimizes anobjective subject to constraints. We propose a post optimization posterioranalysis that computes and visualizes all the models that can generate equallygood or better simulation results, subject to constraints. These optimizationposteriors are desirable for a number of reasons among which easyinterpretability, automatic parameter sensitivity and correlation analysis andposterior predictive analysis. We develop a new sampling framework based onapproximate Bayesian computation (ABC) with one-sided kernels. In collaborationwith two groups of scientists we applied POPE to two important biologicalsimulators: a fast and stochastic simulator of stem-cell cycling and a slow anddeterministic simulator of tumor growth patterns.
arxiv-1412-3078 | Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process Regression |  http://arxiv.org/abs/1412.3078  | author:Jun Wei Ng, Marc Peter Deisenroth category:stat.ML cs.AI cs.LG stat.CO published:2014-12-09 summary:We propose a practical and scalable Gaussian process model for large-scalenonlinear probabilistic regression. Our mixture-of-experts model isconceptually simple and hierarchically recombines computations for an overallapproximation of a full Gaussian process. Closed-form and distributedcomputations allow for efficient and massive parallelisation while keeping thememory consumption small. Given sufficient computing resources, our model canhandle arbitrarily large data sets, without explicit sparse approximations. Weprovide strong experimental evidence that our model can be applied to largedata sets of sizes far beyond millions. Hence, our model has the potential tolay the foundation for general large-scale Gaussian process research.
arxiv-1412-3100 | Semi-Supervised Learning with Heterophily |  http://arxiv.org/abs/1412.3100  | author:Wolfgang Gatterbauer category:cs.LG cs.DB published:2014-12-09 summary:We propose a novel linear semi-supervised learning formulation that isderived from a solid probabilistic framework: belief propagation. We show thatour formulation generalizes a number of label propagation algorithms describedin the literature by allowing them to propagate generalized assumptions aboutinfluences between classes of neighboring nodes. We call this formulationSemi-Supervised Learning with Heterophily (SSL-H). We also show how theaffinity matrix can be learned from observed data with a simple convexoptimization framework that is inspired by locally linear embedding. We callthis approach Linear Heterophily Estimation (LHE). Experiments on syntheticdata show that both approaches combined can learn heterophily of a graph with1M nodes, 10M edges and few labels in under 1min, and give better labelingaccuracies than a baseline method in the case of small fraction of explicitlylabeled nodes.
arxiv-1412-3121 | Multimodal Transfer Deep Learning with Applications in Audio-Visual Recognition |  http://arxiv.org/abs/1412.3121  | author:Seungwhan Moon, Suyoun Kim, Haohan Wang category:cs.NE cs.LG published:2014-12-09 summary:We propose a transfer deep learning (TDL) framework that can transfer theknowledge obtained from a single-modal neural network to a network with adifferent modality. Specifically, we show that we can leverage speech data tofine-tune the network trained for video recognition, given an initial set ofaudio-video parallel dataset within the same semantics. Our approach firstlearns the analogy-preserving embeddings between the abstract representationslearned from intermediate layers of each network, allowing for semantics-leveltransfer between the source and target modalities. We then apply our neuralnetwork operation that fine-tunes the target network with the additionalknowledge transferred from the source network, while keeping the topology ofthe target network unchanged. While we present an audio-visual recognition taskas an application of our approach, our framework is flexible and thus can workwith any multimodal dataset, or with any already-existing deep networks thatshare the common underlying semantics. In this work in progress report, we aimto provide comprehensive results of different configurations of the proposedapproach on two widely used audio-visual datasets, and we discuss potentialapplications of the proposed approach.
arxiv-1412-4005 | Sparsity and adaptivity for the blind separation of partially correlated sources |  http://arxiv.org/abs/1412.4005  | author:Jerome Bobin, Jeremy Rapin, Anthony Larue, Jean-Luc Starck category:stat.AP cs.LG stat.ML published:2014-12-09 summary:Blind source separation (BSS) is a very popular technique to analyzemultichannel data. In this context, the data are modeled as the linearcombination of sources to be retrieved. For that purpose, standard BSS methodsall rely on some discrimination principle, whether it is statisticalindependence or morphological diversity, to distinguish between the sources.However, dealing with real-world data reveals that such assumptions are rarelyvalid in practice: the signals of interest are more likely partiallycorrelated, which generally hampers the performances of standard BSS methods.In this article, we introduce a novel sparsity-enforcing BSS method coinedAdaptive Morphological Component Analysis (AMCA), which is designed to retrievesparse and partially correlated sources. More precisely, it makes profit of anadaptive re-weighting scheme to favor/penalize samples based on their level ofcorrelation. Extensive numerical experiments have been carried out which showthat the proposed method is robust to the partial correlation of sources whilestandard BSS techniques fail. The AMCA algorithm is evaluated in the field ofastrophysics for the separation of physical components from microwave data.
arxiv-1412-2929 | Bayesian Fisher's Discriminant for Functional Data |  http://arxiv.org/abs/1412.2929  | author:Yao-Hsiang Yang, Lu-Hung Chen, Chieh-Chih Wang, Chu-Song Chen category:cs.LG stat.ML published:2014-12-09 summary:We propose a Bayesian framework of Gaussian process in order to extendFisher's discriminant to classify functional data such as spectra and images.The probability structure for our extended Fisher's discriminant is explicitlyformulated, and we utilize the smoothness assumptions of functional data asprior probabilities. Existing methods which directly employ the smoothnessassumption of functional data can be shown as special cases within thisframework given corresponding priors while their estimates of the unknowns areone-step approximations to the proposed MAP estimates. Empirical results onvarious simulation studies and different real applications show that theproposed method significantly outperforms the other Fisher's discriminantmethods for functional data.
arxiv-1412-2954 | Max vs Min: Tensor Decomposition and ICA with nearly Linear Sample Complexity |  http://arxiv.org/abs/1412.2954  | author:Santosh S. Vempala, Ying Xiao category:cs.DS cs.LG stat.ML published:2014-12-09 summary:We present a simple, general technique for reducing the sample complexity ofmatrix and tensor decomposition algorithms applied to distributions. We use thetechnique to give a polynomial-time algorithm for standard ICA with samplecomplexity nearly linear in the dimension, thereby improving substantially onprevious bounds. The analysis is based on properties of random polynomials,namely the spacings of an ensemble of polynomials. Our technique also appliesto other applications of tensor decompositions, including spherical Gaussianmixture models.
arxiv-1412-2873 | Cancer Detection with Multiple Radiologists via Soft Multiple Instance Logistic Regression and $L_1$ Regularization |  http://arxiv.org/abs/1412.2873  | author:Inna Stainvas, Alexandra Manevitch, Isaac Leichter category:cs.CV published:2014-12-09 summary:This paper deals with the multiple annotation problem in medical applicationof cancer detection in digital images. The main assumption is that thoughimages are labeled by many experts, the number of images read by the sameexpert is not large. Thus differing with the existing work on modeling eachexpert and ground truth simultaneously, the multi annotation information isused in a soft manner. The multiple labels from different experts are used toestimate the probability of the findings to be marked as malignant. Thelearning algorithm minimizes the Kullback Leibler (KL) divergence between themodeled probabilities and desired ones constraining the model to be compact.The probabilities are modeled by logit regression and multiple instancelearning concept is used by us. Experiments on a real-life computer aided diagnosis (CAD) problem for CXR CADlung cancer detection demonstrate that the proposed algorithm leads to similarresults as learning with a binary RVMMIL classifier or a mixture of binaryRVMMIL models per annotator. However, this model achieves a smaller complexityand is more preferable in practice.
arxiv-1412-2859 | Circumventing the Curse of Dimensionality in Prediction: Causal Rate-Distortion for Infinite-Order Markov Processes |  http://arxiv.org/abs/1412.2859  | author:Sarah Marzen, James P. Crutchfield category:cs.LG nlin.CD q-bio.NC stat.ML published:2014-12-09 summary:Predictive rate-distortion analysis suffers from the curse of dimensionality:clustering arbitrarily long pasts to retain information about arbitrarily longfutures requires resources that typically grow exponentially with length. Thechallenge is compounded for infinite-order Markov processes, since conditioningon finite sequences cannot capture all of their past dependencies. Spectralarguments show that algorithms which cluster finite-length sequences faildramatically when the underlying process has long-range temporal correlationsand can fail even for processes generated by finite-memory hidden Markovmodels. We circumvent the curse of dimensionality in rate-distortion analysisof infinite-order processes by casting predictive rate-distortion objectivefunctions in terms of the forward- and reverse-time causal states ofcomputational mechanics. Examples demonstrate that the resulting causalrate-distortion theory substantially improves current predictiverate-distortion analyses.
arxiv-1412-2821 | Zipf's Law and the Frequency of Characters or Words of Oracles |  http://arxiv.org/abs/1412.2821  | author:Xiuli Wang category:cs.CL math.ST stat.TH published:2014-12-09 summary:The article discusses the frequency of characters of Oracle,concluding thatthe frequency and the rank of a word or character is fit to Zipf-Mandelboit Lawor Zipf's law with three parameters,and figuring out the parameters based onthe frequency,and pointing out that what some researchers of Oracle call theassembling on the two ends is just a description by their impression about theOracle data.
arxiv-1412-2863 | Score Function Features for Discriminative Learning: Matrix and Tensor Framework |  http://arxiv.org/abs/1412.2863  | author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG stat.ML published:2014-12-09 summary:Feature learning forms the cornerstone for tackling challenging learningproblems in domains such as speech, computer vision and natural languageprocessing. In this paper, we consider a novel class of matrix andtensor-valued features, which can be pre-trained using unlabeled samples. Wepresent efficient algorithms for extracting discriminative information, giventhese pre-trained features and labeled samples for any related task. Our classof features are based on higher-order score functions, which capture localvariations in the probability density function of the input. We establish atheoretical framework to characterize the nature of discriminative informationthat can be extracted from score-function features, when used in conjunctionwith labeled samples. We employ efficient spectral decomposition algorithms (onmatrices and tensors) for extracting discriminative components. The advantageof employing tensor-valued features is that we can extract richerdiscriminative information in the form of an overcomplete representations.Thus, we present a novel framework for employing generative models of the inputfor discriminative learning.
arxiv-1412-3046 | Provable Tensor Methods for Learning Mixtures of Generalized Linear Models |  http://arxiv.org/abs/1412.3046  | author:Hanie Sedghi, Majid Janzamin, Anima Anandkumar category:cs.LG stat.ML published:2014-12-09 summary:We consider the problem of learning mixtures of generalized linear models(GLM) which arise in classification and regression problems. Typical learningapproaches such as expectation maximization (EM) or variational Bayes can getstuck in spurious local optima. In contrast, we present a tensor decompositionmethod which is guaranteed to correctly recover the parameters. The key insightis to employ certain feature transformations of the input, which depend on theinput generative model. Specifically, we employ score function tensors of theinput and compute their cross-correlation with the response variable. Weestablish that the decomposition of this tensor consistently recovers theparameters, under mild non-degeneracy conditions. We demonstrate that thecomputational and sample complexity of our method is a low order polynomial ofthe input and the latent dimensions.
arxiv-1412-5027 | What is a salient object? A dataset and a baseline model for salient object detection |  http://arxiv.org/abs/1412.5027  | author:Ali Borji category:cs.CV published:2014-12-08 summary:Salient object detection or salient region detection models, diverging fromfixation prediction models, have traditionally been dealing with locating andsegmenting the most salient object or region in a scene. While the notion ofmost salient object is sensible when multiple objects exist in a scene, currentdatasets for evaluation of saliency detection approaches often have scenes withonly one single object. We introduce three main contributions in this paper:First, we take an indepth look at the problem of salient object detection bystudying the relationship between where people look in scenes and what theychoose as the most salient object when they are explicitly asked. Based on theagreement between fixations and saliency judgments, we then suggest that themost salient object is the one that attracts the highest fraction of fixations.Second, we provide two new less biased benchmark datasets containing sceneswith multiple objects that challenge existing saliency models. Indeed, weobserved a severe drop in performance of 8 state-of-the-art models on ourdatasets (40% to 70%). Third, we propose a very simple yet powerful model basedon superpixels to be used as a baseline for model evaluation and comparison.While on par with the best models on MSRA-5K dataset, our model wins over othermodels on our data highlighting a serious drawback of existing models, which isconvoluting the processes of locating the most salient object and itssegmentation. We also provide a review and statistical analysis of some labeledscene datasets that can be used for evaluating salient object detection models.We believe that our work can greatly help remedy the over-fitting of models toexisting biased datasets and opens new venues for future research in thisfast-evolving field.
arxiv-1412-2813 | Joint Segmentation and Deconvolution of Ultrasound Images Using a Hierarchical Bayesian Model based on Generalized Gaussian Priors |  http://arxiv.org/abs/1412.2813  | author:Ningning Zhao, Adrian Basarab, Denis Kouame, Jean-Yves Tourneret category:cs.CV published:2014-12-08 summary:This paper proposes a joint segmentation and deconvolution Bayesian methodfor medical ultrasound (US) images. Contrary to piecewise homogeneous images,US images exhibit heavy characteristic speckle patterns correlated with thetissue structures. The generalized Gaussian distribution (GGD) has been shownto be one of the most relevant distributions for characterizing the speckle inUS images. Thus, we propose a GGD-Potts model defined by a label map couplingUS image segmentation and deconvolution. The Bayesian estimators of the unknownmodel parameters, including the US image, the label map and all thehyperparameters are difficult to be expressed in closed form. Thus, weinvestigate a Gibbs sampler to generate samples distributed according to theposterior of interest. These generated samples are finally used to compute theBayesian estimators of the unknown parameters. The performance of the proposedBayesian model is compared with existing approaches via several experimentsconducted on realistic synthetic data and in vivo US images.
arxiv-1412-2812 | Unsupervised Induction of Semantic Roles within a Reconstruction-Error Minimization Framework |  http://arxiv.org/abs/1412.2812  | author:Ivan Titov, Ehsan Khoddam category:cs.CL cs.AI cs.LG stat.ML published:2014-12-08 summary:We introduce a new approach to unsupervised estimation of feature-richsemantic role labeling models. Our model consists of two components: (1) anencoding component: a semantic role labeling model which predicts roles given arich set of syntactic and lexical features; (2) a reconstruction component: atensor factorization model which relies on roles to predict argument fillers.When the components are estimated jointly to minimize errors in argumentreconstruction, the induced roles largely correspond to roles defined inannotated resources. Our method performs on par with most accurate roleinduction methods on English and German, even though, unlike these previousapproaches, we do not incorporate any prior linguistic knowledge about thelanguages.
arxiv-1412-2444 | An Approach for Reducing Outliers of Non Local Means Image Denoising Filter |  http://arxiv.org/abs/1412.2444  | author:Raka Kundu, Amlan Chakrabarti, Prasanna Lenka category:cs.CV published:2014-12-08 summary:We propose an adaptive approach for non local means (NLM) image filteringtermed as non local adaptive clipped means (NLACM), which reduces the effect ofoutliers and improves the denoising quality as compared to traditional NLM.Common method to neglect outliers from a data population is computation of meanin a range defined by mean and standard deviation. In NLACM we perform themedian within the defined range based on statistical estimation of theneighbourhood region of a pixel to be denoised. As parameters of the range areindependent of any additional input and is based on local intensity values,hence the approach is adaptive. Experimental results for NLACM show betterestimation of true intensity from noisy neighbourhood observation as comparedto NLM at high noise levels. We have verified the technique for speckle noisereduction and we have tested it on ultrasound (US) image of lumbar spine. Theseultrasound images act as guidance for injection therapy for treatment of lumbarradiculopathy. We believe that the proposed approach for image denoising isfirst of its kind and its efficiency can be well justified as it shows betterperformance in image restoration.
arxiv-1412-2620 | Cells in Multidimensional Recurrent Neural Networks |  http://arxiv.org/abs/1412.2620  | author:G. Leifert, T. Strauß, T. Grüning, R. Labahn category:cs.AI cs.NE 68T10, 68T05 published:2014-12-08 summary:The transcription of handwritten text on images is one task in machinelearning and one solution to solve it is using multi-dimensional recurrentneural networks (MDRNN) with connectionist temporal classification (CTC). TheRNNs can contain special units, the long short-term memory (LSTM) cells. Theyare able to learn long term dependencies but they get unstable when thedimension is chosen greater than one. We defined some useful and necessaryproperties for the one-dimensional LSTM cell and extend them in themulti-dimensional case. Thereby we introduce several new cells with betterstability. We present a method to design cells using the theory of linear shiftinvariant systems. The new cells are compared to the LSTM cell on the IFN/ENITand Rimes database, where we can improve the recognition rate compared to theLSTM cell. So each application where the LSTM cells in MDRNNs are used could beimproved by substituting them by the new developed cells.
arxiv-1412-2432 | MLitB: Machine Learning in the Browser |  http://arxiv.org/abs/1412.2432  | author:Edward Meeds, Remco Hendriks, Said Al Faraby, Magiel Bruntink, Max Welling category:cs.DC cs.LG stat.ML published:2014-12-08 summary:With few exceptions, the field of Machine Learning (ML) research has largelyignored the browser as a computational engine. Beyond an educational resourcefor ML, the browser has vast potential to not only improve the state-of-the-artin ML research, but also, inexpensively and on a massive scale, to bringsophisticated ML learning and prediction to the public at large. This paperintroduces MLitB, a prototype ML framework written entirely in JavaScript,capable of performing large-scale distributed computing with heterogeneousclasses of devices. The development of MLitB has been driven by severalunderlying objectives whose aim is to make ML learning and usage ubiquitous (byusing ubiquitous compute devices), cheap and effortlessly distributed, andcollaborative. This is achieved by allowing every internet capable device torun training algorithms and predictive models with no software installation andby saving models in universally readable formats. Our prototype library iscapable of training deep neural networks with synchronized, distributedstochastic gradient descent. MLitB offers several important opportunities fornovel ML research, including: development of distributed learning algorithms,advancement of web GPU algorithms, novel field and mobile applications, privacypreserving computing, and green grid-computing. MLitB is available as opensource software.
arxiv-1412-2697 | Image quality assessment measure based on natural image statistics in the Tetrolet domain |  http://arxiv.org/abs/1412.2697  | author:Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi, Driss Aboutajdine category:cs.CV published:2014-12-08 summary:This paper deals with a reduced reference (RR) image quality measure based onnatural image statistics modeling. For this purpose, Tetrolet transform is usedsince it provides a convenient way to capture local geometric structures. Thistransform is applied to both reference and distorted images. Then, GaussianScale Mixture (GSM) is proposed to model subbands in order to take accountstatistical dependencies between tetrolet coefficients. In order to quantifythe visual degradation, a measure based on Kullback Leibler Divergence (KLD) isprovided. The proposed measure was tested on the Cornell VCL A-57 dataset andcompared with other measures according to FR-TV1 VQEG framework.
arxiv-1412-2632 | Probabilistic low-rank matrix completion on finite alphabets |  http://arxiv.org/abs/1412.2632  | author:Jean Lafond, Olga Klopp, Eric Moulines, Jospeh Salmon category:math.ST stat.ML stat.TH published:2014-12-08 summary:The task of reconstructing a matrix given a sample of observedentries isknown as the matrix completion problem. It arises ina wide range of problems,including recommender systems, collaborativefiltering, dimensionalityreduction, image processing, quantum physics or multi-class classificationtoname a few. Most works have focused on recovering an unknown real-valuedlow-rankmatrix from randomly sub-sampling its entries.Here, we investigate thecase where the observations take a finite number of values, corresponding forexamples to ratings in recommender systems or labels in multi-classclassification.We also consider a general sampling scheme (not necessarilyuniform) over the matrix entries.The performance of a nuclear-norm penalizedestimator is analyzed theoretically.More precisely, we derive bounds for theKullback-Leibler divergence between the true and estimated distributions.Inpractice, we have also proposed an efficient algorithm based on liftedcoordinate gradient descent in order to tacklepotentially high dimensionalsettings.
arxiv-1412-3352 | Web image annotation by diffusion maps manifold learning algorithm |  http://arxiv.org/abs/1412.3352  | author:Neda Pourali category:cs.CV cs.IR cs.LG 68T10 published:2014-12-08 summary:Automatic image annotation is one of the most challenging problems in machinevision areas. The goal of this task is to predict number of keywordsautomatically for images captured in real data. Many methods are based onvisual features in order to calculate similarities between image samples. Butthe computation cost of these approaches is very high. These methods requiremany training samples to be stored in memory. To lessen this burden, a numberof techniques have been developed to reduce the number of features in adataset. Manifold learning is a popular approach to nonlinear dimensionalityreduction. In this paper, we investigate Diffusion maps manifold learningmethod for web image auto-annotation task. Diffusion maps manifold learningmethod is used to reduce the dimension of some visual features. Extensiveexperiments and analysis on NUS-WIDE-LITE web image dataset with differentvisual features show how this manifold learning dimensionality reduction methodcan be applied effectively to image annotation.
arxiv-1412-2486 | Optimization models of natural communication |  http://arxiv.org/abs/1412.2486  | author:Ramon Ferrer-i-Cancho category:physics.soc-ph cs.CL published:2014-12-08 summary:A family of information theoretic models of communication was introduced morethan a decade ago to explain the origins of Zipf's law for word frequencies.The family is a based on a combination of two information theoretic principles:maximization of mutual information between forms and meanings and minimizationof form entropy. The family also sheds light on the origins of three otherpatterns: the principle of contrast, a related a vocabulary learning bias andthe meaning-frequency law. Here two important components of the family, namelythe information theoretic principles and the energy function that combines themlinearly, are reviewed from the perspective of psycholinguistics, languagelearning, information theory and synergetic linguistics. The minimization ofthis linear function resembles a sort of agnostic information theoretic modelselection that might be tuned by self-organization.
arxiv-1412-2485 | Accurate Streaming Support Vector Machines |  http://arxiv.org/abs/1412.2485  | author:Vikram Nathan, Sharath Raghvendra category:cs.LG published:2014-12-08 summary:A widely-used tool for binary classification is the Support Vector Machine(SVM), a supervised learning technique that finds the "maximum margin" linearseparator between the two classes. While SVMs have been well studied in thebatch (offline) setting, there is considerably less work on the streaming(online) setting, which requires only a single pass over the data usingsub-linear space. Existing streaming algorithms are not yet competitive withthe batch implementation. In this paper, we use the formulation of the SVM as aminimum enclosing ball (MEB) problem to provide a streaming SVM algorithm basedoff of the blurred ball cover originally proposed by Agarwal and Sharathkumar.Our implementation consistently outperforms existing streaming SVM approachesand provides higher accuracies than libSVM on several datasets, thus making itcompetitive with the standard SVM batch implementation.
arxiv-1412-2457 | Weighted Polynomial Approximations: Limits for Learning and Pseudorandomness |  http://arxiv.org/abs/1412.2457  | author:Mark Bun, Thomas Steinke category:cs.CC cs.LG published:2014-12-08 summary:Polynomial approximations to boolean functions have led to many positiveresults in computer science. In particular, polynomial approximations to thesign function underly algorithms for agnostically learning halfspaces, as wellas pseudorandom generators for halfspaces. In this work, we investigate thelimits of these techniques by proving inapproximability results for the signfunction. Firstly, the polynomial regression algorithm of Kalai et al. (SIAM J. Comput.2008) shows that halfspaces can be learned with respect to log-concavedistributions on $\mathbb{R}^n$ in the challenging agnostic learning model. Thepower of this algorithm relies on the fact that under log-concavedistributions, halfspaces can be approximated arbitrarily well by low-degreepolynomials. We ask whether this technique can be extended beyond log-concavedistributions, and establish a negative result. We show that polynomials of anydegree cannot approximate the sign function to within arbitrarily low error fora large class of non-log-concave distributions on the real line, includingthose with densities proportional to $\exp(-x^{0.99})$. Secondly, we investigate the derandomization of Chernoff-type concentrationinequalities. Chernoff-type tail bounds on sums of independent random variableshave pervasive applications in theoretical computer science. Schmidt et al.(SIAM J. Discrete Math. 1995) showed that these inequalities can be establishedfor sums of random variables with only $O(\log(1/\delta))$-wise independence,for a tail probability of $\delta$. We show that their results are tight up toconstant factors. These results rely on techniques from weighted approximation theory, whichstudies how well functions on the real line can be approximated by polynomialsunder various distributions. We believe that these techniques will have furtherapplications in other areas of computer science.
arxiv-1412-2442 | Rediscovering the Alphabet - On the Innate Universal Grammar |  http://arxiv.org/abs/1412.2442  | author:M. Yahia Kaadan, Asaad Kaadan category:cs.CL published:2014-12-08 summary:Universal Grammar (UG) theory has been one of the most important researchtopics in linguistics since introduced five decades ago. UG specifies therestricted set of languages learnable by human brain, and thus, manyresearchers believe in its biological roots. Numerous empirical studies ofneurobiological and cognitive functions of the human brain, and of many naturallanguages, have been conducted to unveil some aspects of UG. This, however,resulted in different and sometimes contradicting theories that do not indicatea universally unique grammar. In this research, we tackle the UG problem froman entirely different perspective. We search for the Unique Universal Grammar(UUG) that facilitates communication and knowledge transfer, the sole purposeof a language. We formulate this UG and show that it is unique, intrinsic, andcosmic, rather than humanistic. Initial analysis on a widespread naturallanguage already showed some positive results.
arxiv-1412-2672 | When Computer Vision Gazes at Cognition |  http://arxiv.org/abs/1412.2672  | author:Tao Gao, Daniel Harari, Joshua Tenenbaum, Shimon Ullman category:cs.AI cs.CV published:2014-12-08 summary:Joint attention is a core, early-developing form of social interaction. It isbased on our ability to discriminate the third party objects that other peopleare looking at. While it has been shown that people can accurately determinewhether another person is looking directly at them versus away, little is knownabout human ability to discriminate a third person gaze directed towardsobjects that are further away, especially in unconstraint cases where thelooker can move her head and eyes freely. In this paper we address thisquestion by jointly exploring human psychophysics and a cognitively motivatedcomputer vision model, which can detect the 3D direction of gaze from 2D faceimages. The synthesis of behavioral study and computer vision yields severalinteresting discoveries. (1) Human accuracy of discriminating targets8{\deg}-10{\deg} of visual angle apart is around 40% in a free looking gazetask; (2) The ability to interpret gaze of different lookers vary dramatically;(3) This variance can be captured by the computational model; (4) Humanoutperforms the current model significantly. These results collectively showthat the acuity of human joint attention is indeed highly impressive, given thecomputational challenge of the natural looking task. Moreover, the gap betweenhuman and model performance, as well as the variability of gaze interpretationacross different lookers, require further understanding of the underlyingmechanisms utilized by humans for this challenging task.
arxiv-1412-2684 | HyperSpectral classification with adaptively weighted L1-norm regularization and spatial postprocessing |  http://arxiv.org/abs/1412.2684  | author:Victor Stefan Aldea, M. O. Ahmad, W. E. Lynch category:math.OC cs.CV published:2014-12-08 summary:Sparse regression methods have been proven effective in a wide range ofsignal processing problems such as image compression, speech coding, channelequalization, linear regression and classification. In this paper we develop anew method of hyperspectral image classification based on the sparse unmixingalgorithm SUnSAL for which a pixel adaptive L1-norm regularization term isintroduced. To further enhance class separability, the algorithm is kernelizedusing a RBF kernel and the final results are improved by a combination ofspatial pre and post-processing operations. We show that our method iscompetitive with state of the art algorithms such as SVM-CK, KLR-CK, KSOMP andKSSP.
arxiv-1412-2689 | A New Approach of Learning Hierarchy Construction Based on Fuzzy Logic |  http://arxiv.org/abs/1412.2689  | author:Ali Aajli, Karim Afdel category:cs.CY cs.AI cs.LG published:2014-12-08 summary:In recent years, adaptive learning systems rely increasingly on learninghierarchy to customize the educational logic developed in their courses. Mostapproaches do not consider that the relationships of prerequisites between theskills are fuzzy relationships. In this article, we describe a new approach ofa practical application of fuzzy logic techniques to the construction oflearning hierarchies. For this, we use a learning hierarchy predefined by oneor more experts of a specific field. However, the relationships ofprerequisites between the skills in the learning hierarchy are not definitiveand they are fuzzy relationships. Indeed, we measure relevance degree of allrelationships existing in this learning hierarchy and we try to answer to thefollowing question: Is the relationships of prerequisites predefined in initiallearning hierarchy are correctly established or not?
arxiv-1412-2693 | Provable Methods for Training Neural Networks with Sparse Connectivity |  http://arxiv.org/abs/1412.2693  | author:Hanie Sedghi, Anima Anandkumar category:cs.LG cs.NE stat.ML published:2014-12-08 summary:We provide novel guaranteed approaches for training feedforward neuralnetworks with sparse connectivity. We leverage on the techniques developedpreviously for learning linear networks and show that they can also beeffectively adopted to learn non-linear networks. We operate on the momentsinvolving label and the score function of the input, and show that theirfactorization provably yields the weight matrix of the first layer of a deepnetwork under mild conditions. In practice, the output of our method can beemployed as effective initializers for gradient descent.
arxiv-1412-2487 | Word learning under infinite uncertainty |  http://arxiv.org/abs/1412.2487  | author:Richard A. Blythe, Andrew D. M. Smith, Kenny Smith category:physics.soc-ph cs.CL published:2014-12-08 summary:Language learners must learn the meanings of many thousands of words, despitethose words occurring in complex environments in which infinitely many meaningsmight be inferred by the learner as a word's true meaning. This problem ofinfinite referential uncertainty is often attributed to Willard Van OrmanQuine. We provide a mathematical formalisation of an ideal cross-situationallearner attempting to learn under infinite referential uncertainty, andidentify conditions under which word learning is possible. As Quine'sintuitions suggest, learning under infinite uncertainty is in fact possible,provided that learners have some means of ranking candidate word meanings interms of their plausibility; furthermore, our analysis shows that this rankingcould in fact be exceedingly weak, implying that constraints which allowlearners to infer the plausibility of candidate word meanings could themselvesbe weak. This approach lifts the burden of explanation from `smart' wordlearning constraints in learners, and suggests a programme of research intoweak, unreliable, probabilistic constraints on the inference of word meaning inreal word learners.
arxiv-1412-2604 | Actions and Attributes from Wholes and Parts |  http://arxiv.org/abs/1412.2604  | author:Georgia Gkioxari, Ross Girshick, Jitendra Malik category:cs.CV published:2014-12-08 summary:We investigate the importance of parts for the tasks of action and attributeclassification. We develop a part-based approach by leveraging convolutionalnetwork features inspired by recent advances in computer vision. Our partdetectors are a deep version of poselets and capture parts of the human bodyunder a distinct set of poses. For the tasks of action and attributeclassification, we train holistic convolutional neural networks and show thatadding parts leads to top-performing results for both tasks. In addition, wedemonstrate the effectiveness of our approach when we replace an oracle persondetector, as is the default in the current evaluation protocol for both tasks,with a state-of-the-art person detection system.
arxiv-1502-07243 | Real-Time System of Hand Detection And Gesture Recognition In Cyber Presence Interactive System For E-Learning |  http://arxiv.org/abs/1502.07243  | author:Bousaaid Mourad, Ayaou Tarik, Afdel Karim, Estraillier Pascal category:cs.CV published:2014-12-08 summary:The development of technologies of multimedia, linked to that of Internet anddemocratization of high outflow, has made henceforth E-learning possible forlearners being in virtual classes and geographically distributed. The qualityand quantity of asynchronous and synchronous communications are the keyelements for E-learning success. It is important to have a propitioussupervision to reduce the feeling of isolation in E-learning. This feeling ofisolation is among the main causes of loss and high rates of stalling inE-learning. The researches to be conducted in this domain aim to bringsolutions of convergence coming from real time image for the capture andrecognition of hand gestures. These gestures will be analyzed by the system andtransformed as indicator of participation. This latter is displayed in thetable of performance of the tutor as a curve according to the time. In case ofisolation of learner, the indicator of participation will become red and thetutor will be informed of learners with difficulties to participate duringlearning session.
arxiv-1412-2314 | $\ell_p$ Testing and Learning of Discrete Distributions |  http://arxiv.org/abs/1412.2314  | author:Bo Waggoner category:cs.DS cs.LG math.ST stat.TH F.2.0; G.3 published:2014-12-07 summary:The classic problems of testing uniformity of and learning a discretedistribution, given access to independent samples from it, are examined undergeneral $\ell_p$ metrics. The intuitions and results often contrast with theclassic $\ell_1$ case. For $p > 1$, we can learn and test with a number ofsamples that is independent of the support size of the distribution: With an$\ell_p$ tolerance $\epsilon$, $O(\max\{ \sqrt{1/\epsilon^q}, 1/\epsilon^2 \})$samples suffice for testing uniformity and $O(\max\{ 1/\epsilon^q,1/\epsilon^2\})$ samples suffice for learning, where $q=p/(p-1)$ is theconjugate of $p$. As this parallels the intuition that $O(\sqrt{n})$ and $O(n)$samples suffice for the $\ell_1$ case, it seems that $1/\epsilon^q$ acts as anupper bound on the "apparent" support size. For some $\ell_p$ metrics, uniformity testing becomes easier over largersupports: a 6-sided die requires fewer trials to test for fairness than a2-sided coin, and a card-shuffler requires fewer trials than the die. In fact,this inverse dependence on support size holds if and only if $p > \frac{4}{3}$.The uniformity testing algorithm simply thresholds the number of "collisions"or "coincidences" and has an optimal sample complexity up to constant factorsfor all $1 \leq p \leq 2$. Another algorithm gives order-optimal samplecomplexity for $\ell_{\infty}$ uniformity testing. Meanwhile, the most naturallearning algorithm is shown to have order-optimal sample complexity for all$\ell_p$ metrics. The author thanks Cl\'{e}ment Canonne for discussions and contributions tothis work.
arxiv-1412-2306 | Deep Visual-Semantic Alignments for Generating Image Descriptions |  http://arxiv.org/abs/1412.2306  | author:Andrej Karpathy, Li Fei-Fei category:cs.CV published:2014-12-07 summary:We present a model that generates natural language descriptions of images andtheir regions. Our approach leverages datasets of images and their sentencedescriptions to learn about the inter-modal correspondences between languageand visual data. Our alignment model is based on a novel combination ofConvolutional Neural Networks over image regions, bidirectional RecurrentNeural Networks over sentences, and a structured objective that aligns the twomodalities through a multimodal embedding. We then describe a MultimodalRecurrent Neural Network architecture that uses the inferred alignments tolearn to generate novel descriptions of image regions. We demonstrate that ouralignment model produces state of the art results in retrieval experiments onFlickr8K, Flickr30K and MSCOCO datasets. We then show that the generateddescriptions significantly outperform retrieval baselines on both full imagesand on a new dataset of region-level annotations.
arxiv-1412-2378 | Learning Word Representations from Relational Graphs |  http://arxiv.org/abs/1412.2378  | author:Danushka Bollegala, Takanori Maehara, Yuichi Yoshida, Ken-ichi Kawarabayashi category:cs.CL published:2014-12-07 summary:Attributes of words and relations between two words are central to numeroustasks in Artificial Intelligence such as knowledge representation, similaritymeasurement, and analogy detection. Often when two words share one or moreattributes in common, they are connected by some semantic relations. On theother hand, if there are numerous semantic relations between two words, we canexpect some of the attributes of one of the words to be inherited by the other.Motivated by this close connection between attributes and relations, given arelational graph in which words are inter- connected via numerous semanticrelations, we propose a method to learn a latent representation for theindividual words. The proposed method considers not only the co-occurrences ofwords as done by existing approaches for word representation learning, but alsothe semantic relations in which two words co-occur. To evaluate the accuracy ofthe word representations learnt using the proposed method, we use the learntword representations to solve semantic word analogy problems. Our experimentalresults show that it is possible to learn better word representations by usingsemantic semantics between words.
arxiv-1412-2404 | Dimensionality Reduction with Subspace Structure Preservation |  http://arxiv.org/abs/1412.2404  | author:Devansh Arpit, Ifeoma Nwogu, Venu Govindaraju category:cs.LG stat.ML published:2014-12-07 summary:Modeling data as being sampled from a union of independent subspaces has beenwidely applied to a number of real world applications. However, dimensionalityreduction approaches that theoretically preserve this independence assumptionhave not been well studied. Our key contribution is to show that $2K$projection vectors are sufficient for the independence preservation of any $K$class data sampled from a union of independent subspaces. It is thisnon-trivial observation that we use for designing our dimensionality reductiontechnique. In this paper, we propose a novel dimensionality reduction algorithmthat theoretically preserves this structure for a given dataset. We support ourtheoretical analysis with empirical results on both synthetic and real worlddata achieving \textit{state-of-the-art} results compared to populardimensionality reduction techniques.
arxiv-1412-2309 | Visual Causal Feature Learning |  http://arxiv.org/abs/1412.2309  | author:Krzysztof Chalupka, Pietro Perona, Frederick Eberhardt category:stat.ML cs.AI cs.CV cs.LG published:2014-12-07 summary:We provide a rigorous definition of the visual cause of a behavior that isbroadly applicable to the visually driven behavior in humans, animals, neurons,robots and other perceiving systems. Our framework generalizes standardaccounts of causal learning to settings in which the causal variables need tobe constructed from micro-variables. We prove the Causal Coarsening Theorem,which allows us to gain causal knowledge from observational data with minimalexperimental effort. The theorem provides a connection to standard inferencetechniques in machine learning that identify features of an image thatcorrelate with, but may not cause, the target behavior. Finally, we propose anactive learning scheme to learn a manipulator function that performs optimalmanipulations on the image to automatically identify the visual cause of atarget behavior. We illustrate our inference and learning algorithms inexperiments based on both synthetic and real data.
arxiv-1412-2302 | Theano-based Large-Scale Visual Recognition with Multiple GPUs |  http://arxiv.org/abs/1412.2302  | author:Weiguang Ding, Ruoyan Wang, Fei Mao, Graham Taylor category:cs.LG published:2014-12-07 summary:In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012)implementation and its naive data parallelism on multiple GPUs. Our performanceon 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014)run on 1 GPU. To the best of our knowledge, this is the first open-sourcePython-based AlexNet implementation to-date.
arxiv-1412-2342 | Bayesian Image Restoration for Poisson Corrupted Image using a Latent Variational Method with Gaussian MRF |  http://arxiv.org/abs/1412.2342  | author:Hayaru Shouno category:cs.CV published:2014-12-07 summary:We treat an image restoration problem with a Poisson noise chan- nel using aBayesian framework. The Poisson randomness might be appeared in observation oflow contrast object in the field of imaging. The noise observation is oftenhard to treat in a theo- retical analysis. In our formulation, we interpret theobservation through the Poisson noise channel as a likelihood, and evaluate thebound of it with a Gaussian function using a latent variable method. We thenintroduce a Gaussian Markov random field (GMRF) as the prior for the Bayesianapproach, and derive the posterior as a Gaussian distribution. The latentparameters in the likelihood and the hyperparameter in the GMRF prior could betreated as hid- den parameters, so that, we propose an algorithm to infer themin the expectation maximization (EM) framework using loopy beliefpropagation(LBP). We confirm the ability of our algorithm in the computersimulation, and compare it with the results of other im- age restorationframeworks.
arxiv-1412-2316 | Iterative Bayesian Reconstruction of Non-IID Block-Sparse Signals |  http://arxiv.org/abs/1412.2316  | author:Mehdi Korki, Jingxin Zhang, Cishen Zhang, Hadi Zayyani category:stat.ML cs.IT math.IT published:2014-12-07 summary:This paper presents a novel Block Iterative Bayesian Algorithm (Block-IBA)for reconstructing block-sparse signals with unknown block structures. Unlikethe existing algorithms for block sparse signal recovery which assume thecluster structure of the nonzero elements of the unknown signal to beindependent and identically distributed (i.i.d.), we use a more realisticBernoulli-Gaussian hidden Markov model (BGHMM) to characterize the non-i.i.d.block-sparse signals commonly encountered in practice. The Block-IBAiteratively estimates the amplitudes and positions of the block-sparse signalusing the steepest-ascent based Expectation-Maximization (EM), and optimallyselects the nonzero elements of the block-sparse signal by adaptivethresholding. The global convergence of Block-IBA is analyzed and proved, andthe effectiveness of Block-IBA is demonstrated by numerical experiments andsimulations on synthetic and real-life data.
arxiv-1412-5902 | A Physically Inspired Clustering Algorithm: to Evolve Like Particles |  http://arxiv.org/abs/1412.5902  | author:Teng Qiu, Kaifu Yang, Chaoyi Li, Yongjie Li category:cs.LG cs.CV published:2014-12-07 summary:Clustering analysis is a method to organize raw data into categories based ona measure of similarity. It has been successfully applied to diverse fieldsfrom science to business and engineering. By endowing data points with physicalmeaning like particles in the physical world and then leaning their evolvingtendency of moving from higher to lower potentials, data points in the proposedclustering algorithm sequentially hop to the locations of their transfer pointsand gather, after a few steps, at the locations of cluster centers with thelocally lowest potentials, where cluster members can be easily identified. Thewhole clustering process is simple and efficient, and can be performed eitherautomatically or interactively, with reliable performances on test data ofdiverse shapes, attributes, and dimensionalities.
arxiv-1412-2197 | Practice in Synonym Extraction at Large Scale |  http://arxiv.org/abs/1412.2197  | author:Liangliang Cao, Chang Wang category:cs.CL published:2014-12-06 summary:Synonym extraction is an important task in natural language processing andoften used as a submodule in query expansion, question answering and otherapplications. Automatic synonym extractor is highly preferred for large scaleapplications. Previous studies in synonym extraction are most limited to smallscale datasets. In this paper, we build a large dataset with 3.4 millionsynonym/non-synonym pairs to capture the challenges in real world scenarios. Weproposed (1) a new cost function to accommodate the unbalanced learningproblem, and (2) a feature learning based deep neural network to model thecomplicated relationships in synonym pairs. We compare several differentapproaches based on SVMs and neural networks, and find out a novel featurelearning based neural network outperforms the methods with hand-assignedfeatures. Specifically, the best performance of our model surpasses the SVMbaseline with a significant 97\% relative improvement.
arxiv-1412-2196 | Relations among Some Low Rank Subspace Recovery Models |  http://arxiv.org/abs/1412.2196  | author:Hongyang Zhang, Zhouchen Lin, Chao Zhang, Junbin Gao category:cs.LG math.OC published:2014-12-06 summary:Recovering intrinsic low dimensional subspaces from data distributed on themis a key preprocessing step to many applications. In recent years, there hasbeen a lot of work that models subspace recovery as low rank minimizationproblems. We find that some representative models, such as Robust PrincipalComponent Analysis (R-PCA), Robust Low Rank Representation (R-LRR), and RobustLatent Low Rank Representation (R-LatLRR), are actually deeply connected. Morespecifically, we discover that once a solution to one of the models isobtained, we can obtain the solutions to other models in closed-formformulations. Since R-PCA is the simplest, our discovery makes it the center oflow rank subspace recovery models. Our work has two important implications.First, R-PCA has a solid theoretical foundation. Under certain conditions, wecould find better solutions to these low rank models at overwhelmingprobabilities, although these models are non-convex. Second, we can obtainsignificantly faster algorithms for these models by solving R-PCA first. Thecomputation cost can be further cut by applying low complexity randomizedalgorithms, e.g., our novel $\ell_{2,1}$ filtering algorithm, to R-PCA.Experiments verify the advantages of our algorithms over other state-of-the-artones that are based on the alternating direction method.
arxiv-1412-2231 | Generalized Singular Value Thresholding |  http://arxiv.org/abs/1412.2231  | author:Canyi Lu, Changbo Zhu, Chunyan Xu, Shuicheng Yan, Zhouchen Lin category:cs.CV cs.LG cs.NA math.NA published:2014-12-06 summary:This work studies the Generalized Singular Value Thresholding (GSVT) operator${\Prox}_{g}^{\bm{\sigma}}(\cdot)$, \begin{equation*} {\Prox}_{g}^{\bm{\sigma}}(\B)=\arg\min\limits_{\X}\sum_{i=1}^{m}g(\sigma_{i}(\X))+ \frac{1}{2}\X-\B_{F}^{2}, \end{equation*} associated with a nonconvexfunction $g$ defined on the singular values of $\X$. We prove that GSVT can beobtained by performing the proximal operator of $g$ (denoted as$\Prox_g(\cdot)$) on the singular values since $\Prox_g(\cdot)$ is monotonewhen $g$ is lower bounded. If the nonconvex $g$ satisfies some conditions (manypopular nonconvex surrogate functions, e.g., $\ell_p$-norm, $0<p<1$, of$\ell_0$-norm are special cases), a general solver to find $\Prox_g(b)$ isproposed for any $b\geq0$. GSVT greatly generalizes the known Singular ValueThresholding (SVT) which is a basic subroutine in many convex low rankminimization methods. We are able to solve the nonconvex low rank minimizationproblem by using GSVT in place of SVT.
arxiv-1412-2186 | Using Artificial Neural Network Techniques for Prediction of Electric Energy Consumption |  http://arxiv.org/abs/1412.2186  | author:Hasan M. H. Owda, Babatunji Omoniwa, Ahmad R. Shahid, Sheikh Ziauddin category:cs.NE cs.AI published:2014-12-06 summary:Due to imprecision and uncertainties in predicting real world problems,artificial neural network (ANN) techniques have become increasingly useful formodeling and optimization. This paper presents an artificial neural networkapproach for forecasting electric energy consumption. For effective planningand operation of power systems, optimal forecasting tools are needed for energyoperators to maximize profit and also to provide maximum satisfaction to energyconsumers. Monthly data for electric energy consumed in the Gaza strip wascollected from year 1994 to 2013. Data was trained and the proposed model wasvalidated using 2-Fold and K-Fold cross validation techniques. The model hasbeen tested with actual energy consumption data and yields satisfactoryperformance.
arxiv-1412-2210 | Risk Estimation Without Using Stein's Lemma -- Application to Image Denoising |  http://arxiv.org/abs/1412.2210  | author:Sagar Venkatesh Gubbi, Chandra Sekhar Seelamantula category:cs.CV published:2014-12-06 summary:We address the problem of image denoising in additive white noise withoutplacing restrictive assumptions on its statistical distribution. In the recentliterature, specific noise distributions have been considered andcorrespondingly, optimal denoising techniques have been developed. One of thesuccessful approaches for denoising relies on the notion of unbiased riskestimation, which enables one to obtain a useful substitute for the mean-squareerror. For the case of additive white Gaussian noise contamination, the riskestimation procedure relies on Stein's lemma. Sophisticated wavelet-baseddenoising techniques, which are essentially nonlinear, have been developed withthe help of the lemma. We show that, for linear, shift-invariant denoisers, itis possible to obtain unbiased risk estimates of the mean-square error withoutusing Stein's lemma. An interesting consequence of this development is that theunbiased risk estimator becomes agnostic to the statistical distribution of thenoise. As a proof of principle, we show how the new methodology can be used tooptimize the parameters of a simple Gaussian smoother. By locally adapting theparameters of the Gaussian smoother, we obtain a shift-variant smoother, whichhas a denoising performance (quantified by the improvement in peaksignal-to-noise ratio (PSNR)) that is competitive to far more sophisticatedmethods reported in the literature. The proposed solution exhibits considerableparallelism, which we exploit in a Graphics Processing Unit (GPU)implementation.
arxiv-1412-2291 | Adjusted least squares fitting of algebraic hypersurfaces |  http://arxiv.org/abs/1412.2291  | author:Konstantin Usevich, Ivan Markovsky category:stat.CO cs.CG cs.CV math.NA published:2014-12-06 summary:We consider the problem of fitting a set of points in Euclidean space by analgebraic hypersurface. We assume that points on a true hypersurface, describedby a polynomial equation, are corrupted by zero mean independent Gaussiannoise, and we estimate the coefficients of the true polynomial equation. Theadjusted least squares estimator accounts for the bias present in the ordinaryleast squares estimator. The adjusted least squares estimator is based onconstructing a quasi-Hankel matrix, which is a bias-corrected matrix ofmoments. For the case of unknown noise variance, the estimator is defined as asolution of a polynomial eigenvalue problem. In this paper, we present newresults on invariance properties of the adjusted least squares estimator and animproved algorithm for computing the estimator for an arbitrary set ofmonomials in the polynomial equation.
arxiv-1412-2295 | A Likelihood Ratio Framework for High Dimensional Semiparametric Regression |  http://arxiv.org/abs/1412.2295  | author:Yang Ning, Tianqi Zhao, Han Liu category:stat.ML published:2014-12-06 summary:We propose a likelihood ratio based inferential framework for highdimensional semiparametric generalized linear models. This framework addressesa variety of challenging problems in high dimensional data analysis, includingincomplete data, selection bias, and heterogeneous multitask learning. Our workhas three main contributions. (i) We develop a regularized statisticalchromatography approach to infer the parameter of interest under the proposedsemiparametric generalized linear model without the need of estimating theunknown base measure function. (ii) We propose a new framework to constructpost-regularization confidence regions and tests for the low dimensionalcomponents of high dimensional parameters. Unlike existing post-regularizationinferential methods, our approach is based on a novel directional likelihood.In particular, the framework naturally handles generic regularized estimatorswith nonconvex penalty functions and it can be used to infer least falseparameters under misspecified models. (iii) We develop new concentrationinequalities and normal approximation results for U-statistics with unboundedkernels, which are of independent interest. We demonstrate the consequences ofthe general theory by using an example of missing data problem. Extensivesimulation studies and real data analysis are provided to illustrate ourproposed approach.
arxiv-1412-2066 | Learning Multi-target Tracking with Quadratic Object Interactions |  http://arxiv.org/abs/1412.2066  | author:Shaofei Wang, Charless C. Fowlkes category:cs.CV cs.LG published:2014-12-05 summary:We describe a model for multi-target tracking based on associatingcollections of candidate detections across frames of a video. In order to modelpairwise interactions between different tracks, such as suppression ofoverlapping tracks and contextual cues about co-occurence of different objects,we augment a standard min-cost flow objective with quadratic terms betweendetection variables. We learn the parameters of this model using structuredprediction and a loss function which approximates the multi-target trackingaccuracy. We evaluate two different approaches to finding an optimal set oftracks under model objective based on an LP relaxation and a novel greedyextension to dynamic programming that handles pairwise interactions. We findthe greedy algorithm achieves equivalent performance to the LP relaxation whilebeing 2-7x faster than a commercial solver. The resulting model with learnedparameters outperforms existing methods across several categories on the KITTItracking benchmark.
arxiv-1412-1927 | Quantile universal threshold: model selection at the detection edge for high-dimensional linear regression |  http://arxiv.org/abs/1412.1927  | author:Jairo Diaz Rodriguez, Sylvain Sardy category:stat.ML stat.ME published:2014-12-05 summary:To estimate a sparse linear model from data with Gaussian noise, consiliencefrom lasso and compressed sensing literatures is that thresholding estimatorslike lasso and the Dantzig selector have the ability in some situations toidentify with high probability part of the significant covariatesasymptotically, and are numerically tractable thanks to convexity. Yet, the selection of a threshold parameter $\lambda$ remains crucial inpractice. To that aim we propose Quantile Universal Thresholding, a selectionof $\lambda$ at the detection edge. We show with extensive simulations and realdata that an excellent compromise between high true positive rate and low falsediscovery rate is achieved, leading also to good predictive risk.
arxiv-1412-2007 | On Using Very Large Target Vocabulary for Neural Machine Translation |  http://arxiv.org/abs/1412.2007  | author:Sébastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio category:cs.CL published:2014-12-05 summary:Neural machine translation, a recently proposed approach to machinetranslation based purely on neural networks, has shown promising resultscompared to the existing approaches such as phrase-based statistical machinetranslation. Despite its recent success, neural machine translation has itslimitation in handling a larger vocabulary, as training complexity as well asdecoding complexity increase proportionally to the number of target words. Inthis paper, we propose a method that allows us to use a very large targetvocabulary without increasing training complexity, based on importancesampling. We show that decoding can be efficiently done even with the modelhaving a very large target vocabulary by selecting only a small subset of thewhole target vocabulary. The models trained by the proposed approach areempirically found to outperform the baseline models with a small vocabulary aswell as the LSTM-based neural machine translation models. Furthermore, when weuse the ensemble of a few models with very large target vocabularies, weachieve the state-of-the-art translation performance (measured by BLEU) on theEnglish->German translation and almost as high performance as state-of-the-artEnglish->French translation system.
arxiv-1412-1897 | Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images |  http://arxiv.org/abs/1412.1897  | author:Anh Nguyen, Jason Yosinski, Jeff Clune category:cs.CV cs.AI cs.NE published:2014-12-05 summary:Deep neural networks (DNNs) have recently been achieving state-of-the-artperformance on a variety of pattern-recognition tasks, most notably visualclassification problems. Given that DNNs are now able to classify objects inimages with near-human-level performance, questions naturally arise as to whatdifferences remain between computer and human vision. A recent study revealedthat changing an image (e.g. of a lion) in a way imperceptible to humans cancause a DNN to label the image as something else entirely (e.g. mislabeling alion a library). Here we show a related result: it is easy to produce imagesthat are completely unrecognizable to humans, but that state-of-the-art DNNsbelieve to be recognizable objects with 99.99% confidence (e.g. labeling withcertainty that white noise static is a lion). Specifically, we takeconvolutional neural networks trained to perform well on either the ImageNet orMNIST datasets and then find images with evolutionary algorithms or gradientascent that DNNs label with high confidence as belonging to each dataset class.It is possible to produce images totally unrecognizable to human eyes that DNNsbelieve with near certainty are familiar objects, which we call "foolingimages" (more generally, fooling examples). Our results shed light oninteresting differences between human vision and current DNNs, and raisequestions about the generality of DNN computer vision.
arxiv-1412-2129 | An iterative step-function estimator for graphons |  http://arxiv.org/abs/1412.2129  | author:Diana Cai, Nathanael Ackerman, Cameron Freer category:math.ST stat.CO stat.ML stat.TH published:2014-12-05 summary:Exchangeable graphs arise via a sampling procedure from measurable functionsknown as graphons. A natural estimation problem is how well we can recover agraphon given a single graph sampled from it. One general framework forestimating a graphon uses step-functions obtained by partitioning the nodes ofthe graph according to some clustering algorithm. We propose an iterativestep-function estimator (ISFE) that, given an initial partition, iterativelyclusters nodes based on their edge densities with respect to the previousiteration's partition. We analyze ISFE and demonstrate its performance incomparison with other graphon estimation techniques.
arxiv-1412-2700 | Subspace based low rank and joint sparse matrix recovery |  http://arxiv.org/abs/1412.2700  | author:Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai, Mathews Jacob category:cs.NA cs.CV published:2014-12-05 summary:We consider the recovery of a low rank and jointly sparse matrix from undersampled measurements of its columns. This problem is highly relevant in therecovery of dynamic MRI data with high spatio-temporal resolution, where eachcolumn of the matrix corresponds to a frame in the image time series; thematrix is highly low-rank since the frames are highly correlated. Similarly thenon-zero locations of the matrix in appropriate transform/frame domains (e.g.wavelet, gradient) are roughly the same in different frame. The superset of thesupport can be safely assumed to be jointly sparse. Unlike the classicalmultiple measurement vector (MMV) setup that measures all the snapshots usingthe same matrix, we consider each snapshot to be measured using a differentmeasurement matrix. We show that this approach reduces the total number ofmeasurements, especially when the rank of the matrix is much smaller than thanits sparsity. Our experiments in the context of dynamic imaging shows that thisapproach is very useful in realizing free breathing cardiac MRI.
arxiv-1412-1947 | A parallel sampling based clustering |  http://arxiv.org/abs/1412.1947  | author:Aditya AV Sastry, Kalyan Netti category:cs.LG 68Q32 published:2014-12-05 summary:The problem of automatically clustering data is an age old problem. Peoplehave created numerous algorithms to tackle this problem. The execution time ofany of this algorithm grows with the number of input points and the number ofcluster centers required. To reduce the number of input points we could averagethe points locally and use the means or the local centers as the input forclustering. However since the required number of local centers is very high,running the clustering algorithm on the entire dataset to obtain theserepresentational points is very time consuming. To remedy this problem, in thispaper we are proposing two subclustering schemes where by we subdivide thedataset into smaller sets and run the clustering algorithm on the smallerdatasets to obtain the required number of datapoints to run our clusteringalgorithm with. As we are subdividing the given dataset, we could runclustering algorithm on each smaller piece of the dataset in parallel. We foundthat both parallel and serial execution of this method to be much faster thanthe original clustering algorithm and error in running the clustering algorithmon a reduced set to be very less.
arxiv-1412-1945 | Background Modelling using Octree Color Quantization |  http://arxiv.org/abs/1412.1945  | author:Aditya A. V. Sastry category:cs.CV 65D19 published:2014-12-05 summary:By assuming that the most frequently occuring color in a video or a region ofa video I propose a new algorithm for detecting foreground objects in a video.The process of detecting the foreground objects is complicated because of thefact that there may be swaying trees, objects of the background being movedaround or lighting changes in the video. To deal with such complexities manyhave come up with solutions which heavily rely on expensive floating pointoperations. In this paper I used a data structure called Octree which isimplemented only using binary operations. Traditionally octrees were used forcolor quantization but here in this paper I used it as a data structure tostore the most frequently occuring colors in a video as well. For each of thestarting few video frames, I constructed a Octree using all the colors of thatframe. Next I pruned all the trees by removing nodes below a certain height andgave the leaf nodes a color which is dependant on the topological path fromthat node to its parent. Hence any two leaf nodes in two different octrees withthe same topological path from themselves to the root will represent the samecolor. Next I merged all these individual trees into a single tree retainingonly those nodes whose topological path to itself from the root is most commonamong all the trees. The colors represented by the leaf nodes in the resultanttree will be the most frequently occuring colors in the starting video framesof the video. Hence any color of an incomming frame that is not close to any ofthe colors represented by the leaf node of the merged tree can be regarded asbelonging to a foreground object. As an Octree is constructed using only binary operations, it is very fastcompared to other leading algorithms.
arxiv-1412-1871 | A higher homotopic extension of persistent (co)homology |  http://arxiv.org/abs/1412.1871  | author:Estanislao Herscovich category:math.AT cs.CG cs.CV math.KT published:2014-12-05 summary:Our objective in this article is to show a possibly interesting structure ofhomotopic nature appearing in persistent (co)homology. Assuming that thefiltration of the (say) simplicial set embedded in a finite dimensional vectorspace induces a multiplicative filtration (which would not be a so harshhypothesis in our setting) on the dg algebra given by the complex of simplicialcochains, we may use a result by T. Kadeishvili to get a unique (up tononcanonical equivalence) A_infinity-algebra structure on the completepersistent cohomology of the filtered simplicial (or topological) set. We thenprovide a construction of a (pseudo)metric on the set of all (generalized)barcodes (that is, of all cohomological degrees) enriched with theA_infinity-algebra structure stated before, refining the usual bottleneckmetric, and which is also independent of the particular A_infinity-algebrastructure chosen (among those equivalent to each other). We think that thisdistance might deserve some attention for topological data analysis, for it inparticular can recognize different linking or foldings patterns, as in theBorromean rings. As an aside, we give a simple proof of a result relating thebarcode structure between persistent homology and cohomology. This result wasobserved in a recent article by V. de Silva, D. Morozov and M.Vejdemo-Johansson under some restricted assumptions, which we do not suppose.
arxiv-1412-1957 | CoMIC: Good features for detection and matching at object boundaries |  http://arxiv.org/abs/1412.1957  | author:Swarna Kamlam Ravindran, Anurag Mittal category:cs.CV published:2014-12-05 summary:Feature or interest points typically use information aggregation in 2Dpatches which does not remain stable at object boundaries when there is objectmotion against a significantly varying background. Level or iso-intensitycurves are much more stable under such conditions, especially the longer ones.In this paper, we identify stable portions on long iso-curves and detectcorners on them. Further, the iso-curve associated with a corner is used todiscard portions from the background and improve matching. Such CoMIC (Cornerson Maximally-stable Iso-intensity Curves) points yield superior results at theobject boundary regions compared to state-of-the-art detectors while performingcomparably at the interior regions as well. This is illustrated in exhaustivematching experiments for both boundary and non-boundary regions in applicationssuch as stereo and point tracking for structure from motion in video sequences.
arxiv-1412-1866 | Integer Programming Ensemble of Classifiers for Temporal Relations |  http://arxiv.org/abs/1412.1866  | author:Catherine Kerr, Terri Hoare, Jakub Marecek, Paula Carroll category:cs.CL cs.LG math.OC published:2014-12-05 summary:Extraction of events and understanding related temporal expression among themis a major challenge in natural language processing. In longer texts,processing on sentence-by-sentence or expression-by-expression basis oftenfails, in part due to the disregard for the consistency of the processed data.We present an ensemble method, which reconciles the output of multipleclassifiers for temporal expressions, subject to consistency constraints acrossthe whole text. The use of integer programming to enforce the consistencyconstraints globally improves upon the best published results from theTempEval-3 Challenge considerably.
arxiv-1412-2041 | Multi-Target Shrinkage |  http://arxiv.org/abs/1412.2041  | author:Daniel Bartz, Johannes Höhne, Klaus-Robert Müller category:stat.ME stat.ML published:2014-12-05 summary:Stein showed that the multivariate sample mean is outperformed by "shrinking"to a constant target vector. Ledoit and Wolf extended this approach to thesample covariance matrix and proposed a multiple of the identity as shrinkagetarget. In a general framework, independent of a specific estimator, we extendthe shrinkage concept by allowing simultaneous shrinkage to a set of targets.Application scenarios include settings with (A) additional data sets frompotentially similar distributions, (B) non-stationarity, (C) a natural groupingof the data or (D) multiple alternative estimators which could serve astargets. We show that this Multi-Target Shrinkage can be translated into a quadraticprogram and derive conditions under which the estimation of the shrinkageintensities yields optimal expected squared error in the limit. For the samplemean and the sample covariance as specific instances, we derive conditionsunder which the optimality of MTS is applicable. We consider two asymptoticsettings: the large dimensional limit (LDL), where the dimensionality and thenumber of observations go to infinity at the same rate, and the finiteobservations large dimensional limit (FOLDL), where only the dimensionalitygoes to infinity while the number of observations remains constant. We thenshow the effectiveness in extensive simulations and on real world data.
arxiv-1412-2106 | Consistent optimization of AMS by logistic loss minimization |  http://arxiv.org/abs/1412.2106  | author:Wojciech Kotłowski category:cs.LG published:2014-12-05 summary:In this paper, we theoretically justify an approach popular amongparticipants of the Higgs Boson Machine Learning Challenge to optimizeapproximate median significance (AMS). The approach is based on the followingtwo-stage procedure. First, a real-valued function is learned by minimizing asurrogate loss for binary classification, such as logistic loss, on thetraining sample. Then, a threshold is tuned on a separate validation sample, bydirect optimization of AMS. We show that the regret of the resulting(thresholded) classifier measured with respect to the squared AMS, isupperbounded by the regret of the underlying real-valued function measured withrespect to the logistic loss. Hence, we prove that minimizing logisticsurrogate is a consistent method of optimizing AMS.
arxiv-1412-2113 | Consistent Collective Matrix Completion under Joint Low Rank Structure |  http://arxiv.org/abs/1412.2113  | author:Suriya Gunasekar, Makoto Yamada, Dawei Yin, Yi Chang category:stat.ML cs.LG published:2014-12-05 summary:We address the collective matrix completion problem of jointly recovering acollection of matrices with shared structure from partial (and potentiallynoisy) observations. To ensure well--posedness of the problem, we impose ajoint low rank structure, wherein each component matrix is low rank and thelatent space of the low rank factors corresponding to each entity is sharedacross the entire collection. We first develop a rigorous algebra forrepresenting and manipulating collective--matrix structure, and identifysufficient conditions for consistent estimation of collective matrices. We thenpropose a tractable convex estimator for solving the collective matrixcompletion problem, and provide the first non--trivial theoretical guaranteesfor consistency of collective matrix completion. We show that under reasonableassumptions stated in Section 3.1, with high probability, the proposedestimator exactly recovers the true matrices whenever sample complexityrequirements dictated by Theorem 1 are met. The sample complexity requirementderived in the paper are optimum up to logarithmic factors, and significantlyimprove upon the requirements obtained by trivial extensions of standard matrixcompletion. Finally, we propose a scalable approximate algorithm to solve theproposed convex program, and corroborate our results through simulatedexperiments.
arxiv-1412-1908 | Person Re-identification by Saliency Learning |  http://arxiv.org/abs/1412.1908  | author:Rui Zhao, Wanli Ouyang, Xiaogang Wang category:cs.CV published:2014-12-05 summary:Human eyes can recognize person identities based on small salient regions,i.e. human saliency is distinctive and reliable in pedestrian matching acrossdisjoint camera views. However, such valuable information is often hidden whencomputing similarities of pedestrian images with existing approaches. Inspiredby our user study result of human perception on human saliency, we propose anovel perspective for person re-identification based on learning human saliencyand matching saliency distribution. The proposed saliency learning and matchingframework consists of four steps: (1) To handle misalignment caused by drasticviewpoint change and pose variations, we apply adjacency constrained patchmatching to build dense correspondence between image pairs. (2) We propose twoalternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate asaliency score for each image patch, through which distinctive features standout without using identity labels in the training procedure. (3) saliencymatching is proposed based on patch matching. Matching patches withinconsistent saliency brings penalty, and images of the same identity arerecognized by minimizing the saliency matching cost. (4) Furthermore, saliencymatching is tightly integrated with patch matching in a unified structuralRankSVM learning framework. The effectiveness of our approach is validated onthe VIPeR dataset and the CUHK01 dataset. Our approach outperforms thestate-of-the-art person re-identification methods on both datasets.
arxiv-1412-2669 | Two step recovery of jointly sparse and low-rank matrices: theoretical guarantees |  http://arxiv.org/abs/1412.2669  | author:Sampurna Biswas, Sunrita Poddar, Soura Dasgupta, Raghuraman Mudumbai, Mathews Jacob category:stat.ML cs.IT math.IT published:2014-12-05 summary:We introduce a two step algorithm with theoretical guarantees to recover ajointly sparse and low-rank matrix from undersampled measurements of itscolumns. The algorithm first estimates the row subspace of the matrix using aset of common measurements of the columns. In the second step, the subspaceaware recovery of the matrix is solved using a simple least square algorithm.The results are verified in the context of recovering CINE data fromundersampled measurements; we obtain good recovery when the sampling conditionsare satisfied.
arxiv-1412-1684 | How Many Communities Are There? |  http://arxiv.org/abs/1412.1684  | author:Diego Franco Saldana, Yi Yu, Yang Feng category:stat.ME stat.AP stat.CO stat.ML published:2014-12-04 summary:Stochastic blockmodels and variants thereof are among the most widely usedapproaches to community detection for social networks and relational data. Astochastic blockmodel partitions the nodes of a network into disjoint sets,called communities. The approach is inherently related to clustering withmixture models; and raises a similar model selection problem for the number ofcommunities. The Bayesian information criterion (BIC) is a popular solution,however, for stochastic blockmodels, the conditional independence assumptiongiven the communities of the endpoints among different edges is usuallyviolated in practice. In this regard, we propose composite likelihood BIC(CL-BIC) to select the number of communities, and we show it is robust againstpossible misspecifications in the underlying stochastic blockmodel assumptions.We derive the requisite methodology and illustrate the approach using bothsimulated and real data. Supplementary materials containing the relevantcomputer code are available online.
arxiv-1412-1716 | Nonparametric modal regression |  http://arxiv.org/abs/1412.1716  | author:Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, Larry Wasserman category:stat.ME math.ST stat.ML stat.TH published:2014-12-04 summary:Modal regression estimates the local modes of the distribution of $Y$ given$X=x$, instead of the mean, as in the usual regression sense, and can hencereveal important structure missed by usual regression methods. We study asimple nonparametric method for modal regression, based on a kernel densityestimate (KDE) of the joint distribution of $Y$ and $X$. We derive asymptoticerror bounds for this method, and propose techniques for constructingconfidence sets and prediction sets. The latter is used to select the smoothingbandwidth of the underlying KDE. The idea behind modal regression is connectedto many others, such as mixture regression and density ridge estimation, and wediscuss these ties as well.
arxiv-1412-1842 | Reading Text in the Wild with Convolutional Neural Networks |  http://arxiv.org/abs/1412.1842  | author:Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-12-04 summary:In this work we present an end-to-end system for text spotting -- localisingand recognising text in natural scene images -- and text based image retrieval.This system is based on a region proposal mechanism for detection and deepconvolutional neural networks for recognition. Our pipeline uses a novelcombination of complementary proposal generation techniques to ensure highrecall, and a fast subsequent filtering stage for improving precision. For therecognition and ranking of proposals, we train very large convolutional neuralnetworks to perform word recognition on the whole proposal region at the sametime, departing from the character classifier based systems of the past. Thesenetworks are trained solely on data produced by a synthetic text generationengine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performancethroughout. We perform rigorous experiments across a number of standardend-to-end text spotting benchmarks and text-based image retrieval datasets,showing a large improvement over all previous methods. Finally, we demonstratea real-world application of our text spotting system to allow thousands ofhours of news footage to be instantly searchable via a text query.
arxiv-1412-1788 | Primal-Dual Algorithms for Non-negative Matrix Factorization with the Kullback-Leibler Divergence |  http://arxiv.org/abs/1412.1788  | author:Felipe Yanez, Francis Bach category:cs.LG math.OC published:2014-12-04 summary:Non-negative matrix factorization (NMF) approximates a given matrix as aproduct of two non-negative matrices. Multiplicative algorithms deliverreliable results, but they show slow convergence for high-dimensional data andmay be stuck away from local minima. Gradient descent methods have betterbehavior, but only apply to smooth losses such as the least-squares loss. Inthis article, we propose a first-order primal-dual algorithm for non-negativedecomposition problems (where one factor is fixed) with the KL divergence,based on the Chambolle-Pock algorithm. All required computations may beobtained in closed form and we provide an efficient heuristic way to selectstep-sizes. By using alternating optimization, our algorithm readily extends toNMF and, on synthetic examples, face recognition or music source separationdatasets, it is either faster than existing algorithms, or leads to improvedlocal optima, or both.
arxiv-1412-1710 | Convolutional Neural Networks at Constrained Time Cost |  http://arxiv.org/abs/1412.1710  | author:Kaiming He, Jian Sun category:cs.CV published:2014-12-04 summary:Though recent advanced convolutional neural networks (CNNs) have beenimproving the image recognition accuracy, the models are getting more complexand time-consuming. For real-world applications in industrial and commercialscenarios, engineers and developers are often faced with the requirement ofconstrained time budget. In this paper, we investigate the accuracy of CNNsunder constrained time cost. Under this constraint, the designs of the networkarchitectures should exhibit as trade-offs among the factors like depth,numbers of filters, filter sizes, etc. With a series of controlled comparisons,we progressively modify a baseline model while preserving its time complexity.This is also helpful for understanding the importance of the factors in networkdesigns. We present an architecture that achieves very competitive accuracy inthe ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than"AlexNet" (16.0% top-5 error, 10-view test).
arxiv-1412-1587 | The entropic barrier: a simple and optimal universal self-concordant barrier |  http://arxiv.org/abs/1412.1587  | author:Sébastien Bubeck, Ronen Eldan category:math.OC cs.IT cs.LG math.IT published:2014-12-04 summary:We prove that the Cram\'er transform of the uniform measure on a convex bodyin $\mathbb{R}^n$ is a $(1+o(1)) n$-self-concordant barrier, improving aseminal result of Nesterov and Nemirovski. This gives the first explicitconstruction of a universal barrier for convex bodies with optimalself-concordance parameter. The proof is based on basic geometry of log-concavedistributions, and elementary duality in exponential families.
arxiv-1412-1632 | Deep Learning for Answer Sentence Selection |  http://arxiv.org/abs/1412.1632  | author:Lei Yu, Karl Moritz Hermann, Phil Blunsom, Stephen Pulman category:cs.CL published:2014-12-04 summary:Answer sentence selection is the task of identifying sentences that containthe answer to a given question. This is an important problem in its own rightas well as in the larger context of open domain question answering. We proposea novel approach to solving this task via means of distributed representations,and learn to match questions with answers by considering their semanticencoding. This contrasts prior work on this task, which typically relies onclassifiers with large numbers of hand-crafted syntactic and semantic featuresand various external resources. Our approach does not require any featureengineering nor does it involve specialist linguistic data, making this modeleasily applicable to a wide range of domains and languages. Experimentalresults on a standard benchmark dataset from TREC demonstrate that---despiteits simplicity---our model matches state of the art performance on the answersentence selection task.
arxiv-1412-1628 | Fisher Kernel for Deep Neural Activations |  http://arxiv.org/abs/1412.1628  | author:Donggeun Yoo, Sunggyun Park, Joon-Young Lee, In So Kweon category:cs.CV cs.LG cs.NE published:2014-12-04 summary:Compared to image representation based on low-level local descriptors, deepneural activations of Convolutional Neural Networks (CNNs) are richer inmid-level representation, but poorer in geometric invariance properties. Inthis paper, we present a straightforward framework for better imagerepresentation by combining the two approaches. To take advantages of bothrepresentations, we propose an efficient method to extract a fair amount ofmulti-scale dense local activations from a pre-trained CNN. We then aggregatethe activations by Fisher kernel framework, which has been modified with asimple scale-wise normalization essential to make it suitable for CNNactivations. Replacing the direct use of a single activation vector with ourrepresentation demonstrates significant performance improvements: +17.76 (Acc.)on MIT Indoor 67 and +7.18 (mAP) on PASCAL VOC 2007. The results suggest thatour proposal can be used as a primary image representation for betterperformances in visual recognition tasks.
arxiv-1412-1526 | Parsing Occluded People by Flexible Compositions |  http://arxiv.org/abs/1412.1526  | author:Xianjie Chen, Alan Yuille category:cs.CV published:2014-12-04 summary:This paper presents an approach to parsing humans when there is significantocclusion. We model humans using a graphical model which has a tree structurebuilding on recent work [32, 6] and exploit the connectivity prior that, evenin presence of occlusion, the visible nodes form a connected subtree of thegraphical model. We call each connected subtree a flexible composition ofobject parts. This involves a novel method for learning occlusion cues. Duringinference we need to search over a mixture of different flexible models. Byexploiting part sharing, we show that this inference can be done extremelyefficiently requiring only twice as many computations as searching for theentire object (i.e., not modeling occlusion). We evaluate our model on thestandard benchmarked "We Are Family" Stickmen dataset and obtain significantperformance improvements over the best alternative algorithms.
arxiv-1412-1732 | Statistical models and regularization strategies in statistical image reconstruction of low-dose X-ray CT: a survey |  http://arxiv.org/abs/1412.1732  | author:Hao Zhang, Jing Wang, Jianhua Ma, Hongbing Lu, Zhengrong Liang category:physics.med-ph cs.CV published:2014-12-04 summary:Statistical image reconstruction (SIR) methods have shown potential tosubstantially improve the image quality of low-dose X-ray computed tomography(CT) as compared to the conventional filtered back-projection (FBP) method forvarious clinical tasks. According to the maximum a posterior (MAP) estimation,the SIR methods can be typically formulated by an objective function consistingof two terms: (1) data-fidelity (or equivalently, data-fitting ordata-mismatch) term modeling the statistics of projection measurements, and (2)regularization (or equivalently, prior or penalty) term reflecting priorknowledge or expectation on the characteristics of the image to bereconstructed. Existing SIR methods for low-dose CT can be divided into twogroups: (1) those that use calibrated transmitted photon counts (beforelog-transform) with penalized maximum likelihood (pML) criterion, and (2) thosethat use calibrated line-integrals (after log-transform) with penalizedweighted least-squares (PWLS) criterion. Accurate statistical modeling of theprojection measurements is a prerequisite for SIR, while the regularizationterm in the objective function also plays a critical role for successful imagereconstruction. This paper reviews several statistical models on CT projectionmeasurements and various regularization strategies incorporating priorknowledge or expected properties of the image to be reconstructed, whichtogether formulate the objective function of the SIR methods for low-dose X-rayCT.
arxiv-1412-1559 | Iterative Subsampling in Solution Path Clustering of Noisy Big Data |  http://arxiv.org/abs/1412.1559  | author:Yuliya Marchetti, Qing Zhou category:stat.ME stat.ML published:2014-12-04 summary:We develop an iterative subsampling approach to improve the computationalefficiency of our previous work on solution path clustering (SPC). The SPCmethod achieves clustering by concave regularization on the pairwise distancesbetween cluster centers. This clustering method has the important capability torecognize noise and to provide a short path of clustering solutions; however,it is not sufficiently fast for big datasets. Thus, we propose a method thatiterates between clustering a small subsample of the full data and sequentiallyassigning the other data points to attain orders of magnitude of computationalsavings. The new method preserves the ability to isolate noise, includes asolution selection mechanism that ultimately provides one clustering solutionwith an estimated number of clusters, and is shown to be able to extract smalltight clusters from noisy data. The method's relatively minor losses inaccuracy are demonstrated through simulation studies, and its ability to handlelarge datasets is illustrated through applications to gene expression datasets.An R package, SPClustering, for the SPC method with iterative subsampling isavailable at http://www.stat.ucla.edu/~zhou/Software.html.
arxiv-1412-1602 | End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results |  http://arxiv.org/abs/1412.1602  | author:Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML published:2014-12-04 summary:We replace the Hidden Markov Model (HMM) which is traditionally used in incontinuous speech recognition with a bi-directional recurrent neural networkencoder coupled to a recurrent neural network decoder that directly emits astream of phonemes. The alignment between the input and output sequences isestablished using an attention mechanism: the decoder emits each symbol basedon a context created with a subset of input symbols elected by the attentionmechanism. We report initial results demonstrating that this new approachachieves phoneme error rates that are comparable to the state-of-the-artHMM-based decoders, on the TIMIT dataset.
arxiv-1412-1576 | LightLDA: Big Topic Models on Modest Compute Clusters |  http://arxiv.org/abs/1412.1576  | author:Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng, Eric P. Xing, Tie-Yan Liu, Wei-Ying Ma category:stat.ML cs.DC cs.IR cs.LG published:2014-12-04 summary:When building large-scale machine learning (ML) programs, such as big topicmodels or deep neural nets, one usually assumes such tasks can only beattempted with industrial-sized clusters with thousands of nodes, which are outof reach for most practitioners or academic researchers. We consider thischallenge in the context of topic modeling on web-scale corpora, and show thatwith a modest cluster of as few as 8 machines, we can train a topic model with1 million topics and a 1-million-word vocabulary (for a total of 1 trillionparameters), on a document collection with 200 billion tokens -- a scale notyet reported even with thousands of machines. Our major contributions include:1) a new, highly efficient O(1) Metropolis-Hastings sampling algorithm, whoserunning cost is (surprisingly) agnostic of model size, and empiricallyconverges nearly an order of magnitude faster than current state-of-the-artGibbs samplers; 2) a structure-aware model-parallel scheme, which leveragesdependencies within the topic model, yielding a sampling strategy that isfrugal on machine memory and network communication; 3) a differentialdata-structure for model storage, which uses separate data structures for high-and low-frequency words to allow extremely large models to fit in memory, whilemaintaining high inference speed; and 4) a bounded asynchronous data-parallelscheme, which allows efficient distributed processing of massive data via aparameter server. Our distribution strategy is an instance of themodel-and-data-parallel programming model underlying the Petuum framework forgeneral distributed ML, and was implemented on top of the Petuum open-sourcesystem. We provide experimental evidence showing how this development putsmassive models within reach on a small cluster while still enjoyingproportional time cost reductions with increasing cluster size, in comparisonwith alternative options.
arxiv-1412-1523 | Information Exchange and Learning Dynamics over Weakly-Connected Adaptive Networks |  http://arxiv.org/abs/1412.1523  | author:Bicheng Ying, Ali H. Sayed category:cs.MA cs.IT cs.LG math.IT published:2014-12-04 summary:The paper examines the learning mechanism of adaptive agents overweakly-connected graphs and reveals an interesting behavior on how informationflows through such topologies. The results clarify how asymmetries in theexchange of data can mask local information at certain agents and make themtotally dependent on other agents. A leader-follower relationship develops withthe performance of some agents being fully determined by the performance ofother agents that are outside their domain of influence. This scenario canarise, for example, due to intruder attacks by malicious agents or as theresult of failures by some critical links. The findings in this work helpexplain why strong-connectivity of the network topology, adaptation of thecombination weights, and clustering of agents are important ingredients toequalize the learning abilities of all agents against such disturbances. Theresults also clarify how weak-connectivity can be helpful in reducing theeffect of outlier data on learning performance.
arxiv-1412-1574 | Metric Learning Driven Multi-Task Structured Output Optimization for Robust Keypoint Tracking |  http://arxiv.org/abs/1412.1574  | author:Liming Zhao, Xi Li, Jun Xiao, Fei Wu, Yueting Zhuang category:cs.CV cs.LG published:2014-12-04 summary:As an important and challenging problem in computer vision and graphics,keypoint-based object tracking is typically formulated in a spatio-temporalstatistical learning framework. However, most existing keypoint trackers areincapable of effectively modeling and balancing the following three aspects ina simultaneous manner: temporal model coherence across frames, spatial modelconsistency within frames, and discriminative feature construction. To addressthis issue, we propose a robust keypoint tracker based on spatio-temporalmulti-task structured output optimization driven by discriminative metriclearning. Consequently, temporal model coherence is characterized by multi-taskstructured keypoint model learning over several adjacent frames, while spatialmodel consistency is modeled by solving a geometric verification basedstructured learning problem. Discriminative feature construction is enabled bymetric learning to ensure the intra-class compactness and inter-classseparability. Finally, the above three modules are simultaneously optimized ina joint learning scheme. Experimental results have demonstrated theeffectiveness of our tracker.
arxiv-1412-2032 | On using the Microsoft Kinect$^{\rm TM}$ sensors in the analysis of human motion |  http://arxiv.org/abs/1412.2032  | author:M. J. Malinowski, E. Matsinos, S. Roth category:physics.med-ph cs.CV cs.RO published:2014-12-04 summary:The present paper aims at providing the theoretical background required forinvestigating the use of the Microsoft Kinect$^{\rm TM}$ (`Kinect', for short)sensors (original and upgraded) in the analysis of human motion. Ourmethodology is developed in such a way that its application be easily adaptableto comparative studies of other systems used in capturing human-motion data.Our future plans include the application of this methodology to two situations:first, in a comparative study of the performance of the two Kinect sensors;second, in pursuing their validation on the basis of comparisons with amarker-based system (MBS). One important feature in our approach is thetransformation of the MBS output into Kinect-output format, thus enabling theanalysis of the measurements, obtained from different systems, with the samesoftware application, i.e., the one we use in the analysis of Kinect-captureddata; one example of such a transformation, for one popular marker-placementscheme (`Plug-in Gait'), is detailed. We propose that the similarity of theoutput, obtained from the different systems, be assessed on the basis of thecomparison of a number of waveforms, representing the variation within the gaitcycle of quantities which are commonly used in the modelling of the humanmotion. The data acquisition may involve commercially-available treadmills anda number of velocity settings: for instance, walking-motion data may beacquired at $5$ km/h, running-motion data at $8$ and $11$ km/h. We recommendthat particular attention be called to systematic effects associated with thesubject's knee and lower leg, as well as to the ability of the Kinect sensorsin reliably capturing the details in the asymmetry of the motion for the leftand right parts of the human body. The previous versions of the study have beenwithdrawn due to the use of a non-representative database.
arxiv-1412-1740 | Image Data Compression for Covariance and Histogram Descriptors |  http://arxiv.org/abs/1412.1740  | author:Matt J. Kusner, Nicholas I. Kolkin, Stephen Tyree, Kilian Q. Weinberger category:stat.ML cs.CV cs.LG published:2014-12-04 summary:Covariance and histogram image descriptors provide an effective way tocapture information about images. Both excel when used in combination withspecial purpose distance metrics. For covariance descriptors these metricsmeasure the distance along the non-Euclidean Riemannian manifold of symmetricpositive definite matrices. For histogram descriptors the Earth Mover'sdistance measures the optimal transport between two histograms. Although moreprecise, these distance metrics are very expensive to compute, making themimpractical in many applications, even for data sets of only a few thousandexamples. In this paper we present two methods to compress the size ofcovariance and histogram datasets with only marginal increases in test errorfor k-nearest neighbor classification. Specifically, we show that we can reducedata sets to 16% and in some cases as little as 2% of their original size,while approximately matching the test error of kNN classification on the fulltraining set. In fact, because the compressed set is learned in a supervisedfashion, it sometimes even outperforms the full data set, while requiring onlya fraction of the space and drastically reducing test-time computation.
arxiv-1412-1619 | Fast Rates by Transferring from Auxiliary Hypotheses |  http://arxiv.org/abs/1412.1619  | author:Ilja Kuzborskij, Francesco Orabona category:cs.LG published:2014-12-04 summary:In this work we consider the learning setting where, in addition to thetraining set, the learner receives a collection of auxiliary hypothesesoriginating from other tasks. We focus on a broad class of ERM-based linearalgorithms that can be instantiated with any non-negative smooth loss functionand any strongly convex regularizer. We establish generalization and excessrisk bounds, showing that, if the algorithm is fed with a good combination ofsource hypotheses, generalization happens at the fast rate $\mathcal{O}(1/m)$instead of the usual $\mathcal{O}(1/\sqrt{m})$. On the other hand, if thesource hypotheses combination is a misfit for the target task, we recover theusual learning rate. As a byproduct of our study, we also prove a new bound onthe Rademacher complexity of the smooth loss class under weaker assumptionscompared to previous works.
arxiv-1412-1271 | Deep Distributed Random Samplings for Supervised Learning: An Alternative to Random Forests? |  http://arxiv.org/abs/1412.1271  | author:Xiao-Lei Zhang category:cs.LG stat.ML published:2014-12-03 summary:In (\cite{zhang2014nonlinear,zhang2014nonlinear2}), we have viewed machinelearning as a coding and dimensionality reduction problem, and further proposeda simple unsupervised dimensionality reduction method, entitled deepdistributed random samplings (DDRS). In this paper, we further extend it tosupervised learning incrementally. The key idea here is to incorporate labelinformation into the coding process by reformulating that each center in DDRShas multiple output units indicating which class the center belongs to. Thesupervised learning method seems somewhat similar with random forests(\cite{breiman2001random}), here we emphasize their differences as follows. (i)Each layer of our method considers the relationship between part of the datapoints in training data with all training data points, while random forestsfocus on building each decision tree on only part of training data pointsindependently. (ii) Our method builds gradually-narrowed network by samplingless and less data points, while random forests builds gradually-narrowednetwork by merging subclasses. (iii) Our method is trained more straightforwardfrom bottom layer to top layer, while random forests build each tree from toplayer to bottom layer by splitting. (iv) Our method encodes output targetsimplicitly in sparse codes, while random forests encode output targets byremembering the class attributes of the activated nodes. Therefore, our methodis a simpler, more straightforward, and maybe a better alternative choice,though both methods use two very basic elements---randomization and nearestneighbor optimization---as the core. This preprint is used to protect theincremental idea from (\cite{zhang2014nonlinear,zhang2014nonlinear2}). Fullempirical evaluation will be announced carefully later.
arxiv-1412-1454 | Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation |  http://arxiv.org/abs/1412.1454  | author:Noam Shazeer, Joris Pelemans, Ciprian Chelba category:cs.LG cs.CL published:2014-12-03 summary:We present a novel family of language model (LM) estimation techniques namedSparse Non-negative Matrix (SNM) estimation. A first set of experimentsempirically evaluating it on the One Billion Word Benchmark shows that SNM$n$-gram LMs perform almost as well as the well-established Kneser-Ney (KN)models. When using skip-gram features the models are able to match thestate-of-the-art recurrent neural network (RNN) LMs; combining the two modelingtechniques yields the best known result on the benchmark. The computationaladvantages of SNM over both maximum entropy and RNN LM estimation are probablyits main strength, promising an approach that has the same flexibility incombining arbitrary features effectively and yet should scale to very largeamounts of data as gracefully as $n$-gram LMs do.
arxiv-1412-1820 | Context-Dependent Fine-Grained Entity Type Tagging |  http://arxiv.org/abs/1412.1820  | author:Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, David Huynh category:cs.CL published:2014-12-03 summary:Entity type tagging is the task of assigning category labels to each mentionof an entity in a document. While standard systems focus on a small set oftypes, recent work (Ling and Weld, 2012) suggests that using a largefine-grained label set can lead to dramatic improvements in downstream tasks.In the absence of labeled training data, existing fine-grained tagging systemsobtain examples automatically, using resolved entities and their typesextracted from a knowledge base. However, since the appropriate type oftendepends on context (e.g. Washington could be tagged either as city orgovernment), this procedure can result in spurious labels, leading to poorergeneralization. We propose the task of context-dependent fine type tagging,where the set of acceptable labels for a mention is restricted to only thosededucible from the local context (e.g. sentence or document). We introduce newresources for this task: 11,304 mentions annotated with their context-dependentfine types, and we provide baseline experimental results on this data.
arxiv-1412-1506 | Textural Approach for Mass Abnormality Segmentation in Mammographic Images |  http://arxiv.org/abs/1412.1506  | author:Khamsa Djaroudib, Abdelmalik Taleb Ahmed, Abdelmadjid Zidani category:cs.CV 68U10 published:2014-12-03 summary:Mass abnormality segmentation is a vital step for the medical diagnosticprocess and is attracting more and more the interest of many research groups.Currently, most of the works achieved in this area have used the Gray LevelCo-occurrence Matrix (GLCM) as texture features with a region-based approach.These features come in previous phase for segmentation stage or are using asinputs to classification stage. The work discussed in this paper attempts toexperiment the GLCM method under a contour-based approach. Besides, weexperiment the proposed approach on various tissues densities to bring moresignificant results. At this end, we explored some challenging breast imagesfrom BIRADS medical Data Base. Our first experimentations showed promisingresults with regard to the edges mass segmentation methods. This paperdiscusses first the main works achieved in this area. Sections 2 and 3 presentmaterials and our methodology. The main results are showed and evaluated beforeconcluding our paper.
arxiv-1412-1193 | New insights and perspectives on the natural gradient method |  http://arxiv.org/abs/1412.1193  | author:James Martens category:cs.LG stat.ML published:2014-12-03 summary:Natural gradient descent is an optimization method traditionally motivatedfrom the perspective of information geometry, and works well for manyapplications as an alternative to stochastic gradient descent. In this paper wecritically analyze this method and its properties, and show how it can beviewed as a type of approximate 2nd-order optimization method, where the Fisherinformation matrix used to compute the natural gradient direction can be viewedas an approximation of the Hessian. This perspective turns out to havesignificant implications for how to design a practical and robust version ofthe method. Among our various other contributions is a thorough analysis of theconvergence speed of natural gradient descent and more general stochasticmethods, a critical examination of the oft-used "empirical" approximation ofthe Fisher matrix, and an analysis of the (approximate) parameterizationinvariance property possessed by the method, which we show still holds forcertain other choices of the curvature matrix, but notably not the Hessian.
arxiv-1412-1455 | Event Retrieval Using Motion Barcodes |  http://arxiv.org/abs/1412.1455  | author:Gil Ben-Artzi, Michael Werman, Shmuel Peleg category:cs.CV published:2014-12-03 summary:We introduce a simple and effective method for retrieval of videos showing aspecific event, even when the videos of that event were captured fromsignificantly different viewpoints. Appearance-based methods fail in suchcases, as appearances change with large changes of viewpoints. Our method is based on a pixel-based feature, "motion barcode", which recordsthe existence/non-existence of motion as a function of time. While appearance,motion magnitude, and motion direction can vary greatly between disparateviewpoints, the existence of motion is viewpoint invariant. Based on the motionbarcode, a similarity measure is developed for videos of the same event takenfrom very different viewpoints. This measure is robust to occlusions commonunder different viewpoints, and can be computed efficiently. Event retrieval is demonstrated using challenging videos from stationary andhand held cameras.
arxiv-1412-1283 | Convolutional Feature Masking for Joint Object and Stuff Segmentation |  http://arxiv.org/abs/1412.1283  | author:Jifeng Dai, Kaiming He, Jian Sun category:cs.CV published:2014-12-03 summary:The topic of semantic segmentation has witnessed considerable progress due tothe powerful features learned by convolutional neural networks (CNNs). Thecurrent leading approaches for semantic segmentation exploit shape informationby extracting CNN features from masked image regions. This strategy introducesartificial boundaries on the images and may impact the quality of the extractedfeatures. Besides, the operations on the raw image domain require to computethousands of networks on a single image, which is time-consuming. In thispaper, we propose to exploit shape information via masking convolutionalfeatures. The proposal segments (e.g., super-pixels) are treated as masks onthe convolutional feature maps. The CNN features of segments are directlymasked out from these maps and used to train classifiers for recognition. Wefurther propose a joint method to handle objects and "stuff" (e.g., grass, sky,water) in the same framework. State-of-the-art results are demonstrated onbenchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compellingcomputational speed.
arxiv-1412-1138 | Highly comparative fetal heart rate analysis |  http://arxiv.org/abs/1412.1138  | author:B. D. Fulcher, A. E. Georgieva, C. W. G. Redman, Nick S. Jones category:cs.LG cs.AI q-bio.QM published:2014-12-03 summary:A database of fetal heart rate (FHR) time series measured from 7221 patientsduring labor is analyzed with the aim of learning the types of features ofthese recordings that are informative of low cord pH. Our 'highly comparative'analysis involves extracting over 9000 time-series analysis features from eachFHR time series, including measures of autocorrelation, entropy, distribution,and various model fits. This diverse collection of features was developed inprevious work, and is publicly available. We describe five features that mostaccurately classify a balanced training set of 59 'low pH' and 59 'normal pH'FHR recordings. We then describe five of the features with the strongest linearcorrelation to cord pH across the full dataset of FHR time series. The featuresidentified in this work may be used as part of a system for guidingintervention during labor in future. This work successfully demonstrates theutility of comparing across a large, interdisciplinary literature ontime-series analysis to automatically contribute new scientific results forspecific biomedical signal processing challenges.
arxiv-1412-1441 | Scalable, High-Quality Object Detection |  http://arxiv.org/abs/1412.1441  | author:Christian Szegedy, Scott Reed, Dumitru Erhan, Dragomir Anguelov, Sergey Ioffe category:cs.CV published:2014-12-03 summary:Current high-quality object detection approaches use the scheme ofsalience-based object proposal methods followed by post-classification usingdeep convolutional features. This spurred recent research in improving objectproposal methods. However, domain agnostic proposal generation has theprincipal drawback that the proposals come unranked or with very weak ranking,making it hard to trade-off quality for running time. This raises the morefundamental question of whether high-quality proposal generation requirescareful engineering or can be derived just from data alone. We demonstrate thatlearning-based proposal methods can effectively match the performance ofhand-engineered methods while allowing for very efficient runtime-qualitytrade-offs. Using the multi-scale convolutional MultiBox (MSC-MultiBox)approach, we substantially advance the state-of-the-art on the ILSVRC 2014detection challenge data set, with $0.5$ mAP for a single model and $0.52$ mAPfor an ensemble of two models. MSC-Multibox significantly improves the proposalquality over its predecessor MultiBox~method: AP increases from $0.42$ to$0.53$ for the ILSVRC detection challenge. Finally, we demonstrate improvedbounding-box recall compared to Multiscale Combinatorial Grouping with lessproposals on the Microsoft-COCO data set.
arxiv-1412-1194 | Gradient Boundary Histograms for Action Recognition |  http://arxiv.org/abs/1412.1194  | author:Feng Shi, Robert Laganiere, Emil Petriu category:cs.CV published:2014-12-03 summary:This paper introduces a high efficient local spatiotemporal descriptor,called gradient boundary histograms (GBH). The proposed GBH descriptor is builton simple spatio-temporal gradients, which are fast to compute. We demonstratethat it can better represent local structure and motion than othergradient-based descriptors, and significantly outperforms them on largerealistic datasets. A comprehensive evaluation shows that the recognitionaccuracy is preserved while the spatial resolution is greatly reduced, whichyields both high efficiency and low memory usage.
arxiv-1412-1215 | Mary Astell's words in A Serious Proposal to the Ladies (part I), a lexicographic inquiry with NooJ |  http://arxiv.org/abs/1412.1215  | author:Hélène Pignot, Odile Piton category:cs.CL published:2014-12-03 summary:In the following article we elected to study with NooJ the lexis of a 17 thcentury text, Mary Astell's seminal essay, A Serious Proposal to the Ladies,part I, published in 1694. We first focused on the semantics to see how Astellbuilds her vindication of the female sex, which words she uses to sensitisewomen to their alienated condition and promote their education. Then we studiedthe morphology of the lexemes (which is different from contemporary English)used by the author, thanks to the NooJ tools we have devised for this purpose.NooJ has great functionalities for lexicographic work. Its commands and graphsprove to be most efficient in the spotting of archaic words or variants inspelling. Introduction In our previous articles, we have studied thesingularities of 17 th century English within the framework of a diachronicanalysis thanks to syntactical and morphological graphs and thanks to thedictionaries we have compiled from a corpus that may be expanded overtime. Ourearly work was based on a limited corpus of English travel literature to Greecein the 17 th century. This article deals with a late seventeenth century textwritten by a woman philosopher and essayist, Mary Astell (1666--1731),considered as one of the first English feminists. Astell wrote her essay at atime in English history when women were "the weaker vessel" and their mainbusiness in life was to charm and please men by their looks and submissiveness.In this essay we will see how NooJ can help us analyse Astell's rhetoric (whatpoint of view does she adopt, does she speak in her own name, in the name ofall women, what is her representation of men and women and their relationshipsin the text, what are the goals of education?). Then we will turn our attentionto the morphology of words in the text and use NooJ commands and graphs tocarry out a lexicographic inquiry into Astell's lexemes.
arxiv-1412-1216 | Simple Two-Dimensional Object Tracking based on a Graph Algorithm |  http://arxiv.org/abs/1412.1216  | author:Alexandra Heidsieck category:cs.CV published:2014-12-03 summary:The visual observation and tracking of cells and other micrometer-sizedobjects has many different biomedical applications. The automation of thosetasks based on computer methods helps in the evaluation of such measurements.In this work, we present a general purpose algorithm that excels at evaluatingdeterministic behavior of micrometer-sized objects. Our concrete application isthe tracking of fast moving objects over large distances along deterministictrajectories in a microscopic video. Thereby, we are able to determinecharacteristic properties of the objects. For this purpose, we use a set ofbasic algorithms, including blob recognition, feature-based shape recognitionand a graph algorithm, and combined them in a novel way. An evaluation of thealgorithms performance shows a high accuracy in the recognition of objects aswell as of complete trajectories. Moreover, a direct comparison to a similaralgorithm shows superior recognition rates.
arxiv-1412-1219 | Colorisation et texturation temps réel d'environnements urbains par système mobile avec scanner laser et caméra fish-eye |  http://arxiv.org/abs/1412.1219  | author:Jean-Emmanuel Deschaud, Xavier Brun, François Goulette category:cs.RO cs.CV published:2014-12-03 summary:We present here a real time mobile mapping system mounted on a vehicle. Theterrestrial acquisition system is based on a geolocation system and twosensors, namely, a laser scanner and a camera with a fish-eye lens. We produce3D colored points cloud and textured models of the environment. Once the systemhas been calibrated, the data acquisition and processing are done "on the way".This article mainly presents our methods of colorization of point cloud,triangulation and texture mapping.
arxiv-1412-1463 | On the String Kernel Pre-Image Problem with Applications in Drug Discovery |  http://arxiv.org/abs/1412.1463  | author:Sébastien Giguère, Amélie Rolland, François Laviolette, Mario Marchand category:cs.LG cs.CE I.2.6; K.3.2 published:2014-12-03 summary:The pre-image problem has to be solved during inference by most structuredoutput predictors. For string kernels, this problem corresponds to finding thestring associated to a given input. An algorithm capable of solving or findinggood approximations to this problem would have many applications incomputational biology and other fields. This work uses a recent result oncombinatorial optimization of linear predictors based on string kernels todevelop, for the pre-image, a low complexity upper bound valid for many stringkernels. This upper bound is used with success in a branch and bound searchingalgorithm. Applications and results in the discovery of druggable peptides arepresented and discussed.
arxiv-1412-1265 | Deeply learned face representations are sparse, selective, and robust |  http://arxiv.org/abs/1412.1265  | author:Yi Sun, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-12-03 summary:This paper designs a high-performance deep convolutional network (DeepID2+)for face recognition. It is learned with the identification-verificationsupervisory signal. By increasing the dimension of hidden representations andadding supervision to early convolutional layers, DeepID2+ achieves newstate-of-the-art on LFW and YouTube Faces benchmarks. Through empiricalstudies, we have discovered three properties of its deep neural activationscritical for the high performance: sparsity, selectiveness and robustness. (1)It is observed that neural activations are moderately sparse. Moderate sparsitymaximizes the discriminative power of the deep net as well as the distancebetween images. It is surprising that DeepID2+ still can achieve highrecognition accuracy even after the neural responses are binarized. (2) Itsneurons in higher layers are highly selective to identities andidentity-related attributes. We can identify different subsets of neurons whichare either constantly excited or inhibited when different identities orattributes are present. Although DeepID2+ is not taught to distinguishattributes during training, it has implicitly learned such high-level concepts.(3) It is much more robust to occlusions, although occlusion patterns are notincluded in the training set.
arxiv-1412-1443 | Structure learning of antiferromagnetic Ising models |  http://arxiv.org/abs/1412.1443  | author:Guy Bresler, David Gamarnik, Devavrat Shah category:stat.ML cs.IT cs.LG math.IT published:2014-12-03 summary:In this paper we investigate the computational complexity of learning thegraph structure underlying a discrete undirected graphical model from i.i.d.samples. We first observe that the notoriously difficult problem of learningparities with noise can be captured as a special case of learning graphicalmodels. This leads to an unconditional computational lower bound of $\Omega(p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree$d$, for the class of so-called statistical algorithms recently introduced byFeldman et al (2013). The lower bound suggests that the $O(p^d)$ runtimerequired to exhaustively search over neighborhoods cannot be significantlyimproved without restricting the class of models. Aside from structural assumptions on the graph such as it being a tree,hypertree, tree-like, etc., many recent papers on structure learning assumethat the model has the correlation decay property. Indeed, focusing onferromagnetic Ising models, Bento and Montanari (2009) showed that all knownlow-complexity algorithms fail to learn simple graphs when the interactionstrength exceeds a number related to the correlation decay threshold. Oursecond set of results gives a class of repelling (antiferromagnetic) modelsthat have the opposite behavior: very strong interaction allows efficientlearning in time $O(p^2)$. We provide an algorithm whose performanceinterpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of therepulsion.
arxiv-1412-1285 | The inductive theory of natural selection |  http://arxiv.org/abs/1412.1285  | author:Steven A. Frank category:q-bio.PE cs.NE physics.bio-ph published:2014-12-03 summary:The theory of natural selection has two forms. Deductive theory describes howpopulations change over time. One starts with an initial population and somerules for change. From those assumptions, one calculates the future state ofthe population. Deductive theory predicts how populations adapt toenvironmental challenge. Inductive theory describes the causes of change inpopulations. One starts with a given amount of change. One then assignsdifferent parts of the total change to particular causes. Inductive theoryanalyzes alternative causal models for how populations have adapted toenvironmental challenge. This chapter emphasizes the inductive analysis ofcause.
arxiv-1412-1342 | A perspective on the advancement of natural language processing tasks via topological analysis of complex networks |  http://arxiv.org/abs/1412.1342  | author:Diego R. Amancio category:cs.CL published:2014-12-03 summary:Comment on "Approaching human language with complex networks" by Cong and Liu(Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618).
arxiv-1412-1442 | Memory Bounded Deep Convolutional Networks |  http://arxiv.org/abs/1412.1442  | author:Maxwell D. Collins, Pushmeet Kohli category:cs.CV published:2014-12-03 summary:In this work, we investigate the use of sparsity-inducing regularizers duringtraining of Convolution Neural Networks (CNNs). These regularizers encouragethat fewer connections in the convolution and fully connected layers takenon-zero values and in effect result in sparse connectivity between hiddenunits in the deep network. This in turn reduces the memory and runtime costinvolved in deploying the learned CNNs. We show that training with suchregularization can still be performed using stochastic gradient descentimplying that it can be used easily in existing codebases. Experimentalevaluation of our approach on MNIST, CIFAR, and ImageNet datasets shows thatour regularizers can result in dramatic reductions in memory requirements. Forinstance, when applied on AlexNet, our method can reduce the memory consumptionby a factor of four with minimal loss in accuracy.
arxiv-1412-1353 | Curriculum Learning of Multiple Tasks |  http://arxiv.org/abs/1412.1353  | author:Anastasia Pentina, Viktoriia Sharmanska, Christoph H. Lampert category:stat.ML cs.LG published:2014-12-03 summary:Sharing information between multiple tasks enables algorithms to achieve goodgeneralization performance even from small amounts of training data. However,in a realistic scenario of multi-task learning not all tasks are equallyrelated to each other, hence it could be advantageous to transfer informationonly between the most related tasks. In this work we propose an approach thatprocesses multiple tasks in a sequence with sharing between subsequent tasksinstead of solving all tasks jointly. Subsequently, we address the question ofcurriculum learning of tasks, i.e. finding the best order of tasks to belearned. Our approach is based on a generalization bound criterion for choosingthe task order that optimizes the average expected classification performanceover all tasks. Our experimental results show that learning multiple relatedtasks sequentially can be more effective than learning them jointly, the orderin which tasks are being solved affects the overall performance, and that ourmodel is able to automatically discover the favourable order of tasks.
arxiv-1412-1370 | Nested Variational Compression in Deep Gaussian Processes |  http://arxiv.org/abs/1412.1370  | author:James Hensman, Neil D. Lawrence category:stat.ML published:2014-12-03 summary:Deep Gaussian processes provide a flexible approach to probabilisticmodelling of data using either supervised or unsupervised learning. Fortractable inference approximations to the marginal likelihood of the model mustbe made. The original approach to approximate inference in these models usedvariational compression to allow for approximate variational marginalization ofthe hidden variables leading to a lower bound on the marginal likelihood of themodel [Damianou and Lawrence, 2013]. In this paper we extend this idea with anested variational compression. The resulting lower bound on the likelihood canbe easily parallelized or adapted for stochastic variational inference.
arxiv-1412-0781 | Fast Steerable Principal Component Analysis |  http://arxiv.org/abs/1412.0781  | author:Zhizhen Zhao, Yoel Shkolnisky, Amit Singer category:cs.CV published:2014-12-02 summary:Cryo-electron microscopy nowadays often requires the analysis of hundreds ofthousands of 2D images as large as a few hundred pixels in each direction. Herewe introduce an algorithm that efficiently and accurately performs principalcomponent analysis (PCA) for a large set of two-dimensional images, and, foreach image, the set of its uniform rotations in the plane and theirreflections. For a dataset consisting of $n$ images of size $L \times L$pixels, the computational complexity of our algorithm is $O(nL^3 + L^4)$, whileexisting algorithms take $O(nL^4)$. The new algorithm computes the expansioncoefficients of the images in a Fourier-Bessel basis efficiently using thenon-uniform fast Fourier transform. We compare the accuracy and efficiency ofthe new algorithm with traditional PCA and existing algorithms for steerablePCA.
arxiv-1412-0774 | Feedforward semantic segmentation with zoom-out features |  http://arxiv.org/abs/1412.0774  | author:Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich category:cs.CV published:2014-12-02 summary:We introduce a purely feed-forward architecture for semantic segmentation. Wemap small image elements (superpixels) to rich feature representationsextracted from a sequence of nested regions of increasing extent. These regionsare obtained by "zooming out" from the superpixel all the way to scene-levelresolution. This approach exploits statistical structure in the image and inthe label space without setting up explicit structured prediction mechanisms,and thus avoids complex and expensive inference. Instead superpixels areclassified by a feedforward multilayer network. Our architecture achieves newstate of the art performance in semantic segmentation, obtaining 64.4% averageaccuracy on the PASCAL VOC 2012 test set.
arxiv-1412-1841 | Exemplar Dynamics and Sound Merger in Language |  http://arxiv.org/abs/1412.1841  | author:P. F. Tupper category:cs.CL math.DS nlin.AO 91F20, 70F99 published:2014-12-02 summary:We develop a model of phonological contrast in natural language.Specifically, the model describes the maintenance of contrast between differentwords in a language, and the elimination of such contrast when sounds in thewords merge. An example of such a contrast is that provided by the two vowelsounds 'i' and 'e', which distinguish pairs of words such as 'pin' and 'pen' inmost dialects of English. We model language users' knowledge of thepronunciation of a word as consisting of collections of labeled exemplarsstored in memory. Each exemplar is a detailed memory of a particular utteranceof the word in question. In our model an exemplar is represented by one or twophonetic variables along with a weight indicating how strong the memory of theutterance is. Starting from an exemplar-level model we deriveintegro-differential equations for the evolution of exemplar density fields inphonetic space. Using these latter equations we investigate under whatconditions two sounds merge, thus eliminating the contrast. Our main conclusionis that for the preservation of phonological contrast, it is necessary thatanomalous utterances of a given word are discarded, and not merely stored inmemory as an exemplar of another word.
arxiv-1412-1123 | DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection |  http://arxiv.org/abs/1412.1123  | author:Gedas Bertasius, Jianbo Shi, Lorenzo Torresani category:cs.CV published:2014-12-02 summary:Contour detection has been a fundamental component in many image segmentationand object detection systems. Most previous work utilizes low-level featuressuch as texture or saliency to detect contours and then use them as cues for ahigher-level task such as object detection. However, we claim that recognizingobjects and predicting contours are two mutually related tasks. Contrary totraditional approaches, we show that we can invert the commonly establishedpipeline: instead of detecting contours with low-level cues for a higher-levelrecognition task, we exploit object-related features as high-level cues forcontour detection. We achieve this goal by means of a multi-scale deep network that consists offive convolutional layers and a bifurcated fully-connected sub-network. Thesection from the input layer to the fifth convolutional layer is fixed anddirectly lifted from a pre-trained network optimized over a large-scale objectclassification task. This section of the network is applied to four differentscales of the image input. These four parallel and identical streams are thenattached to a bifurcated sub-network consisting of two independently-trainedbranches. One branch learns to predict the contour likelihood (with aclassification objective) whereas the other branch is trained to learn thefraction of human labelers agreeing about the contour presence at a given point(with a regression criterion). We show that without any feature engineering our multi-scale deep learningapproach achieves state-of-the-art results in contour detection.
arxiv-1412-1114 | Easy Hyperparameter Search Using Optunity |  http://arxiv.org/abs/1412.1114  | author:Marc Claesen, Jaak Simm, Dusan Popovic, Yves Moreau, Bart De Moor category:cs.LG published:2014-12-02 summary:Optunity is a free software package dedicated to hyperparameter optimization.It contains various types of solvers, ranging from undirected methods to directsearch, particle swarm and evolutionary optimization. The design focuses onease of use, flexibility, code clarity and interoperability with existingsoftware in all machine learning environments. Optunity is written in Pythonand contains interfaces to environments such as R and MATLAB. Optunity uses aBSD license and is freely available online at http://www.optunity.net.
arxiv-1412-0767 | Learning Spatiotemporal Features with 3D Convolutional Networks |  http://arxiv.org/abs/1412.0767  | author:Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri category:cs.CV published:2014-12-02 summary:We propose a simple, yet effective approach for spatiotemporal featurelearning using deep 3-dimensional convolutional networks (3D ConvNets) trainedon a large scale supervised video dataset. Our findings are three-fold: 1) 3DConvNets are more suitable for spatiotemporal feature learning compared to 2DConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels inall layers is among the best performing architectures for 3D ConvNets; and 3)Our learned features, namely C3D (Convolutional 3D), with a simple linearclassifier outperform state-of-the-art methods on 4 different benchmarks andare comparable with current best methods on the other 2 benchmarks. Inaddition, the features are compact: achieving 52.8% accuracy on UCF101 datasetwith only 10 dimensions and also very efficient to compute due to the fastinference of ConvNets. Finally, they are conceptually very simple and easy totrain and use.
