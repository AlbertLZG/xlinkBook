arxiv-12900-1 | Objects2action: Classifying and localizing actions without any video example | http://arxiv.org/pdf/1510.06939v1.pdf | author:Mihir Jain, Jan C. van Gemert, Thomas Mensink, Cees G. M. Snoek category:cs.CV published:2015-10-23 summary:The goal of this paper is to recognize actions in video without the need forexamples. Different from traditional zero-shot approaches we do not demand thedesign and specification of attribute classifiers and class-to-attributemappings to allow for transfer from seen classes to unseen classes. Our keycontribution is objects2action, a semantic word embedding that is spanned by askip-gram model of thousands of object categories. Action labels are assignedto an object encoding of unseen video based on a convex combination of actionand object affinities. Our semantic embedding has three main characteristics toaccommodate for the specifics of actions. First, we propose a mechanism toexploit multiple-word descriptions of actions and objects. Second, weincorporate the automated selection of the most responsive objects per action.And finally, we demonstrate how to extend our zero-shot approach to thespatio-temporal localization of actions in video. Experiments on four actiondatasets demonstrate the potential of our approach.
arxiv-12900-2 | On the complexity of switching linear regression | http://arxiv.org/pdf/1510.06920v1.pdf | author:Fabien Lauer category:stat.ML cs.CC cs.LG published:2015-10-23 summary:This technical note extends recent results on the computational complexity ofglobally minimizing the error of piecewise-affine models to the related problemof minimizing the error of switching linear regression models. In particular,we show that, on the one hand the problem is NP-hard, but on the other hand, itadmits a polynomial-time algorithm with respect to the number of data for anyfixed data dimension and number of modes.
arxiv-12900-3 | Semi-Automatic Segmentation of Autosomal Dominant Polycystic Kidneys using Random Forests | http://arxiv.org/pdf/1510.06915v1.pdf | author:Kanishka Sharma, Loic Peter, Christian Rupprecht, Anna Caroli, Lichao Wang, Andrea Remuzzi, Maximilian Baust, Nassir Navab category:cs.CV published:2015-10-23 summary:This paper presents a method for 3D segmentation of kidneys from patientswith autosomal dominant polycystic kidney disease (ADPKD) and severe renalinsufficiency, using computed tomography (CT) data. ADPKD severely alters theshape of the kidneys due to non-uniform formation of cysts. As a consequence,fully automatic segmentation of such kidneys is very challenging. We present asegmentation method with minimal user interaction based on a random forestclassifier. One of the major novelties of the proposed approach is the usage ofgeodesic distance volumes as additional source of information. These volumescontain the intensity weighted distance to a manual outline of the respectivekidney in only one slice (for each kidney) of the CT volume. We evaluate ourmethod qualitatively and quantitatively on 55 CT acquisitions using groundtruth annotations from clinical experts.
arxiv-12900-4 | Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted Nuclear Norm | http://arxiv.org/pdf/1510.06895v1.pdf | author:Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin category:cs.LG cs.CV cs.NA published:2015-10-23 summary:The nuclear norm is widely used as a convex surrogate of the rank function incompressive sensing for low rank matrix recovery with its applications in imagerecovery and signal processing. However, solving the nuclear norm based relaxedconvex problem usually leads to a suboptimal solution of the original rankminimization problem. In this paper, we propose to perform a family ofnonconvex surrogates of $L_0$-norm on the singular values of a matrix toapproximate the rank function. This leads to a nonconvex nonsmooth minimizationproblem. Then we propose to solve the problem by Iteratively Reweighted NuclearNorm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular ValueThresholding (WSVT) problem, which has a closed form solution due to thespecial properties of the nonconvex surrogate functions. We also extend IRNN tosolve the nonconvex problem with two or more blocks of variables. In theory, weprove that IRNN decreases the objective function value monotonically, and anylimit point is a stationary point. Extensive experiments on both synthesizeddata and real images demonstrate that IRNN enhances the low-rank matrixrecovery compared with state-of-the-art convex algorithms.
arxiv-12900-5 | Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks | http://arxiv.org/pdf/1509.03475v2.pdf | author:Minhyung Cho, Chandra Shekhar Dhir, Jaehyung Lee category:cs.LG cs.NE stat.ML published:2015-09-11 summary:Multidimensional recurrent neural networks (MDRNNs) have shown a remarkableperformance in the area of speech and handwriting recognition. The performanceof an MDRNN is improved by further increasing its depth, and the difficulty oflearning the deeper network is overcome by using Hessian-free (HF)optimization. Given that connectionist temporal classification (CTC) isutilized as an objective of learning an MDRNN for sequence labeling, thenon-convexity of CTC poses a problem when applying HF to the network. As asolution, a convex approximation of CTC is formulated and its relationship withthe EM algorithm and the Fisher information matrix is discussed. An MDRNN up toa depth of 15 layers is successfully trained using HF, resulting in an improvedperformance for sequence labeling.
arxiv-12900-6 | Fast Latent Variable Models for Inference and Visualization on Mobile Devices | http://arxiv.org/pdf/1510.07035v1.pdf | author:Joseph W Robinson, Aaron Q Li category:cs.LG cs.CL cs.DC cs.IR published:2015-10-23 summary:In this project we outline Vedalia, a high performance distributed networkfor performing inference on latent variable models in the context of Amazonreview visualization. We introduce a new model, RLDA, which extends LatentDirichlet Allocation (LDA) [Blei et al., 2003] for the review space byincorporating auxiliary data available in online reviews to improve modelingwhile simultaneously remaining compatible with pre-existing fast samplingtechniques such as [Yao et al., 2009; Li et al., 2014a] to achieve highperformance. The network is designed such that computation is efficientlyoffloaded to the client devices using the Chital system [Robinson & Li, 2015],improving response times and reducing server costs. The resulting system isable to rapidly compute a large number of specialized latent variable modelswhile requiring minimal server resources.
arxiv-12900-7 | Learning in the Rational Speech Acts Model | http://arxiv.org/pdf/1510.06807v1.pdf | author:Will Monroe, Christopher Potts category:cs.CL published:2015-10-23 summary:The Rational Speech Acts (RSA) model treats language use as a recursiveprocess in which probabilistic speaker and listener agents reason about eachother's intentions to enrich the literal semantics of their language alongbroadly Gricean lines. RSA has been shown to capture many kinds ofconversational implicature, but it has been criticized as an unrealistic modelof speakers, and it has so far required the manual specification of a semanticlexicon, preventing its use in natural language processing applications thatlearn lexical knowledge from data. We address these concerns by showing how todefine and optimize a trained statistical classifier that uses the intermediateagents of RSA as hidden layers of representation forming a non-linearactivation function. This treatment opens up new application domains and newpossibilities for learning effectively from data. We validate the model on areferential expression generation task, showing that the best performance isachieved by incorporating features approximating well-established insightsabout natural language generation into RSA.
arxiv-12900-8 | Efficient Blind Compressed Sensing Using Sparsifying Transforms with Convergence Guarantees and Application to MRI | http://arxiv.org/pdf/1501.02923v2.pdf | author:Saiprasad Ravishankar, Yoram Bresler category:cs.LG stat.ML published:2015-01-13 summary:Natural signals and images are well-known to be approximately sparse intransform domains such as Wavelets and DCT. This property has been heavilyexploited in various applications in image processing and medical imaging.Compressed sensing exploits the sparsity of images or image patches in atransform domain or synthesis dictionary to reconstruct images fromundersampled measurements. In this work, we focus on blind compressed sensing,where the underlying sparsifying transform is a priori unknown, and propose aframework to simultaneously reconstruct the underlying image as well as thesparsifying transform from highly undersampled measurements. The proposed blockcoordinate descent type algorithms involve highly efficient optimal updates.Importantly, we prove that although the proposed blind compressed sensingformulations are highly nonconvex, our algorithms are globally convergent(i.e., they converge from any initialization) to the set of critical points ofthe objectives defining the formulations. These critical points are guaranteedto be at least partial global and partial local minimizers. The exact point(s)of convergence may depend on initialization. We illustrate the usefulness ofthe proposed framework for magnetic resonance image reconstruction from highlyundersampled k-space measurements. As compared to previous methods involvingthe synthesis dictionary model, our approach is much faster, while alsoproviding promising reconstruction quality.
arxiv-12900-9 | Adaptive Mixtures of Factor Analyzers | http://arxiv.org/pdf/1507.02801v2.pdf | author:Heysem Kaya, Albert Ali Salah category:stat.ML cs.IT cs.LG math.IT G.3; I.5.4 published:2015-07-10 summary:A mixture of factor analyzers is a semi-parametric density estimator thatgeneralizes the well-known mixtures of Gaussians model by allowing eachGaussian in the mixture to be represented in a different lower-dimensionalmanifold. This paper presents a robust and parsimonious model selectionalgorithm for training a mixture of factor analyzers, carrying out simultaneousclustering and locally linear, globally nonlinear dimensionality reduction.Permitting different number of factors per mixture component, the algorithmadapts the model complexity to the data complexity. We compare the proposedalgorithm with related automatic model selection algorithms on a number ofbenchmarks. The results indicate the effectiveness of this fast and robustapproach in clustering, manifold learning and class-conditional modeling.
arxiv-12900-10 | ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional Networks on Multi-Core and Many-Core Shared Memory Machines | http://arxiv.org/pdf/1510.06706v1.pdf | author:Aleksandar Zlateski, Kisuk Lee, H. Sebastian Seung category:cs.NE cs.CV cs.DC cs.LG published:2015-10-22 summary:Convolutional networks (ConvNets) have become a popular approach to computervision. It is important to accelerate ConvNet training, which iscomputationally costly. We propose a novel parallel algorithm based ondecomposition into a set of tasks, most of which are convolutions or FFTs.Applying Brent's theorem to the task dependency graph implies that linearspeedup with the number of processors is attainable within the PRAM model ofparallel computation, for wide network architectures. To attain suchperformance on real shared-memory machines, our algorithm computes convolutionsconverging on the same node of the network with temporal locality to reducecache misses, and sums the convergent convolution outputs via an almostwait-free concurrent method to reduce time spent in critical sections. Weimplement the algorithm with a publicly available software package called ZNN.Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughlyequal to the number of physical cores. We also show that ZNN can attain over90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups areachieved for network architectures with widths that are in common use. The taskparallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelismof previous algorithms is compatible with GPUs. Through examples, we show thatZNN can be either faster or slower than certain GPU implementations dependingon specifics of the network architecture, kernel sizes, and density and size ofthe output patch. ZNN may be less costly to develop and maintain, due to therelative ease of general-purpose CPU programming.
arxiv-12900-11 | Partitioning Data on Features or Samples in Communication-Efficient Distributed Optimization? | http://arxiv.org/pdf/1510.06688v1.pdf | author:Chenxin Ma, Martin Takáč category:math.OC cs.LG published:2015-10-22 summary:In this paper we study the effect of the way that the data is partitioned indistributed optimization. The original DiSCO algorithm [Communication-EfficientDistributed Optimization of Self-Concordant Empirical Loss, Yuchen Zhang andLin Xiao, 2015] partitions the input data based on samples. We describe how theoriginal algorithm has to be modified to allow partitioning on features andshow its efficiency both in theory and also in practice.
arxiv-12900-12 | Dual Free SDCA for Empirical Risk Minimization with Adaptive Probabilities | http://arxiv.org/pdf/1510.06684v1.pdf | author:Xi He, Martin Takáč category:math.OC cs.LG published:2015-10-22 summary:In this paper we develop dual free SDCA with adaptive probabilities forregularized empirical risk minimization. This extends recent work of ShaiShalev-Shwartz [SDCA without Duality, arXiv:1502.06177] to allow non-uniformselection of "dual" coordinate in SDCA. Moreover, the probability can changeover time, making it more efficient than uniform selection. Our work focuses ongenerating adaptive probabilities through iterative process, preferring tochoose coordinate with highest potential to decrease sub-optimality. We alsopropose a practical variant Algorithm adfSDCA+ which is more aggressive. Thework is concluded with multiple experiments which shows efficiency of proposedalgorithms.
arxiv-12900-13 | Efficient Unsupervised Temporal Segmentation of Motion Data | http://arxiv.org/pdf/1510.06595v1.pdf | author:Björn Krüger, Anna Vögele, Tobias Willig, Angela Yao, Reinhard Klein, Andreas Weber category:cs.CV published:2015-10-22 summary:We introduce a method for automated temporal segmentation of human motiondata into distinct actions and compositing motion primitives based onself-similar structures in the motion sequence. We use neighbourhood graphs forthe partitioning and the similarity information in the graph is furtherexploited to cluster the motion primitives into larger entities of semanticsignificance. The method requires no assumptions about the motion sequences athand and no user interaction is required for the segmentation or clustering. Inaddition, we introduce a feature bundling preprocessing technique to make thesegmentation more robust to noise, as well as a notion of motion symmetry formore refined primitive detection. We test our method on several sensormodalities, including markered and markerless motion capture as well as onelectromyograph and accelerometer recordings. The results highlight oursystem's capabilities for both segmentation and for analysis of the finerstructures of motion data, all in a completely unsupervised manner.
arxiv-12900-14 | Collective Prediction of Individual Mobility Traces with Exponential Weights | http://arxiv.org/pdf/1510.06582v1.pdf | author:Bartosz Hawelka, Izabela Sitko, Pavlos Kazakopoulos, Euro Beinat category:physics.soc-ph cs.CY cs.LG stat.ML published:2015-10-22 summary:We present and test a sequential learning algorithm for the short-termprediction of human mobility. This novel approach pairs the Exponential Weightsforecaster with a very large ensemble of experts. The experts are individualsequence prediction algorithms constructed from the mobility traces of 10million roaming mobile phone users in a European country. Average predictionaccuracy is significantly higher than that of individual sequence predictionalgorithms, namely constant order Markov models derived from the user's owndata, that have been shown to achieve high accuracy in previous studies ofhuman mobility prediction. The algorithm uses only time stamped location data,and accuracy depends on the completeness of the expert ensemble, which shouldcontain redundant records of typical mobility patterns. The proposed algorithmis applicable to the prediction of any sufficiently large dataset of sequences.
arxiv-12900-15 | Generalized conditional gradient: analysis of convergence and applications | http://arxiv.org/pdf/1510.06567v1.pdf | author:Alain Rakotomamonjy, Rémi Flamary, Nicolas Courty category:cs.LG math.OC stat.ML published:2015-10-22 summary:The objectives of this technical report is to provide additional results onthe generalized conditional gradient methods introduced by Bredies et al.[BLM05]. Indeed , when the objective function is smooth, we provide a novelcertificate of optimality and we show that the algorithm has a linearconvergence rate. Applications of this algorithm are also discussed.
arxiv-12900-16 | Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling | http://arxiv.org/pdf/1510.06549v1.pdf | author:Aaron Q Li category:cs.CL cs.DC cs.LG published:2015-10-22 summary:There is an explosion of data, documents, and other content, and peoplerequire tools to analyze and interpret these, tools to turn the content intoinformation and knowledge. Topic modeling have been developed to solve theseproblems. Topic models such as LDA [Blei et. al. 2003] allow salient patternsin data to be extracted automatically. When analyzing texts, these patterns arecalled topics. Among numerous extensions of LDA, few of them can reliablyanalyze multiple groups of documents and extract topic similarities. Recently,the introduction of differential topic modeling (SPDP) [Chen et. al. 2012]performs uniformly better than many topic models in a discriminative setting. There is also a need to improve the sampling speed for topic models. Whilesome effort has been made for distributed algorithms, there is no workcurrently done using graphical processing units (GPU). Note the GPU frameworkhas already become the most cost-efficient platform for many problems. In this thesis, I propose and implement a scalable multi-GPU distributedparallel framework which approximates SPDP. Through experiments, I have shownmy algorithms have a gain in speed of about 50 times while being almost asaccurate, with only one single cheap laptop GPU. Furthermore, I have shown thespeed improvement is sublinearly scalable when multiple GPUs are used, whilefairly maintaining the accuracy. Therefore on a medium-sized GPU cluster, thespeed improvement could potentially reach a factor of a thousand. Note SPDP is just a representative of other extensions of LDA. Although myalgorithm is implemented to work with SPDP, it is designed to be a generalenough to work with other topic models. The speed-up on smaller collections(i.e., 1000s of documents), means that these more complex LDA extensions couldnow be done in real-time, thus opening up a new way of using these LDA modelsin industry.
arxiv-12900-17 | Modelling, Measuring and Compensating Color Weak Vision | http://arxiv.org/pdf/1510.06507v1.pdf | author:Satoshi Oshima, Rica Mochizuki, Reiner Lenz, Jinhui Chao category:cs.CV published:2015-10-22 summary:We use methods from Riemann geometry to investigate transformations betweenthe color spaces of color-normal and color weak observers. The two mainapplications are the simulation of the perception of a color weak observer fora color normal observer and the compensation of color images in a way that acolor weak observer has approximately the same perception as a color normalobserver. The metrics in the color spaces of interest are characterized withthe help of ellipsoids defined by the just-noticable-differences between colorwhich are measured with the help of color-matching experiments. The constructedmappings are isometries of Riemann spaces that preserve the perceivedcolor-differences for both observers. Among the two approaches to build such anisometry, we introduce normal coordinates in Riemann spaces as a tool toconstruct a global color-weak compensation map. Compared to previously usedmethods this method is free from approximation errors due to locallinearizations and it avoids the problem of shifting locations of the origin ofthe local coordinate system. We analyse the variations of the Riemann metricsfor different observers obtained from new color matching experiments anddescribe three variations of the basic method. The performance of the methodsis evaluated with the help of semantic differential (SD) tests.
arxiv-12900-18 | Personalized Age Progression with Aging Dictionary | http://arxiv.org/pdf/1510.06503v1.pdf | author:Xiangbo Shu, Jinhui Tang, Hanjiang Lai, Luoqi Liu, Shuicheng Yan category:cs.CV published:2015-10-22 summary:In this paper, we aim to automatically render aging faces in a personalizedway. Basically, a set of age-group specific dictionaries are learned, where thedictionary bases corresponding to the same index yet from differentdictionaries form a particular aging process pattern cross different agegroups, and a linear combination of these patterns expresses a particularpersonalized aging process. Moreover, two factors are taken into considerationin the dictionary learning process. First, beyond the aging dictionaries, eachsubject may have extra personalized facial characteristics, e.g. mole, whichare invariant in the aging process. Second, it is challenging or evenimpossible to collect faces of all age groups for a particular subject, yetmuch easier and more practical to get face pairs from neighboring age groups.Thus a personality-aware coupled reconstruction loss is utilized to learn thedictionaries based on face pairs from neighboring age groups. Extensiveexperiments well demonstrate the advantages of our proposed solution over otherstate-of-the-arts in term of personalized aging progression, as well as theperformance gain for cross-age face verification by synthesizing aging faces.
arxiv-12900-19 | Generalized Shortest Path Kernel on Graphs | http://arxiv.org/pdf/1510.06492v1.pdf | author:Linus Hermansson, Fredrik D. Johansson, Osamu Watanabe category:cs.DS cs.LG published:2015-10-22 summary:We consider the problem of classifying graphs using graph kernels. We definea new graph kernel, called the generalized shortest path kernel, based on thenumber and length of shortest paths between nodes. For our exampleclassification problem, we consider the task of classifying random graphs fromtwo well-known families, by the number of clusters they contain. We verifyempirically that the generalized shortest path kernel outperforms the originalshortest path kernel on a number of datasets. We give a theoretical analysisfor explaining our experimental results. In particular, we estimatedistributions of the expected feature vectors for the shortest path kernel andthe generalized shortest path kernel, and we show some evidence explaining whyour graph kernel outperforms the shortest path kernel for our graphclassification problem.
arxiv-12900-20 | Filtering with State-Observation Examples via Kernel Monte Carlo Filter | http://arxiv.org/pdf/1312.4664v4.pdf | author:Motonobu Kanagawa, Yu Nishiyama, Arthur Gretton, Kenji Fukumizu category:stat.ML published:2013-12-17 summary:This paper addresses the problem of filtering with a state-space model.Standard approaches for filtering assume that a probabilistic model forobservations (i.e. the observation model) is given explicitly or at leastparametrically. We consider a setting where this assumption is not satisfied;we assume that the knowledge of the observation model is only provided byexamples of state-observation pairs. This setting is important and appears whenstate variables are defined as quantities that are very different from theobservations. We propose Kernel Monte Carlo Filter, a novel filtering methodthat is focused on this setting. Our approach is based on the framework ofkernel mean embeddings, which enables nonparametric posterior inference usingthe state-observation examples. The proposed method represents statedistributions as weighted samples, propagates these samples by sampling,estimates the state posteriors by Kernel Bayes' Rule, and resamples by KernelHerding. In particular, the sampling and resampling procedures are novel inbeing expressed using kernel mean embeddings, so we theoretically analyze theirbehaviors. We reveal the following properties, which are similar to those ofcorresponding procedures in particle methods: (1) the performance of samplingcan degrade if the effective sample size of a weighted sample is small; (2)resampling improves the sampling performance by increasing the effective samplesize. We first demonstrate these theoretical findings by synthetic experiments.Then we show the effectiveness of the proposed filter by artificial and realdata experiments, which include vision-based mobile robot localization.
arxiv-12900-21 | Scalable MCMC for Mixed Membership Stochastic Blockmodels | http://arxiv.org/pdf/1510.04815v2.pdf | author:Wenzhe Li, Sungjin Ahn, Max Welling category:cs.LG stat.ML published:2015-10-16 summary:We propose a stochastic gradient Markov chain Monte Carlo (SG-MCMC) algorithmfor scalable inference in mixed-membership stochastic blockmodels (MMSB). Ouralgorithm is based on the stochastic gradient Riemannian Langevin sampler andachieves both faster speed and higher accuracy at every iteration than thecurrent state-of-the-art algorithm based on stochastic variational inference.In addition we develop an approximation that can handle models that entertain avery large number of communities. The experimental results show that SG-MCMCstrictly dominates competing algorithms in all cases.
arxiv-12900-22 | Inventory Control Involving Unknown Demand of Discrete Nonperishable Items - Analysis of a Newsvendor-based Policy | http://arxiv.org/pdf/1510.06463v1.pdf | author:Michael N. Katehakis, Jian Yang, Tingting Zhou category:stat.ML published:2015-10-22 summary:Inventory control with unknown demand distribution is considered, withemphasis placed on the case involving discrete nonperishable items. We focus onan adaptive policy which in every period uses, as much as possible, the optimalnewsvendor ordering quantity for the empirical distribution learned up to thatperiod. The policy is assessed using the regret criterion, which measures theprice paid for ambiguity on demand distribution over $T$ periods. When thereare guarantees on the latter's separation from the critical newsvendorparameter $\beta=b/(h+b)$, a constant upper bound on regret can be found.Without any prior information on the demand distribution, we show that theregret does not grow faster than the rate $T^{1/2+\epsilon}$ for any$\epsilon>0$. In view of a known lower bound, this is almost the best one couldhope for. Simulation studies involving this along with other policies are alsoconducted.
arxiv-12900-23 | A Bounded $p$-norm Approximation of Max-Convolution for Sub-Quadratic Bayesian Inference on Additive Factors | http://arxiv.org/pdf/1505.07519v2.pdf | author:Julianus Pfeuffer, Oliver Serang category:stat.CO cs.NA stat.ML published:2015-05-28 summary:Max-convolution is an important problem closely resembling standardconvolution; as such, max-convolution occurs frequently across many fields.Here we extend the method with fastest known worst-case runtime, which can beapplied to nonnegative vectors by numerically approximating the Chebyshev norm$\ \cdot \_\infty$, and use this approach to derive two numerically stablemethods based on the idea of computing $p$-norms via fast convolution: Thefirst method proposed, with runtime in $O( k \log(k) \log(\log(k)) )$ (which isless than $18 k \log(k)$ for any vectors that can be practically realized),uses the $p$-norm as a direct approximation of the Chebyshev norm. The secondapproach proposed, with runtime in $O( k \log(k) )$ (although in practice bothperform similarly), uses a novel null space projection method, which extractsinformation from a sequence of $p$-norms to estimate the maximum value in thevector (this is equivalent to querying a small number of moments from adistribution of bounded support in order to estimate the maximum). The $p$-normapproaches are compared to one another and are shown to compute anapproximation of the Viterbi path in a hidden Markov model where the transitionmatrix is a Toeplitz matrix; the runtime of approximating the Viterbi path isthus reduced from $O( n k^2 )$ steps to $O( n $k \log(k))$ steps in practice,and is demonstrated by inferring the U.S. unemployment rate from the S&P 500stock index.
arxiv-12900-24 | Hidden Markov Models for Gene Sequence Classification: Classifying the VSG genes in the Trypanosoma brucei Genome | http://arxiv.org/pdf/1508.05367v2.pdf | author:Andrea Mesa, Sebastián Basterrech, Gustavo Guerberoff, Fernando Alvarez-Valin category:q-bio.GN cs.CE cs.LG published:2015-07-31 summary:The article presents an application of Hidden Markov Models (HMMs) forpattern recognition on genome sequences. We apply HMM for identifying genesencoding the Variant Surface Glycoprotein (VSG) in the genomes of Trypanosomabrucei (T. brucei) and other African trypanosomes. These are parasitic protozoacausative agents of sleeping sickness and several diseases in domestic and wildanimals. These parasites have a peculiar strategy to evade the host's immunesystem that consists in periodically changing their predominant cellularsurface protein (VSG). The motivation for using patterns recognition methods toidentify these genes, instead of traditional homology based ones, is that thelevels of sequence identity (amino acid and DNA sequence) amongst these genesis often below of what is considered reliable in these methods. Among patternrecognition approaches, HMM are particularly suitable to tackle this problembecause they can handle more naturally the determination of gene edges. Weevaluate the performance of the model using different number of states in theMarkov model, as well as several performance metrics. The model is appliedusing public genomic data. Our empirical results show that the VSG genes on T.brucei can be safely identified (high sensitivity and low rate of falsepositives) using HMM.
arxiv-12900-25 | Towards Direct Medical Image Analysis without Segmentation | http://arxiv.org/pdf/1510.06375v1.pdf | author:Xiantong Zhen, Shuo Li category:cs.CV published:2015-10-21 summary:Direct methods have recently emerged as an effective and efficient tool inautomated medical image analysis and become a trend to solve diversechallenging tasks in clinical practise. Compared to traditional methods, directmethods are of much more clinical significance by straightly targeting to thefinal clinical goal rather than relying on any intermediate steps. Theseintermediate steps, e.g., segmentation, registration and tracking, are actuallynot necessary and only limited to very constrained tasks far from being used inpractical clinical applications; moreover they are computationally expensiveand time-consuming, which causes a high waste of research resources. Theadvantages of direct methods stem from \textbf{1)} removal of intermediatesteps, e.g., segmentation, tracking and registration; \textbf{2)} avoidance ofuser inputs and initialization; \textbf{3)} reformulation of conventionalchallenging problems, e.g., inversion problem, with efficient solutions.
arxiv-12900-26 | Application of Quantum Annealing to Training of Deep Neural Networks | http://arxiv.org/pdf/1510.06356v1.pdf | author:Steven H. Adachi, Maxwell P. Henderson category:quant-ph cs.LG stat.ML published:2015-10-21 summary:In Deep Learning, a well-known approach for training a Deep Neural Networkstarts by training a generative Deep Belief Network model, typically usingContrastive Divergence (CD), then fine-tuning the weights using backpropagationor other discriminative techniques. However, the generative training can betime-consuming due to the slow mixing of Gibbs sampling. We investigated analternative approach that estimates model expectations of Restricted BoltzmannMachines using samples from a D-Wave quantum annealing machine. We tested thismethod on a coarse-grained version of the MNIST data set. In our tests we foundthat the quantum sampling-based training approach achieves comparable or betteraccuracy with significantly fewer iterations of generative training thanconventional CD-based training. Further investigation is needed to determinewhether similar improvements can be achieved for other data sets, and to whatextent these improvements can be attributed to quantum effects.
arxiv-12900-27 | Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues | http://arxiv.org/pdf/1510.05610v2.pdf | author:Nihar B. Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina, Martin J. Wainwright category:stat.ML cs.IT cs.LG math.IT published:2015-10-19 summary:There are various parametric models for analyzing pairwise comparison data,including the Bradley-Terry-Luce (BTL) and Thurstone models, but their relianceon strong parametric assumptions is limiting. In this work, we study a flexiblemodel for pairwise comparisons, under which the probabilities of outcomes arerequired only to satisfy a natural form of stochastic transitivity. This classincludes several parametric models including the BTL and Thurstone models asspecial cases, but is considerably more general. We provide various examples ofmodels in this broader stochastically transitive class for which classicalparametric models provide poor fits. Despite this greater flexibility, we showthat the matrix of probabilities can be estimated at the same rate as instandard parametric models. On the other hand, unlike in the BTL and Thurstonemodels, computing the minimax-optimal estimator in the stochasticallytransitive model is non-trivial, and we explore various computationallytractable alternatives. We show that a simple singular value thresholdingalgorithm is statistically consistent but does not achieve the minimax rate. Wethen propose and study algorithms that achieve the minimax rate overinteresting sub-classes of the full stochastically transitive class. Wecomplement our theoretical results with thorough numerical simulations.
arxiv-12900-28 | Computational and Statistical Boundaries for Submatrix Localization in a Large Noisy Matrix | http://arxiv.org/pdf/1502.01988v2.pdf | author:T. Tony Cai, Tengyuan Liang, Alexander Rakhlin category:math.ST stat.ML stat.TH published:2015-02-06 summary:The interplay between computational efficiency and statistical accuracy inhigh-dimensional inference has drawn increasing attention in the literature. Inthis paper, we study computational and statistical boundaries for submatrixlocalization. Given one observation of (one or multiple non-overlapping) signalsubmatrix (of magnitude $\lambda$ and size $k_m \times k_n$) contaminated witha noise matrix (of size $m \times n$), we establish two transition thresholdsfor the signal to noise $\lambda/\sigma$ ratio in terms of $m$, $n$, $k_m$, and$k_n$. The first threshold, $\sf SNR_c$, corresponds to the computationalboundary. Below this threshold, it is shown that no polynomial time algorithmcan succeed in identifying the submatrix, under the \textit{hidden cliquehypothesis}. We introduce adaptive linear time spectral algorithms thatidentify the submatrix with high probability when the signal strength is abovethe threshold $\sf SNR_c$. The second threshold, $\sf SNR_s$, captures thestatistical boundary, below which no method can succeed with probability goingto one in the minimax sense. The exhaustive search method successfully findsthe submatrix above this threshold. The results show an interesting phenomenonthat $\sf SNR_c$ is always significantly larger than $\sf SNR_s$, which impliesan essential gap between statistical optimality and computational efficiencyfor submatrix localization.
arxiv-12900-29 | Prevalence and recoverability of syntactic parameters in sparse distributed memories | http://arxiv.org/pdf/1510.06342v1.pdf | author:Jeong Joon Park, Ronnel Boettcher, Andrew Zhao, Alex Mun, Kevin Yuh, Vibhor Kumar, Matilde Marcolli category:cs.CL cs.IT math.IT 91F20 published:2015-10-21 summary:We propose a new method, based on Sparse Distributed Memory (KanervaNetworks), for studying dependency relations between different syntacticparameters in the Principles and Parameters model of Syntax. We store data ofsyntactic parameters of world languages in a Kanerva Network and we check therecoverability of corrupted parameter data from the network. We find thatdifferent syntactic parameters have different degrees of recoverability. Weidentify two different effects: an overall underlying relation between theprevalence of parameters across languages and their degree of recoverability,and a finer effect that makes some parameters more easily recoverable beyondwhat their prevalence would indicate. We interpret a higher recoverability fora syntactic parameter as an indication of the existence of a dependencyrelation, through which the given parameter can be determined using theremaining uncorrupted data.
arxiv-12900-30 | GLASSES: Relieving The Myopia Of Bayesian Optimisation | http://arxiv.org/pdf/1510.06299v1.pdf | author:Javier González, Michael Osborne, Neil D. Lawrence category:stat.ML published:2015-10-21 summary:We present GLASSES: Global optimisation with Look-Ahead through StochasticSimulation and Expected-loss Search. The majority of global optimisationapproaches in use are myopic, in only considering the impact of the nextfunction value; the non-myopic approaches that do exist are able to consideronly a handful of future evaluations. Our novel algorithm, GLASSES, permits theconsideration of dozens of evaluations into the future. This is done byapproximating the ideal look-ahead loss function, which is expensive toevaluate, by a cheaper alternative in which the future steps of the algorithmare simulated beforehand. An Expectation Propagation algorithm is used tocompute the expected value of the loss.We show that the far-horizon planningthus enabled leads to substantive performance gains in empirical tests.
arxiv-12900-31 | Similarity Learning for High-Dimensional Sparse Data | http://arxiv.org/pdf/1411.2374v2.pdf | author:Kuan Liu, Aurélien Bellet, Fei Sha category:cs.LG stat.ML published:2014-11-10 summary:A good measure of similarity between data points is crucial to many tasks inmachine learning. Similarity and metric learning methods learn such measuresautomatically from data, but they do not scale well respect to thedimensionality of the data. In this paper, we propose a method that can learnefficiently similarity measure from high-dimensional sparse data. The core ideais to parameterize the similarity measure as a convex combination of rank-onematrices with specific sparsity structures. The parameters are then optimizedwith an approximate Frank-Wolfe procedure to maximally satisfy relativesimilarity constraints on the training data. Our algorithm greedilyincorporates one pair of features at a time into the similarity measure,providing an efficient way to control the number of active features and thusreduce overfitting. It enjoys very appealing convergence guarantees and itstime and memory complexity depends on the sparsity of the data instead of thedimension of the feature space. Our experiments on real-world high-dimensionaldatasets demonstrate its potential for classification, dimensionality reductionand data exploration.
arxiv-12900-32 | Elastic regularization in restricted Boltzmann machines: Dealing with $p\gg N$ | http://arxiv.org/pdf/1510.03623v2.pdf | author:Sai Zhang category:cs.LG published:2015-10-13 summary:Restricted Boltzmann machines (RBMs) are endowed with the universal power ofmodeling (binary) joint distributions. Meanwhile, as a result of theirconfining network structure, training RBMs confronts less difficulties(compared with more complicated models, e.g., Boltzmann machines) when dealingwith approximation and inference issues. However, in certain computationalbiology scenarios, such as the cancer data analysis, employing RBMs to modeldata features may lose its efficacy due to the "$p\gg N$" problem, in which thenumber of features/predictors is much larger than the sample size. The "$p\ggN$" problem puts the bias-variance trade-off in a more crucial place whendesigning statistical learning methods. In this manuscript, we try to addressthis problem by proposing a novel RBM model, called elastic restrictedBoltzmann machine (eRBM), which incorporates the elastic regularization terminto the likelihood/cost function. We provide several theoretical analysis onthe superiority of our model. Furthermore, attributed to the classiccontrastive divergence (CD) algorithm, eRBMs can be trained efficiently. Ournovel model is a promising method for future cancer data analysis.
arxiv-12900-33 | Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network | http://arxiv.org/pdf/1510.06168v1.pdf | author:Peilu Wang, Yao Qian, Frank K. Soong, Lei He, Hai Zhao category:cs.CL published:2015-10-21 summary:Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) hasbeen shown to be very effective for tagging sequential data, e.g. speechutterances or handwritten documents. While word embedding has been demoed as apowerful representation for characterizing the statistical properties ofnatural language. In this study, we propose to use BLSTM-RNN with wordembedding for part-of-speech (POS) tagging task. When tested on Penn TreebankWSJ test set, a state-of-the-art performance of 97.40 tagging accuracy isachieved. Without using morphological features, this approach can also achievea good performance comparable with the Stanford POS tagger.
arxiv-12900-34 | Interactive Volumetry Of Liver Ablation Zones | http://arxiv.org/pdf/1512.04582v1.pdf | author:Jan Egger, Harald Busse, Philipp Brandmaier, Daniel Seider, Matthias Gawlitza, Steffen Strocka, Philip Voglreiter, Mark Dokter, Michael Hofmann, Bernhard Kainz, Alexander Hann, Xiaojun Chen, Tuomas Alhonnoro, Mika Pollari, Dieter Schmalstieg, Michael Moche category:cs.CV cs.GR cs.HC physics.med-ph published:2015-10-21 summary:Percutaneous radiofrequency ablation (RFA) is a minimally invasive techniquethat destroys cancer cells by heat. The heat results from focusing energy inthe radiofrequency spectrum through a needle. Amongst others, this can enablethe treatment of patients who are not eligible for an open surgery. However,the possibility of recurrent liver cancer due to incomplete ablation of thetumor makes post-interventional monitoring via regular follow-up scansmandatory. These scans have to be carefully inspected for any conspicuousness.Within this study, the RF ablation zones from twelve post-interventional CTacquisitions have been segmented semi-automatically to support the visualinspection. An interactive, graph-based contouring approach, which prefersspherically shaped regions, has been applied. For the quantitative andqualitative analysis of the algorithm's results, manual slice-by-slicesegmentations produced by clinical experts have been used as the gold standard(which have also been compared among each other). As evaluation metric for thestatistical validation, the Dice Similarity Coefficient (DSC) has beencalculated. The results show that the proposed tool provides lesionsegmentation with sufficient accuracy much faster than manual segmentation. Thevisual feedback and interactivity make the proposed tool well suitable for theclinical workflow.
arxiv-12900-35 | Constructing Dynamic Treatment Regimes in Infinite-Horizon Settings | http://arxiv.org/pdf/1406.0764v2.pdf | author:Ashkan Ertefaie category:stat.ME stat.ML published:2014-06-03 summary:The application of existing methods for constructing optimal dynamictreatment regimes is limited to cases where investigators are interested inoptimizing a utility function over a fixed period of time (finite horizon). Inthis manuscript, we develop an inferential procedure based on temporaldifference residuals for optimal dynamic treatment regimes in infinite-horizonsettings, where there is no a priori fixed end of follow-up point. The proposedmethod can be used to determine the optimal regime in chronic diseases wherepatients are monitored and treated throughout their life. We derive largesample results necessary for conducting inference. We also simulate a cohort ofpatients with diabetes to mimic the third wave of the National Health andNutrition Examination Survey, and we examine the performance of the proposedmethod in controlling the level of hemoglobin A1c. Supplementary materials forthis article are available online.
arxiv-12900-36 | Multiple co-clustering based on nonparametric mixture models with heterogeneous marginal distributions | http://arxiv.org/pdf/1510.06138v1.pdf | author:Tomoki Tokuda, Junichiro Yoshimoto, Yu Shimizu, Shigeru Toki, Go Okada, Masahiro Takamura, Tetsuya Yamamoto, Shinpei Yoshimura, Yasumasa Okamoto, Shigeto Yamawaki, Kenji Doya category:stat.ML published:2015-10-21 summary:We propose a novel method for multiple clustering that assumes aco-clustering structure (partitions in both rows and columns of the datamatrix) in each view. The new method is applicable to high-dimensional data. Itis based on a nonparametric Bayesian approach in which the number of views andthe number of feature-/subject clusters are inferred in a data-driven manner.We simultaneously model different distribution families, such as Gaussian,Poisson, and multinomial distributions in each cluster block. This makes ourmethod applicable to datasets consisting of both numerical and categoricalvariables, which biomedical data typically do. Clustering solutions are basedon variational inference with mean field approximation. We apply the proposedmethod to synthetic and real data, and show that our method outperforms othermultiple clustering methods both in recovering true cluster structures and incomputation time. Finally, we apply our method to a depression dataset with notrue cluster structure available, from which useful inferences are drawn aboutpossible clustering structures of the data.
arxiv-12900-37 | Dimensionality Reduction for Binary Data through the Projection of Natural Parameters | http://arxiv.org/pdf/1510.06112v1.pdf | author:Andrew J. Landgraf, Yoonkyung Lee category:stat.ML stat.ME published:2015-10-21 summary:Principal component analysis (PCA) for binary data, known as logistic PCA,has become a popular alternative to dimensionality reduction of binary data. Itis motivated as an extension of ordinary PCA by means of a matrixfactorization, akin to the singular value decomposition, that maximizes theBernoulli log-likelihood. We propose a new formulation of logistic PCA whichextends Pearson's formulation of a low dimensional data representation withminimum error to binary data. Our formulation does not require a matrixfactorization, as previous methods do, but instead looks for projections of thenatural parameters from the saturated model. Due to this difference, the numberof parameters does not grow with the number of observations and the principalcomponent scores on new data can be computed with simple matrix multiplication.We derive explicit solutions for data matrices of special structure and providecomputationally efficient algorithms for solving for the principal componentloadings. Through simulation experiments and an analysis of medical diagnosesdata, we compare our formulation of logistic PCA to the previous formulation aswell as ordinary PCA to demonstrate its benefits.
arxiv-12900-38 | Content adaptive screen image scaling | http://arxiv.org/pdf/1510.06093v1.pdf | author:Yao Zhai, Qifei Wang, Yan Lu, Shipeng Li category:cs.CV published:2015-10-21 summary:This paper proposes an efficient content adaptive screen image scaling schemefor the real-time screen applications like remote desktop and screen sharing.In the proposed screen scaling scheme, a screen content classification step isfirst introduced to classify the screen image into text and pictorial regions.Afterward, we propose an adaptive shift linear interpolation algorithm topredict the new pixel values with the shift offset adapted to the content typeof each pixel. The shift offset for each screen content type is offlineoptimized by minimizing the theoretical interpolation error based on thetraining samples respectively. The proposed content adaptive screen imagescaling scheme can achieve good visual quality and also keep the low complexityfor real-time applications.
arxiv-12900-39 | Regularization vs. Relaxation: A conic optimization perspective of statistical variable selection | http://arxiv.org/pdf/1510.06083v1.pdf | author:Hongbo Dong, Kun Chen, Jeff Linderoth category:cs.LG math.NA math.OC stat.ML G.1.3; G.1.6 published:2015-10-20 summary:Variable selection is a fundamental task in statistical data analysis.Sparsity-inducing regularization methods are a popular class of methods thatsimultaneously perform variable selection and model estimation. The centralproblem is a quadratic optimization problem with an l0-norm penalty. Exactlyenforcing the l0-norm penalty is computationally intractable for larger scaleproblems, so dif- ferent sparsity-inducing penalty functions that approximatethe l0-norm have been introduced. In this paper, we show that viewing theproblem from a convex relaxation perspective offers new insights. Inparticular, we show that a popular sparsity-inducing concave penalty functionknown as the Minimax Concave Penalty (MCP), and the reverse Huber penaltyderived in a recent work by Pilanci, Wainwright and Ghaoui, can both be derivedas special cases of a lifted convex relaxation called the perspectiverelaxation. The optimal perspective relaxation is a related minimax problemthat balances the overall convexity and tightness of approximation to the l0norm. We show it can be solved by a semidefinite relaxation. Moreover, aprobabilistic interpretation of the semidefinite relaxation reveals connectionswith the boolean quadric polytope in combinatorial optimization. Finally byreformulating the l0-norm pe- nalized problem as a two-level problem, with theinner level being a Max-Cut problem, our proposed semidefinite relaxation canbe realized by replacing the inner level problem with its semidefiniterelaxation studied by Goemans and Williamson. This interpretation suggestsusing the Goemans-Williamson rounding procedure to find approximate solutionsto the l0-norm penalized problem. Numerical experiments demonstrate thetightness of our proposed semidefinite relaxation, and the effectiveness offinding approximate solutions by Goemans-Williamson rounding.
arxiv-12900-40 | Discovery Radiomics for Computed Tomography Cancer Detection | http://arxiv.org/pdf/1509.00117v2.pdf | author:Devinder Kumar, Mohammad Javad Shafiee, Audrey G. Chung, Farzad Khalvati, Masoom A. Haider, Alexander Wong category:cs.CV published:2015-09-01 summary:Objective: Lung cancer is the leading cause for cancer related deaths. Assuch, there is an urgent need for a streamlined process that can allowradiologists to provide diagnosis with greater efficiency and accuracy. Apowerful tool to do this is radiomics. Method: In this study, we take the ideaof radiomics one step further by introducing the concept of discovery radiomicsfor lung cancer detection using CT imaging data. Rather than using pre-defined,hand-engineered feature models as with current radiomics-driven methods, wediscover custom radiomic sequencers that can generate radiomic sequencesconsisting of abstract imaging-based features tailored for characterizing lungtumour phenotype. In this study, we realize these custom radiomic sequencers asdeep convolutional sequencers using a deep convolutional neural networklearning architecture based on a wealth of CT imaging data. Results: Toillustrate the prognostic power and effectiveness of the radiomic sequencesproduced by the discovered sequencer, we perform a classification betweenmalignant and benign lesions from 93 patients with diagnostic data from theLIDC-IDRI dataset. Using the clinically provided diagnostic data as groundtruth, proposed framework provided an average accuracy of 77.52% via 10-foldcross-validation with a sensitivity of 79.06% and specificity of 76.11%. Wealso perform quantitative analysis to establish the effectiveness of theradiomics sequences. Conclusion: The proposed framework outperforms thestate-of-the art approach for lung lesion classification. Significance: Theseresults illustrate the potential for the proposed discovery radiomics approachin aiding radiologists in improving screening efficiency and accuracy.
arxiv-12900-41 | A latent shared-component generative model for real-time disease surveillance using Twitter data | http://arxiv.org/pdf/1510.05981v1.pdf | author:Roberto C. S. N. P. Souza, Denise E. F de Brito, Renato M. Assunção, Wagner Meira Jr category:cs.SI stat.ML published:2015-10-20 summary:Exploiting the large amount of available data for addressing relevant socialproblems has been one of the key challenges in data mining. Such efforts havebeen recently named "data science for social good" and attracted the attentionof several researchers and institutions. We give a contribution in thisobjective in this paper considering a difficult public health problem, thetimely monitoring of dengue epidemics in small geographical areas. We develop agenerative simple yet effective model to connect the fluctuations of diseasecases and disease-related Twitter posts. We considered a hidden Markov processdriving both, the fluctuations in dengue reported cases and the tweets issuedin each region. We add a stable but random source of tweets to represent theposts when no disease cases are recorded. The model is learned through a Markovchain Monte Carlo algorithm that produces the posterior distribution of therelevant parameters. Using data from a significant number of large Braziliantowns, we demonstrate empirically that our model is able to predict well thenext weeks of the disease counts using the tweets and disease cases jointly.
arxiv-12900-42 | Transductive Optimization of Top k Precision | http://arxiv.org/pdf/1510.05976v1.pdf | author:Li-Ping Liu, Thomas G. Dietterich, Nan Li, Zhi-Hua Zhou category:cs.LG published:2015-10-20 summary:Consider a binary classification problem in which the learner is given alabeled training set, an unlabeled test set, and is restricted to choosingexactly $k$ test points to output as positive predictions. Problems of thiskind---{\it transductive precision@$k$}---arise in information retrieval,digital advertising, and reserve design for endangered species. Previousmethods separate the training of the model from its use in scoring the testpoints. This paper introduces a new approach, Transductive Top K (TTK), thatseeks to minimize the hinge loss over all training instances under theconstraint that exactly $k$ test instances are predicted as positive. The paperpresents two optimization methods for this challenging problem. Experiments andanalysis confirm the importance of incorporating the knowledge of $k$ into thelearning process. Experimental evaluations of the TTK approach show that theperformance of TTK matches or exceeds existing state-of-the-art methods on 7UCI datasets and 3 reserve design problem instances.
arxiv-12900-43 | Computing the Stereo Matching Cost with a Convolutional Neural Network | http://arxiv.org/pdf/1409.4326v2.pdf | author:Jure Žbontar, Yann LeCun category:cs.CV cs.LG cs.NE published:2014-09-15 summary:We present a method for extracting depth information from a rectified imagepair. We train a convolutional neural network to predict how well two imagepatches match and use it to compute the stereo matching cost. The cost isrefined by cross-based cost aggregation and semiglobal matching, followed by aleft-right consistency check to eliminate errors in the occluded regions. Ourstereo method achieves an error rate of 2.61 % on the KITTI stereo dataset andis currently (August 2014) the top performing method on this dataset.
arxiv-12900-44 | Fast and Guaranteed Tensor Decomposition via Sketching | http://arxiv.org/pdf/1506.04448v2.pdf | author:Yining Wang, Hsiao-Yu Tung, Alexander Smola, Animashree Anandkumar category:stat.ML cs.LG published:2015-06-14 summary:Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications instatistical learning of latent variable models and in data mining. In thispaper, we propose fast and randomized tensor CP decomposition algorithms basedon sketching. We build on the idea of count sketches, but introduce many novelideas which are unique to tensors. We develop novel methods for randomizedcomputation of tensor contractions via FFTs, without explicitly forming thetensors. Such tensor contractions are encountered in decomposition methods suchas tensor power iterations and alternating least squares. We also design novelcolliding hashes for symmetric tensors to further save time in computing thesketches. We then combine these sketching ideas with existing whitening andtensor power iterative techniques to obtain the fastest algorithm on bothsparse and dense tensors. The quality of approximation under our method doesnot depend on properties such as sparsity, uniformity of elements, etc. Weapply the method for topic modeling and obtain competitive results.
arxiv-12900-45 | Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation | http://arxiv.org/pdf/1508.05151v2.pdf | author:Christian Bailer, Bertram Taetz, Didier Stricker category:cs.CV I.4.8 published:2015-08-21 summary:Modern large displacement optical flow algorithms usually use aninitialization by either sparse descriptor matching techniques or denseapproximate nearest neighbor fields. While the latter have the advantage ofbeing dense, they have the major disadvantage of being very outlier prone asthey are not designed to find the optical flow, but the visually most similarcorrespondence. In this paper we present a dense correspondence field approachthat is much less outlier prone and thus much better suited for optical flowestimation than approximate nearest neighbor fields. Our approach isconceptually novel as it does not require explicit regularization, smoothing(like median filtering) or a new data term, but solely our novel purely databased search strategy that finds most inliers (even for small objects), whileit effectively avoids finding outliers. Moreover, we present novel enhancementsfor outlier filtering. We show that our approach is better suited for largedisplacement optical flow estimation than state-of-the-art descriptor matchingtechniques. We do so by initializing EpicFlow (so far the best method onMPI-Sintel) with our Flow Fields instead of their originally usedstate-of-the-art descriptor matching technique. We significantly outperform theoriginal EpicFlow on MPI-Sintel, KITTI and Middlebury.
arxiv-12900-46 | What's the point? Frame-wise Pointing Gesture Recognition with Latent-Dynamic Conditional Random Fields | http://arxiv.org/pdf/1510.05879v1.pdf | author:Christian Wittner, Boris Schauerte, Rainer Stiefelhagen category:cs.HC cs.CV cs.RO published:2015-10-20 summary:We use Latent-Dynamic Conditional Random Fields to perform skeleton-basedpointing gesture classification at each time instance of a video sequence,where we achieve a frame-wise pointing accuracy of roughly 83%. Subsequently,we determine continuous time sequences of arbitrary length that form individualpointing gestures and this way reliably detect pointing gestures at a falsepositive detection rate of 0.63%.
arxiv-12900-47 | Partition MCMC for inference on acyclic digraphs | http://arxiv.org/pdf/1504.05006v2.pdf | author:Jack Kuipers, Giusi Moffa category:stat.ML published:2015-04-20 summary:Acyclic digraphs are the underlying representation of Bayesian networks, awidely used class of probabilistic graphical models. Learning the underlyinggraph from data is a way of gaining insights about the structural properties ofa domain. Structure learning forms one of the inference challenges ofstatistical graphical models. MCMC methods, notably structure MCMC, to sample graphs from the posteriordistribution given the data are probably the only viable option for Bayesianmodel averaging. Score modularity and restrictions on the number of parents ofeach node allow the graphs to be grouped into larger collections, which can bescored as a whole to improve the chain's convergence. Current examples ofalgorithms taking advantage of grouping are the biased order MCMC, which actson the alternative space of permuted triangular matrices, and non ergodic edgereversal moves. Here we propose a novel algorithm, which employs the underlying combinatorialstructure of DAGs to define a new grouping. As a result convergence is improvedcompared to structure MCMC, while still retaining the property of producing anunbiased sample. Finally the method can be combined with edge reversal moves toimprove the sampler further.
arxiv-12900-48 | Sequential Score Adaptation with Extreme Value Theory for Robust Railway Track Inspection | http://arxiv.org/pdf/1510.05822v1.pdf | author:Xavier Gibert, Vishal M. Patel, Rama Chellappa category:cs.CV published:2015-10-20 summary:Periodic inspections are necessary to keep railroad tracks in state of goodrepair and prevent train accidents. Automatic track inspection using machinevision technology has become a very effective inspection tool. Because of itsnon-contact nature, this technology can be deployed on virtually any railwayvehicle to continuously survey the tracks and send exception reports to trackmaintenance personnel. However, as appearance and imaging conditions vary,false alarm rates can dramatically change, making it difficult to select a goodoperating point. In this paper, we use extreme value theory (EVT) within aBayesian framework to optimally adjust the sensitivity of anomaly detectors. Weshow that by approximating the lower tail of the probability density function(PDF) of the scores with an Exponential distribution (a special case of theGeneralized Pareto distribution), and using the Gamma conjugate prior learnedfrom the training data, it is possible to reduce the variability in false alarmrate and improve the overall performance. This method has shown an increase inthe defect detection rate of rail fasteners in the presence of clutter (at PFA0.1%) from 95.40% to 99.26% on the 85-mile Northeast Corridor (NEC) 2012-2013concrete tie dataset.
arxiv-12900-49 | Towards stability and optimality in stochastic gradient descent | http://arxiv.org/pdf/1505.02417v2.pdf | author:Panos Toulis, Dustin Tran, Edoardo M. Airoldi category:stat.ME cs.LG stat.CO stat.ML published:2015-05-10 summary:Iterative procedures for parameter estimation based on stochastic gradientdescent allow the estimation to scale to massive data sets. However, in boththeory and practice, they suffer from numerical instability. Moreover, they arestatistically inefficient as estimators of the true parameter value. To addressthese two issues, we propose a new iterative procedure termed averaged implicitstochastic gradient descent (AI-SGD). For statistical efficiency, AISGD employsaveraging of the iterates, which achieves the optimal Cram\'{e}r-Rao boundunder strong convexity, i.e., it is an optimal unbiased estimator of the trueparameter value. For numerical stability, AISGD employs an implicit update ateach iteration, which is related to proximal operators in optimization. Inpractice, AISGD achieves competitive performance with state-of-the-artprocedures. Furthermore, it is more stable than averaging procedures that donot employ proximal operators, and is simpler to implement than procedures thatdo employ proximal operators but require careful tuning of severalhyperparameters.
arxiv-12900-50 | Discovery Radiomics for Multi-Parametric MRI Prostate Cancer Detection | http://arxiv.org/pdf/1509.00111v3.pdf | author:Audrey G. Chung, Mohammad Javad Shafiee, Devinder Kumar, Farzad Khalvati, Masoom A. Haider, Alexander Wong category:cs.CV physics.med-ph q-bio.QM published:2015-09-01 summary:Prostate cancer is the most diagnosed form of cancer in Canadian men, and isthe third leading cause of cancer death. Despite these statistics, prognosis isrelatively good with a sufficiently early diagnosis, making fast and reliableprostate cancer detection crucial. As imaging-based prostate cancer screening,such as magnetic resonance imaging (MRI), requires an experienced medicalprofessional to extensively review the data and perform a diagnosis,radiomics-driven methods help streamline the process and has the potential tosignificantly improve diagnostic accuracy and efficiency, and thus improvingpatient survival rates. These radiomics-driven methods currently rely onhand-crafted sets of quantitative imaging-based features, which are selectedmanually and can limit their ability to fully characterize unique prostatecancer tumour phenotype. In this study, we propose a novel \textit{discoveryradiomics} framework for generating custom radiomic sequences tailored forprostate cancer detection. Discovery radiomics aims to uncover abstractimaging-based features that capture highly unique tumour traits andcharacteristics beyond what can be captured using predefined feature models. Inthis paper, we discover new custom radiomic sequencers for generating newprostate radiomic sequences using multi-parametric MRI data. We evaluated theperformance of the discovered radiomic sequencer against a state-of-the-arthand-crafted radiomic sequencer for computer-aided prostate cancer detectionwith a feedforward neural network using real clinical prostate multi-parametricMRI data. Results for the discovered radiomic sequencer demonstrate goodperformance in prostate cancer detection and clinical decision support relativeto the hand-crafted radiomic sequencer. The use of discovery radiomics showspotential for more efficient and reliable automatic prostate cancer detection.
arxiv-12900-51 | Robust Semi-Supervised Classification for Multi-Relational Graphs | http://arxiv.org/pdf/1510.06024v1.pdf | author:Junting Ye, Leman Akoglu category:cs.LG published:2015-10-19 summary:Graph-regularized semi-supervised learning has been used effectively forclassification when (i) instances are connected through a graph, and (ii)labeled data is scarce. If available, using multiple relations (or graphs)between the instances can improve the prediction performance. On the otherhand, when these relations have varying levels of veracity and exhibit varyingrelevance for the task, very noisy and/or irrelevant relations may deterioratethe performance. As a result, an effective weighing scheme needs to be put inplace. In this work, we propose a robust and scalable approach formulti-relational graph-regularized semi-supervised classification. Under aconvex optimization scheme, we simultaneously infer weights for the multiplegraphs as well as a solution. We provide a careful analysis of the inferredweights, based on which we devise an algorithm that filters out irrelevant andnoisy graphs and produces weights proportional to the informativeness of theremaining graphs. Moreover, the proposed method is linearly scalable w.r.t. thenumber of edges in the union of the multiple graphs. Through extensiveexperiments we show that our method yields superior results under differentnoise models, and under increasing number of noisy graphs and intensity ofnoise, as compared to a list of baselines and state-of-the-art approaches.
arxiv-12900-52 | Single Memristor Logic Gates: From NOT to a Full Adder | http://arxiv.org/pdf/1510.05705v1.pdf | author:Ella Gale category:cs.ET cs.NE 03B-02, 68U02 published:2015-10-19 summary:Memristors have been suggested as a novel route to neuromorphic computingbased on the similarity between them and neurons (specifically synapses and ionpumps). The d.c. action of the memristor is a current spike which imparts ashort-term memory to the device. Here it is demonstrated that this short-termmemory works exactly like habituation (e.g. in \emph{Aplysia}). We elucidatethe physical rules, based on energy conservation, governing the interaction ofthese current spikes: summation, `bounce-back', directionality and `diminishingreturns'. Using these rules, we introduce 4 different logical systems toimplement sequential logic in the memristor and demonstrate how sequentiallogic works by instantiating a NOT gate, an AND gate, an XOR gate and a FullAdder with a single memristor. The Full Adder makes use of the memristor'sshort-term memory to add together three binary values and outputs the sum, thecarry digit and even the order they were input in. A memristor full adder alsooutputs the arithmetical sum of bits, allowing for a logically (but notphysically) reversible system. Essentially, we can replace an input/output portwith an extra time-step, allowing a single memristor to do a hither-tounexpectedly large amount of computation. This makes up for the memristor'sslow operation speed and may relate to how neurons do a similarly-largecomputation with such slow operations speeds. We propose that using spikinglogic, either in gates or as neuron-analogues, with plastic rewritableconnections between them, would allow the building of a neuromorphic computer.
arxiv-12900-53 | When slower is faster | http://arxiv.org/pdf/1506.06796v2.pdf | author:Carlos Gershenson, Dirk Helbing category:nlin.AO cs.NE physics.soc-ph q-bio.QM published:2015-06-22 summary:The slower is faster (SIF) effect occurs when a system performs worse as itscomponents try to do better. Thus, a moderate individual efficiency actuallyleads to a better systemic performance. The SIF effect takes place in a varietyof phenomena. We review studies and examples of the SIF effect in pedestriandynamics, vehicle traffic, traffic light control, logistics, public transport,social dynamics, ecological systems, and adaptation. Drawing on these examples,we generalize common features of the SIF effect and suggest possible futurelines of research.
arxiv-12900-54 | NYTRO: When Subsampling Meets Early Stopping | http://arxiv.org/pdf/1510.05684v1.pdf | author:Tomas Angles, Raffaello Camoriano, Alessandro Rudi, Lorenzo Rosasco category:stat.ML published:2015-10-19 summary:Early stopping is a well known approach to reduce the time complexity forperforming training and model selection of large scale learning machines. Onthe other hand, memory/space (rather than time) complexity is the mainconstraint in many applications, and randomized subsampling techniques havebeen proposed to tackle this issue. In this paper we ask whether early stoppingand subsampling ideas can be combined in a fruitful way. We consider thequestion in a least squares regression setting and propose a form of randomizediterative regularization based on early stopping and subsampling. In thiscontext, we analyze the statistical and computational properties of theproposed method. Theoretical results are complemented and validated by athorough experimental analysis.
arxiv-12900-55 | Protein Structure Prediction by Protein Alignments | http://arxiv.org/pdf/1510.05682v1.pdf | author:Jianzhu Ma category:cs.CE cs.LG q-bio.BM published:2015-10-19 summary:Proteins are the basic building blocks of life. They usually performfunctions by folding to a particular structure. Understanding the foldingprocess could help the researchers to understand the functions of proteins andcould also help to develop supplemental proteins for people with deficienciesand gain more insight into diseases associated with troublesome foldingproteins. Experimental methods are both expensive and time consuming. In thisthesis I introduce a new machine learning based method to predict the proteinstructure. The new method improves the performance from two directions:creating accurate protein alignments and predicting accurate protein contacts.First, I present an alignment framework MRFalign which goes beyondstate-of-the-art methods and uses Markov Random Fields to model a proteinfamily and align two proteins by aligning two MRFs together. Compared to othermethods, that can only model local-range residue correlation, MRFs can modellong-range residue interactions and thus, encodes global information in aprotein. Secondly, I present a Group Graphical Lasso method for contactprediction that integrates joint multi-family Evolutionary Coupling analysisand supervised learning to improve accuracy on proteins without many sequencehomologs. Different from single-family EC analysis that uses residueco-evolution information in only the target protein family, our joint ECanalysis uses residue co-evolution in both the target family and its relatedfamilies, which may have divergent sequences but similar folds. Our method canalso integrate supervised learning methods to further improve accuracy. Weevaluate the performance of both methods including each of its components onlarge public benchmarks. Experiments show that our methods can achieve betteraccuracy than existing state-of-the-art methods under all the measurements onmost of the protein classes.
arxiv-12900-56 | SGD with Variance Reduction beyond Empirical Risk Minimization | http://arxiv.org/pdf/1510.04822v2.pdf | author:Massil Achab, Agathe Guilloux, Stéphane Gaïffas, Emmanuel Bacry category:stat.ML cs.LG published:2015-10-16 summary:We introduce a doubly stochastic proximal gradient algorithm for optimizing afinite average of smooth convex functions, whose gradients depend onnumerically expensive expectations. Our main motivation is the acceleration ofthe optimization of the regularized Cox partial-likelihood (the core model usedin survival analysis), but our algorithm can be used in different settings aswell. The proposed algorithm is doubly stochastic in the sense that gradientsteps are done using stochastic gradient descent (SGD) with variance reduction,where the inner expectations are approximated by a Monte-Carlo Markov-Chain(MCMC) algorithm. We derive conditions on the MCMC number of iterationsguaranteeing convergence, and obtain a linear rate of convergence under strongconvexity and a sublinear rate without this assumption. We illustrate the factthat our algorithm improves the state-of-the-art solver for regularized Coxpartial-likelihood on several datasets from survival analysis.
arxiv-12900-57 | Global and Local Structure Preserving Sparse Subspace Learning: An Iterative Approach to Unsupervised Feature Selection | http://arxiv.org/pdf/1506.01060v2.pdf | author:Nan Zhou, Yangyang Xu, Hong Cheng, Jun Fang, Witold Pedrycz category:cs.LG 05-04 E.0 published:2015-06-02 summary:As we aim at alleviating the curse of high-dimensionality, subspace learningis becoming more popular. Existing approaches use either information aboutglobal or local structure of the data, and few studies simultaneously focus onglobal and local structures as the both of them contain important information.In this paper, we propose a global and local structure preserving sparsesubspace learning (GLoSS) model for unsupervised feature selection. The modelcan simultaneously realize feature selection and subspace learning. Inaddition, we develop a greedy algorithm to establish a generic combinatorialmodel, and an iterative strategy based on an accelerated block coordinatedescent is used to solve the GLoSS problem. We also provide whole iteratesequence convergence analysis of the proposed iterative algorithm. Extensiveexperiments are conducted on real-world datasets to show the superiority of theproposed approach over several state-of-the-art unsupervised feature selectionapproaches.
arxiv-12900-58 | Sequence to Sequence -- Video to Text | http://arxiv.org/pdf/1505.00487v3.pdf | author:Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko category:cs.CV published:2015-05-03 summary:Real-world videos often have complex dynamics; and methods for generatingopen-domain video descriptions should be sensitive to temporal structure andallow both input (sequence of frames) and output (sequence of words) ofvariable length. To approach this problem, we propose a novel end-to-endsequence-to-sequence model to generate captions for videos. For this we exploitrecurrent neural networks, specifically LSTMs, which have demonstratedstate-of-the-art performance in image caption generation. Our LSTM model istrained on video-sentence pairs and learns to associate a sequence of videoframes to a sequence of words in order to generate a description of the eventin the video clip. Our model naturally is able to learn the temporal structureof the sequence of frames as well as the sequence model of the generatedsentences, i.e. a language model. We evaluate several variants of our modelthat exploit different visual features on a standard set of YouTube videos andtwo movie description datasets (M-VAD and MPII-MD).
arxiv-12900-59 | Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits | http://arxiv.org/pdf/1504.06937v3.pdf | author:Huasen Wu, R. Srikant, Xin Liu, Chong Jiang category:cs.LG stat.ML published:2015-04-27 summary:We study contextual bandits with budget and time constraints, referred to asconstrained contextual bandits.The time and budget constraints significantlycomplicate the exploration and exploitation tradeoff because they introducecomplex coupling among contexts over time.Such coupling effects make itdifficult to obtain oracle solutions that assume known statistics of bandits.To gain insight, we first study unit-cost systems with known contextdistribution. When the expected rewards are known, we develop an approximationof the oracle, referred to Adaptive-Linear-Programming (ALP), which achievesnear-optimality and only requires the ordering of expected rewards. With thesehighly desirable features, we then combine ALP with the upper-confidence-bound(UCB) method in the general case where the expected rewards are unknown {\it apriori}. We show that the proposed UCB-ALP algorithm achieves logarithmicregret except for certain boundary cases. Further, we design algorithms andobtain similar regret analysis results for more general systems with unknowncontext distribution and heterogeneous costs. To the best of our knowledge,this is the first work that shows how to achieve logarithmic regret inconstrained contextual bandits. Moreover, this work also sheds light on thestudy of computationally efficient algorithms for general constrainedcontextual bandits.
arxiv-12900-60 | A Sparse Gaussian Process Framework for Photometric Redshift Estimation | http://arxiv.org/pdf/1505.05489v3.pdf | author:Ibrahim A. Almosallam, Sam N. Lindsay, Matt J. Jarvis, Stephen J. Roberts category:astro-ph.IM astro-ph.GA cs.CV published:2015-05-20 summary:Accurate photometric redshifts are a lynchpin for many future experiments topin down the cosmological model and for studies of galaxy evolution. In thisstudy, a novel sparse regression framework for photometric redshift estimationis presented. Simulated and real data from SDSS DR12 were used to train andtest the proposed models. We show that approaches which include careful datapreparation and model design offer a significant improvement in comparison withseveral competing machine learning algorithms. Standard implementations of mostregression algorithms have as the objective the minimization of the sum ofsquared errors. For redshift inference, however, this induces a bias in theposterior mean of the output distribution, which can be problematic. In thispaper we directly target minimizing $\Delta z = (z_\textrm{s} -z_\textrm{p})/(1+z_\textrm{s})$ and address the bias problem via adistribution-based weighting scheme, incorporated as part of the optimizationobjective. The results are compared with other machine learning algorithms inthe field such as Artificial Neural Networks (ANN), Gaussian Processes (GPs)and sparse GPs. The proposed framework reaches a mean absolute $\Delta z =0.0026(1+z_\textrm{s})$, over the redshift range of $0 \le z_\textrm{s} \le 2$on the simulated data, and $\Delta z = 0.0178(1+z_\textrm{s})$ over the entireredshift range on the SDSS DR12 survey, outperforming the standard ANNz used inthe literature. We also investigate how the relative size of the training setaffects the photometric redshift accuracy. We find that a training set of\textgreater 30 per cent of total sample size, provides little additionalconstraint on the photometric redshifts, and note that our GP formalismstrongly outperforms ANNz in the sparse data regime for the simulated data set.
arxiv-12900-61 | Application of Machine Learning Techniques in Human Activity Recognition | http://arxiv.org/pdf/1510.05577v1.pdf | author:Jitenkumar Babubhai Rana, Rashmi Shetty, Tanya Jha category:cs.LG published:2015-10-19 summary:Human activity detection has seen a tremendous growth in the last decadeplaying a major role in the field of pervasive computing. This emergingpopularity can be attributed to its myriad of real-life applications primarilydealing with human-centric problems like healthcare and elder care. Manyresearch attempts with data mining and machine learning techniques have beenundergoing to accurately detect human activities for e-health systems. Thispaper reviews some of the predictive data mining algorithms and compares theaccuracy and performances of these models. A discussion on the future researchdirections is subsequently offered.
arxiv-12900-62 | Optimization for Gaussian Processes via Chaining | http://arxiv.org/pdf/1510.05576v1.pdf | author:Emile Contal, Cédric Malherbe, Nicolas Vayatis category:stat.ML published:2015-10-19 summary:In this paper, we consider the problem of stochastic optimization under abandit feedback model. We generalize the GP-UCB algorithm [Srinivas and al.,2012] to arbitrary kernels and search spaces. To do so, we use a notion oflocalized chaining to control the supremum of a Gaussian process, and provide anovel optimization scheme based on the computation of covering numbers. Thetheoretical bounds we obtain on the cumulative regret are more generic andpresent the same convergence rates as the GP-UCB algorithm. Finally, thealgorithm is shown to be empirically more efficient than its naturalcompetitors on simple and complex input spaces.
arxiv-12900-63 | Across neighbourhood search for numerical optimization | http://arxiv.org/pdf/1401.3376v3.pdf | author:Guohua Wu category:cs.NE published:2014-01-14 summary:Population-based search algorithms (PBSAs), including swarm intelligencealgorithms (SIAs) and evolutionary algorithms (EAs), are competitivealternatives for solving complex optimization problems and they have beenwidely applied to real-world optimization problems in different fields. In thisstudy, a novel population-based across neighbourhood search (ANS) is proposedfor numerical optimization. ANS is motivated by two straightforward assumptionsand three important issues raised in improving and designing efficient PBSAs.In ANS, a group of individuals collaboratively search the solution space for anoptimal solution of the optimization problem considered. A collection ofsuperior solutions found by individuals so far is maintained and updateddynamically. At each generation, an individual directly searches across theneighbourhoods of multiple superior solutions with the guidance of a Gaussiandistribution. This search manner is referred to as across neighbourhood search.The characteristics of ANS are discussed and the concept comparisons with otherPBSAs are given. The principle behind ANS is simple. Moreover, ANS is easy forimplementation and application with three parameters being required to tune.Extensive experiments on 18 benchmark optimization functions of different typesshow that ANS has well balanced exploration and exploitation capabilities andperforms competitively compared with many efficient PBSAs (Related Matlab codesused in the experiments are available fromhttp://guohuawunudt.gotoip2.com/publications.html).
arxiv-12900-64 | Sparse + Low Rank Decomposition of Annihilating Filter-based Hankel Matrix for Impulse Noise Removal | http://arxiv.org/pdf/1510.05559v1.pdf | author:Kyong Hwan Jin, Jong Chul Ye category:cs.CV published:2015-10-19 summary:Recently, so called annihilating filer-based low rank Hankel matrix (ALOHA)approach was proposed as a powerful image inpainting method. Based on theobservation that smoothness or textures within an image patch corresponds tosparse spectral components in the frequency domain, ALOHA exploits theexistence of annihilating filters and the associated rank-deficient Hankelmatrices in the image domain to estimate the missing pixels. By extending thisidea, here we propose a novel impulse noise removal algorithm using sparse +low rank decomposition of an annihilating filter-based Hankel matrix. The newapproach, what we call the robust ALOHA, is motivated by the observation thatan image corrupted with impulse noises has intact pixels; so the impulse noisescan be modeled as sparse components, whereas the underlying image can be stillmodeled using a low-rank Hankel structured matrix. To solve the sparse + lowrank decomposition problem, we propose an alternating direction method ofmultiplier (ADMM) method with initial factorized matrices coming from low rankmatrix fitting (LMaFit) algorithm. To adapt the local image statistics thathave distinct spectral distributions, the robust ALOHA is applied patch bypatch. Experimental results from two types of impulse noises - random valuedimpulse noises and salt/pepper noises - for both single channel andmulti-channel color images demonstrate that the robust ALOHA outperforms theexisting algorithms up to 8dB in terms of the peak signal to noise ratio(PSNR).
arxiv-12900-65 | Clustering with Beta Divergences | http://arxiv.org/pdf/1510.05491v1.pdf | author:Mehmet Emin Basbug, Barbara Engelhardt category:cs.LG published:2015-10-19 summary:Clustering algorithms start with a fixed divergence metric, which capturesthe possibly asymmetric distance between two samples. In a mixture model, thesample distribution plays the role of a divergence metric. It is often the casethat the distributional assumption is not validated, which calls for anadaptive approach. We consider a richer model where the underlying distributionbelongs to a parametrized exponential family, called Tweedie Models. We firstshow the connection between the Tweedie models and beta divergences, and derivethe corresponding hard-assignment clustering algorithm. We exploit thisconnection to identify moment conditions and use Generalized Method ofMoments(GMoM) to learn the data distribution. Based on this adaptive approach,we propose four new hard clustering algorithms and compare them to theclassical k-means and DP-means on synthetic data as well as seven UCI datasetsand one large gene expression dataset. We further compare the GMoM routine toan approximate maximum likelihood routine and validate the computationalbenefits of the GMoM approach.
arxiv-12900-66 | DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection | http://arxiv.org/pdf/1510.05484v1.pdf | author:Xi Li, Liming Zhao, Lina Wei, MingHsuan Yang, Fei Wu, Yueting Zhuang, Haibin Ling, Jingdong Wang category:cs.CV published:2015-10-19 summary:A key problem in salient object detection is how to effectively model thesemantic properties of salient objects in a data-driven manner. In this paper,we propose a multi-task deep saliency model based on a fully convolutionalneural network (FCNN) with global input (whole raw images) and global output(whole saliency maps). In principle, the proposed saliency model takes adata-driven strategy for encoding the underlying saliency prior information,and then sets up a multi-task learning scheme for exploring the intrinsiccorrelations between saliency detection and semantic image segmentation.Through collaborative feature learning from such two correlated tasks, theshared fully convolutional layers produce effective features for objectperception. Moreover, it is capable of capturing the semantic information onsalient objects across different levels using the fully convolutional layers,which investigates the feature-sharing properties of salient object detectionwith great feature redundancy reduction. Finally, we present a graph Laplacianregularized nonlinear regression model for saliency refinement. Experimentalresults demonstrate the effectiveness of our approach in comparison with thestate-of-the-art approaches.
arxiv-12900-67 | Accelerometer based Activity Classification with Variational Inference on Sticky HDP-SLDS | http://arxiv.org/pdf/1510.05477v1.pdf | author:Mehmet Emin Basbug, Koray Ozcan, Senem Velipasalar category:cs.LG stat.ML published:2015-10-19 summary:As part of daily monitoring of human activities, wearable sensors and devicesare becoming increasingly popular sources of data. With the advent ofsmartphones equipped with acceloremeter, gyroscope and camera; it is nowpossible to develop activity classification platforms everyone can useconveniently. In this paper, we propose a fast inference method for anunsupervised non-parametric time series model namely variational inference forsticky HDP-SLDS(Hierarchical Dirichlet Process Switching Linear DynamicalSystem). We show that the proposed algorithm can differentiate various indooractivities such as sitting, walking, turning, going up/down the stairs andtaking the elevator using only the acceloremeter of an Android smartphoneSamsung Galaxy S4. We used the front camera of the smartphone to annotateactivity types precisely. We compared the proposed method with Hidden MarkovModels with Gaussian emission probabilities on a dataset of 10 subjects. Weshowed that the efficacy of the stickiness property. We further compared thevariational inference to the Gibbs sampler on the same model and show thatvariational inference is faster in one order of magnitude.
arxiv-12900-68 | Confidence Sets for the Source of a Diffusion in Regular Trees | http://arxiv.org/pdf/1510.05461v1.pdf | author:Justin Khim, Po-Ling Loh category:math.ST cs.DM cs.SI math.PR stat.ML stat.TH 62M99 published:2015-10-19 summary:We study the problem of identifying the source of a diffusion spreading overa regular tree. When the degree of each node is at least three, we show that itis possible to construct confidence sets for the diffusion source with sizeindependent of the number of infected nodes. Our estimators are motivated byanalogous results in the literature concerning identification of the root nodein preferential attachment and uniform attachment trees. At the core of ourproofs is a probabilistic analysis of P\'{o}lya urns corresponding to thenumber of uninfected neighbors in specific subtrees of the infection tree. Wealso provide an example illustrating the shortcomings of source estimationtechniques in settings where the underlying graph is asymmetric.
arxiv-12900-69 | Color graph based wavelet transform with perceptual information | http://arxiv.org/pdf/1510.05436v1.pdf | author:Mohamed Malek, David Helbert, Philippe Carre category:cs.CV published:2015-10-19 summary:In this paper, we propose a numerical strategy to define a multiscaleanalysis for color and multicomponent images based on the representation ofdata on a graph. Our approach consists in computing the graph of an image usingthe psychovisual information and analysing it by using the spectral graphwavelet transform. We suggest introducing color dimension into the computationof the weights of the graph and using the geodesic distance as a means ofdistance measurement. We thus have defined a wavelet transform based on a graphwith perceptual information by using the CIELab color distance. This newrepresentation is illustrated with denoising and inpainting applications.Overall, by introducing psychovisual information in the graph computation forthe graph wavelet transform we obtain very promising results. Therefore resultsin image restoration highlight the interest of the appropriate use of colorinformation.
arxiv-12900-70 | A Bayesian alternative to mutual information for the hierarchical clustering of dependent random variables | http://arxiv.org/pdf/1501.05194v2.pdf | author:Guillaume Marrelec, Arnaud Messé, Pierre Bellec category:stat.ML cs.LG q-bio.QM published:2015-01-21 summary:The use of mutual information as a similarity measure in agglomerativehierarchical clustering (AHC) raises an important issue: some correction needsto be applied for the dimensionality of variables. In this work, we formulatethe decision of merging dependent multivariate normal variables in an AHCprocedure as a Bayesian model comparison. We found that the Bayesianformulation naturally shrinks the empirical covariance matrix towards a matrixset a priori (e.g., the identity), provides an automated stopping rule, andcorrects for dimensionality using a term that scales up the measure as afunction of the dimensionality of the variables. Also, the resulting log Bayesfactor is asymptotically proportional to the plug-in estimate of mutualinformation, with an additive correction for dimensionality in agreement withthe Bayesian information criterion. We investigated the behavior of theseBayesian alternatives (in exact and asymptotic forms) to mutual information onsimulated and real data. An encouraging result was first derived onsimulations: the hierarchical clustering based on the log Bayes factoroutperformed off-the-shelf clustering techniques as well as raw and normalizedmutual information in terms of classification accuracy. On a toy example, wefound that the Bayesian approaches led to results that were similar to those ofmutual information clustering techniques, with the advantage of an automatedthresholding. On real functional magnetic resonance imaging (fMRI) datasetsmeasuring brain activity, it identified clusters consistent with theestablished outcome of standard procedures. On this application, normalizedmutual information had a highly atypical behavior, in the sense that itsystematically favored very large clusters. These initial experiments suggestthat the proposed Bayesian alternatives to mutual information are a useful newtool for hierarchical clustering.
arxiv-12900-71 | Piecewise-Linear Approximation for Feature Subset Selection in a Sequential Logit Model | http://arxiv.org/pdf/1510.05417v1.pdf | author:Toshiki Sato, Yuichi Takano, Ryuhei Miyashiro category:stat.ME cs.LG math.OC stat.ML published:2015-10-19 summary:This paper concerns a method of selecting a subset of features for asequential logit model. Tanaka and Nakagawa (2014) proposed a mixed integerquadratic optimization formulation for solving the problem based on a quadraticapproximation of the logistic loss function. However, since there is asignificant gap between the logistic loss function and its quadraticapproximation, their formulation may fail to find a good subset of features. Toovercome this drawback, we apply a piecewise-linear approximation to thelogistic loss function. Accordingly, we frame the feature subset selectionproblem of minimizing an information criterion as a mixed integer linearoptimization problem. The computational results demonstrate that ourpiecewise-linear approximation approach found a better subset of features thanthe quadratic approximation approach.
arxiv-12900-72 | Estimating Posterior Ratio for Classification: Transfer Learning from Probabilistic Perspective | http://arxiv.org/pdf/1506.02784v3.pdf | author:Song Liu, Kenji Fukumizu category:stat.ML cs.LG published:2015-06-09 summary:Transfer learning assumes classifiers of similar tasks share certainparameter structures. Unfortunately, modern classifiers uses sophisticatedfeature representations with huge parameter spaces which lead to costlytransfer. Under the impression that changes from one classifier to anothershould be ``simple'', an efficient transfer learning criteria that only learnsthe ``differences'' is proposed in this paper. We train a \emph{posteriorratio} which turns out to minimizes the upper-bound of the target learningrisk. The model of posterior ratio does not have to share the same parameterspace with the source classifier at all so it can be easily modelled andefficiently trained. The resulting classifier therefore is obtained by simplymultiplying the existing probabilistic-classifier with the learned posteriorratio.
arxiv-12900-73 | Fast Embedding for JOFC Using the Raw Stress Criterion | http://arxiv.org/pdf/1502.03391v2.pdf | author:Vince Lyzinski, Youngser Park, Carey E. Priebe, Michael W. Trosset category:stat.ML stat.ME published:2015-02-11 summary:The Joint Optimization of Fidelity and Commensurability (JOFC) manifoldmatching methodology embeds an omnibus dissimilarity matrix consisting ofmultiple dissimilarities on the same set of objects. One approach to thisembedding optimizes the preservation of fidelity to each individualdissimilarity matrix together with commensurability of each given observationacross modalities via iterative majorizations of a raw stress error criterionby successive Guttman transforms. In this paper, we exploit the specialstructure inherent to JOFC to exactly and efficiently compute the successiveGuttman transforms, and as a result we are able to greatly speed up andparallelize the JOFC procedure for both in-sample and out-of-sample embedding.We demonstrate the scalability of our implementation on both real and simulateddata examples.
arxiv-12900-74 | Clustering is Easy When ....What? | http://arxiv.org/pdf/1510.05336v1.pdf | author:Shai Ben-David category:stat.ML cs.LG published:2015-10-19 summary:It is well known that most of the common clustering objectives are NP-hard tooptimize. In practice, however, clustering is being routinely carried out. Oneapproach for providing theoretical understanding of this seeming discrepancy isto come up with notions of clusterability that distinguish realisticallyinteresting input data from worst-case data sets. The hope is that there willbe clustering algorithms that are provably efficient on such "clusterable"instances. This paper addresses the thesis that the computational hardness ofclustering tasks goes away for inputs that one really cares about. In otherwords, that "Clustering is difficult only when it does not matter" (the\emph{CDNM thesis} for short). I wish to present a a critical bird's eye overview of the results publishedon this issue so far and to call attention to the gap between available anddesirable results on this issue. A longer, more detailed version of this noteis available as arXiv:1507.05307. I discuss which requirements should be met in order to provide formal supportto the the CDNM thesis and then examine existing results in view of theserequirements and list some significant unsolved research challenges in thatdirection.
arxiv-12900-75 | Constrained Convolutional Neural Networks for Weakly Supervised Segmentation | http://arxiv.org/pdf/1506.03648v2.pdf | author:Deepak Pathak, Philipp Krähenbühl, Trevor Darrell category:cs.CV cs.LG published:2015-06-11 summary:We present an approach to learn a dense pixel-wise labeling from image-leveltags. Each image-level tag imposes constraints on the output labeling of aConvolutional Neural Network (CNN) classifier. We propose Constrained CNN(CCNN), a method which uses a novel loss function to optimize for any set oflinear constraints on the output space (i.e. predicted label distribution) of aCNN. Our loss formulation is easy to optimize and can be incorporated directlyinto standard stochastic gradient descent optimization. The key idea is tophrase the training objective as a biconvex optimization for linear models,which we then relax to nonlinear deep networks. Extensive experimentsdemonstrate the generality of our new learning framework. The constrained lossyields state-of-the-art results on weakly supervised semantic imagesegmentation. We further demonstrate that adding slightly more supervision cangreatly improve the performance of the learning algorithm.
arxiv-12900-76 | Latent Space Model for Multi-Modal Social Data | http://arxiv.org/pdf/1510.05318v1.pdf | author:Yoon-Sik Cho, Greg Ver Steeg, Emilio Ferrara, Aram Galstyan category:cs.SI cs.LG physics.soc-ph published:2015-10-18 summary:With the emergence of social networking services, researchers enjoy theincreasing availability of large-scale heterogenous datasets capturing onlineuser interactions and behaviors. Traditional analysis of techno-social systemsdata has focused mainly on describing either the dynamics of socialinteractions, or the attributes and behaviors of the users. However,overwhelming empirical evidence suggests that the two dimensions affect oneanother, and therefore they should be jointly modeled and analyzed in amulti-modal framework. The benefits of such an approach include the ability tobuild better predictive models, leveraging social network information as wellas user behavioral signals. To this purpose, here we propose the ConstrainedLatent Space Model (CLSM), a generalized framework that combines MixedMembership Stochastic Blockmodels (MMSB) and Latent Dirichlet Allocation (LDA)incorporating a constraint that forces the latent space to concurrentlydescribe the multiple data modalities. We derive an efficient inferencealgorithm based on Variational Expectation Maximization that has acomputational cost linear in the size of the network, thus making it feasibleto analyze massive social datasets. We validate the proposed framework on twoproblems: prediction of social interactions from user attributes and behaviors,and behavior prediction exploiting network information. We perform experimentswith a variety of multi-modal social systems, spanning location-based socialnetworks (Gowalla), social media services (Instagram, Orkut), e-commerce andreview sites (Amazon, Ciao), and finally citation networks (Cora). The resultsindicate significant improvement in prediction accuracy over state of the artmethods, and demonstrate the flexibility of the proposed approach foraddressing a variety of different learning problems commonly occurring withmulti-modal social data.
arxiv-12900-77 | An optimal randomized incremental gradient method | http://arxiv.org/pdf/1507.02000v3.pdf | author:Guanghui Lan, Yi Zhou category:math.OC cs.CC stat.ML published:2015-07-08 summary:In this paper, we consider a class of finite-sum convex optimization problemswhose objective function is given by the summation of $m$ ($\ge 1$) smoothcomponents together with some other relatively simple terms. We first introducea deterministic primal-dual gradient (PDG) method that can achieve the optimalblack-box iteration complexity for solving these composite optimizationproblems using a primal-dual termination criterion. Our major contribution isto develop a randomized primal-dual gradient (RPDG) method, which needs tocompute the gradient of only one randomly selected smooth component at eachiteration, but can possibly achieve better complexity than PDG in terms of thetotal number of gradient evaluations. More specifically, we show that the totalnumber of gradient evaluations performed by RPDG can be ${\cal O} (\sqrt{m})$times smaller, both in expectation and with high probability, than thoseperformed by deterministic optimal first-order methods under favorablesituations. We also show that the complexity of the RPDG method is notimprovable by developing a new lower complexity bound for a general class ofrandomized methods for solving large-scale finite-sum convex optimizationproblems. Moreover, through the development of PDG and RPDG, we introduce anovel game-theoretic interpretation for these optimal methods for convexoptimization.
arxiv-12900-78 | EESEN: End-to-End Speech Recognition using Deep RNN Models and WFST-based Decoding | http://arxiv.org/pdf/1507.08240v3.pdf | author:Yajie Miao, Mohammad Gowayyed, Florian Metze category:cs.CL cs.LG published:2015-07-29 summary:The performance of automatic speech recognition (ASR) has improvedtremendously due to the application of deep neural networks (DNNs). Despitethis progress, building a new ASR system remains a challenging task, requiringvarious resources, multiple training stages and significant expertise. Thispaper presents our Eesen framework which drastically simplifies the existingpipeline to build state-of-the-art ASR systems. Acoustic modeling in Eeseninvolves learning a single recurrent neural network (RNN) predictingcontext-independent targets (phonemes or characters). To remove the need forpre-generated frame labels, we adopt the connectionist temporal classification(CTC) objective function to infer the alignments between speech and labelsequences. A distinctive feature of Eesen is a generalized decoding approachbased on weighted finite-state transducers (WFSTs), which enables the efficientincorporation of lexicons and language models into CTC decoding. Experimentsshow that compared with the standard hybrid DNN systems, Eesen achievescomparable word error rates (WERs), while at the same time speeding up decodingsignificantly.
arxiv-12900-79 | Geometry-aware Deep Transform | http://arxiv.org/pdf/1509.05360v2.pdf | author:Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro category:cs.CV published:2015-09-17 summary:Many recent efforts have been devoted to designing sophisticated deeplearning structures, obtaining revolutionary results on benchmark datasets. Thesuccess of these deep learning methods mostly relies on an enormous volume oflabeled training samples to learn a huge number of parameters in a network;therefore, understanding the generalization ability of a learned deep networkcannot be overlooked, especially when restricted to a small training set, whichis the case for many applications. In this paper, we propose a novel deeplearning objective formulation that unifies both the classification and metriclearning criteria. We then introduce a geometry-aware deep transform to enablea non-linear discriminative and robust feature transform, which showscompetitive performance on small training sets for both synthetic andreal-world data. We further support the proposed framework with a formal$(K,\epsilon)$-robustness analysis.
arxiv-12900-80 | Real-time Tracking Based on Neuromrophic Vision | http://arxiv.org/pdf/1510.05275v1.pdf | author:Hongmin Li, Pei Jing, Guoqi Li category:cs.CV published:2015-10-18 summary:Real-time tracking is an important problem in computer vision in which mostmethods are based on the conventional cameras. Neuromorphic vision is a conceptdefined by incorporating neuromorphic vision sensors such as silicon retinas invision processing system. With the development of the silicon technology,asynchronous event-based silicon retinas that mimic neuro-biologicalarchitectures has been developed in recent years. In this work, we combine thevision tracking algorithm of computer vision with the information encodingmechanism of event-based sensors which is inspired from the neural rate codingmechanism. The real-time tracking of single object with the advantage of highspeed of 100 time bins per second is successfully realized. Our methoddemonstrates that the computer vision methods could be used for theneuromorphic vision processing and we can realize fast real-time tracking usingneuromorphic vision sensors compare to the conventional camera.
arxiv-12900-81 | Scalable inference for a full multivariate stochastic volatility model | http://arxiv.org/pdf/1510.05257v1.pdf | author:P. Dellaportas, A. Plataniotis, M. K. Titsias category:stat.ML published:2015-10-18 summary:We introduce a multivariate stochastic volatility model for asset returnsthat imposes no restrictions to the structure of the volatility matrix andtreats all its elements as functions of latent stochastic processes. When thenumber of assets is prohibitively large, we propose a factor multivariatestochastic volatility model in which the variances and correlations of thefactors evolve stochastically over time. Inference is achieved via a carefullydesigned feasible and scalable Markov chain Monte Carlo algorithm that combinestwo computationally important ingredients: it utilizes invariant to the priorMetropolis proposal densities for simultaneously updating all latent paths andhas quadratic, rather than cubic, computational complexity when evaluating themultivariate normal densities required. We apply our modelling andcomputational methodology to $571$ stock daily returns of Euro STOXX index fordata over a period of $10$ years.
arxiv-12900-82 | Large Enforced Sparse Non-Negative Matrix Factorization | http://arxiv.org/pdf/1510.05237v1.pdf | author:Brendan Gavin, Vijay Gadepally, Jeremy Kepner category:cs.LG cs.NA cs.SI published:2015-10-18 summary:Non-negative matrix factorization (NMF) is a common method for generatingtopic models from text data. NMF is widely accepted for producing good resultsdespite its relative simplicity of implementation and ease of computation. Onechallenge with applying NMF to large datasets is that intermediate matrixproducts often become dense, stressing the memory and compute elements of asystem. In this article, we investigate a simple but powerful modification of acommon NMF algorithm that enforces the generation of sparse intermediate andoutput matrices. This method enables the application of NMF to large datasetsthrough improved memory and compute performance. Further, we demonstrateempirically that this method of enforcing sparsity in the NMF either preservesor improves both the accuracy of the resulting topic model and the convergencerate of the underlying algorithm.
arxiv-12900-83 | Clustering Noisy Signals with Structured Sparsity Using Time-Frequency Representation | http://arxiv.org/pdf/1510.05214v1.pdf | author:Tom Hope, Avishai Wagner, Or Zuk category:cs.LG stat.ML 62H30, 65T60 published:2015-10-18 summary:We propose a simple and efficient time-series clustering frameworkparticularly suited for low Signal-to-Noise Ratio (SNR), by simultaneoussmoothing and dimensionality reduction aimed at preserving clusteringinformation. We extend the sparse K-means algorithm by incorporating structuredsparsity, and use it to exploit the multi-scale property of wavelets and groupstructure in multivariate signals. Finally, we extract features invariant totranslation and scaling with the scattering transform, which corresponds to aconvolutional network with filters given by a wavelet operator, and use thenetwork's structure in sparse clustering. By promoting sparsity, this transformcan yield a low-dimensional representation of signals that gives improvedclustering results on several real datasets.
arxiv-12900-84 | Expresso : A user-friendly GUI for Designing, Training and Exploring Convolutional Neural Networks | http://arxiv.org/pdf/1505.06605v2.pdf | author:Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu category:cs.CV cs.NE published:2015-05-25 summary:With a view to provide a user-friendly interface for designing, training anddeveloping deep learning frameworks, we have developed Expresso, a GUI toolwritten in Python. Expresso is built atop Caffe, the open-source, prize-winningframework popularly used to develop Convolutional Neural Networks. Expressoprovides a convenient wizard-like graphical interface which guides the userthrough various common scenarios -- data import, construction and training ofdeep networks, performing various experiments, analyzing and visualizing theresults of these experiments. The multi-threaded nature of Expresso enablesconcurrent execution and notification of events related to the aforementionedscenarios. The GUI sub-components and inter-component interfaces in Expressohave been designed with extensibility in mind. We believe Expresso'sflexibility and ease of use will come in handy to researchers, newcomers andseasoned alike, in their explorations related to deep learning.
arxiv-12900-85 | Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015 | http://arxiv.org/pdf/1510.05203v1.pdf | author:Graham Neubig, Makoto Morishita, Satoshi Nakamura category:cs.CL published:2015-10-18 summary:This year, the Nara Institute of Science and Technology (NAIST)'s submissionto the 2015 Workshop on Asian Translation was based on syntax-based statisticalmachine translation, with the addition of a reranking component using neuralattentional machine translation models. Experiments re-confirmed results fromprevious work stating that neural MT reranking provides a large gain inobjective evaluation measures such as BLEU, and also confirmed for the firsttime that these results also carry over to manual evaluation. We furtherperform a detailed analysis of reasons for this increase, finding that the maincontributions of the neural models lie in improvement of the grammaticalcorrectness of the output, as opposed to improvements in lexical choice ofcontent words.
arxiv-12900-86 | Sum-of-Squares Lower Bounds for Sparse PCA | http://arxiv.org/pdf/1507.06370v2.pdf | author:Tengyu Ma, Avi Wigderson category:cs.LG cs.CC math.ST stat.CO stat.ML stat.TH published:2015-07-23 summary:This paper establishes a statistical versus computational trade-off forsolving a basic high-dimensional machine learning problem via a basic convexrelaxation method. Specifically, we consider the {\em Sparse PrincipalComponent Analysis} (Sparse PCA) problem, and the family of {\emSum-of-Squares} (SoS, aka Lasserre/Parillo) convex relaxations. It was wellknown that in large dimension $p$, a planted $k$-sparse unit vector can be {\emin principle} detected using only $n \approx k\log p$ (Gaussian or Bernoulli)samples, but all {\em efficient} (polynomial time) algorithms known require $n\approx k^2$ samples. It was also known that this quadratic gap cannot beimproved by the the most basic {\em semi-definite} (SDP, aka spectral)relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that alsodegree-4 SoS algorithms cannot improve this quadratic gap. This average-caselower bound adds to the small collection of hardness results in machinelearning for this powerful family of convex relaxation algorithms. Moreover,our design of moments (or "pseudo-expectations") for this lower bound is quitedifferent than previous lower bounds. Establishing lower bounds for higherdegree SoS algorithms for remains a challenging problem.
arxiv-12900-87 | Efficient Hand Articulations Tracking using Adaptive Hand Model and Depth map | http://arxiv.org/pdf/1510.00981v3.pdf | author:Byeongkeun Kang, Yeejin Lee, Truong Q. Nguyen category:cs.CV published:2015-10-04 summary:Real-time hand articulations tracking is important for many applications suchas interacting with virtual / augmented reality devices or tablets. However,most of existing algorithms highly rely on expensive and high power-consumingGPUs to achieve real-time processing. Consequently, these systems areinappropriate for mobile and wearable devices. In this paper, we propose anefficient hand tracking system which does not require high performance GPUs. Inour system, we track hand articulations by minimizing discrepancy between depthmap from sensor and computer-generated hand model. We also initialize hand poseat each frame using finger detection and classification. Our contributions are:(a) propose adaptive hand model to consider different hand shapes of userswithout generating personalized hand model; (b) improve the highly efficientframe initialization for robust tracking and automatic initialization; (c)propose hierarchical random sampling of pixels from each depth map to improvetracking accuracy while limiting required computations. To the best of ourknowledge, it is the first system that achieves both automatic hand modeladjustment and real-time tracking without using GPUs.
arxiv-12900-88 | Performance Characterization of Image Feature Detectors in Relation to the Scene Content Utilizing a Large Image Database | http://arxiv.org/pdf/1510.05157v1.pdf | author:Bruno Ferrarini, Shoaib Ehsan, Naveed Ur Rehman, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:Selecting the most suitable local invariant feature detector for a particularapplication has rendered the task of evaluating feature detectors a criticalissue in vision research. No state-of-the-art image feature detector workssatisfactorily under all types of image transformations. Although theliterature offers a variety of comparison works focusing on performanceevaluation of image feature detectors under several types of imagetransformation, the influence of the scene content on the performance of localfeature detectors has received little attention so far. This paper aims tobridge this gap with a new framework for determining the type of scenes, whichmaximize and minimize the performance of detectors in terms of repeatabilityrate. Several state-of-the-art feature detectors have been assessed utilizing alarge database of 12936 images generated by applying uniform light and blurchanges to 539 scenes captured from the real world. The results obtainedprovide new insights into the behaviour of feature detectors.
arxiv-12900-89 | Assessing The Performance Bounds Of Local Feature Detectors: Taking Inspiration From Electronics Design Practices | http://arxiv.org/pdf/1510.05156v1.pdf | author:Shoaib Ehsan, Adrian F. Clark, Bruno Ferrarini, Naveed Ur Rehman, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:Since local feature detection has been one of the most active research areasin computer vision, a large number of detectors have been proposed. This hasrendered the task of characterizing the performance of various featuredetection methods an important issue in vision research. Inspired by the goodpractices of electronic system design, a generic framework based on theimproved repeatability measure is presented in this paper that allowsassessment of the upper and lower bounds of detector performance in an effortto design more reliable and effective vision systems. This framework is thenemployed to establish operating and guarantee regions for several state-of-theart detectors for JPEG compression and uniform light changes. The results areobtained using a newly acquired, large image database (15092 images) with 539different scenes. These results provide new insights into the behavior ofdetectors and are also useful from the vision systems design perspective.
arxiv-12900-90 | A Historical Analysis of the Field of OR/MS using Topic Models | http://arxiv.org/pdf/1510.05154v1.pdf | author:Christopher J. Gatti, James D. Brooks, Sarah G. Nurre category:stat.ML cs.DL stat.AP published:2015-10-17 summary:This study investigates the content of the published scientific literature inthe fields of operations research and management science (OR/MS) since theearly 1950s. Our study is based on 80,757 published journal abstracts from 37of the leading OR/MS journals. We have developed a topic model, using LatentDirichlet Allocation (LDA), and extend this analysis to reveal the temporaldynamics of the field, journals, and topics. Our analysis shows the generalityor specificity of each of the journals, and we identify groups of journals withsimilar content, which are both consistent and inconsistent with intuition. Wealso show how journals have become more or less unique in their scope. A moredetailed analysis of each journals' topics over time shows significant temporaldynamics, especially for journals with niche content. This study presents anobservational, yet objective, view of the published literature from OR/MS thatwould be of interest to authors, editors, journals, and publishers.Furthermore, this work can be used by new entrants to the fields of OR/MS tounderstand the content landscape, as a starting point for discussions andinquiry of the field at large, or as a model for other fields to performsimilar analyses.
arxiv-12900-91 | Robust Non-linear Wiener-Granger Causality For Large High-dimensional Data | http://arxiv.org/pdf/1510.05149v1.pdf | author:Mehrdad Jafari-Mamaghani category:stat.ML stat.ME published:2015-10-17 summary:Wiener-Granger causality is a widely used framework of causal analysis fortemporally resolved events. We introduce a new measure of Wiener-Grangercausality based on kernelization of partial canonical correlation analysis withspecific advantages in the context of large high-dimensional data. Theintroduced measure is able to detect non-linear and non-monotonous signals, isdesigned to be immune to noise, and offers tunability in terms of computationalcomplexity in its estimations. Furthermore, we show that, under specifiedconditions, the introduced measure can be regarded as an estimate ofconditional mutual information (transfer entropy). The functionality of thismeasure is assessed using comparative simulations where it outperforms otherexisting methods. The paper is concluded with an application to climatologicaldata.
arxiv-12900-92 | Rapid Online Analysis of Local Feature Detectors and Their Complementarity | http://arxiv.org/pdf/1510.05145v1.pdf | author:Shoaib Ehsan, Adrian F. Clark, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:A vision system that can assess its own performance and take appropriateactions online to maximize its effectiveness would be a step towards achievingthe long-cherished goal of imitating humans. This paper proposes a method forperforming an online performance analysis of local feature detectors, theprimary stage of many practical vision systems. It advocates the spatialdistribution of local image features as a good performance indicator andpresents a metric that can be calculated rapidly, concurs with human visualassessments and is complementary to existing offline measures such asrepeatability. The metric is shown to provide a measure of complementarity forcombinations of detectors, correctly reflecting the underlying principles ofindividual detectors. Qualitative results on well-established datasets forseveral state-of-the-art detectors are presented based on the proposed measure.Using a hypothesis testing approach and a newly-acquired, larger imagedatabase, statistically-significant performance differences are identified.Different detector pairs and triplets are examined quantitatively and theresults provide a useful guideline for combining detectors in applications thatrequire a reasonable spatial distribution of image features. A principledframework for combining feature detectors in these applications is alsopresented. Timing results reveal the potential of the metric for onlineapplications.
arxiv-12900-93 | Memory-Efficient Design Strategy for a Parallel Embedded Integral Image Computation Engine | http://arxiv.org/pdf/1510.05142v1.pdf | author:Shoaib Ehsan, Adrian F. Clark, Wah M. Cheung, Arjunsingh M. Bais, Bayar I. Menzat, Nadia Kanwal, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:In embedded vision systems, parallel computation of the integral imagepresents several design challenges in terms of hardware resources, speed andpower consumption. Although recursive equations significantly reduce the numberof operations for computing the integral image, the required internal memorybecomes prohibitively large for an embedded integral image computation enginefor increasing image sizes. With the objective of achieving high-throughputwith minimum hardware resources, this paper proposes a memory-efficient designstrategy for a parallel embedded integral image computation engine. Resultsshow that the design achieves nearly 35% reduction in memory for common HDvideo.
arxiv-12900-94 | Integral Images: Efficient Algorithms for Their Computation and Storage in Resource-Constrained Embedded Vision Systems | http://arxiv.org/pdf/1510.05138v1.pdf | author:Shoaib Ehsan, Adrian F. Clark, Naveed ur Rehman, Klaus D. McDonald-Maier category:cs.CV published:2015-10-17 summary:The integral image, an intermediate image representation, has found extensiveuse in multi-scale local feature detection algorithms, such as Speeded-UpRobust Features (SURF), allowing fast computation of rectangular features atconstant speed, independent of filter size. For resource-constrained real-timeembedded vision systems, computation and storage of integral image presentsseveral design challenges due to strict timing and hardware limitations.Although calculation of the integral image only consists of simple additionoperations, the total number of operations is large owing to the generallylarge size of image data. Recursive equations allow substantial decrease in thenumber of operations but require calculation in a serial fashion. This paperpresents two new hardware algorithms that are based on the decomposition ofthese recursive equations, allowing calculation of up to four integral imagevalues in a row-parallel way without significantly increasing the number ofoperations. An efficient design strategy is also proposed for a parallelintegral image computation unit to reduce the size of the required internalmemory (nearly 35% for common HD video). Addressing the storage problem ofintegral image in embedded vision systems, the paper presents two algorithmswhich allow substantial decrease (at least 44.44%) in the memory requirements.Finally, the paper provides a case study that highlights the utility of theproposed architectures in embedded vision systems.
arxiv-12900-95 | Large Vocabulary Arabic Online Handwriting Recognition System | http://arxiv.org/pdf/1410.4688v3.pdf | author:Ibrahim Abdelaziz, Sherif Abdou, Hassanin Al-Barhamtoshy category:cs.CV published:2014-10-17 summary:Arabic handwriting is a consonantal and cursive writing. The analysis ofArabic script is further complicated due to obligatory dots/strokes that areplaced above or below most letters and usually written delayed in order. Due toambiguities and diversities of writing styles, recognition systems aregenerally based on a set of possible words called lexicon. When the lexicon issmall, recognition accuracy is more important as the recognition time isminimal. On the other hand, recognition speed as well as the accuracy are bothcritical when handling large lexicons. Arabic is rich in morphology and syntaxwhich makes its lexicon large. Therefore, a practical online handwritingrecognition system should be able to handle a large lexicon with reasonableperformance in terms of both accuracy and time. In this paper, we introduce afully-fledged Hidden Markov Model (HMM) based system for Arabic onlinehandwriting recognition that provides solutions for most of the difficultiesinherent in recognizing the Arabic script. A new preprocessing technique forhandling the delayed strokes is introduced. We use advanced modeling techniquesfor building our recognition system from the training data to provide moredetailed representation for the differences between the writing units, minimizethe variances between writers in the training data and have a betterrepresentation for the features space. System results are enhanced using anadditional post-processing step with a higher order language model andcross-word HMM models. The system performance is evaluated using two differentdatabases covering small and large lexicons. Our system outperforms thestate-of-art systems for the small lexicon database. Furthermore, it showspromising results (accuracy and time) when supporting large lexicon with thepossibility for adapting the models for specific writers to get even betterresults.
arxiv-12900-96 | A General Method for Robust Bayesian Modeling | http://arxiv.org/pdf/1510.05078v1.pdf | author:Chong Wang, David M. Blei category:stat.ML published:2015-10-17 summary:Robust Bayesian models are appealing alternatives to standard models,providing protection from data that contains outliers or other departures fromthe model assumptions. Historically, robust models were mostly developed on acase-by-case basis; examples include robust linear regression, robust mixturemodels, and bursty topic models. In this paper we develop a general approach torobust Bayesian modeling. We show how to turn an existing Bayesian model into arobust model, and then develop a generic strategy for computing with it. We useour method to study robust variants of several models, including linearregression, Poisson regression, logistic regression, and probabilistic topicmodels. We discuss the connections between our methods and existing approaches,especially empirical Bayes and James-Stein estimation.
arxiv-12900-97 | A Critical Review of Recurrent Neural Networks for Sequence Learning | http://arxiv.org/pdf/1506.00019v4.pdf | author:Zachary C. Lipton, John Berkowitz, Charles Elkan category:cs.LG cs.NE published:2015-05-29 summary:Countless learning tasks require dealing with sequential data. Imagecaptioning, speech synthesis, and music generation all require that a modelproduce outputs that are sequences. In other domains, such as time seriesprediction, video analysis, and musical information retrieval, a model mustlearn from inputs that are sequences. Interactive tasks, such as translatingnatural language, engaging in dialogue, and controlling a robot, often demandboth capabilities. Recurrent neural networks (RNNs) are connectionist modelsthat capture the dynamics of sequences via cycles in the network of nodes.Unlike standard feedforward neural networks, recurrent networks retain a statethat can represent information from an arbitrarily long context window.Although recurrent neural networks have traditionally been difficult to train,and often contain millions of parameters, recent advances in networkarchitectures, optimization techniques, and parallel computation have enabledsuccessful large-scale learning with them. In recent years, systems based onlong short-term memory (LSTM) and bidirectional (BRNN) architectures havedemonstrated ground-breaking performance on tasks as varied as imagecaptioning, language translation, and handwriting recognition. In this survey,we review and synthesize the research that over the past three decades firstyielded and then made practical these powerful learning models. Whenappropriate, we reconcile conflicting notation and nomenclature. Our goal is toprovide a self-contained explication of the state of the art together with ahistorical perspective and references to primary research.
arxiv-12900-98 | A Distance Measure for the Analysis of Polar Opinion Dynamics in Social Networks | http://arxiv.org/pdf/1510.05058v1.pdf | author:Victor Amelkin, Ambuj Singh, Petko Bogdanov category:cs.SI cs.DM cs.DS stat.ML published:2015-10-17 summary:Analysis of opinion dynamics in social networks plays an important role intoday's life. For applications such as predicting users' political preference,it is particularly important to be able to analyze the dynamics of competingopinions. While observing the evolution of polar opinions of a social network'susers over time, can we tell when the network "behaved" abnormally?Furthermore, can we predict how the opinions of the users will change in thefuture? Do opinions evolve according to existing network opinion dynamicsmodels? To answer such questions, it is not sufficient to study individual userbehavior, since opinions can spread far beyond users' egonets. We need a methodto analyze opinion dynamics of all network users simultaneously and capture theeffect of individuals' behavior on the global evolution pattern of the socialnetwork. In this work, we introduce Social Network Distance (SND) - a distance measurethat quantifies the "cost" of evolution of one snapshot of a social networkinto another snapshot under various models of polar opinion propagation. SNDhas a rich semantics of a transportation problem, yet, is computable in timelinear in the number of users, which makes SND applicable to the analysis oflarge-scale online social networks. In our experiments with synthetic andreal-world Twitter data, we demonstrate the utility of our distance measure foranomalous event detection. It achieves a true positive rate of 0.83, twice ashigh as that of alternatives. When employed for opinion prediction in Twitter,our method's accuracy is 75.63%, which is 7.5% higher than that of the nextbest method. Source Code: https://cs.ucsb.edu/~victor/pub/ucsb/dbl/snd/
arxiv-12900-99 | A cost function for similarity-based hierarchical clustering | http://arxiv.org/pdf/1510.05043v1.pdf | author:Sanjoy Dasgupta category:cs.DS cs.LG stat.ML published:2015-10-16 summary:The development of algorithms for hierarchical clustering has been hamperedby a shortage of precise objective functions. To help address this situation,we introduce a simple cost function on hierarchies over a set of points, givenpairwise similarities between those points. We show that this criterion behavessensibly in canonical instances and that it admits a top-down constructionprocedure with a provably good approximation ratio.
arxiv-12900-100 | Stats-Calculus Pose Descriptor Feeding A Discrete HMM Low-latency Detection and Recognition System For 3D Skeletal Actions | http://arxiv.org/pdf/1509.09014v4.pdf | author:Rofael Emil Fayez Behnam category:cs.CV published:2015-09-30 summary:Recognition of human actions, under low observational latency, is a growinginterest topic, nowadays. Many approaches have been represented based on aprovided set of 3D Cartesian coordinates system originated at a certainspecific point located on a root joint. In this paper, We will present astatistical detection and recognition system using Hidden Markov Model using 7types of pose descriptors. * Cartesian Calculus Pose descriptor. * AngularCalculus Pose descriptor. * Mixed-mode Stats-Calculus Pose descriptor. *Centro-Stats-Calculus Pose descriptor. * Rela-Centro-Stats-Calculus Posedescriptor. * Rela-Centro-Stats-Calculus DCT Pose descriptor. *Rela-Centro-Stats-Calculus DCT-AMDF Pose descriptor. Stats-Calculus is afeature extracting technique, that is developed on Moving Pose descriptor , butusing a combination of Statistics measures and Calculus measures.
arxiv-12900-101 | Normalization of Relative and Incomplete Temporal Expressions in Clinical Narratives | http://arxiv.org/pdf/1510.04972v1.pdf | author:Weiyi Sun, Anna Rumshisky, Ozlem Uzuner category:cs.CL cs.AI cs.IR published:2015-10-16 summary:We analyze the RI-TIMEXes in temporally annotated corpora and propose twohypotheses regarding the normalization of RI-TIMEXes in the clinical narrativedomain: the anchor point hypothesis and the anchor relation hypothesis. Weannotate the RI-TIMEXes in three corpora to study the characteristics ofRI-TMEXes in different domains. This informed the design of our RI-TIMEXnormalization system for the clinical domain, which consists of an anchor pointclassifier, an anchor relation classifier and a rule-based RI-TIMEX text spanparser. We experiment with different feature sets and perform error analysisfor each system component. The annotation confirmed the hypotheses that we cansimplify the RI-TIMEXes normalization task using two multi-label classifiers.Our system achieves anchor point classification, anchor relation classificationand rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% underrelaxed matching criteria) respectively on the held-out test set of the 2012i2b2 temporal relation challenge. Experiments with feature sets reveals someinteresting findings such as the verbal tense feature does not inform theanchor relation classification in clinical narratives as much as the tokensnear the RI-TIMEX. Error analysis shows that underrepresented anchor point andanchor relation classes are difficult to detect. We formulate the RI-TIMEXnormalization problem as a pair of multi-label classification problems.Considering only the RI-TIMEX extraction and normalization, the system achievesstatistically significant improvement over the RI-TIMEX results of the bestsystems in the 2012 i2b2 challenge.
arxiv-12900-102 | Optimizing and Contrasting Recurrent Neural Network Architectures | http://arxiv.org/pdf/1510.04953v1.pdf | author:Ben Krause category:stat.ML cs.LG cs.NE published:2015-10-16 summary:Recurrent Neural Networks (RNNs) have long been recognized for theirpotential to model complex time series. However, it remains to be determinedwhat optimization techniques and recurrent architectures can be used to bestrealize this potential. The experiments presented take a deep look into Hessianfree optimization, a powerful second order optimization method that has shownpromising results, but still does not enjoy widespread use. This algorithm wasused to train to a number of RNN architectures including standard RNNs, longshort-term memory, multiplicative RNNs, and stacked RNNs on the task ofcharacter prediction. The insights from these experiments led to the creationof a new multiplicative LSTM hybrid architecture that outperformed both LSTMand multiplicative RNNs. When tested on a larger scale, multiplicative LSTMachieved character level modelling results competitive with the state of theart for RNNs using very different methodology.
arxiv-12900-103 | Does network complexity help organize Babel's library? | http://arxiv.org/pdf/1409.7336v2.pdf | author:Juan Pablo Cárdenas, Iván González, Gerardo Vidal, Miguel Fuentes category:physics.soc-ph cs.CL nlin.AO published:2014-09-23 summary:In this work, we study properties of texts from the perspective of complexnetwork theory. Words in given texts are linked by co-occurrence andtransformed into networks, and we observe that these display topologicalproperties common to other complex systems. However, there are some propertiesthat seem to be exclusive to texts; many of these properties depend on thefrequency of words in the text, while others seem to be strictly determined bythe grammar. Precisely, these properties allow for a categorization of texts aseither with a sense and others encoded or senseless.
arxiv-12900-104 | Bad Universal Priors and Notions of Optimality | http://arxiv.org/pdf/1510.04931v1.pdf | author:Jan Leike, Marcus Hutter category:cs.AI cs.LG published:2015-10-16 summary:A big open question of algorithmic information theory is the choice of theuniversal Turing machine (UTM). For Kolmogorov complexity and Solomonoffinduction we have invariance theorems: the choice of the UTM changes boundsonly by a constant. For the universally intelligent agent AIXI (Hutter, 2005)no invariance theorem is known. Our results are entirely negative: we discusscases in which unlucky or adversarial choices of the UTM cause AIXI tomisbehave drastically. We show that Legg-Hutter intelligence and thus balancedPareto optimality is entirely subjective, and that every policy is Paretooptimal in the class of all computable environments. This undermines allexisting optimality properties for AIXI. While it may still serve as a goldstandard for AI, our results imply that AIXI is a relative theory, dependent onthe choice of the UTM.
arxiv-12900-105 | Characterizing predictable classes of processes | http://arxiv.org/pdf/1408.2036v2.pdf | author:Daniil Ryabko category:cs.LG stat.ML published:2014-08-09 summary:The problem is sequence prediction in the following setting. A sequencex1,..., xn,... of discrete-valued observations is generated according to someunknown probabilistic law (measure) mu. After observing each outcome, it isrequired to give the conditional probabilities of the next observation. Themeasure mu belongs to an arbitrary class C of stochastic processes. We areinterested in predictors ? whose conditional probabilities converge to the'true' mu-conditional probabilities if any mu { C is chosen to generate thedata. We show that if such a predictor exists, then a predictor can also beobtained as a convex combination of a countably many elements of C. In otherwords, it can be obtained as a Bayesian predictor whose prior is concentratedon a countable set. This result is established for two very different measuresof performance of prediction, one of which is very strong, namely, totalvariation, and the other is very weak, namely, prediction in expected averageKullback-Leibler divergence.
arxiv-12900-106 | No Spare Parts: Sharing Part Detectors for Image Categorization | http://arxiv.org/pdf/1510.04908v1.pdf | author:Pascal Mettes, Jan C. van Gemert, Cees G. M. Snoek category:cs.CV published:2015-10-16 summary:This work aims for image categorization using a representation of distinctiveparts. Different from existing part-based work, we argue that parts arenaturally shared between image categories and should be modeled as such. Wemotivate our approach with a quantitative and qualitative analysis bybacktracking where selected parts come from. Our analysis shows that inaddition to the category parts defining the class, the parts coming from thebackground context and parts from other image categories improve categorizationperformance. Part selection should not be done separately for each category,but instead be shared and optimized over all categories. To incorporate partsharing between categories, we present an algorithm based on AdaBoost tojointly optimize part sharing and selection, as well as fusion with the globalimage representation. We achieve results competitive to the state-of-the-art onobject, scene, and action categories, further improving over deep convolutionalneural networks.
arxiv-12900-107 | Robust Partially-Compressed Least-Squares | http://arxiv.org/pdf/1510.04905v1.pdf | author:Stephen Becker, Ban Kawas, Marek Petrik, Karthikeyan N. Ramamurthy category:stat.ML cs.LG published:2015-10-16 summary:Randomized matrix compression techniques, such as the Johnson-Lindenstrausstransform, have emerged as an effective and practical way for solvinglarge-scale problems efficiently. With a focus on computational efficiency,however, forsaking solutions quality and accuracy becomes the trade-off. Inthis paper, we investigate compressed least-squares problems and propose newmodels and algorithms that address the issue of error and noise introduced bycompression. While maintaining computational efficiency, our models providerobust solutions that are more accurate--relative to solutions of uncompressedleast-squares--than those of classical compressed variants. We introduce toolsfrom robust optimization together with a form of partial compression to improvethe error-time trade-offs of compressed least-squares solvers. We develop anefficient solution algorithm for our Robust Partially-Compressed (RPC) modelbased on a reduction to a one-dimensional search. We also derive the firstapproximation error bounds for Partially-Compressed least-squares solutions.Empirical results comparing numerous alternatives suggest that robust andpartially compressed solutions are effectively insulated against aggressiverandomized transforms.
arxiv-12900-108 | Topic-adjusted visibility metric for scientific articles | http://arxiv.org/pdf/1502.07190v3.pdf | author:Linda S. L. Tan, Aik Hui Chan, Tian Zheng category:stat.ML cs.LG published:2015-02-25 summary:Measuring the impact of scientific articles is important for evaluating theresearch output of individual scientists, academic institutions and journals.While citations are raw data for constructing impact measures, there existbiases and potential issues if factors affecting citation patterns are notproperly accounted for. In this work, we address the problem of field variationand introduce an article level metric useful for evaluating individualarticles' visibility. This measure derives from joint probabilistic modeling ofthe content in the articles and the citations amongst them using latentDirichlet allocation (LDA) and the mixed membership stochastic blockmodel(MMSB). Our proposed model provides a visibility metric for individual articlesadjusted for field variation in citation rates, a structural understanding ofcitation behavior in different fields, and article recommendations which takeinto account article visibility and citation patterns. We develop an efficientalgorithm for model fitting using variational methods. To scale up to largenetworks, we develop an online variant using stochastic gradient methods andcase-control likelihood approximation. We apply our methods to the benchmarkKDD Cup 2003 dataset with approximately 30,000 high energy physics papers.
arxiv-12900-109 | An Extension to Hough Transform Based on Gradient Orientation | http://arxiv.org/pdf/1510.04863v1.pdf | author:Tomislav Petković, Sven Lončarić category:cs.CV published:2015-10-16 summary:The Hough transform is one of the most common methods for line detection. Inthis paper we propose a novel extension of the regular Hough transform. Theproposed extension combines the extension of the accumulator space and thelocal gradient orientation resulting in clutter reduction and yielding moreprominent peaks, thus enabling better line identification. We demonstratebenefits in applications such as visual quality inspection and rectangledetection.
arxiv-12900-110 | Towards Reversible De-Identification in Video Sequences Using 3D Avatars and Steganography | http://arxiv.org/pdf/1510.04861v1.pdf | author:Martin Blažević, Karla Brkić, Tomislav Hrkać category:cs.CV cs.MM published:2015-10-16 summary:We propose a de-identification pipeline that protects the privacy of humansin video sequences by replacing them with rendered 3D human models, henceconcealing their identity while retaining the naturalness of the scene. Theoriginal images of humans are steganographically encoded in the carrier image,i.e. the image containing the original scene and the rendered 3D human models.We qualitatively explore the feasibility of our approach, utilizing the Kinectsensor and its libraries to detect and localize human joints. A 3D avatar isrendered into the scene using the obtained joint positions, and the originalhuman image is steganographically encoded in the new scene. Our qualitativeevaluation shows reasonably good results that merit further exploration.
arxiv-12900-111 | Measurement of Road Traffic Parameters Based on Multi-Vehicle Tracking | http://arxiv.org/pdf/1510.04860v1.pdf | author:Kristian Kovačić, Edouard Ivanjko, Niko Jelušić category:cs.CV published:2015-10-16 summary:Development of computing power and cheap video cameras enabled today'straffic management systems to include more cameras and computer visionapplications for transportation system monitoring and control. Combined withimage processing algorithms cameras are used as sensors to measure road trafficparameters like flow volume, origin-destination matrices, classify vehicles,etc. In this paper we propose a system for measurement of road trafficparameters (basic motion model parameters and macro-scopic traffic parameters).The system is based on Local Binary Pattern (LBP) image features classificationwith a cascade of Gentle Adaboost (GAB) classifiers to determine vehicleexistence and its location in an image. Additionally, vehicle tracking andcounting in a road traffic video is performed by using Extended Kalman Filter(EKF) and virtual markers. The newly proposed system is compared with a systembased on background subtraction. Comparison is performed by the means ofexecution time and accuracy.
arxiv-12900-112 | Adaptive Smoothing Algorithms for Nonsmooth Composite Convex Minimization | http://arxiv.org/pdf/1509.00106v4.pdf | author:Quoc Tran-Dinh category:math.OC stat.ML published:2015-09-01 summary:We propose novel adaptive smoothing algorithms based on Nesterov's smoothingtechnique in [26] for solving nonsmooth composite convex optimization problems.Our methods combine both Nesterov's accelerated proximal gradient scheme and anew homotopy strategy for smoothness parameter. By an appropriate choice ofsmoothing functions, we develop new algorithms that have upto the$\mathcal{O}\left(\frac{1}{\varepsilon}\right)$-optimal worst-case iterationcomplexity while allow one to automatically update the smoothness parameter ateach iteration. We then further exploit the structure of problems to selectsmoothing functions and develop suitable algorithmic variants that reduce thecomplexity-per-iteration. We also specify our algorithms to solve constrainedconvex optimization problems and show their convergence guarantee on the primalsequence of iterates. We demonstrate our algorithms through three numericalexamples and compare them with the nonadaptive algorithm in [26].
arxiv-12900-113 | Multiresolution hierarchy co-clustering for semantic segmentation in sequences with small variations | http://arxiv.org/pdf/1510.04842v1.pdf | author:David Varas, Mónica Alfaro, Ferran Marques category:cs.CV published:2015-10-16 summary:This paper presents a co-clustering technique that, given a collection ofimages and their hierarchies, clusters nodes from these hierarchies to obtain acoherent multiresolution representation of the image collection. We formalizethe co-clustering as a Quadratic Semi-Assignment Problem and solve it with alinear programming relaxation approach that makes effective use of informationfrom hierarchies. Initially, we address the problem of generating an optimal,coherent partition per image and, afterwards, we extend this method to amultiresolution framework. Finally, we particularize this framework to aniterative multiresolution video segmentation algorithm in sequences with smallvariations. We evaluate the algorithm on the Video Occlusion/Object BoundaryDetection Dataset, showing that it produces state-of-the-art results in thesescenarios.
arxiv-12900-114 | Learning A Task-Specific Deep Architecture For Clustering | http://arxiv.org/pdf/1509.00151v3.pdf | author:Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, Thomas S. Huang category:cs.LG cs.CV stat.ML published:2015-09-01 summary:While sparse coding-based clustering methods have shown to be successful,their bottlenecks in both efficiency and scalability limit the practical usage.In recent years, deep learning has been proved to be a highly effective,efficient and scalable feature learning tool. In this paper, we propose toemulate the sparse coding-based clustering pipeline in the context of deeplearning, leading to a carefully crafted deep model benefiting from both. Afeed-forward network structure, named TAGnet, is constructed based on agraph-regularized sparse coding algorithm. It is then trained withtask-specific loss functions from end to end. We discover that connecting deeplearning to sparse coding benefits not only the model performance, but also itsinitialization and interpretation. Moreover, by introducing auxiliaryclustering tasks to the intermediate feature hierarchy, we formulate DTAGnetand obtain a further performance boost. Extensive experiments demonstrate thatthe proposed model gains remarkable margins over several state-of-the-artmethods.
arxiv-12900-115 | A Graph Traversal Based Approach to Answer Non-Aggregation Questions Over DBpedia | http://arxiv.org/pdf/1510.04780v1.pdf | author:Chenhao Zhu, Kan Ren, Xuan Liu, Haofen Wang, Yiding Tian, Yong Yu category:cs.CL cs.IR published:2015-10-16 summary:We present a question answering system over DBpedia, filling the gap betweenuser information needs expressed in natural language and a structured queryinterface expressed in SPARQL over the underlying knowledge base (KB). Giventhe KB, our goal is to comprehend a natural language query and providecorresponding accurate answers. Focusing on solving the non-aggregationquestions, in this paper, we construct a subgraph of the knowledge base fromthe detected entities and propose a graph traversal method to solve both thesemantic item mapping problem and the disambiguation problem in a joint way.Compared with existing work, we simplify the process of query intentionunderstanding and pay more attention to the answer path ranking. We evaluateour method on a non-aggregation question dataset and further on a completedataset. Experimental results show that our method achieves best performancecompared with several state-of-the-art systems.
arxiv-12900-116 | Active Learning from Weak and Strong Labelers | http://arxiv.org/pdf/1510.02847v2.pdf | author:Chicheng Zhang, Kamalika Chaudhuri category:cs.LG stat.ML published:2015-10-09 summary:An active learner is given a hypothesis class, a large set of unlabeledexamples and the ability to interactively query labels to an oracle of a subsetof these examples; the goal of the learner is to learn a hypothesis in theclass that fits the data well by making as few label queries as possible. This work addresses active learning with labels obtained from strong and weaklabelers, where in addition to the standard active learning setting, we have anextra weak labeler which may occasionally provide incorrect labels. An exampleis learning to classify medical images where either expensive labels may beobtained from a physician (oracle or strong labeler), or cheaper butoccasionally incorrect labels may be obtained from a medical resident (weaklabeler). Our goal is to learn a classifier with low error on data labeled bythe oracle, while using the weak labeler to reduce the number of label queriesmade to this labeler. We provide an active learning algorithm for this setting,establish its statistical consistency, and analyze its label complexity tocharacterize when it can provide label savings over using the strong labeleralone.
arxiv-12900-117 | A Method for Modeling Co-Occurrence Propensity of Clinical Codes with Application to ICD-10-PCS Auto-Coding | http://arxiv.org/pdf/1510.04734v1.pdf | author:Michael Subotin, Anthony R. Davis category:cs.CL published:2015-10-15 summary:Objective. Natural language processing methods for medical auto-coding, orautomatic generation of medical billing codes from electronic health records,generally assign each code independently of the others. They may thus assigncodes for closely related procedures or diagnoses to the same document, evenwhen they do not tend to occur together in practice, simply because the rightchoice can be difficult to infer from the clinical narrative. Materials and Methods. We propose a method that injects awareness of thepropensities for code co-occurrence into this process. First, a model istrained to estimate the conditional probability that one code is assigned by ahuman coder, given than another code is known to have been assigned to the samedocument. Then, at runtime, an iterative algorithm is used to apply this modelto the output of an existing statistical auto-coder to modify the confidencescores of the codes. Results. We tested this method in combination with a primary auto-coder forICD-10 procedure codes, achieving a 12% relative improvement in F-score overthe primary auto-coder baseline. Discussion. The proposed method can be used, with appropriate features, incombination with any auto-coder that generates codes with different levels ofconfidence. Conclusion. The promising results obtained for ICD-10 procedure codes suggestthat the proposed method may have wider applications in auto-coding.
arxiv-12900-118 | Shape Complexes in Continuous Max-Flow Hierarchical Multi-Labeling Problems | http://arxiv.org/pdf/1510.04706v1.pdf | author:John S. H. Baxter, Jing Yuan, Terry M. Peters category:cs.CV published:2015-10-15 summary:Although topological considerations amongst multiple labels have beenpreviously investigated in the context of continuous max-flow imagesegmentation, similar investigations have yet to be made about shapeconsiderations in a general and extendable manner. This paper presents shapecomplexes for segmentation, which capture more complex shapes by combiningmultiple labels and super-labels constrained by geodesic star convexity. Shapecomplexes combine geodesic star convexity constraints with hierarchical labelorganization, which together allow for more complex shapes to be represented.This framework avoids the use of co-ordinate system warping techniques toconvert shape constraints into topological constraints, which may be ambiguousor ill-defined for certain segmentation problems.
arxiv-12900-119 | Deep Networks for Image Super-Resolution with Sparse Prior | http://arxiv.org/pdf/1507.08905v4.pdf | author:Zhaowen Wang, Ding Liu, Jianchao Yang, Wei Han, Thomas Huang category:cs.CV published:2015-07-31 summary:Deep learning techniques have been successfully applied in many areas ofcomputer vision, including low-level image restoration problems. For imagesuper-resolution, several models based on deep neural networks have beenrecently proposed and attained superior performance that overshadows allprevious handcrafted models. The question then arises whether large-capacityand data-driven models have become the dominant solution to the ill-posedsuper-resolution problem. In this paper, we argue that domain expertiserepresented by the conventional sparse coding model is still valuable, and itcan be combined with the key ingredients of deep learning to achieve furtherimproved results. We show that a sparse coding model particularly designed forsuper-resolution can be incarnated as a neural network, and trained in acascaded structure from end to end. The interpretation of the network based onsparse coding leads to much more efficient and effective training, as well as areduced model size. Our model is evaluated on a wide range of images, and showsclear advantage over existing state-of-the-art methods in terms of bothrestoration accuracy and human subjective quality.
arxiv-12900-120 | Layer-Specific Adaptive Learning Rates for Deep Networks | http://arxiv.org/pdf/1510.04609v1.pdf | author:Bharat Singh, Soham De, Yangmuzi Zhang, Thomas Goldstein, Gavin Taylor category:cs.CV cs.AI cs.LG cs.NE published:2015-10-15 summary:The increasing complexity of deep learning architectures is resulting intraining time requiring weeks or even months. This slow training is due in partto vanishing gradients, in which the gradients used by back-propagation areextremely large for weights connecting deep layers (layers near the outputlayer), and extremely small for shallow layers (near the input layer); thisresults in slow learning in the shallow layers. Additionally, it has also beenshown that in highly non-convex problems, such as deep neural networks, thereis a proliferation of high-error low curvature saddle points, which slows downlearning dramatically. In this paper, we attempt to overcome the two aboveproblems by proposing an optimization method for training deep neural networkswhich uses learning rates which are both specific to each layer in the networkand adaptive to the curvature of the function, increasing the learning rate atlow curvature points. This enables us to speed up learning in the shallowlayers of the network and quickly escape high-error low curvature saddlepoints. We test our method on standard image classification datasets such asMNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracyas well as reduces the required training time over standard algorithms.
arxiv-12900-121 | Telemedicine as a special case of Machine Translation | http://arxiv.org/pdf/1510.04600v1.pdf | author:Krzysztof Wołk, Krzysztof Marasek, Wojciech Glinkowski category:cs.CL published:2015-10-15 summary:Machine translation is evolving quite rapidly in terms of quality. Nowadays,we have several machine translation systems available in the web, which providereasonable translations. However, these systems are not perfect, and theirquality may decrease in some specific domains. This paper examines the effectsof different training methods when it comes to Polish - English StatisticalMachine Translation system used for the medical data. Numerous elements of theEMEA parallel text corpora and not related OPUS Open Subtitles project wereused as the ground for creation of phrase tables and different language modelsincluding the development, tuning and testing of these translation systems. TheBLEU, NIST, METEOR, and TER metrics have been used in order to evaluate theresults of various systems. Our experiments deal with the systems that includePOS tagging, factored phrase models, hierarchical models, syntactic taggers,and other alignment methods. We also executed a deep analysis of Polish data aspreparatory work before automatized data processing such as true casing orpunctuation normalization phase. Normalized metrics was used to compareresults. Scores lower than 15% mean that Machine Translation engine is unableto provide satisfying quality, scores greater than 30% mean that translationsshould be understandable without problems and scores over 50 reflect adequatetranslations. The average results of Polish to English translations scores forBLEU, NIST, METEOR, and TER were relatively high and ranged from 70,58 to82,72. The lowest score was 64,38. The average results ranges for English toPolish translations were little lower (67,58 - 78,97). The real-lifeimplementations of presented high quality Machine Translation Systems areanticipated in general medical practice and telemedicine.
arxiv-12900-122 | Recognizing Fine-Grained and Composite Activities using Hand-Centric Features and Script Data | http://arxiv.org/pdf/1502.06648v2.pdf | author:Marcus Rohrbach, Anna Rohrbach, Michaela Regneri, Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, Bernt Schiele category:cs.CV published:2015-02-23 summary:Activity recognition has shown impressive progress in recent years. However,the challenges of detecting fine-grained activities and understanding how theyare combined into composite activities have been largely overlooked. In thiswork we approach both tasks and present a dataset which provides detailedannotations to address them. The first challenge is to detect fine-grainedactivities, which are defined by low inter-class variability and are typicallycharacterized by fine-grained body motions. We explore how human pose and handscan help to approach this challenge by comparing two pose-based and twohand-centric features with state-of-the-art holistic features. To attack thesecond challenge, recognizing composite activities, we leverage the fact thatthese activities are compositional and that the essential components of theactivities can be obtained from textual descriptions or scripts. We show thebenefits of our hand-centric approach for fine-grained activity classificationand detection. For composite activity recognition we find that decompositioninto attributes allows sharing information across composites and is essentialto attack this hard task. Using script data we can recognize novel compositeswithout having training data for them.
arxiv-12900-123 | A Brief Survey of Image Processing Algorithms in Electrical Capacitance Tomography | http://arxiv.org/pdf/1510.04585v1.pdf | author:Kezhi Li category:cs.CV published:2015-10-15 summary:To study the fundamental physics of complex multiphase flow systems usingadvanced measurement techniques, especially the electrical capacitancetomography (ECT) approach, this article carries out an initial literaturereview of the ECT method from a point of view of signal processing andalgorithm design. After introducing the physical laws governing the ECT system,we will focus on various reconstruction techniques that are capable to recoverthe image of the internal characteristics of a specified region based on themeasuring capacitances of multi-electrode sensors surrounding the region. Eachtechnique has its own advantages and limitations, and many algorithms have beenexamined by simulations or experiments. Future researches in 3D reconstructionand other potential improvements of the system are discussed in the end.
arxiv-12900-124 | Beyond Spatial Pyramid Matching: Space-time Extended Descriptor for Action Recognition | http://arxiv.org/pdf/1510.04565v1.pdf | author:Zhenzhong Lan, Alexander G. Hauptmann category:cs.CV published:2015-10-15 summary:We address the problem of generating video features for action recognition.The spatial pyramid and its variants have been very popular feature models dueto their success in balancing spatial location encoding and spatial invariance.Although it seems straightforward to extend spatial pyramid to the temporaldomain (spatio-temporal pyramid), the large spatio-temporal diversity ofunconstrained videos and the resulting significantly higher dimensionalrepresentations make it less appealing. This paper introduces the space-timeextended descriptor, a simple but efficient alternative way to include thespatio-temporal location into the video features. Instead of only coding motioninformation and leaving the spatio-temporal location to be represented at thepooling stage, location information is used as part of the encoding step. Thismethod is a much more effective and efficient location encoding method ascompared to the fixed grid model because it avoids the danger of overcommitting to artificial boundaries and its dimension is relatively low.Experimental results on several benchmark datasets show that, despite itssimplicity, this method achieves comparable or better results thanspatio-temporal pyramid.
arxiv-12900-125 | Elasticity-based Matching by Minimizing the Symmetric Difference of Shapes | http://arxiv.org/pdf/1510.04563v1.pdf | author:Konrad Simon, Ronen Basri category:cs.CV cs.CG published:2015-10-15 summary:We consider the problem of matching two shapes assuming these shapes arerelated by an elastic deformation. Using linearized elasticity theory and thefinite element method we seek an elastic deformation that is caused by simpleexternal boundary forces and accounts for the difference between the twoshapes. Our main contribution is in proposing a cost function and anoptimization procedure to minimize the symmetric difference between thedeformed and the target shapes as an alternative to point matches that guidethe matching in other techniques. We show how to approximate the nonlinearoptimization problem by a sequence of convex problems. We demonstrate theutility of our method in experiments and compare it to an ICP-like matchingalgorithm.
arxiv-12900-126 | Noisy-parallel and comparable corpora filtering methodology for the extraction of bi-lingual equivalent data at sentence level | http://arxiv.org/pdf/1510.04500v1.pdf | author:Krzysztof Wołk category:cs.CL published:2015-10-15 summary:Text alignment and text quality are critical to the accuracy of MachineTranslation (MT) systems, some NLP tools, and any other text processing tasksrequiring bilingual data. This research proposes a language independentbi-sentence filtering approach based on Polish (not a position-sensitivelanguage) to English experiments. This cleaning approach was developed on theTED Talks corpus and also initially tested on the Wikipedia comparable corpus,but it can be used for any text domain or language pair. The proposed approachimplements various heuristics for sentence comparison. Some of them leveragesynonyms and semantic and structural analysis of text as additionalinformation. Minimization of data loss was ensured. An improvement in MT systemscore with text processed using the tool is discussed.
arxiv-12900-127 | Sparsity-aware Possibilistic Clustering Algorithms | http://arxiv.org/pdf/1510.04493v1.pdf | author:Spyridoula D. Xenaki, Konstantinos D. Koutroumbas, Athanasios A. Rontogiannis category:cs.CV published:2015-10-15 summary:In this paper two novel possibilistic clustering algorithms are presented,which utilize the concept of sparsity. The first one, called sparsepossibilistic c-means, exploits sparsity and can deal well with closely locatedclusters that may also be of significantly different densities. The second one,called sparse adaptive possibilistic c-means, is an extension of the first,where now the involved parameters are dynamically adapted. The latter can dealwell with even more challenging cases, where, in addition to the above,clusters may be of significantly different variances. More specifically, itprovides improved estimates of the cluster representatives, while, in addition,it has the ability to estimate the actual number of clusters, given anoverestimate of it. Extensive experimental results on both synthetic and realdata sets support the previous statements.
arxiv-12900-128 | Some Theorems for Feed Forward Neural Networks | http://arxiv.org/pdf/1509.05177v4.pdf | author:K. Eswaran, Vishwajeet Singh category:cs.NE 62M45 published:2015-09-17 summary:In this paper we introduce a new method which employs the concept of"Orientation Vectors" to train a feed forward neural network and suitable forproblems where large dimensions are involved and the clusters arecharacteristically sparse. The new method is not NP hard as the problem sizeincreases. We `derive' the method by starting from Kolmogrov's method and thenrelax some of the stringent conditions. We show for most classificationproblems three layers are sufficient and the network size depends on the numberof clusters. We prove as the number of clusters increase from N to N+dN thenumber of processing elements in the first layer only increases by d(logN), andare proportional to the number of classes, and the method is not NP hard. Many examples are solved to demonstrate that the method of OrientationVectors requires much less computational effort than Radial Basis Functionmethods and other techniques wherein distance computations are required, infact the present method increases logarithmically with problem size compared tothe Radial Basis Function method and the other methods which depend on distancecomputations e.g statistical methods where probabilistic distances arecalculated. A practical method of applying the concept of Occum's razor tochoose between two architectures which solve the same classification problemhas been described. The ramifications of the above findings on the field ofDeep Learning have also been briefly investigated and we have found that itdirectly leads to the existence of certain types of NN architectures which canbe used as a "mapping engine", which has the property of "invertibility", thusimproving the prospect of their deployment for solving problems involving DeepLearning and hierarchical classification. The latter possibility has a lot offuture scope in the areas of machine learning and cloud computing.
arxiv-12900-129 | A Novel Adaptive Possibilistic Clustering Algorithm | http://arxiv.org/pdf/1412.3613v3.pdf | author:Spyridoula D. Xenaki, Konstantinos D. Koutroumbas, Athanasios A. Rontogiannis category:cs.CV published:2014-12-11 summary:In this paper a novel possibilistic c-means clustering algorithm, calledAdaptive Possibilistic c-means, is presented. Its main feature is that {\itall} its parameters, after their initialization, are properly adapted duringits execution. Provided that the algorithm starts with a reasonableoverestimate of the number of physical clusters formed by the data, it iscapable, in principle, to unravel them (a long-standing issue in the clusteringliterature). This is due to the fully adaptive nature of the proposed algorithmthat enables the removal of the clusters that gradually become obsolete. Inaddition, the adaptation of all its parameters increases the flexibility of thealgorithm in following the variations in the formation of the clusters thatoccur from iteration to iteration. Theoretical results that are indicative ofthe convergence behavior of the algorithm are also provided. Finally, extensivesimulation results on both synthetic and real data highlight the effectivenessof the proposed algorithm.
arxiv-12900-130 | Online Markov decision processes with policy iteration | http://arxiv.org/pdf/1510.04454v1.pdf | author:Yao Ma, Hao Zhang, Masashi Sugiyama category:cs.LG published:2015-10-15 summary:The online Markov decision process (MDP) is a generalization of the classicalMarkov decision process that incorporates changing reward functions. In thispaper, we propose practical online MDP algorithms with policy iteration andtheoretically establish a sublinear regret bound. A notable advantage of theproposed algorithm is that it can be easily combined with functionapproximation, and thus large and possibly continuous state spaces can beefficiently handled. Through experiments, we demonstrate the usefulness of theproposed algorithm.
arxiv-12900-131 | Distributed Deep Q-Learning | http://arxiv.org/pdf/1508.04186v2.pdf | author:Hao Yi Ong, Kevin Chavez, Augustus Hong category:cs.LG cs.AI cs.DC cs.NE published:2015-08-18 summary:We propose a distributed deep learning model to successfully learn controlpolicies directly from high-dimensional sensory input using reinforcementlearning. The model is based on the deep Q-network, a convolutional neuralnetwork trained with a variant of Q-learning. Its input is raw pixels and itsoutput is a value function estimating future rewards from taking an actiongiven a system state. To distribute the deep Q-network training, we adapt theDistBelief software framework to the context of efficiently trainingreinforcement learning agents. As a result, the method is completelyasynchronous and scales well with the number of machines. We demonstrate thatthe deep Q-network agent, receiving only the pixels and the game score asinputs, was able to achieve reasonable success on a simple game with minimalparameter tuning.
arxiv-12900-132 | DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers | http://arxiv.org/pdf/1510.04445v1.pdf | author:Amir Ghodrati, Ali Diba, Marco Pedersoli, Tinne Tuytelaars, Luc Van Gool category:cs.CV published:2015-10-15 summary:In this paper we evaluate the quality of the activation layers of aconvolutional neural network (CNN) for the gen- eration of object proposals. Wegenerate hypotheses in a sliding-window fashion over different activationlayers and show that the final convolutional layers can find the object ofinterest with high recall but poor localization due to the coarseness of thefeature maps. Instead, the first layers of the network can better localize theobject of interest but with a reduced recall. Based on this observation wedesign a method for proposing object locations that is based on CNN featuresand that combines the best of both worlds. We build an inverse cascade that,going from the final to the initial convolutional layers of the CNN, selectsthe most promising object locations and refines their boxes in a coarse-to-finemanner. The method is efficient, because i) it uses the same features extractedfor detection, ii) it aggregates features using integral images, and iii) itavoids a dense evaluation of the proposals due to the inverse coarse-to-finecascade. The method is also accurate; it outperforms most of the previouslyproposed object proposals approaches and when plugged into a CNN-based detectorproduces state-of-the- art detection performance.
arxiv-12900-133 | A Novel Approach for Human Action Recognition from Silhouette Images | http://arxiv.org/pdf/1510.04437v1.pdf | author:Satyabrata Maity, Debotosh Bhattacharjee, Amlan Chakrabarti category:cs.CV published:2015-10-15 summary:In this paper, a novel human action recognition technique from video ispresented. Any action of human is a combination of several micro actionsequences performed by one or more body parts of the human. The proposedapproach uses spatio-temporal body parts movement (STBPM) features extractedfrom foreground silhouette of the human objects. The newly proposed STBPMfeature estimates the movements of different body parts for any given timesegment to classify actions. We also proposed a rule based logic named ruleaction classifier (RAC), which uses a series of condition action rules based onprior knowledge and hence does not required training to classify any action.Since we don't require training to classify actions, the proposed approach isview independent. The experimental results on publicly available Wizeman andMuHVAi datasets are compared with that of the related research work in terms ofaccuracy in the human action detection, and proposed technique outperforms theothers.
arxiv-12900-134 | On Semiparametric Exponential Family Graphical Models | http://arxiv.org/pdf/1412.8697v2.pdf | author:Zhuoran Yang, Yang Ning, Han Liu category:stat.ML published:2014-12-30 summary:We propose a new class of semiparametric exponential family graphical modelsfor the analysis of high dimensional mixed data. Different from the existingmixed graphical models, we allow the nodewise conditional distributions to besemiparametric generalized linear models with unspecified base measurefunctions. Thus, one advantage of our method is that it is unnecessary tospecify the type of each node and the method is more convenient to apply inpractice. Under the proposed model, we consider both problems of parameterestimation and hypothesis testing in high dimensions. In particular, we proposea symmetric pairwise score test for the presence of a single edge in the graph.Compared to the existing methods for hypothesis tests, our approach takes intoaccount of the symmetry of the parameters, such that the inferential resultsare invariant with respect to the different parametrizations of the same edge.Thorough numerical simulations and a real data example are provided to back upour results.
arxiv-12900-135 | Filtrated Spectral Algebraic Subspace Clustering | http://arxiv.org/pdf/1510.04396v1.pdf | author:Manolis C. Tsakiris, Rene Vidal category:cs.CV cs.LG published:2015-10-15 summary:Algebraic Subspace Clustering (ASC) is a simple and elegant method based onpolynomial fitting and differentiation for clustering noiseless data drawn froman arbitrary union of subspaces. In practice, however, ASC is limited toequi-dimensional subspaces because the estimation of the subspace dimension viaalgebraic methods is sensitive to noise. This paper proposes a new ASCalgorithm that can handle noisy data drawn from subspaces of arbitrarydimensions. The key ideas are (1) to construct, at each point, a decreasingsequence of subspaces containing the subspace passing through that point; (2)to use the distances from any other point to each subspace in the sequence toconstruct a subspace clustering affinity, which is superior to alternativeaffinities both in theory and in practice. Experiments on the Hopkins 155dataset demonstrate the superiority of the proposed method with respect tosparse and low rank subspace clustering methods.
arxiv-12900-136 | Dual Principal Component Pursuit | http://arxiv.org/pdf/1510.04390v1.pdf | author:Manolis C. Tsakiris, Rene Vidal category:cs.CV cs.LG published:2015-10-15 summary:We consider the problem of outlier rejection in single subspace learning.Classical approaches work directly with a low-dimensional representation of thesubspace. Our approach works with a dual representation of the subspace andhence aims to find its orthogonal complement. We pose this problem as an$\ell_1$-minimization problem on the sphere and show that, under certainconditions on the distribution of the data, any global minimizer of thisnon-convex problem gives a vector orthogonal to the subspace. Moreover, we showthat such a vector can still be found by relaxing the non-convex problem with asequence of linear programs. Experiments on synthetic and real data show thatthe proposed approach, which we call Dual Principal Component Pursuit (DPCP),outperforms state-of-the art methods, especially in the case ofhigh-dimensional subspaces.
arxiv-12900-137 | Sketch-based Manga Retrieval using Manga109 Dataset | http://arxiv.org/pdf/1510.04389v1.pdf | author:Yusuke Matsui, Kota Ito, Yuji Aramaki, Toshihiko Yamasaki, Kiyoharu Aizawa category:cs.CV cs.IR cs.MM published:2015-10-15 summary:Manga (Japanese comics) are popular worldwide. However, current e-mangaarchives offer very limited search support, including keyword-based search bytitle or author, or tag-based categorization. To make the manga searchexperience more intuitive, efficient, and enjoyable, we propose a content-basedmanga retrieval system. First, we propose a manga-specific image-describingframework. It consists of efficient margin labeling, edge orientation histogramfeature description, and approximate nearest-neighbor search using productquantization. Second, we propose a sketch-based interface as a natural way tointeract with manga content. The interface provides sketch-based querying,relevance feedback, and query retouch. For evaluation, we built a novel datasetof manga images, Manga109, which consists of 109 comic books of 21,142 pagesdrawn by professional manga artists. To the best of our knowledge, Manga109 iscurrently the biggest dataset of manga images available for research. Weconducted a comparative study, a localization evaluation, and a large-scalequalitative study. From the experiments, we verified that: (1) the retrievalaccuracy of the proposed method is higher than those of previous methods; (2)the proposed method can localize an object instance with reasonable runtime andaccuracy; and (3) sketch querying is useful for manga search.
arxiv-12900-138 | Robust Learning for Optimal Treatment Decision with NP-Dimensionality | http://arxiv.org/pdf/1510.04378v1.pdf | author:Chengchun Shi, Rui Song, Wenbin Lu category:stat.ML published:2015-10-15 summary:In order to identify important variables that are involved in making optimaltreatment decision, Lu et al. (2013) proposed a penalized least squaredregression framework for a fixed number of predictors, which is robust againstthe misspecification of the conditional mean model. Two problems arise: (i) ina world of explosively big data, effective methods are needed to handleultra-high dimensional data set, for example, with the dimension of predictorsis of the non-polynomial (NP) order of the sample size; (ii) both thepropensity score and conditional mean models need to be estimated from dataunder NP dimensionality. In this paper, we propose a two-step estimation procedure for deriving theoptimal treatment regime under NP dimensionality. In both steps, penalizedregressions are employed with the non-concave penalty function, where theconditional mean model of the response given predictors may be misspecified.The asymptotic properties, such as weak oracle properties, selectionconsistency and oracle distributions, of the proposed estimators areinvestigated. In addition, we study the limiting distribution of the estimatedvalue function for the obtained optimal treatment regime. The empiricalperformance of the proposed estimation method is evaluated by simulations andan application to a depression dataset from the STAR*D study.
arxiv-12900-139 | Scatter Component Analysis: A Unified Framework for Domain Adaptation and Domain Generalization | http://arxiv.org/pdf/1510.04373v1.pdf | author:Muhammad Ghifary, David Balduzzi, W. Bastiaan Kleijn, Mengjie Zhang category:cs.CV published:2015-10-15 summary:This paper addresses classification tasks on a particular target domain inwhich labeled training data are only available from source domains differentfrom (but related to) the target. Two closely related frameworks, domainadaptation and domain generalization, are concerned with such tasks, where theonly difference between those frameworks is the availability of the unlabeledtarget data: domain adaptation can leverage unlabeled target information, whiledomain generalization cannot. We propose Scatter Component Analyis (SCA), a fast representation learningalgorithm that can be applied to both domain adaptation and domaingeneralization. SCA is based on a simple geometrical measure, i.e., scatter,which operates on reproducing kernel Hilbert space. SCA finds a representationthat trades between maximizing the separability of classes, minimizing themismatch between domains, and maximizing the separability of data; each ofwhich is quantified through scatter. The optimization problem of SCA can bereduced to a generalized eigenvalue problem, which results in a fast and exactsolution. Comprehensive experiments on benchmark cross-domain object recognitiondatasets verify that SCA performs much faster than several state-of-the-artalgorithms and also provides state-of-the-art classification accuracy in bothdomain adaptation and domain generalization. We also show that scatter can beused to establish a theoretical generalization bound in the case of domainadaptation.
arxiv-12900-140 | Batch Bayesian Optimization via Local Penalization | http://arxiv.org/pdf/1505.08052v4.pdf | author:Javier González, Zhenwen Dai, Philipp Hennig, Neil D. Lawrence category:stat.ML published:2015-05-29 summary:The popularity of Bayesian optimization methods for efficient exploration ofparameter spaces has lead to a series of papers applying Gaussian processes assurrogates in the optimization of functions. However, most proposed approachesonly allow the exploration of the parameter space to occur sequentially. Often,it is desirable to simultaneously propose batches of parameter values toexplore. This is particularly the case when large parallel processingfacilities are available. These facilities could be computational or physicalfacets of the process being optimized. E.g. in biological experiments manyexperimental set ups allow several samples to be simultaneously processed.Batch methods, however, require modeling of the interaction between theevaluations in the batch, which can be expensive in complex scenarios. Weinvestigate a simple heuristic based on an estimate of the Lipschitz constantthat captures the most important aspect of this interaction (i.e. localrepulsion) at negligible computational overhead. The resulting algorithmcompares well, in running time, with much more elaborate alternatives. Theapproach assumes that the function of interest, $f$, is a Lipschitz continuousfunction. A wrap-loop around the acquisition function is used to collectbatches of points of certain size minimizing the non-parallelizablecomputational effort. The speed-up of our method with respect to previousapproaches is significant in a set of computationally expensive experiments.
arxiv-12900-141 | Group-Invariant Subspace Clustering | http://arxiv.org/pdf/1510.04356v1.pdf | author:Shuchin Aeron, Eric Kernfeld category:cs.IT cs.LG math.IT stat.ML published:2015-10-15 summary:In this paper we consider the problem of group invariant subspace clusteringwhere the data is assumed to come from a union of group-invariant subspaces ofa vector space, i.e. subspaces which are invariant with respect to action of agiven group. Algebraically, such group-invariant subspaces are also referred toas submodules. Similar to the well known Sparse Subspace Clustering approachwhere the data is assumed to come from a union of subspaces, we analyze analgorithm which, following a recent work [1], we refer to as Sparse Sub-moduleClustering (SSmC). The method is based on finding group-sparseself-representation of data points. In this paper we primarily derive generalconditions under which such a group-invariant subspace identification ispossible. In particular we extend the geometric analysis in [2] and in theprocess we identify a related problem in geometric functional analysis.
arxiv-12900-142 | Tumor Motion Tracking in Liver Ultrasound Images Using Mean Shift and Active Contour | http://arxiv.org/pdf/1509.00154v4.pdf | author:Jalil Rasekhi category:cs.CV stat.ML published:2015-09-01 summary:In this paper we present a new method for motion tracking of tumors in liverultrasound image sequences. Our algorithm has two main steps. In the firststep, we apply mean shift algorithm with multiple features to estimate thecenter of the target in each frame. Target in the first frame is defined usingan ellipse. Edge, texture, and intensity features are extracted from the firstframe, and then mean shift algorithm is applied to each feature separately tofind the center of ellipse related to that feature in the next frame. Thecenter of ellipse will be the weighted average of these centers. By using meanshift actually we estimate the target movement between two consecutive frames.Once the correct ellipsoid in each frame is known, in the second step we applythe Dynamic Directional Gradient Vector Flow (DDGVF) version of active contourmodels, in order to find the correct boundary of tumors. We sample a few pointson the boundary of active contour then translate those points based on thetranslation of the center of ellipsoid in two consecutive frames to determinethe target movement. We use these translated sample points as an initial guessfor active contour in the next frame. Our experimental results show that, thesuggested method provides a reliable performance for liver tumor tracking inultrasound image sequences.
arxiv-12900-143 | Backhaul-Constrained Multi-Cell Cooperation Leveraging Sparsity and Spectral Clustering | http://arxiv.org/pdf/1409.8359v2.pdf | author:Swayambhoo Jain, Seung-Jun Kim, Georgios B. Giannakis category:cs.IT cs.NI math.IT stat.ML published:2014-09-30 summary:Multi-cell cooperative processing with limited backhaul traffic is studiedfor cellular uplinks. Aiming at reduced backhaul overhead, asparsity-regularized multi-cell receive-filter design problem is formulated.Both unstructured distributed cooperation as well as clustered cooperation, inwhich base station groups are formed for tight cooperation, are considered.Dynamic clustered cooperation, where the sparse equalizer and the cooperationclusters are jointly determined, is solved via alternating minimization basedon spectral clustering and group-sparse regression. Furthermore, decentralizedimplementations of both unstructured and clustered cooperation schemes aredeveloped for scalability, robustness and computational efficiency. Extensivenumerical tests verify the efficacy of the proposed methods.
arxiv-12900-144 | Language Models for Image Captioning: The Quirks and What Works | http://arxiv.org/pdf/1505.01809v3.pdf | author:Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta, Li Deng, Xiaodong He, Geoffrey Zweig, Margaret Mitchell category:cs.CL cs.AI cs.CV cs.LG published:2015-05-07 summary:Two recent approaches have achieved state-of-the-art results in imagecaptioning. The first uses a pipelined process where a set of candidate wordsis generated by a convolutional neural network (CNN) trained on images, andthen a maximum entropy (ME) language model is used to arrange these words intoa coherent sentence. The second uses the penultimate activation layer of theCNN as input to a recurrent neural network (RNN) that then generates thecaption sequence. In this paper, we compare the merits of these differentlanguage modeling approaches for the first time by using the samestate-of-the-art CNN as input. We examine issues in the different approaches,including linguistic irregularities, caption repetition, and data set overlap.By combining key aspects of the ME and RNN methods, we achieve a new recordperformance over previously published results on the benchmark COCO dataset.However, the gains we see in BLEU do not translate to human judgments.
arxiv-12900-145 | Real-time Sign Language Fingerspelling Recognition using Convolutional Neural Networks from Depth map | http://arxiv.org/pdf/1509.03001v3.pdf | author:Byeongkeun Kang, Subarna Tripathi, Truong Q. Nguyen category:cs.CV published:2015-09-10 summary:Sign language recognition is important for natural and convenientcommunication between deaf community and hearing majority. We take the highlyefficient initial step of automatic fingerspelling recognition system usingconvolutional neural networks (CNNs) from depth maps. In this work, we considerrelatively larger number of classes compared with the previous literature. Wetrain CNNs for the classification of 31 alphabets and numbers using a subset ofcollected depth data from multiple subjects. While using different learningconfigurations, such as hyper-parameter selection with and without validation,we achieve 99.99% accuracy for observed signers and 83.58% to 85.49% accuracyfor new signers. The result shows that accuracy improves as we include moredata from different subjects during training. The processing time is 3 ms forthe prediction of a single image. To the best of our knowledge, the systemachieves the highest accuracy and speed. The trained model and dataset isavailable on our repository.
arxiv-12900-146 | Dynamical spectral unmixing of multitemporal hyperspectral images | http://arxiv.org/pdf/1510.04238v1.pdf | author:Simon Henrot, Jocelyn Chanussot, Christian Jutten category:cs.CV published:2015-10-14 summary:In this paper, we consider the problem of unmixing a time series ofhyperspectral images. We propose a dynamical model based on linear mixingprocesses at each time instant. The spectral signatures and fractionalabundances of the pure materials in the scene are seen as latent variables, andassumed to follow a general dynamical structure. Based on a simplified versionof this model, we derive an efficient spectral unmixing algorithm to estimatethe latent variables by performing alternating minimizations. The performanceof the proposed approach is demonstrated on synthetic and real multitemporalhyperspectral images.
arxiv-12900-147 | Simultaneously sparse and low-rank abundance matrix estimation for hyperspectral image unmixing | http://arxiv.org/pdf/1504.01515v2.pdf | author:Paris Giampouras, Konstantinos Themelis, Athanasios Rontogiannis, Konstantinos Koutroumbas category:cs.CV math.OC stat.ML published:2015-04-07 summary:In a plethora of applications dealing with inverse problems, e.g. in imageprocessing, social networks, compressive sensing, biological data processingetc., the signal of interest is known to be structured in several ways at thesame time. This premise has recently guided the research to the innovative andmeaningful idea of imposing multiple constraints on the parameters involved inthe problem under study. For instance, when dealing with problems whoseparameters form sparse and low-rank matrices, the adoption of suitably combinedconstraints imposing sparsity and low-rankness, is expected to yieldsubstantially enhanced estimation results. In this paper, we address thespectral unmixing problem in hyperspectral images. Specifically, two novelunmixing algorithms are introduced, in an attempt to exploit both spatialcorrelation and sparse representation of pixels lying in homogeneous regions ofhyperspectral images. To this end, a novel convex mixed penalty term is firstdefined consisting of the sum of the weighted $\ell_1$ and the weighted nuclearnorm of the abundance matrix corresponding to a small area of the imagedetermined by a sliding square window. This penalty term is then used toregularize a conventional quadratic cost function and impose simultaneouslysparsity and row-rankness on the abundance matrix. The resulting regularizedcost function is minimized by a) an incremental proximal sparse and low-rankunmixing algorithm and b) an algorithm based on the alternating minimizationmethod of multipliers (ADMM). The effectiveness of the proposed algorithms isillustrated in experiments conducted both on simulated and real data.
arxiv-12900-148 | VB calibration to improve the interface between phone recognizer and i-vector extractor | http://arxiv.org/pdf/1510.03203v2.pdf | author:Niko Brümmer category:stat.ML cs.LG published:2015-10-12 summary:The EM training algorithm of the classical i-vector extractor is oftenincorrectly described as a maximum-likelihood method. The i-vector model ishowever intractable: the likelihood itself and the hidden-variable posteriorsneeded for the EM algorithm cannot be computed in closed form. We show herethat the classical i-vector extractor recipe is actually a mean-fieldvariational Bayes (VB) recipe. This theoretical VB interpretation turns out to be of further use, because italso offers an interpretation of the newer phonetic i-vector extractor recipe,thereby unifying the two flavours of extractor. More importantly, the VB interpretation is also practically useful: itsuggests ways of modifying existing i-vector extractors to make them moreaccurate. In particular, in existing methods, the approximate VB posterior forthe GMM states is fixed, while only the parameters of the generative model areadapted. Here we explore the possibility of also mildly adjusting (calibrating)those posteriors, so that they better fit the generative model.
arxiv-12900-149 | Embarrassingly Parallel Variational Inference in Nonconjugate Models | http://arxiv.org/pdf/1510.04163v1.pdf | author:Willie Neiswanger, Chong Wang, Eric Xing category:stat.ML cs.AI cs.DC cs.LG stat.CO published:2015-10-14 summary:We develop a parallel variational inference (VI) procedure for use indata-distributed settings, where each machine only has access to a subset ofdata and runs VI independently, without communicating with other machines. Thistype of "embarrassingly parallel" procedure has recently been developed forMCMC inference algorithms; however, in many cases it is not possible todirectly extend this procedure to VI methods without requiring certainrestrictive exponential family conditions on the form of the model.Furthermore, most existing (nonparallel) VI methods are restricted to use onconditionally conjugate models, which limits their applicability. To combatthese issues, we make use of the recently proposed nonparametric VI tofacilitate an embarrassingly parallel VI procedure that can be applied to awider scope of models, including to nonconjugate models. We derive ourembarrassingly parallel VI algorithm, analyze our method theoretically, anddemonstrate our method empirically on a few nonconjugate models.
arxiv-12900-150 | A Bayesian Network Model for Interesting Itemsets | http://arxiv.org/pdf/1510.04130v1.pdf | author:Jaroslav Fowkes, Charles Sutton category:stat.ML cs.DB cs.LG published:2015-10-14 summary:Mining itemsets that are the most interesting under a statistical model ofthe underlying data is a frequently used and well-studied technique forexploratory data analysis. The most recent models of interestingness arepredominantly based on maximum entropy distributions over items or tile entrieswith itemset constraints, and while computationally tractable are not easilyinterpretable. We therefore propose the first, to the best of our knowledge,generative model over itemsets, in the form of a Bayesian network, and anassociated novel measure of interestingness. Our model is able to efficientlyinfer interesting itemsets directly from the transaction database usingstructural EM, in which the E-step employs the greedy approximation to weightedset cover. Our approach is theoretically simple, straightforward to implement,trivially parallelizable and exhibits competitive performance as we demonstrateon both synthetic and real-world examples.
arxiv-12900-151 | Structure Learning with Bow-free Acyclic Path Diagrams | http://arxiv.org/pdf/1508.01717v2.pdf | author:Christopher Nowzohour, Marloes H. Maathuis, Peter Bühlmann category:stat.ML published:2015-08-07 summary:We consider the problem of structure learning for bow-free acyclic pathdiagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAGmodels that allow for certain hidden variables. We present a first method forthis problem using a greedy score-based search algorithm. We also investigatesome distributional equivalence properties of BAPs which are used in analgorithmic approach to compute (nearly) equivalent model structures, allowingto infer lower bounds of causal effects. The application of our method to somedatasets reveals that BAP models can represent the data much better than DAGmodels in these cases.
arxiv-12900-152 | Distributional Sentence Entailment Using Density Matrices | http://arxiv.org/pdf/1506.06534v2.pdf | author:Esma Balkir, Mehrnoosh Sadrzadeh, Bob Coecke category:cs.CL cs.IT cs.LO math.CT math.IT published:2015-06-22 summary:Categorical compositional distributional model of Coecke et al. (2010)suggests a way to combine grammatical composition of the formal, type logicalmodels with the corpus based, empirical word representations of distributionalsemantics. This paper contributes to the project by expanding the model to alsocapture entailment relations. This is achieved by extending the representationsof words from points in meaning space to density operators, which areprobability distributions on the subspaces of the space. A symmetric measure ofsimilarity and an asymmetric measure of entailment is defined, where lexicalentailment is measured using von Neumann entropy, the quantum variant ofKullback-Leibler divergence. Lexical entailment, combined with the compositionmap on word representations, provides a method to obtain entailment relationson the level of sentences. Truth theoretic and corpus-based examples areprovided.
arxiv-12900-153 | Fine-Grained Product Class Recognition for Assisted Shopping | http://arxiv.org/pdf/1510.04074v1.pdf | author:Marian George, Dejan Mircic, Gábor Sörös, Christian Floerkemeier, Friedemann Mattern category:cs.CV published:2015-10-14 summary:Assistive solutions for a better shopping experience can improve the qualityof life of people, in particular also of visually impaired shoppers. We presenta system that visually recognizes the fine-grained product classes of items ona shopping list, in shelves images taken with a smartphone in a grocery store.Our system consists of three components: (a) We automatically recognize usefultext on product packaging, e.g., product name and brand, and build a mapping ofwords to product classes based on the large-scale GroceryProducts dataset. Whenthe user populates the shopping list, we automatically infer the product classof each entered word. (b) We perform fine-grained product class recognitionwhen the user is facing a shelf. We discover discriminative patches on productpackaging to differentiate between visually similar product classes and toincrease the robustness against continuous changes in product design. (c) Wecontinuously improve the recognition accuracy through active learning. Ourexperiments show the robustness of the proposed method against cross-domainchallenges, and the scalability to an increasing number of products withminimal re-training.
arxiv-12900-154 | Quantifying origin and character of long-range correlations in narrative texts | http://arxiv.org/pdf/1412.8319v2.pdf | author:Stanisław Drożdż, Paweł Oświęcimka, Andrzej Kulig, Jarosław Kwapień, Katarzyna Bazarnik, Iwona Grabska-Gradzińska, Jan Rybicki, Marek Stanuszek category:cs.CL physics.soc-ph published:2014-12-29 summary:In natural language using short sentences is considered efficient forcommunication. However, a text composed exclusively of such sentences lookstechnical and reads boring. A text composed of long ones, on the other hand,demands significantly more effort for comprehension. Studying characteristicsof the sentence length variability (SLV) in a large corpus of world-famousliterary texts shows that an appealing and aesthetic optimum appears somewherein between and involves selfsimilar, cascade-like alternation of variouslengths sentences. A related quantitative observation is that the power spectraS(f) of thus characterized SLV universally develop a convincing `1/f^beta'scaling with the average exponent beta =~ 1/2, close to what has beenidentified before in musical compositions or in the brain waves. Anoverwhelming majority of the studied texts simply obeys such fractal attributesbut especially spectacular in this respect are hypertext-like, "stream ofconsciousness" novels. In addition, they appear to develop structurescharacteristic of irreducibly interwoven sets of fractals called multifractals.Scaling of S(f) in the present context implies existence of the long-rangecorrelations in texts and appearance of multifractality indicates that theycarry even a nonlinear component. A distinct role of the full stops in inducingthe long-range correlations in texts is evidenced by the fact that the abovequantitative characteristics on the long-range correlations manifest themselvesin variation of the full stops recurrence times along texts, thus in SLV, butto a much lesser degree in the recurrence times of the most frequent words. Inthis latter case the nonlinear correlations, thus multifractality, disappeareven completely for all the texts considered. Treated as one extra word, thefull stops at the same time appear to obey the Zipfian rank-frequencydistribution, however.
arxiv-12900-155 | Multiresolution Search of the Rigid Motion Space for Intensity Based Registration | http://arxiv.org/pdf/1510.04004v1.pdf | author:Behrooz Nasihatkon, Fredrik Kahl category:cs.CV published:2015-10-14 summary:We study the relation between the target functions of low-resolution andhigh-resolution intensity-based registration for the class of rigidtransformations. Our results show that low resolution target values can tightlybound the high-resolution target function in natural images. This can help withanalyzing and better understanding the process of multiresolution imageregistration. It also gives a guideline for designing multiresolutionalgorithms in which the search space in higher resolution registration isrestricted given the fitness values for lower resolution image pairs. Todemonstrate this, we incorporate our multiresolution technique into a Lipschitzglobal optimization framework. We show that using the multiresolution schemecan result in large gains in the efficiency of such algorithms. The method isevaluated by applying to 2D and 3D registration problems as well as thedetection of reflective symmetry in 2D and 3D images.
arxiv-12900-156 | Varying-coefficient models with isotropic Gaussian process priors | http://arxiv.org/pdf/1508.07192v2.pdf | author:Matthias Bussas, Christoph Sawade, Tobias Scheffer, Niels Landwehr category:cs.LG stat.ML I.2.6 published:2015-08-28 summary:We study learning problems in which the conditional distribution of theoutput given the input varies as a function of additional task variables. Invarying-coefficient models with Gaussian process priors, a Gaussian processgenerates the functional relationship between the task variables and theparameters of this conditional. Varying-coefficient models subsume hierarchicalBayesian multitask models, but also generalizations in which the conditionalvaries continuously, for instance, in time or space. However, Bayesianinference in varying-coefficient models is generally intractable. We show thatinference for varying-coefficient models with isotropic Gaussian process priorsresolves to standard inference for a Gaussian process that can be solvedefficiently. MAP inference in this model resolves to multitask learning usingtask and instance kernels, and inference for hierarchical Bayesian multitaskmodels can be carried out efficiently using graph-Laplacian kernels. We reporton experiments for geospatial prediction.
arxiv-12900-157 | Better Exploiting OS-CNNs for Better Event Recognition in Images | http://arxiv.org/pdf/1510.03979v1.pdf | author:Limin Wang, Zhe Wang, Sheng Guo, Yu Qiao category:cs.CV published:2015-10-14 summary:Event recognition from still images is one of the most important problems forimage understanding. However, compared with object recognition and scenerecognition, event recognition has received much less research attention incomputer vision community. This paper addresses the problem of cultural eventrecognition in still images and focuses on applying deep learning methods onthis problem. In particular, we utilize the successful architecture ofObject-Scene Convolutional Neural Networks (OS-CNNs) to perform eventrecognition. OS-CNNs are composed of object nets and scene nets, which transferthe learned representations from the pre-trained models on large-scale objectand scene recognition datasets, respectively. We propose four types ofscenarios to explore OS-CNNs for event recognition by treating them as either"end-to-end event predictors" or "generic feature extractors". Our experimentalresults demonstrate that the global and local representations of OS-CNNs arecomplementary to each other. Finally, based on our investigation of OS-CNNs, wecome up with a solution for the cultural event recognition track at the ICCVChaLearn Looking at People (LAP) challenge 2015. Our team secures the thirdplace at this challenge and our result is very close to the best performance.
arxiv-12900-158 | On Equivalence of Martingale Tail Bounds and Deterministic Regret Inequalities | http://arxiv.org/pdf/1510.03925v1.pdf | author:Alexander Rakhlin, Karthik Sridharan category:math.PR cs.LG stat.ML published:2015-10-13 summary:We study an equivalence of (i) deterministic pathwise statements appearing inthe online learning literature (termed \emph{regret bounds}), (ii)high-probability tail bounds for the supremum of a collection of martingales(of a specific form arising from uniform laws of large numbers formartingales), and (iii) in-expectation bounds for the supremum. By virtue ofthe equivalence, we prove exponential tail bounds for norms of Banach spacevalued martingales via deterministic regret bounds for the online mirrordescent algorithm with an adaptive step size. We extend these results beyondthe linear structure of the Banach space: we define a notion of\emph{martingale type} for general classes of real-valued functions and showits equivalence (up to a logarithmic factor) to various sequential complexitiesof the class (in particular, the sequential Rademacher complexity and itsoffset version). For classes with the general martingale type 2, we exhibit afiner notion of variation that allows partial adaptation to the functionindexing the martingale. Our proof technique rests on sequential symmetrizationand on certifying the \emph{existence} of regret minimization strategies forcertain online prediction problems.
arxiv-12900-159 | Visual Search at Pinterest | http://arxiv.org/pdf/1505.07647v2.pdf | author:Yushi Jing, David Liu, Dmitry Kislyuk, Andrew Zhai, Jiajing Xu, Jeff Donahue, Sarah Tavel category:cs.CV published:2015-05-28 summary:We demonstrate that, with the availability of distributed computationplatforms such as Amazon Web Services and open-source tools, it is possible fora small engineering team to build, launch and maintain a cost-effective,large-scale visual search system with widely available tools. We alsodemonstrate, through a comprehensive set of live experiments at Pinterest, thatcontent recommendation powered by visual search improve user engagement. Bysharing our implementation details and the experiences learned from launching acommercial visual search engines from scratch, we hope visual search are morewidely incorporated into today's commercial applications.
arxiv-12900-160 | Variable-state Latent Conditional Random Fields for Facial Expression Recognition and Action Unit Detection | http://arxiv.org/pdf/1510.03909v1.pdf | author:Robert Walecki, Ognjen Rudovic, Vladimir Pavlovic, Maja Pantic category:cs.CV cs.HC published:2015-10-13 summary:Automated recognition of facial expressions of emotions, and detection offacial action units (AUs), from videos depends critically on modeling of theirdynamics. These dynamics are characterized by changes in temporal phases(onset-apex-offset) and intensity of emotion expressions and AUs, theappearance of which may vary considerably among target subjects, making therecognition/detection task very challenging. The state-of-the-art LatentConditional Random Fields (L-CRF) framework allows one to efficiently encodethese dynamics through the latent states accounting for the temporalconsistency in emotion expression and ordinal relationships between itsintensity levels, these latent states are typically assumed to be eitherunordered (nominal) or fully ordered (ordinal). Yet, such an approach is oftentoo restrictive. For instance, in the case of AU detection, the goal is todiscriminate between the segments of an image sequence in which this AU isactive or inactive. While the sequence segments containing activation of thetarget AU may better be described using ordinal latent states, the inactivesegments better be described using unordered (nominal) latent states, as noassumption can be made about their underlying structure (since they can containeither neutral faces or activations of non-target AUs). To address this, wepropose the variable-state L-CRF (VSL-CRF) model that automatically selects theoptimal latent states for the target image sequence. To reduce the modeloverfitting either the nominal or ordinal latent states, we propose a novelgraph-Laplacian regularization of the latent states. Our experiments on threepublic expression databases show that the proposed model achieves bettergeneralization performance compared to traditional L-CRFs and other relatedstate-of-the-art models.
arxiv-12900-161 | Nonlinear memory capacity of parallel time-delay reservoir computers in the processing of multidimensional signals | http://arxiv.org/pdf/1510.03891v1.pdf | author:Lyudmila Grigoryeva, Julie Henriques, Laurent Larger, Juan-Pablo Ortega category:cs.NE published:2015-10-13 summary:This paper addresses the reservoir design problem in the context ofdelay-based reservoir computers for multidimensional input signals, parallelarchitectures, and real-time multitasking. First, an approximating reservoirmodel is presented in those frameworks that provides an explicit functionallink between the reservoir parameters and architecture and its performance inthe execution of a specific task. Second, the inference properties of the ridgeregression estimator in the multivariate context is used to assess the impactof finite sample training on the decrease of the reservoir capacity. Finally,an empirical study is conducted that shows the adequacy of the theoreticalresults with the empirical performances exhibited by various reservoirarchitectures in the execution of several nonlinear tasks with multidimensionalinputs. Our results confirm the robustness properties of the parallel reservoirarchitecture with respect to task misspecification and parameter choice thathad already been documented in the literature.
arxiv-12900-162 | Complex Politics: A Quantitative Semantic and Topological Analysis of UK House of Commons Debates | http://arxiv.org/pdf/1510.03797v1.pdf | author:Stefano Gurciullo, Michael Smallegan, María Pereda, Federico Battiston, Alice Patania, Sebastian Poledna, Daniel Hedblom, Bahattin Tolga Oztan, Alexander Herzog, Peter John, Slava Mikhaylov category:physics.soc-ph cs.CL cs.SI 91F10 published:2015-10-13 summary:This study is a first, exploratory attempt to use quantitative semanticstechniques and topological analysis to analyze systemic patterns arising in acomplex political system. In particular, we use a rich data set covering allspeeches and debates in the UK House of Commons between 1975 and 2014. By theuse of dynamic topic modeling (DTM) and topological data analysis (TDA) we showthat both members and parties feature specific roles within the system,consistent over time, and extract global patterns indicating levels ofpolitical cohesion. Our results provide a wide array of novel hypotheses aboutthe complex dynamics of political systems, with valuable policy applications.
arxiv-12900-163 | Accelerating Optimization via Adaptive Prediction | http://arxiv.org/pdf/1509.05760v3.pdf | author:Mehryar Mohri, Scott Yang category:stat.ML cs.LG published:2015-09-18 summary:We present a powerful general framework for designing data-dependentoptimization algorithms, building upon and unifying recent techniques inadaptive regularization, optimistic gradient predictions, and problem-dependentrandomization. We first present a series of new regret guarantees that hold atany time and under very minimal assumptions, and then show how differentrelaxations recover existing algorithms, both basic as well as more recentsophisticated ones. Finally, we show how combining adaptivity, optimism, andproblem-dependent randomization can guide the design of algorithms that benefitfrom more favorable guarantees than recent state-of-the-art methods.
arxiv-12900-164 | Wide-Area Image Geolocalization with Aerial Reference Imagery | http://arxiv.org/pdf/1510.03743v1.pdf | author:Scott Workman, Richard Souvenir, Nathan Jacobs category:cs.CV published:2015-10-13 summary:We propose to use deep convolutional neural networks to address the problemof cross-view image geolocalization, in which the geolocation of a ground-levelquery image is estimated by matching to georeferenced aerial images. We usestate-of-the-art feature representations for ground-level images and introducea cross-view training approach for learning a joint semantic featurerepresentation for aerial images. We also propose a network architecture thatfuses features extracted from aerial images at multiple spatial scales. Tosupport training these networks, we introduce a massive database that containspairs of aerial and ground-level images from across the United States. Ourmethods significantly out-perform the state of the art on two benchmarkdatasets. We also show, qualitatively, that the proposed featurerepresentations are discriminative at both local and continental spatialscales.
arxiv-12900-165 | Fast sequential forensic camera identification | http://arxiv.org/pdf/1510.03730v1.pdf | author:Fernando Pérez-González, Iria González-Iglesias, Miguel Masciopinto, Pedro Comesaña category:cs.CR cs.CV published:2015-10-13 summary:Two sequential camera source identification methods are proposed. Sequentialtests implement a log-likelihood ratio test in an incremental way, thusenabling a reliable decision with a minimal number of observations. One of ourmethods adapts Goljan et al.'s to sequential operation. The second, whichoffers better performance in terms of error probabilities and average number oftest observations, is based on treating the alternative hypothesis as a doublystochastic model. We also discuss how the standard sequential test can becorrected to account for the event of weak fingerprints. Finally, we validatethe goodness of our methods with experiments.
arxiv-12900-166 | SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes | http://arxiv.org/pdf/1510.03727v1.pdf | author:Stuart Golodetz, Michael Sapienza, Julien P. C. Valentin, Vibhav Vineet, Ming-Ming Cheng, Anurag Arnab, Victor A. Prisacariu, Olaf Kähler, Carl Yuheng Ren, David W. Murray, Shahram Izadi, Philip H. S. Torr category:cs.CV I.2.10 published:2015-10-13 summary:We present an open-source, real-time implementation of SemanticPaint, asystem for geometric reconstruction, object-class segmentation and learning of3D scenes. Using our system, a user can walk into a room wearing a depth cameraand a virtual reality headset, and both densely reconstruct the 3D scene andinteractively segment the environment into object classes such as 'chair','floor' and 'table'. The user interacts physically with the real-world scene,touching objects and using voice commands to assign them appropriate labels.These user-generated labels are leveraged by an online random forest-basedmachine learning algorithm, which is used to predict labels for previouslyunseen parts of the scene. The entire pipeline runs in real time, and the userstays 'in the loop' throughout the process, receiving immediate feedback aboutthe progress of the labelling and interacting with the scene as necessary torefine the predicted segmentation.
arxiv-12900-167 | Lifted Relational Neural Networks | http://arxiv.org/pdf/1508.05128v2.pdf | author:Gustav Sourek, Vojtech Aschenbrenner, Filip Zelezny, Ondrej Kuzelka category:cs.AI cs.LG cs.NE published:2015-08-20 summary:We propose a method combining relational-logic representations with neuralnetwork learning. A general lifted architecture, possibly reflecting somebackground domain knowledge, is described through relational rules which may behandcrafted or learned. The relational rule-set serves as a template forunfolding possibly deep neural networks whose structures also reflect thestructures of given training or testing relational examples. Different networkscorresponding to different examples share their weights, which co-evolve duringtraining by stochastic gradient descent algorithm. The framework allows forhierarchical relational modeling constructs and learning of latent relationalconcepts through shared hidden layers weights corresponding to the rules.Discovery of notable relational concepts and experiments on 78 relationallearning benchmarks demonstrate favorable performance of the method.
arxiv-12900-168 | Convective regularization for optical flow | http://arxiv.org/pdf/1505.04938v2.pdf | author:José A. Iglesias, Clemens Kirisits category:math.OC cs.CV published:2015-05-19 summary:We argue that the time derivative in a fixed coordinate frame may not be themost appropriate measure of time regularity of an optical flow field. Instead,for a given velocity field $v$ we consider the convective acceleration $v_t +\nabla v v$ which describes the acceleration of objects moving according to$v$. Consequently we investigate the suitability of the nonconvex functional$\v_t + \nabla v v\^2_{L^2}$ as a regularization term for optical flow. Wedemonstrate that this term acts as both a spatial and a temporal regularizerand has an intrinsic edge-preserving property. We incorporate it into acontrast invariant and time-regularized variant of the Horn-Schunck functional,prove existence of minimizers and verify experimentally that it addresses someof the problems of basic quadratic models. For the minimization we use aniterative scheme that approximates the original nonlinear problem with asequence of linear ones. We believe that the convective acceleration may begainfully introduced in a variety of optical flow models.
arxiv-12900-169 | Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection | http://arxiv.org/pdf/1504.07029v2.pdf | author:David Novotny, Jiri Matas category:cs.CV published:2015-04-27 summary:A novel efficient method for extraction of object proposals is introduced.Its "objectness" function exploits deep spatial pyramid features, a novelfast-to-compute HoG-based edge statistic and the EdgeBoxes score. Theefficiency is achieved by the use of spatial bins in a novel combination withsparsity-inducing group normalized SVM. State-of-the-art recall performance isachieved on Pascal VOC07, significantly outperforming methods with comparablespeed. Interestingly, when only 100 proposals per image are considered themethod attains 78% recall on VOC07. The method improves mAP of the RCNNstate-of-the-art class-specific detector, increasing it by 10 points when only50 proposals are used in each image. The system trained on twenty classesperforms well on the two hundred class ILSVRC2013 set confirming generalizationcapability.
arxiv-12900-170 | A language model based approach towards large scale and lightweight language identification systems | http://arxiv.org/pdf/1510.03602v1.pdf | author:Brij Mohan Lal Srivastava, Hari Krishna Vydana, Anil Kumar Vuppala, Manish Shrivastava category:cs.SD cs.CL published:2015-10-13 summary:Multilingual spoken dialogue systems have gained prominence in the recentpast necessitating the requirement for a front-end Language Identification(LID) system. Most of the existing LID systems rely on modeling the languagediscriminative information from low-level acoustic features. Due to thevariabilities of speech (speaker and emotional variabilities, etc.),large-scale LID systems developed using low-level acoustic features suffer froma degradation in the performance. In this approach, we have attempted to modelthe higher level language discriminative phonotactic information for developingan LID system. In this paper, the input speech signal is tokenized to phonesequences by using a language independent phone recognizer. The languagediscriminative phonotactic information in the obtained phone sequences aremodeled using statistical and recurrent neural network based language modelingapproaches. As this approach, relies on higher level phonotactical informationit is more robust to variabilities of speech. Proposed approach iscomputationally light weight, highly scalable and it can be used in complementwith the existing LID systems.
arxiv-12900-171 | Dual Control for Approximate Bayesian Reinforcement Learning | http://arxiv.org/pdf/1510.03591v1.pdf | author:Edgar D. Klenske, Philipp Hennig category:stat.ML cs.SY math.OC published:2015-10-13 summary:Control of non-episodic, finite-horizon dynamical systems with uncertaindynamics poses a tough and elementary case of the exploration-exploitationtrade-off. Bayesian reinforcement learning, reasoning about the effect ofactions and future observations, offers a principled solution, but isintractable. We review, then extend an old approximate approach from controltheory---where the problem is known as dual control---in the context of modernregression methods, specifically generalized linear regression. Experiments onsimulated systems show that this framework offers a useful approximation to theintractable aspects of Bayesian RL, producing structured exploration strategiesthat differ from standard RL approaches. We provide simple examples for the useof this framework in (approximate) Gaussian process regression and feedforwardneural networks for the control of exploration.
arxiv-12900-172 | $\ell_1$-regularized Neural Networks are Improperly Learnable in Polynomial Time | http://arxiv.org/pdf/1510.03528v1.pdf | author:Yuchen Zhang, Jason D. Lee, Michael I. Jordan category:cs.LG published:2015-10-13 summary:We study the improper learning of multi-layer neural networks. Suppose thatthe neural network to be learned has $k$ hidden layers and that the$\ell_1$-norm of the incoming weights of any neuron is bounded by $L$. Wepresent a kernel-based method, such that with probability at least $1 -\delta$, it learns a predictor whose generalization error is at most $\epsilon$worse than that of the neural network. The sample complexity and the timecomplexity of the presented method are polynomial in the input dimension and in$(1/\epsilon,\log(1/\delta),F(k,L))$, where $F(k,L)$ is a function depending on$(k,L)$ and on the activation function, independent of the number of neurons.The algorithm applies to both sigmoid-like activation functions and ReLU-likeactivation functions. It implies that any sufficiently sparse neural network islearnable in polynomial time.
arxiv-12900-173 | Multi-Region Probabilistic Dice Similarity Coefficient using the Aitchison Distance and Bipartite Graph Matching | http://arxiv.org/pdf/1509.07244v3.pdf | author:Shawn Andrews, Ghassan Hamarneh category:cs.CV published:2015-09-24 summary:Validation of image segmentation methods is of critical importance.Probabilistic image segmentation is increasingly popular as it capturesuncertainty in the results. Image segmentation methods that supportmulti-region (as opposed to binary) delineation are more favourable as theycapture interactions between the different objects in the image. The Dicesimilarity coefficient (DSC) has been a popular metric for evaluating theaccuracy of automated or semi-automated segmentation methods by comparing theirresults to the ground truth. In this work, we develop an extension of the DSCto multi-region probabilistic segmentations (with unordered labels). We usebipartite graph matching to establish label correspondences and propose twofunctions that extend the DSC, one based on absolute probability differencesand one based on the Aitchison distance. These provide a robust and accuratemeasure of multi-region probabilistic segmentation accuracy.
arxiv-12900-174 | The intrinsic value of HFO features as a biomarker of epileptic activity | http://arxiv.org/pdf/1510.03507v1.pdf | author:Stephen V. Gliske, Kevin R. Moon, William C. Stacey, Alfred O. Hero III category:q-bio.NC cs.LG stat.ML published:2015-10-13 summary:High frequency oscillations (HFOs) are a promising biomarker of epilepticbrain tissue and activity. HFOs additionally serve as a prototypical example ofchallenges in the analysis of discrete events in high-temporal resolution,intracranial EEG data. Two primary challenges are 1) dimensionality reduction,and 2) assessing feasibility of classification. Dimensionality reductionassumes that the data lie on a manifold with dimension less than that of thefeature space. However, previous HFO analyses have assumed a linear manifold,global across time, space (i.e. recording electrode/channel), and individualpatients. Instead, we assess both a) whether linear methods are appropriate andb) the consistency of the manifold across time, space, and patients. We alsoestimate bounds on the Bayes classification error to quantify the distinctionbetween two classes of HFOs (those occurring during seizures and thoseoccurring due to other processes). This analysis provides the foundation forfuture clinical use of HFO features and buides the analysis for other discreteevents, such as individual action potentials or multi-unit activity.
arxiv-12900-175 | On Wasserstein Two Sample Testing and Related Families of Nonparametric Tests | http://arxiv.org/pdf/1509.02237v2.pdf | author:Aaditya Ramdas, Nicolas Garcia, Marco Cuturi category:math.ST stat.ML stat.TH published:2015-09-08 summary:Nonparametric two sample or homogeneity testing is a decision theoreticproblem that involves identifying differences between two random variableswithout making parametric assumptions about their underlying distributions. Theliterature is old and rich, with a wide variety of statistics having beingintelligently designed and analyzed, both for the unidimensional and themultivariate setting. Our contribution is to tie together many of these tests,drawing connections between seemingly very different statistics. In this work,our central object is the Wasserstein distance, as we form a chain ofconnections from univariate methods like the Kolmogorov-Smirnov test, PP/QQplots and ROC/ODC curves, to multivariate tests involving energy statistics andkernel based maximum mean discrepancy. Some connections proceed through theconstruction of a \textit{smoothed} Wasserstein distance, and others throughthe pursuit of a "distribution-free" Wasserstein test. Some observations inthis chain are implicit in the literature, while others seem to have not beennoticed thus far. Given nonparametric two sample testing's classical andcontinued importance, we aim to provide useful connections for theorists andpractitioners familiar with one subset of methods but not others.
arxiv-12900-176 | Consistent Estimation of Low-Dimensional Latent Structure in High-Dimensional Data | http://arxiv.org/pdf/1510.03497v1.pdf | author:Xiongzhi Chen, John D. Storey category:stat.ML published:2015-10-13 summary:We consider the problem of extracting a low-dimensional, linear latentvariable structure from high-dimensional random variables. Specifically, weshow that under mild conditions and when this structure manifests itself as alinear space that spans the conditional means, it is possible to consistentlyrecover the structure using only information up to the second moments of theserandom variables. This finding, specialized to one-parameter exponentialfamilies whose variance function is quadratic in their means, allows for thederivation of an explicit estimator of such latent structure. This approachserves as a latent variable model estimator and as a tool for dimensionreduction for a high-dimensional matrix of data composed of many relatedvariables. Our theoretical results are verified by simulation studies and anapplication to genomic data.
arxiv-12900-177 | Implicit stochastic approximation | http://arxiv.org/pdf/1510.00967v2.pdf | author:Panos Toulis, Edoardo M. Airoldi category:math.ST stat.ML stat.TH published:2015-10-04 summary:The need to carry out parameter estimation from massive data hasreinvigorated interest in iterative estimation methods, in statistics andmachine learning. Classic work includes deterministic gradient-based methods,such as quasi-Newton, and stochastic gradient descent and its variants,including adaptive learning rates, acceleration and averaging. Current workincreasingly relies on methods that employ proximal operators, leading toupdates defined through implicit equations, which need to be solved at eachiteration. Such methods are especially attractive in modern problems withmassive data because they are numerically stable and converge with minimalassumptions, among other reasons. However, while the majority of existingmethods can be subsumed into the gradient-free stochastic approximationframework developed by Robbins and Monro (1951), there is no such framework formethods with implicit updates. Here, we conceptualize a gradient-free implicitstochastic approximation procedure, and develop asymptotic and non-asymptotictheory for it. This new framework provides a theoretical foundation forgradient-based procedures that rely on implicit updates, and opens the door toiterative estimation methods that do not require a gradient, nor a fully knownlikelihood.
arxiv-12900-178 | Correlational Neural Networks | http://arxiv.org/pdf/1504.07225v3.pdf | author:Sarath Chandar, Mitesh M. Khapra, Hugo Larochelle, Balaraman Ravindran category:cs.CL cs.LG cs.NE stat.ML published:2015-04-27 summary:Common Representation Learning (CRL), wherein different descriptions (orviews) of the data are embedded in a common subspace, is receiving a lot ofattention recently. Two popular paradigms here are Canonical CorrelationAnalysis (CCA) based approaches and Autoencoder (AE) based approaches. CCAbased approaches learn a joint representation by maximizing correlation of theviews when projected to the common subspace. AE based methods learn a commonrepresentation by minimizing the error of reconstructing the two views. Each ofthese approaches has its own advantages and disadvantages. For example, whileCCA based approaches outperform AE based approaches for the task of transferlearning, they are not as scalable as the latter. In this work we propose an AEbased approach called Correlational Neural Network (CorrNet), that explicitlymaximizes correlation among the views when projected to the common subspace.Through a series of experiments, we demonstrate that the proposed CorrNet isbetter than the above mentioned approaches with respect to its ability to learncorrelated common representations. Further, we employ CorrNet for several crosslanguage tasks and show that the representations learned using CorrNet performbetter than the ones learned using other state of the art approaches.
arxiv-12900-179 | Asymptotic Logical Uncertainty and The Benford Test | http://arxiv.org/pdf/1510.03370v1.pdf | author:Scott Garrabrant, Siddharth Bhaskar, Abram Demski, Joanna Garrabrant, George Koleszarik, Evan Lloyd category:cs.LG cs.AI F.4.1 published:2015-10-12 summary:We give an algorithm A which assigns probabilities to logical sentences. Forany simple infinite sequence of sentences whose truth-values appearindistinguishable from a biased coin that outputs "true" with probability p, wehave that the sequence of probabilities that A assigns to these sentencesconverges to p.
arxiv-12900-180 | Toward a Better Understanding of Leaderboard | http://arxiv.org/pdf/1510.03349v1.pdf | author:Wenjie Zheng category:stat.ML cs.LG stat.AP published:2015-10-12 summary:The leaderboard in machine learning competitions is a tool to show theperformance of various participants and to compare them. However, theleaderboard quickly becomes no longer accurate, due to hack or overfitting.This article gives two advices to avoid this. It also points out that theLadder leaderboard successfully prevents this with $\tilde{O}(\epsilon^{-3})$samples in the validation set.
arxiv-12900-181 | The Inductive Constraint Programming Loop | http://arxiv.org/pdf/1510.03317v1.pdf | author:Christian Bessiere, Luc De Raedt, Tias Guns, Lars Kotthoff, Mirco Nanni, Siegfried Nijssen, Barry O'Sullivan, Anastasia Paparrizou, Dino Pedreschi, Helmut Simonis category:cs.AI cs.LG published:2015-10-12 summary:Constraint programming is used for a variety of real-world optimisationproblems, such as planning, scheduling and resource allocation problems. At thesame time, one continuously gathers vast amounts of data about these problems.Current constraint programming software does not exploit such data to updateschedules, resources and plans. We propose a new framework, that we call theInductive Constraint Programming loop. In this approach data is gathered andanalyzed systematically, in order to dynamically revise and adapt constraintsand optimization criteria. Inductive Constraint Programming aims at bridgingthe gap between the areas of data mining and machine learning on the one hand,and constraint programming on the other hand.
arxiv-12900-182 | Penalized estimation in large-scale generalized linear array models | http://arxiv.org/pdf/1510.03298v1.pdf | author:Adam Lund, Martin Vincent, Niels Richard Hansen category:stat.CO stat.ML published:2015-10-12 summary:Large-scale generalized linear array models (GLAMs) can be challenging tofit. Computation and storage of its tensor product design matrix can beimpossible due to time and memory constraints, and previously considered designmatrix free algorithms do not scale well with the dimension of the parametervector. A new design matrix free algorithm is proposed for computing thepenalized maximum likelihood estimate for GLAMs, which, in particular, handlesnondifferentiable penalty functions. The proposed algorithm is implemented andavailable via the R package \verb+glamlasso+. It combines several ideas --previously considered separately -- to obtain sparse estimates while at thesame time efficiently exploiting the GLAM structure. In this paper theconvergence of the algorithm is treated and the performance of itsimplementation is investigated and compared to that of \verb+glmnet+ onsimulated as well as real data. It is shown that the computation time for
arxiv-12900-183 | On the Robustness of Regularized Pairwise Learning Methods Based on Kernels | http://arxiv.org/pdf/1510.03267v1.pdf | author:Andreas Christmann, Ding-Xuan Zhou category:math.ST math.FA stat.ML stat.TH published:2015-10-12 summary:Regularized empirical risk minimization including support vector machinesplays an important role in machine learning theory. In this paper regularizedpairwise learning (RPL) methods based on kernels will be investigated. Oneexample is regularized minimization of the error entropy loss which hasrecently attracted quite some interest from the viewpoint of consistency andlearning rates. This paper shows that such RPL methods have additionally goodstatistical robustness properties, if the loss function and the kernel arechosen appropriately. We treat two cases of particular interest: (i) a boundedand non-convex loss function and (ii) an unbounded convex loss functionsatisfying a certain Lipschitz type condition.
arxiv-12900-184 | Using Anatomical Markers for Left Ventricular Segmentation of Long Axis Ultrasound Images | http://arxiv.org/pdf/1510.03250v1.pdf | author:Yael Petrank, Nahum Smirin, Yossi Tsadok, Zvi Friedman, Peter Lysiansky, Dan Adam category:cs.CV published:2015-10-12 summary:Left ventricular segmentation is essential for measuring left ventricularfunction indices. Segmentation of one or several images requires an initialguess of the contour. It is hypothesized here that creating an initial guess byfirst detecting anatomical markers, would lead to correct detection of theendocardium. The first step of the algorithm presented here includes automaticdetection of the mitral valve. Next, the apex is detected in the same frame.The valve is then tracked throughout the cardiac cycle. Contours passing fromthe apex to each valve corner are then found using a dynamic programmingalgorithm. The resulting contour is used as an input to an active contouralgorithm. The algorithm was tested on 21 long axis ultrasound clips and showedgood agreement with manually traced contours. Thus, this study demonstratesthat detection of anatomic markers leads to a reliable initial guess of theleft ventricle border.
arxiv-12900-185 | Interactive multiclass segmentation using superpixel classification | http://arxiv.org/pdf/1510.03199v1.pdf | author:Bérengère Mathieu, Alain Crouzil, Jean-Baptiste Puel category:cs.CV published:2015-10-12 summary:This paper adresses the problem of interactive multiclass segmentation. Wepropose a fast and efficient new interactive segmentation method calledSuperpixel Classification-based Interactive Segmentation (SCIS). From a fewstrokes drawn by a human user over an image, this method extracts relevantsemantic objects. To get a fast calculation and an accurate segmentation, SCISuses superpixel over-segmentation and support vector machine classification. Inthis paper, we demonstrate that SCIS significantly outperfoms competingalgorithms by evaluating its performances on the reference benchmarks ofMcGuinness and Santner.
arxiv-12900-186 | On Correcting Inputs: Inverse Optimization for Online Structured Prediction | http://arxiv.org/pdf/1510.03130v1.pdf | author:Hal Daumé III, Samir Khuller, Manish Purohit, Gregory Sanders category:cs.LG published:2015-10-12 summary:Algorithm designers typically assume that the input data is correct, and thenproceed to find "optimal" or "sub-optimal" solutions using this input data.However this assumption of correct data does not always hold in practice,especially in the context of online learning systems where the objective is tolearn appropriate feature weights given some training samples. Such scenariosnecessitate the study of inverse optimization problems where one is given aninput instance as well as a desired output and the task is to adjust the inputdata so that the given output is indeed optimal. Motivated by learningstructured prediction models, in this paper we consider inverse optimizationwith a margin, i.e., we require the given output to be better than all otherfeasible outputs by a desired margin. We consider such inverse optimizationproblems for maximum weight matroid basis, matroid intersection, perfectmatchings, minimum cost maximum flows, and shortest paths and derive the firstknown results for such problems with a non-zero margin. The effectiveness ofthese algorithmic approaches to online learning for structured prediction isalso discussed.
arxiv-12900-187 | Fast detection of multiple objects in traffic scenes with a common detection framework | http://arxiv.org/pdf/1510.03125v1.pdf | author:Qichang Hu, Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel, Fatih Porikli category:cs.CV published:2015-10-12 summary:Traffic scene perception (TSP) aims to real-time extract accurate on-roadenvironment information, which in- volves three phases: detection of objects ofinterest, recognition of detected objects, and tracking of objects in motion.Since recognition and tracking often rely on the results from detection, theability to detect objects of interest effectively plays a crucial role in TSP.In this paper, we focus on three important classes of objects: traffic signs,cars, and cyclists. We propose to detect all the three important objects in asingle learning based detection framework. The proposed framework consists of adense feature extractor and detectors of three important classes. Once thedense features have been extracted, these features are shared with alldetectors. The advantage of using one common framework is that the detectionspeed is much faster, since all dense features need only to be evaluated oncein the testing phase. In contrast, most previous works have designed specificdetectors using different features for each of these objects. To enhance thefeature robustness to noises and image deformations, we introduce spatiallypooled features as a part of aggregated channel features. In order to furtherimprove the generalization performance, we propose an object subcategorizationmethod as a means of capturing intra-class variation of objects. Weexperimentally demonstrate the effectiveness and efficiency of the proposedframework in three detection applications: traffic sign detection, cardetection, and cyclist detection. The proposed framework achieves thecompetitive performance with state-of- the-art approaches on several benchmarkdatasets.
arxiv-12900-188 | Rivalry of Two Families of Algorithms for Memory-Restricted Streaming PCA | http://arxiv.org/pdf/1506.01490v2.pdf | author:Chun-Liang Li, Hsuan-Tien Lin, Chi-Jen Lu category:stat.ML cs.LG published:2015-06-04 summary:We study the problem of recovering the subspace spanned by the first $k$principal components of $d$-dimensional data under the streaming setting, witha memory bound of $O(kd)$. Two families of algorithms are known for thisproblem. The first family is based on the framework of stochastic gradientdescent. Nevertheless, the convergence rate of the family can be seriouslyaffected by the learning rate of the descent steps and deserves more seriousstudy. The second family is based on the power method over blocks of data, butsetting the block size for its existing algorithms is not an easy task. In thispaper, we analyze the convergence rate of a representative algorithm withdecayed learning rate (Oja and Karhunen, 1985) in the first family for thegeneral $k>1$ case. Moreover, we propose a novel algorithm for the secondfamily that sets the block sizes automatically and dynamically with fasterconvergence rate. We then conduct empirical studies that fairly compare the twofamilies on real-world data. The studies reveal the advantages anddisadvantages of these two families.
arxiv-12900-189 | Learning Deep Object Detectors from 3D Models | http://arxiv.org/pdf/1412.7122v4.pdf | author:Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko category:cs.CV cs.LG cs.NE published:2014-12-22 summary:Crowdsourced 3D CAD models are becoming easily accessible online, and canpotentially generate an infinite number of training images for almost anyobject category.We show that augmenting the training data of contemporary DeepConvolutional Neural Net (DCNN) models with such synthetic data can beeffective, especially when real training data is limited or not well matched tothe target domain. Most freely available CAD models capture 3D shape but areoften missing other low level cues, such as realistic object texture, pose, orbackground. In a detailed analysis, we use synthetic CAD-rendered images toprobe the ability of DCNN to learn without these cues, with surprisingfindings. In particular, we show that when the DCNN is fine-tuned on the targetdetection task, it exhibits a large degree of invariance to missing low-levelcues, but, when pretrained on generic ImageNet classification, it learns betterwhen the low-level cues are simulated. We show that our synthetic DCNN trainingapproach significantly outperforms previous methods on the PASCAL VOC2007dataset when learning in the few-shot scenario and improves performance in adomain shift scenario on the Office benchmark.
arxiv-12900-190 | A Markov Jump Process for More Efficient Hamiltonian Monte Carlo | http://arxiv.org/pdf/1509.03808v3.pdf | author:Andrew B. Berger, Mayur Mudigonda, Michael R. DeWeese, Jascha Sohl-Dickstein category:stat.ML stat.CO published:2015-09-13 summary:In most sampling algorithms, including Hamiltonian Monte Carlo, transitionrates between states correspond to the probability of making a transition in asingle time step, and are constrained to be less than or equal to 1. We derivea Hamiltonian Monte Carlo algorithm using a continuous time Markov jumpprocess, and are thus able to escape this constraint. Transition rates in aMarkov jump process need only be non-negative. We demonstrate that the newalgorithm leads to improved mixing for several example problems, both byevaluating the spectral gap of the Markov operator, and by computingautocorrelation as a function of compute time. We release the algorithm as anopen source Python package.
arxiv-12900-191 | ParallelPC: an R package for efficient constraint based causal exploration | http://arxiv.org/pdf/1510.03042v1.pdf | author:Thuc Duy Le, Tao Hoang, Jiuyong Li, Lin Liu, Shu Hu category:cs.AI stat.ML published:2015-10-11 summary:Discovering causal relationships from data is the ultimate goal of manyresearch areas. Constraint based causal exploration algorithms, such as PC,FCI, RFCI, PC-simple, IDA and Joint-IDA have achieved significant progress andhave many applications. A common problem with these methods is the highcomputational complexity, which hinders their applications in real world highdimensional datasets, e.g gene expression datasets. In this paper, we presentan R package, ParallelPC, that includes the parallelised versions of thesecausal exploration algorithms. The parallelised algorithms help speed up theprocedure of experimenting big datasets and reduce the memory used when runningthe algorithms. The package is not only suitable for super-computers orclusters, but also convenient for researchers using personal computers withmulti core CPUs. Our experiment results on real world datasets show that usingthe parallelised algorithms it is now practical to explore causal relationshipsin high dimensional datasets with thousands of variables in a single multicorecomputer. ParallelPC is available in CRAN repository athttps://cran.rproject.org/web/packages/ParallelPC/index.html.
arxiv-12900-192 | Choice of V for V-Fold Cross-Validation in Least-Squares Density Estimation | http://arxiv.org/pdf/1210.5830v3.pdf | author:Sylvain Arlot, Matthieu Lerasle category:math.ST cs.LG stat.TH published:2012-10-22 summary:This paper studies V-fold cross-validation for model selection inleast-squares density estimation. The goal is to provide theoretical groundsfor choosing V in order to minimize the least-squares loss of the selectedestimator. We first prove a non-asymptotic oracle inequality for V-foldcross-validation and its bias-corrected version (V-fold penalization). Inparticular, this result implies that V-fold penalization is asymptoticallyoptimal in the nonparametric case. Then, we compute the variance of V-foldcross-validation and related criteria, as well as the variance of keyquantities for model selection performance. We show that these variances dependon V like 1+4/(V-1), at least in some particular cases, suggesting that theperformance increases much from V=2 to V=5 or 10, and then is almost constant.Overall, this can explain the common advice to take V=5---at least in oursetting and when the computational power is limited---, as supported by somesimulation experiments. An oracle inequality and exact formulas for thevariance are also proved for Monte-Carlo cross-validation, also known asrepeated cross-validation, where the parameter V is replaced by the number B ofrandom splits of the data.
arxiv-12900-193 | Textual Analysis for Studying Chinese Historical Documents and Literary Novels | http://arxiv.org/pdf/1510.03021v1.pdf | author:Chao-Lin Liu, Guan-Tao Jin, Hongsu Wang, Qing-Feng Liu, Wen-Huei Cheng, Wei-Yun Chiu, Richard Tzong-Han Tsai, Yu-Chun Wang category:cs.CL cs.DL published:2015-10-11 summary:We analyzed historical and literary documents in Chinese to gain insightsinto research issues, and overview our studies which utilized four differentsources of text materials in this paper. We investigated the history ofconcepts and transliterated words in China with the Database for the Study ofModern China Thought and Literature, which contains historical documents aboutChina between 1830 and 1930. We also attempted to disambiguate names that wereshared by multiple government officers who served between 618 and 1912 and wererecorded in Chinese local gazetteers. To showcase the potentials and challengesof computer-assisted analysis of Chinese literatures, we explored someinteresting yet non-trivial questions about two of the Four Great ClassicalNovels of China: (1) Which monsters attempted to consume the Buddhist monkXuanzang in the Journey to the West (JTTW), which was published in the 16thcentury, (2) Which was the most powerful monster in JTTW, and (3) Which majorrole smiled the most in the Dream of the Red Chamber, which was published inthe 18th century. Similar approaches can be applied to the analysis and studyof modern documents, such as the newspaper articles published about the 228incident that occurred in 1947 in Taiwan.
arxiv-12900-194 | OmniGraph: Rich Representation and Graph Kernel Learning | http://arxiv.org/pdf/1510.02983v1.pdf | author:Boyi Xie, Rebecca J. Passonneau category:cs.CL cs.LG published:2015-10-10 summary:OmniGraph, a novel representation to support a range of NLP classificationtasks, integrates lexical items, syntactic dependencies and frame semanticparses into graphs. Feature engineering is folded into the learning throughconvolution graph kernel learning to explore different extents of the graph. Ahigh-dimensional space of features includes individual nodes as well as complexsubgraphs. In experiments on a text-forecasting problem that predicts stockprice change from news for company mentions, OmniGraph beats several benchmarksbased on bag-of-words, syntactic dependencies, and semantic trees. The highlyexpressive features OmniGraph discovers provide insights into the semanticsacross distinct market sectors. To demonstrate the method's generality, we alsoreport its high performance results on a fine-grained sentiment corpus.
arxiv-12900-195 | Optimal Piecewise Linear Function Approximation for GPU-based Applications | http://arxiv.org/pdf/1510.02975v1.pdf | author:Daniel Berjón, Guillermo Gallego, Carlos Cuevas, Francisco Morán, Narciso García category:math.OC cs.CV cs.DC cs.NA cs.SY published:2015-10-10 summary:Many computer vision and human-computer interaction applications developed inrecent years need evaluating complex and continuous mathematical functions asan essential step toward proper operation. However, rigorous evaluation of thiskind of functions often implies a very high computational cost, unacceptable inreal-time applications. To alleviate this problem, functions are commonlyapproximated by simpler piecewise-polynomial representations. Following thisidea, we propose a novel, efficient, and practical technique to evaluatecomplex and continuous functions using a nearly optimal design of two types ofpiecewise linear approximations in the case of a large budget of evaluationsubintervals. To this end, we develop a thorough error analysis that yieldsasymptotically tight bounds to accurately quantify the approximationperformance of both representations. It provides an improvement upon previouserror estimates and allows the user to control the trade-off between theapproximation error and the number of evaluation subintervals. To guaranteereal-time operation, the method is suitable for, but not limited to, anefficient implementation in modern Graphics Processing Units (GPUs), where itoutperforms previous alternative approaches by exploiting the fixed-functioninterpolation routines present in their texture units. The proposed techniqueis a perfect match for any application requiring the evaluation of continuousfunctions, we have measured in detail its quality and efficiency on severalfunctions, and, in particular, the Gaussian function because it is extensivelyused in many areas of computer vision and cybernetics, and it is expensive toevaluate.
arxiv-12900-196 | Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition? | http://arxiv.org/pdf/1510.02969v1.pdf | author:Pooya Khorrami, Tom Le Paine, Thomas S. Huang category:cs.CV cs.LG cs.NE published:2015-10-10 summary:Despite being the appearance-based classifier of choice in recent years,relatively few works have examined how much convolutional neural networks(CNNs) can improve performance on accepted expression recognition benchmarksand, more importantly, examine what it is they actually learn. In this work,not only do we show that CNNs can achieve strong performance, but we alsointroduce an approach to decipher which portions of the face influence theCNN's predictions. First, we train a zero-bias CNN on facial expression dataand achieve, to our knowledge, state-of-the-art performance on two expressionrecognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the TorontoFace Dataset (TFD). We then qualitatively analyze the network by visualizingthe spatial patterns that maximally excite different neurons in theconvolutional layers and show how they resemble Facial Action Units (FAUs).Finally, we use the FAU labels provided in the CK+ dataset to verify that theFAUs observed in our filter visualizations indeed align with the subject'sfacial movements.
arxiv-12900-197 | Spatial Semantic Regularisation for Large Scale Object Detection | http://arxiv.org/pdf/1510.02949v1.pdf | author:Damian Mrowca, Marcus Rohrbach, Judy Hoffman, Ronghang Hu, Kate Saenko, Trevor Darrell category:cs.CV published:2015-10-10 summary:Large scale object detection with thousands of classes introduces the problemof many contradicting false positive detections, which have to be suppressed.Class-independent non-maximum suppression has traditionally been used for thisstep, but it does not scale well as the number of classes grows. Traditionalnon-maximum suppression does not consider label- and instance-levelrelationships nor does it allow an exploitation of the spatial layout ofdetection proposals. We propose a new multi-class spatial semanticregularisation method based on affinity propagation clustering, whichsimultaneously optimises across all categories and all proposed locations inthe image, to improve both the localisation and categorisation of selecteddetection proposals. Constraints are shared across the labels through thesemantic WordNet hierarchy. Our approach proves to be especially useful inlarge scale settings with thousands of classes, where spatial and semanticinteractions are very frequent and only weakly supervised detectors can bebuilt due to a lack of bounding box annotations. Detection experiments areconducted on the ImageNet and COCO dataset, and in settings with thousands ofdetected categories. Our method provides a significant precision improvement byreducing false positives, while simultaneously improving the recall.
arxiv-12900-198 | Evaluation of Joint Multi-Instance Multi-Label Learning For Breast Cancer Diagnosis | http://arxiv.org/pdf/1510.02942v1.pdf | author:Baris Gecer, Ozge Yalcinkaya, Onur Tasar, Selim Aksoy category:cs.CV cs.LG published:2015-10-10 summary:Multi-instance multi-label (MIML) learning is a challenging problem in manyaspects. Such learning approaches might be useful for many medical diagnosisapplications including breast cancer detection and classification. In thisstudy subset of digiPATH dataset (whole slide digital breast cancerhistopathology images) are used for training and evaluation of sixstate-of-the-art MIML methods. At the end, performance comparison of these approaches are given by means ofeffective evaluation metrics. It is shown that MIML-kNN achieve the bestperformance that is %65.3 average precision, where most of other methods attainacceptable results as well.
arxiv-12900-199 | Fast and Accurate Poisson Denoising with Optimized Nonlinear Diffusion | http://arxiv.org/pdf/1510.02930v1.pdf | author:Wensen Feng, Yunjin Chen category:cs.CV published:2015-10-10 summary:The degradation of the acquired signal by Poisson noise is a common problemfor various imaging applications, such as medical imaging, night vision andmicroscopy. Up to now, many state-of-the-art Poisson denoising techniquesmainly concentrate on achieving utmost performance, with little considerationfor the computation efficiency. Therefore, in this study we aim to propose anefficient Poisson denoising model with both high computational efficiency andrecovery quality. To this end, we exploit the newly-developed trainablenonlinear reaction diffusion model which has proven an extremely fast imagerestoration approach with performance surpassing recent state-of-the-arts. Weretrain the model parameters, including the linear filters and influencefunctions by taking into account the Poisson noise statistics, and end up withan optimized nonlinear diffusion model specialized for Poisson denoising. Thetrained model provides strongly competitive results against state-of-the-artapproaches, meanwhile bearing the properties of simple structure and highefficiency. Furthermore, our proposed model comes along with an additionaladvantage, that the diffusion process is well-suited for parallel computationon GPUs. For images of size $512 \times 512$, our GPU implementation takes lessthan 0.1 seconds to produce state-of-the-art Poisson denoising performance.
arxiv-12900-200 | DeepFix: A Fully Convolutional Neural Network for predicting Human Eye Fixations | http://arxiv.org/pdf/1510.02927v1.pdf | author:Srinivas S. S. Kruthiventi, Kumar Ayush, R. Venkatesh Babu category:cs.CV published:2015-10-10 summary:Understanding and predicting the human visual attentional mechanism is anactive area of research in the fields of neuroscience and computer vision. Inthis work, we propose DeepFix, a first-of-its-kind fully convolutional neuralnetwork for accurate saliency prediction. Unlike classical works whichcharacterize the saliency map using various hand-crafted features, our modelautomatically learns features in a hierarchical fashion and predicts saliencymap in an end-to-end manner. DeepFix is designed to capture semantics atmultiple scales while taking global context into account using network layerswith very large receptive fields. Generally, fully convolutional nets arespatially invariant which prevents them from modeling location dependentpatterns (e.g. centre-bias). Our network overcomes this limitation byincorporating a novel Location Biased Convolutional layer. We evaluate ourmodel on two challenging eye fixation datasets -- MIT300, CAT2000 and show thatit outperforms other recent approaches by a significant margin.
arxiv-12900-201 | On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image Rician Denoising | http://arxiv.org/pdf/1510.02923v1.pdf | author:Adrian Martin, Emanuele Schiavi, Sergio Segura de Leon category:math.AP cs.CV math.NA published:2015-10-10 summary:Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in aBayesian or generalized Tikhonov framework using Total Variation (TV) leadsnaturally to the consideration of nonlinear elliptic equations. These involvethe so called $1$-Laplacian operator and special care is needed to properlyformulate the problem. The rician statistics of the data are introduced througha singular equation with a reaction term defined in terms of modified firstorder Bessel functions. An existence theory is provided here together withother qualitative properties of the solutions. Remarkably, each positive globalminimum of the associated functional is one of such solutions. Moreover, wedirectly solve this non--smooth non--convex minimization problem using aconvergent Proximal Point Algorithm. Numerical results based on synthetic andreal MRI demonstrate a better performance of the proposed method when comparedto previous TV based models for rician denoising which regularize or convexifythe problem. Finally, an application on real Diffusion Tensor Images, astrongly affected by rician noise MRI modality, is presented and discussed.
arxiv-12900-202 | Temporal Dynamic Appearance Modeling for Online Multi-Person Tracking | http://arxiv.org/pdf/1510.02906v1.pdf | author:Min Yang, Yunde Jia category:cs.CV published:2015-10-10 summary:Robust online multi-person tracking requires the correct associations ofonline detection responses with existing trajectories. We address this problemby developing a novel appearance modeling approach to provide accurateappearance affinities to guide data association. In contrast to most existingalgorithms that only consider the spatial structure of human appearances, weexploit the temporal dynamic characteristics within temporal appearancesequences to discriminate different persons. The temporal dynamic makes asufficient complement to the spatial structure of varying appearances in thefeature space, which significantly improves the affinity measurement betweentrajectories and detections. We propose a feature selection algorithm todescribe the appearance variations with mid-level semantic features, anddemonstrate its usefulness in terms of temporal dynamic appearance modeling.Moreover, the appearance model is learned incrementally by alternativelyevaluating newly-observed appearances and adjusting the model parameters to besuitable for online tracking. Reliable tracking of multiple persons in complexscenes is achieved by incorporating the learned model into an onlinetracking-by-detection framework. Our experiments on the challenging benchmarkMOTChallenge 2015 demonstrate that our method outperforms the state-of-the-artmulti-person tracking algorithms.
arxiv-12900-203 | Survey on Feature Selection | http://arxiv.org/pdf/1510.02892v1.pdf | author:Tarek Amr Abdallah, Beatriz de La Iglesia category:cs.LG published:2015-10-10 summary:Feature selection plays an important role in the data mining process. It isneeded to deal with the excessive number of features, which can become acomputational burden on the learning algorithms. It is also necessary, evenwhen computational resources are not scarce, since it improves the accuracy ofthe machine learning tasks, as we will see in the upcoming sections. In thisreview, we discuss the different feature selection approaches, and the relationbetween them and the various machine learning algorithms.
arxiv-12900-204 | Remarks on kernel Bayes' rule | http://arxiv.org/pdf/1507.01059v2.pdf | author:Hisashi Johno, Kazunori Nakamoto, Tatsuhiko Saigo category:stat.ML published:2015-07-04 summary:Kernel Bayes' rule has been proposed as a nonparametric kernel-based methodto realize Bayesian inference in reproducing kernel Hilbert spaces. However, wedemonstrate both theoretically and experimentally that the prediction result bykernel Bayes' rule is in some cases unnatural. We consider that this phenomenonis in part due to the fact that the assumptions in kernel Bayes' rule do nothold in general.
arxiv-12900-205 | Learn to Evaluate Image Perceptual Quality Blindly from Statistics of Self-similarity | http://arxiv.org/pdf/1510.02884v1.pdf | author:Wufeng Xue, Xuanqin Mou, Lei Zhang category:cs.CV published:2015-10-10 summary:Among the various image quality assessment (IQA) tasks, blind IQA (BIQA) isparticularly challenging due to the absence of knowledge about the referenceimage and distortion type. Features based on natural scene statistics (NSS)have been successfully used in BIQA, while the quality relevance of the featureplays an essential role to the quality prediction performance. Motivated by thefact that the early processing stage in human visual system aims to remove thesignal redundancies for efficient visual coding, we propose a simple but veryeffective BIQA method by computing the statistics of self-similarity (SOS) inan image. Specifically, we calculate the inter-scale similarity and intra-scalesimilarity of the distorted image, extract the SOS features from thesesimilarities, and learn a regression model to map the SOS features to thesubjective quality score. Extensive experiments demonstrate very competitivequality prediction performance and generalization ability of the proposed SOSbased BIQA method.
arxiv-12900-206 | TSEB: More Efficient Thompson Sampling for Policy Learning | http://arxiv.org/pdf/1510.02874v1.pdf | author:P. Prasanna, Sarath Chandar, Balaraman Ravindran category:cs.LG published:2015-10-10 summary:In model-based solution approaches to the problem of learning in an unknownenvironment, exploring to learn the model parameters takes a toll on theregret. The optimal performance with respect to regret or PAC bounds isachievable, if the algorithm exploits with respect to reward or explores withrespect to the model parameters, respectively. In this paper, we propose TSEB,a Thompson Sampling based algorithm with adaptive exploration bonus that aimsto solve the problem with tighter PAC guarantees, while being cautious on theregret as well. The proposed approach maintains distributions over the modelparameters which are successively refined with more experience. At any giventime, the agent solves a model sampled from this distribution, and the sampledreward distribution is skewed by an exploration bonus in order to generate moreinformative exploration. The policy by solving is then used for generating moreexperience that helps in updating the posterior over the model parameters. Weprovide a detailed analysis of the PAC guarantees, and convergence of theproposed approach. We show that our adaptive exploration bonus encourages theadditional exploration required for better PAC bounds on the algorithm. Weprovide empirical analysis on two different simulated domains.
arxiv-12900-207 | Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree | http://arxiv.org/pdf/1509.08985v2.pdf | author:Chen-Yu Lee, Patrick W. Gallagher, Zhuowen Tu category:stat.ML cs.LG cs.NE published:2015-09-30 summary:We seek to improve deep neural networks by generalizing the poolingoperations that play a central role in current architectures. We pursue acareful exploration of approaches to allow pooling to learn and to adapt tocomplex and variable patterns. The two primary directions lie in (1) learning apooling function via (two strategies of) combining of max and average pooling,and (2) learning a pooling function in the form of a tree-structured fusion ofpooling filters that are themselves learned. In our experiments everygeneralized pooling operation we explore improves performance when used inplace of average or max pooling. We experimentally demonstrate that theproposed pooling operations provide a boost in invariance properties relativeto conventional pooling and set the state of the art on several widely adoptedbenchmark datasets; they are also easy to implement, and can be applied withinvarious deep neural network architectures. These benefits come with only alight increase in computational overhead during training and a very modestincrease in the number of model parameters.
arxiv-12900-208 | Wavelet Frame Based Image Restoration Using Sparsity, Nonlocal and Support Prior of Frame Coefficients | http://arxiv.org/pdf/1510.02866v1.pdf | author:Liangtian He, Yilun Wang category:cs.CV 90-08 I.4.4 published:2015-10-10 summary:The wavelet frame systems have been widely investigated and applied for imagerestoration and many other image processing problems over the past decades,attributing to their good capability of sparsely approximating piece-wisesmooth functions such as images. Most wavelet frame based models exploit the$l_1$ norm of frame coefficients for a sparsity constraint in the past. Theauthors in \cite{ZhangY2013, Dong2013} proposed an $l_0$ minimization model,where the $l_0$ norm of wavelet frame coefficients is penalized instead, andhave demonstrated that significant improvements can be achieved compared to thecommonly used $l_1$ minimization model. Very recently, the authors in\cite{Chen2015} proposed $l_0$-$l_2$ minimization model, where the nonlocalprior of frame coefficients is incorporated. This model proved to outperformthe single $l_0$ minimization based model in terms of better recovered imagequality. In this paper, we propose a truncated $l_0$-$l_2$ minimization modelwhich combines sparsity, nonlocal and support prior of the frame coefficients.The extensive experiments have shown that the recovery results from theproposed regularization method performs better than existing state-of-the-artwavelet frame based methods, in terms of edge enhancement and texturepreserving performance.
arxiv-12900-209 | Data-Driven Learning of the Number of States in Multi-State Autoregressive Models | http://arxiv.org/pdf/1506.02107v3.pdf | author:Jie Ding, Mohammad Noshad, Vahid Tarokh category:stat.ML cs.LG published:2015-06-06 summary:In this work, we consider the class of multi-state autoregressive processesthat can be used to model non-stationary time-series of interest. In order tocapture different autoregressive (AR) states underlying an observed timeseries, it is crucial to select the appropriate number of states. We propose anew model selection technique based on the Gap statistics, which uses a nullreference distribution on the stable AR filters to check whether adding a newAR state significantly improves the performance of the model. To that end, wedefine a new distance measure between AR filters based on mean squaredprediction error (MSPE), and propose an efficient method to generate randomstable filters that are uniformly distributed in the coefficient space.Numerical results are provided to evaluate the performance of the proposedapproach.
arxiv-12900-210 | AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery | http://arxiv.org/pdf/1510.02855v1.pdf | author:Izhar Wallach, Michael Dzamba, Abraham Heifets category:cs.LG cs.NE q-bio.BM stat.ML published:2015-10-10 summary:Deep convolutional neural networks comprise a subclass of deep neuralnetworks (DNN) with a constrained architecture that leverages the spatial andtemporal structure of the domain they model. Convolutional networks achieve thebest predictive performance in areas such as speech and image recognition byhierarchically composing simple local features into complex models. AlthoughDNNs have been used in drug discovery for QSAR and ligand-based bioactivitypredictions, none of these models have benefited from this powerfulconvolutional architecture. This paper introduces AtomNet, the firststructure-based, deep convolutional neural network designed to predict thebioactivity of small molecules for drug discovery applications. We demonstratehow to apply the convolutional concepts of feature locality and hierarchicalcomposition to the modeling of bioactivity and chemical interactions. Infurther contrast to existing DNN techniques, we show that AtomNet's applicationof local convolutional filters to structural target information successfullypredicts new active molecules for targets with no previously known modulators.Finally, we show that AtomNet outperforms previous docking approaches on adiverse set of benchmarks by a large margin, achieving an AUC greater than 0.9on 57.8% of the targets in the DUDE benchmark.
arxiv-12900-211 | Efficient Per-Example Gradient Computations | http://arxiv.org/pdf/1510.01799v2.pdf | author:Ian Goodfellow category:stat.ML cs.LG published:2015-10-07 summary:This technical report describes an efficient technique for computing the normof the gradient of the loss function for a neural network with respect to itsparameters. This gradient norm can be computed efficiently for every example.
arxiv-12900-212 | Earth Mover's Distance Yields Positive Definite Kernels For Certain Ground Distances | http://arxiv.org/pdf/1510.02833v1.pdf | author:Andrew Gardner, Christian A. Duncan, Jinko Kanno, Rastko R. Selmic category:cs.LG stat.ML published:2015-10-09 summary:Positive definite kernels are an important tool in machine learning thatenable efficient solutions to otherwise difficult or intractable problems byimplicitly linearizing the problem geometry. In this paper we develop aset-theoretic interpretation of the Earth Mover's Distance (EMD) that naturallyyields metric and kernel forms of EMD as generalizations of elementary setoperations. In particular, EMD is generalized to sets of unequal size. We alsooffer the first proof of positive definite kernels based directly on EMD, andprovide propositions and conjectures concerning what properties are necessaryand sufficient for EMD to be conditionally negative definite. In particular, weshow that three distinct positive definite kernels -- intersection, minimum,and Jaccard index -- can be derived from EMD with various ground distances. Inthe process we show that the Jaccard index is simply the result of a positivedefinite preserving transformation that can be applied to any kernel. Finally,we evaluate the proposed kernels in various computer vision tasks.
arxiv-12900-213 | p-Markov Gaussian Processes for Scalable and Expressive Online Bayesian Nonparametric Time Series Forecasting | http://arxiv.org/pdf/1510.02830v1.pdf | author:Yves-Laurent Kom Samo, Stephen J. Roberts category:stat.ML published:2015-10-09 summary:In this paper we introduce a novel online time series forecasting model werefer to as the pM-GP filter. We show that our model is equivalent to Gaussianprocess regression, with the advantage that both online forecasting and onlinelearning of the hyper-parameters have a constant (rather than cubic) timecomplexity and a constant (rather than squared) memory requirement in thenumber of observations, without resorting to approximations. Moreover, theproposed model is expressive in that the family of covariance functions of theimplied latent process, namely the spectral Matern kernels, have recently beenproven to be capable of approximating arbitrarily well anytranslation-invariant covariance function. The benefit of our approach comparedto competing models is demonstrated using experiments on several real-lifedatasets.
arxiv-12900-214 | Generalized Spectral Kernels | http://arxiv.org/pdf/1506.02236v2.pdf | author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML published:2015-06-07 summary:In this paper we propose a family of tractable kernels that is dense in thefamily of bounded positive semi-definite functions (i.e. can approximate anybounded kernel with arbitrary precision). We start by discussing the case ofstationary kernels, and propose a family of spectral kernels that extendsexisting approaches such as spectral mixture kernels and sparse spectrumkernels. Our extension has two primary advantages. Firstly, unlike existingspectral approaches that yield infinite differentiability, the kernels weintroduce allow learning the degree of differentiability of the latent functionin Gaussian process (GP) models and functions in the reproducing kernel Hilbertspace (RKHS) in other kernel methods. Secondly, we show that some of thekernels we propose require fewer parameters than existing spectral kernels forthe same accuracy, thereby leading to faster and more robust inference.Finally, we generalize our approach and propose a flexible and tractable familyof spectral kernels that we prove can approximate any continuous boundednonstationary kernel.
arxiv-12900-215 | Mapping Generative Models onto a Network of Digital Spiking Neurons | http://arxiv.org/pdf/1509.07302v2.pdf | author:Bruno U. Pedroni, Srinjoy Das, John V. Arthur, Paul A. Merolla, Bryan L. Jackson, Dharmendra S. Modha, Kenneth Kreutz-Delgado, Gert Cauwenberghs category:cs.NE q-bio.NC published:2015-09-24 summary:Stochastic neural networks such as Restricted Boltzmann Machines (RBMs) havebeen successfully used in applications ranging from speech recognition to imageclassification. Inference and learning in these algorithms use a Markov ChainMonte Carlo procedure called Gibbs sampling, where a logistic function formsthe kernel of this sampler. On the other side of the spectrum, neuromorphicsystems have shown great promise for low-power and parallelized cognitivecomputing, but lack well-suited applications and automation procedures. In thiswork, we propose a systematic method for bridging the RBM algorithm and digitalneuromorphic systems, with a generative pattern completion task as proof ofconcept. For this, we first propose a method of producing the Gibbs samplerusing bio-inspired digital noisy integrate-and-fire neurons. Next, we describethe process of mapping generative RBMs trained offline onto the IBM TrueNorthneurosynaptic processor -- a low-power digital neuromorphic VLSI substrate.Mapping these algorithms onto neuromorphic hardware presents unique challengesin network connectivity and weight and bias quantization, which, in turn,require architectural and design strategies for the physical realization.Generative performance metrics are analyzed to validate the neuromorphicrequirements and to best select the neuron parameters for the model. Lastly, wedescribe a design automation procedure which achieves optimal resource usage,accounting for the novel hardware adaptations. This work represents the firstimplementation of generative RBM inference on a neuromorphic VLSI substrate.
arxiv-12900-216 | Human languages order information efficiently | http://arxiv.org/pdf/1510.02823v1.pdf | author:Daniel Gildea, T. Florian Jaeger category:cs.CL published:2015-10-09 summary:Most languages use the relative order between words to encode meaningrelations. Languages differ, however, in what orders they use and how theseorders are mapped onto different meanings. We test the hypothesis that, despitethese differences, human languages might constitute different `solutions' tocommon pressures of language use. Using Monte Carlo simulations over data fromfive languages, we find that their word orders are efficient for processing interms of both dependency length and local lexical probability. This suggeststhat biases originating in how the brain understands language stronglyconstrain how human languages change over generations.
arxiv-12900-217 | Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation | http://arxiv.org/pdf/1510.02795v1.pdf | author:Søren Hauberg, Oren Freifeld, Anders Boesen Lindbo Larsen, John W. Fisher III, Lars Kai Hansen category:cs.CV published:2015-10-09 summary:Data augmentation is a key element in training high-dimensional models. Inthis approach, one synthesizes new observations by applying pre-specifiedtransformations to the original training data; e.g. new images are formed byrotating old ones. Current augmentation schemes, however, rely on manualspecification of the applied transformations, making data augmentation animplicit form of feature engineering. Working towards true end-to-end learning,we suggest to learn the applied transformations on a per-class basis.Particularly, we align image pairs within each class under the assumption thatthe spatial transformation between images belongs to a large class ofdiffeomorphisms. For each class, we then build a probabilistic generative modelof the transformations in a Riemannian submanifold of the Lie group ofdiffeomorphisms. We demonstrate significant performance improvements intraining deep neural nets over manually-specified augmentation schemes.
arxiv-12900-218 | Recovering a Hidden Community Beyond the Spectral Limit in $O(E \log^*V)$ Time | http://arxiv.org/pdf/1510.02786v1.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.CC cs.SI math.PR published:2015-10-09 summary:The stochastic block model for one community with parameters $n, K, p,$ and$q$ is considered: $K$ out of $n$ vertices are in the community; two verticesare connected by an edge with probability $p$ if they are both in the communityand with probability $q$ otherwise, where $p > q > 0$ and $p/q$ is assumed tobe bounded. An estimator based on observation of the graph $G=(V,E)$ is said toachieve weak recovery if the mean number of misclassified vertices is $o(K)$ as$n \to \infty$. A critical role is played by the effective signal-to-noiseratio $\lambda=K^2(p-q)^2/((n-K)q).$ In the regime $K=\Theta(n)$, a na\"{i}vedegree-thresholding algorithm achieves weak recovery in $O(E)$ time if$\lambda \to \infty$, which coincides with the information theoreticpossibility of weak recovery. The main focus of the paper is on weak recovery in the sublinear regime$K=o(n)$ and $np = n^{o(1)}.$ It is shown that weak recovery is provided by abelief propagation algorithm running for $\log^\ast(n)+O(1) $ iterations, if$\lambda > 1/e,$ with the total time complexity $O(E \log^*n)$. Conversely,no local algorithm with radius $t$ of interaction satisfying $t = o(\frac{\logn}{\log(2+np)})$ can asymptotically outperform trivial random guessing if$\lambda \leq 1/e.$ By analyzing a linear message-passing algorithm thatcorresponds to applying power iteration to the non-backtracking matrix of thegraph, we provide evidence to suggest that spectral methods fail to provideweak recovery if $\lambda \leq 1.$
arxiv-12900-219 | Where is my puppy? Retrieving lost dogs by facial features | http://arxiv.org/pdf/1510.02781v1.pdf | author:Thierry Pinheiro Moreira, Mauricio Lisboa Perez, Rafael de Oliveira Werneck, Eduardo Valle category:cs.CV 68T45 I.5.4 published:2015-10-09 summary:A pet that goes missing is among many people's worst fears. A moment ofdistraction is enough for a dog or a cat wandering off from home. Animalmanagement services collect stray animals and try to find their owners, but notalways successfully. Some measures may improve the chances of matching lostanimals to their owners; but automated visual recognition is one that --although convenient, highly available, and low-cost -- is surprisinglyoverlooked. In this paper, we inaugurate that promising avenue by pursuing facerecognition for dogs. We contrast three ready-to-use human facial recognizers(EigenFaces, FisherFaces and LBPH) to two original solutions based uponexisting convolutional neural networks: BARK (inspired inarchitecture-optimized networks employed for human facial recognition) and WOOF(based upon off-the-shelf OverFeat features). Human facial recognizers performpoorly for dogs (up to 56.1% accuracy), showing that dog facial recognition isnot a trivial extension of human facial recognition. The convolutional networksolutions work much better, with BARK attaining up to 81.1% accuracy, and WOOF,89.4%. The tests were conducted in two datasets: Flickr-dog, with 42 dogs oftwo breeds (pugs and huskies); and Snoopybook, with 18 mongrel dogs.
arxiv-12900-220 | Human Head Pose Estimation by Facial Features Location | http://arxiv.org/pdf/1510.02774v1.pdf | author:Eugene Borovikov category:cs.CV published:2015-10-09 summary:We describe a method for estimating human head pose in a color image thatcontains enough of information to locate the head silhouette and detectnon-trivial color edges of individual facial features. The method works byspotting the human head on an arbitrary background, extracting the headoutline, and locating facial features necessary to describe the headorientation in the 3D space. It is robust enough to work with both color andgray-level images featuring quasi-frontal views of a human head under variablelighting conditions.
arxiv-12900-221 | Procams-Based Cybernetics | http://arxiv.org/pdf/1510.02710v1.pdf | author:Kosuke Sato, Daisuke Iwai, Sei Ikeda, Noriko Takemura category:cs.CV cs.GR cs.HC published:2015-10-09 summary:Procams-based cybernetics is a unique, emerging research field, which aims atenhancing and supporting our activities by naturally connecting human andcomputers/machines as a cooperative integrated system via projector-camerasystems (procams). It rests on various research domains such asvirtual/augmented reality, computer vision, computer graphics, projectiondisplay, human computer interface, human robot interaction and so on. Thislaboratory presentation provides a brief history including recent achievementsof our procams-based cybernetics project.
arxiv-12900-222 | Large-scale Artificial Neural Network: MapReduce-based Deep Learning | http://arxiv.org/pdf/1510.02709v1.pdf | author:Kairan Sun, Xu Wei, Gengtao Jia, Risheng Wang, Ruizhi Li category:cs.DC cs.LG cs.NE published:2015-10-09 summary:Faced with continuously increasing scale of data, original back-propagationneural network based machine learning algorithm presents two non-trivialchallenges: huge amount of data makes it difficult to maintain both efficiencyand accuracy; redundant data aggravates the system workload. This project ismainly focused on the solution to the issues above, combining deep learningalgorithm with cloud computing platform to deal with large-scale data. AMapReduce-based handwriting character recognizer will be designed in thisproject to verify the efficiency improvement this mechanism will achieve ontraining and practical large-scale data. Careful discussion and experiment willbe developed to illustrate how deep learning algorithm works to trainhandwritten digits data, how MapReduce is implemented on deep learning neuralnetwork, and why this combination accelerates computation. Besides performance,the scalability and robustness will be mentioned in this report as well. Oursystem comes with two demonstration software that visually illustrates ourhandwritten digit recognition/encoding application.
arxiv-12900-223 | Data-Efficient Learning of Feedback Policies from Image Pixels using Deep Dynamical Models | http://arxiv.org/pdf/1510.02173v2.pdf | author:John-Alexander M. Assael, Niklas Wahlström, Thomas B. Schön, Marc Peter Deisenroth category:cs.AI cs.CV cs.LG stat.ML published:2015-10-08 summary:Data-efficient reinforcement learning (RL) in continuous state-action spacesusing very high-dimensional observations remains a key challenge in developingfully autonomous systems. We consider a particularly important instance of thischallenge, the pixels-to-torques problem, where an RL agent learns aclosed-loop control policy ("torques") from pixel information only. Weintroduce a data-efficient, model-based reinforcement learning algorithm thatlearns such a closed-loop policy directly from pixel information. The keyingredient is a deep dynamical model for learning a low-dimensional featureembedding of images jointly with a predictive model in this low-dimensionalfeature space. Joint learning is crucial for long-term predictions, which lieat the core of the adaptive nonlinear model predictive control strategy that weuse for closed-loop control. Compared to state-of-the-art RL methods forcontinuous states and actions, our approach learns quickly, scales tohigh-dimensional state spaces, is lightweight and an important step towardfully autonomous end-to-end learning from pixels to torques.
arxiv-12900-224 | Feedforward Sequential Memory Neural Networks without Recurrent Feedback | http://arxiv.org/pdf/1510.02693v1.pdf | author:ShiLiang Zhang, Hui Jiang, Si Wei, LiRong Dai category:cs.NE cs.CL cs.LG published:2015-10-09 summary:We introduce a new structure for memory neural networks, called feedforwardsequential memory networks (FSMN), which can learn long-term dependency withoutusing recurrent feedback. The proposed FSMN is a standard feedforward neuralnetworks equipped with learnable sequential memory blocks in the hidden layers.In this work, we have applied FSMN to several language modeling (LM) tasks.Experimental results have shown that the memory blocks in FSMN can learneffective representations of long history. Experiments have shown that FSMNbased language models can significantly outperform not only feedforward neuralnetwork (FNN) based LMs but also the popular recurrent neural network (RNN)LMs.
arxiv-12900-225 | Clustering Network Layers With the Strata Multilayer Stochastic Block Model | http://arxiv.org/pdf/1507.01826v2.pdf | author:Natalie Stanley, Saray Shai, Dane Taylor, Peter J. Mucha category:cs.SI physics.soc-ph stat.ML published:2015-07-07 summary:Multilayer networks are a useful data structure for simultaneously capturingmultiple types of relationships between a set of nodes. In such networks, eachrelational definition gives rise to a layer. While each layer provides its ownset of information, community structure across layers can be collectivelyutilized to discover and quantify underlying relational patterns between nodes.To concisely extract information from a multilayer network, we propose toidentify and combine sets of layers with meaningful similarities in communitystructure. In this paper, we describe the "strata multilayer stochastic blockmodel'' (sMLSBM), a probabilistic model for multilayer community structure. Thecentral extension of the model is that there exist groups of layers, called"strata'', which are defined such that all layers in a given stratum havecommunity structure described by a common stochastic block model (SBM). Thatis, layers in a stratum exhibit similar node-to-community assignments and SBMprobability parameters. Fitting the sMLSBM to a multilayer network provides ajoint clustering that yields node-to-community and layer-to-stratumassignments, which cooperatively aid one another during inference. We describean algorithm for separating layers into their appropriate strata and aninference technique for estimating the SBM parameters for each stratum. Wedemonstrate our method using synthetic networks and a multilayer networkinferred from data collected in the Human Microbiome Project.
arxiv-12900-226 | Some Theory For Practical Classifier Validation | http://arxiv.org/pdf/1510.02676v1.pdf | author:Eric Bax, Ya Le category:stat.ML cs.LG published:2015-10-09 summary:We compare and contrast two approaches to validating a trained classifierwhile using all in-sample data for training. One is simultaneous validationover an organized set of hypotheses (SVOOSH), the well-known method that beganwith VC theory. The other is withhold and gap (WAG). WAG withholds a validationset, trains a holdout classifier on the remaining data, uses the validationdata to validate that classifier, then adds the rate of disagreement betweenthe holdout classifier and one trained using all in-sample data, which is anupper bound on the difference in error rates. We show that complex hypothesisclasses and limited training data can make WAG a favorable alternative.
arxiv-12900-227 | Technical Report of Participation in Higgs Boson Machine Learning Challenge | http://arxiv.org/pdf/1510.02674v1.pdf | author:S. Raza Ahmad category:cs.LG published:2015-10-09 summary:This report entails the detailed description of the approach andmethodologies taken as part of competing in the Higgs Boson Machine LearningCompetition hosted by Kaggle Inc. and organized by CERN et al. It brieflydescribes the theoretical background of the problem and the motivation fortaking part in the competition. Furthermore, the various machine learningmodels and algorithms analyzed and implemented during the 4 month period ofparticipation are discussed and compared. Special attention is paid to the DeepLearning techniques and architectures implemented from scratch using Python andNumPy for this competition.
arxiv-12900-228 | Free-hand Sketch Synthesis with Deformable Stroke Models | http://arxiv.org/pdf/1510.02644v1.pdf | author:Yi Li, Yi-Zhe Song, Timothy Hospedales, Shaogang Gong category:cs.CV published:2015-10-09 summary:We present a generative model which can automatically summarize the strokecomposition of free-hand sketches of a given category. When our model is fit toa collection of sketches with similar poses, it discovers and learns thestructure and appearance of a set of coherent parts, with each part representedby a group of strokes. It represents both consistent (topology) as well asdiverse aspects (structure and appearance variations) of each sketch category.Key to the success of our model are important insights learned from acomprehensive study performed on human stroke data. By fitting this model toimages, we are able to synthesize visually similar and pleasant free-handsketches.
arxiv-12900-229 | Stochastic recursive inclusion in two timescales with an application to the Lagrangian dual problem | http://arxiv.org/pdf/1502.01956v2.pdf | author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY math.DS stat.ML published:2015-02-06 summary:In this paper we present a framework to analyze the asymptotic behavior oftwo timescale stochastic approximation algorithms including those withset-valued mean fields. This paper builds on the works of Borkar and Perkins &Leslie. The framework presented herein is more general as compared to thesynchronous two timescale framework of Perkins \& Leslie, however theassumptions involved are easily verifiable. As an application, we use thisframework to analyze the two timescale stochastic approximation algorithmcorresponding to the Lagrangian dual problem in optimization theory.
arxiv-12900-230 | Functional Frank-Wolfe Boosting for General Loss Functions | http://arxiv.org/pdf/1510.02558v1.pdf | author:Chu Wang, Yingfei Wang, Weinan E, Robert Schapire category:stat.ML cs.LG published:2015-10-09 summary:Boosting is a generic learning method for classification and regression. Yet,as the number of base hypotheses becomes larger, boosting can lead to adeterioration of test performance. Overfitting is an important and ubiquitousphenomenon, especially in regression settings. To avoid overfitting, weconsider using $l_1$ regularization. We propose a novel Frank-Wolfe typeboosting algorithm (FWBoost) applied to general loss functions. By usingexponential loss, the FWBoost algorithm can be rewritten as a variant ofAdaBoost for binary classification. FWBoost algorithms have exactly the sameform as existing boosting methods, in terms of making calls to a base learningalgorithm with different weights update. This direct connection betweenboosting and Frank-Wolfe yields a new algorithm that is as practical asexisting boosting methods but with new guarantees and rates of convergence.Experimental results show that the test performance of FWBoost is not degradedwith larger rounds in boosting, which is consistent with the theoreticalanalysis.
arxiv-12900-231 | Fixed-point algorithms for learning determinantal point processes | http://arxiv.org/pdf/1508.00792v2.pdf | author:Zelda Mariet, Suvrit Sra category:cs.LG published:2015-08-04 summary:Determinantal point processes (DPPs) offer an elegant tool for encodingprobabilities over subsets of a ground set. Discrete DPPs are parametrized by apositive semidefinite matrix (called the DPP kernel), and estimating thiskernel is key to learning DPPs from observed data. We consider the task oflearning the DPP kernel, and develop for it a surprisingly simple yet effectivenew algorithm. Our algorithm offers the following benefits over previousapproaches: (a) it is much simpler; (b) it yields equally good and sometimeseven better local maxima; and (c) it runs an order of magnitude faster on largeproblems. We present experimental results on both real and simulated data toillustrate the numerical performance of our technique.
arxiv-12900-232 | Statistical Analysis of Persistence Intensity Functions | http://arxiv.org/pdf/1510.02502v1.pdf | author:Yen-Chi Chen, Daren Wang, Alessandro Rinaldo, Larry Wasserman category:stat.ME stat.ML published:2015-10-08 summary:Persistence diagrams are two-dimensional plots that summarize the topologicalfeatures of functions and are an important part of topological data analysis. Aproblem that has received much attention is how deal with sets of persistencediagrams. How do we summarize them, average them or cluster them? One approach-- the persistence intensity function -- was introduced informally byEdelsbrunner, Ivanov, and Karasev (2012). Here we provide a modification andformalization of this approach. Using the persistence intensity function, wecan visualize multiple diagrams, perform clustering and conduct two-sampletests.
arxiv-12900-233 | Uniform Learning in a Deep Neural Network via "Oddball" Stochastic Gradient Descent | http://arxiv.org/pdf/1510.02442v1.pdf | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-10-08 summary:When training deep neural networks, it is typically assumed that the trainingexamples are uniformly difficult to learn. Or, to restate, it is assumed thatthe training error will be uniformly distributed across the training examples.Based on these assumptions, each training example is used an equal number oftimes. However, this assumption may not be valid in many cases. "Oddball SGD"(novelty-driven stochastic gradient descent) was recently introduced to drivetraining probabilistically according to the error distribution - trainingfrequency is proportional to training error magnitude. In this article, using adeep neural network to encode a video, we show that oddball SGD can be used toenforce uniform error across the training set.
arxiv-12900-234 | Distilling Model Knowledge | http://arxiv.org/pdf/1510.02437v1.pdf | author:George Papamakarios category:stat.ML cs.LG published:2015-10-08 summary:Top-performing machine learning systems, such as deep neural networks, largeensembles and complex probabilistic graphical models, can be expensive tostore, slow to evaluate and hard to integrate into larger systems. Ideally, wewould like to replace such cumbersome models with simpler models that performequally well. In this thesis, we study knowledge distillation, the idea of extracting theknowledge contained in a complex model and injecting it into a more convenientmodel. We present a general framework for knowledge distillation, whereby aconvenient model of our choosing learns how to mimic a complex model, byobserving the latter's behaviour and being penalized whenever it fails toreproduce it. We develop our framework within the context of three distinct machinelearning applications: (a) model compression, where we compress largediscriminative models, such as ensembles of neural networks, into models ofmuch smaller size; (b) compact predictive distributions for Bayesian inference,where we distil large bags of MCMC samples into compact predictivedistributions in closed form; (c) intractable generative models, where wedistil unnormalizable models such as RBMs into tractable models such as NADEs. We contribute to the state of the art with novel techniques and ideas. Inmodel compression, we describe and implement derivative matching, which allowsfor better distillation when data is scarce. In compact predictivedistributions, we introduce online distillation, which allows for significantsavings in memory. Finally, in intractable generative models, we show how touse distilled models to robustly estimate intractable quantities of theoriginal model, such as its intractable partition function.
arxiv-12900-235 | Exact Inference Techniques for the Dynamic Analysis of Attack Graphs | http://arxiv.org/pdf/1510.02427v1.pdf | author:Luis Muñoz-González, Daniele Sgandurra, Martín Barrère, Emil Lupu category:cs.CR stat.AP stat.ML 62F15 published:2015-10-08 summary:Attack graphs are a powerful tool for security risk assessment by analysingnetwork vulnerabilities and the paths attackers can use to compromise valuablenetwork resources. The uncertainty about the attacker's behaviour andcapabilities make Bayesian networks suitable to model attack graphs to performstatic and dynamic analysis. Previous approaches have focused on theformalization of traditional attack graphs into a Bayesian model rather thanproposing mechanisms for their analysis. In this paper we propose to useefficient algorithms to make exact inference in Bayesian attack graphs,enabling the static and dynamic network risk assessments. To support thevalidity of our proposed approach we have performed an extensive experimentalevaluation on synthetic Bayesian attack graphs with different topologies,showing the computational advantages in terms of time and memory use of theproposed techniques when compared to existing approaches.
arxiv-12900-236 | Learning Data-driven Reflectance Priors for Intrinsic Image Decomposition | http://arxiv.org/pdf/1510.02413v1.pdf | author:Tinghui Zhou, Philipp Krähenbühl, Alexei A. Efros category:cs.CV published:2015-10-08 summary:We propose a data-driven approach for intrinsic image decomposition, which isthe process of inferring the confounding factors of reflectance and shading inan image. We pose this as a two-stage learning problem. First, we train a modelto predict relative reflectance ordering between image patches (`brighter',`darker', `same') from large-scale human annotations, producing a data-drivenreflectance prior. Second, we show how to naturally integrate this learnedprior into existing energy minimization frameworks for intrinsic imagedecomposition. We compare our method to the state-of-the-art approach of Bellet al. on both decomposition and image relighting tasks, demonstrating thebenefits of the simple relative reflectance prior, especially for scenes underchallenging lighting conditions.
arxiv-12900-237 | Particle Metropolis adjusted Langevin algorithms | http://arxiv.org/pdf/1412.7299v2.pdf | author:Christopher Nemeth, Chris Sherlock, Paul Fearnhead category:stat.ME stat.CO stat.ML published:2014-12-23 summary:Pseudo-marginal and particle MCMC have recently been introduced as classes ofalgorithms that can be used to analyse models where the likelihood function isintractable. They use Monte Carlo methods, such as particle filters, toestimate the posterior density, and MCMC moves to update the model parameters.Particle filter algorithms can also produce Monte Carlo estimates of thegradient of the log-posterior which can then be used within the MCMC proposaldistribution for the parameters. The resulting particle MCMC algorithm can beviewed as an approximation to the Metropolis adjusted Langevin algorithm, whichwe call particle MALA. We investigate the theoretical properties of particleMALA under standard asymptotics, which correspond to an increasing dimension ofthe parameters, n. Our results show that the behaviour of particle MALA dependscrucially on how accurately one can estimate the gradient of the log-posterior.If the error in the estimate of the gradient is not controlled sufficientlywell as dimension increases, then asymptotically there will be no advantage inusing particle MALA over the simpler random-walk proposal. However, if theerror is well-behaved, then the optimal scaling of particle MALA proposals willbe $O(n^{-1/6})$ compared to $O(n^{-1/2})$ for the random-walk. Our theory alsogives guidelines as to how to tune the number of particles and the step sizeused within particle MALA.
arxiv-12900-238 | Constrained $H^1$-regularization schemes for diffeomorphic image registration | http://arxiv.org/pdf/1503.00757v2.pdf | author:Andreas Mang, George Biros category:math.OC cs.CV published:2015-03-02 summary:We propose regularization schemes for deformable registration and efficientalgorithms for its numerical approximation. We treat image registration as avariational optimal control problem. The deformation map is parametrized by itsvelocity. Tikhonov regularization ensures well-posedness. Our scheme augmentsstandard smoothness regularization operators based on $H^1$- and$H^2$-seminorms with a constraint on the divergence of the velocity field,which resembles variational formulations for Stokes incompressible flows. Inour formulation, we invert for a stationary velocity field and a mass sourcemap. This allows us to explicitly control the compressibility of thedeformation map and by that the determinant of the deformation gradient. Wealso introduce a new regularization scheme that allows us to control shear. We use a globalized, preconditioned, matrix-free, reduced spaceGauss-Newton-Krylov scheme for numerical optimization. We exploit variableelimination techniques to reduce the number of unknowns of our system; we onlyiterate on the reduced space of the velocity field. The numerical experimentsdemonstrate that we can control the determinant of the deformation gradientwithout compromising registration quality. This additional control allows us toavoid oversmoothing of the deformation map. We also demonstrate that we canpromote or penalize shear whilst controlling the determinant of the deformationgradient.
arxiv-12900-239 | Mapping Unseen Words to Task-Trained Embedding Spaces | http://arxiv.org/pdf/1510.02387v1.pdf | author:Pranava Swaroop Madhyastha, Mohit Bansal, Kevin Gimpel, Karen Livescu category:cs.CL cs.LG published:2015-10-08 summary:We consider the setting in which we train a supervised model that learnstask-specific word representations. We assume that we have access to someinitial word representations (e.g., unsupervised embeddings), and that thesupervised learning procedure updates them to task-specific representations forwords contained in the training data. But what about words not contained in thesupervised training data? When such unseen words are encountered at test time,they are typically represented by either their initial vectors or a singleunknown vector, which often leads to errors. In this paper, we address thisissue by learning to map from initial representations to task-specific ones. Wepresent a general technique that uses a neural network mapper with a weightedmultiple-loss criterion. This allows us to use the same learned modelparameters at test time but now with appropriate task-specific representationsfor unseen words. We consider the task of dependency parsing and reportimprovements in performance (and reductions in out-of-vocabulary rates) acrossmultiple domains such as news, Web, and speech. We also achieve downstreamimprovements on the task of parsing-based sentiment analysis.
arxiv-12900-240 | Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory | http://arxiv.org/pdf/1509.08255v2.pdf | author:Fergal Byrne category:cs.NE cs.AI published:2015-09-28 summary:In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM)as a model of neocortical computation, the theory and the algorithms haveevolved dramatically. This paper presents a detailed description of HTM'sCortical Learning Algorithm (CLA), including for the first time a rigorousmathematical formulation of all aspects of the computations. PredictionAssisted CLA (paCLA), a refinement of the CLA is presented, which is bothcloser to the neuroscience and adds significantly to the computational power.Finally, we summarise the key functions of neocortex which are expressed inpaCLA implementations.
arxiv-12900-241 | Distance-weighted Support Vector Machine | http://arxiv.org/pdf/1310.3003v3.pdf | author:Xingye Qiao, Lingsong Zhang category:stat.ML published:2013-10-11 summary:A novel linear classification method that possesses the merits of both theSupport Vector Machine (SVM) and the Distance-weighted Discrimination (DWD) isproposed in this article. The proposed Distance-weighted Support Vector Machinemethod can be viewed as a hybrid of SVM and DWD that finds the classificationdirection by minimizing mainly the DWD loss, and determines the intercept termin the SVM manner. We show that our method inheres the merit of DWD, and hence,overcomes the data-piling and overfitting issue of SVM. On the other hand, thenew method is not subject to imbalanced data issue which was a main advantageof SVM over DWD. It uses an unusual loss which combines the Hinge loss (of SVM)and the DWD loss through a trick of axillary hyperplane. Several theoreticalproperties, including Fisher consistency and asymptotic normality of the DWSVMsolution are developed. We use some simulated examples to show that the newmethod can compete DWD and SVM on both classification performance andinterpretability. A real data application further establishes the usefulness ofour approach.
arxiv-12900-242 | Texture Modelling with Nested High-order Markov-Gibbs Random Fields | http://arxiv.org/pdf/1510.02364v1.pdf | author:Ralph Versteegen, Georgy Gimel'farb, Patricia Riddle category:cs.CV cs.LG stat.ML published:2015-10-08 summary:Currently, Markov-Gibbs random field (MGRF) image models which includehigh-order interactions are almost always built by modelling responses of astack of local linear filters. Actual interaction structure is specifiedimplicitly by the filter coefficients. In contrast, we learn an explicithigh-order MGRF structure by considering the learning process in terms ofgeneral exponential family distributions nested over base models, so thatpotentials added later can build on previous ones. We relatively rapidly addnew features by skipping over the costly optimisation of parameters. We introduce the use of local binary patterns as features in MGRF texturemodels, and generalise them by learning offsets to the surrounding pixels.These prove effective as high-order features, and are fast to compute. Severalschemes for selecting high-order features by composition or search of a smallsubclass are compared. Additionally we present a simple modification of themaximum likelihood as a texture modelling-specific objective function whichaims to improve generalisation by local windowing of statistics. The proposed method was experimentally evaluated by learning high-order MGRFmodels for a broad selection of complex textures and then performing texturesynthesis, and succeeded on much of the continuum from stochastic throughirregularly structured to near-regular textures. Learning interaction structureis very beneficial for textures with large-scale structure, although those withcomplex irregular structure still provide difficulties. The texture models werealso quantitatively evaluated on two tasks and found to be competitive withother works: grading of synthesised textures by a panel of observers; andcomparison against several recent MGRF models by evaluation on a constrainedinpainting task.
arxiv-12900-243 | The Knowledge Gradient with Logistic Belief Models for Binary Classification | http://arxiv.org/pdf/1510.02354v1.pdf | author:Yingfei Wang, Chu Wang, Warren Powell category:stat.ML published:2015-10-08 summary:We consider sequential decision making problems for binary classificationscenario in which the learner takes an active role in repeatedly selectingsamples from the action pool and receives the binary label of the selectedalternatives. Our problem is motivated by applications where observations aretime consuming and/or expensive, resulting in small samples. The goal is toidentify the best alternative with the highest response. We use Bayesianlogistic regression to predict the response of each alternative. By formulatingthe problem as a Markov decision process, we develop a knowledge-gradient typepolicy to guide the experiment by maximizing the expected value of informationof labeling each alternative and provide a finite-time analysis on theestimated error. Experiments on benchmark UCI datasets demonstrate theeffectiveness of the proposed method.
arxiv-12900-244 | Automata networks for memory loss effects in the formation of linguistic conventions | http://arxiv.org/pdf/1508.01580v2.pdf | author:Javier Vera, Eric Goles category:cs.CL physics.soc-ph published:2015-08-07 summary:This work attempts to give new theoretical insights to the absence ofintermediate stages in the evolution of language. In particular, it isdeveloped an automata networks approach to a crucial question: how a populationof language users can reach agreement on a linguistic convention? To describethe appearance of sharp transitions in the self-organization of language, it isadopted an extremely simple model of (working) memory. At each time step,language users simply loss part of their word-memories. Through computersimulations of low-dimensional lattices, it appear sharp transitions atcritical values that depend on the size of the vicinities of the individuals.
arxiv-12900-245 | DeepMatching: Hierarchical Deformable Dense Matching | http://arxiv.org/pdf/1506.07656v2.pdf | author:Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid category:cs.CV published:2015-06-25 summary:We introduce a novel matching algorithm, called DeepMatching, to computedense correspondences between images. DeepMatching relies on a hierarchical,multi-layer, correlational architecture designed for matching images and wasinspired by deep convolutional approaches. The proposed matching algorithm canhandle non-rigid deformations and repetitive textures and efficientlydetermines dense correspondences in the presence of significant changes betweenimages. We evaluate the performance of DeepMatching, in comparison withstate-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013)datasets. DeepMatching outperforms the state-of-the-art algorithms and showsexcellent results in particular for repetitive textures.We also propose amethod for estimating optical flow, called DeepFlow, by integratingDeepMatching in the large displacement optical flow (LDOF) approach of Brox andMalik (2011). Compared to existing matching algorithms, additional robustnessto large displacements and complex motion is obtained thanks to our matchingapproach. DeepFlow obtains competitive performance on public benchmarks foroptical flow estimation.
arxiv-12900-246 | Scalable Greedy Algorithms for Transfer Learning | http://arxiv.org/pdf/1408.1292v3.pdf | author:Ilja Kuzborskij, Francesco Orabona, Barbara Caputo category:cs.CV cs.LG published:2014-08-06 summary:In this paper we consider the binary transfer learning problem, focusing onhow to select and combine sources from a large pool to yield a good performanceon a target task. Constraining our scenario to real world, we do not assume thedirect access to the source data, but rather we employ the source hypothesestrained from them. We propose an efficient algorithm that selects relevantsource hypotheses and feature dimensions simultaneously, building on theliterature on the best subset selection problem. Our algorithm achievesstate-of-the-art results on three computer vision datasets, substantiallyoutperforming both transfer learning and popular feature selection baselines ina small-sample setting. We also present a randomized variant that achieves thesame results with a fraction of the computational cost. Also, we theoreticallyprove that, under reasonable assumptions on the source hypotheses, ouralgorithm can learn effectively from few examples.
arxiv-12900-247 | Reduced-Order Modeling Of Hidden Dynamics | http://arxiv.org/pdf/1510.02267v1.pdf | author:Patrick Héas, Cédric Herzet category:stat.ML stat.AP published:2015-10-08 summary:The objective of this paper is to investigate how noisy and incompleteobservations can be integrated in the process of building a reduced-ordermodel. This problematic arises in many scientific domains where there exists a needfor accurate low-order descriptions of highly-complex phenomena, which can notbe directly and/or deterministically observed. Within this context, the paperproposes a probabilistic framework for the construction of "POD-Galerkin"reduced-order models. Assuming a hidden Markov chain, the inference integratesthe uncertainty of the hidden states relying on their posterior distribution.Simulations show the benefits obtained by exploiting the proposed framework.
arxiv-12900-248 | Empirical Analysis of Sampling Based Estimators for Evaluating RBMs | http://arxiv.org/pdf/1510.02255v1.pdf | author:Vidyadhar Upadhya, P. S. Sastry category:cs.LG stat.ML published:2015-10-08 summary:The Restricted Boltzmann Machines (RBM) can be used either as classifiers oras generative models. The quality of the generative RBM is measured through theaverage log-likelihood on test data. Due to the high computational complexityof evaluating the partition function, exact calculation of test log-likelihoodis very difficult. In recent years some estimation methods are suggested forapproximate computation of test log-likelihood. In this paper we present anempirical comparison of the main estimation methods, namely, the AIS algorithmfor estimating the partition function, the CSL method for directly estimatingthe log-likelihood, and the RAISE algorithm that combines these two ideas. Weuse the MNIST data set to learn the RBM and then compare these methods forestimating the test log-likelihood.
arxiv-12900-249 | Attribute-Graph: A Graph based approach to Image Ranking | http://arxiv.org/pdf/1509.06658v2.pdf | author:Nikita Prabhu, R. Venkatesh Babu category:cs.CV published:2015-09-22 summary:We propose a novel image representation, termed Attribute-Graph, to rankimages by their semantic similarity to a given query image. An Attribute-Graphis an undirected fully connected graph, incorporating both local and globalimage characteristics. The graph nodes characterise objects as well as theoverall scene context using mid-level semantic attributes, while the edgescapture the object topology. We demonstrate the effectiveness ofAttribute-Graphs by applying them to the problem of image ranking. We benchmarkthe performance of our algorithm on the 'rPascal' and 'rImageNet' datasets,which we have created in order to evaluate the ranking performance on complexqueries containing multiple objects. Our experimental evaluation shows thatmodelling images as Attribute-Graphs results in improved ranking performanceover existing techniques.
arxiv-12900-250 | Simultaneous Deep Transfer Across Domains and Tasks | http://arxiv.org/pdf/1510.02192v1.pdf | author:Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko category:cs.CV published:2015-10-08 summary:Recent reports suggest that a generic supervised deep CNN model trained on alarge-scale dataset reduces, but does not remove, dataset bias. Fine-tuningdeep models in a new domain can require a significant amount of labeled data,which for many applications is simply not available. We propose a new CNNarchitecture to exploit unlabeled and sparsely labeled target domain data. Ourapproach simultaneously optimizes for domain invariance to facilitate domaintransfer and uses a soft label distribution matching loss to transferinformation between tasks. Our proposed adaptation method offers empiricalperformance which exceeds previously published results on two standardbenchmark visual domain adaptation tasks, evaluated across supervised andsemi-supervised adaptation settings.
arxiv-12900-251 | Learning Summary Statistic for Approximate Bayesian Computation via Deep Neural Network | http://arxiv.org/pdf/1510.02175v1.pdf | author:Bai Jiang, Tung-yu Wu, Charles Zheng, Wing H. Wong category:stat.ME stat.CO stat.ML published:2015-10-08 summary:Approximate Bayesian Computation (ABC) methods are used to approximateposterior distributions in models with unknown or computationally intractablelikelihoods. Both the accuracy and computational efficiency of ABC depend onthe choice of summary statistic, but outside of special cases where the optimalsummary statistics are known, it is unclear which guiding principles can beused to construct effective summary statistics. In this paper we explore thepossibility of automating the process of constructing summary statistics bytraining deep neural networks to predict the parameters from artificiallygenerated data: the resulting summary statistics are approximately posteriormeans of the parameters. With minimal model-specific tuning, our methodconstructs summary statistics for the Ising model and the moving-average model,which match or exceed theoretically-motivated summary statistics in terms ofthe accuracies of the resulting posteriors.
arxiv-12900-252 | Webly Supervised Learning of Convolutional Networks | http://arxiv.org/pdf/1505.01554v2.pdf | author:Xinlei Chen, Abhinav Gupta category:cs.CV published:2015-05-07 summary:We present an approach to utilize large amounts of web data for learningCNNs. Specifically inspired by curriculum learning, we present a two-stepapproach for CNN training. First, we use easy images to train an initial visualrepresentation. We then use this initial CNN and adapt it to harder, morerealistic images by leveraging the structure of data and categories. Wedemonstrate that our two-stage CNN outperforms a fine-tuned CNN trained onImageNet on Pascal VOC 2012. We also demonstrate the strength of weblysupervised learning by localizing objects in web images and training a R-CNNstyle detector. It achieves the best performance on VOC 2007 where no VOCtraining data is used. Finally, we show our approach is quite robust to noiseand performs comparably even when we use image search results from March 2013(pre-CNN image search era).
arxiv-12900-253 | DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer | http://arxiv.org/pdf/1510.02131v1.pdf | author:Forrest N. Iandola, Anting Shen, Peter Gao, Kurt Keutzer category:cs.CV published:2015-10-07 summary:Recently, there has been a flurry of industrial activity around logorecognition, such as Ditto's service for marketers to track their brands inuser-generated images, and LogoGrab's mobile app platform for logo recognition.However, relatively little academic or open-source logo recognition progresshas been made in the last four years. Meanwhile, deep convolutional neuralnetworks (DCNNs) have revolutionized a broad range of object recognitionapplications. In this work, we apply DCNNs to logo recognition. We proposeseveral DCNN architectures, with which we surpass published state-of-artaccuracy on a popular logo recognition dataset.
arxiv-12900-254 | On the Projective Geometry of Kalman Filter | http://arxiv.org/pdf/1503.09113v2.pdf | author:Francesca Paola Carli, Rodolphe Sepulchre category:math.OC stat.ML published:2015-03-31 summary:Convergence of the Kalman filter is best analyzed by studying the contractionof the Riccati map in the space of positive definite (covariance) matrices. Inthis paper, we explore how this contraction property relates to a morefundamental non-expansiveness property of filtering maps in the space ofprobability distributions endowed with the Hilbert metric. This is viewed as apreliminary step towards improving the convergence analysis of filteringalgorithms over general graphical models.
arxiv-12900-255 | Leveraging Context to Support Automated Food Recognition in Restaurants | http://arxiv.org/pdf/1510.02078v1.pdf | author:Vinay Bettadapura, Edison Thomaz, Aman Parnami, Gregory Abowd, Irfan Essa category:cs.CV published:2015-10-07 summary:The pervasiveness of mobile cameras has resulted in a dramatic increase infood photos, which are pictures reflecting what people eat. In this paper, westudy how taking pictures of what we eat in restaurants can be used for thepurpose of automating food journaling. We propose to leverage the context ofwhere the picture was taken, with additional information about the restaurant,available online, coupled with state-of-the-art computer vision techniques torecognize the food being consumed. To this end, we demonstrate image-basedrecognition of foods eaten in restaurants by training a classifier with imagesfrom restaurant's online menu databases. We evaluate the performance of oursystem in unconstrained, real-world settings with food images taken in 10restaurants across 5 different types of food (American, Indian, Italian,Mexican and Thai).
arxiv-12900-256 | Egocentric Field-of-View Localization Using First-Person Point-of-View Devices | http://arxiv.org/pdf/1510.02073v1.pdf | author:Vinay Bettadapura, Irfan Essa, Caroline Pantofaru category:cs.CV published:2015-10-07 summary:We present a technique that uses images, videos and sensor data taken fromfirst-person point-of-view devices to perform egocentric field-of-view (FOV)localization. We define egocentric FOV localization as capturing the visualinformation from a person's field-of-view in a given environment andtransferring this information onto a reference corpus of images and videos ofthe same space, hence determining what a person is attending to. Our methodmatches images and video taken from the first-person perspective with thereference corpus and refines the results using the first-person's headorientation information obtained using the device sensors. We demonstratesingle and multi-user egocentric FOV localization in different indoor andoutdoor environments with applications in augmented reality, eventunderstanding and studying social interactions.
arxiv-12900-257 | Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and Structural Information for Activity Recognition | http://arxiv.org/pdf/1510.02071v1.pdf | author:Vinay Bettadapura, Grant Schindler, Thomaz Plotz, Irfan Essa category:cs.CV published:2015-10-07 summary:We present data-driven techniques to augment Bag of Words (BoW) models, whichallow for more robust modeling and recognition of complex long-term activities,especially when the structure and topology of the activities are not known apriori. Our approach specifically addresses the limitations of standard BoWapproaches, which fail to represent the underlying temporal and causalinformation that is inherent in activity streams. In addition, we also proposethe use of randomly sampled regular expressions to discover and encode patternsin activities. We demonstrate the effectiveness of our approach in experimentalevaluations where we successfully recognize activities and detect anomalies infour complex datasets.
arxiv-12900-258 | Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology | http://arxiv.org/pdf/1508.03868v3.pdf | author:Brendan Jou, Tao Chen, Nikolaos Pappas, Miriam Redi, Mercan Topkara, Shih-Fu Chang category:cs.MM cs.CL cs.CV cs.IR published:2015-08-16 summary:Every culture and language is unique. Our work expressly focuses on theuniqueness of culture and language in relation to human affect, specificallysentiment and emotion semantics, and how they manifest in social multimedia. Wedevelop sets of sentiment- and emotion-polarized visual concepts by adaptingsemantic structures called adjective-noun pairs, originally introduced by Borthet al. (2013), but in a multilingual context. We propose a newlanguage-dependent method for automatic discovery of these adjective-nounconstructs. We show how this pipeline can be applied on a social multimediaplatform for the creation of a large-scale multilingual visual sentimentconcept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), ourunified ontology is organized hierarchically by multilingual clusters ofvisually detectable nouns and subclusters of emotionally biased versions ofthese nouns. In addition, we present an image-based prediction task to show howgeneralizable language-specific models are in a multilingual context. A new,publicly available dataset of >15.6K sentiment-biased visual concepts across 12languages with language-specific detector banks, >7.36M images and theirmetadata is also released.
arxiv-12900-259 | Diverse Large-Scale ITS Dataset Created from Continuous Learning for Real-Time Vehicle Detection | http://arxiv.org/pdf/1510.02055v1.pdf | author:Justin A. Eichel, Akshaya Mishra, Nicholas Miller, Nicholas Jankovic, Mohan A. Thomas, Tyler Abbott, Douglas Swanson, Joel Keller category:cs.CV published:2015-10-07 summary:In traffic engineering, vehicle detectors are trained on limited datasetsresulting in poor accuracy when deployed in real world applications. Annotatinglarge-scale high quality datasets is challenging. Typically, these datasetshave limited diversity; they do not reflect the real-world operatingenvironment. There is a need for a large-scale, cloud based positive andnegative mining (PNM) process and a large-scale learning and evaluation systemfor the application of traffic event detection. The proposed positive andnegative mining process addresses the quality of crowd sourced ground truthdata through machine learning review and human feedback mechanisms. Theproposed learning and evaluation system uses a distributed cloud computingframework to handle data-scaling issues associated with large numbers ofsamples and a high-dimensional feature space. The system is trained usingAdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotatedvideo frames. The trained real-time vehicle detector achieves an accuracy of atleast $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested onapproximately $7,500,000$ video frames. At the end of 2015, the dataset isexpect to have over one billion annotated video frames.
arxiv-12900-260 | Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations | http://arxiv.org/pdf/1510.02054v1.pdf | author:Weiran Wang, Raman Arora, Karen Livescu, Nathan Srebro category:cs.LG published:2015-10-07 summary:Deep CCA is a recently proposed deep neural network extension to thetraditional canonical correlation analysis (CCA), and has been successful formulti-view representation learning in several domains. However, stochasticoptimization of the deep CCA objective is not straightforward, because it doesnot decouple over training examples. Previous optimizers for deep CCA areeither batch-based algorithms or stochastic optimization using largeminibatches, which can have high memory consumption. In this paper, we tacklethe problem of stochastic optimization for deep CCA with small minibatches,based on an iterative solution to the CCA objective, and show that we canachieve as good performance as previous optimizers and thus alleviate thememory requirement.
arxiv-12900-261 | Assisting Composition of Email Responses: a Topic Prediction Approach | http://arxiv.org/pdf/1510.02049v1.pdf | author:Spandana Gella, Marc Dymetman, Jean Michel Renders, Sriram Venkatapathy category:cs.CL published:2015-10-07 summary:We propose an approach for helping agents compose email replies to customerrequests. To enable that, we use LDA to extract latent topics from a collectionof email exchanges. We then use these latent topics to label our data,obtaining a so-called "silver standard" topic labelling. We exploit thislabelled set to train a classifier to: (i) predict the topic distribution ofthe entire agent's email response, based on features of the customer's email;and (ii) predict the topic distribution of the next sentence in the agent'sreply, based on the customer's email features and on features of the agent'scurrent sentence. The experimental results on a large email collection from acontact center in the tele- com domain show that the proposed ap- proach iseffective in predicting the best topic of the agent's next sentence. In 80% ofthe cases, the correct topic is present among the top five recommended topics(out of fifty possible ones). This shows the potential of this method to beapplied in an interactive setting, where the agent is presented a small list oflikely topics to choose from for the next sentence.
arxiv-12900-262 | Event-based Camera Pose Tracking using a Generative Event Model | http://arxiv.org/pdf/1510.01972v1.pdf | author:Guillermo Gallego, Christian Forster, Elias Mueggler, Davide Scaramuzza category:cs.CV cs.RO published:2015-10-07 summary:Event-based vision sensors mimic the operation of biological retina and theyrepresent a major paradigm shift from traditional cameras. Instead of providingframes of intensity measurements synchronously, at artificially chosen rates,event-based cameras provide information on brightness changes asynchronously,when they occur. Such non-redundant pieces of information are called "events".These sensors overcome some of the limitations of traditional cameras (responsetime, bandwidth and dynamic range) but require new methods to deal with thedata they output. We tackle the problem of event-based camera localization in aknown environment, without additional sensing, using a probabilistic generativeevent model in a Bayesian filtering framework. Our main contribution is thedesign of the likelihood function used in the filter to process the observedevents. Based on the physical characteristics of the sensor and on empiricalevidence of the Gaussian-like distribution of spiked events with respect to thebrightness change, we propose to use the contrast residual as a measure of howwell the estimated pose of the event-based camera and the environment explainthe observed events. The filter allows for localization in the general case ofsix degrees-of-freedom motions.
arxiv-12900-263 | Quantitative evaluation of the performance of discrete-time reservoir computers in the forecasting, filtering, and reconstruction of stochastic stationary signals | http://arxiv.org/pdf/1508.00144v3.pdf | author:Lyudmila Grigoryeva, Julie Henriques, Juan-Pablo Ortega category:cs.ET cs.NE math.ST stat.TH published:2015-08-01 summary:This paper extends the notion of information processing capacity fornon-independent input signals in the context of reservoir computing (RC). Thepresence of input autocorrelation makes worthwhile the treatment of forecastingand filtering problems for which we explicitly compute this generalizedcapacity as a function of the reservoir parameter values using a streamlinedmodel. The reservoir model leading to these developments is used to show that,whenever that approximation is valid, this computational paradigm satisfies theso called separation and fading memory properties that are usually associatedwith good information processing performances. We show that several standardmemory, forecasting, and filtering problems that appear in the parametricstochastic time series context can be readily formulated and tackled via RCwhich, as we show, significantly outperforms standard techniques in someinstances.
arxiv-12900-264 | Hierarchical Representation of Prosody for Statistical Speech Synthesis | http://arxiv.org/pdf/1510.01949v1.pdf | author:Antti Suni, Daniel Aalto, Martti Vainio category:cs.CL cs.SD published:2015-10-07 summary:Prominences and boundaries are the essential constituents of prosodicstructure in speech. They provide for means to chunk the speech stream intolinguistically relevant units by providing them with relative saliences anddemarcating them within coherent utterance structures. Prominences andboundaries have both been widely used in both basic research on prosody as wellas in text-to-speech synthesis. However, there are no representation schemesthat would provide for both estimating and modelling them in a unified fashion.Here we present an unsupervised unified account for estimating and representingprosodic prominences and boundaries using a scale-space analysis based oncontinuous wavelet transform. The methods are evaluated and compared to earlierwork using the Boston University Radio News corpus. The results show that theproposed method is comparable with the best published supervised annotationmethods.
arxiv-12900-265 | "Memory foam" approach to unsupervised learning | http://arxiv.org/pdf/1107.0674v3.pdf | author:Natalia B. Janson, Christopher J. Marsden category:nlin.AO cs.LG 34F05, 60G99 published:2011-07-04 summary:We propose an alternative approach to construct an artificial learningsystem, which naturally learns in an unsupervised manner. Its mathematicalprototype is a dynamical system, which automatically shapes its vector field inresponse to the input signal. The vector field converges to a gradient of amulti-dimensional probability density distribution of the input process, takenwith negative sign. The most probable patterns are represented by the stablefixed points, whose basins of attraction are formed automatically. Theperformance of this system is illustrated with musical signals.
arxiv-12900-266 | Helping Domain Experts Build Speech Translation Systems | http://arxiv.org/pdf/1510.01942v1.pdf | author:Manny Rayner, Alejandro Armando, Pierrette Bouillon, Sarah Ebling, Johanna Gerlach, Sonia Halimi, Irene Strasly, Nikos Tsourakis category:cs.HC cs.CL published:2015-10-07 summary:We present a new platform, "Regulus Lite", which supports rapid developmentand web deployment of several types of phrasal speech translation systems usinga minimal formalism. A distinguishing feature is that most development work canbe performed directly by domain experts. We motivate the need for platforms ofthis type and discuss three specific cases: medical speech translation,speech-to-sign-language translation and voice questionnaires. We brieflydescribe initial experiences in developing practical systems.
arxiv-12900-267 | Using Ontology-Based Context in the Portuguese-English Translation of Homographs in Textual Dialogues | http://arxiv.org/pdf/1510.01886v1.pdf | author:Diego Moussallem, Ricardo Choren category:cs.CL published:2015-10-07 summary:This paper introduces a novel approach to tackle the existing gap on messagetranslations in dialogue systems. Currently, submitted messages to the dialoguesystems are considered as isolated sentences. Thus, missing context informationimpede the disambiguation of homographs words in ambiguous sentences. Ourapproach solves this disambiguation problem by using concepts over existingontologies.
arxiv-12900-268 | Learning Spatiotemporal Features with 3D Convolutional Networks | http://arxiv.org/pdf/1412.0767v4.pdf | author:Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri category:cs.CV published:2014-12-02 summary:We propose a simple, yet effective approach for spatiotemporal featurelearning using deep 3-dimensional convolutional networks (3D ConvNets) trainedon a large scale supervised video dataset. Our findings are three-fold: 1) 3DConvNets are more suitable for spatiotemporal feature learning compared to 2DConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels inall layers is among the best performing architectures for 3D ConvNets; and 3)Our learned features, namely C3D (Convolutional 3D), with a simple linearclassifier outperform state-of-the-art methods on 4 different benchmarks andare comparable with current best methods on the other 2 benchmarks. Inaddition, the features are compact: achieving 52.8% accuracy on UCF101 datasetwith only 10 dimensions and also very efficient to compute due to the fastinference of ConvNets. Finally, they are conceptually very simple and easy totrain and use.
arxiv-12900-269 | Jointly Learning Multiple Measures of Similarities from Triplet Comparisons | http://arxiv.org/pdf/1503.01521v3.pdf | author:Liwen Zhang, Subhransu Maji, Ryota Tomioka category:stat.ML cs.AI cs.CV cs.LG published:2015-03-05 summary:Similarity between objects is multi-faceted and it can be easier for humanannotators to measure it when the focus is on a specific aspect. We considerthe problem of mapping objects into view-specific embeddings where the distancebetween them is consistent with the similarity comparisons of the form "fromthe t-th view, object A is more similar to B than to C". Our framework jointlylearns view-specific embeddings exploiting correlations between views.Experiments on a number of datasets, including one of multi-view crowdsourcedcomparison on bird images, show the proposed method achieves lower tripletgeneralization error when compared to both learning embeddings independentlyfor each view and all views pooled into one view. Our method can also be usedto learn multiple measures of similarity over input features taking classlabels into account and compares favorably to existing approaches formulti-task metric learning on the ISOLET dataset.
arxiv-12900-270 | A Quorum Sensing Inspired Algorithm for Dynamic Clustering | http://arxiv.org/pdf/1303.3934v2.pdf | author:Feng Tan, Jean-Jacques Slotine category:cs.LG published:2013-03-16 summary:Quorum sensing is a decentralized biological process, through which acommunity of cells with no global awareness coordinate their functionalbehaviors based solely on cell-medium interactions and local decisions. Thispaper draws inspirations from quorum sensing and colony competition to derive anew algorithm for data clustering. The algorithm treats each data as a singlecell, and uses knowledge of local connectivity to cluster cells into multiplecolonies simultaneously. It simulates auto-inducers secretion in quorum sensingto tune the influence radius for each cell. At the same time, sparselydistributed core cells spread their influences to form colonies, andinteractions between colonies eventually determine each cell's identity. Thealgorithm has the flexibility to analyze not only static but also time-varyingdata, which surpasses the capacity of many existing algorithms. Its stabilityand convergence properties are established. The algorithm is tested on severalapplications, including both synthetic and real benchmarks data sets, allelesclustering, community detection, image segmentation. In particular, thealgorithm's distinctive capability to deal with time-varying data allows us toexperiment it on novel applications such as robotic swarms grouping andswitching model identification. We believe that the algorithm's promisingperformance would stimulate many more exciting applications.
arxiv-12900-271 | Isometric sketching of any set via the Restricted Isometry Property | http://arxiv.org/pdf/1506.03521v2.pdf | author:Samet Oymak, Benjamin Recht, Mahdi Soltanolkotabi category:cs.IT cs.DS math.IT math.PR math.ST stat.ML stat.TH published:2015-06-11 summary:In this paper we show that for the purposes of dimensionality reductioncertain class of structured random matrices behave similarly to random Gaussianmatrices. This class includes several matrices for which matrix-vector multiplycan be computed in log-linear time, providing efficient dimensionalityreduction of general sets. In particular, we show that using such matrices anyset from high dimensions can be embedded into lower dimensions with nearoptimal distortion. We obtain our results by connecting dimensionalityreduction of any set to dimensionality reduction of sparse vectors via achaining argument.
arxiv-12900-272 | Structured Transforms for Small-Footprint Deep Learning | http://arxiv.org/pdf/1510.01722v1.pdf | author:Vikas Sindhwani, Tara N. Sainath, Sanjiv Kumar category:stat.ML cs.CV cs.LG published:2015-10-06 summary:We consider the task of building compact deep learning pipelines suitable fordeployment on storage and power constrained mobile devices. We propose aunified framework to learn a broad family of structured parameter matrices thatare characterized by the notion of low displacement rank. Our structuredtransforms admit fast function and gradient evaluation, and span a rich rangeof parameter sharing configurations whose statistical modeling capacity can beexplicitly tuned along a continuum from structured to unstructured.Experimental results show that these transforms can significantly accelerateinference and forward/backward passes during training, and offer superioraccuracy-compactness-speed tradeoffs in comparison to a number of existingtechniques. In keyword spotting applications in mobile speech recognition, ourmethods are much more effective than standard linear low-rank bottleneck layersand nearly retain the performance of state of the art models, while providingmore than 3.5-fold compression.
arxiv-12900-273 | Language Segmentation | http://arxiv.org/pdf/1510.01717v1.pdf | author:David Alfter category:cs.CL published:2015-10-06 summary:Language segmentation consists in finding the boundaries where one languageends and another language begins in a text written in more than one language.This is important for all natural language processing tasks. The problem can besolved by training language models on language data. However, in the case oflow- or no-resource languages, this is problematic. I therefore investigatewhether unsupervised methods perform better than supervised methods when it isdifficult or impossible to train supervised approaches. A special focus isgiven to difficult texts, i.e. texts that are rather short (one sentence),containing abbreviations, low-resource languages and non-standard language. Icompare three approaches: supervised n-gram language models, unsupervisedclustering and weakly supervised n-gram language model induction. I devised theweakly supervised approach in order to deal with difficult text specifically.In order to test the approach, I compiled a small corpus of different texttypes, ranging from one-sentence texts to texts of about 300 words. The weaklysupervised language model induction approach works well on short and difficulttexts, outperforming the clustering algorithm and reaching scores in thevicinity of the supervised approach. The results look promising, but there isroom for improvement and a more thorough investigation should be undertaken.
arxiv-12900-274 | Unsupervised Learning of Visual Representations using Videos | http://arxiv.org/pdf/1505.00687v2.pdf | author:Xiaolong Wang, Abhinav Gupta category:cs.CV published:2015-05-04 summary:Is strong supervision necessary for learning a good visual representation? Dowe really need millions of semantically-labeled images to train a ConvolutionalNeural Network (CNN)? In this paper, we present a simple yet surprisinglypowerful approach for unsupervised learning of CNN. Specifically, we usehundreds of thousands of unlabeled videos from the web to learn visualrepresentations. Our key idea is that visual tracking provides the supervision.That is, two patches connected by a track should have similar visualrepresentation in deep feature space since they probably belong to the sameobject or object part. We design a Siamese-triplet network with a ranking lossfunction to train this CNN representation. Without using a single image fromImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we trainan ensemble of unsupervised networks that achieves 52% mAP (no bounding boxregression). This performance comes tantalizingly close to itsImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. Wealso show that our unsupervised network can perform competitively in othertasks such as surface-normal estimation.
arxiv-12900-275 | Euclidean Auto Calibration of Camera Networks: Baseline Constraint Removes Scale Ambiguity | http://arxiv.org/pdf/1510.01663v1.pdf | author:Kiran Kumar Vupparaboina, Kamala Raghavan, Soumya Jana category:cs.CV published:2015-10-06 summary:Metric auto calibration of a camera network from multiple views has beenreported by several authors. Resulting 3D reconstruction recovers shapefaithfully, but not scale. However, preservation of scale becomes critical inapplications, such as multi-party telepresence, where multiple 3D scenes needto be fused into a single coordinate system. In this context, we propose acamera network configuration that includes a stereo pair with known baselineseparation, and analytically demonstrate Euclidean auto calibration of suchnetwork under mild conditions. Further, we experimentally validate our theoryusing a four-camera network. Importantly, our method not only recovers scale,but also compares favorably with the well known Zhang and Pollefeys methods interms of shape recovery.
arxiv-12900-276 | Improved Spectral Clustering via Embedded Label Propagation | http://arxiv.org/pdf/1411.6241v2.pdf | author:Xiaojun Chang, Feiping Nie, Yi Yang, Heng Huang category:cs.LG published:2014-11-23 summary:Spectral clustering is a key research topic in the field of machine learningand data mining. Most of the existing spectral clustering algorithms are builtupon Gaussian Laplacian matrices, which are sensitive to parameters. We proposea novel parameter free, distance consistent Locally Linear Embedding. Theproposed distance consistent LLE promises that edges between closer data pointshave greater weight.Furthermore, we propose a novel improved spectralclustering via embedded label propagation. Our algorithm is built upon twoadvancements of the state of the art:1) label propagation,which propagates anode\'s labels to neighboring nodes according to their proximity; and 2)manifold learning, which has been widely used in its capacity to leverage themanifold structure of data points. First we perform standard spectralclustering on original data and assign each cluster to k nearest data points.Next, we propagate labels through dense, unlabeled data regions. Extensiveexperiments with various datasets validate the superiority of the proposedalgorithm compared to current state of the art spectral algorithms.
arxiv-12900-277 | A Latent Source Model for Patch-Based Image Segmentation | http://arxiv.org/pdf/1510.01648v1.pdf | author:George Chen, Devavrat Shah, Polina Golland category:cs.CV published:2015-10-06 summary:Despite the popularity and empirical success of patch-based nearest-neighborand weighted majority voting approaches to medical image segmentation, therehas been no theoretical development on when, why, and how well thesenonparametric methods work. We bridge this gap by providing a theoreticalperformance guarantee for nearest-neighbor and weighted majority votingsegmentation under a new probabilistic model for patch-based imagesegmentation. Our analysis relies on a new local property for how similarnearby patches are, and fuses existing lines of work on modeling naturalimagery patches and theory for nonparametric classification. We use the modelto derive a new patch-based segmentation algorithm that iterates betweeninferring local label patches and merging these local segmentations to producea globally consistent image segmentation. Many existing patch-based algorithmsarise as special cases of the new algorithm.
arxiv-12900-278 | Large-scale subspace clustering using sketching and validation | http://arxiv.org/pdf/1510.01628v1.pdf | author:Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis category:cs.LG cs.CV stat.ML published:2015-10-06 summary:The nowadays massive amounts of generated and communicated data present majorchallenges in their processing. While capable of successfully classifyingnonlinearly separable objects in various settings, subspace clustering (SC)methods incur prohibitively high computational complexity when processinglarge-scale data. Inspired by the random sampling and consensus (RANSAC)approach to robust regression, the present paper introduces a randomized schemefor SC, termed sketching and validation (SkeVa-)SC, tailored for large-scaledata. At the heart of SkeVa-SC lies a randomized scheme for approximating theunderlying probability density function of the observed data by kernelsmoothing arguments. Sparsity in data representations is also exploited toreduce the computational burden of SC, while achieving high clusteringaccuracy. Performance analysis as well as extensive numerical tests onsynthetic and real data corroborate the potential of SkeVa-SC and itscompetitive performance relative to state-of-the-art scalable SC approaches.Keywords: Subspace clustering, big data, kernel smoothing, randomization,sketching, validation, sparsity.
arxiv-12900-279 | Predicting Daily Activities From Egocentric Images Using Deep Learning | http://arxiv.org/pdf/1510.01576v1.pdf | author:Daniel Castro, Steven Hickson, Vinay Bettadapura, Edison Thomaz, Gregory Abowd, Henrik Christensen, Irfan Essa category:cs.CV I.5; J.4; J.3 published:2015-10-06 summary:We present a method to analyze images taken from a passive egocentricwearable camera along with the contextual information, such as time and day ofweek, to learn and predict everyday activities of an individual. We collected adataset of 40,103 egocentric images over a 6 month period with 19 activityclasses and demonstrate the benefit of state-of-the-art deep learningtechniques for learning and predicting daily activities. Classification isconducted using a Convolutional Neural Network (CNN) with a classificationmethod we introduce called a late fusion ensemble. This late fusion ensembleincorporates relevant contextual information and increases our classificationaccuracy. Our technique achieves an overall accuracy of 83.07% in predicting aperson's activity across the 19 activity classes. We also demonstrate somepromising results from two additional users by fine-tuning the classifier withone day of training data.
arxiv-12900-280 | Analyzer and generator for Pali | http://arxiv.org/pdf/1510.01570v1.pdf | author:David Alfter category:cs.CL published:2015-10-06 summary:This work describes a system that performs morphological analysis andgeneration of Pali words. The system works with regular inflectional paradigmsand a lexical database. The generator is used to build a collection ofinflected and derived words, which in turn is used by the analyzer. Generatingand storing morphological forms along with the corresponding morphologicalinformation allows for efficient and simple look up by the analyzer. Indeed, bylooking up a word and extracting the attached morphological information, theanalyzer does not have to compute this information. As we must, however, assumethe lexical database to be incomplete, the system can also work without thedictionary component, using a rule-based approach.
arxiv-12900-281 | Parameterized Neural Network Language Models for Information Retrieval | http://arxiv.org/pdf/1510.01562v1.pdf | author:Benjamin Piwowarski, Sylvain Lamprier, Nicolas Despres category:cs.IR cs.CL H.3.3; I.2.6 published:2015-10-06 summary:Information Retrieval (IR) models need to deal with two difficult issues,vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds tothe difficulty of retrieving relevant documents that do not contain exact queryterms but semantically related terms. Term dependencies refers to the need ofconsidering the relationship between the words of the query when estimating therelevance of a document. A multitude of solutions has been proposed to solveeach of these two problems, but no principled model solve both. In parallel, inthe last few years, language models based on neural networks have been used tocope with complex natural language processing tasks like emotion and paraphrasedetection. Although they present good abilities to cope with both termdependencies and vocabulary mismatch problems, thanks to the distributedrepresentation of words they are based upon, such models could not be usedreadily in IR, where the estimation of one language model per document (orquery) is required. This is both computationally unfeasible and prone toover-fitting. Based on a recent work that proposed to learn a generic languagemodel that can be modified through a set of document-specific parameters, weexplore use of new neural network models that are adapted to ad-hoc IR tasks.Within the language model IR framework, we propose and study the use of ageneric language model as well as a document-specific language model. Both canbe used as a smoothing component, but the latter is more adapted to thedocument at hand and has the potential of being used as a full documentlanguage model. We experiment with such models and analyze their results onTREC-1 to 8 datasets.
arxiv-12900-282 | RAID: A Relation-Augmented Image Descriptor | http://arxiv.org/pdf/1510.01113v2.pdf | author:Paul Guerrero, Niloy J. Mitra, Peter Wonka category:cs.GR cs.CV I.4.8; I.4.7 published:2015-10-05 summary:As humans, we regularly interpret images based on the relations between imageregions. For example, a person riding object X, or a plank bridging twoobjects. Current methods provide limited support to search for images based onsuch relations. We present RAID, a relation-augmented image descriptor thatsupports queries based on inter-region relations. The key idea of ourdescriptor is to capture the spatial distribution of simple point-to-regionrelationships to describe more complex relationships between two image regions.We evaluate the proposed descriptor by querying into a large subset of theMicrosoft COCO database and successfully extract nontrivial imagesdemonstrating complex inter-region relations, which are easily missed orerroneously classified by existing methods.
arxiv-12900-283 | Learning Deep Representations of Appearance and Motion for Anomalous Event Detection | http://arxiv.org/pdf/1510.01553v1.pdf | author:Dan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, Nicu Sebe category:cs.CV published:2015-10-06 summary:We present a novel unsupervised deep learning framework for anomalous eventdetection in complex video scenes. While most existing works merely usehand-crafted appearance and motion features, we propose Appearance and MotionDeepNet (AMDN) which utilizes deep neural networks to automatically learnfeature representations. To exploit the complementary information of bothappearance and motion patterns, we introduce a novel double fusion framework,combining both the benefits of traditional early fusion and late fusionstrategies. Specifically, stacked denoising autoencoders are proposed toseparately learn both appearance and motion features as well as a jointrepresentation (early fusion). Based on the learned representations, multipleone-class SVM models are used to predict the anomaly scores of each input,which are then integrated with a late fusion strategy for final anomalydetection. We evaluate the proposed method on two publicly available videosurveillance datasets, showing competitive performance with respect to state ofthe art approaches.
arxiv-12900-284 | Active Transfer Learning with Zero-Shot Priors: Reusing Past Datasets for Future Tasks | http://arxiv.org/pdf/1510.01544v1.pdf | author:Efstratios Gavves, Thomas Mensink, Tatiana Tommasi, Cees G. M. Snoek, Tinne Tuytelaars category:cs.CV published:2015-10-06 summary:How can we reuse existing knowledge, in the form of available datasets, whensolving a new and apparently unrelated target task from a set of unlabeleddata? In this work we make a first contribution to answer this question in thecontext of image classification. We frame this quest as an active learningproblem and use zero-shot classifiers to guide the learning process by linkingthe new task to the existing classifiers. By revisiting the dual formulation ofadaptive SVM, we reveal two basic conditions to choose greedily only the mostrelevant samples to be annotated. On this basis we propose an effective activelearning algorithm which learns the best possible target classification modelwith minimum human labeling effort. Extensive experiments on two challengingdatasets show the value of our approach compared to the state-of-the-art activelearning methodologies, as well as its potential to reuse past datasets withminimal effort for future tasks.
arxiv-12900-285 | DC Decomposition of Nonconvex Polynomials with Algebraic Techniques | http://arxiv.org/pdf/1510.01518v1.pdf | author:Amir Ali Ahmadi, Georgina Hall category:math.OC cs.DS stat.ML published:2015-10-06 summary:We consider the problem of decomposing a multivariate polynomial as thedifference of two convex polynomials. We introduce algebraic techniques whichreduce this task to linear, second order cone, and semidefinite programming.This allows us to optimize over subsets of valid difference of convexdecompositions (dcds) and find ones that speed up the convex-concave procedure(CCP). We prove, however, that optimizing over the entire set of dcds isNP-hard.
arxiv-12900-286 | Quantifying Emergent Behavior of Autonomous Robots | http://arxiv.org/pdf/1510.01495v1.pdf | author:Georg Martius, Eckehard Olbrich category:cs.IT cs.LG cs.RO math.DS math.IT H.1.1; I.2.9 published:2015-10-06 summary:Quantifying behaviors of robots which were generated autonomously fromtask-independent objective functions is an important prerequisite for objectivecomparisons of algorithms and movements of animals. The temporal sequence ofsuch a behavior can be considered as a time series and hence complexitymeasures developed for time series are natural candidates for itsquantification. The predictive information and the excess entropy are suchcomplexity measures. They measure the amount of information the past containsabout the future and thus quantify the nonrandom structure in the temporalsequence. However, when using these measures for systems with continuous statesone has to deal with the fact that their values will depend on the resolutionwith which the systems states are observed. For deterministic systems bothmeasures will diverge with increasing resolution. We therefore propose a newdecomposition of the excess entropy in resolution dependent and resolutionindependent parts and discuss how they depend on the dimensionality of thedynamics, correlations and the noise level. For the practical estimation wepropose to use estimates based on the correlation integral instead of thedirect estimation of the mutual information using the algorithm by Kraskov etal. (2004) which is based on next neighbor statistics because the latter allowsless control of the scale dependencies. Using our algorithm we are able to showhow autonomous learning generates behavior of increasing complexity withincreasing learning duration.
arxiv-12900-287 | Directional Global Three-part Image Decomposition | http://arxiv.org/pdf/1510.01490v1.pdf | author:Duy Hoang Thai, Carsten Gottschlich category:cs.CV published:2015-10-06 summary:We consider the task of image decomposition and we introduce a new modelcoined directional global three-part decomposition (DG3PD) for solving it. Askey ingredients of the DG3PD model, we introduce a discrete multi-directionaltotal variation norm and a discrete multi-directional G-norm. Using these novelnorms, the proposed discrete DG3PD model can decompose an image into two partsor into three parts. Existing models for image decomposition by Vese and Osher,by Aujol and Chambolle, by Starck et al., and by Thai and Gottschlich areincluded as special cases in the new model. Decomposition of an image by DG3PDresults in a cartoon image, a texture image and a residual image. Advantages ofthe DG3PD model over existing ones lie in the properties enforced on thecartoon and texture images. The geometric objects in the cartoon image have avery smooth surface and sharp edges. The texture image yields oscillatingpatterns on a defined scale which is both smooth and sparse. Moreover, theDG3PD method achieves the goal of perfect reconstruction by summation of allcomponents better than the other considered methods. Relevant applications ofDG3PD are a novel way of image compression as well as feature extraction forapplications such as latent fingerprint processing and optical characterrecognition.
arxiv-12900-288 | Bayesian Markov Blanket Estimation | http://arxiv.org/pdf/1510.01485v1.pdf | author:Dinu Kaufmann, Sonali Parbhoo, Aleksander Wieczorek, Sebastian Keller, David Adametz, Volker Roth category:stat.ML cs.LG published:2015-10-06 summary:This paper considers a Bayesian view for estimating a sub-network in a Markovrandom field. The sub-network corresponds to the Markov blanket of a set ofquery variables, where the set of potential neighbours here is big. Wefactorize the posterior such that the Markov blanket is conditionallyindependent of the network of the potential neighbours. By exploiting thisblockwise decoupling, we derive analytic expressions for posteriorconditionals. Subsequently, we develop an inference scheme which makes use ofthe factorization. As a result, estimation of a sub-network is possible withoutinferring an entire network. Since the resulting Gibbs sampler scales linearlywith the number of variables, it can handle relatively large neighbourhoods.The proposed scheme results in faster convergence and superior mixing of theMarkov chain than existing Bayesian network estimation techniques.
arxiv-12900-289 | Local Rademacher Complexity Bounds based on Covering Numbers | http://arxiv.org/pdf/1510.01463v1.pdf | author:Yunwen Lei, Lixin Ding, Yingzhou Bi category:cs.AI cs.LG stat.ML published:2015-10-06 summary:This paper provides a general result on controlling local Rademachercomplexities, which captures in an elegant form to relate the complexities withconstraint on the expected norm to the corresponding ones with constraint onthe empirical norm. This result is convenient to apply in real applications andcould yield refined local Rademacher complexity bounds for function classessatisfying general entropy conditions. We demonstrate the power of ourcomplexity bounds by applying them to derive effective generalization errorbounds.
arxiv-12900-290 | A Waveform Representation Framework for High-quality Statistical Parametric Speech Synthesis | http://arxiv.org/pdf/1510.01443v1.pdf | author:Bo Fan, Siu Wa Lee, Xiaohai Tian, Lei Xie, Minghui Dong category:cs.SD cs.LG 68T10 published:2015-10-06 summary:State-of-the-art statistical parametric speech synthesis (SPSS) generallyuses a vocoder to represent speech signals and parameterize them into featuresfor subsequent modeling. Magnitude spectrum has been a dominant feature overthe years. Although perceptual studies have shown that phase spectrum isessential to the quality of synthesized speech, it is often ignored by using aminimum phase filter during synthesis and the speech quality suffers. To bypassthis bottleneck in vocoded speech, this paper proposes a phase-embeddedwaveform representation framework and establishes a magnitude-phase jointmodeling platform for high-quality SPSS. Our experiments on waveformreconstruction show that the performance is better than that of the widely-usedSTRAIGHT. Furthermore, the proposed modeling and synthesis platform outperformsa leading-edge, vocoded, deep bidirectional long short-term memory recurrentneural network (DBLSTM-RNN)-based baseline system in various objectiveevaluation metrics conducted.
arxiv-12900-291 | Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias | http://arxiv.org/pdf/1509.01004v2.pdf | author:Yohei Kondo, Kohei Hayashi, Shin-ichi Maeda category:stat.ML cs.LG published:2015-09-03 summary:A common strategy for sparse linear regression is to introduceregularization, which eliminates irrelevant features by letting thecorresponding weights be zeros. However, regularization often shrinks theestimator for relevant features, which leads to incorrect feature selection.Motivated by the above-mentioned issue, we propose Bayesian masking (BM), asparse estimation method which imposes no regularization on the weights. Thekey concept of BM is to introduce binary latent variables that randomly maskfeatures. Estimating the masking rates determines the relevance of the featuresautomatically. We derive a variational Bayesian inference algorithm thatmaximizes the lower bound of the factorized information criterion (FIC), whichis a recently developed asymptotic criterion for evaluating the marginallog-likelihood. In addition, we propose reparametrization to accelerate theconvergence of the derived algorithm. Finally, we show that BM outperformsLasso and automatic relevance determination (ARD) in terms of thesparsity-shrinkage trade-off.
arxiv-12900-292 | Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders | http://arxiv.org/pdf/1510.01442v1.pdf | author:Huan Yang, Baoyuan Wang, Stephen Lin, David Wipf, Minyi Guo, Baining Guo category:cs.CV published:2015-10-06 summary:With the growing popularity of short-form video sharing platforms such as\em{Instagram} and \em{Vine}, there has been an increasing need for techniquesthat automatically extract highlights from video. Whereas prior works haveapproached this problem with heuristic rules or supervised learning, we presentan unsupervised learning approach that takes advantage of the abundance ofuser-edited videos on social media websites such as YouTube. Based on the ideathat the most significant sub-events within a video class are commonly presentamong edited videos while less interesting ones appear less frequently, weidentify the significant sub-events via a robust recurrent auto-encoder trainedon a collection of user-edited videos queried for each particular class ofinterest. The auto-encoder is trained using a proposed shrinking exponentialloss function that makes it robust to noise in the web-crawled training data,and is configured with bidirectional long short term memory(LSTM)~\cite{LSTM:97} cells to better model the temporal structure of highlightsegments. Different from supervised techniques, our method can infer highlightsusing only a set of downloaded edited videos, without also needing theirpre-edited counterparts which are rarely available online. Extensiveexperiments indicate the promise of our proposed solution in this challengingunsupervised settin
arxiv-12900-293 | Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification | http://arxiv.org/pdf/1510.01440v1.pdf | author:Ruobing Wu, Baoyuan Wang, Wenping Wang, Yizhou Yu category:cs.CV published:2015-10-06 summary:Recent work on scene classification still makes use of generic CNN featuresin a rudimentary manner. In this ICCV 2015 paper, we present a novel pipelinebuilt upon deep CNN features to harvest discriminative visual objects and partsfor scene classification. We first use a region proposal technique to generatea set of high-quality patches potentially containing objects, and apply apre-trained CNN to extract generic deep features from these patches. Then weperform both unsupervised and weakly supervised learning to screen thesepatches and discover discriminative ones representing category-specific objectsand parts. We further apply discriminative clustering enhanced with local CNNfine-tuning to aggregate similar objects and parts into groups, called metaobjects. A scene image representation is constructed by pooling the featureresponse maps of all the learned meta objects at multiple spatial scales. Wehave confirmed that the scene image representation obtained using this newpipeline is capable of delivering state-of-the-art performance on two popularscene benchmark datasets, MIT Indoor 67~\cite{MITIndoor67} andSun397~\cite{Sun397}
arxiv-12900-294 | Improved Estimation of Class Prior Probabilities through Unlabeled Data | http://arxiv.org/pdf/1510.01422v1.pdf | author:Norman Matloff category:stat.ML cs.LG published:2015-10-06 summary:Work in the classification literature has shown that in computing aclassification function, one need not know the class membership of allobservations in the training set; the unlabeled observations still provideinformation on the marginal distribution of the feature set, and can thuscontribute to increased classification accuracy for future observations. Thepresent paper will show that this scheme can also be used for the estimation ofclass prior probabilities, which would be very useful in applications in whichit is difficult or expensive to determine class membership. Both parametric andnonparametric estimators are developed. Asymptotic distributions of theestimators are derived, and it is proven that the use of the unlabeledobservations does reduce asymptotic variance. This methodology is also extendedto the estimation of subclass probabilities.
arxiv-12900-295 | Fuzzy Adaptive Resonance Theory, Diffusion Maps and their applications to Clustering and Biclustering | http://arxiv.org/pdf/1411.5737v5.pdf | author:S. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu category:cs.NE cs.LG published:2014-11-21 summary:In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive ResonanceDif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theoryto do clustering on high dimensional data. We describe some applications ofthis method and some problems for future research.
arxiv-12900-296 | On the Existence of Epipolar Matrices | http://arxiv.org/pdf/1510.01401v1.pdf | author:Sameer Agarwal, Hon-Leung Lee, Bernd Sturmfels, Rekha R. Thomas category:cs.CV math.AG published:2015-10-06 summary:This paper considers the foundational question of the existence of afundamental (resp. essential) matrix given $m$ point correspondences in twoviews. We present a complete answer for the existence of fundamental matricesfor any value of $m$. Using examples we disprove the widely held beliefs thatfundamental matrices always exist whenever $m \leq 7$. At the same time, weprove that they exist unconditionally when $m \leq 5$. Under a mild genericitycondition, we show that an essential matrix always exists when $m \leq 4$. Wealso characterize the six and seven point configurations in two views for whichall matrices satisfying the epipolar constraint have rank at most one.
arxiv-12900-297 | Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation | http://arxiv.org/pdf/1502.02734v3.pdf | author:George Papandreou, Liang-Chieh Chen, Kevin Murphy, Alan L. Yuille category:cs.CV published:2015-02-09 summary:Deep convolutional neural networks (DCNNs) trained on a large number ofimages with strong pixel-level annotations have recently significantly pushedthe state-of-art in semantic image segmentation. We study the more challengingproblem of learning DCNNs for semantic image segmentation from either (1)weakly annotated training data such as bounding boxes or image-level labels or(2) a combination of few strongly labeled and many weakly labeled images,sourced from one or multiple datasets. We develop Expectation-Maximization (EM)methods for semantic image segmentation model training under these weaklysupervised and semi-supervised settings. Extensive experimental evaluationshows that the proposed techniques can learn models delivering competitiveresults on the challenging PASCAL VOC 2012 image segmentation benchmark, whilerequiring significantly less annotation effort. We share source codeimplementing the proposed system athttps://bitbucket.org/deeplab/deeplab-public.
arxiv-12900-298 | Batch Normalized Recurrent Neural Networks | http://arxiv.org/pdf/1510.01378v1.pdf | author:César Laurent, Gabriel Pereyra, Philémon Brakel, Ying Zhang, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2015-10-05 summary:Recurrent Neural Networks (RNNs) are powerful models for sequential data thathave the potential to learn long-term dependencies. However, they arecomputationally expensive to train and difficult to parallelize. Recent workhas shown that normalizing intermediate representations of neural networks cansignificantly improve convergence rates in feedforward neural networks . Inparticular, batch normalization, which uses mini-batch statistics tostandardize features, was shown to significantly reduce training time. In thispaper, we show that applying batch normalization to the hidden-to-hiddentransitions of our RNNs doesn't help the training procedure. We also show thatwhen applied to the input-to-hidden transitions, batch normalization can leadto a faster convergence of the training criterion but doesn't seem to improvethe generalization performance on both our language modelling and speechrecognition tasks. All in all, applying batch normalization to RNNs turns outto be more challenging than applying it to feedforward networks, but certainvariants of it can still be beneficial.
arxiv-12900-299 | Within-Brain Classification for Brain Tumor Segmentation | http://arxiv.org/pdf/1510.01344v1.pdf | author:Mohammad Havaei, Hugo Larochelle, Philippe Poulin, Pierre-Marc Jodoin category:cs.CV cs.AI published:2015-10-05 summary:Purpose: In this paper, we investigate a framework for interactive braintumor segmentation which, at its core, treats the problem of interactive braintumor segmentation as a machine learning problem. Methods: This method has an advantage over typical machine learning methodsfor this task where generalization is made across brains. The problem withthese methods is that they need to deal with intensity bias correction andother MRI-specific noise. In this paper, we avoid these issues by approachingthe problem as one of within brain generalization. Specifically, we propose asemi-automatic method that segments a brain tumor by training and generalizingwithin that brain only, based on some minimum user interaction. Conclusion: We investigate how adding spatial feature coordinates (i.e. $i$,$j$, $k$) to the intensity features can significantly improve the performanceof different classification methods such as SVM, kNN and random forests. Thiswould only be possible within an interactive framework. We also investigate theuse of a more appropriate kernel and the adaptation of hyper-parametersspecifically for each brain. Results: As a result of these experiments, we obtain an interactive methodwhose results reported on the MICCAI-BRATS 2013 dataset are the second mostaccurate compared to published methods, while using significantly less memoryand processing power than most state-of-the-art methods.
arxiv-12900-300 | Tight Variational Bounds via Random Projections and I-Projections | http://arxiv.org/pdf/1510.01308v1.pdf | author:Lun-Kai Hsu, Tudor Achim, Stefano Ermon category:cs.LG published:2015-10-05 summary:Information projections are the key building block of variational inferencealgorithms and are used to approximate a target probabilistic model byprojecting it onto a family of tractable distributions. In general, there is noguarantee on the quality of the approximation obtained. To overcome this issue,we introduce a new class of random projections to reduce the dimensionality andhence the complexity of the original model. In the spirit of randomprojections, the projection preserves (with high probability) key properties ofthe target distribution. We show that information projections can be combinedwith random projections to obtain provable guarantees on the quality of theapproximation obtained, regardless of the complexity of the original model. Wedemonstrate empirically that augmenting mean field with a random projectionstep dramatically improves partition function and marginal probabilityestimates, both on synthetic and real world data.
