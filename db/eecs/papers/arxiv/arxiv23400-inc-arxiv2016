arxiv-1611-07954 | Emergent Logical Structure in Vector Representations of Neural Readers | http://arxiv.org/abs/1611.07954 | id:1611.07954 author:Hai Wang, Takeshi Onishi, Kevin Gimpel, David McAllester category:cs.CL  published:2016-11-23 summary:Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the existence of logical structure in the hidden state vectors of "aggregation readers" such as the Attentive Reader and Stanford Reader. The logical structure of aggregation readers reflects the architecture of "explicit reference readers" such as the Attention-Sum Reader, the Gated Attention Reader and the Attention-over-Attention Reader. This relationship between aggregation readers and explicit reference readers presents a case study in emergent logical structure. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What datasets. version:1
arxiv-1611-06972 | Measuring Sample Quality with Diffusions | http://arxiv.org/abs/1611.06972 | id:1611.06972 author:Jack Gorham, Andrew B. Duncan, Sebastian J. Vollmer, Lester Mackey category:stat.ML cs.LG math.PR  published:2016-11-21 summary:Standard Markov chain Monte Carlo diagnostics, like effective sample size, are ineffective for biased sampling procedures that sacrifice asymptotic correctness for computational speed. Recent work addresses this issue for a class of strongly log-concave target distributions by constructing a computable discrepancy measure based on Stein's method that provably determines convergence to the target. We generalize this approach to cover any target with a fast-coupling Ito diffusion by bounding the derivatives of Stein equation solutions in terms of Markov process coupling rates. As example applications, we develop computable and convergence-determining diffusion Stein discrepancies for log-concave, heavy-tailed, and multimodal targets and use these quality measures to select the hyperparameters of biased samplers, compare random and deterministic quadrature rules, and quantify bias-variance tradeoffs in approximate Markov chain Monte Carlo. Our explicit multivariate Stein factor bounds may be of independent interest. version:2
arxiv-1611-07941 | Multi-Modal Mean-Fields via Cardinality-Based Clamping | http://arxiv.org/abs/1611.07941 | id:1611.07941 author:Pierre Baqué, François Fleuret, Pascal Fua category:cs.CV cs.AI  published:2016-11-23 summary:Mean Field inference is central to statistical physics. It has attracted much interest in the Computer Vision community to efficiently solve problems expressible in terms of large Conditional Random Fields. However, since it models the posterior probability distribution as a product of marginal probabilities, it may fail to properly account for important dependencies between variables. We therefore replace the fully factorized distribution of Mean Field by a weighted mixture of such distributions, that similarly minimizes the KL-Divergence to the true posterior. By introducing two new ideas, namely, conditioning on groups of variables instead of single ones and using a parameter of the conditional random field potentials, that we identify to the temperature in the sense of statistical physics to select such groups, we can perform this minimization efficiently. Our extension of the clamping method proposed in previous works allows us to both produce a more descriptive approximation of the true posterior and, inspired by the diverse MAP paradigms, fit a mixture of Mean Field approximations. We demonstrate that this positively impacts real-world algorithms that initially relied on mean fields. version:1
arxiv-1611-07932 | Straight to Shapes: Real-time Detection of Encoded Shapes | http://arxiv.org/abs/1611.07932 | id:1611.07932 author:Saumya Jetley, Michael Sapienza, Stuart Golodetz, Philip H. S. Torr category:cs.CV  published:2016-11-23 summary:Current object detection approaches predict bounding boxes, but these provide little instance-specific information beyond location, scale and aspect ratio. In this work, we propose to directly regress to objects' shapes in addition to their bounding boxes and categories. It is crucial to find an appropriate shape representation that is compact and decodable, and in which objects can be compared for higher-order concepts such as view similarity, pose variation and occlusion. To achieve this, we use a denoising convolutional auto-encoder to establish an embedding space, and place the decoder after a fast end-to-end network trained to regress directly to the encoded shape vectors. This yields what to the best of our knowledge is the first real-time shape prediction network, running at ~35 FPS on a high-end desktop. With higher-order shape reasoning well-integrated into the network pipeline, the network shows the useful practical quality of generalising to unseen categories that are similar to the ones in the training set, something that most existing approaches fail to handle. version:1
arxiv-1611-07909 | Image Segmentation Using Overlapping Group Sparsity | http://arxiv.org/abs/1611.07909 | id:1611.07909 author:Shervin Minaee, Amirali Abdolrashidi category:cs.CV  published:2016-11-23 summary:Sparse decomposition has been widely used for different applications, such as source separation, image classification and image denoising. This paper presents a new algorithm for segmentation of an image into background and foreground text and graphics using sparse decomposition. First, the background is represented using a suitable smooth model, which is a linear combination of a few smoothly varying basis functions, and the foreground text and graphics are modeled as a sparse component overlaid on the smooth background. Then the background and foreground are separated using a sparse decomposition framework and imposing some prior information, which promote the smoothness of background, and the sparsity and connectivity of foreground pixels. This algorithm has been tested on a dataset of images extracted from HEVC standard test sequences for screen content coding, and is shown to outperform prior methods, including least absolute deviation fitting, k-means clustering based segmentation in DjVu, and shape primitive extraction and coding algorithm. version:1
arxiv-1611-07897 | Unsupervised Learning of Sentence Representations using Convolutional Neural Networks | http://arxiv.org/abs/1611.07897 | id:1611.07897 author:Zhe Gan, Yunchen Pu, Ricardo Henao, Chunyuan Li, Xiaodong He, Lawrence Carin category:cs.CL cs.LG  published:2016-11-23 summary:We propose a new encoder-decoder approach to learn distributed sentence representations from unlabeled sentences. The word-to-vector representation is used, and convolutional neural networks are employed as sentence encoders, mapping an input sentence into a fixed-length vector. This representation is decoded using long short-term memory recurrent neural networks, considering several tasks, such as reconstructing the input sentence, or predicting the future sentence. We further describe a hierarchical encoder-decoder model to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods. version:1
arxiv-1611-07890 | Image-based Localization with Spatial LSTMs | http://arxiv.org/abs/1611.07890 | id:1611.07890 author:Florian Walch, Caner Hazirbas, Laura Leal-Taixé, Torsten Sattler, Sebastian Hilsenbeck, Daniel Cremers category:cs.CV  published:2016-11-23 summary:In this work we propose a new CNN+LSTM architecture for camera pose regression for indoor and outdoor scenes. CNNs allow us to learn suitable feature representations for localization that are robust against motion blur and illumination changes. We make use of LSTM units on the CNN output in spatial coordinates in order to capture contextual information. This substantially enlarges the receptive field of each pixel leading to drastic improvements in localization performance. We provide extensive quantitative comparison of CNN-based vs SIFT-based localization methods, showing the weaknesses and strengths of each. Furthermore, we present a new large-scale indoor dataset with accurate ground truth from a laser scanner. Experimental results on both indoor and outdoor public datasets show our method outperforms existing deep architectures, and can localize images in hard conditions, e.g., in the presence of mostly textureless surfaces. version:1
arxiv-1611-07889 | The World of Fast Moving Objects | http://arxiv.org/abs/1611.07889 | id:1611.07889 author:Denys Rozumnyi, Jan Kotera, Filip Sroubek, Lukas Novotny, Jiri Matas category:cs.CV  published:2016-11-23 summary:The notion of a Fast Moving Object (FMO), i.e. an object that moves over a distance exceeding its size within the exposure time, is introduced. FMOs may, and typically do, rotate with high angular speed. FMOs are very common in sports videos, but are not rare elsewhere. In a single frame, such objects are often barely visible and appear as semi-transparent streaks. A method for the detection and tracking of FMOs is proposed. The method consists of three distinct algorithms, which form an efficient localization pipeline that operates successfully in a broad range of conditions. We show that it is possible to recover the appearance of the object and its axis of rotation, despite its blurred appearance. The proposed method is evaluated on a new annotated dataset. The results show that existing trackers are inadequate for the problem of FMO localization and a new approach is required. Two applications of localization, temporal super-resolution and highlighting, are presented. version:1
arxiv-1611-07873 | Piecewise Deterministic Markov Processes for Continuous-Time Monte Carlo | http://arxiv.org/abs/1611.07873 | id:1611.07873 author:Paul Fearnhead, Joris Bierkens, Murray Pollock, Gareth O Roberts category:stat.CO math.ST stat.ME stat.ML stat.TH  published:2016-11-23 summary:Recently there have been exciting developments in Monte Carlo methods, with the development of new MCMC and sequential Monte Carlo (SMC) algorithms which are based on continuous-time, rather than discrete-time, Markov processes. This has led to some fundamentally new Monte Carlo algorithms which can be used to sample from, say, a posterior distribution. Interestingly, continuous-time algorithms seem particularly well suited to Bayesian analysis in big-data settings as they need only access a small sub-set of data points at each iteration, and yet are still guaranteed to target the true posterior distribution. Whilst continuous-time MCMC and SMC methods have been developed independently we show here that they are related by the fact that both involve simulating a piecewise deterministic Markov process. Furthermore we show that the methods developed to date are just specific cases of a potentially much wider class of continuous-time Monte Carlo algorithms. We give an informal introduction to piecewise deterministic Markov processes, covering the aspects relevant to these new Monte Carlo algorithms, with a view to making the development of new continuous-time Monte Carlo more accessible. We focus on how and why sub-sampling ideas can be used with these algorithms, and aim to give insight into how these new algorithms can be implemented, and what are some of the issues that affect their efficiency. version:1
arxiv-1611-07865 | Controlling Perceptual Factors in Neural Style Transfer | http://arxiv.org/abs/1611.07865 | id:1611.07865 author:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann, Eli Shechtman category:cs.CV  published:2016-11-23 summary:Neural Style Transfer has shown very exciting results enabling new forms of image manipulation. Here we extend the existing method beyond the paradigm of transferring global style information between pairs of images. In particular, we introduce control over spatial location, colour information and across spatial scale. We demonstrate how this enhances the method by allowing high-resolution controlled stylisation and helps to alleviate common failure cases such as applying ground textures to sky regions. Furthermore, by decomposing style into these perceptual factors we enable the combination of style information from multiple sources to generate new, perceptually appealing styles from existing ones. Finally we show how the introduced control measures can be applied in recent methods for Fast Neural Style Transfer. version:1
arxiv-1611-07850 | Robust Unsupervised Transient Detection With Invariant Representation based on the Scattering Network | http://arxiv.org/abs/1611.07850 | id:1611.07850 author:Randall Balestriero, Behnaam Aazhang category:stat.ML stat.AP  published:2016-11-23 summary:We present a sparse and invariant representation with low asymptotic complexity for robust unsupervised transient and onset zone detection in noisy environments. This unsupervised approach is based on wavelet transforms and leverages the scattering network from Mallat et al. by deriving frequency invariance. This frequency invariance is a key concept to enforce robust representations of transients in presence of possible frequency shifts and perturbations occurring in the original signal. Implementation details as well as complexity analysis are provided in addition of the theoretical framework and the invariance properties. In this work, our primary application consists of predicting the onset of seizure in epileptic patients from subdural recordings as well as detecting inter-ictal spikes. version:1
arxiv-1611-07837 | Adaptive Feature Abstraction for Translating Video to Language | http://arxiv.org/abs/1611.07837 | id:1611.07837 author:Yunchen Pu, Martin Renqiang Min, Zhe Gan, Lawrence Carin category:cs.CV cs.CL  published:2016-11-23 summary:A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. We consider both "hard" and "soft" attention mechanisms, to adaptively and sequentially focus on different layers of features (levels of feature "abstraction"), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics. version:1
arxiv-1611-07828 | Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose | http://arxiv.org/abs/1611.07828 | id:1611.07828 author:Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas Daniilidis category:cs.CV  published:2016-11-23 summary:This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization only and recover 3D pose by a subsequent optimization step. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a fine discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-fine prediction scheme. This step addresses the large dimensionality increase and enables iterative refinement and repeated processing of the image features. The proposed approach allows us to train a ConvNet that outperforms all state-of-the-art approaches on standard benchmarks achieving relative error reduction greater than 35% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our end-to-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images. version:1
arxiv-1611-07810 | A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering | http://arxiv.org/abs/1611.07810 | id:1611.07810 author:Tegan Maharaj, Nicolas Ballas, Aaron Courville, Christopher Pal category:cs.CV  published:2016-11-23 summary:While deep convolutional neural networks frequently approach or exceed human-level performance at benchmark tasks involving static images, extending this success to moving images is not straightforward. Having models which can learn to understand video is of interest for many applications, including content recommendation, prediction, summarization, event/object detection and understanding human visual perception, but many domains lack sufficient data to explore and perfect video models. In order to address the need for a simple, quantitative benchmark for developing and understanding video, we present MovieFIB, a fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. In addition to presenting statistics and a description of the dataset, we perform a detailed analysis of 5 different models' predictions, and compare these with human performance. We investigate the relative importance of language, static (2D) visual features, and moving (3D) visual features; the effects of increasing dataset size, the number of frames sampled; and of vocabulary size. We illustrate that: this task is not solvable by a language model alone; our model combining 2D and 3D visual information indeed provides the best result; all models perform significantly worse than human-level. We provide human evaluations for responses given by different models and find that accuracy on the MovieFIB evaluation corresponds well with human judgement. We suggest avenues for improving video models, and hope that the proposed dataset can be useful for measuring and encouraging progress in this very interesting field. version:1
arxiv-1611-07807 | Learning Invariant Representations Of Planar Curves | http://arxiv.org/abs/1611.07807 | id:1611.07807 author:Gautam Pai, Aaron Wetzler, Ron Kimmel category:cs.CV  published:2016-11-23 summary:We propose a metric learning framework for the construction of invariant geometric functions of planar curves for the Eucledian and Similarity group of transformations. We leverage on the representational power of convolutional neural networks to compute these geometric quantities. In comparison with axiomatic constructions, we show that the invariants approximated by the learning architectures have better numerical qualities such as robustness to noise, resiliency to sampling, as well as the ability to adapt to occlusion and partiality. Finally, we develop a novel multi-scale representation in a similarity metric learning paradigm. version:1
arxiv-1611-07804 | ATR4S: Toolkit with State-of-the-art Automatic Terms Recognition Methods in Scala | http://arxiv.org/abs/1611.07804 | id:1611.07804 author:N. Astrakhantsev category:cs.CL  published:2016-11-23 summary:Automatically recognized terminology is widely used for various domain-specific texts processing tasks, such as machine translation, information retrieval or sentiment analysis. However, there is still no agreement on which methods are best suited for particular settings and, moreover, there is no reliable comparison of already developed methods. We believe that one of the main reasons is the lack of state-of-the-art methods implementations, which are usually non-trivial to recreate. In order to address these issues, we present ATR4S, an open-source software written in Scala that comprises more than 15 methods for automatic terminology recognition (ATR) and implements the whole pipeline from text document preprocessing, to term candidates collection, term candidates scoring, and finally, term candidates ranking. It is highly scalable, modular and configurable tool with support of automatic caching. We also compare 10 state-of-the-art methods on 7 open datasets by average precision and processing time. Experimental comparison reveals that no single method demonstrates best average precision for all datasets and that other available tools for ATR do not contain the best methods. version:1
arxiv-1611-07800 | Infinite Variational Autoencoder for Semi-Supervised Learning | http://arxiv.org/abs/1611.07800 | id:1611.07800 author:Ehsan Abbasnejad, Anthony Dick, Anton van den Hengel category:cs.LG stat.ML  published:2016-11-23 summary:This paper presents an infinite variational autoencoder (VAE) whose capacity adapts to suit the input data. This is achieved using a mixture model where the mixing coefficients are modeled by a Dirichlet process, allowing us to integrate over the coefficients when performing inference. Critically, this then allows us to automatically vary the number of autoencoders in the mixture based on the data. Experiments show the flexibility of our method, particularly for semi-supervised learning, where only a small number of training samples are available. version:1
arxiv-1611-07791 | Object Detection using Image Processing | http://arxiv.org/abs/1611.07791 | id:1611.07791 author:Fares Jalled, Ilia Voronkov category:cs.CV  published:2016-11-23 summary:An Unmanned Ariel vehicle (UAV) has greater importance in the army for border security. The main objective of this article is to develop an OpenCV-Python code using Haar Cascade algorithm for object and face detection. Currently, UAVs are used for detecting and attacking the infiltrated ground targets. The main drawback for this type of UAVs is that sometimes the object are not properly detected, which thereby causes the object to hit the UAV. This project aims to avoid such unwanted collisions and damages of UAV. UAV is also used for surveillance that uses Voila-jones algorithm to detect and track humans. This algorithm uses cascade object detector function and vision. train function to train the algorithm. The main advantage of this code is the reduced processing time. The Python code was tested with the help of available database of video and image, the output was verified. version:1
arxiv-1611-07781 | Adaptive Down-Sampling and Dimension Reduction in Time Elastic Kernel Machines for Efficient Recognition of Isolated Gestures | http://arxiv.org/abs/1611.07781 | id:1611.07781 author:Pierre-François Marteau, Sylvie Gibet, Clément Reverdy category:cs.CV cs.LG  published:2016-11-23 summary:In the scope of gestural action recognition, the size of the feature vector representing movements is in general quite large especially when full body movements are considered. Furthermore, this feature vector evolves during the movement performance so that a complete movement is fully represented by a matrix M of size DxT , whose element M i, j represents the value of feature i at timestamps j. Many studies have addressed dimensionality reduction considering only the size of the feature vector lying in R D to reduce both the variability of gestural sequences expressed in the reduced space, and the computational complexity of their processing. In return, very few of these methods have explicitly addressed the dimensionality reduction along the time axis. Yet this is a major issue when considering the use of elastic distances which are characterized by a quadratic complexity along the time axis. We present in this paper an evaluation of straightforward approaches aiming at reducing the dimensionality of the matrix M for each movement, leading to consider both the dimensionality reduction of the feature vector as well as its reduction along the time axis. The dimensionality reduction of the feature vector is achieved by selecting remarkable joints in the skeleton performing the movement, basically the extremities of the articulatory chains composing the skeleton. The temporal dimen-sionality reduction is achieved using either a regular or adaptive down-sampling that seeks to minimize the reconstruction error of the movements. Elastic and Euclidean kernels are then compared through support vector machine learning. Two data sets 1 that are widely referenced in the domain of human gesture recognition, and quite distinctive in terms of quality of motion capture, are used for the experimental assessment of the proposed approaches. On these data sets we experimentally show that it is feasible, and possibly desirable, to significantly reduce simultaneously the size of the feature vector and the number of skeleton frames to represent body movements while maintaining a very good recognition rate. The method proves to give satisfactory results at a level currently reached by state-of-the-art methods on these data sets. We experimentally show that the computational complexity reduction that is obtained makes this approach eligible for real-time applications. version:1
arxiv-1611-07767 | Multiframe Motion Coupling via Infimal Convolution Regularization for Video Super Resolution | http://arxiv.org/abs/1611.07767 | id:1611.07767 author:Hendrik Dirks, Jonas Geiping, Daniel Cremers, Michael Moeller category:cs.CV math.OC I.4; G.1.6; G.4  published:2016-11-23 summary:The idea of video super resolution is to use the temporal information of seeing a scene from many slightly different viewpoints in the successive frames of a video to enhance the overall resolution and quality of each frame. Classical energy minimization approaches first establish a correspondence of the current video frame to several of its neighbors and then use this temporal information to enhance it. In this paper we propose the first variational super resolution approach that computes several super resolved frames in one joint optimization procedure by incorporating motion information between the high resolution image frames themselves. As a consequence, the number of motion estimation problems grows linearly in the number of frames, opposed to a quadratic growth of classical methods. In addition, we use infimal convolution regularization to automatically determine the reliability of the motion information and reweight the regularization locally. We demonstrate that our approach yields state-of-the-art results and even is competitive with learning based approaches that require a significant amount of training data. version:1
arxiv-1611-07759 | Multi-View 3D Object Detection Network for Autonomous Driving | http://arxiv.org/abs/1611.07759 | id:1611.07759 author:Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia category:cs.CV  published:2016-11-23 summary:This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods. version:1
arxiv-1611-07752 | Convergence Analysis of MAP based Blur Kernel Estimation | http://arxiv.org/abs/1611.07752 | id:1611.07752 author:Sunghyun Cho, Seungyong Lee category:cs.CV  published:2016-11-23 summary:One popular approach for blind deconvolution is to formulate a maximum a posteriori (MAP) problem with sparsity priors on the gradients of the latent image, and then alternatingly estimate the blur kernel and the latent image. While several successful MAP based methods have been proposed, there has been much controversy and confusion about their convergence, because sparsity priors have been shown to prefer blurry images to sharp natural images. In this paper, we revisit this problem and provide an analysis on the convergence of MAP based approaches. We first introduce a slight modification to a conventional joint energy function for blind deconvolution. The reformulated energy function yields the same alternating estimation process, but more clearly reveals how blind deconvolution works. We then show the global optimum of the energy function can actually be the right solution instead of the no-blur solution under certain conditions, which explains the success of previous MAP based approaches. The reformulated energy function and our conditions for the convergence also provide a way to compare the qualities of different blur kernels, and we demonstrate its applicability to automatic blur kernel size selection, blur kernel estimation using light streaks, and defocus estimation. version:1
arxiv-1611-07743 | Tunable Sensitivity to Large Errors in Neural Network Training | http://arxiv.org/abs/1611.07743 | id:1611.07743 author:Gil Keren, Sivan Sabato, Björn Schuller category:stat.ML cs.LG cs.NE  published:2016-11-23 summary:When humans learn a new concept, they might ignore examples that they cannot make sense of at first, and only later focus on such examples, when they are more useful for learning. We propose incorporating this idea of tunable sensitivity for hard examples in neural network learning, using a new generalization of the cross-entropy gradient step, which can be used in place of the gradient in any gradient-based training method. The generalized gradient is parameterized by a value that controls the sensitivity of the training process to harder training examples. We tested our method on several benchmark datasets. We propose, and corroborate in our experiments, that the optimal level of sensitivity to hard example is positively correlated with the depth of the network. Moreover, the test prediction error obtained by our method is generally lower than that of the vanilla cross-entropy gradient learner. We therefore conclude that tunable sensitivity can be helpful for neural network learning. version:1
arxiv-1611-07727 | Pose-Track: Joint Multi-Person Pose Estimation and Tracking | http://arxiv.org/abs/1611.07727 | id:1611.07727 author:Umar Iqbal, Anton Milan, Juergen Gall category:cs.CV  published:2016-11-23 summary:In this work, we introduce the challenging problem of joint multi-person pose estimation and tracking of an unknown number of persons in unconstrained videos. Existing methods for multi-person pose estimation in images cannot be applied directly to this problem, since it also requires to solve the problem of person association over time in addition to the pose estimation for each person. We therefore propose a novel method that jointly models multi-person pose estimation and tracking in a single formulation. To this end, we represent body joint detections in a video by a spatio-temporal graph and solve an integer linear program to partition the graph into sub-graphs that correspond to plausible body pose trajectories for each person. The proposed approach implicitly handles occlusions and truncations of persons. Since the problem has not been addressed quantitatively in the literature, we introduce a challenging "Multi-Person Pose-Track" dataset, and also propose a completely unconstrained evaluation protocol that does not make any assumptions on the scale, size, location or the number of persons. Finally, we evaluate the proposed approach and several baseline methods on our new dataset. version:1
arxiv-1611-07725 | iCaRL: Incremental Classifier and Representation Learning | http://arxiv.org/abs/1611.07725 | id:1611.07725 author:Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Christoph H. Lampert category:cs.CV cs.LG stat.ML  published:2016-11-23 summary:A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on the CIFAR-100 and ImageNet ILSVRC 2012 datasets that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail. version:1
arxiv-1611-07718 | On the Connection of Deep Fusion to Ensembling | http://arxiv.org/abs/1611.07718 | id:1611.07718 author:Liming Zhao, Jingdong Wang, Xi Li, Zhuowen Tu, Wenjun Zeng category:cs.CV  published:2016-11-23 summary:In this paper, we provide a systematic study to the prevailing ResNet architecture by showing a connection from a general deeply-fused net view to ensembling. We start by empirically demonstrating the resemblance between the expanded form of the deeply-fused net and an ensemble of neural networks. Our empirical results uncover that the deepest network among the ensemble components does not contribute the most significantly to the overall performance and instead it provides a manner to introduce many layers and thus guarantee the ensemble size. Guided by the above study and observation, we develop a new deeply-fused network that combines two networks in a \emph{merge-and-run} fusion manner. It is less deep than a ResNet with the same number of parameters but yields an ensemble of the same number of more-capable component networks, thus improving the classification accuracy. We evaluate the proposed network on the standard recognition tasks. Our approach demonstrates consistent improvements over the ResNet with the comparable setup, and achieves the state-of-the-art results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$, $1.51\%$ on SVHN). version:1
arxiv-1611-07715 | Deep Feature Flow for Video Recognition | http://arxiv.org/abs/1611.07715 | id:1611.07715 author:Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei category:cs.CV  published:2016-11-23 summary:Deep convolutional neutral networks have achieved great success on image recognition tasks. Yet, it is non-trivial to transfer the state-of-the-art image recognition networks to videos as per-frame evaluation is too slow and unaffordable. We present deep feature flow, a fast and accurate framework for video recognition. It runs the expensive convolutional sub-network only on sparse key frames and propagates their deep feature maps to other frames via a flow field. It achieves significant speedup as flow computation is relatively fast. The end-to-end training of the whole architecture significantly boosts the recognition accuracy. Deep feature flow is flexible and general. It is validated on two recent large scale video datasets. It makes a large step towards practical video recognition. version:1
arxiv-1611-07710 | Spatio-Temporal Modeling of Check-ins in Location-Based Social Networks | http://arxiv.org/abs/1611.07710 | id:1611.07710 author:Ali Zarezade, Sina Jafarzadeh, Hamid R. Rabiee category:cs.SI stat.ML  published:2016-11-23 summary:Social networks are getting closer to our real physical world. People share the exact location and time of their check-ins and are influenced by their friends. Modeling the spatio-temporal behavior of users in social networks is of great importance for predicting the future behavior of users, controlling the users' movements, and finding the latent influence network. It is observed that users have periodic patterns in their movements. Also, they are influenced by the locations that their close friends recently visited. Leveraging these two observations, we propose a probabilistic model based on a doubly stochastic point process with a periodic decaying kernel for the time of check-ins and a time-varying multinomial distribution for the location of check-ins of users in the location-based social networks. We learn the model parameters by using an efficient EM algorithm, which distributes over the users. Experiments on synthetic and real data gathered from Foursquare show that the proposed inference algorithm learns the parameters efficiently and our method models the real data better than other alternatives. version:1
arxiv-1611-07709 | Fully Convolutional Instance-aware Semantic Segmentation | http://arxiv.org/abs/1611.07709 | id:1611.07709 author:Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei category:cs.CV  published:2016-11-23 summary:We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. The code would be released at \url{https://github.com/daijifeng001/TA-FCN}. version:1
arxiv-1611-07703 | 'Part'ly first among equals: Semantic part-based benchmarking for state-of-the-art object recognition systems | http://arxiv.org/abs/1611.07703 | id:1611.07703 author:Ravi Kiran Sarvadevabhatla, Shanthakumar Venkatraman, Venkatesh R Babu category:cs.CV  published:2016-11-23 summary:An examination of object recognition challenge leaderboards (ILSVRC, PASCAL-VOC) reveals that the top-performing classifiers typically exhibit small differences amongst themselves in terms of error rate/mAP. To better differentiate the top performers, additional criteria are required. Moreover, the (test) images, on which the performance scores are based, predominantly contain fully visible objects. Therefore, `harder' test images, mimicking the challenging conditions (e.g. occlusion) in which humans routinely recognize objects, need to be utilized for benchmarking. To address the concerns mentioned above, we make two contributions. First, we systematically vary the level of local object-part content, global detail and spatial context in images from PASCAL VOC 2010 to create a new benchmarking dataset dubbed PPSS-12. Second, we propose an object-part based benchmarking procedure which quantifies classifiers' robustness to a range of visibility and contextual settings. The benchmarking procedure relies on a semantic similarity measure that naturally addresses potential semantic granularity differences between the category labels in training and test datasets, thus eliminating manual mapping. We use our procedure on the PPSS-12 dataset to benchmark top-performing classifiers trained on the ILSVRC-2012 dataset. Our results show that the proposed benchmarking procedure enables additional differentiation among state-of-the-art object classifiers in terms of their ability to handle missing content and insufficient object detail. Given this capability for additional differentiation, our approach can potentially supplement existing benchmarking procedures used in object recognition challenge leaderboards. version:1
arxiv-1611-07700 | 3D Menagerie: Modeling the 3D shape and pose of animals | http://arxiv.org/abs/1611.07700 | id:1611.07700 author:Silvia Zuffi, Angjoo Kanazawa, David Jacobs, Michael J. Black category:cs.CV  published:2016-11-23 summary:There has been significant prior work on learning realistic, articulated, 3D statistical shape models of the human body. In contrast, there are few such models for animals, despite their many applications. The main challenge is that animals are much less cooperative subjects than humans. The best human body models are learned from thousands of 3D scans of people in specific poses, which is infeasible with live animals. Consequently, here we extend a state-of-the-art articulated 3D human body model to animals and learn it from a limited set of 3D scans of toy figurines in arbitrary poses. We employ a novel part-based shape model to compute an initial registration to the scans. We then normalize their pose, learn a statistical shape model, and refine the alignments and the model together. In this way, we accurately align animal scans from different quadruped families with very different shapes and poses. With the alignment to a common template we learn a shape space representing animals including lions, cats, dogs, horses, cows and hippos. Animal shapes can be sampled from the model, posed, animated, and fitted to data. In particular, we demonstrate the generalization of the model by fitting it to images of real animals, and show that it captures realistic animal shapes, even for new species not seen in training. We make our model available for research, enabling the extension of methods for human shape and pose estimation to animals. version:1
arxiv-1611-07688 | UniMiB SHAR: a new dataset for human activity recognition using acceleration data from smartphones | http://arxiv.org/abs/1611.07688 | id:1611.07688 author:Daniela Micucci, Marco Mobilio, Paolo Napoletano category:cs.CV  published:2016-11-23 summary:Smartphones, smartwatches, fitness trackers, and ad-hoc wearable devices are being increasingly used to monitor human activities. Usually, machine-learning-based algorithms process data acquired by their sensors to classify human activities. The success of those algorithms mostly depends on the availability of training (labeled) data. In this letter we present a new smartphone accelerometer dataset designed for activity recognition. The dataset includes 7,013 activities performed by 30 subjects, mostly females, of ages ranging from 18 to 60 years. Activities are divided in 17 fine grained classes grouped in two coarse grained classes: 9 types of activities of daily living (ADL) and 8 types of falls. The dataset, benchmarked with two different classifiers, thanks to its unique features will be of interest to the scientific community. version:1
arxiv-1611-07675 | Video Captioning with Transferred Semantic Attributes | http://arxiv.org/abs/1611.07675 | id:1611.07675 author:Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei category:cs.CV  published:2016-11-23 summary:Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNN) to encode video content and Recurrent Neural Networks (RNN) to decode a sentence. In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic attributes play a significant contribution to captioning, and 2) images and videos carry complementary semantics and thus can reinforce each other for captioning. To boost video captioning, we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD and MPII-MD. Our proposed LSTM-TSA achieves to-date the best published performance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4 and CIDEr-D. Superior results when compared to state-of-the-art methods are also reported on M-VAD and MPII-MD. version:1
arxiv-1611-07663 | Learning Cost-Effective and Interpretable Regimes for Treatment Recommendation | http://arxiv.org/abs/1611.07663 | id:1611.07663 author:Himabindu Lakkaraju, Cynthia Rudin category:stat.ML  published:2016-11-23 summary:Decision makers, such as doctors and judges, make crucial decisions such as recommending treatments to patients, and granting bails to defendants on a daily basis. Such decisions typically involve weighting the potential benefits of taking an action against the costs involved. In this work, we aim to automate this task of learning {cost-effective, interpretable and actionable treatment regimes. We formulate this as a problem of learning a decision list -- a sequence of if-then-else rules -- which maps characteristics of subjects (eg., diagnostic test results of patients) to treatments. We propose a novel objective to construct a decision list which maximizes outcomes for the population, and minimizes overall costs. We model the problem of learning such a list as a Markov Decision Process (MDP) and employ a variant of the Upper Confidence Bound for Trees (UCT) strategy which leverages customized checks for pruning the search space effectively. Experimental results on real world observational data capturing treatment recommendations for asthma patients demonstrate the effectiveness of our approach. version:1
arxiv-1611-07661 | Neural Multigrid | http://arxiv.org/abs/1611.07661 | id:1611.07661 author:Tsung-Wei Ke, Michael Maire, Stella X. Yu category:cs.CV cs.LG cs.NE  published:2016-11-23 summary:We propose a multigrid extension of convolutional neural networks (CNNs). Rather than manipulating representations living on a single spatial grid, our network layers operate across scale space, on a pyramid of tensors. They consume multigrid inputs and produce multigrid outputs; convolutional filters themselves have both within-scale and cross-scale extent. This aspect is distinct from simple multiscale designs, which only process the input at different scales. Viewed in terms of information flow, a multigrid network passes messages across a spatial pyramid. As a consequence, receptive field size grows exponentially with depth, facilitating rapid integration of context. Most critically, multigrid structure enables networks to learn internal attention and dynamic routing mechanisms, and use them to accomplish tasks on which modern CNNs fail. Experiments demonstrate wide-ranging performance advantages of multigrid. On CIFAR image classification, flipping from single to multigrid within standard CNN architectures improves accuracy at modest compute and parameter increase. Multigrid is independent of other architectural choices; we show synergistic results in combination with residual connections. On tasks demanding per-pixel output, gains can be substantial. We show dramatic improvement on a synthetic semantic segmentation dataset. Strikingly, we show that relatively shallow multigrid networks can learn to directly perform spatial transformation tasks, where, in contrast, current CNNs fail. Together, our results suggest that continuous evolution of features on a multigrid pyramid could replace virtually all existing CNN designs. version:1
arxiv-1611-07659 | Improving Efficiency of SVM k-fold Cross-validation by Alpha Seeding | http://arxiv.org/abs/1611.07659 | id:1611.07659 author:Zeyi Wen, Bin Li, Rao Kotagiri, Jian Chen, Yawen Chen, Rui Zhang category:cs.LG  published:2016-11-23 summary:The k-fold cross-validation is commonly used to evaluate the effectiveness of SVMs with the selected hyper-parameters. It is known that the SVM k-fold cross-validation is expensive, since it requires training k SVMs. However, little work has explored reusing the h-th SVM for training the (h+1)-th SVM for improving the efficiency of k-fold cross-validation. In this paper, we propose three algorithms that reuse the h-th SVM for improving the efficiency of training the (h+1)-th SVM. Our key idea is to efficiently identify the support vectors and to accurately estimate their associated weights (also called alpha values) of the next SVM by using the previous SVM. Our experimental results show that our algorithms are several times faster than the k-fold cross-validation which does not make use of the previously trained SVM. Moreover, our algorithms produce the same results (hence same accuracy) as the k-fold cross-validation which does not make use of the previously trained SVM. version:1
arxiv-1611-07635 | T-CONV: A Convolutional Neural Network For Multi-scale Taxi Trajectory Prediction | http://arxiv.org/abs/1611.07635 | id:1611.07635 author:Jianming Lv, Qing Li, Xintong Wang category:cs.CV  published:2016-11-23 summary:Precise destination prediction of taxi trajectories can benefit efficient scheduling of taxies and accurate advertisement. Traditional prediction algorithms treat trajectories as sequences of spatial points and process them in one single predefined spatial scale. In this paper, we show that taxi trajectories can exhibit different patterns in different spatial scales, and the combination of multi-scale patterns can improve the accuracy of prediction. Based on this, we propose T-CONV to achieve higher accuracy, which models trajectories as images and adopts multi-layer convolutional neural networks to combine multi-scale trajectory patterns. Furthermore, in order to solve the sparsity problem of trajectories, we integrate multiple local convolutional fields in T-CONV to capture important specific areas of trajectories. Comprehensive experiments based on real trajectory data show that T-CONV can achieve much better results than state-of-the-art methods. version:1
arxiv-1611-07634 | Interpretation of Prediction Models Using the Input Gradient | http://arxiv.org/abs/1611.07634 | id:1611.07634 author:Yotam Hechtlinger category:stat.ML cs.LG  published:2016-11-23 summary:State of the art machine learning algorithms are highly optimized to provide the optimal prediction possible, naturally resulting in complex models. While these models often outperform simpler more interpretable models by order of magnitudes, in terms of understanding the way the model functions, we are often facing a "black box". In this paper we suggest a simple method to interpret the behavior of any predictive model, both for regression and classification. Given a particular model, the information required to interpret it can be obtained by studying the partial derivatives of the model with respect to the input. We exemplify this insight by interpreting convolutional and multi-layer neural networks in the field of natural language processing. version:1
arxiv-1611-07627 | SyGuS-Comp 2016: Results and Analysis | http://arxiv.org/abs/1611.07627 | id:1611.07627 author:Rajeev Alur, Dana Fisman, Rishabh Singh, Armando Solar-Lezama category:cs.SE cs.LG cs.LO  published:2016-11-23 summary:Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an implementation f that meets both a semantic constraint given by a logical formula $\varphi$ in a background theory T, and a syntactic constraint given by a grammar G, which specifies the allowed set of candidate implementations. Such a synthesis problem can be formally defined in SyGuS-IF, a language that is built on top of SMT-LIB. The Syntax-Guided Synthesis Competition (SyGuS-Comp) is an effort to facilitate, bring together and accelerate research and development of efficient solvers for SyGuS by providing a platform for evaluating different synthesis techniques on a comprehensive set of benchmarks. In this year's competition we added a new track devoted to programming by examples. This track consisted of two categories, one using the theory of bit-vectors and one using the theory of strings. This paper presents and analyses the results of SyGuS-Comp'16. version:1
arxiv-1611-07609 | Adaptive Accelerated Gradient Converging Methods under Holderian Error Bound Condition | http://arxiv.org/abs/1611.07609 | id:1611.07609 author:Tianbao Yang category:math.OC stat.ML  published:2016-11-23 summary:In this paper, we focus our study on the convergence of (proximal) gradient methods and accelerated (proximal) gradient methods for smooth (composite) optimization under a H\"{o}lderian error bound (HEB) condition. We first show that proximal gradient (PG) method is automatically adaptive to HEB while accelerated proximal gradient (APG) method can be adaptive to HEB by restart with an improved iteration complexity. However, the number of iterations to restart APG hinges on a possibly unknown parameter. To address this issue, we propose to develop adaptive gradient converging methods, i.e., using the magnitude of gradient as a criterion for restart and termination. We develop adaptive accelerated gradient converging (adaAGC) methods for solving smooth (composite) optimization under the HEB condition with an explicit exponent $\theta$, and establish an better iteration complexity than PG. Furthermore, we demonstrate that these results have important implication and applications in machine learning: (i) if the considered objective function is coercive and semi-algebraic, PG's convergence speed is essentially $o(\frac{1}{t})$, where $t$ is the total number of iterations; (ii) if the objective function consists of an $\ell_1$, $\ell_\infty$, $\ell_{1,\infty}$, or huber norm regularization and a convex smooth piecewise quadratic loss, which includes least-squares loss, smoothed hinge loss and huber loss, the proposed adaAGC is parameter-free and enjoys a faster linear convergence than PG without any other assumptions (e.g., restricted eigen-value condition). It is notable that for all aforementioned problems, our linear convergence results are global instead of local. To the best of our knowledge, these improved results are the first shown in this work. version:1
arxiv-1611-06962 | Sampled Image Tagging and Retrieval Methods on User Generated Content | http://arxiv.org/abs/1611.06962 | id:1611.06962 author:Karl Ni, Kyle Zaragoza, Carmen Carrano, Barry Chen, Yonas Tesfaye, Alex Gude category:cs.CV  published:2016-11-21 summary:Traditional image tagging and retrieval algorithms have limited value as a result of being trained with heavily curated datasets. These limitations are most evident when arbitrary search words are used that do not intersect with training set labels. Weak labels from user generated content (UGC) found in the wild (e.g., Google Photos, FlickR, etc.) have an almost unlimited number of unique words in the metadata tags. Prior work on word embeddings successfully leveraged unstructured text with large vocabularies, and our proposed method seeks to apply similar cost functions to open source imagery. Specifically, we train a deep learning image tagging and retrieval system on large scale, user generated content (UGC) using sampling methods and joint optimization of word embeddings. By using the Yahoo! FlickR Creative Commons (YFCC100M) dataset, such an approach builds robustness to common unstructured data issues that include but are not limited to irrelevant tags, misspellings, multiple languages, polysemy, and tag imbalance. As a result, the final proposed algorithm will not only yield comparable results to state of the art in conventional image tagging, but will enable new capability to train algorithms on large, scale unstructured text in the YFCC100M dataset and outperform cited work in zero-shot capability. version:2
arxiv-1611-07596 | Fast Fourier Color Constancy | http://arxiv.org/abs/1611.07596 | id:1611.07596 author:Jonathan T. Barron, Yun-Ta Tsai category:cs.CV  published:2016-11-23 summary:We present Fast Fourier Color Constancy (FFCC), a color constancy algorithm which solves illuminant estimation by reducing it to a spatial localization task on a torus. By operating in the frequency domain, FFCC produces lower error rates than the previous state-of-the-art by 10-13% while being 250-3000 times faster. This unconventional approach introduces challenges regarding aliasing, directional statistics, and preconditioning, which we address. By producing a complete posterior distribution over illuminants instead of a single illuminant estimate, FFCC enables better training techniques, an effective temporal smoothing technique, and richer methods for error analysis. Our implementation of FFCC runs at ~700 frames per second on a mobile device, allowing it to be used as an accurate, real-time, temporally-coherent automatic white balance algorithm. version:1
arxiv-1611-07593 | Learning Joint Feature Adaptation for Zero-Shot Recognition | http://arxiv.org/abs/1611.07593 | id:1611.07593 author:Ziming Zhang, Venkatesh Saligrama category:cs.CV  published:2016-11-23 summary:Zero-shot recognition (ZSR) aims to recognize target-domain data instances of unseen classes based on the models learned from associated pairs of seen-class source and target domain data. One of the key challenges in ZSR is the relative scarcity of source-domain features (e.g. one feature vector per class), which do not fully account for wide variability in target-domain instances. In this paper we propose a novel framework of learning data-dependent feature transforms for scoring similarity between an arbitrary pair of source and target data instances to account for the wide variability in target domain. Our proposed approach is based on optimizing over a parameterized family of local feature displacements that maximize the source-target adaptive similarity functions. Accordingly we propose formulating zero-shot learning (ZSL) using latent structural SVMs to learn our similarity functions from training data. As demonstration we design a specific algorithm under the proposed framework involving bilinear similarity functions and regularized least squares as penalties for feature displacement. We test our approach on several benchmark datasets for ZSR and show significant improvement over the state-of-the-art. For instance, on aP&Y dataset we can achieve 80.89% in terms of recognition accuracy, outperforming the state-of-the-art by 11.15%. version:1
arxiv-1611-07588 | A Neural Network Model to Classify Liver Cancer Patients Using Data Expansion and Compression | http://arxiv.org/abs/1611.07588 | id:1611.07588 author:Ashkan Zeinalzadeh, Tom Wenska, Gordon Okimoto category:stat.ML cs.LG q-bio.QM  published:2016-11-23 summary:We develop a neural network model to classify liver cancer patients into high-risk and low-risk groups using genomic data. Our approach provides a novel technique to classify big data sets using neural network models. We preprocess the data before training the neural network models. We first expand the data using wavelet analysis. We then compress the wavelet coefficients by mapping them onto a new scaled orthonormal coordinate system. Then the data is used to train a neural network model that enables us to classify cancer patients into two different classes of high-risk and low-risk patients. We use the leave-one-out approach to build a neural network model. This neural network model enables us to classify a patient using genomic data as a high-risk or low-risk patient without any information about the survival time of the patient. The results from genomic data analysis are compared with survival time analysis. It is shown that the expansion and compression of data using wavelet analysis and singular value decomposition (SVD) is essential to train the neural network model. version:1
arxiv-1611-07583 | Alternating Direction Graph Matching | http://arxiv.org/abs/1611.07583 | id:1611.07583 author:D. Khuê Lê-Huu, Nikos Paragios category:cs.CV  published:2016-11-22 summary:In this paper, we introduce a graph matching method that can account for constraints of arbitrary order, with arbitrary potential functions. Unlike previous decomposition approaches that rely on the graph structures, we introduce a decomposition of the matching constraints. Graph matching is then reformulated as a non-convex non-separable optimization problem that can be split into smaller and much-easier-to-solve subproblems, by means of the alternating direction method of multipliers. The proposed framework is modular, scalable, and can be instantiated into different variants. Two instantiations are studied exploring pairwise and higher-order constraints. Experimental results on widely adopted benchmarks involving synthetic and real examples demonstrate that the proposed solutions outperform existing pairwise graph matching methods, and competitive with the state of the art in higher-order settings. version:1
arxiv-1611-07579 | Programs as Black-Box Explanations | http://arxiv.org/abs/1611.07579 | id:1611.07579 author:Sameer Singh, Marco Tulio Ribeiro, Carlos Guestrin category:stat.ML cs.AI cs.LG  published:2016-11-22 summary:Recent work in model-agnostic explanations of black-box machine learning has demonstrated that interpretability of complex models does not have to come at the cost of accuracy or model flexibility. However, it is not clear what kind of explanations, such as linear models, decision trees, and rule lists, are the appropriate family to consider, and different tasks and models may benefit from different kinds of explanations. Instead of picking a single family of representations, in this work we propose to use "programs" as model-agnostic explanations. We show that small programs can be expressive yet intuitive as explanations, and generalize over a number of existing interpretable families. We propose a prototype program induction method based on simulated annealing that approximates the local behavior of black-box classifiers around a specific prediction using random perturbations. Finally, we present preliminary application on small datasets and show that the generated explanations are intuitive and accurate for a number of classifiers. version:1
arxiv-1611-07573 | Relaxed Earth Mover's Distances for Chain- and Tree-connected Spaces and their use as a Loss Function in Deep Learning | http://arxiv.org/abs/1611.07573 | id:1611.07573 author:Manuel Martinez, Monica Haurilet, Ziad Al-Halah, Makarand Tapaswi, Rainer Stiefelhagen category:cs.CV  published:2016-11-22 summary:The Earth Mover's Distance (EMD) computes the optimal cost of transforming one distribution into another, given a known transport metric between them. In deep learning, the EMD loss allows us to embed information during training about the output space structure like hierarchical or semantic relations. This helps in achieving better output smoothness and generalization. However EMD is computationally expensive.Moreover, solving EMD optimization problems usually require complex techniques like lasso. These properties limit the applicability of EMD-based approaches in large scale machine learning. We address in this work the difficulties facing incorporation of EMD-based loss in deep learning frameworks. Additionally, we provide insight and novel solutions on how to integrate such loss function in training deep neural networks. Specifically, we make three main contributions: (i) we provide an in-depth analysis of the fastest state-of-the-art EMD algorithm (Sinkhorn Distance) and discuss its limitations in deep learning scenarios. (ii) we derive fast and numerically stable closed-form solutions for the EMD gradient in output spaces with chain- and tree- connectivity; and (iii) we propose a relaxed form of the EMD gradient with equivalent computational complexity but faster convergence rate. We support our claims with experiments on real datasets. In a restricted data setting on the ImageNet dataset, we train a model to classify 1000 categories using 50K images, and demonstrate that our relaxed EMD loss achieves better Top-1 accuracy than the cross entropy loss. Overall, we show that our relaxed EMD loss criterion is a powerful asset for deep learning in the small data regime. version:1
arxiv-1611-07571 | Quad-networks: unsupervised learning to rank for interest point detection | http://arxiv.org/abs/1611.07571 | id:1611.07571 author:Nikolay Savinov, Akihito Seki, Lubor Ladicky, Torsten Sattler, Marc Pollefeys category:cs.CV cs.LG cs.NE  published:2016-11-22 summary:Several machine learning tasks require to represent the data using only a sparse set of interest points. An ideal detector is able to find the corresponding interest points even if the data undergo a transformation typical for a given domain. Since the task is of high practical interest in computer vision, many hand-crafted solutions were proposed. In this paper, we ask a fundamental question: can we learn such detectors from scratch? Since it is often unclear, what points are "interesting", human labelling cannot be used to find a truly unbiased solution. Therefore, the task requires an unsupervised formulation. We are the first to propose such a formulation: training a neural network to rank points in a transformation-invariant manner. Interest points are then extracted from the top/bottom quantiles of this ranking. We validate our approach on two tasks: standard RGB image interest point detection and challenging cross-modal interest point detection between RGB and depth images. We quantitatively show that our unsupervised method performs better or on-par with baselines. version:1
arxiv-1611-07567 | Feature Importance Measure for Non-linear Learning Algorithms | http://arxiv.org/abs/1611.07567 | id:1611.07567 author:Marina M. -C. Vidovic, Nico Görnitz, Klaus-Robert Müller, Marius Kloft category:cs.AI cs.LG stat.ML  published:2016-11-22 summary:Complex problems may require sophisticated, non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies. However, high prediction accuracies are not the only objective to consider when solving problems using machine learning. Instead, particular scientific applications require some explanation of the learned prediction function. Unfortunately, most methods do not come with out of the box straight forward interpretation. Even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure. In this paper, we propose the Measure of Feature Importance (MFI). MFI is general and can be applied to any arbitrary learning machine (including kernel machines and deep learning). MFI is intrinsically non-linear and can detect features that by itself are inconspicuous and only impact the prediction function through their interaction with other features. Lastly, MFI can be used for both --- model-based feature importance and instance-based feature importance (i.e, measuring the importance of a feature for a particular data point). version:1
arxiv-1611-07559 | Sar image despeckling based on nonlocal similarity sparse decomposition | http://arxiv.org/abs/1611.07559 | id:1611.07559 author:Chengwei Sang, Hong Sun, Quisong Xia category:cs.CV  published:2016-11-22 summary:This letter presents a method of synthetic aperture radar (SAR) image despeckling aimed to preserve the detail information while suppressing speckle noise. This method combines the nonlocal self-similarity partition and a proposed modified sparse decomposition. The nonlocal partition method groups a series of structure-similarity data sets. Each data set has a good sparsity for learning an over-complete dictionary in sparse representation. In the sparse decomposition, we propose a novel method to identify principal atoms from over-complete dictionary to form a principal dictionary. Despeckling is performed on each data set over the principal dictionary with principal atoms. Experimental results demonstrate that the proposed method can achieve high performances in terms of both speckle noise reduction and structure details preservation. version:1
arxiv-1611-07555 | Randomized Distributed Mean Estimation: Accuracy vs Communication | http://arxiv.org/abs/1611.07555 | id:1611.07555 author:Jakub Konečný, Peter Richtárik category:cs.DC math.NA stat.ML  published:2016-11-22 summary:We consider the problem of estimating the arithmetic average of a finite collection of real vectors stored in a distributed fashion across several compute nodes subject to a communication budget constraint. Our analysis does not rely on any statistical assumptions about the source of the vectors. This problem arises as a subproblem in many applications, including reduce-all operations within algorithms for distributed and federated optimization and learning. We propose a flexible family of randomized algorithms exploring the trade-off between expected communication cost and estimation error. Our family contains the full-communication and zero-error method on one extreme, and an $\epsilon$-bit communication and ${\cal O}\left(1/(\epsilon n)\right)$ error method on the opposite extreme. In the special case where we communicate, in expectation, a single bit per coordinate of each vector, we improve upon existing results by obtaining $\mathcal{O}(r/n)$ error, where $r$ is the number of bits used to represent a floating point value. version:1
arxiv-1611-06459 | Gendered Conversation in a Social Game-Streaming Platform | http://arxiv.org/abs/1611.06459 | id:1611.06459 author:Supun Nakandala, Giovanni Luca Ciampaglia, Norman Makoto Su, Yong-Yeol Ahn category:cs.SI cs.CL cs.CY  published:2016-11-20 summary:Online social media and games are increasingly replacing offline social activities. Social media is now an indispensable mode of communication; online gaming is not only a genuine social activity but also a popular spectator sport. With support for anonymity and larger audiences, online interaction shrinks social and geographical barriers. Despite such benefits, social disparities such as gender inequality persist in online social media. In particular, online gaming communities have been criticized for persistent gender disparities and objectification. As gaming evolves into a social platform, persistence of gender disparity is a pressing question. Yet, there are few large-scale, systematic studies of gender inequality and objectification in social gaming platforms. Here we analyze more than one billion chat messages from Twitch, a social game-streaming platform, to study how the gender of streamers is associated with the nature of conversation. Using a combination of computational text analysis methods, we show that gendered conversation and objectification is prevalent in chats. Female streamers receive significantly more objectifying comments while male streamers receive more game-related comments. This difference is more pronounced for popular streamers. There also exists a large number of users who post only on female or male streams. Employing a neural vector-space embedding (paragraph vector) method, we analyze gendered chat messages and create prediction models that (i) identify the gender of streamers based on messages posted in the channel and (ii) identify the gender a viewer prefers to watch based on their chat messages. Our findings suggest that disparities in social game-streaming platforms is a nuanced phenomenon that involves the gender of streamers as well as those who produce gendered and game-related conversation. version:2
arxiv-1611-07544 | Self-learning Scene-specific Pedestrian Detectors using a Progressive Latent Model | http://arxiv.org/abs/1611.07544 | id:1611.07544 author:Qixiang Ye, Tianliang Zhang, Qiang Qiu, Baochang Zhang, Jie Chen, Guillermo Sapiro category:cs.CV  published:2016-11-22 summary:In this paper, a self-learning approach is proposed towards solving scene-specific pedestrian detection problem without any human' annotation involved. The self-learning approach is deployed as progressive steps of object discovery, object enforcement, and label propagation. In the learning procedure, object locations in each frame are treated as latent variables that are solved with a progressive latent model (PLM). Compared with conventional latent models, the proposed PLM incorporates a spatial regularization term to reduce ambiguities in object proposals and to enforce object localization, and also a graph-based label propagation to discover harder instances in adjacent frames. With the difference of convex (DC) objective functions, PLM can be efficiently optimized with a concave-convex programming and thus guaranteeing the stability of self-learning. Extensive experiments demonstrate that even without annotation the proposed self-learning approach outperforms weakly supervised learning approaches, while achieving comparable performance with transfer learning and fully supervised approaches. version:1
arxiv-1611-07509 | A causal framework for discovering and removing direct and indirect discrimination | http://arxiv.org/abs/1611.07509 | id:1611.07509 author:Lu Zhang, Yongkai Wu, Xintao Wu category:cs.LG  published:2016-11-22 summary:Anti-discrimination is an increasingly important task in data science. In this paper, we investigate the problem of discovering both direct and indirect discrimination from the historical data, and removing the discriminatory effects before the data is used for predictive analysis (e.g., building classifiers). We make use of the causal network to capture the causal structure of the data. Then we model direct and indirect discrimination as the path-specific effects, which explicitly distinguish the two types of discrimination as the causal effects transmitted along different paths in the network. Based on that, we propose an effective algorithm for discovering direct and indirect discrimination, as well as an algorithm for precisely removing both types of discrimination while retaining good data utility. Different from previous works, our approaches can ensure that the predictive models built from the modified data will not incur discrimination in decision making. Experiments using real datasets show the effectiveness of our approaches. version:1
arxiv-1611-07507 | Variational Intrinsic Control | http://arxiv.org/abs/1611.07507 | id:1611.07507 author:Karol Gregor, Danilo Jimenez Rezende, Daan Wierstra category:cs.LG cs.AI  published:2016-11-22 summary:In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks. version:1
arxiv-1611-07492 | Inducing Interpretable Representations with Variational Autoencoders | http://arxiv.org/abs/1611.07492 | id:1611.07492 author:N. Siddharth, Brooks Paige, Alban Desmaison, Jan-Willem Van de Meent, Frank Wood, Noah D. Goodman, Pushmeet Kohli, Philip H. S. Torr category:stat.ML cs.CV cs.LG  published:2016-11-22 summary:We develop a framework for incorporating structured graphical models in the \emph{encoders} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes. version:1
arxiv-1611-07490 | Can Co-robots Learn to Teach? | http://arxiv.org/abs/1611.07490 | id:1611.07490 author:Harshal Maske, Emily Kieson, Girish Chowdhary, Charles Abramson category:cs.RO cs.LG  published:2016-11-22 summary:We explore beyond existing work on learning from demonstration by asking the question: Can robots learn to teach?, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct or collaborate with humans in executing complex tasks in uncertain environments? In this paper we pursue a solution to this problem by leveraging the idea that humans often implicitly decompose a higher level task into several subgoals whose execution brings the task closer to completion. We propose Dirichlet process based non-parametric Inverse Reinforcement Learning (DPMIRL) approach for reward based unsupervised clustering of task space into subgoals. This approach is shown to capture the latent subgoals that a human teacher would have utilized to train a novice. The notion of action primitive is introduced as the means to communicate instruction policy to humans in the least complicated manner, and as a computationally efficient tool to segment demonstration data. We evaluate our approach through experiments on hydraulic actuated scaled model of an excavator and evaluate and compare different teaching strategies utilized by the robot. version:1
arxiv-1611-07485 | Scene Labeling using Recurrent Neural Networks with Explicit Long Range Contextual Dependency | http://arxiv.org/abs/1611.07485 | id:1611.07485 author:Qiangui Huang, Weiyue Wang, Kevin Zhou, Suya You, Ulrich Neumann category:cs.CV  published:2016-11-22 summary:Spatial contextual dependencies are crucial for scene labeling problems. Recurrent neural network (RNN) is one of state-of-the-art methods for modeling contextual dependencies. However, RNNs are fundamentally designed for sequential data, not spatial data. This work shows that directly applying traditional RNN architectures, which unfold a 2D lattice grid into a sequence, is not sufficient to model structure dependencies in images due to the "impact vanishing" problem. A new RNN unit with Explicit Long-range Conditioning (RNN-ELC) is designed to overcome this problem. Based on this new RNN-ELC unit, a novel neural network architecture is built for scene labeling tasks. This architecture achieves state-of-the-art performances on several standard scene labeling datasets. Comprehensive experiments demonstrate that scene labeling tasks benefit a lot from the explicit long range contextual dependencies encoded in our algorithm. version:1
arxiv-1611-07476 | Singularity of the Hessian in Deep Learning | http://arxiv.org/abs/1611.07476 | id:1611.07476 author:Levent Sagun, Leon Bottou, Yann LeCun category:cs.LG  published:2016-11-22 summary:We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges indicating the complexity of the input data. version:1
arxiv-1611-07460 | Poisson Random Fields for Dynamic Feature Models | http://arxiv.org/abs/1611.07460 | id:1611.07460 author:Valerio Perrone, Paul A. Jenkins, Dario Spano, Yee Whye Teh category:stat.ML  published:2016-11-22 summary:We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015. version:1
arxiv-1611-06284 | Understanding Anatomy Classification Through Visualization | http://arxiv.org/abs/1611.06284 | id:1611.06284 author:Devinder Kumar, Vlado Menkovski category:cs.CV  published:2016-11-19 summary:One of the main challenges for broad adoption of deep convolutional neural network (DCNN) models is the lack of understanding of their decision process. In many applications a simpler less capable model that can be easily understood is favorable to a black-box model that has superior performance. In this paper, we present an approach for designing DCNN models based on visualization of the internal activations of the model. We visualize the model's response using fractional stride convolution technique and compare the results with known imaging landmarks from the medical literature. We show that sufficiently deep and capable models can be successfully trained to use the same medical landmarks a human expert would use. The presented approach allows for communicating the model decision process well, but also offers insight towards detecting biases. version:2
arxiv-1611-07450 | Grad-CAM: Why did you say that? | http://arxiv.org/abs/1611.07450 | id:1611.07450 author:Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra category:stat.ML cs.CV cs.LG  published:2016-11-22 summary:We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract. version:1
arxiv-1611-07443 | Mapping chemical performance on molecular structures using locally interpretable explanations | http://arxiv.org/abs/1611.07443 | id:1611.07443 author:Leanne S. Whitmore, Anthe George, Corey M. Hudson category:stat.ML physics.chem-ph  published:2016-11-22 summary:In this work, we present an application of Locally Interpretable Machine-Agnostic Explanations to 2-D chemical structures. Using this framework we are able to provide a structural interpretation for an existing black-box model for classifying biologically produced fuel compounds with regard to Research Octane Number. This method of "painting" locally interpretable explanations onto 2-D chemical structures replicates the chemical intuition of synthetic chemists, allowing researchers in the field to directly accept, reject, inform and evaluate decisions underlying inscrutably complex quantitative structure-activity relationship models. version:1
arxiv-1611-07438 | Achieving non-discrimination in data release | http://arxiv.org/abs/1611.07438 | id:1611.07438 author:Lu Zhang, Yongkai Wu, Xintao Wu category:cs.LG  published:2016-11-22 summary:Discrimination discovery and prevention/removal are increasingly important tasks in data mining. Discrimination discovery aims to unveil discriminatory practices on the protected attribute (e.g., gender) by analyzing the dataset of historical decision records, and discrimination prevention aims to remove discrimination by modifying the biased data before conducting predictive analysis. In this paper, we show that the key to discrimination discovery and prevention is to find the meaningful partitions that can be used to provide quantitative evidences for the judgment of discrimination. With the support of the causal graph, we present a graphical condition for identifying a meaningful partition. Based on that, we develop a simple criterion for the claim of non-discrimination, and propose discrimination removal algorithms which accurately remove discrimination while retaining good data utility. Experiments using real datasets show the effectiveness of our approaches. version:1
arxiv-1611-07429 | TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning | http://arxiv.org/abs/1611.07429 | id:1611.07429 author:Jayaraman J. Thiagarajan, Bhavya Kailkhura, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy category:stat.ML cs.LG  published:2016-11-22 summary:With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models. Existing approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy. This introduces a risk of producing interpretable but misleading explanations. As humans, we are prone to engage in this kind of behavior \cite{mythos}. In this paper, we take a step in the direction of tackling the problem of interpretability without compromising the model accuracy. We propose to build a Treeview representation of the complex model via hierarchical partitioning of the feature space, which reveals the iterative rejection of unlikely class labels until the correct association is predicted. version:1
arxiv-1611-07390 | 3D Image Reconstruction from X-Ray Measurements with Overlap | http://arxiv.org/abs/1611.07390 | id:1611.07390 author:Maria Klodt, Raphael Hauser category:physics.med-ph cs.CV  published:2016-11-22 summary:3D image reconstruction from a set of X-ray projections is an important image reconstruction problem, with applications in medical imaging, industrial inspection and airport security. The innovation of X-ray emitter arrays allows for a novel type of X-ray scanners with multiple simultaneously emitting sources. However, two or more sources emitting at the same time can yield measurements from overlapping rays, imposing a new type of image reconstruction problem based on nonlinear constraints. Using traditional linear reconstruction methods, respective scanner geometries have to be implemented such that no rays overlap, which severely restricts the scanner design. We derive a new type of 3D image reconstruction model with nonlinear constraints, based on measurements with overlapping X-rays. Further, we show that the arising optimization problem is partially convex, and present an algorithm to solve it. Experiments show highly improved image reconstruction results from both simulated and real-world measurements. version:1
arxiv-1611-07385 | Smart Library: Identifying Books in a Library using Richly Supervised Deep Scene Text Reading | http://arxiv.org/abs/1611.07385 | id:1611.07385 author:Xiao Yang, Dafang He, Wenyi Huang, Zihan Zhou, Alex Ororbia, Dan Kifer, C. Lee Giles category:cs.CV  published:2016-11-22 summary:Physical library collections are valuable and long standing resources for knowledge and learning. However, managing books in a large bookshelf and finding books on it often leads to tedious manual work, especially for large book collections where books might be missing or misplaced. Recently, deep neural models, such as Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) have achieved great success for scene text detection and recognition. Motivated by these recent successes, we aim to investigate their viability in facilitating book management, a task that introduces further challenges including large amounts of cluttered scene text, distortion, and varied lighting conditions. In this paper, we present a library inventory building and retrieval system based on scene text reading methods. We specifically design our scene text recognition model using rich supervision to accelerate training and achieve state-of-the-art performance on several benchmark datasets. Our proposed system has the potential to greatly reduce the amount of human labor required in managing book inventories as well as the space needed to store book information. version:1
