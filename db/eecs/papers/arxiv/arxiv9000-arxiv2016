arxiv-1412-6122 | Spread Unary Coding |  http://arxiv.org/abs/1412.6122  | author:Subhash Kak category:cs.NE cs.IT math.IT published:2014-12-02 summary:Unary coding is useful but it is redundant in its standard form. Unary codingcan also be seen as spatial coding where the value of the number is determinedby its place in an array. Motivated by biological finding that several neuronsin the vicinity represent the same number, we propose a variant of unarynumeration in its spatial form, where each number is represented by several 1s.We call this spread unary coding where the number of 1s used is the spread ofthe code. Spread unary coding is associated with saturation of the Hammingdistance between code words.
arxiv-1412-1135 | Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning |  http://arxiv.org/abs/1412.1135  | author:Judy Hoffman, Deepak Pathak, Trevor Darrell, Kate Saenko category:cs.CV published:2014-12-02 summary:We develop methods for detector learning which exploit joint training overboth weak and strong labels and which transfer learned perceptualrepresentations from strongly-labeled auxiliary tasks. Previous methods forweak-label learning often learn detector models independently using latentvariable optimization, but fail to share deep representation knowledge acrossclasses and usually require strong initialization. Other previous methodstransfer deep representations from domains with strong labels to those withonly weak labels, but do not optimize over individual latent boxes, and thusmay miss specific salient structures for a particular category. We propose amodel that subsumes these previous approaches, and simultaneously trains arepresentation and detectors for categories with either weak or strong labelspresent. We provide a novel formulation of a joint multiple instance learningmethod that includes examples from classification-style data when available,and also performs domain transfer learning to improve the underlying detectorrepresentation. Our model outperforms known methods on ImageNet-200 detectionwith weak labels.
arxiv-1412-1074 | Learning interpretable models of phenotypes from whole genome sequences with the Set Covering Machine |  http://arxiv.org/abs/1412.1074  | author:Alexandre Drouin, Sébastien Giguère, Vladana Sagatovich, Maxime Déraspe, François Laviolette, Mario Marchand, Jacques Corbeil category:q-bio.GN cs.CE cs.LG stat.ML published:2014-12-02 summary:The increased affordability of whole genome sequencing has motivated its usefor phenotypic studies. We address the problem of learning interpretable modelsfor discrete phenotypes from whole genomes. We propose a general approach thatrelies on the Set Covering Machine and a k-mer representation of the genomes.We show results for the problem of predicting the resistance of PseudomonasAeruginosa, an important human pathogen, against 4 antibiotics. Our resultsdemonstrate that extremely sparse models which are biologically relevant can belearnt using this approach.
arxiv-1412-0879 | Watsonsim: Overview of a Question Answering Engine |  http://arxiv.org/abs/1412.0879  | author:Sean Gallagher, Wlodek Zadrozny, Walid Shalaby, Adarsh Avadhani category:cs.CL cs.IR published:2014-12-02 summary:The objective of the project is to design and run a system similar to Watson,designed to answer Jeopardy questions. In the course of a semester, wedeveloped an open source question answering system using the Indri, Lucene,Bing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka amongadditional modules. By the end of the semester, we achieved 18% accuracy onJeopardy questions, and work has not stopped since then.
arxiv-1412-0801 | Analytical Comparison of Noise Reduction Filters for Image Restoration Using SNR Estimation |  http://arxiv.org/abs/1412.0801  | author:Poorna Banerjee Dasgupta category:cs.CV published:2014-12-02 summary:Noise removal from images is a part of image restoration in which we try toreconstruct or recover an image that has been degraded by using aprioriknowledge of the degradation phenomenon. Noises present in images can be ofvarious types with their characteristic Probability Distribution Functions(PDF). Noise removal techniques depend on the kind of noise present in theimage rather than on the image itself. This paper explores the effects ofapplying noise reduction filters having similar properties on noisy images withemphasis on Signal-to-Noise-Ratio (SNR) value estimation for comparing theresults.
arxiv-1412-0826 | Hashing on Nonlinear Manifolds |  http://arxiv.org/abs/1412.0826  | author:Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin Tang, Heng Tao Shen category:cs.CV published:2014-12-02 summary:Learning based hashing methods have attracted considerable attention due totheir ability to greatly increase the scale at which existing algorithms mayoperate. Most of these methods are designed to generate binary codes preservingthe Euclidean similarity in the original space. Manifold learning techniques,in contrast, are better able to model the intrinsic structure embedded in theoriginal high-dimensional data. The complexities of these models, and theproblems with out-of-sample data, have previously rendered them unsuitable forapplication to large-scale embedding, however. In this work, how to learncompact binary embeddings on their intrinsic manifolds is considered. In orderto address the above-mentioned difficulties, an efficient, inductive solutionto the out-of-sample data problem, and a process by which non-parametricmanifold learning may be used as the basis of a hashing method is proposed. Theproposed approach thus allows the development of a range of new hashingtechniques exploiting the flexibility of the wide variety of manifold learningapproaches available. It is particularly shown that hashing on the basis oft-SNE outperforms state-of-the-art hashing methods on large-scale benchmarkdatasets, and is very effective for image classification with very short codelengths. The proposed hashing framework is shown to be easily improved, forexample, by minimizing the quantization error with learned orthogonalrotations. In addition, a supervised inductive manifold hashing framework isdeveloped by incorporating the label information, which is shown to greatlyadvance the semantic retrieval performance.
arxiv-1412-0744 | Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from the Literature |  http://arxiv.org/abs/1412.0744  | author:Artemy Kolchinsky, Anália Lourenço, Heng-Yi Wu, Lang Li, Luis M. Rocha category:stat.ML cs.IR q-bio.QM published:2014-12-02 summary:Drug-drug interaction (DDI) is a major cause of morbidity and mortality and asubject of intense scientific interest. Biomedical literature mining can aidDDI research by extracting evidence for large numbers of potential interactionsfrom published literature and clinical databases. Though DDI is investigated indomains ranging in scale from intracellular biochemistry to human populations,literature mining has not been used to extract specific types of experimentalevidence, which are reported differently for distinct experimental goals. Wefocus on pharmacokinetic evidence for DDI, essential for identifying causalmechanisms of putative interactions and as input for further pharmacologicaland pharmaco-epidemiology investigations. We used manually curated corpora ofPubMed abstracts and annotated sentences to evaluate the efficacy of literaturemining on two tasks: first, identifying PubMed abstracts containingpharmacokinetic evidence of DDIs; second, extracting sentences containing suchevidence from abstracts. We implemented a text mining pipeline and evaluated itusing several linear classifiers and a variety of feature transforms. The mostimportant textual features in the abstract and sentence classification taskswere analyzed. We also investigated the performance benefits of using featuresderived from PubMed metadata fields, various publicly available named entityrecognizers, and pharmacokinetic dictionaries. Several classifiers performedvery well in distinguishing relevant and irrelevant abstracts (reachingF1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65,iAUC~=0.83). We found that word bigram features were important for achievingoptimal classifier performance and that features derived from Medical SubjectHeadings (MeSH) terms significantly improved abstract classification. ...
arxiv-1412-0985 | Covariance estimation using conjugate gradient for 3D classification in Cryo-EM |  http://arxiv.org/abs/1412.0985  | author:Joakim Andén, Eugene Katsevich, Amit Singer category:cs.CV published:2014-12-02 summary:Classifying structural variability in noisy projections of biologicalmacromolecules is a central problem in Cryo-EM. In this work, we build on aprevious method for estimating the covariance matrix of the three-dimensionalstructure present in the molecules being imaged. Our proposed method allows forincorporation of contrast transfer function and non-uniform distribution ofviewing angles, making it more suitable for real-world data. We evaluate itsperformance on a synthetic dataset and an experimental dataset obtained byimaging a 70S ribosome complex.
arxiv-1412-0751 | Tiered Clustering to Improve Lexical Entailment |  http://arxiv.org/abs/1412.0751  | author:John Wieting category:cs.CL published:2014-12-02 summary:Many tasks in Natural Language Processing involve recognizing lexicalentailment. Two different approaches to this problem have been proposedrecently that are quite different from each other. The first is an asymmetricsimilarity measure designed to give high scores when the contexts of thenarrower term in the entailment are a subset of those of the broader term. Thesecond is a supervised approach where a classifier is learned to predictentailment given a concatenated latent vector representation of the word. Bothof these approaches are vector space models that use a single context vector asa representation of the word. In this work, I study the effects of clusteringwords into senses and using these multiple context vectors to infer entailmentusing extensions of these two algorithms. I find that this approach offers someimprovement to these entailment algorithms.
arxiv-1412-0494 | Orthogonal Matrix Retrieval in Cryo-Electron Microscopy |  http://arxiv.org/abs/1412.0494  | author:Tejal Bhamre, Teng Zhang, Amit Singer category:cs.CV published:2014-12-01 summary:In single particle reconstruction (SPR) from cryo-electron microscopy(cryo-EM), the 3D structure of a molecule needs to be determined from its 2Dprojection images taken at unknown viewing directions. Zvi Kam showed alreadyin 1980 that the autocorrelation function of the 3D molecule over the rotationgroup SO(3) can be estimated from 2D projection images whose viewing directionsare uniformly distributed over the sphere. The autocorrelation functiondetermines the expansion coefficients of the 3D molecule in spherical harmonicsup to an orthogonal matrix of size $(2l+1)\times (2l+1)$ for each$l=0,1,2,...$. In this paper we show how techniques for solving the phaseretrieval problem in X-ray crystallography can be modified for the cryo-EMsetup for retrieving the missing orthogonal matrices. Specifically, we presenttwo new approaches that we term Orthogonal Extension and OrthogonalReplacement, in which the main algorithmic components are the singular valuedecomposition and semidefinite programming. We demonstrate the utility of theseapproaches through numerical experiments on simulated data.
arxiv-1412-0543 | Game-theoretical control with continuous action sets |  http://arxiv.org/abs/1412.0543  | author:Steven Perkins, Panayotis Mertikopoulos, David S. Leslie category:math.OC cs.GT cs.MA stat.ML published:2014-12-01 summary:Motivated by the recent applications of game-theoretical learning techniquesto the design of distributed control systems, we study a class of controlproblems that can be formulated as potential games with continuous action sets,and we propose an actor-critic reinforcement learning algorithm that provablyconverges to equilibrium in this class of problems. The method employed is toanalyse the learning process under study through a mean-field dynamical systemthat evolves in an infinite-dimensional function space (the space ofprobability distributions over the players' continuous controls). To do so, weextend the theory of finite-dimensional two-timescale stochastic approximationto an infinite-dimensional, Banach space setting, and we prove that thecontinuous dynamics of the process converge to equilibrium in the case ofpotential games. These results combine to give a provably-convergent learningalgorithm in which players do not need to keep track of the controls selectedby the other agents.
arxiv-1412-0620 | Low-Rank Approximation and Completion of Positive Tensors |  http://arxiv.org/abs/1412.0620  | author:Anil Aswani category:math.ST cs.LG stat.TH published:2014-12-01 summary:Unlike the matrix case, computing low-rank approximations of tensors isNP-hard and numerically ill-posed in general. Even the best rank-1approximation of a tensor is NP-hard. In this paper, we use convex optimizationto develop polynomial-time algorithms for low-rank approximation and completionof positive tensors. Our approach is to use algebraic topology to define a new(numerically well-posed) decomposition for positive tensors, which we show isequivalent to the standard tensor decomposition in important cases. Thoughcomputing this decomposition is a nonconvex optimization problem, we prove itcan be exactly reformulated as a convex optimization problem. This allows us toconstruct polynomial-time randomized algorithms for computing thisdecomposition and for solving low-rank tensor approximation problems. Among theconsequences is that best rank-1 approximations of positive tensors can becomputed in polynomial time. Our framework is next extended to the tensorcompletion problem, where noisy entries of a tensor are observed and then usedto estimate missing entries. We provide a polynomial-time algorithm thatrequires a polynomial (in tensor order) number of measurements, in contrast toexisting approaches that require an exponential number of measurements forspecific cases. These algorithms are extended to exploit sparsity in the tensorto reduce the number of measurements needed. We conclude by providing a novelinterpretation of statistical regression problems with categorical variables astensor completion problems, and numerical examples with synthetic data and datafrom a bioengineered metabolic network show the improved performance of ourapproach on this problem.
arxiv-1412-6145 | Study of the Influence of the Number Normalization Scheme Used in Two Chaotic Pseudo Random Number Generators Used as the Source of Randomness in Differential Evolution |  http://arxiv.org/abs/1412.6145  | author:Lenka Skanderova, Tomas Fabian category:cs.NE cs.CR published:2014-12-01 summary:In many publications, authors showed that chaotic pseudo random numbergenerators (PRNGs) may improve performance of the evolutionary algorithms. Inthis paper, we use two chaotic maps Gingerbread man and Tinkerbell as thechaotic PRNGs instead of the classical PRNG in the differential evolution.Numbers generated by this maps are normalized to the unit interval by threedifferent methods -- operation modulo, straightforward number normalizationwhere we know minimal and maximal generated number and arctangent of the twovariables $x$ and $y$, where numbers $x$ and $y$ are generated by theGingerbread man map and Tinkerbell map. The first goal of this paper is to showwhether the differential evolution convergence speed might be affected by theway how we normalize number generated by the chaotic map. The second goal is tofind out the influence of the probability distribution function of the selectedchaotic PRNGs. The results mentioned below showed that the selectednormalization method may improve differential evolution convergence speed,especially in the case of arctangent and straightforward number normalization,where we know the minimal and maximal generated numbers.
arxiv-1412-0595 | Scalability and Optimization Strategies for GPU Enhanced Neural Networks (GeNN) |  http://arxiv.org/abs/1412.0595  | author:Naresh Balaji, Esin Yavuz, Thomas Nowotny category:cs.DC cs.NE q-bio.NC published:2014-12-01 summary:Simulation of spiking neural networks has been traditionally done onhigh-performance supercomputers or large-scale clusters. Utilizing the parallelnature of neural network computation algorithms, GeNN (GPU Enhanced NeuralNetwork) provides a simulation environment that performs on General PurposeNVIDIA GPUs with a code generation based approach. GeNN allows the users todesign and simulate neural networks by specifying the populations of neurons atdifferent stages, their synapse connection densities and the model ofindividual neurons. In this report we describe work on how to scale synapticweights based on the configuration of the user-defined network to ensuresufficient spiking and subsequent effective learning. We also discussoptimization strategies particular to GPU computing: sparse representation ofsynapse connections and occupancy based block-size determination.
arxiv-1412-0436 | An Infra-Structure for Performance Estimation and Experimental Comparison of Predictive Models in R |  http://arxiv.org/abs/1412.0436  | author:Luis Torgo category:cs.MS cs.LG cs.SE stat.CO published:2014-12-01 summary:This document describes an infra-structure provided by the R packageperformanceEstimation that allows to estimate the predictive performance ofdifferent approaches (workflows) to predictive tasks. The infra-structure isgeneric in the sense that it can be used to estimate the values of anyperformance metrics, for any workflow on different predictive tasks, namely,classification, regression and time series tasks. The package also includesseveral standard workflows that allow users to easily set up their experimentslimiting the amount of work and information they need to provide. The overallgoal of the infra-structure provided by our package is to facilitate the taskof estimating the predictive performance of different modeling approaches topredictive tasks in the R environment.
arxiv-1412-0623 | Material Recognition in the Wild with the Materials in Context Database |  http://arxiv.org/abs/1412.0623  | author:Sean Bell, Paul Upchurch, Noah Snavely, Kavita Bala category:cs.CV published:2014-12-01 summary:Recognizing materials in real-world images is a challenging task. Real-worldmaterials have rich surface texture, geometry, lighting conditions, andclutter, which combine to make the problem particularly difficult. In thispaper, we introduce a new, large-scale, open dataset of materials in the wild,the Materials in Context Database (MINC), and combine this dataset with deeplearning to achieve material recognition and segmentation of images in thewild. MINC is an order of magnitude larger than previous material databases, whilebeing more diverse and well-sampled across its 23 categories. Using MINC, wetrain convolutional neural networks (CNNs) for two tasks: classifying materialsfrom patches, and simultaneous material recognition and segmentation in fullimages. For patch-based classification on MINC we found that the bestperforming CNN architectures can achieve 85.2% mean class accuracy. We convertthese trained CNN classifiers into an efficient fully convolutional frameworkcombined with a fully connected conditional random field (CRF) to predict thematerial at every pixel in an image, achieving 73.1% mean class accuracy. Ourexperiments demonstrate that having a large, well-sampled dataset such as MINCis crucial for real-world material recognition and segmentation.
arxiv-1412-0477 | Recovering Spatiotemporal Correspondence between Deformable Objects by Exploiting Consistent Foreground Motion in Video |  http://arxiv.org/abs/1412.0477  | author:Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari category:cs.CV published:2014-12-01 summary:Given unstructured videos of deformable objects, we automatically recoverspatiotemporal correspondences to map one object to another (such as animals inthe wild). While traditional methods based on appearance fail in suchchallenging conditions, we exploit consistency in object motion betweeninstances. Our approach discovers pairs of short video intervals where theobject moves in a consistent manner and uses these candidates as seeds forspatial alignment. We model the spatial correspondence between the pointtrajectories on the object in one interval to those in the other using atime-varying Thin Plate Spline deformation model. On a large dataset of tigerand horse videos, our method automatically aligns thousands of pairs of framesto a high accuracy, and outperforms the popular SIFT Flow algorithm.
arxiv-1412-0439 | Fuzzy human motion analysis: A review |  http://arxiv.org/abs/1412.0439  | author:Chern Hong Lim, Ekta Vats, Chee Seng Chan category:cs.CV cs.AI published:2014-12-01 summary:Human Motion Analysis (HMA) is currently one of the most popularly activeresearch domains as such significant research interests are motivated by anumber of real world applications such as video surveillance, sports analysis,healthcare monitoring and so on. However, most of these real world applicationsface high levels of uncertainties that can affect the operations of suchapplications. Hence, the fuzzy set theory has been applied and showed greatsuccess in the recent past. In this paper, we aim at reviewing the fuzzy setoriented approaches for HMA, individuating how the fuzzy set may improve theHMA, envisaging and delineating the future perspectives. To the best of ourknowledge, there is not found a single survey in the current literature thathas discussed and reviewed fuzzy approaches towards the HMA. For ease ofunderstanding, we conceptually classify the human motion into three broadlevels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA.
arxiv-1412-0473 | Sparse Variational Bayesian Approximations for Nonlinear Inverse Problems: applications in nonlinear elastography |  http://arxiv.org/abs/1412.0473  | author:Isabell M. Franck, P. S. Koutsourelakis category:stat.AP math.NA stat.ML published:2014-12-01 summary:This paper presents an efficient Bayesian framework for solving nonlinear,high-dimensional model calibration problems. It is based on a VariationalBayesian formulation that aims at approximating the exact posterior by means ofsolving an optimization problem over an appropriately selected family ofdistributions. The goal is two-fold. Firstly, to find lower-dimensionalrepresentations of the unknown parameter vector that capture as much aspossible of the associated posterior density, and secondly to enable thecomputation of the approximate posterior density with as few forward calls aspossible. We discuss how these objectives can be achieved by using a fullyBayesian argumentation and employing the marginal likelihood or evidence as theultimate model validation metric for any proposed dimensionality reduction. Wedemonstrate the performance of the proposed methodology for problems innonlinear elastography where the identification of the mechanical properties ofbiological materials can inform non-invasive, medical diagnosis. An ImportanceSampling scheme is finally employed in order to validate the results and assessthe efficacy of the approximations provided.
arxiv-1412-0694 | Streaming Variational Inference for Bayesian Nonparametric Mixture Models |  http://arxiv.org/abs/1412.0694  | author:Alex Tank, Nicholas J. Foti, Emily B. Fox category:stat.ML published:2014-12-01 summary:In theory, Bayesian nonparametric (BNP) models are well suited to streamingdata scenarios due to their ability to adapt model complexity with the observeddata. Unfortunately, such benefits have not been fully realized in practice;existing inference algorithms are either not applicable to streamingapplications or not extensible to BNP models. For the special case of Dirichletprocesses, streaming inference has been considered. However, there is growinginterest in more flexible BNP models building on the class of normalized randommeasures (NRMs). We work within this general framework and present a streamingvariational inference algorithm for NRM mixture models. Our algorithm is basedon assumed density filtering (ADF), leading straightforwardly to expectationpropagation (EP) for large-scale batch inference as well. We demonstrate theefficacy of the algorithm on clustering documents in large, streaming textcorpora.
arxiv-1412-0607 | How to monitor and mitigate stair-casing in l1 trend filtering |  http://arxiv.org/abs/1412.0607  | author:Cristian R. Rojas, Bo Wahlberg category:math.ST cs.SY stat.ML stat.TH published:2014-12-01 summary:In this paper we study the estimation of changing trends in time-series using$\ell_1$ trend filtering. This method generalizes 1D Total Variation (TV)denoising for detection of step changes in means to detecting changes intrends, and it relies on a convex optimization problem for which there are veryefficient numerical algorithms. It is known that TV denoising suffers from theso-called stair-case effect, which leads to detecting false change points. Theobjective of this paper is to show that $\ell_1$ trend filtering also suffersfrom a certain stair-case problem. The analysis is based on an interpretationof the dual variables of the optimization problem in the method as integratedrandom walk. We discuss consistency conditions for $\ell_1$ trend filtering,how to monitor their fulfillment, and how to modify the algorithm to avoid thestair-case false detection problem.
arxiv-1412-0680 | Fast Sublinear Sparse Representation using Shallow Tree Matching Pursuit |  http://arxiv.org/abs/1412.0680  | author:Ali Ayremlou, Thomas Goldstein, Ashok Veeraraghavan, Richard Baraniuk category:cs.CV published:2014-12-01 summary:Sparse approximations using highly over-complete dictionaries is astate-of-the-art tool for many imaging applications including denoising,super-resolution, compressive sensing, light-field analysis, and objectrecognition. Unfortunately, the applicability of such methods is severelyhampered by the computational burden of sparse approximation: these algorithmsare linear or super-linear in both the data dimensionality and size of thedictionary. We propose a framework for learning the hierarchical structure ofover-complete dictionaries that enables fast computation of sparserepresentations. Our method builds on tree-based strategies for nearestneighbor matching, and presents domain-specific enhancements that are highlyefficient for the analysis of image patches. Contrary to most popular methodsfor building spatial data structures, out methods rely on shallow, balancedtrees with relatively few layers. We show an extensive array of experiments onseveral applications such as image denoising/superresolution, compressivevideo/light-field sensing where we practically achieve 100-1000x speedup (witha less than 1dB loss in accuracy).
arxiv-1412-0696 | Understanding confounding effects in linguistic coordination: an information-theoretic approach |  http://arxiv.org/abs/1412.0696  | author:Shuyang Gao, Greg Ver Steeg, Aram Galstyan category:cs.CL cs.IT cs.SI math.IT published:2014-12-01 summary:We suggest an information-theoretic approach for measuring stylisticcoordination in dialogues. The proposed measure has a simple predictiveinterpretation and can account for various confounding factors through properconditioning. We revisit some of the previous studies that reported strongsignatures of stylistic accommodation, and find that a significant part of theobserved coordination can be attributed to a simple confounding effect - lengthcoordination. Specifically, longer utterances tend to be followed by longerresponses, which gives rise to spurious correlations in the other stylisticfeatures. We propose a test to distinguish correlations in length due tocontextual factors (topic of conversation, user verbosity, etc.) andturn-by-turn coordination. We also suggest a test to identify whether stylisticcoordination persists even after accounting for length coordination andcontextual factors.
arxiv-1412-0614 | Classification and Reconstruction of High-Dimensional Signals from Low-Dimensional Features in the Presence of Side Information |  http://arxiv.org/abs/1412.0614  | author:Francesco Renna, Liming Wang, Xin Yuan, Jianbo Yang, Galen Reeves, Robert Calderbank, Lawrence Carin, Miguel R. D. Rodrigues category:cs.IT cs.CV math.IT math.ST stat.ML stat.TH published:2014-12-01 summary:This paper offers a characterization of fundamental limits on theclassification and reconstruction of high-dimensional signals fromlow-dimensional features, in the presence of side information. We consider ascenario where a decoder has access both to linear features of the signal ofinterest and to linear features of the side information signal; while the sideinformation may be in a compressed form, the objective is recovery orclassification of the primary signal, not the side information. The signal ofinterest and the side information are each assumed to have (distinct) latentdiscrete labels; conditioned on these two labels, the signal of interest andside information are drawn from a multivariate Gaussian distribution. Withjoint probabilities on the latent labels, the overall signal-(side information)representation is defined by a Gaussian mixture model. We then provide sharpsufficient and/or necessary conditions for these quantities to approach zerowhen the covariance matrices of the Gaussians are nearly low-rank. Theseconditions, which are reminiscent of the well-known Slepian-Wolf and Wyner-Zivconditions, are a function of the number of linear features extracted from thesignal of interest, the number of linear features extracted from the sideinformation signal, and the geometry of these signals and their interplay.Moreover, on assuming that the signal of interest and the side information obeysuch an approximately low-rank model, we derive expansions of thereconstruction error as a function of the deviation from an exactly low-rankmodel; such expansions also allow identification of operational regimes wherethe impact of side information on signal reconstruction is most relevant. Ourframework, which offers a principled mechanism to integrate side information inhigh-dimensional data problems, is also tested in the context of imagingapplications.
arxiv-1412-1058 | Effective Use of Word Order for Text Categorization with Convolutional Neural Networks |  http://arxiv.org/abs/1412.1058  | author:Rie Johnson, Tong Zhang category:cs.CL cs.LG stat.ML published:2014-12-01 summary:Convolutional neural network (CNN) is a neural network that can make use ofthe internal structure of data such as the 2D structure of image data. Thispaper studies CNN on text categorization to exploit the 1D structure (namely,word order) of text data for accurate prediction. Instead of usinglow-dimensional word vectors as input as is often done, we directly apply CNNto high-dimensional text data, which leads to directly learning embedding ofsmall text regions for use in classification. In addition to a straightforwardadaptation of CNN from image to text, a simple but new variation which employsbag-of-word conversion in the convolution layer is proposed. An extension tocombine multiple convolution layers is also explored for higher accuracy. Theexperiments demonstrate the effectiveness of our approach in comparison withstate-of-the-art methods.
arxiv-1412-0296 | Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection |  http://arxiv.org/abs/1412.0296  | author:George Papandreou, Iasonas Kokkinos, Pierre-André Savalle category:cs.CV published:2014-11-30 summary:Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling'(MP) layers to extract deformation-invariant features, but we argue in favor ofa more refined treatment. First, we introduce epitomic convolution as abuilding block alternative to the common convolution-MP cascade of DCNNs; whilehaving identical complexity to MP, Epitomic Convolution allows for parametersharing across different filters, resulting in faster convergence and bettergeneralization. Second, we introduce a Multiple Instance Learning approach toexplicitly accommodate global translation and scaling when training a DCNNexclusively with class labels. For this we rely on a `patchwork' data structurethat efficiently lays out all image scales and positions as candidates to aDCNN. Factoring global and local deformations allows a DCNN to `focus itsresources' on the treatment of non-rigid deformations and yields a substantialclassification accuracy improvement. Third, further pursuing this idea, wedevelop an efficient DCNN sliding window object detector that employs explicitsearch over position, scale, and aspect ratio. We provide competitive imageclassification and localization results on the ImageNet dataset and objectdetection results on the Pascal VOC 2007 benchmark.
arxiv-1412-0265 | Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels |  http://arxiv.org/abs/1412.0265  | author:Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV published:2014-11-30 summary:In this paper, we develop an approach to exploiting kernel methods withmanifold-valued data. In many computer vision problems, the data can benaturally represented as points on a Riemannian manifold. Due to thenon-Euclidean geometry of Riemannian manifolds, usual Euclidean computer visionand machine learning algorithms yield inferior results on such data. In thispaper, we define Gaussian radial basis function (RBF)-based positive definitekernels on manifolds that permit us to embed a given manifold with acorresponding metric in a high dimensional reproducing kernel Hilbert space.These kernels make it possible to utilize algorithms developed for linearspaces on nonlinear manifold-valued data. Since the Gaussian RBF defined withany given metric is not always positive definite, we present a unifiedframework for analyzing the positive definiteness of the Gaussian RBF on ageneric metric space. We then use the proposed framework to identify positivedefinite kernels on two specific manifolds commonly encountered in computervision: the Riemannian manifold of symmetric positive definite matrices and theGrassmann manifold, i.e., the Riemannian manifold of linear subspaces of aEuclidean space. We show that many popular algorithms designed for Euclideanspaces, such as support vector machines, discriminant analysis and principalcomponent analysis can be generalized to Riemannian manifolds with the help ofsuch positive definite Gaussian kernels.
arxiv-1412-0233 | The Loss Surfaces of Multilayer Networks |  http://arxiv.org/abs/1412.0233  | author:Anna Choromanska, Mikael Henaff, Michael Mathieu, Gérard Ben Arous, Yann LeCun category:cs.LG published:2014-11-30 summary:We study the connection between the highly non-convex loss function of asimple model of the fully-connected feed-forward neural network and theHamiltonian of the spherical spin-glass model under the assumptions of: i)variable independence, ii) redundancy in network parametrization, and iii)uniformity. These assumptions enable us to explain the complexity of the fullydecoupled neural network through the prism of the results from random matrixtheory. We show that for large-size decoupled networks the lowest criticalvalues of the random loss function form a layered structure and they arelocated in a well-defined band lower-bounded by the global minimum. The numberof local minima outside that band diminishes exponentially with the size of thenetwork. We empirically verify that the mathematical model exhibits similarbehavior as the computer simulations, despite the presence of high dependenciesin real networks. We conjecture that both simulated annealing and SGD convergeto the band of low critical points, and that all critical points found thereare local minima of high quality measured by the test error. This emphasizes amajor difference between large- and small-size networks where for the latterpoor quality local minima have non-zero probability of being recovered.Finally, we prove that recovering the global minimum becomes harder as thenetwork size increases and that it is in practice irrelevant as global minimumoften leads to overfitting.
arxiv-1412-0251 | A Clearer Picture of Blind Deconvolution |  http://arxiv.org/abs/1412.0251  | author:Daniele Perrone, Paolo Favaro category:cs.CV published:2014-11-30 summary:Blind deconvolution is the problem of recovering a sharp image and a blurkernel from a noisy blurry image. Recently, there has been a significant efforton understanding the basic mechanisms to solve blind deconvolution. While thiseffort resulted in the deployment of effective algorithms, the theoreticalfindings generated contrasting views on why these approaches worked. On the onehand, one could observe experimentally that alternating energy minimizationalgorithms converge to the desired solution. On the other hand, it has beenshown that such alternating minimization algorithms should fail to converge andone should instead use a so-called Variational Bayes approach. To clarify thisconundrum, recent work showed that a good image and blur prior is instead whatmakes a blind deconvolution algorithm work. Unfortunately, this analysis didnot apply to algorithms based on total variation regularization. In thismanuscript, we provide both analysis and experiments to get a clearer pictureof blind deconvolution. Our analysis reveals the very reason why an algorithmbased on total variation works. We also introduce an implementation of thisalgorithm and show that, in spite of its extreme simplicity, it is very robustand achieves a performance comparable to the state of the art.
arxiv-1412-0307 | Seeding the Initial Population of Multi-Objective Evolutionary Algorithms: A Computational Study |  http://arxiv.org/abs/1412.0307  | author:Tobias Friedrich, Markus Wagner category:cs.NE published:2014-11-30 summary:Most experimental studies initialize the population of evolutionaryalgorithms with random genotypes. In practice, however, optimizers aretypically seeded with good candidate solutions either previously known orcreated according to some problem-specific method. This "seeding" has beenstudied extensively for single-objective problems. For multi-objectiveproblems, however, very little literature is available on the approaches toseeding and their individual benefits and disadvantages. In this article, weare trying to narrow this gap via a comprehensive computational study on commonreal-valued test functions. We investigate the effect of two seeding techniquesfor five algorithms on 48 optimization problems with 2, 3, 4, 6, and 8objectives. We observe that some functions (e.g., DTLZ4 and the LZ family)benefit significantly from seeding, while others (e.g., WFG) profit less. Theadvantage of seeding also depends on the examined algorithm.
arxiv-1412-0180 | Empirical Q-Value Iteration |  http://arxiv.org/abs/1412.0180  | author:Dileep Kalathil, Vivek S. Borkar, Rahul Jain category:math.OC cs.LG published:2014-11-30 summary:We propose a new simple and natural algorithm for learning the optimal$Q$-value function of a discounted-cost Markov Decision Process (MDP) when thetransition kernels are unknown. Unlike the classical learning algorithms forMDPs, such as $Q$-learning and `actor-critic' algorithms, this algorithmdoesn't depend on a stochastic approximation-based method. We show that ouralgorithm, which we call the empirical $Q$-value iteration (EQVI) algorithm,converges almost surely to the optimal $Q$-value function. To the best of ourknowledge, this is the first algorithm for learning in MDPs that guarantees analmost sure convergence without using stochastic approximations. We also give arate of convergence or a non-aymptotic sample complexity bound, and also showthat an asynchronous (or online) version of the algorithm will also work.Preliminary experimental results suggest a faster rate of convergence to a ballpark estimate for our algorithm compared to stochastic approximation-basedalgorithms. In fact, the asynchronous setting EQVI vastly outperforms thepopular and widely-used Q-learning algorithm.
arxiv-1412-0218 | Simple pairs of points in digital spaces. Topology-preserving transformations of digital spaces by contracting simple pairs of points |  http://arxiv.org/abs/1412.0218  | author:Alexander V. Evako category:cs.DM cs.CV published:2014-11-30 summary:Transformations of digital spaces preserving local and global topology playan important role in thinning, skeletonization and simplification of digitalimages. In the present paper, we introduce and study contractions of simplepair of points based on the notions of a digital contractible space andcontractible transformations of digital spaces. We show that the contraction ofa simple pair of points preserves local and global topology of a digital space.Relying on the obtained results, we study properties if digital manifolds. Inparticular, we show that a digital n-manifold can be transformed to itscompressed form with the minimal number of points by sequential contractions ofsimple pairs. Key Words: Graph, digital space, contraction, splitting, simple pair,homotopy, thinning
arxiv-1412-0100 | Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images |  http://arxiv.org/abs/1412.0100  | author:Stefan Mathe, Cristian Sminchisescu category:cs.CV cs.LG published:2014-11-29 summary:State-of-the-art visual recognition and detection systems increasingly relyon large amounts of training data and complex classifiers. Therefore it becomesincreasingly expensive both to manually annotate datasets and to keep runningtimes at levels acceptable for practical applications. In this paper, wepropose two solutions to address these issues. First, we introduce a weaklysupervised, segmentation-based approach to learn accurate detectors and imageclassifiers from weak supervisory signals that provide only approximateconstraints on target localization. We illustrate our system on the problem ofaction detection in static images (Pascal VOC Actions 2012), using human visualsearch patterns as our training signal. Second, inspired from thesaccade-and-fixate operating principle of the human visual system, we usereinforcement learning techniques to train efficient search models fordetection. Our sequential method is weakly supervised and general (it does notrequire eye movements), finds optimal search strategies for any given detectionconfidence function and achieves performance similar to exhaustive slidingwindow search at a fraction of its computational cost.
arxiv-1412-0060 | Egocentric Pose Recognition in Four Lines of Code |  http://arxiv.org/abs/1412.0060  | author:Gregory Rogez, James S. Supancic III, Deva Ramanan category:cs.CV published:2014-11-29 summary:We tackle the problem of estimating the 3D pose of an individual's upperlimbs (arms+hands) from a chest mounted depth-camera. Importantly, we considerpose estimation during everyday interactions with objects. Past work shows thatstrong pose+viewpoint priors and depth-based features are crucial for robustperformance. In egocentric views, hands and arms are observable within a welldefined volume in front of the camera. We call this volume an egocentricworkspace. A notable property is that hand appearance correlates with workspacelocation. To exploit this correlation, we classify arm+hand configurations in aglobal egocentric coordinate frame, rather than a local scanning window. Thisgreatly simplify the architecture and improves performance. We propose anefficient pipeline which 1) generates synthetic workspace exemplars fortraining using a virtual chest-mounted camera whose intrinsic parameters matchour physical camera, 2) computes perspective-aware depth features on thisentire volume and 3) recognizes discrete arm+hand pose classes through a sparsemulti-class SVM. Our method provides state-of-the-art hand pose recognitionperformance from egocentric RGB-D images in real-time.
arxiv-1412-0069 | Pedestrian Detection aided by Deep Learning Semantic Tasks |  http://arxiv.org/abs/1412.0069  | author:Yonglong Tian, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-11-29 summary:Deep learning methods have achieved great success in pedestrian detection,owing to its ability to learn features from raw pixels. However, they mainlycapture middle-level representations, such as pose of pedestrian, but confusepositive with hard negative samples, which have large ambiguity, e.g. the shapeand appearance of `tree trunk' or `wire pole' are similar to pedestrian incertain viewpoint. This ambiguity can be distinguished by high-levelrepresentation. To this end, this work jointly optimizes pedestrian detectionwith semantic tasks, including pedestrian attributes (e.g. `carrying backpack')and scene attributes (e.g. `road', `tree', and `horizontal'). Rather thanexpensively annotating scene attributes, we transfer attributes informationfrom existing scene segmentation datasets to the pedestrian dataset, byproposing a novel deep model to learn high-level features from multiple tasksand multiple data sources. Since distinct tasks have distinct convergence ratesand data from different datasets have different distributions, a multi-taskobjective function is carefully designed to coordinate tasks and reducediscrepancies among datasets. The importance coefficients of tasks and networkparameters in this objective function can be iteratively estimated. Extensiveevaluations show that the proposed approach outperforms the state-of-the-art onthe challenging Caltech and ETH datasets, where it reduces the miss rates ofprevious deep models by 17 and 5.5 percent, respectively.
arxiv-1412-0062 | A Bayesian Framework for Sparse Representation-Based 3D Human Pose Estimation |  http://arxiv.org/abs/1412.0062  | author:Behnam Babagholami-Mohamadabadi, Amin Jourabloo, Ali Zarghami, Shohreh Kasaei category:cs.CV published:2014-11-29 summary:A Bayesian framework for 3D human pose estimation from monocular images basedon sparse representation (SR) is introduced. Our probabilistic approach aims atsimultaneously learning two overcomplete dictionaries (one for the visual inputspace and the other for the pose space) with a shared sparse representation.Existing SR-based pose estimation approaches only offer a point estimation ofthe dictionary and the sparse codes. Therefore, they might be unreliable whenthe number of training examples is small. Our Bayesian framework estimates aposterior distribution for the sparse codes and the dictionaries from labeledtraining data. Hence, it is robust to overfitting on small-size training data.Experimental results on various human activities show that the proposed methodis superior to the state of-the-art pose estimation algorithms.
arxiv-1412-0156 | Constant Step Size Least-Mean-Square: Bias-Variance Trade-offs and Optimal Sampling Distributions |  http://arxiv.org/abs/1412.0156  | author:Alexandre Défossez, Francis Bach category:cs.LG math.OC stat.ML published:2014-11-29 summary:We consider the least-squares regression problem and provide a detailedasymptotic analysis of the performance of averaged constant-step-sizestochastic gradient descent (a.k.a. least-mean-squares). In the strongly-convexcase, we provide an asymptotic expansion up to explicit exponentially decayingterms. Our analysis leads to new insights into stochastic approximationalgorithms: (a) it gives a tighter bound on the allowed step-size; (b) thegeneralization error may be divided into a variance term which is decaying asO(1/n), independently of the step-size $\gamma$, and a bias term that decays asO(1/$\gamma$ 2 n 2); (c) when allowing non-uniform sampling, the choice of agood sampling density depends on whether the variance or bias terms dominate.In particular, when the variance term dominates, optimal sampling densities donot lead to much gain, while when the bias term dominates, we can choose largerstep-sizes that leads to significant improvements.
arxiv-1412-0111 | Color image quality assessment measure using multivariate generalized Gaussian distribution |  http://arxiv.org/abs/1412.0111  | author:Mounir Omari, Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi category:cs.CV published:2014-11-29 summary:This paper deals with color image quality assessment in the reduced-referenceframework based on natural scenes statistics. In this context, we propose tomodel the statistics of the steerable pyramid coefficients by a MultivariateGeneralized Gaussian distribution (MGGD). This model allows taking into accountthe high correlation between the components of the RGB color space. For eachselected scale and orientation, we extract a parameter matrix from the threecolor components subbands. In order to quantify the visual degradation, we usea closed-form of Kullback-Leibler Divergence (KLD) between two MGGDs. Using"TID 2008" benchmark, the proposed measure has been compared with the mostinfluential methods according to the FRTV1 VQEG framework. Results demonstratesits effectiveness for a great variety of distortion type. Among other benefitsthis measure uses only very little information about the original image.
arxiv-1412-0065 | 3D Hand Pose Detection in Egocentric RGB-D Images |  http://arxiv.org/abs/1412.0065  | author:Gregory Rogez, James S. Supancic III, Maryam Khademi, Jose Maria Martinez Montiel, Deva Ramanan category:cs.CV published:2014-11-29 summary:We focus on the task of everyday hand pose estimation from egocentricviewpoints. For this task, we show that depth sensors are particularlyinformative for extracting near-field interactions of the camera wearer withhis/her environment. Despite the recent advances in full-body pose estimationusing Kinect-like sensors, reliable monocular hand pose estimation in RGB-Dimages is still an unsolved problem. The problem is considerably exacerbatedwhen analyzing hands performing daily activities from a first-person viewpoint,due to severe occlusions arising from object manipulations and a limitedfield-of-view. Our system addresses these difficulties by exploiting strongpriors over viewpoint and pose in a discriminative tracking-by-detectionframework. Our priors are operationalized through a photorealistic syntheticmodel of egocentric scenes, which is used to generate training data forlearning depth-based pose classifiers. We evaluate our approach on an annotateddataset of real egocentric object manipulation scenes and compare to bothcommercial and academic approaches. Our method provides state-of-the-artperformance for both hand detection and pose estimation in egocentric RGB-Dimages.
arxiv-1412-0650 | A review of "Mem-computing NP-complete problems in polynomial time using polynomial resources" (arXiv:1411.4798) |  http://arxiv.org/abs/1412.0650  | author:Igor L. Markov category:cs.ET cs.NE published:2014-11-29 summary:The reviewed paper describes an analog device that empirically solves smallinstances of the NP-complete Subset Sum Problem (SSP). The authors claim thatthis device can solve the SSP in polynomial time using polynomial space, inprinciple, and observe no exponential scaling in resource requirements. Wepoint out that (a) the properties ascribed by the authors to their device areinsufficient to solve NP-complete problems in poly-time, (b) runtime analysisoffered does not cover the spectral measurement step, (c) the overall techniquerequires exponentially increasing resources when scaled up because of thespectral measurement step.
arxiv-1412-0165 | Robust Camera Location Estimation by Convex Programming |  http://arxiv.org/abs/1412.0165  | author:Onur Ozyesil, Amit Singer category:cs.CV published:2014-11-29 summary:$3$D structure recovery from a collection of $2$D images requires theestimation of the camera locations and orientations, i.e. the camera motion.For large, irregular collections of images, existing methods for the locationestimation part, which can be formulated as the inverse problem of estimating$n$ locations $\mathbf{t}_1, \mathbf{t}_2, \ldots, \mathbf{t}_n$ in$\mathbb{R}^3$ from noisy measurements of a subset of the pairwise directions$\frac{\mathbf{t}_i - \mathbf{t}_j}{\\mathbf{t}_i - \mathbf{t}_j\}$, aresensitive to outliers in direction measurements. In this paper, we firstlyprovide a complete characterization of well-posed instances of the locationestimation problem, by presenting its relation to the existing theory ofparallel rigidity. For robust estimation of camera locations, we introduce atwo-step approach, comprised of a pairwise direction estimation method robustto outliers in point correspondences between image pairs, and a convex programto maintain robustness to outlier directions. In the presence of partiallycorrupted measurements, we empirically demonstrate that our convex formulationcan even recover the locations exactly. Lastly, we demonstrate the utility ofour formulations through experiments on Internet photo collections.
arxiv-1411-7942 | Using Sentence Plausibility to Learn the Semantics of Transitive Verbs |  http://arxiv.org/abs/1411.7942  | author:Tamara Polajnar, Laura Rimell, Stephen Clark category:cs.CL published:2014-11-28 summary:The functional approach to compositional distributional semantics considerstransitive verbs to be linear maps that transform the distributional vectorsrepresenting nouns into a vector representing a sentence. We conduct an initialinvestigation that uses a matrix consisting of the parameters of a logisticregression classifier trained on a plausibility task as a transitive verbfunction. We compare our method to a commonly used corpus-based method forconstructing a verb matrix and find that the plausibility training may be moreeffective for disambiguation tasks.
arxiv-1411-7855 | V-variable image compression |  http://arxiv.org/abs/1411.7855  | author:Franklin Mendivil, Örjan Stenflo category:cs.CV published:2014-11-28 summary:V-variable fractals, where $V$ is a positive integer, are intuitivelyfractals with at most $V$ different "forms" or "shapes" at all levels ofmagnification. In this paper we describe how V-variable fractals can be usedfor the purpose of image compression.
arxiv-1411-7864 | Efficient inference of overlapping communities in complex networks |  http://arxiv.org/abs/1411.7864  | author:Bjarne Ørum Fruergaard, Tue Herlau category:stat.ML cs.SI physics.soc-ph published:2014-11-28 summary:We discuss two views on extending existing methods for complex networkmodeling which we dub the communities first and the networks first view,respectively. Inspired by the networks first view that we attribute to White,Boorman, and Breiger (1976)[1], we formulate the multiple-networks stochasticblockmodel (MNSBM), which seeks to separate the observed network intosubnetworks of different types and where the problem of inferring structure ineach subnetwork becomes easier. We show how this model is specified in agenerative Bayesian framework where parameters can be inferred efficientlyusing Gibbs sampling. The result is an effective multiple-membership modelwithout the drawbacks of introducing complex definitions of "groups" and howthey interact. We demonstrate results on the recovery of planted structure insynthetic networks and show very encouraging results on link predictionperformances using multiple-networks models on a number of real-world networkdata sets.
arxiv-1411-7911 | On Rendering Synthetic Images for Training an Object Detector |  http://arxiv.org/abs/1411.7911  | author:Artem Rozantsev, Vincent Lepetit, Pascal Fua category:cs.CV published:2014-11-28 summary:We propose a novel approach to synthesizing images that are effective fortraining object detectors. Starting from a small set of real images, ouralgorithm estimates the rendering parameters required to synthesize similarimages given a coarse 3D model of the target object. These parameters can thenbe reused to generate an unlimited number of training images of the object ofinterest in arbitrary 3D poses, which can then be used to increaseclassification performances. A key insight of our approach is that the synthetically generated imagesshould be similar to real images, not in terms of image quality, but rather interms of features used during the detector training. We show in the context ofdrone, plane, and car detection that using such synthetically generated imagesyields significantly better performances than simply perturbing real images oreven synthesizing images in such way that they look very realistic, as is oftendone when only limited amounts of training data are available.
arxiv-1411-7820 | Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic Models and Encyclopedic Knowledge |  http://arxiv.org/abs/1411.7820  | author:Vivi Nastase, Angela Fahrni category:cs.CL published:2014-11-28 summary:We present a method for coarse-grained cross-lingual alignment of comparabletexts: segments consisting of contiguous paragraphs that discuss the same theme(e.g. history, economy) are aligned based on induced multilingual topics. Themethod combines three ideas: a two-level LDA model that filters out words thatdo not convey themes, an HMM that models the ordering of themes in thecollection of documents, and language-independent concept annotations to serveas a cross-language bridge and to strengthen the connection between paragraphsin the same segment through concept relations. The method is evaluated onEnglish and French data previously used for monolingual alignment. The resultsshow state-of-the-art performance in both monolingual and cross-lingualsettings.
arxiv-1411-7923 | Learning Face Representation from Scratch |  http://arxiv.org/abs/1411.7923  | author:Dong Yi, Zhen Lei, Shengcai Liao, Stan Z. Li category:cs.CV published:2014-11-28 summary:Pushing by big data and deep convolutional neural network (CNN), theperformance of face recognition is becoming comparable to human. Using privatelarge scale training datasets, several groups achieve very high performance onLFW, i.e., 97% to 99%. While there are many open source implementations of CNN,none of large scale face dataset is publicly available. The current situationin the field of face recognition is that data is more important than algorithm.To solve this problem, this paper proposes a semi-automatical way to collectface images from Internet and builds a large scale dataset containing about10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database,we use a 11-layer CNN to learn discriminative representation and obtainstate-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace willattract more research groups entering this field and accelerate the developmentof face recognition in the wild.
arxiv-1411-7817 | Learning with Algebraic Invariances, and the Invariant Kernel Trick |  http://arxiv.org/abs/1411.7817  | author:Franz J. Király, Andreas Ziehe, Klaus-Robert Müller category:stat.ML cs.LG math.ST stat.TH published:2014-11-28 summary:When solving data analysis problems it is important to integrate priorknowledge and/or structural invariances. This paper contributes by a novelframework for incorporating algebraic invariance structure into kernels. Inparticular, we show that algebraic properties such as sign symmetries in data,phase independence, scaling etc. can be included easily by essentiallyperforming the kernel trick twice. We demonstrate the usefulness of our theoryin simulations on selected applications such as sign-invariant spectralclustering and underdetermined ICA.
arxiv-1411-7924 | Predicting clicks in online display advertising with latent features and side-information |  http://arxiv.org/abs/1411.7924  | author:Bjarne Ørum Fruergaard category:stat.ML cs.LG stat.AP published:2014-11-28 summary:We review a method for click-through rate prediction based on the work ofMenon et al. [11], which combines collaborative filtering and matrixfactorization with a side-information model and fuses the outputs to properprobabilities in [0,1]. In addition we provide details, both for the modelingas well as the experimental part, that are not found elsewhere. We rigorouslytest the performance on several test data sets from consecutive days in aclick-through rate prediction setup, in a manner which reflects a real-worldpipeline. Our results confirm that performance can be increased using latentfeatures, albeit the differences in the measures are small but significant.
arxiv-1411-7964 | Effective Face Frontalization in Unconstrained Images |  http://arxiv.org/abs/1411.7964  | author:Tal Hassner, Shai Harel, Eran Paz, Roee Enbar category:cs.CV published:2014-11-28 summary:"Frontalization" is the process of synthesizing frontal facing views of facesappearing in single unconstrained photos. Recent reports have suggested thatthis process may substantially boost the performance of face recognitionsystems. This, by transforming the challenging problem of recognizing facesviewed from unconstrained viewpoints to the easier problem of recognizing facesin constrained, forward facing poses. Previous frontalization methods did thisby attempting to approximate 3D facial shapes for each query image. We observethat 3D face shape estimation from unconstrained photos may be a harder problemthan frontalization and can potentially introduce facial misalignments.Instead, we explore the simpler approach of using a single, unmodified, 3Dsurface as an approximation to the shape of all input faces. We show that thisleads to a straightforward, efficient and easy to implement method forfrontalization. More importantly, it produces aesthetic new frontal views andis surprisingly effective when used for face recognition and gender estimation.
arxiv-1411-7806 | Two Gaussian Approaches to Black-Box Optomization |  http://arxiv.org/abs/1411.7806  | author:Lukáš Bajer, Martin Holeňa category:cs.NE cs.AI published:2014-11-28 summary:Outline of several strategies for using Gaussian processes as surrogatemodels for the covariance matrix adaptation evolution strategy (CMA-ES).
arxiv-1411-7783 | From neural PCA to deep unsupervised learning |  http://arxiv.org/abs/1411.7783  | author:Harri Valpola category:stat.ML cs.LG cs.NE published:2014-11-28 summary:A network supporting deep unsupervised learning is presented. The network isan autoencoder with lateral shortcut connections from the encoder to decoder ateach level of the hierarchy. The lateral shortcut connections allow the higherlevels of the hierarchy to focus on abstract invariant features. While standardautoencoders are analogous to latent variable models with a single layer ofstochastic variables, the proposed network is analogous to hierarchical latentvariables models. Learning combines denoising autoencoder and denoising sourcesseparation frameworks. Each layer of the network contributes to the costfunction a term which measures the distance of the representations produced bythe encoder and the decoder. Since training signals originate from all levelsof the network, all layers can learn efficiently even in deep networks. Thespeedup offered by cost terms from higher levels of the hierarchy and theability to learn invariant features are demonstrated in experiments.
arxiv-1411-7973 | Bus Travel Time Predictions Using Additive Models |  http://arxiv.org/abs/1411.7973  | author:Matthias Kormaksson, Luciano Barbosa, Marcos R. Vieira, Bianca Zadrozny category:cs.LG stat.AP published:2014-11-28 summary:Many factors can affect the predictability of public bus services such astraffic, weather and local events. Other aspects, such as day of week or hourof day, may influence bus travel times as well, either directly or inconjunction with other variables. However, the exact nature of suchrelationships between travel times and predictor variables is, in mostsituations, not known. In this paper we develop a framework that allows forflexible modeling of bus travel times through the use of Additive Models. Inparticular, we model travel times as a sum of linear as well as nonlinear termsthat are modeled as smooth functions of predictor variables. The proposed classof models provides a principled statistical framework that is highly flexiblein terms of model building. The experimental results demonstrate uniformlysuperior performance of our best model as compared to previous predictionmethods when applied to a very large GPS data set obtained from buses operatingin the city of Rio de Janeiro.
arxiv-1411-7798 | Cross-Modal Learning via Pairwise Constraints |  http://arxiv.org/abs/1411.7798  | author:Ran He, Man Zhang, Liang Wang, Ye Ji, Qiyue Yin category:cs.CV published:2014-11-28 summary:In multimedia applications, the text and image components in a web documentform a pairwise constraint that potentially indicates the same semanticconcept. This paper studies cross-modal learning via the pairwise constraint,and aims to find the common structure hidden in different modalities. We firstpropose a compound regularization framework to deal with the pairwiseconstraint, which can be used as a general platform for developing cross-modalalgorithms. For unsupervised learning, we propose a cross-modal subspaceclustering method to learn a common structure for different modalities. Forsupervised learning, to reduce the semantic gap and the outliers in pairwiseconstraints, we propose a cross-modal matching method based on compound ?21regularization along with an iteratively reweighted algorithm to find theglobal optimum. Extensive experiments demonstrate the benefits of joint textand image modeling with semantically induced pairwise constraints, and showthat the proposed cross-modal methods can further reduce the semantic gapbetween different modalities and improve the clustering/retrieval accuracy.
arxiv-1411-8003 | Guaranteed Matrix Completion via Non-convex Factorization |  http://arxiv.org/abs/1411.8003  | author:Ruoyu Sun, Zhi-Quan Luo category:cs.LG published:2014-11-28 summary:Matrix factorization is a popular approach for large-scale matrix completion.In this approach, the unknown low-rank matrix is expressed as the product oftwo much smaller matrices so that the low-rank property is automaticallyfulfilled. The resulting optimization problem, even with huge size, can besolved (to stationary points) very efficiently through standard optimizationalgorithms such as alternating minimization and stochastic gradient descent(SGD). However, due to the non-convexity caused by the factorization model,there is a limited theoretical understanding of whether these algorithms willgenerate a good solution. In this paper, we establish a theoretical guaranteefor the factorization based formulation to correctly recover the underlyinglow-rank matrix. In particular, we show that under similar conditions to thosein previous works, many standard optimization algorithms converge to the globaloptima of a factorization based formulation, and recover the true low-rankmatrix. A major difference of our work from the existing results is that we donot need resampling (i.e., using independent samples at each iteration) ineither the algorithm or its analysis. To the best of our knowledge, our resultis the first one that provides exact recovery guarantee for many standardalgorithms such as gradient descent, SGD and block coordinate gradient descent.
arxiv-1411-7766 | Deep Learning Face Attributes in the Wild |  http://arxiv.org/abs/1411.7766  | author:Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-11-28 summary:Predicting face attributes in the wild is challenging due to complex facevariations. We propose a novel deep learning framework for attribute predictionin the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointlywith attribute tags, but pre-trained differently. LNet is pre-trained bymassive general object categories for face localization, while ANet ispre-trained by massive face identities for attribute prediction. This frameworknot only outperforms the state-of-the-art with a large margin, but also revealsvaluable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attributeprediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only withimage-level attribute tags, their response maps over entire images have strongindication of face locations. This fact enables training LNet for facelocalization with only image-level annotations, but without face bounding boxesor landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANetautomatically discover semantic concepts after pre-training with massive faceidentities, and such concepts are significantly enriched after fine-tuning withattribute tags. Each attribute can be well explained with a sparse linearcombination of these concepts.
arxiv-1411-7883 | Articulated motion discovery using pairs of trajectories |  http://arxiv.org/abs/1411.7883  | author:Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari category:cs.CV published:2014-11-28 summary:We propose an unsupervised approach for discovering characteristic motionpatterns in videos of highly articulated objects performing natural, unscriptedbehaviors, such as tigers in the wild. We discover consistent patterns in abottom-up manner by analyzing the relative displacements of large numbers ofordered trajectory pairs through time, such that each trajectory is attached toa different moving part on the object. The pairs of trajectories descriptorrelies entirely on motion and is more discriminative than state-of-the-artfeatures that employ single trajectories. Our method generates temporal videointervals, each automatically trimmed to one instance of the discoveredbehavior, and clusters them by type (e.g., running, turning head, drinkingwater). We present experiments on two datasets: dogs from YouTube-Objects and anew dataset of National Geographic tiger videos. Results confirm that ourproposed descriptor outperforms existing appearance- and trajectory-baseddescriptors (e.g., HOG and DTFs) on both datasets and enables us to segmentunconstrained animal video into intervals containing single behaviors.
arxiv-1411-7445 | Bi-objective Optimization for Robust RGB-D Visual Odometry |  http://arxiv.org/abs/1411.7445  | author:Tao Han, Chao Xu, Ryan Loxton, Lei Xie category:cs.RO cs.CV published:2014-11-27 summary:This paper considers a new bi-objective optimization formulation for robustRGB-D visual odometry. We investigate two methods for solving the proposedbi-objective optimization problem: the weighted sum method (in which theobjective functions are combined into a single objective function) and thebounded objective method (in which one of the objective functions is optimizedand the value of the other objective function is bounded via a constraint). Ourexperimental results for the open source TUM RGB-D dataset show that the newbi-objective optimization formulation is superior to several existing RGB-Dodometry methods. In particular, the new formulation yields more accuratemotion estimates and is more robust when textural or structural features in theimage sequence are lacking.
arxiv-1411-7612 | A Parallel Genetic Algorithm for Generalized Vertex Cover Problem |  http://arxiv.org/abs/1411.7612  | author:Drona Pratap Chandu category:cs.DC cs.NE published:2014-11-27 summary:This paper presents a parallel genetic algorithm for generalised vertex coverproblem (GVCP) using Hadoop Map-Reduce framework. The proposed Map-Reduceimplementation helps to run the genetic algorithm for generalized vertex coverproblem (GVCP) on multiple machines parallely and computes the solution inrelatively short time.
arxiv-1411-7655 | A statistical reduced-reference method for color image quality assessment |  http://arxiv.org/abs/1411.7655  | author:Mounir Omari, Mohammed El Hassouni, Abdelkaher Ait Abdelouahad, Hocine Cherifi category:cs.CV published:2014-11-27 summary:Although color is a fundamental feature of human visual perception, it hasbeen largely unexplored in the reduced-reference (RR) image quality assessment(IQA) schemes. In this paper, we propose a natural scene statistic (NSS)method, which efficiently uses this information. It is based on the statisticaldeviation between the steerable pyramid coefficients of the reference colorimage and the degraded one. We propose and analyze the multivariate generalizedGaussian distribution (MGGD) to model the underlying statistics. In order toquantify the degradation, we develop and evaluate two measures basedrespectively on the Geodesic distance between two MGGDs and on the closed-formof the Kullback Leibler divergence. We performed an extensive evaluation ofboth metrics in various color spaces (RGB, HSV, CIELAB and YCrCb) using the TID2008 benchmark and the FRTV Phase I validation process. Experimental resultsdemonstrate the effectiveness of the proposed framework to achieve a goodconsistency with human visual perception. Furthermore, the best configurationis obtained with CIELAB color space associated to KLD deviation measure.
arxiv-1411-7441 | Pattern Decomposition with Complex Combinatorial Constraints: Application to Materials Discovery |  http://arxiv.org/abs/1411.7441  | author:Stefano Ermon, Ronan Le Bras, Santosh K. Suram, John M. Gregoire, Carla Gomes, Bart Selman, Robert B. van Dover category:cs.AI cs.LG stat.ML published:2014-11-27 summary:Identifying important components or factors in large amounts of noisy data isa key problem in machine learning and data mining. Motivated by a patterndecomposition problem in materials discovery, aimed at discovering newmaterials for renewable energy, e.g. for fuel and solar cells, we introduceCombiFD, a framework for factor based pattern decomposition that allows theincorporation of a-priori knowledge as constraints, including complexcombinatorial constraints. In addition, we propose a new pattern decompositionalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)quadratic programs. Our approach considerably outperforms the state of the arton the materials discovery problem, scaling to larger datasets and recoveringmore precise and physically meaningful decompositions. We also show theeffectiveness of our approach for enforcing background knowledge on otherapplication domains.
arxiv-1411-7682 | On color image quality assessment using natural image statistics |  http://arxiv.org/abs/1411.7682  | author:Mounir Omari, Mohammed El Hassouni, Hocine Cherifi, Abdelkaher Ait Abdelouahad category:cs.CV published:2014-11-27 summary:Color distortion can introduce a significant damage in visual qualityperception, however, most of existing reduced-reference quality measures aredesigned for grayscale images. In this paper, we consider a basic extension ofwell-known image-statistics based quality assessment measures to color images.In order to evaluate the impact of color information on the measuresefficiency, two color spaces are investigated: RGB and CIELAB. Results of anextensive evaluation using TID 2013 benchmark demonstrates that significantimprovement can be achieved for a great number of distortion type when theCIELAB color representation is used.
arxiv-1411-7706 | A Nonparametric Bayesian Approach to Uncovering Rat Hippocampal Population Codes During Spatial Navigation |  http://arxiv.org/abs/1411.7706  | author:Scott W. Linderman, Matthew J. Johnson, Matthew A. Wilson, Zhe Chen category:stat.ML q-bio.NC published:2014-11-27 summary:Rodent hippocampal population codes represent important spatial informationabout the environment during navigation. Several computational methods havebeen developed to uncover the neural representation of spatial topologyembedded in rodent hippocampal ensemble spike activity. Here we extend ourprevious work and propose a nonparametric Bayesian approach to infer rathippocampal population codes during spatial navigation. To tackle the modelselection problem, we leverage a nonparametric Bayesian model. Specifically, toanalyze rat hippocampal ensemble spiking activity, we apply a hierarchicalDirichlet process-hidden Markov model (HDP-HMM) using two Bayesian inferencemethods, one based on Markov chain Monte Carlo (MCMC) and the other based onvariational Bayes (VB). We demonstrate the effectiveness of our Bayesianapproaches on recordings from a freely-behaving rat navigating in an open fieldenvironment. We find that MCMC-based inference with Hamiltonian Monte Carlo(HMC) hyperparameter sampling is flexible and efficient, and outperforms VB andMCMC approaches with hyperparameters set by empirical Bayes.
arxiv-1411-7714 | Features in Concert: Discriminative Feature Selection meets Unsupervised Clustering |  http://arxiv.org/abs/1411.7714  | author:Marius Leordeanu, Alexandra Radu, Rahul Sukthankar category:cs.CV published:2014-11-27 summary:Feature selection is an essential problem in computer vision, important forcategory learning and recognition. Along with the rapid development of a widevariety of visual features and classifiers, there is a growing need forefficient feature selection and combination methods, to construct powerfulclassifiers for more complex and higher-level recognition tasks. We propose analgorithm that efficiently discovers sparse, compact representations of inputfeatures or classifiers, from a vast sea of candidates, with importantoptimality properties, low computational cost and excellent accuracy inpractice. Different from boosting, we start with a discriminant linearclassification formulation that encourages sparse solutions. Then we obtain anequivalent unsupervised clustering problem that jointly discovers ensembles ofdiverse features. They are independently valuable but even more powerful whenunited in a cluster of classifiers. We evaluate our method on the task oflarge-scale recognition in video and show that it significantly outperformsclassical selection approaches, such as AdaBoost and greedy forward-backwardselection, and powerful classifiers such as SVMs, in speed of training andperformance, especially in the case of limited training data.
arxiv-1411-7432 | Metrics for Probabilistic Geometries |  http://arxiv.org/abs/1411.7432  | author:Alessandra Tosi, Søren Hauberg, Alfredo Vellido, Neil D. Lawrence category:stat.ML cs.LG published:2014-11-27 summary:We investigate the geometrical structure of probabilistic generativedimensionality reduction models using the tools of Riemannian geometry. Weexplicitly define a distribution over the natural metric given by the models.We provide the necessary algorithms to compute expected metric tensors wherethe distribution over mappings is given by a Gaussian process. We treat thecorresponding latent variable model as a Riemannian manifold and we use theexpectation of the metric under the Gaussian process prior to defineinterpolating paths and measure distance between latent points. We show howdistances that respect the expected metric lead to more appropriate generationof new data.
arxiv-1411-7715 | Flying Objects Detection from a Single Moving Camera |  http://arxiv.org/abs/1411.7715  | author:Artem Rozantsev, Vincent Lepetit, Pascal Fua category:cs.CV published:2014-11-27 summary:We propose an approach to detect flying objects such as UAVs and aircraftswhen they occupy a small portion of the field of view, possibly moving againstcomplex backgrounds, and are filmed by a camera that itself moves. Solving such a difficult problem requires combining both appearance andmotion cues. To this end we propose a regression-based approach to motionstabilization of local image patches that allows us to achieve effectiveclassification on spatio-temporal image cubes and outperform state-of-the-arttechniques. As the problem is relatively new, we collected two challenging datasets forUAVs and Aircrafts, which can be used as benchmarks for flying objectsdetection and vision-guided collision avoidance.
arxiv-1411-7542 | Scalability of using Restricted Boltzmann Machines for Combinatorial Optimization |  http://arxiv.org/abs/1411.7542  | author:Malte Probst, Franz Rothlauf, Jörn Grahl category:cs.NE I.2.6; I.2.8 published:2014-11-27 summary:Estimation of Distribution Algorithms (EDAs) require flexible probabilitymodels that can be efficiently learned and sampled. Restricted BoltzmannMachines (RBMs) are generative neural networks with these desired properties.We integrate an RBM into an EDA and evaluate the performance of this system insolving combinatorial optimization problems with a single objective. We assesshow the number of fitness evaluations and the CPU time scale with problem sizeand with problem complexity. The results are compared to the BayesianOptimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requireslarger population sizes and a larger number of fitness evaluations, itoutperforms BOA in terms of CPU times, in particular if the problem is large orcomplex. RBM-EDA requires less time for model building than BOA. These resultshighlight the potential of using generative neural networks for combinatorialoptimization.
arxiv-1411-7718 | Classification with Noisy Labels by Importance Reweighting |  http://arxiv.org/abs/1411.7718  | author:Tongliang Liu, Dacheng Tao category:stat.ML cs.LG published:2014-11-27 summary:In this paper, we study a classification problem in which sample labels arerandomly corrupted. In this scenario, there is an unobservable sample withnoise-free labels. However, before being observed, the true labels areindependently flipped with a probability $\rho\in[0,0.5)$, and the random labelnoise can be class-conditional. Here, we address two fundamental problemsraised by this scenario. The first is how to best use the abundant surrogateloss functions designed for the traditional classification problem when thereis label noise. We prove that any surrogate loss function can be used forclassification with noisy labels by using importance reweighting, withconsistency assurance that the label noise does not ultimately hinder thesearch for the optimal classifier of the noise-free sample. The other is theopen problem of how to obtain the noise rate $\rho$. We show that the rate isupper bounded by the conditional probability $P(yx)$ of the noisy sample.Consequently, the rate can be estimated, because the upper bound can be easilyreached in classification problems. Experimental results on synthetic and realdatasets confirm the efficiency of our methods.
arxiv-1411-7508 | Forecasting the Colorado River Discharge Using an Artificial Neural Network (ANN) Approach |  http://arxiv.org/abs/1411.7508  | author:Amirhossein Mehrkesh, Maryam Ahmadi category:stat.ML physics.soc-ph published:2014-11-27 summary:Artificial Neural Network (ANN) based model is a computational approachcommonly used for modeling the complex relationships between input and outputparameters. Prediction of the flow rate of a river is a requisite for anysuccessful water resource management and river basin planning. In the currentsurvey, the effectiveness of an Artificial Neural Network was examined topredict the Colorado River discharge. In this modeling process, an ANN modelwas used to relate the discharge of the Colorado River to such parameters asthe amount of precipitation, ambient temperature and snowpack level at aspecific time of the year. The model was able to precisely study the impact ofclimatic parameters on the flow rate of the Colorado River.
arxiv-1411-7466 | The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification |  http://arxiv.org/abs/1411.7466  | author:Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2014-11-27 summary:A number of recent studies have shown that a Deep Convolutional NeuralNetwork (DCNN) pretrained on a large dataset can be adopted as a universalimage description which leads to astounding performance in many visualclassification tasks. Most of these studies, if not all, adopt activations ofthe fully-connected layer of a DCNN as the image or region representation andit is believed that convolutional layer activations are less discriminative. This paper, however, advocates that if used appropriately convolutional layeractivations can be turned into a powerful image representation which enjoysmany advantages over fully-connected layer activations. This is achieved byadopting a new technique proposed in this paper calledcross-convolutional-layer pooling. More specifically, it extracts subarrays offeature maps of one convolutional layer as local features and pools theextracted features with the guidance of feature maps of the successiveconvolutional layer. Compared with exising methods that apply DCNNs in thelocal feature setting, the proposed method is significantly faster since itrequires much fewer times of DCNN forward computation. Moreover, it avoids thedomain mismatch issue which is usually encountered when applying fullyconnected layer activations to describe local regions. By applying our methodto four popular visual classification tasks, it is demonstrated that theproposed method can achieve comparable or in some cases significantly betterperformance than existing fully-connected layer based image representationswhile incurring much lower computational cost.
arxiv-1411-7450 | Worst-Case Linear Discriminant Analysis as Scalable Semidefinite Feasibility Problems |  http://arxiv.org/abs/1411.7450  | author:Hui Li, Chunhua Shen, Anton van den Hengel, Qinfeng Shi category:cs.LG published:2014-11-27 summary:In this paper, we propose an efficient semidefinite programming (SDP)approach to worst-case linear discriminant analysis (WLDA). Compared with thetraditional LDA, WLDA considers the dimensionality reduction problem from theworst-case viewpoint, which is in general more robust for classification.However, the original problem of WLDA is non-convex and difficult to optimize.In this paper, we reformulate the optimization problem of WLDA into a sequenceof semidefinite feasibility problems. To efficiently solve the semidefinitefeasibility problems, we design a new scalable optimization method withquasi-Newton methods and eigen-decomposition being the core components. Theproposed method is orders of magnitude faster than standard interior-pointbased SDP solvers. Experiments on a variety of classification problems demonstrate that ourapproach achieves better performance than standard LDA. Our method is also muchfaster and more scalable than standard interior-point SDP solvers based WLDA.The computational complexity for an SDP with $m$ constraints and matrices ofsize $d$ by $d$ is roughly reduced from $\mathcal{O}(m^3+md^3+m^2d^2)$ to$\mathcal{O}(d^3)$ ($m>d$ in our case).
arxiv-1411-7591 | An Egocentric Look at Video Photographer Identity |  http://arxiv.org/abs/1411.7591  | author:Yedid Hoshen, Shmuel Peleg category:cs.CV published:2014-11-27 summary:Egocentric cameras are being worn by an increasing number of users, amongthem many security forces worldwide. GoPro cameras already penetrated the massmarket, reporting substantial increase in sales every year. As head-worncameras do not capture the photographer, it may seem that the anonymity of thephotographer is preserved even when the video is publicly distributed. We show that camera motion, as can be computed from the egocentric video,provides unique identity information. The photographer can be reliablyrecognized from a few seconds of video captured when walking. The proposedmethod achieves more than 90% recognition accuracy in cases where the randomsuccess rate is only 3%. Applications can include theft prevention by locking the camera when not wornby its rightful owner. Searching video sharing services (e.g. YouTube) foregocentric videos shot by a specific photographer may also become possible. Animportant message in this paper is that photographers should be aware thatsharing egocentric video will compromise their anonymity, even when their faceis not visible.
arxiv-1411-7717 | On the Expressive Efficiency of Sum Product Networks |  http://arxiv.org/abs/1411.7717  | author:James Martens, Venkatesh Medabalimi category:cs.LG stat.ML published:2014-11-27 summary:Sum Product Networks (SPNs) are a recently developed class of deep generativemodels which compute their associated unnormalized density functions using aspecial type of arithmetic circuit. When certain sufficient conditions, calledthe decomposability and completeness conditions (or "D&C" conditions), areimposed on the structure of these circuits, marginal densities and other usefulquantities, which are typically intractable for other deep generative models,can be computed by what amounts to a single evaluation of the network (which isa property known as "validity"). However, the effect that the D&C conditionshave on the capabilities of D&C SPNs is not well understood. In this work we analyze the D&C conditions, expose the various connectionsthat D&C SPNs have with multilinear arithmetic circuits, and consider thequestion of how well they can capture various distributions as a function oftheir size and depth. Among our various contributions is a result whichestablishes the existence of a relatively simple distribution with fullytractable marginal densities which cannot be efficiently captured by D&C SPNsof any depth, but which can be efficiently captured by various other deepgenerative models. We also show that with each additional layer of depthpermitted, the set of distributions which can be efficiently captured by D&CSPNs grows in size. This kind of "depth hierarchy" property has been widelyconjectured to hold for various deep models, but has never been proven for anyof them. Some of our other contributions include a new characterization of theD&C conditions as sufficient and necessary ones for a slightly strengthenednotion of validity, and various state-machine characterizations of the types ofcomputations that can be performed efficiently by D&C SPNs.
arxiv-1411-7676 | Visual Representations: Definint Properties and Deep Approximations |  http://arxiv.org/abs/1411.7676  | author:Stefano Soatto, Alessandro Chiuso category:cs.CV published:2014-11-27 summary:Visual representations are defined in terms of minimal sufficient statisticsof visual data, for a class of tasks, that are also invariant to nuisancevariability. Minimal sufficiency guarantees that we can store a representationin lieu of raw data with smallest complexity and no performance loss on thetask at hand. Invariance guarantees that the statistic is constant with respectto uninformative transformations of the data. We derive analytical expressionsfor such representations and show they are related to feature descriptorscommonly used in computer vision, as well as to convolutional neural networks.This link highlights the assumptions and approximations tacitly assumed bythese methods and explains empirical practices such as clamping, pooling andjoint normalization.
arxiv-1411-7494 | An Evolutionary Optimization Approach to Risk Parity Portfolio Selection |  http://arxiv.org/abs/1411.7494  | author:Ronald Hochreiter category:q-fin.PM cs.NE published:2014-11-27 summary:In this paper we present an evolutionary optimization approach to solve therisk parity portfolio selection problem. While there exist convex optimizationapproaches to solve this problem when long-only portfolios are considered, theoptimization problem becomes non-trivial in the long-short case. To solve thisproblem, we propose a genetic algorithm as well as a local search heuristic.This algorithmic framework is able to compute solutions successfully. Numericalresults using real-world data substantiate the practicability of the approachpresented in this paper.
arxiv-1411-7564 | Large-scale Binary Quadratic Optimization Using Semidefinite Relaxation and Applications |  http://arxiv.org/abs/1411.7564  | author:Peng Wang, Chunhua Shen, Anton van den Hengel, Philip H. S. Torr category:cs.CV published:2014-11-27 summary:In computer vision, many problems such as image segmentation, pixellabelling, and scene parsing can be formulated as binary quadratic programs(BQPs). For submodular problems, cuts based methods can be employed toefficiently solve large-scale problems. However, general nonsubmodular problemsare significantly more challenging to solve. Finding a solution when theproblem is of large size to be of practical interest, however, typicallyrequires relaxation. Two standard relaxation methods are widely used forsolving general BQPs--spectral methods and semidefinite programming (SDP), eachwith their own advantages and disadvantages. Spectral relaxation is simple andeasy to implement, but its bound is loose. Semidefinite relaxation has atighter bound, but its computational complexity is high, especially for largescale problems. In this work, we present a new SDP formulation for BQPs, withtwo desirable properties. First, it has a similar relaxation bound toconventional SDP formulations. Second, compared with conventional SDP methods,the new SDP formulation leads to a significantly more efficient and scalabledual optimization approach, which has the same degree of complexity as spectralmethods. We then propose two solvers, namely, quasi-Newton and smoothing Newtonmethods, for the dual problem. Both of them are significantly more efficientlythan standard interior-point methods. In practice, the smoothing Newton solveris faster than the quasi-Newton solver for dense or medium-sized problems,while the quasi-Newton solver is preferable for large sparse/structuredproblems. Our experiments on a few computer vision applications includingclustering, image segmentation, co-segmentation and registration show thepotential of our SDP formulation for solving large-scale BQPs.
arxiv-1411-7596 | Convex Techniques for Model Selection |  http://arxiv.org/abs/1411.7596  | author:Dustin Tran category:math.OC stat.ML published:2014-11-27 summary:We develop a robust convex algorithm to select the regularization parameterin model selection. In practice this would be automated in order to savepractitioners time from having to tune it manually. In particular, we implementand test the convex method for $K$-fold cross validation on ridge regression,although the same concept extends to more complex models. We then compare itsperformance with standard methods.
arxiv-1411-7582 | Graph Sensitive Indices for Comparing Clusterings |  http://arxiv.org/abs/1411.7582  | author:Zaeem Hussain, Marina Meila category:cs.LG published:2014-11-27 summary:This report discusses two new indices for comparing clusterings of a set ofpoints. The motivation for looking at new ways for comparing clusterings stemsfrom the fact that the existing clustering indices are based on set cardinalityalone and do not consider the positions of data points. The new indices,namely, the Random Walk index (RWI) and Variation of Information with Neighbors(VIN), are both inspired by the clustering metric Variation of Information(VI). VI possesses some interesting theoretical properties which are alsodesirable in a metric for comparing clusterings. We define our indices anddiscuss some of their explored properties which appear relevant for aclustering index. We also include the results of these indices on clusteringsof some example data sets.
arxiv-1411-7610 | Learning Stochastic Recurrent Networks |  http://arxiv.org/abs/1411.7610  | author:Justin Bayer, Christian Osendorfer category:stat.ML cs.LG published:2014-11-27 summary:Leveraging advances in variational inference, we propose to enhance recurrentneural networks with latent variables, resulting in Stochastic RecurrentNetworks (STORNs). The model i) can be trained with stochastic gradientmethods, ii) allows structured and multi-modal conditionals at each time step,iii) features a reliable estimator of the marginal likelihood and iv) is ageneralisation of deterministic recurrent neural networks. We evaluate themethod on four polyphonic musical data sets and motion capture data.
arxiv-1411-7336 | Edge direction matrixes-based local binar patterns descriptor for shape pattern recognition |  http://arxiv.org/abs/1411.7336  | author:Mohammed A. Talab, Siti Norul Huda Sheikh Abdullah, Bilal Bataineh category:cs.CV cs.IR published:2014-11-26 summary:Shapes and texture image recognition usage is an essential branch of patternrecognition. It is made up of techniques that aim at extracting informationfrom images via human knowledge and works. Local Binary Pattern (LBP) ensuresencoding global and local information and scaling invariance by introducing alook-up table to reflect the uniformity structure of an object. However, edgedirection matrixes (EDMS) only apply global invariant descriptor which employsfirst and secondary order relationships. The main idea behind this methodologyis the need of improved recognition capabilities, a goal achieved by thecombinative use of these descriptors. This collaboration aims to make use ofthe major advantages each one presents, by simultaneously complementing eachother, in order to elevate their weak points. By using multiple classifierapproaches such as random forest and multi-layer perceptron neural network, theproposed combinative descriptor are compared with the state of the artcombinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS),LBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPSimage, Enghlishfnt and Arabic calligraphy . The experiments have shown thesuperiority of the introduced descriptor over the GLCM with EDMS, LBP andmoment invariants and other well-known descriptor such as Scale InvariantFeature Transform from the literature.
arxiv-1411-7200 | Localized Complexities for Transductive Learning |  http://arxiv.org/abs/1411.7200  | author:Ilya Tolstikhin, Gilles Blanchard, Marius Kloft category:stat.ML cs.LG published:2014-11-26 summary:We show two novel concentration inequalities for suprema of empiricalprocesses when sampling without replacement, which both take the variance ofthe functions into account. While these inequalities may potentially have broadapplications in learning theory in general, we exemplify their significance bystudying the transductive setting of learning theory. For which we provide thefirst excess risk bounds based on the localized complexity of the hypothesisclass, which can yield fast rates of convergence also in the transductivelearning setting. We give a preliminary analysis of the localized complexitiesfor the prominent case of kernel classes.
arxiv-1411-7245 | Heuristics for Exact Nonnegative Matrix Factorization |  http://arxiv.org/abs/1411.7245  | author:Arnaud Vandaele, Nicolas Gillis, François Glineur, Daniel Tuyttens category:math.OC cs.LG cs.NA stat.ML published:2014-11-26 summary:The exact nonnegative matrix factorization (exact NMF) problem is thefollowing: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank$r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$nonnegative matrix $H$ such that $X = WH$. In this paper, we propose twoheuristics for exact NMF, one inspired from simulated annealing and the otherfrom the greedy randomized adaptive search procedure. We show that these twoheuristics are able to compute exact nonnegative factorizations for severalclasses of nonnegative matrices (namely, linear Euclidean distance matrices,slack matrices, unique-disjointness matrices, and randomly generated matrices)and as such demonstrate their superiority over standard multi-start strategies.We also consider a hybridization between these two heuristics that allows us tocombine the advantages of both methods. Finally, we discuss the use of theseheuristics to gain insight on the behavior of the nonnegative rank, i.e., theminimum factorization rank such that an exact NMF exists. In particular, wedisprove a conjecture on the nonnegative rank of a Kronecker product, propose anew upper bound on the extension complexity of generic $n$-gons and conjecturethe exact value of (i) the extension complexity of regular $n$-gons and (ii)the nonnegative rank of a submatrix of the slack matrix of the correlationpolytope.
arxiv-1412-0003 | 3D-Assisted Image Feature Synthesis for Novel Views of an Object |  http://arxiv.org/abs/1412.0003  | author:Hao Su, Fan Wang, Li Yi, Leonidas Guibas category:cs.CV published:2014-11-26 summary:Comparing two images in a view-invariant way has been a challenging problemin computer vision for a long time, as visual features are not stable underlarge view point changes. In this paper, given a single input image of anobject, we synthesize new features for other views of the same object. Toaccomplish this, we introduce an aligned set of 3D models in the same class asthe input object image. Each 3D model is represented by a set of views, and westudy the correlation of image patches between different views, seeking what wecall surrogates --- patches in one view whose feature content predicts well thefeatures of a patch in another view. In particular, for each patch in the noveldesired view, we seek surrogates from the observed view of the given image. Fora given surrogate, we predict that surrogate using linear combination of thecorresponding patches of the 3D model views, learn the coefficients, and thentransfer these coefficients on a per patch basis to synthesize the features ofthe patch in the novel view. In this way we can create feature sets for allviews of the latent object, providing us a multi-view representation of theobject. View-invariant object comparisons are achieved simply by computing the$L^2$ distances between the features of corresponding views. We providetheoretical and empirical analysis of the feature synthesis process, andevaluate the proposed view-agnostic distance (VAD) in fine-grained imageretrieval (100 object classes) and classification tasks. Experimental resultsshow that our synthesized features do enable view-independent comparisonbetween images and perform significantly better than traditional image featuresin this respect.
arxiv-1411-7889 | Open-source code for manifold-based 3D rotation recovery of X-ray scattering patterns |  http://arxiv.org/abs/1411.7889  | author:Aliakbar Jafarpour category:physics.optics cs.CV published:2014-11-26 summary:Single particle 3D imaging with ultrashort X-ray laser pulses is based oncollecting and combining the information content of 2D scattering patterns ofan object at different orientations. Typical sample-delivery schemes leavelittle or no room for controlling the orientations. As such, the orientationassociated with a given snapshot should be estimated after the experiment. Herewe present an open-source code for the most rigorous technique having beenreported in this context. Some practical issues along with proposed solutionsare also discussed.
arxiv-1411-7414 | Signal Recovery on Graphs: Variation Minimization |  http://arxiv.org/abs/1411.7414  | author:Siheng Chen, Aliaksei Sandryhaila, José M. F. Moura, Jelena Kovačević category:cs.SI cs.LG stat.ML published:2014-11-26 summary:We consider the problem of signal recovery on graphs as graphs model datawith complex structure as signals on a graph. Graph signal recovery impliesrecovery of one or multiple smooth graph signals from noisy, corrupted, orincomplete measurements. We propose a graph signal model and formulate signalrecovery as a corresponding optimization problem. We provide a general solutionby using the alternating direction methods of multipliers. We next show howsignal inpainting, matrix completion, robust principal component analysis, andanomaly detection all relate to graph signal recovery, and providecorresponding specific solutions and theoretical analysis. Finally, we validatethe proposed methods on real-world recovery problems, including online blogclassification, bridge condition identification, temperature estimation,recommender system, and expert opinion combination of online blogclassification.
arxiv-1412-0035 | Understanding Deep Image Representations by Inverting Them |  http://arxiv.org/abs/1412.0035  | author:Aravindh Mahendran, Andrea Vedaldi category:cs.CV published:2014-11-26 summary:Image representations, from SIFT and Bag of Visual Words to ConvolutionalNeural Networks (CNNs), are a crucial component of almost any imageunderstanding system. Nevertheless, our understanding of them remains limited.In this paper we conduct a direct analysis of the visual information containedin representations by asking the following question: given an encoding of animage, to which extent is it possible to reconstruct the image itself? Toanswer this question we contribute a general framework to invertrepresentations. We show that this method can invert representations such asHOG and SIFT more accurately than recent alternatives while being applicable toCNNs too. We then use this technique to study the inverse of recentstate-of-the-art CNN image representations for the first time. Among ourfindings, we show that several layers in CNNs retain photographically accurateinformation about the image, with different degrees of geometric andphotometric invariance.
arxiv-1411-7346 | A Chasm Between Identity and Equivalence Testing with Conditional Queries |  http://arxiv.org/abs/1411.7346  | author:Jayadev Acharya, Clément L. Canonne, Gautam Kamath category:cs.DS cs.CC cs.LG math.PR math.ST stat.TH published:2014-11-26 summary:A recent model for property testing of probability distributions enablestremendous savings in the sample complexity of testing algorithms, by allowingthem to condition the sampling on subsets of the domain. In particular, Canonne, Ron, and Servedio showed that, in this setting,testing identity of an unknown distribution $D$ (i.e., whether $D=D^*$ for anexplicitly known $D^*$) can be done with a constant number of samples,independent of the support size $n$ -- in contrast to the required $\sqrt{n}$in the standard sampling model. However, it was unclear whether the same heldfor the case of testing equivalence, where both distributions are unknown.Indeed, while Canonne, Ron, and Servedio established a$\mathrm{poly}\log(n)$-query upper bound for equivalence testing, very recentlybrought down to $\tilde O(\log\log n)$ by Falahatgar et al., whether adependence on the domain size $n$ is necessary was still open, and explicitlyposed by Fischer at the Bertinoro Workshop on Sublinear Algorithms. In thiswork, we answer the question in the positive, showing that any testingalgorithm for equivalence must make $\Omega(\sqrt{\log\log n})$ queries in theconditional sampling model. Interestingly, this demonstrates an intrinsicqualitative gap between identity and equivalence testing, absent in thestandard sampling model (where both problems have sampling complexity$n^{\Theta(1)}$). Turning to another question, we investigate the complexity of support sizeestimation. We provide a doubly-logarithmic upper bound for the adaptiveversion of this problem, generalizing work of Ron and Tsur to our weaker model.We also establish a logarithmic lower bound for the non-adaptive version ofthis problem. This latter result carries on to the related problem ofnon-adaptive uniformity testing, an exponential improvement over previousresults that resolves an open question of Chakraborty et al.
arxiv-1411-7399 | Fisher Vectors Derived from Hybrid Gaussian-Laplacian Mixture Models for Image Annotation |  http://arxiv.org/abs/1411.7399  | author:Benjamin Klein, Guy Lev, Gil Sadeh, Lior Wolf category:cs.CV published:2014-11-26 summary:In the traditional object recognition pipeline, descriptors are denselysampled over an image, pooled into a high dimensional non-linear representationand then passed to a classifier. In recent years, Fisher Vectors have provenempirically to be the leading representation for a large variety ofapplications. The Fisher Vector is typically taken as the gradients of thelog-likelihood of descriptors, with respect to the parameters of a GaussianMixture Model (GMM). Motivated by the assumption that different distributionsshould be applied for different datasets, we present two other Mixture Modelsand derive their Expectation-Maximization and Fisher Vector expressions. Thefirst is a Laplacian Mixture Model (LMM), which is based on the Laplaciandistribution. The second Mixture Model presented is a Hybrid Gaussian-LaplacianMixture Model (HGLMM) which is based on a weighted geometric mean of theGaussian and Laplacian distribution. An interesting property of theExpectation-Maximization algorithm for the latter is that in the maximizationstep, each dimension in each component is chosen to be either a Gaussian or aLaplacian. Finally, by using the new Fisher Vectors derived from HGLMMs, weachieve state-of-the-art results for both the image annotation and the imagesearch by a sentence tasks.
arxiv-1411-7113 | Real time Detection of Lane Markers in Urban Streets |  http://arxiv.org/abs/1411.7113  | author:Mohamed Aly category:cs.CV cs.RO published:2014-11-26 summary:We present a robust and real time approach to lane marker detection in urbanstreets. It is based on generating a top view of the road, filtering usingselective oriented Gaussian filters, using RANSAC line fitting to give initialguesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which isthen followed by a post-processing step. Our algorithm can detect all lanes instill images of the street in various conditions, while operating at a rate of50 Hz and achieving comparable results to previous techniques.
arxiv-1411-7405 | A note relating ridge regression and OLS p-values to preconditioned sparse penalized regression |  http://arxiv.org/abs/1411.7405  | author:Karl Rohe category:stat.ML stat.ME published:2014-11-26 summary:When the design matrix has orthonormal columns, "soft thresholding" theordinary least squares (OLS) solution produces the Lasso solution [Tibshirani,1996]. If one uses the Puffer preconditioned Lasso [Jia and Rohe, 2012], thenthis result generalizes from orthonormal designs to full rank designs (Theorem1). Theorem 2 refines the Puffer preconditioner to make the Lasso select thesame model as removing the elements of the OLS solution with the largestp-values. Using a generalized Puffer preconditioner, Theorem 3 relates ridgeregression to the preconditioned Lasso; this result is for the high dimensionalsetting, p > n. Where the standard Lasso is akin to forward selection [Efron etal., 2004], Theorems 1, 2, and 3 suggest that the preconditioned Lasso is moreakin to backward elimination. These results hold for sparse penalties beyondl1; for a broad class of sparse and non-convex techniques (e.g. SCAD and MC+),the results hold for all local minima.
arxiv-1411-6718 | LABR: A Large Scale Arabic Sentiment Analysis Benchmark |  http://arxiv.org/abs/1411.6718  | author:Mahmoud Nabil, Mohamed Aly, Amir Atiya category:cs.CL cs.LG published:2014-11-25 summary:We introduce LABR, the largest sentiment analysis dataset to-date for theArabic language. It consists of over 63,000 book reviews, each rated on a scaleof 1 to 5 stars. We investigate the properties of the dataset, and present itsstatistics. We explore using the dataset for two tasks: (1) sentiment polarityclassification; and (2) ratings classification. Moreover, we provide standardsplits of the dataset into training, validation and testing, for both polarityand ratings classification, in both balanced and unbalanced settings. We extendour previous work by performing a comprehensive analysis on the dataset. Inparticular, we perform an extended survey of the different classifierstypically used for the sentiment polarity classification problem. We alsoconstruct a sentiment lexicon from the dataset that contains both single andcompound sentiment words and we explore its effectiveness. We make the datasetand experimental details publicly available.
arxiv-1411-6948 | PLUTO: Penalized Unbiased Logistic Regression Trees |  http://arxiv.org/abs/1411.6948  | author:Wenwen Zhang, Wei-Yin Loh category:stat.ML stat.ME published:2014-11-25 summary:We propose a new algorithm called PLUTO for building logistic regressiontrees to binary response data. PLUTO can capture the nonlinear and interactionpatterns in messy data by recursively partitioning the sample space. It fits asimple or a multiple linear logistic regression model in each partition. PLUTOemploys the cyclical coordinate descent method for estimation of multiplelinear logistic regression models with elastic net penalties, which allows itto deal with high-dimensional data efficiently. The tree structure comprises agraphical description of the data. Together with the logistic regressionmodels, it provides an accurate classifier as well as a piecewise smoothestimate of the probability of "success". PLUTO controls selection bias by: (1)separating split variable selection from split point selection; (2) applying anadjusted chi-squared test to find the split variable instead of exhaustivesearch. A bootstrap calibration technique is employed to further correctselection bias. Comparison on real datasets shows that on average, the multiplelinear PLUTO models predict more accurately than other algorithms.
arxiv-1411-6912 | Short-Term Memory Through Persistent Activity: Evolution of Self-Stopping and Self-Sustaining Activity in Spiking Neural Networks |  http://arxiv.org/abs/1411.6912  | author:Julien Hubert, Takashi Ikegami category:cs.NE q-bio.NC published:2014-11-25 summary:Memories in the brain are separated in two categories: short-term andlong-term memories. Long-term memories remain for a lifetime, while short-termones exist from a few milliseconds to a few minutes. Within short-term memorystudies, there is debate about what neural structure could implement it.Indeed, mechanisms responsible for long-term memories appear inadequate for thetask. Instead, it has been proposed that short-term memories could be sustainedby the persistent activity of a group of neurons. In this work, we explore whattopology could sustain short-term memories, not by designing a model fromspecific hypotheses, but through Darwinian evolution in order to obtain newinsights into its implementation. We evolved 10 networks capable of retaininginformation for a fixed duration between 2 and 11s. Our main finding has beenthat the evolution naturally created two functional modules in the network: onewhich sustains the information containing primarily excitatory neurons, whilethe other, which is responsible for forgetting, was composed mainly ofinhibitory neurons. This demonstrates how the balance between inhibition andexcitation plays an important role in cognition.
arxiv-1411-6850 | Similarity- based approach for outlier detection |  http://arxiv.org/abs/1411.6850  | author:Amina Dik, Khalid Jebari, Abdelaziz Bouroumi, Aziz Ettouhami category:cs.CV published:2014-11-25 summary:This paper presents a new approach for detecting outliers by introducing thenotion of object's proximity. The main idea is that normal point has similarcharacteristics with several neighbors. So the point in not an outlier if ithas a high degree of proximity and its neighbors are several. The performanceof this approach is illustrated through real datasets
arxiv-1411-6725 | Accelerated Parallel Optimization Methods for Large Scale Machine Learning |  http://arxiv.org/abs/1411.6725  | author:Haipeng Luo, Patrick Haffner, Jean-Francois Paiement category:cs.LG published:2014-11-25 summary:The growing amount of high dimensional data in different machine learningapplications requires more efficient and scalable optimization algorithms. Inthis work, we consider combining two techniques, parallelism and Nesterov'sacceleration, to design faster algorithms for L1-regularized loss. We firstsimplify BOOM, a variant of gradient descent, and study it in a unifiedframework, which allows us to not only propose a refined measurement ofsparsity to improve BOOM, but also show that BOOM is provably slower thanFISTA. Moving on to parallel coordinate descent methods, we then propose anefficient accelerated version of Shotgun, improving the convergence rate from$O(1/t)$ to $O(1/t^2)$. Our algorithm enjoys a concise form and analysiscompared to previous work, and also allows one to study several connected workin a unified way.
arxiv-1411-6880 | An Automated Images-to-Graphs Framework for High Resolution Connectomics |  http://arxiv.org/abs/1411.6880  | author:William Gray Roncal, Dean M. Kleissas, Joshua T. Vogelstein, Priya Manavalan, Kunal Lillaney, Michael Pekala, Randal Burns, R. Jacob Vogelstein, Carey E. Priebe, Mark A. Chevillet, Gregory D. Hager category:q-bio.QM cs.CV published:2014-11-25 summary:Reconstructing a map of neuronal connectivity is a critical challenge incontemporary neuroscience. Recent advances in high-throughput serial sectionelectron microscopy (EM) have produced massive 3D image volumes of nanoscalebrain tissue for the first time. The resolution of EM allows for individualneurons and their synaptic connections to be directly observed. Recoveringneuronal networks by manually tracing each neuronal process at this scale isunmanageable, and therefore researchers are developing automated imageprocessing modules. Thus far, state-of-the-art algorithms focus only on thesolution to a particular task (e.g., neuron segmentation or synapseidentification). In this manuscript we present the first fully automated images-to-graphspipeline (i.e., a pipeline that begins with an imaged volume of neural tissueand produces a brain graph without any human interaction). To evaluate overallperformance and select the best parameters and methods, we also develop ametric to assess the quality of the output graphs. We evaluate a set ofalgorithms and parameters, searching possible operating points to identify thebest available brain graph for our assessment metric. Finally, we deploy areference end-to-end version of the pipeline on a large, publicly availabledata set. This provides a baseline result and framework for community analysisand future algorithm development and testing. All code and data derivativeshave been made publicly available toward eventually unlocking new biofideliccomputational primitives and understanding of neuropathologies.
arxiv-1411-6757 | Echo State Condition at the Critical Point |  http://arxiv.org/abs/1411.6757  | author:Norbert Michael Mayer category:cs.NE published:2014-11-25 summary:Recurrent networks that have transfer functions that fulfill the Lipschitzcontinuity with L=1, may be echo state networks if certain limitations on therecurrent connectivity are applied. Initially it has been shown that it issufficient if the largest singular value of the recurrent connectivity S issmaller than 1. The main achievement of this paper is a proof under whichconditions the network is an echo state network even if S=1. It turns out thatin this critical case the exact shape of the transfer function plays a decisiverole whether or not the network still fulfills the echo state condition. Inaddition, several intuitive examples with one neuron networks are outlined toillustrate effects of critical connectivity.
arxiv-1411-6909 | Image Classification and Retrieval from User-Supplied Tags |  http://arxiv.org/abs/1411.6909  | author:Hamid Izadinia, Ali Farhadi, Aaron Hertzmann, Matthew D. Hoffman category:cs.CV published:2014-11-25 summary:This paper proposes direct learning of image classification fromuser-supplied tags, without filtering. Each tag is supplied by the user whoshared the image online. Enormous numbers of these tags are freely availableonline, and they give insight about the image categories important to users andto image classification. Our approach is complementary to the conventionalapproach of manual annotation, which is extremely costly. We analyze of theFlickr 100 Million Image dataset, making several useful observations about thestatistics of these tags. We introduce a large-scale robust classificationalgorithm, in order to handle the inherent noise in these tags, and acalibration procedure to better predict objective annotations. We show thatfreely available, user-supplied tags can obtain similar or superior results tolarge databases of costly manual annotations.
arxiv-1411-7014 | Efficient Algorithms for Bayesian Network Parameter Learning from Incomplete Data |  http://arxiv.org/abs/1411.7014  | author:Guy Van den Broeck, Karthika Mohan, Arthur Choi, Judea Pearl category:cs.LG cs.AI published:2014-11-25 summary:We propose an efficient family of algorithms to learn the parameters of aBayesian network from incomplete data. In contrast to textbook approaches suchas EM and the gradient method, our approach is non-iterative, yields closedform parameter estimates, and eliminates the need for inference in a Bayesiannetwork. Our approach provides consistent parameter estimates for missing dataproblems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approachis orders of magnitude faster than EM (as our approach requires no inference).Given sufficient data, we learn parameters that can be orders of magnitude moreaccurate.
arxiv-1412-6149 | Design, Implementation and Simulation of a Cloud Computing System for Enhancing Real-time Video Services by using VANET and Onboard Navigation Systems |  http://arxiv.org/abs/1412.6149  | author:Karim Hammoudi, Nabil Ajam, Mohamed Kasraoui, Fadi Dornaika, Karan Radhakrishnan, Karthik Bandi, Qing Cai, Sai Liu category:cs.NI cs.CV published:2014-11-25 summary:In this paper, we propose a design for novel and experimental cloud computingsystems. The proposed system aims at enhancing computational, communicationaland annalistic capabilities of road navigation services by merging severalindependent technologies, namely vision-based embedded navigation systems,prominent Cloud Computing Systems (CCSs) and Vehicular Ad-hoc NETwork (VANET).This work presents our initial investigations by describing the design of aglobal generic system. The designed system has been experimented with variousscenarios of video-based road services. Moreover, the associated architecturehas been implemented on a small-scale simulator of an in-vehicle embeddedsystem. The implemented architecture has been experimented in the case of asimulated road service to aid the police agency. The goal of this service is torecognize and track searched individuals and vehicles in a real-time monitoringsystem remotely connected to moving cars. The presented work demonstrates thepotential of our system for efficiently enhancing and diversifying real-timevideo services in road environments.
arxiv-1411-6836 | Deep convolutional filter banks for texture recognition and segmentation |  http://arxiv.org/abs/1411.6836  | author:Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi category:cs.CV published:2014-11-25 summary:Research in texture recognition often concentrates on the problem of materialrecognition in uncluttered conditions, an assumption rarely met byapplications. In this work we conduct a first study of material and describabletexture at- tributes recognition in clutter, using a new dataset derived fromthe OpenSurface texture repository. Motivated by the challenge posed by thisproblem, we propose a new texture descriptor, D-CNN, obtained by Fisher Vectorpooling of a Convolutional Neural Network (CNN) filter bank. D-CNNsubstantially improves the state-of-the-art in texture, mate- rial and scenerecognition. Our approach achieves 82.3% accuracy on Flickr material datasetand 81.1% accuracy on MIT indoor scenes, providing absolute gains of more than10% over existing approaches. D-CNN easily trans- fers across domains withoutrequiring feature adaptation as for methods that build on the fully-connectedlayers of CNNs. Furthermore, D-CNN can seamlessly incorporate multi-scaleinformation and describe regions of arbitrary shapes and sizes. Our approach isparticularly suited at lo- calizing stuff categories and obtainsstate-of-the-art re- sults on MSRC segmentation dataset, as well as promisingresults on recognizing materials and surface attributes in clutter on theOpenSurfaces dataset.
arxiv-1411-6768 | Hypotheses of neural code and the information model of the neuron-detector |  http://arxiv.org/abs/1411.6768  | author:Yuri Parzhin category:cs.NE cs.AI q-bio.NC published:2014-11-25 summary:This paper deals with the problem of neural code solving. On the basis of theformulated hypotheses the information model of a neuron-detector is suggested,the detector being one of the basic elements of an artificial neural network(ANN). The paper subjects the connectionist paradigm of ANN building tocriticism and suggests a new presentation paradigm for ANN building andneuroelements (NE) learning. The adequacy of the suggested model is proved bythe fact that is does not contradict the modern propositions of neuropsychologyand neurophysiology.
arxiv-1411-6699 | One Vector is Not Enough: Entity-Augmented Distributional Semantics for Discourse Relations |  http://arxiv.org/abs/1411.6699  | author:Yangfeng Ji, Jacob Eisenstein category:cs.CL cs.LG published:2014-11-25 summary:Discourse relations bind smaller linguistic units into coherent texts.However, automatically identifying discourse relations is difficult, because itrequires understanding the semantics of the linked arguments. A more subtlechallenge is that it is not enough to represent the meaning of each argument ofa discourse relation, because the relation may depend on links betweenlower-level components, such as entity mentions. Our solution computesdistributional meaning representations by composition up the syntactic parsetree. A key difference from previous work on compositional distributionalsemantics is that we also compute representations for entity mentions, using anovel downward compositional pass. Discourse relations are predicted from thedistributional representations of the arguments, and also of their coreferententity mentions. The resulting system obtains substantial improvements over theprevious state-of-the-art in predicting implicit discourse relations in thePenn Discourse Treebank.
arxiv-1411-6970 | Post-acquisition image based compensation for thickness variation in microscopy section series |  http://arxiv.org/abs/1411.6970  | author:Philipp Hanslovsky, John A. Bogovic, Stephan Saalfeld category:cs.CV q-bio.QM stat.AP published:2014-11-25 summary:Serial section Microscopy is an established method for volumetric anatomyreconstruction. Section series imaged with Electron Microscopy are currentlyvital for the reconstruction of the synaptic connectivity of entire animalbrains such as that of Drosophila melanogaster. The process of removingultrathin layers from a solid block containing the specimen, however, is afragile procedure and has limited precision with respect to section thickness.We have developed a method to estimate the relative z-position of eachindividual section as a function of signal change across the section series.First experiments show promising results on both serial section TransmissionElectron Microscopy (ssTEM) data and Focused Ion Beam Scanning ElectronMicroscopy (FIB-SEM) series. We made our solution available as Open Sourceplugins for the TrakEM2 software and the ImageJ distribution Fiji.
arxiv-1411-6370 | Big Learning with Bayesian Methods |  http://arxiv.org/abs/1411.6370  | author:Jun Zhu, Jianfei Chen, Wenbo Hu category:cs.LG stat.AP stat.CO stat.ME stat.ML F.1.2; G.3 published:2014-11-24 summary:Explosive growth in data and availability of cheap computing resources havesparked increasing interest in Big learning, an emerging subfield that studiesscalable machine learning algorithms, systems, and applications with Big Data.Bayesian methods represent one important class of statistic methods for machinelearning, with substantial recent developments on adaptive, flexible andscalable Bayesian learning. This article provides a survey of the recentadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,including nonparametric Bayesian methods for adaptively inferring modelcomplexity, regularized Bayesian inference for improving the flexibility viaposterior regularization, and scalable algorithms and systems based onstochastic subsampling and distributed computing for dealing with large-scaleapplications.
arxiv-1411-6509 | Persistent Evidence of Local Image Properties in Generic ConvNets |  http://arxiv.org/abs/1411.6509  | author:Ali Sharif Razavian, Hossein Azizpour, Atsuto Maki, Josephine Sullivan, Carl Henrik Ek, Stefan Carlsson category:cs.CV published:2014-11-24 summary:Supervised training of a convolutional network for object classificationshould make explicit any information related to the class of objects anddisregard any auxiliary information associated with the capture of the image orthe variation within the object class. Does this happen in practice? Althoughthis seems to pertain to the very final layers in the network, if we look atearlier layers we find that this is not the case. Surprisingly, strong spatialinformation is implicit. This paper addresses this, in particular, exploitingthe image representation at the first fully connected layer, i.e. the globalimage descriptor which has been recently shown to be most effective in a rangeof visual recognition tasks. We empirically demonstrate evidences for thefinding in the contexts of four different tasks: 2d landmark detection, 2dobject keypoints prediction, estimation of the RGB values of input image, andrecovery of semantic label of each pixel. We base our investigation on a simpleframework with ridge rigression commonly across these tasks, and show resultswhich all support our insight. Such spatial information can be used forcomputing correspondence of landmarks to a good accuracy, but shouldpotentially be useful for improving the training of the convolutional nets forclassification purposes.
arxiv-1411-6382 | Mid-level Deep Pattern Mining |  http://arxiv.org/abs/1411.6382  | author:Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2014-11-24 summary:Mid-level visual element discovery aims to find clusters of image patchesthat are both representative and discriminative. In this work, we study thisproblem from the prospective of pattern mining while relying on the recentlypopularized Convolutional Neural Networks (CNNs). Specifically, we find thatfor an image patch, activations extracted from the first fully-connected layerof CNNs have two appealing properties which enable its seamless integrationwith pattern mining. Patterns are then discovered from a large number of CNNactivations of image patches through the well-known association rule mining.When we retrieve and visualize image patches with the same pattern,surprisingly, they are not only visually similar but also semanticallyconsistent. We apply our approach to scene and object classification tasks, anddemonstrate that our approach outperforms all previous works on mid-levelvisual element discovery by a sizeable margin with far fewer elements beingused. Our approach also outperforms or matches recent works using CNN for thesetasks. Source code of the complete system is available online.
arxiv-1411-6358 | A Hybrid Solution to improve Iteration Efficiency in the Distributed Learning |  http://arxiv.org/abs/1411.6358  | author:Junxiong Wang, Hongzhi Wang, Chenxu Zhao category:cs.DC cs.LG 68 published:2014-11-24 summary:Currently, many machine learning algorithms contain lots of iterations. Whenit comes to existing large-scale distributed systems, some slave nodes maybreak down or have lower efficiency. Therefore traditional machine learningalgorithm may fail because of the instability of distributed system.We presentsa hybrid approach which not only own a high fault-tolerant but also achieve abalance of performance and efficiency.For each iteration, the result of slowmachines will be abandoned. Then, we discuss the relationship between accuracyand abandon rate. Next we debate the convergence speed of this process.Finally, our experiments demonstrate our idea can dramatically reducecalculation time and be used in many platforms.
arxiv-1411-7935 | Multiple object tracking with context awareness |  http://arxiv.org/abs/1411.7935  | author:Laura Leal-Taixé category:cs.CV published:2014-11-24 summary:Multiple people tracking is a key problem for many applications such assurveillance, animation or car navigation, and a key input for tasks such asactivity recognition. In crowded environments occlusions and false detectionsare common, and although there have been substantial advances in recent years,tracking is still a challenging task. Tracking is typically divided into twosteps: detection, i.e., locating the pedestrians in the image, and dataassociation, i.e., linking detections across frames to form completetrajectories. For the data association task, approaches typically aim at developing new,more complex formulations, which in turn put the focus on the optimizationtechniques required to solve them. However, they still utilize very basicinformation such as distance between detections. In this thesis, I focus on thedata association task and argue that there is contextual information that hasnot been fully exploited yet in the tracking community, mainly social contextand spatial context coming from different views.
arxiv-1411-6406 | Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors |  http://arxiv.org/abs/1411.6406  | author:Lingqiao Liu, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang category:cs.CV published:2014-11-24 summary:Deriving from the gradient vector of a generative model of local features,Fisher vector coding (FVC) has been identified as an effective coding methodfor image classification. Most, if not all, % FVC implementations employ theGaussian mixture model (GMM) to characterize the generation process of localfeatures. This choice has shown to be sufficient for traditional lowdimensional local features, e.g., SIFT; and typically, good performance can beachieved with only a few hundred Gaussian distributions. However, the samenumber of Gaussians is insufficient to model the feature space spanned byhigher dimensional local features, which have become popular recently. In orderto improve the modeling capacity for high dimensional features, it turns out tobe inefficient and computationally impractical to simply increase the number ofGaussians. In this paper, we propose a model in which each local feature isdrawn from a Gaussian distribution whose mean vector is sampled from asubspace. With certain approximation, this model can be converted to a sparsecoding procedure and the learning/inference problems can be readily solved bystandard sparse coding methods. By calculating the gradient vector of theproposed model, we derive a new fisher vector encoding strategy, termed SparseCoding based Fisher Vector Coding (SCFVC). Moreover, we adopt the recentlydeveloped Deep Convolutional Neural Network (CNN) descriptor as a highdimensional local feature and implement image classification with the proposedSCFVC. Our experimental evaluations demonstrate that our method not onlysignificantly outperforms the traditional GMM based Fisher vector encoding butalso achieves the state-of-the-art performance in generic object recognition,indoor scene, and fine-grained image classification problems.
arxiv-1411-6590 | Consistency of Cheeger and Ratio Graph Cuts |  http://arxiv.org/abs/1411.6590  | author:Nicolas Garcia Trillos, Dejan Slepcev, James von Brecht, Thomas Laurent, Xavier Bresson category:stat.ML cs.LG math.ST stat.TH published:2014-11-24 summary:This paper establishes the consistency of a family of graph-cut-basedalgorithms for clustering of data clouds. We consider point clouds obtained assamples of a ground-truth measure. We investigate approaches to clusteringbased on minimizing objective functionals defined on proximity graphs of thegiven sample. Our focus is on functionals based on graph cuts like the Cheegerand ratio cuts. We show that minimizers of the these cuts converge as thesample size increases to a minimizer of a corresponding continuum cut (whichpartitions the ground truth measure). Moreover, we obtain sharp conditions onhow the connectivity radius can be scaled with respect to the number of samplepoints for the consistency to hold. We provide results for two-way and formultiway cuts. Furthermore we provide numerical experiments that illustrate theresults and explore the optimality of scaling in dimension two.
arxiv-1411-6520 | Distributed Coordinate Descent for L1-regularized Logistic Regression |  http://arxiv.org/abs/1411.6520  | author:Ilya Trofimov, Alexander Genkin category:stat.ML cs.LG published:2014-11-24 summary:Solving logistic regression with L1-regularization in distributed settings isan important problem. This problem arises when training dataset is very largeand cannot fit the memory of a single machine. We present d-GLMNET, a newalgorithm solving logistic regression with L1-regularization in the distributedsettings. We empirically show that it is superior over distributed onlinelearning via truncated gradient.
arxiv-1411-6340 | Iteratively Reweighted Graph Cut for Multi-label MRFs with Non-convex Priors |  http://arxiv.org/abs/1411.6340  | author:Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann, Hongdong Li category:cs.CV published:2014-11-24 summary:While widely acknowledged as highly effective in computer vision, multi-labelMRFs with non-convex priors are difficult to optimize. To tackle this, weintroduce an algorithm that iteratively approximates the original energy withan appropriately weighted surrogate energy that is easier to minimize. Ouralgorithm guarantees that the original energy decreases at each iteration. Inparticular, we consider the scenario where the global minimizer of the weightedsurrogate energy can be obtained by a multi-label graph cut algorithm, and showthat our algorithm then lets us handle of large variety of non-convex priors.We demonstrate the benefits of our method over state-of-the-art MRF energyminimization techniques on stereo and inpainting problems.
arxiv-1411-6622 | Noise Benefits in Expectation-Maximization Algorithms |  http://arxiv.org/abs/1411.6622  | author:Osonde Adekorede Osoba category:stat.ML cs.LG math.ST stat.TH published:2014-11-24 summary:This dissertation shows that careful injection of noise into sample data cansubstantially speed up Expectation-Maximization algorithms.Expectation-Maximization algorithms are a class of iterative algorithms forextracting maximum likelihood estimates from corrupted or incomplete data. Theconvergence speed-up is an example of a noise benefit or "stochastic resonance"in statistical signal processing. The dissertation presents derivations ofsufficient conditions for such noise-benefits and demonstrates the speed-up insome ubiquitous signal-processing algorithms. These algorithms includeparameter estimation for mixture models, the $k$-means clustering algorithm,the Baum-Welch algorithm for training hidden Markov models, and backpropagationfor training feedforward artificial neural networks. This dissertation alsoanalyses the effects of data and model corruption on the more general Bayesianinference estimation framework. The main finding is a theorem guaranteeing thatuniform approximators for Bayesian model functions produce uniformapproximators for the posterior pdf via Bayes theorem. This result also appliesto hierarchical and multidimensional Bayesian models.
arxiv-1411-6369 | Scale-Invariant Convolutional Neural Networks |  http://arxiv.org/abs/1411.6369  | author:Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, Zheng Zhang category:cs.CV cs.LG cs.NE published:2014-11-24 summary:Even though convolutional neural networks (CNN) has achieved near-humanperformance in various computer vision tasks, its ability to tolerate scalevariations is limited. The popular practise is making the model bigger first,and then train it with data augmentation using extensive scale-jittering. Inthis paper, we propose a scaleinvariant convolutional neural network (SiCNN), amodeldesigned to incorporate multi-scale feature exaction and classificationinto the network structure. SiCNN uses a multi-column architecture, with eachcolumn focusing on a particular scale. Unlike previous multi-column strategies,these columns share the same set of filter parameters by a scale transformationamong them. This design deals with scale variation without blowing up the modelsize. Experimental results show that SiCNN detects features at various scales,and the classification result exhibits strong robustness against object scalevariations.
arxiv-1411-6365 | On the mathematic modeling of non-parametric curves based on cubic Bézier curves |  http://arxiv.org/abs/1411.6365  | author:Ha Jong Won, Choe Chun Hwa, Li Kum Song category:cs.CV published:2014-11-24 summary:B\'ezier splines are widely available in various systems with the curves andsurface designs. In general, the B\'ezier spline can be specified with theB\'ezier curve segments and a B\'ezier curve segment can be fitted to anynumber of control points. The number of control points determines the degree ofthe B\'ezier polynomial. This paper presents a method which determines controlpoints for B\'ezier curves approximating segments of obtained imageoutline(non-parametric curve) by using the properties of cubic B\'ezier curves.Proposed method is a technique to determine the control points that hasgenerality and reduces the error of the B\'ezier curve approximation. Mainadvantage of proposed method is that it has higher accuracy and compressionrate than previous methods. The cubic B\'ezier spline is obtained from cubicB\'ezier curve segments. To demonstrate the various performances of theproposed algorithm, experimental results are compared.
arxiv-1411-6447 | The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification |  http://arxiv.org/abs/1411.6447  | author:Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, Zheng Zhang category:cs.CV published:2014-11-24 summary:Fine-grained classification is challenging because categories can only bediscriminated by subtle and local differences. Variances in the pose, scale orrotation usually make the problem more difficult. Most fine-grainedclassification systems follow the pipeline of finding foreground object orobject parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grainedclassification task using deep neural network. Our pipeline integrates threetypes of attention: the bottom-up attention that propose candidate patches, theobject-level top-down attention that selects relevant patches to a certainobject, and the part-level top-down attention that localizes discriminativeparts. We combine these attentions to train domain-specific deep nets, then useit to improve both the what and where aspects. Importantly, we avoid usingexpensive annotations like bounding box or part information from end-to-end.The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012dataset and CUB200_2011 dataset. Our pipeline delivered significantimprovements and achieved the best accuracy under the weakest supervisioncondition. The performance is competitive against other methods that rely onadditional annotations.
arxiv-1411-6998 | Solving the Periodic Timetabling Problem using a Genetic Algorithm |  http://arxiv.org/abs/1411.6998  | author:Diego Arenas, Remy Chevirer, Said Hanafi, Joaquin Rodriguez category:cs.AI cs.NE published:2014-11-24 summary:In railway operations, a timetable is established to determine the departureand arrival times for the trains or other rolling stock at the differentstations or relevant points inside the rail network or a subset of thisnetwork. The elaboration of this timetable is done to respond to the commercialrequirements for both passenger and freight traffic, but also it must respect aset of security and capacity constraints associated with the railway network,rolling stock and legislation. Combining these requirements and constraints, aswell as the important number of trains and schedules to plan, makes thepreparation of a feasible timetable a complex and time-consuming process, thatnormally takes several months to be completed. This article addresses theproblem of generating periodic timetables, which means that the involved trainsoperate in a recurrent pattern. For instance, the trains belonging to the sametrain line, depart from some station every 15 minutes or one hour. To tacklethe problem, we present a constraint-based model suitable for this kind ofproblem. Then, we propose a genetic algorithm, allowing a rapid generation offeasible periodic timetables. Finally, two case studies are presented, thefirst, describing a sub-set of the Netherlands rail network, and the second alarge portion of the Nord-pas-de-Calais regional rail network, both of them arethen solved using our algorithm and the results are presented and discussed.
arxiv-1411-6660 | Beyond Gaussian Pyramid: Multi-skip Feature Stacking for Action Recognition |  http://arxiv.org/abs/1411.6660  | author:Zhenzhong Lan, Ming Lin, Xuanchong Li, Alexander G. Hauptmann, Bhiksha Raj category:cs.CV published:2014-11-24 summary:Most state-of-the-art action feature extractors involve differentialoperators, which act as highpass filters and tend to attenuate low frequencyaction information. This attenuation introduces bias to the resulting featuresand generates ill-conditioned feature matrices. The Gaussian Pyramid has beenused as a feature enhancing technique that encodes scale-invariantcharacteristics into the feature space in an attempt to deal with thisattenuation. However, at the core of the Gaussian Pyramid is a convolutionalsmoothing operation, which makes it incapable of generating new features atcoarse scales. In order to address this problem, we propose a novel featureenhancing technique called Multi-skIp Feature Stacking (MIFS), which stacksfeatures extracted using a family of differential filters parameterized withmultiple time skips and encodes shift-invariance into the frequency space. MIFScompensates for information lost from using differential operators byrecapturing information at coarse scales. This recaptured information allows usto match actions at different speeds and ranges of motion. We prove that MIFSenhances the learnability of differential-based features exponentially. Theresulting feature matrices from MIFS have much smaller conditional numbers andvariances than those from conventional methods. Experimental results showsignificantly improved performance on challenging action recognition and eventdetection tasks. Specifically, our method exceeds the state-of-the-arts onHollywood2, UCF101 and UCF50 datasets and is comparable to state-of-the-arts onHMDB51 and Olympics Sports datasets. MIFS can also be used as a speedupstrategy for feature extraction with minimal or no accuracy cost.
arxiv-1412-6156 | Achieving Exact Cluster Recovery Threshold via Semidefinite Programming |  http://arxiv.org/abs/1412.6156  | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.DS math.PR published:2014-11-24 summary:The binary symmetric stochastic block model deals with a random graph of $n$vertices partitioned into two equal-sized clusters, such that each pair ofvertices is connected independently with probability $p$ within clusters and$q$ across clusters. In the asymptotic regime of $p=a \log n/n$ and $q=b \logn/n$ for fixed $a,b$ and $n \to \infty$, we show that the semidefiniteprogramming relaxation of the maximum likelihood estimator achieves the optimalthreshold for exactly recovering the partition from the graph with probabilitytending to one, resolving a conjecture of Abbe et al. \cite{Abbe14}.Furthermore, we show that the semidefinite programming relaxation also achievesthe optimal recovery threshold in the planted dense subgraph model containing asingle cluster of size proportional to $n$.
arxiv-1411-6651 | A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network Structure |  http://arxiv.org/abs/1411.6651  | author:Amir Arsalan Soltani category:cs.AI stat.ML published:2014-11-24 summary:In this report paper we first present a report of the Advanced MachineLearning Course Project on the provided data set and then present a novelheuristic algorithm for exact Bayesian network (BN) structure discovery thatuses decomposable scoring functions. Our algorithm follows a different approachto solve the problem of BN structure discovery than the previously used methodssuch as Dynamic Programming (DP) and Branch and Bound to reduce the searchspace and find the global optima space for the problem. The algorithm wepropose has some degree of flexibility that can make it more or less greedy.The more the algorithm is set to be greedy, the more the speed of the algorithmwill be, and the less optimal the final structure. Our algorithm runs in a muchless time than the previously known methods and guarantees to have anoptimality of close to 99%. Therefore, it sacrifices less than one percent ofscore of an optimal structure in order to gain a much lower running time andmake the algorithm feasible for large data sets (we may note that we never usedany toolbox except for result validation)
arxiv-1411-6326 | Vision and Learning for Deliberative Monocular Cluttered Flight |  http://arxiv.org/abs/1411.6326  | author:Debadeepta Dey, Kumar Shaurya Shankar, Sam Zeng, Rupesh Mehta, M. Talha Agcayazi, Christopher Eriksen, Shreyansh Daftry, Martial Hebert, J. Andrew Bagnell category:cs.RO cs.CV cs.LG published:2014-11-24 summary:Cameras provide a rich source of information while being passive, cheap andlightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this workwe present the first implementation of receding horizon control, which iswidely used in ground vehicles, with monocular vision as the only sensing modefor autonomous UAV flight in dense clutter. We make it feasible on UAVs via anumber of contributions: novel coupling of perception and control via relevantand diverse, multiple interpretations of the scene around the robot, leveragingrecent advances in machine learning to showcase anytime budgeted cost-sensitivefeature selection, and fast non-linear regression for monocular depthprediction. We empirically demonstrate the efficacy of our novel pipeline viareal world experiments of more than 2 kms through dense trees with a quadrotorbuilt from off-the-shelf parts. Moreover our pipeline is designed to combineinformation from other modalities like stereo and lidar as well if available.
arxiv-1411-6400 | Mutual Information-Based Unsupervised Feature Transformation for Heterogeneous Feature Subset Selection |  http://arxiv.org/abs/1411.6400  | author:Min Wei, Tommy W. S. Chow, Rosa H. M. Chan category:stat.ML cs.LG published:2014-11-24 summary:Conventional mutual information (MI) based feature selection (FS) methods areunable to handle heterogeneous feature subset selection properly because ofdata format differences or estimation methods of MI between feature subset andclass label. A way to solve this problem is feature transformation (FT). Inthis study, a novel unsupervised feature transformation (UFT) which cantransform non-numerical features into numerical features is developed andtested. The UFT process is MI-based and independent of class label. MI-based FSalgorithms, such as Parzen window feature selector (PWFS), minimum redundancymaximum relevance feature selection (mRMR), and normalized MI feature selection(NMIFS), can all adopt UFT for pre-processing of non-numerical features. Unliketraditional FT methods, the proposed UFT is unbiased while PWFS is utilized toits full advantage. Simulations and analyses of large-scale datasets showedthat feature subset selected by the integrated method, UFT-PWFS, outperformedother FT-FS integrated methods in classification accuracy.
arxiv-1411-6387 | Deep Convolutional Neural Fields for Depth Estimation from a Single Image |  http://arxiv.org/abs/1411.6387  | author:Fayao Liu, Chunhua Shen, Guosheng Lin category:cs.CV published:2014-11-24 summary:We consider the problem of depth estimation from a single monocular image inthis work. It is a challenging task as no reliable depth cues are available,e.g., stereo correspondences, motions, etc. Previous efforts have been focusingon exploiting geometric priors or additional sources of information, with allusing hand-crafted features. Recently, there is mounting evidence that featuresfrom deep convolutional neural networks (CNN) are setting new records forvarious vision applications. On the other hand, considering the continuouscharacteristic of the depth values, depth estimations can be naturallyformulated into a continuous conditional random field (CRF) learning problem.Therefore, we in this paper present a deep convolutional neural field model forestimating depths from a single image, aiming to jointly explore the capacityof deep CNN and continuous CRF. Specifically, we propose a deep structuredlearning scheme which learns the unary and pairwise potentials of continuousCRF in a unified deep CNN framework. The proposed method can be used for depth estimations of general scenes withno geometric priors nor any extra information injected. In our case, theintegral of the partition function can be analytically calculated, thus we canexactly solve the log-likelihood optimization. Moreover, solving the MAPproblem for predicting depths of a new image is highly efficient as closed-formsolutions exist. We experimentally demonstrate that the proposed methodoutperforms state-of-the-art depth estimation methods on both indoor andoutdoor scene datasets.
arxiv-1411-6203 | Efficient Minimax Signal Detection on Graphs |  http://arxiv.org/abs/1411.6203  | author:Jing Qian, Venkatesh Saligrama category:stat.ML published:2014-11-23 summary:Several problems such as network intrusion, community detection, and diseaseoutbreak can be described by observations attributed to nodes or edges of agraph. In these applications presence of intrusion, community or diseaseoutbreak is characterized by novel observations on some unknown connectedsubgraph. These problems can be formulated in terms of optimization of suitableobjectives on connected subgraphs, a problem which is generally computationallydifficult. We overcome the combinatorics of connectivity by embedding connectedsubgraphs into linear matrix inequalities (LMI). Computationally efficienttests are then realized by optimizing convex objective functions subject tothese LMI constraints. We prove, by means of a novel Euclidean embeddingargument, that our tests are minimax optimal for exponential family ofdistributions on 1-D and 2-D lattices. We show that internal conductance of theconnected subgraph family plays a fundamental role in characterizingdetectability.
arxiv-1411-6314 | On the High-dimensional Power of Linear-time Kernel Two-Sample Testing under Mean-difference Alternatives |  http://arxiv.org/abs/1411.6314  | author:Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry Wasserman category:math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH published:2014-11-23 summary:Nonparametric two sample testing deals with the question of consistentlydeciding if two distributions are different, given samples from both, withoutmaking any parametric assumptions about the form of the distributions. Thecurrent literature is split into two kinds of tests - those which areconsistent without any assumptions about how the distributions may differ(\textit{general} alternatives), and those which are designed to specificallytest easier alternatives, like a difference in means (\textit{mean-shift}alternatives). The main contribution of this paper is to explicitly characterize the powerof a popular nonparametric two sample test, designed for general alternatives,under a mean-shift alternative in the high-dimensional setting. Specifically,we explicitly derive the power of the linear-time Maximum Mean Discrepancystatistic using the Gaussian kernel, where the dimension and sample size canboth tend to infinity at any rate, and the two distributions differ in theirmeans. As a corollary, we find that if the signal-to-noise ratio is heldconstant, then the test's power goes to one if the number of samples increasesfaster than the dimension increases. This is the first explicit powerderivation for a general nonparametric test in the high-dimensional setting,and also the first analysis of how tests designed for general alternativesperform when faced with easier ones.
arxiv-1411-6307 | Diversifying Sparsity Using Variational Determinantal Point Processes |  http://arxiv.org/abs/1411.6307  | author:Nematollah Kayhan Batmanghelich, Gerald Quon, Alex Kulesza, Manolis Kellis, Polina Golland, Luke Bornn category:cs.LG cs.AI stat.ML published:2014-11-23 summary:We propose a novel diverse feature selection method based on determinantalpoint processes (DPPs). Our model enables one to flexibly define diversitybased on the covariance of features (similar to orthogonal matching pursuit) oralternatively based on side information. We introduce our approach in thecontext of Bayesian sparse regression, employing a DPP as a variationalapproximation to the true spike and slab posterior distribution. Wesubsequently show how this variational DPP approximation generalizes andextends mean-field approximation, and can be learned efficiently by exploitingthe fast sampling properties of DPPs. Our motivating application comes frombioinformatics, where we aim to identify a diverse set of genes whoseexpression profiles predict a tumor type where the diversity is defined withrespect to a gene-gene interaction network. We also explore an application inspatial statistics. In both cases, we demonstrate that the proposed methodyields significantly more diverse feature sets than classic sparse methods,without compromising accuracy.
arxiv-1411-6305 | Revenue Optimization in Posted-Price Auctions with Strategic Buyers |  http://arxiv.org/abs/1411.6305  | author:Mehryar Mohri, Andres Muñoz Medina category:cs.LG published:2014-11-23 summary:We study revenue optimization learning algorithms for posted-price auctionswith strategic buyers. We analyze a very broad family of monotone regretminimization algorithms for this problem, which includes the previously bestknown algorithm, and show that no algorithm in that family admits a strategicregret more favorable than $\Omega(\sqrt{T})$. We then introduce a newalgorithm that achieves a strategic regret differing from the lower bound onlyby a factor in $O(\log T)$, an exponential improvement upon the previous bestalgorithm. Our new algorithm admits a natural analysis and simpler proofs, andthe ideas behind its design are general. We also report the results ofempirical evaluations comparing our algorithm with the previous state of theart and show a consistent exponential improvement in several differentscenarios.
arxiv-1411-6308 | A Convex Formulation for Spectral Shrunk Clustering |  http://arxiv.org/abs/1411.6308  | author:Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang, Xiaofang Zhou category:cs.LG published:2014-11-23 summary:Spectral clustering is a fundamental technique in the field of data miningand information processing. Most existing spectral clustering algorithmsintegrate dimensionality reduction into the clustering process assisted bymanifold learning in the original space. However, the manifold inreduced-dimensional subspace is likely to exhibit altered properties incontrast with the original space. Thus, applying manifold information obtainedfrom the original space to the clustering process in a low-dimensional subspaceis prone to inferior performance. Aiming to address this issue, we propose anovel convex algorithm that mines the manifold structure in the low-dimensionalsubspace. In addition, our unified learning process makes the manifold learningparticularly tailored for the clustering. Compared with other related methods,the proposed algorithm results in more structured clustering result. Tovalidate the efficacy of the proposed algorithm, we perform extensiveexperiments on several benchmark datasets in comparison with somestate-of-the-art clustering approaches. The experimental results demonstratethat the proposed algorithm has quite promising clustering performance.
arxiv-1411-6311 | Optimal variable selection in multi-group sparse discriminant analysis |  http://arxiv.org/abs/1411.6311  | author:Irina Gaynanova, Mladen Kolar category:stat.ML published:2014-11-23 summary:This article considers the problem of multi-group classification in thesetting where the number of variables $p$ is larger than the number ofobservations $n$. Several methods have been proposed in the literature thataddress this problem, however their variable selection performance is eitherunknown or suboptimal to the results known in the two-group case. In this workwe provide sharp conditions for the consistent recovery of relevant variablesin the multi-group case using the discriminant analysis proposal of Gaynanovaet al., 2014. We achieve the rates of convergence that attain the optimalscaling of the sample size $n$, number of variables $p$ and the sparsity level$s$. These rates are significantly faster than the best known results in themulti-group case. Moreover, they coincide with the optimal minimax rates forthe two-group case. We validate our theoretical results with numericalanalysis.
arxiv-1411-6206 | Low-Rank and Sparse Matrix Decomposition with a-priori knowledge for Dynamic 3D MRI reconstruction |  http://arxiv.org/abs/1411.6206  | author:Dornoosh Zonoobi, Shahrooz Faghih Roohi, Ashraf A. Kassim category:cs.CV published:2014-11-23 summary:It has been recently shown that incorporating priori knowledge significantlyimproves the performance of basic compressive sensing based approaches. We havemanaged to successfully exploit this idea for recovering a matrix as asummation of a Low-rank and a Sparse component from compressive measurements.When applied to the problem of construction of 4D Cardiac MR image sequences inreal-time from highly under-sampled $k-$space data, our proposed methodachieves superior reconstruction quality compared to the other state-of-the-artmethods.
arxiv-1411-6228 | From Image-level to Pixel-level Labeling with Convolutional Networks |  http://arxiv.org/abs/1411.6228  | author:Pedro O. Pinheiro, Ronan Collobert category:cs.CV published:2014-11-23 summary:We are interested in inferring object segmentation by leveraging only objectclass information, and by considering only minimal priors on the objectsegmentation task. This problem could be viewed as a kind of weakly supervisedsegmentation task, and naturally fits the Multiple Instance Learning (MIL)framework: every training image is known to have (or not) at least one pixelcorresponding to the image class label, and the segmentation task can berewritten as inferring the pixels belonging to the class of the object (givenone image, and its object class). We propose a Convolutional NeuralNetwork-based model, which is constrained during training to put more weight onpixels which are important for classifying the image. We show that at testtime, the model has learned to discriminate the right pixels well enough, suchthat it performs very well on an existing segmentation benchmark, by addingonly few smoothing priors. Our system is trained using a subset of the Imagenetdataset and the segmentation experiments are performed on the challengingPascal VOC dataset (with no fine-tuning of the model on Pascal VOC). Our modelbeats the state of the art results in weakly supervised object segmentationtask by a large margin. We also compare the performance of our model with stateof the art fully-supervised segmentation approaches.
arxiv-1411-6285 | Target Fishing: A Single-Label or Multi-Label Problem? |  http://arxiv.org/abs/1411.6285  | author:Avid M. Afzal, Hamse Y. Mussa, Richard E. Turner, Andreas Bender, Robert C. Glen category:q-bio.BM cs.LG stat.ML published:2014-11-23 summary:According to Cobanoglu et al and Murphy, it is now widely acknowledged thatthe single target paradigm (one protein or target, one disease, one drug) thathas been the dominant premise in drug development in the recent past isuntenable. More often than not, a drug-like compound (ligand) can bepromiscuous - that is, it can interact with more than one target protein. Inrecent years, in in silico target prediction methods the promiscuity issue hasbeen approached computationally in different ways. In this study we confineattention to the so-called ligand-based target prediction machine learningapproaches, commonly referred to as target-fishing. With a few exceptions, thetarget-fishing approaches that are currently ubiquitous in cheminformaticsliterature can be essentially viewed as single-label multi-classificationschemes; these approaches inherently bank on the single target paradigmassumption that a ligand can home in on one specific target. In order toaddress the ligand promiscuity issue, one might be able to cast target-fishingas a multi-label multi-class classification problem. For illustrative andcomparison purposes, single-label and multi-label Naive Bayes classificationmodels (denoted here by SMM and MMM, respectively) for target-fishing wereimplemented. The models were constructed and tested on 65,587 compounds and 308targets retrieved from the ChEMBL17 database. SMM and MMM performeddifferently: for 16,344 test compounds, the MMM model returned recall andprecision values of 0.8058 and 0.6622, respectively; the corresponding recalland precision values yielded by the SMM model were 0.7805 and 0.7596,respectively. However, at a significance level of 0.05 and one degree offreedom McNemar test performed on the target prediction results returned by SMMand MMM for the 16,344 test ligands gave a chi-squared value of 15.656, infavour of the MMM approach.
arxiv-1411-6233 | A Convex Sparse PCA for Feature Analysis |  http://arxiv.org/abs/1411.6233  | author:Xiaojun Chang, Feiping Nie, Yi Yang, Heng Huang category:cs.LG published:2014-11-23 summary:Principal component analysis (PCA) has been widely applied to dimensionalityreduction and data pre-processing for different applications in engineering,biology and social science. Classical PCA and its variants seek for linearprojections of the original variables to obtain a low dimensional featurerepresentation with maximal variance. One limitation is that it is verydifficult to interpret the results of PCA. In addition, the classical PCA isvulnerable to certain noisy data. In this paper, we propose a convex sparseprincipal component analysis (CSPCA) algorithm and apply it to featureanalysis. First we show that PCA can be formulated as a low-rank regressionoptimization problem. Based on the discussion, the l 2 , 1 -norm minimizationis incorporated into the objective function to make the regression coefficientssparse, thereby robust to the outliers. In addition, based on the sparse modelused in CSPCA, an optimal weight is assigned to each of the original feature,which in turn provides the output with good interpretability. With the outputof our CSPCA, we can effectively analyze the importance of each feature underthe PCA criteria. The objective function is convex, and we propose an iterativealgorithm to optimize it. We apply the CSPCA algorithm to feature selection andconduct extensive experiments on six different benchmark datasets. Experimentalresults demonstrate that the proposed algorithm outperforms state-of-the-artunsupervised feature selection algorithms.
arxiv-1411-6275 | Detection of Non-Stationary Photometric Perturbations on Projection Screens |  http://arxiv.org/abs/1411.6275  | author:Miguel Castañeda-Garay, Oscar Belmonte-Fernández, Hebert Pérez-Rosés, Antonio Diaz-Tula category:cs.CV H.5.2; I.4.1 published:2014-11-23 summary:Interfaces based on projection screens have become increasingly more popularin recent years, mainly due to the large screen size and resolution that theyprovide, as well as their stereo-vision capabilities. This work shows a localmethod for real-time detection of non-stationary photometric perturbations inprojected images by means of computer vision techniques. The method is based onthe computation of differences between the images in the projector's framebuffer and the corresponding images on the projection screen observed by thecamera. It is robust under spatial variations in the intensity of light emittedby the projector on the projection surface and also robust under stationaryphotometric perturbations caused by external factors. Moreover, we describe theexperiments carried out to show the reliability of the method.
arxiv-1411-6231 | Compound Rank-k Projections for Bilinear Analysis |  http://arxiv.org/abs/1411.6231  | author:Xiaojun Chang, Feiping Nie, Sen Wang, Yi Yang, Xiaofang Zhou, Chengqi Zhang category:cs.LG published:2014-11-23 summary:In many real-world applications, data are represented by matrices orhigh-order tensors. Despite the promising performance, the existingtwo-dimensional discriminant analysis algorithms employ a single projectionmodel to exploit the discriminant information for projection, making the modelless flexible. In this paper, we propose a novel Compound Rank-k Projection(CRP) algorithm for bilinear analysis. CRP deals with matrices directly withouttransforming them into vectors, and it therefore preserves the correlationswithin the matrix and decreases the computation complexity. Different from theexisting two dimensional discriminant analysis algorithms, objective functionvalues of CRP increase monotonically.In addition, CRP utilizes multiple rank-kprojection models to enable a larger search space in which the optimal solutioncan be found. In this way, the discriminant ability is enhanced.
arxiv-1411-6235 | Balanced k-Means and Min-Cut Clustering |  http://arxiv.org/abs/1411.6235  | author:Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang category:cs.LG published:2014-11-23 summary:Clustering is an effective technique in data mining to generate groups thatare the matter of interest. Among various clustering approaches, the family ofk-means algorithms and min-cut algorithms gain most popularity due to theirsimplicity and efficacy. The classical k-means algorithm partitions a number ofdata points into several subsets by iteratively updating the clustering centersand the associated data points. By contrast, a weighted undirected graph isconstructed in min-cut algorithms which partition the vertices of the graphinto two sets. However, existing clustering algorithms tend to cluster minorityof data points into a subset, which shall be avoided when the target dataset isbalanced. To achieve more accurate clustering for balanced dataset, we proposeto leverage exclusive lasso on k-means and min-cut to regulate the balancedegree of the clustering results. By optimizing our objective functions thatbuild atop the exclusive lasso, we can make the clustering result as muchbalanced as possible. Extensive experiments on several large-scale datasetsvalidate the advantage of the proposed algorithms compared to thestate-of-the-art clustering algorithms.
arxiv-1411-6232 | Semi-supervised Feature Analysis by Mining Correlations among Multiple Tasks |  http://arxiv.org/abs/1411.6232  | author:Xiaojun Chang, Yi Yang category:cs.LG published:2014-11-23 summary:In this paper, we propose a novel semi-supervised feature selection frameworkby mining correlations among multiple tasks and apply it to differentmultimedia applications. Instead of independently computing the importance offeatures for each task, our algorithm leverages shared knowledge from multiplerelated tasks, thus, improving the performance of feature selection. Note thatwe build our algorithm on assumption that different tasks share commonstructures. The proposed algorithm selects features in a batch mode, by whichthe correlations between different features are taken into consideration.Besides, considering the fact that labeling a large amount of training data inreal world is both time-consuming and tedious, we adopt manifold learning whichexploits both labeled and unlabeled training data for feature space analysis.Since the objective function is non-smooth and difficult to solve, we proposean iterative algorithm with fast convergence. Extensive experiments ondifferent applications demonstrate that our algorithm outperforms otherstate-of-the-art feature selection algorithms.
arxiv-1411-6243 | Structure Regularization for Structured Prediction: Theories and Experiments |  http://arxiv.org/abs/1411.6243  | author:Xu Sun category:cs.LG published:2014-11-23 summary:While there are many studies on weight regularization, the study on structureregularization is rare. Many existing systems on structured prediction focus onincreasing the level of structural dependencies within the model. However, thistrend could have been misdirected, because our study suggests that complexstructures are actually harmful to generalization ability in structuredprediction. To control structure-based overfitting, we propose a structureregularization framework via \emph{structure decomposition}, which decomposestraining samples into mini-samples with simpler structures, deriving a modelwith better generalization power. We show both theoretically and empiricallythat structure regularization can effectively control overfitting risk and leadto better accuracy. As a by-product, the proposed method can also substantiallyaccelerate the training speed. The method and the theoretical results can applyto general graphical models with arbitrary structures. Experiments onwell-known tasks demonstrate that our method can easily beat the benchmarksystems on those highly-competitive tasks, achieving state-of-the-artaccuracies yet with substantially faster training speed.
arxiv-1411-6241 | Improved Spectral Clustering via Embedded Label Propagation |  http://arxiv.org/abs/1411.6241  | author:Xiaojun Chang, Feiping Nie, Yi Yang, Heng Huang category:cs.LG published:2014-11-23 summary:Spectral clustering is a key research topic in the field of machine learningand data mining. Most of the existing spectral clustering algorithms are builtupon Gaussian Laplacian matrices, which are sensitive to parameters. We proposea novel parameter free, distance consistent Locally Linear Embedding. Theproposed distance consistent LLE promises that edges between closer data pointshave greater weight.Furthermore, we propose a novel improved spectralclustering via embedded label propagation. Our algorithm is built upon twoadvancements of the state of the art:1) label propagation,which propagates anode\'s labels to neighboring nodes according to their proximity; and 2)manifold learning, which has been widely used in its capacity to leverage themanifold structure of data points. First we perform standard spectralclustering on original data and assign each cluster to k nearest data points.Next, we propagate labels through dense, unlabeled data regions. Extensiveexperiments with various datasets validate the superiority of the proposedalgorithm compared to current state of the art spectral algorithms.
arxiv-1411-6191 | Kickback cuts Backprop's red-tape: Biologically plausible credit assignment in neural networks |  http://arxiv.org/abs/1411.6191  | author:David Balduzzi, Hastagiri Vanchinathan, Joachim Buhmann category:cs.LG cs.NE q-bio.NC published:2014-11-23 summary:Error backpropagation is an extremely effective algorithm for assigningcredit in artificial neural networks. However, weight updates under Backpropdepend on lengthy recursive computations and require separate output and errormessages -- features not shared by biological neurons, that are perhapsunnecessary. In this paper, we revisit Backprop and the credit assignmentproblem. We first decompose Backprop into a collection of interacting learningalgorithms; provide regret bounds on the performance of these sub-algorithms;and factorize Backprop's error signals. Using these results, we derive a newcredit assignment algorithm for nonparametric regression, Kickback, that issignificantly simpler than Backprop. Finally, we provide a sufficient conditionfor Kickback to follow error gradients, and show that Kickback matchesBackprop's performance on real-world regression benchmarks.
arxiv-1411-6067 | Viewpoints and Keypoints |  http://arxiv.org/abs/1411.6067  | author:Shubham Tulsiani, Jitendra Malik category:cs.CV published:2014-11-22 summary:We characterize the problem of pose estimation for rigid objects in terms ofdetermining viewpoint to explain coarse pose and keypoint prediction to capturethe finer details. We address both these tasks in two different settings - theconstrained setting with known bounding boxes and the more challengingdetection setting where the aim is to simultaneously detect and correctlyestimate pose of objects. We present Convolutional Neural Network basedarchitectures for these and demonstrate that leveraging viewpoint estimates cansubstantially improve local appearance based keypoint predictions. In additionto achieving significant improvements over state-of-the-art in the above tasks,we analyze the error modes and effect of object characteristics on performanceto guide future efforts towards this goal.
arxiv-1411-6156 | Efficiently learning Ising models on arbitrary graphs |  http://arxiv.org/abs/1411.6156  | author:Guy Bresler category:cs.LG cs.IT math.IT stat.ML published:2014-11-22 summary:We consider the problem of reconstructing the graph underlying an Ising modelfrom i.i.d. samples. Over the last fifteen years this problem has been ofsignificant interest in the statistics, machine learning, and statisticalphysics communities, and much of the effort has been directed towards findingalgorithms with low computational cost for various restricted classes ofmodels. Nevertheless, for learning Ising models on general graphs with $p$nodes of degree at most $d$, it is not known whether or not it is possible toimprove upon the $p^{d}$ computation needed to exhaustively search over allpossible neighborhoods for each node. In this paper we show that a simple greedy procedure allows to learn thestructure of an Ising model on an arbitrary bounded-degree graph in time on theorder of $p^2$. We make no assumptions on the parameters except what isnecessary for identifiability of the model, and in particular the results holdat low-temperatures as well as for highly non-uniform models. The proof restson a new structural property of Ising models: we show that for any node thereexists at least one neighbor with which it has a high mutual information. Thisstructural property may be of independent interest.
arxiv-1411-6081 | PU Learning for Matrix Completion |  http://arxiv.org/abs/1411.6081  | author:Cho-Jui Hsieh, Nagarajan Natarajan, Inderjit S. Dhillon category:cs.LG cs.NA stat.ML published:2014-11-22 summary:In this paper, we consider the matrix completion problem when theobservations are one-bit measurements of some underlying matrix M, and inparticular the observed samples consist only of ones and no zeros. This problemis motivated by modern applications such as recommender systems and socialnetworks where only "likes" or "friendships" are observed. The problem oflearning from only positive and unlabeled examples, called PU(positive-unlabeled) learning, has been studied in the context of binaryclassification. We consider the PU matrix completion problem, where anunderlying real-valued matrix M is first quantized to generate one-bitobservations and then a subset of positive entries is revealed. Under theassumption that M has bounded nuclear norm, we provide recovery guarantees fortwo different observation models: 1) M parameterizes a distribution thatgenerates a binary matrix, 2) M is thresholded to obtain a binary matrix. Forthe first case, we propose a "shifted matrix completion" method that recovers Musing only a subset of indices corresponding to ones, while for the secondcase, we propose a "biased matrix completion" method that recovers the(thresholded) binary matrix. Both methods yield strong error bounds --- if M isn by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotesthe fraction of ones observed. This implies a sample complexity of O(n\log n)ones to achieve a small error, when M is dense and n is large. We extend ourmethods and guarantees to the inductive matrix completion problem, where rowsand columns of M have associated features. We provide efficient and scalableoptimization procedures for both the methods and demonstrate the effectivenessof the proposed methods for link prediction (on real-world networks consistingof over 2 million nodes and 90 million links) and semi-supervised clusteringtasks.
arxiv-1411-6069 | Category-Specific Object Reconstruction from a Single Image |  http://arxiv.org/abs/1411.6069  | author:Abhishek Kar, Shubham Tulsiani, João Carreira, Jitendra Malik category:cs.CV published:2014-11-22 summary:Object reconstruction from a single image -- in the wild -- is a problemwhere we can make progress and get meaningful results today. This is the mainmessage of this paper, which introduces an automated pipeline with pixels asinputs and 3D surfaces of various rigid categories as outputs in images ofrealistic scenes. At the core of our approach are deformable 3D models that canbe learned from 2D annotations available in existing object detection datasets,that can be driven by noisy automatic object segmentations and which wecomplement with a bottom-up module for recovering high-frequency shape details.We perform a comprehensive quantitative analysis and ablation study of ourapproach using the recently introduced PASCAL 3D+ dataset and show veryencouraging automatic reconstructions on PASCAL VOC.
arxiv-1411-6160 | Characterization of the equivalence of robustification and regularization in linear, median, and matrix regression |  http://arxiv.org/abs/1411.6160  | author:Dimitris Bertsimas, Martin S. Copenhaver category:math.ST cs.LG math.OC stat.ML stat.TH published:2014-11-22 summary:Sparsity is a key driver in modern statistical problems, from linearregression via the Lasso to matrix regression with nuclear norm penalties inmatrix completion and beyond. In stark contrast to sparsity motivations forsuch problems, it is known in the field of robust optimization that a varietyof vector regression problems, such as Lasso which appears as a loss functionplus a regularization penalty, can arise by simply immunizing a nominal problem(with only a loss function) to uncertainty in the data. Such a robustificationoffers an explanation for why some linear regression methods perform well inthe face of noise, even when these methods do not produce reliably sparsesolutions. In this paper we deepen and extend the understanding of theconnection between robustification and regularization in regression problems.Specifically, (a) in the context of linear regression, we characterize underwhich conditions on the model of uncertainty used and on the loss functionpenalties robustification and regularization are equivalent; (b) we show how totractably robustify median regression problems; and (c) we extend thecharacterization of robustification and regularization to matrix regressionproblems (matrix completion and Principal Component Analysis).
arxiv-1411-6091 | Virtual View Networks for Object Reconstruction |  http://arxiv.org/abs/1411.6091  | author:João Carreira, Abhishek Kar, Shubham Tulsiani, Jitendra Malik category:cs.CV published:2014-11-22 summary:All that structure from motion algorithms "see" are sets of 2D points. Weshow that these impoverished views of the world can be faked for the purpose ofreconstructing objects in challenging settings, such as from a single image, orfrom a few ones far apart, by recognizing the object and getting help from acollection of images of other objects from the same class. We synthesizevirtual views by computing geodesics on novel networks connecting objects withsimilar viewpoints, and introduce techniques to increase the specificity androbustness of factorization-based object reconstruction in this setting. Wereport accurate object shape reconstruction from a single image on challengingPASCAL VOC data, which suggests that the current domain of applications ofrigid structure-from-motion techniques may be significantly extended.
arxiv-1411-5732 | A Joint Probabilistic Classification Model of Relevant and Irrelevant Sentences in Mathematical Word Problems |  http://arxiv.org/abs/1411.5732  | author:Suleyman Cetintas, Luo Si, Yan Ping Xin, Dake Zhang, Joo Young Park, Ron Tzur category:cs.CL cs.IR cs.LG stat.ML published:2014-11-21 summary:Estimating the difficulty level of math word problems is an important taskfor many educational applications. Identification of relevant and irrelevantsentences in math word problems is an important step for calculating thedifficulty levels of such problems. This paper addresses a novel application oftext categorization to identify two types of sentences in mathematical wordproblems, namely relevant and irrelevant sentences. A novel joint probabilisticclassification model is proposed to estimate the joint probability ofclassification decisions for all sentences of a math word problem by utilizingthe correlation among all sentences along with the correlation between thequestion sentence and other sentences, and sentence text. The proposed model iscompared with i) a SVM classifier which makes independent classificationdecisions for individual sentences by only using the sentence text and ii) anovel SVM classifier that considers the correlation between the questionsentence and other sentences along with the sentence text. An extensive set ofexperiments demonstrates the effectiveness of the joint probabilisticclassification model for identifying relevant and irrelevant sentences as wellas the novel SVM classifier that utilizes the correlation between the questionsentence and other sentences. Furthermore, empirical results and analysis showthat i) it is highly beneficial not to remove stopwords and ii) utilizing partof speech tagging does not make a significant improvement although it has beenshown to be effective for the related task of math word problem typeclassification.
arxiv-1411-5796 | Pre-processing of Domain Ontology Graph Generation System in Punjabi |  http://arxiv.org/abs/1411.5796  | author:Rajveer Kaur, Saurabh Sharma category:cs.CL published:2014-11-21 summary:This paper describes pre-processing phase of ontology graph generation systemfrom Punjabi text documents of different domains. This research paper focuseson pre-processing of Punjabi text documents. Pre-processing is structuredrepresentation of the input text. Pre-processing of ontology graph generationincludes allowing input restrictions to the text, removal of special symbolsand punctuation marks, removal of duplicate terms, removal of stop words,extract terms by matching input terms with dictionary and gazetteer liststerms.
arxiv-1411-5825 | Assessment of algorithms for mitosis detection in breast cancer histopathology images |  http://arxiv.org/abs/1411.5825  | author:Mitko Veta, Paul J. van Diest, Stefan M. Willems, Haibo Wang, Anant Madabhushi, Angel Cruz-Roa, Fabio Gonzalez, Anders B. L. Larsen, Jacob S. Vestergaard, Anders B. Dahl, Dan C. Cireşan, Jürgen Schmidhuber, Alessandro Giusti, Luca M. Gambardella, F. Boray Tek, Thomas Walter, Ching-Wei Wang, Satoshi Kondo, Bogdan J. Matuszewski, Frederic Precioso, Violet Snell, Josef Kittler, Teofilo E. de Campos, Adnan M. Khan, Nasir M. Rajpoot, Evdokia Arkoumani, Miangela M. Lacle, Max A. Viergever, Josien P. W. Pluim category:cs.CV published:2014-11-21 summary:The proliferative activity of breast tumors, which is routinely estimated bycounting of mitotic figures in hematoxylin and eosin stained histologysections, is considered to be one of the most important prognostic markers.However, mitosis counting is laborious, subjective and may suffer from lowinter-observer agreement. With the wider acceptance of whole slide images inpathology labs, automatic image analysis has been proposed as a potentialsolution for these issues. In this paper, the results from the Assessment ofMitosis Detection Algorithms 2013 (AMIDA13) challenge are described. Thechallenge was based on a data set consisting of 12 training and 11 testingsubjects, with more than one thousand annotated mitotic figures by multipleobservers. Short descriptions and results from the evaluation of eleven methodsare presented. The top performing method has an error rate that is comparableto the inter-observer agreement among pathologists.
arxiv-1411-5873 | Randomized Dual Coordinate Ascent with Arbitrary Sampling |  http://arxiv.org/abs/1411.5873  | author:Zheng Qu, Peter Richtárik, Tong Zhang category:math.OC cs.LG cs.NA math.NA published:2014-11-21 summary:We study the problem of minimizing the average of a large number of smoothconvex functions penalized with a strongly convex regularizer. We propose andanalyze a novel primal-dual method (Quartz) which at every iteration samplesand updates a random subset of the dual variables, chosen according to anarbitrary distribution. In contrast to typical analysis, we directly bound thedecrease of the primal-dual error (in expectation), without the need to firstanalyze the dual error. Depending on the choice of the sampling, we obtainefficient serial, parallel and distributed variants of the method. In theserial case, our bounds match the best known bounds for SDCA (both with uniformand importance sampling). With standard mini-batching, our bounds predictinitial data-independent speedup as well as additional data-driven speedupwhich depends on spectral and sparsity properties of the data. We calculatetheoretical speedup factors and find that they are excellent predictors ofactual speedup in practice. Moreover, we illustrate that it is possible todesign an efficient mini-batch importance sampling. The distributed variant ofQuartz is the first distributed SDCA-like method with an analysis fornon-separable data.
arxiv-1411-5928 | Learning to Generate Chairs, Tables and Cars with Convolutional Networks |  http://arxiv.org/abs/1411.5928  | author:Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko, Thomas Brox category:cs.CV cs.LG cs.NE published:2014-11-21 summary:We train generative 'up-convolutional' neural networks which are able togenerate images of objects given object style, viewpoint, and color. We trainthe networks on rendered 3D models of chairs, tables, and cars. Our experimentsshow that the networks do not merely learn all images by heart, but rather finda meaningful representation of 3D models allowing them to assess the similarityof different models, interpolate between given views to generate the missingones, extrapolate views, and invent new objects not present in the training setby recombining training instances, or even two different object classes.Moreover, we show that such generative networks can be used to findcorrespondences between different objects from the dataset, outperformingexisting approaches on this task.
arxiv-1411-6031 | Finding Action Tubes |  http://arxiv.org/abs/1411.6031  | author:Georgia Gkioxari, Jitendra Malik category:cs.CV published:2014-11-21 summary:We address the problem of action detection in videos. Driven by the latestprogress in object detection from 2D images, we build action models using richfeature hierarchies derived from shape and kinematic cues. We incorporateappearance and motion in two ways. First, starting from image region proposalswe select those that are motion salient and thus are more likely to contain theaction. This leads to a significant reduction in the number of regions beingprocessed and allows for faster computations. Second, we extractspatio-temporal feature representations to build strong classifiers usingConvolutional Neural Networks. We link our predictions to produce detectionsconsistent in time, which we call action tubes. We show that our approachoutperforms other techniques in the task of action detection.
arxiv-1411-5915 | Robust EM kernel-based methods for linear system identification |  http://arxiv.org/abs/1411.5915  | author:Giulio Bottegal, Aleksandr Y. Aravkin, Håkan Hjalmarsson, Gianluigi Pillonetto category:cs.SY stat.ML published:2014-11-21 summary:Recent developments in system identifi?cation have brought attention toregularized kernel-based methods. This type of approach has been proven tocompare favorably with classic parametric methods. However, currentformulations are not robust with respect to outliers. In this paper, weintroduce a novel method to robustify kernel-based system identi?cationmethods. To this end, we model the output measurement noise using randomvariables with heavy-tailed probability density functions (pdfs), focusing onthe Laplacian and the Student's t distributions. Exploiting the representationof these pdfs as scale mixtures of Gaussians, we cast our systemidentifi?cation problem into a Gaussian process regression framework, whichrequires estimating a number of hyperparameters of the data size order. Toovercome this di?culty, we design a new maximum a posteriori (MAP) estimator ofthe hyperparameters, and solve the related optimization problem with a noveliterative scheme based on the Expectation-Maximization (EM) method. In presenceof outliers, tests on simulated data and on a real system show a substantialperformance improvement compared to currently used kernel-based methods forlinear system identifi?cation.
arxiv-1411-5752 | Hypercolumns for Object Segmentation and Fine-grained Localization |  http://arxiv.org/abs/1411.5752  | author:Bharath Hariharan, Pablo Arbeláez, Ross Girshick, Jitendra Malik category:cs.CV published:2014-11-21 summary:Recognition algorithms based on convolutional networks (CNNs) typically usethe output of the last layer as feature representation. However, theinformation in this layer may be too coarse to allow precise localization. Onthe contrary, earlier layers may be precise in localization but will notcapture semantics. To get the best of both worlds, we define the hypercolumn ata pixel as the vector of activations of all CNN units above that pixel. Usinghypercolumns as pixel descriptors, we show results on three fine-grainedlocalization tasks: simultaneous detection and segmentation[22], where weimprove state-of-the-art from 49.7[22] mean AP^r to 60.0, keypointlocalization, where we get a 3.3 point boost over[20] and part labeling, wherewe show a 6.6 point gain over a strong baseline.
arxiv-1411-5737 | Fuzzy Adaptive Resonance Theory, Diffusion Maps and their applications to Clustering and Biclustering |  http://arxiv.org/abs/1411.5737  | author:S. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu category:cs.NE cs.LG published:2014-11-21 summary:In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive ResonanceDif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theoryto do clustering on high dimensional data. We describe some applications ofthis method and some problems for future research.
arxiv-1411-5731 | Visual Sentiment Prediction with Deep Convolutional Neural Networks |  http://arxiv.org/abs/1411.5731  | author:Can Xu, Suleyman Cetintas, Kuang-Chih Lee, Li-Jia Li category:cs.CV cs.NE stat.ML published:2014-11-21 summary:Images have become one of the most popular types of media through which usersconvey their emotions within online social networks. Although vast amount ofresearch is devoted to sentiment analysis of textual data, there has been verylimited work that focuses on analyzing sentiment of image data. In this work,we propose a novel visual sentiment prediction framework that performs imageunderstanding with Deep Convolutional Neural Networks (CNN). Specifically, theproposed sentiment prediction framework performs transfer learning from a CNNwith millions of parameters, which is pre-trained on large-scale data forobject recognition. Experiments conducted on two real-world datasets fromTwitter and Tumblr demonstrate the effectiveness of the proposed visualsentiment analysis framework.
arxiv-1411-5977 | On the Impossibility of Convex Inference in Human Computation |  http://arxiv.org/abs/1411.5977  | author:Nihar B. Shah, Dengyong Zhou category:stat.ML cs.HC cs.LG published:2014-11-21 summary:Human computation or crowdsourcing involves joint inference of theground-truth-answers and the worker-abilities by optimizing an objectivefunction, for instance, by maximizing the data likelihood based on an assumedunderlying model. A variety of methods have been proposed in the literature toaddress this inference problem. As far as we know, none of the objectivefunctions in existing methods is convex. In machine learning and appliedstatistics, a convex function such as the objective function of support vectormachines (SVMs) is generally preferred, since it can leverage thehigh-performance algorithms and rigorous guarantees established in theextensive literature on convex optimization. One may thus wonder if thereexists a meaningful convex objective function for the inference problem inhuman computation. In this paper, we investigate this convexity issue for humancomputation. We take an axiomatic approach by formulating a set of axioms thatimpose two mild and natural assumptions on the objective function for theinference. Under these axioms, we show that it is unfortunately impossible toensure convexity of the inference problem. On the other hand, we show thatinterestingly, in the absence of a requirement to model "spammers", one canconstruct reasonable objective functions for crowdsourcing that guaranteeconvex inference.
arxiv-1411-5899 | Falling Rule Lists |  http://arxiv.org/abs/1411.5899  | author:Fulton Wang, Cynthia Rudin category:cs.AI cs.LG published:2014-11-21 summary:Falling rule lists are classification models consisting of an ordered list ofif-then rules, where (i) the order of rules determines which example should beclassified by each rule, and (ii) the estimated probability of successdecreases monotonically down the list. These kinds of rule lists are inspiredby healthcare applications where patients would be stratified into risk setsand the highest at-risk patients should be considered first. We provide aBayesian framework for learning falling rule lists that does not rely ontraditional greedy decision tree learning methods.
arxiv-1411-5908 | Understanding image representations by measuring their equivariance and equivalence |  http://arxiv.org/abs/1411.5908  | author:Karel Lenc, Andrea Vedaldi category:cs.CV cs.LG cs.NE published:2014-11-21 summary:Despite the importance of image representations such as histograms oforiented gradients and deep Convolutional Neural Networks (CNN), ourtheoretical understanding of them remains limited. Aiming at filling this gap,we investigate three key mathematical properties of representations:equivariance, invariance, and equivalence. Equivariance studies howtransformations of the input image are encoded by the representation,invariance being a special case where a transformation has no effect.Equivalence studies whether two representations, for example two differentparametrisations of a CNN, capture the same visual information or not. A numberof methods to establish these properties empirically are proposed, includingintroducing transformation and stitching layers in CNNs. These methods are thenapplied to popular representations to reveal insightful aspects of theirstructure, including clarifying at which layers in a CNN certain geometricinvariances are achieved. While the focus of the paper is theoretical, directapplications to structured-output regression are demonstrated too.
arxiv-1411-5799 | Group Factor Analysis |  http://arxiv.org/abs/1411.5799  | author:Arto Klami, Seppo Virtanen, Eemeli Leppäaho, Samuel Kaski category:stat.ML published:2014-11-21 summary:Factor analysis provides linear factors that describe relationships betweenindividual variables of a data set. We extend this classical formulation intolinear factors that describe relationships between groups of variables, whereeach group represents either a set of related variables or a data set. Themodel also naturally extends canonical correlation analysis to more than twosets, in a way that is more flexible than previous extensions. Our solution isformulated as variational inference of a latent variable model with structuralsparsity, and it consists of two hierarchical levels: The higher level modelsthe relationships between the groups, whereas the lower models the observedvariables given the higher level. We show that the resulting solution solvesthe group factor analysis problem accurately, outperforming alternative factoranalysis based solutions as well as more straightforward implementations ofgroup factor analysis. The method is demonstrated on two life science datasets, one on brain activation and the other on systems biology, illustratingits applicability to the analysis of different types of high-dimensional datasources.
arxiv-1411-5639 | N-sphere chord length distribution |  http://arxiv.org/abs/1411.5639  | author:Panagiotis Sidiropoulos category:math.PR stat.ML published:2014-11-20 summary:This work studies the chord length distribution, in the case where both endslie on a $N$-dimensional hypersphere ($N \geq 2$). Actually, after connectingthis distribution to the recently estimated surface of a hyperspherical cap\cite{SLi11}, closed-form expressions of both the probability density functionand the cumulative distribution function are straightforwardly extracted, whichare followed by a discussion on its basic properties, among which itsdependence from the hypersphere dimension. Additionally, the distribution ofthe dot product of unitary vectors is estimated, a problem that is related tothe chord length.
arxiv-1411-5881 | Hardware-Amenable Structural Learning for Spike-based Pattern Classification using a Simple Model of Active Dendrites |  http://arxiv.org/abs/1411.5881  | author:Shaista Hussain, Shih-Chii Liu, Arindam Basu category:cs.NE q-bio.NC published:2014-11-20 summary:This paper presents a spike-based model which employs neurons withfunctionally distinct dendritic compartments for classifying high dimensionalbinary patterns. The synaptic inputs arriving on each dendritic subunit arenonlinearly processed before being linearly integrated at the soma, giving theneuron a capacity to perform a large number of input-output mappings. The modelutilizes sparse synaptic connectivity; where each synapse takes a binary value.The optimal connection pattern of a neuron is learned by using a simplehardware-friendly, margin enhancing learning algorithm inspired by themechanism of structural plasticity in biological neurons. The learningalgorithm groups correlated synaptic inputs on the same dendritic branch. Sincethe learning results in modified connection patterns, it can be incorporatedinto current event-based neuromorphic systems with little overhead. This workalso presents a branch-specific spike-based version of this structuralplasticity rule. The proposed model is evaluated on benchmark binaryclassification problems and its performance is compared against that achievedusing Support Vector Machine (SVM) and Extreme Learning Machine (ELM)techniques. Our proposed method attains comparable performance while utilizing10 to 50% less computational resources than the other reported techniques.
arxiv-1411-5620 | Maximum Entropy Kernels for System Identification |  http://arxiv.org/abs/1411.5620  | author:Francesca Paola Carli, Tianshi Chen, Lennart Ljung category:math.OC cs.IT math.IT stat.ML published:2014-11-20 summary:A new nonparametric approach for system identification has been recentlyproposed where the impulse response is modeled as the realization of azero-mean Gaussian process whose covariance (kernel) has to be estimated fromdata. In this scheme, quality of the estimates crucially depends on theparametrization of the covariance of the Gaussian process. A family of kernelsthat have been shown to be particularly effective in the system identificationframework is the family of Diagonal/Correlated (DC) kernels. Maximum entropyproperties of a related family of kernels, the Tuned/Correlated (TC) kernels,have been recently pointed out in the literature. In this paper we show thatmaximum entropy properties indeed extend to the whole family of DC kernels. Themaximum entropy interpretation can be exploited in conjunction with results onmatrix completion problems in the graphical models literature to shed light onthe structure of the DC kernel. In particular, we prove that the DC kerneladmits a closed-form factorization, inverse and determinant. These results canbe exploited both to improve the numerical stability and to reduce thecomputational complexity associated with the computation of the DC estimator.
arxiv-1411-5595 | Linking GloVe with word2vec |  http://arxiv.org/abs/1411.5595  | author:Tianze Shi, Zhiyuan Liu category:cs.CL cs.LG stat.ML published:2014-11-20 summary:The Global Vectors for word representation (GloVe), introduced by JeffreyPennington et al. is reported to be an efficient and effective method forlearning vector representations of words. State-of-the-art performance is alsoprovided by skip-gram with negative-sampling (SGNS) implemented in the word2vectool. In this note, we explain the similarities between the training objectivesof the two models, and show that the objective of SGNS is similar to theobjective of a specialized form of GloVe, though their cost functions aredefined differently.
arxiv-1411-5417 | Private Empirical Risk Minimization Beyond the Worst Case: The Effect of the Constraint Set Geometry |  http://arxiv.org/abs/1411.5417  | author:Kunal Talwar, Abhradeep Thakurta, Li Zhang category:cs.LG cs.CR stat.ML published:2014-11-20 summary:Empirical Risk Minimization (ERM) is a standard technique in machinelearning, where a model is selected by minimizing a loss function overconstraint set. When the training dataset consists of private information, itis natural to use a differentially private ERM algorithm, and this problem hasbeen the subject of a long line of work started with Chaudhuri and Monteleoni2008. A private ERM algorithm outputs an approximate minimizer of the lossfunction and its error can be measured as the difference from the optimal valueof the loss function. When the constraint set is arbitrary, the required errorbounds are fairly well understood~\cite{BassilyST14}. In this work, we showthat the geometric properties of the constraint set can be used to derivesignificantly better results. Specifically, we show that a differentiallyprivate version of Mirror Descent leads to error bounds of the form$\tilde{O}(G_{\mathcal{C}}/n)$ for a lipschitz loss function, improving on the$\tilde{O}(\sqrt{p}/n)$ bounds in Bassily, Smith and Thakurta 2014. Here $p$ isthe dimensionality of the problem, $n$ is the number of data points in thetraining set, and $G_{\mathcal{C}}$ denotes the Gaussian width of theconstraint set that we optimize over. We show similar improvements for stronglyconvex functions, and for smooth functions. In addition, we show that when theloss function is Lipschitz with respect to the $\ell_1$ norm and $\mathcal{C}$is $\ell_1$-bounded, a differentially private version of the Frank-Wolfealgorithm gives error bounds of the form $\tilde{O}(n^{-2/3})$. This capturesthe important and common case of sparse linear regression (LASSO), when thedata $x_i$ satisfies $x_i_{\infty} \leq 1$ and we optimize over the $\ell_1$ball. We show new lower bounds for this setting, that together with knownbounds, imply that all our upper bounds are tight.
arxiv-1411-5428 | Differentially Private Algorithms for Empirical Machine Learning |  http://arxiv.org/abs/1411.5428  | author:Ben Stoddard, Yan Chen, Ashwin Machanavajjhala category:cs.LG published:2014-11-20 summary:An important use of private data is to build machine learning classifiers.While there is a burgeoning literature on differentially private classificationalgorithms, we find that they are not practical in real applications due to tworeasons. First, existing differentially private classifiers provide pooraccuracy on real world datasets. Second, there is no known differentiallyprivate algorithm for empirically evaluating the private classifier on aprivate test dataset. In this paper, we develop differentially private algorithms that mirror realworld empirical machine learning workflows. We consider the private classifiertraining algorithm as a blackbox. We present private algorithms for selectingfeatures that are input to the classifier. Though adding a preprocessing steptakes away some of the privacy budget from the actual classification process(thus potentially making it noisier and less accurate), we show that our novelpreprocessing techniques significantly increase classifier accuracy on threereal-world datasets. We also present the first private algorithms forempirically constructing receiver operating characteristic (ROC) curves on aprivate test set.
arxiv-1411-5555 | Maximum Likelihood Directed Enumeration Method in Piecewise-Regular Object Recognition |  http://arxiv.org/abs/1411.5555  | author:Andrey Savchenko category:cs.CV 68T10 I.5.1; I.5.4 published:2014-11-20 summary:We explore the problems of classification of composite object (images, speechsignals) with low number of models per class. We study the question ofimproving recognition performance for medium-sized database (thousands ofclasses). The key issue of fast approximate nearest-neighbor methods widelyapplied in this task is their heuristic nature. It is possible to stronglyprove their efficiency by using the theory of algorithms only for simplesimilarity measures and artificially generated tasks. On the contrary, in thispaper we propose an alternative, statistically optimal greedy algorithm. Ateach step of this algorithm joint density (likelihood) of distances topreviously checked models is estimated for each class. The next model to checkis selected from the class with the maximal likelihood. The latter is estimatedbased on the asymptotic properties of the Kullback-Leibler informationdiscrimination and mathematical model of piecewise-regular object withdistribution of each regular segment of exponential type. Experimental resultsin face recognition for FERET dataset prove that the proposed method is muchmore effective than not only brute force and the baseline (directed enumerationmethod) but also approximate nearest neighbor methods from FLANN andNonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort).
arxiv-1411-5458 | Liquid State Machine with Dendritically Enhanced Readout for Low-power, Neuromorphic VLSI Implementations |  http://arxiv.org/abs/1411.5458  | author:Subhrajit Roy, Amitava Banerjee, Arindam Basu category:cs.ET cs.NE published:2014-11-20 summary:In this paper, we describe a new neuro-inspired, hardware-friendly readoutstage for the liquid state machine (LSM), a popular model for reservoircomputing. Compared to the parallel perceptron architecture trained by thep-delta algorithm, which is the state of the art in terms of performance ofreadout stages, our readout architecture and learning algorithm can attainbetter performance with significantly less synaptic resources making itattractive for VLSI implementation. Inspired by the nonlinear properties ofdendrites in biological neurons, our readout stage incorporates neurons havingmultiple dendrites with a lumped nonlinearity. The number of synapticconnections on each branch is significantly lower than the total number ofconnections from the liquid neurons and the learning algorithm tries to findthe best 'combination' of input connections on each branch to reduce the error.Hence, the learning involves network rewiring (NRW) of the readout networksimilar to structural plasticity observed in its biological counterparts. Weshow that compared to a single perceptron using analog weights, thisarchitecture for the readout can attain, even by using the same number ofbinary valued synapses, up to 3.3 times less error for a two-class spike trainclassification problem and 2.4 times less error for an input rate approximationtask. Even with 60 times larger synapses, a group of 60 parallel perceptronscannot attain the performance of the proposed dendritically enhanced readout.An additional advantage of this method for hardware implementations is that the'choice' of connectivity can be easily implemented exploiting address eventrepresentation (AER) protocols commonly used in current neuromorphic systemswhere the connection matrix is stored in memory. Also, due to the use of binarysynapses, our proposed method is more robust against statistical variations.
arxiv-1411-5726 | CIDEr: Consensus-based Image Description Evaluation |  http://arxiv.org/abs/1411.5726  | author:Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh category:cs.CV cs.CL cs.IR published:2014-11-20 summary:Automatically describing an image with a sentence is a long-standingchallenge in computer vision and natural language processing. Due to recentprogress in object detection, attribute classification, action recognition,etc., there is renewed interest in this area. However, evaluating the qualityof descriptions has proven to be challenging. We propose a novel paradigm forevaluating image descriptions that uses human consensus. This paradigm consistsof three main parts: a new triplet-based method of collecting human annotationsto measure consensus, a new automated metric (CIDEr) that captures consensus,and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentencesdescribing each image. Our simple metric captures human judgment of consensusbetter than existing metrics across sentences generated by various sources. Wealso evaluate five state-of-the-art image description approaches using this newprotocol and provide a benchmark for future comparisons. A version of CIDErnamed CIDEr-D is available as a part of MS COCO evaluation server to enablesystematic evaluation and benchmarking.
arxiv-1411-5654 | Learning a Recurrent Visual Representation for Image Caption Generation |  http://arxiv.org/abs/1411.5654  | author:Xinlei Chen, C. Lawrence Zitnick category:cs.CV cs.AI cs.CL published:2014-11-20 summary:In this paper we explore the bi-directional mapping between images and theirsentence-based descriptions. We propose learning this mapping using a recurrentneural network. Unlike previous approaches that map both sentences and imagesto a common embedding, we enable the generation of novel sentences given animage. Using the same model, we can also reconstruct the visual featuresassociated with an image given its visual description. We use a novel recurrentvisual memory that automatically learns to remember long-term visual conceptsto aid in both sentence generation and visual feature reconstruction. Weevaluate our approach on several tasks. These include sentence generation,sentence retrieval and image retrieval. State-of-the-art results are shown forthe task of generating novel image descriptions. When compared to humangenerated captions, our automatically generated captions are preferred byhumans over $19.8\%$ of the time. Results are better than or comparable tostate-of-the-art results on the image and sentence retrieval tasks for methodsusing similar visual features.
arxiv-1411-5649 | No-Regret Learnability for Piecewise Linear Losses |  http://arxiv.org/abs/1411.5649  | author:Arthur Flajolet, Patrick Jaillet category:cs.LG published:2014-11-20 summary:In the convex optimization approach to online regret minimization, manymethods have been developed to guarantee a $O(\sqrt{T})$ regret bound forsubdifferentiable convex loss functions with bounded subgradients by means of areduction to bounded linear loss functions. This suggests that the latter tendto be the hardest loss functions to learn against. We investigate this questionin a systematic fashion as a function of the decision set and the environment'sset of moves. On the one hand, we exhibit a localization property for linearlosses leading to $o(\sqrt{T})$ learning rates and provide examples where thisproperty holds. On the other hand, we establish $\Omega(\sqrt{T})$ lower boundson the minimum achievable regret for a class of piecewise linear loss functionsthat subsumes the class of bounded linear loss functions and for polyhedraldecision sets. These results hold in a completely adversarial setting. Incontrast, we show that the minimum achievable regret can be significantlysmaller when the opponent is greedy.
arxiv-1411-5988 | Clustering evolving data using kernel-based methods |  http://arxiv.org/abs/1411.5988  | author:Rocco Langone category:cs.SI cs.LG stat.ML published:2014-11-20 summary:In this thesis, we propose several modelling strategies to tackle evolvingdata in different contexts. In the framework of static clustering, we start byintroducing a soft kernel spectral clustering (SKSC) algorithm, which canbetter deal with overlapping clusters with respect to kernel spectralclustering (KSC) and provides more interpretable outcomes. Afterwards, a wholestrategy based upon KSC for community detection of static networks is proposed,where the extraction of a high quality training sub-graph, the choice of thekernel function, the model selection and the applicability to large-scale dataare key aspects. This paves the way for the development of a novel clusteringalgorithm for the analysis of evolving networks called kernel spectralclustering with memory effect (MKSC), where the temporal smoothness betweenclustering results in successive time steps is incorporated at the level of theprimal optimization problem, by properly modifying the KSC formulation. Lateron, an application of KSC to fault detection of an industrial machine ispresented. Here, a smart pre-processing of the data by means of a properwindowing operation is necessary to catch the ongoing degradation processaffecting the machine. In this way, in a genuinely unsupervised manner, it ispossible to raise an early warning when necessary, in an online fashion.Finally, we propose a new algorithm called incremental kernel spectralclustering (IKSC) for online learning of non-stationary data. This ambitiouschallenge is faced by taking advantage of the out-of-sample property of kernelspectral clustering (KSC) to adapt the initial model, in order to tacklemerging, splitting or drifting of clusters across time. Real-world applicationsconsidered in this thesis include image segmentation, time-series clustering,community detection of static and evolving networks.
arxiv-1412-2067 | An algorithm for improving Non-Local Means operators via low-rank approximation |  http://arxiv.org/abs/1412.2067  | author:Victor May, Yosi Keller, Nir Sharon, Yoel Shkolnisky category:cs.CV math.GM published:2014-11-20 summary:We present a method for improving a Non Local Means operator by computing itslow-rank approximation. The low-rank operator is constructed by applying afilter to the spectrum of the original Non Local Means operator. This resultsin an operator which is less sensitive to noise while preserving importantproperties of the original operator. The method is efficiently implementedbased on Chebyshev polynomials and is demonstrated on the application ofnatural images denoising. For this application, we provide a comprehensivecomparison of our method with leading denoising methods.
arxiv-1411-5720 | Metric recovery from directed unweighted graphs |  http://arxiv.org/abs/1411.5720  | author:Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola category:stat.ML cs.SI math.ST stat.ME stat.TH published:2014-11-20 summary:We analyze directed, unweighted graphs obtained from $x_i\in \mathbb{R}^d$ byconnecting vertex $i$ to $j$ iff $x_i - x_j < \epsilon(x_i)$. Examples ofsuch graphs include $k$-nearest neighbor graphs, where $\epsilon(x_i)$ variesfrom point to point, and, arguably, many real world graphs such asco-purchasing graphs. We ask whether we can recover the underlying Euclideanmetric $\epsilon(x_i)$ and the associated density $p(x_i)$ given only thedirected graph and $d$. We show that consistent recovery is possible up to isometric scaling when thevertex degree is at least $\omega(n^{2/(2+d)}\log(n)^{d/(d+2)})$. Our estimatoris based on a careful characterization of a random walk over the directed graphand the associated continuum limit. As an algorithm, it resembles the PageRankcentrality metric. We demonstrate empirically that the estimator performs wellon simulated examples as well as on real-world co-purchasing graphs even with asmall number of points and degree scaling as low as $\log(n)$.
arxiv-1411-5340 | Affordances Provide a Fundamental Categorization Principle for Visual Scenes |  http://arxiv.org/abs/1411.5340  | author:Michelle R. Greene, Christopher Baldassano, Andre Esteva, Diane M. Beck, Li Fei-Fei category:q-bio.NC cs.CV cs.HC published:2014-11-19 summary:How do we know that a kitchen is a kitchen by looking? Relatively little isknown about how we conceptualize and categorize different visual environments.Traditional models of visual perception posit that scene categorization isachieved through the recognition of a scene's objects, yet these models cannotaccount for the mounting evidence that human observers are relativelyinsensitive to the local details in an image. Psychologists have long theorizedthat the affordances, or actionable possibilities of a stimulus are pivotal toits perception. To what extent are scene categories created from similaraffordances? Using a large-scale experiment using hundreds of scene categories,we show that the activities afforded by a visual scene provide a fundamentalcategorization principle. Affordance-based similarity explained the majority ofthe structure in the human scene categorization patterns, outperformingalternative similarities based on objects or visual features. We all modelswere combined, affordances provided the majority of the predictive power in thecombined model, and nearly half of the total explained variance is capturedonly by affordances. These results challenge many existing models of high-levelvisual perception, and provide immediately testable hypotheses for thefunctional organization of the human perceptual system.
arxiv-1411-5319 | Fashion Apparel Detection: The Role of Deep Convolutional Neural Network and Pose-dependent Priors |  http://arxiv.org/abs/1411.5319  | author:Kota Hara, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV published:2014-11-19 summary:In this work, we propose and address a new computer vision task, which wecall fashion item detection, where the aim is to detect various fashion items aperson in the image is wearing or carrying. The types of fashion items weconsider in this work include hat, glasses, bag, pants, shoes and so on. Thedetection of fashion items can be an important first step of various e-commerceapplications for fashion industry. Our method is based on state-of-the-artobject detection method pipeline which combines object proposal methods with aDeep Convolutional Neural Network. Since the locations of fashion items are instrong correlation with the locations of body joints positions, we incorporatecontextual information from body poses in order to improve the detectionperformance. Through the experiments, we demonstrate the effectiveness of theproposed method.
arxiv-1411-5331 | Visual Noise from Natural Scene Statistics Reveals Human Scene Category Representations |  http://arxiv.org/abs/1411.5331  | author:Michelle R. Greene, Abraham P. Botros, Diane M. Beck, Li Fei-Fei category:cs.CV cs.HC published:2014-11-19 summary:Our perceptions are guided both by the bottom-up information entering oureyes, as well as our top-down expectations of what we will see. Althoughbottom-up visual processing has been extensively studied, comparatively littleis known about top-down signals. Here, we describe REVEAL (RepresentationsEnvisioned Via Evolutionary ALgorithm), a method for visualizing an observer'sinternal representation of a complex, real-world scene, allowing us to, for thefirst time, visualize the top-down information in an observer's mind. REVEALrests on two innovations for solving this high dimensional problem: visualnoise that samples from natural image statistics, and a computer algorithm thatcollaborates with human observers to efficiently obtain a solution. In thiswork, we visualize observers' internal representations of a visual scenecategory (street) using an experiment in which the observer views thenaturalistic visual noise and collaborates with the algorithm to externalizehis internal representation. As no scene information was presented, observershad to use their internal knowledge of the target, matching it with the visualfeatures in the noise. We matched reconstructed images with images ofreal-world street scenes to enhance visualization. Critically, we show that thevisualized mental images can be used to predict rapid scene detectionperformance, as each observer had faster and more accurate responses todetecting real-world images that were the most similar to his reconstructedstreet templates. These results show that it is possible to visualizepreviously unobservable mental representations of real world stimuli. Morebroadly, REVEAL provides a general method for objectively examining the contentof previously private, subjective mental experiences.
arxiv-1411-5404 | Stochastic Block Transition Models for Dynamic Networks |  http://arxiv.org/abs/1411.5404  | author:Kevin S. Xu category:cs.SI cs.LG physics.soc-ph stat.ME published:2014-11-19 summary:There has been great interest in recent years on statistical models fordynamic networks. In this paper, I propose a stochastic block transition model(SBTM) for dynamic networks that is inspired by the well-known stochastic blockmodel (SBM) for static networks and previous dynamic extensions of the SBM.Unlike most existing dynamic network models, it does not make a hidden Markovassumption on the edge-level dynamics, allowing the presence or absence ofedges to directly influence future edge probabilities while retaining theinterpretability of the SBM. I derive an approximate inference procedure forthe SBTM and demonstrate that it is significantly better at reproducingdurations of edges in real social network data.
arxiv-1411-5328 | ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image Collections |  http://arxiv.org/abs/1411.5328  | author:Bolei Zhou, Vignesh Jagadeesh, Robinson Piramuthu category:cs.CV cs.AI cs.LG published:2014-11-19 summary:Discovering visual knowledge from weakly labeled data is crucial to scale upcomputer vision recognition system, since it is expensive to obtain fullylabeled data for a large number of concept categories. In this paper, wepropose ConceptLearner, which is a scalable approach to discover visualconcepts from weakly labeled image collections. Thousands of visual conceptdetectors are learned automatically, without human in the loop for additionalannotation. We show that these learned detectors could be applied to recognizeconcepts at image-level and to detect concepts at image region-levelaccurately. Under domain-specific supervision, we further evaluate the learnedconcepts for scene recognition on SUN database and for object detection onPascal VOC 2007. ConceptLearner shows promising performance compared to fullysupervised and weakly supervised methods.
arxiv-1411-5379 | Type-Driven Incremental Semantic Parsing with Polymorphism |  http://arxiv.org/abs/1411.5379  | author:Kai Zhao, Liang Huang category:cs.CL published:2014-11-19 summary:Semantic parsing has made significant progress, but most current semanticparsers are extremely slow (CKY-based) and rather primitive in representation.We introduce three new techniques to tackle these problems. First, we designthe first linear-time incremental shift-reduce-style semantic parsing algorithmwhich is more efficient than conventional cubic-time bottom-up semanticparsers. Second, our parser, being type-driven instead of syntax-driven, usestype-checking to decide the direction of reduction, which eliminates the needfor a syntactic grammar such as CCG. Third, to fully exploit the power oftype-driven semantic parsing beyond simple types (such as entities and truthvalues), we borrow from programming language theory the concepts of subtypepolymorphism and parametric polymorphism to enrich the type system in order tobetter guide the parsing. Our system learns very accurate parses in GeoQuery,Jobs and Atis domains.
arxiv-1411-5086 | Designing Optimal Mortality Risk Prediction Scores that Preserve Clinical Knowledge |  http://arxiv.org/abs/1411.5086  | author:Natalia M. Arzeno, Karla A. Lawson, Sarah V. Duzinski, Haris Vikalo category:stat.ML stat.AP published:2014-11-19 summary:Many in-hospital mortality risk prediction scores dichotomize predictivevariables to simplify the score calculation. However, hard thresholding inthese additive stepwise scores of the form "add x points if variable v isabove/below threshold t" may lead to critical failures. In this paper, we seekto develop risk prediction scores that preserve clinical knowledge embedded infeatures and structure of the existing additive stepwise scores whileaddressing limitations caused by variable dichotomization. To this end, wepropose a novel score structure that relies on a transformation of predictivevariables by means of nonlinear logistic functions facilitating smoothdifferentiation between critical and normal values of the variables. We developan optimization framework for inferring parameters of the logistic functionsfor a given patient population via cyclic block coordinate descent. Theparameters may readily be updated as the patient population and standards ofcare evolve. We tested the proposed methodology on two populations: (1) braintrauma patients admitted to the intensive care unit of the Dell Children'sMedical Center of Central Texas between 2007 and 2012, and (2) adult ICUpatient data from the MIMIC II database. The results are compared with thoseobtained by the widely used PRISM III and SOFA scores. The prediction power ofa score is evaluated using area under ROC curve, Youden's index, andprecision-recall balance in a cross-validation study. The results demonstratethat the new framework enables significant performance improvements over PRISMIII and SOFA in terms of all three criteria.
arxiv-1411-5309 | End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression |  http://arxiv.org/abs/1411.5309  | author:Li Wan, David Eigen, Rob Fergus category:cs.CV published:2014-11-19 summary:Deformable Parts Models and Convolutional Networks each have achieved notableperformance in object detection. Yet these two approaches find their strengthsin complementary areas: DPMs are well-versed in object composition, modelingfine-grained spatial relationships between parts; likewise, ConvNets are adeptat producing powerful image features, having been discriminatively traineddirectly on the pixels. In this paper, we propose a new model that combinesthese two approaches, obtaining the advantages of each. We train this modelusing a new structured loss function that considers all bounding boxes withinan image, rather than isolated object instances. This enables the non-maximalsuppression (NMS) operation, previously treated as a separate post-processingstage, to be integrated into the model. This allows for discriminative trainingof our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluateour system on PASCAL VOC 2007 and 2011 datasets, achieving competitive resultson both benchmarks.
arxiv-1411-5307 | Efficient Media Retrieval from Non-Cooperative Queries |  http://arxiv.org/abs/1411.5307  | author:Kevin Shih, Wei Di, Vignesh Jagadeesh, Robinson Piramuthu category:cs.IR cs.CV published:2014-11-19 summary:Text is ubiquitous in the artificial world and easily attainable when itcomes to book title and author names. Using the images from the book cover setfrom the Stanford Mobile Visual Search dataset and additional book covers andmetadata from openlibrary.org, we construct a large scale book cover retrievaldataset, complete with 100K distractor covers and title and author strings foreach. Because our query images are poorly conditioned for clean textextraction, we propose a method for extracting a matching noisy and erroneousOCR readings and matching it against clean author and book title strings in astandard document look-up problem setup. Finally, we demonstrate how to usethis text-matching as a feature in conjunction with popular retrieval featuressuch as VLAD using a simple learning setup to achieve significant improvementsin retrieval accuracy over that of either VLAD or the text alone.
arxiv-1411-5371 | Unification of field theory and maximum entropy methods for learning probability densities |  http://arxiv.org/abs/1411.5371  | author:Justin B. Kinney category:cs.LG q-bio.QM stat.ML published:2014-11-19 summary:The need to estimate smooth probability distributions (a.k.a. probabilitydensities) from finite sampled data is ubiquitous in science. Many approachesto this problem have been described, but none is yet regarded as providing adefinitive solution. Maximum entropy estimation and Bayesian field theory aretwo such approaches. Both have origins in statistical physics, but therelationship between them has remained unclear. Here I unify these two methodsby showing that every maximum entropy density estimate can be recovered in theinfinite smoothness limit of an appropriate Bayesian field theory. I also showthat Bayesian field theory estimation can be performed without imposing anyboundary conditions on candidate densities, and that the infinite smoothnesslimit of these theories recovers the most common types of maximum entropyestimates. Bayesian field theory is thus seen to provide a natural test of thevalidity of the maximum entropy null hypothesis. Bayesian field theory alsoreturns a lower entropy density estimate when the maximum entropy hypothesis isfalsified. The computations necessary for this approach can be performedrapidly for one-dimensional data, and software for doing this is provided.Based on these results, I argue that Bayesian field theory is poised to providea definitive solution to the density estimation problem in one dimension.
arxiv-1411-5268 | Sparse distributed localized gradient fused features of objects |  http://arxiv.org/abs/1411.5268  | author:Swathikiran Sudhakarana, Alex Pappachen James category:cs.CV cs.AI published:2014-11-19 summary:The sparse, hierarchical, and modular processing of natural signals isrelated to the ability of humans to recognize objects with high accuracy. Inthis study, we report a sparse feature processing and encoding method, whichimproved the recognition performance of an automated object recognition system.Randomly distributed localized gradient enhanced features were selected beforeemploying aggregate functions for representation, where we used a modular andhierarchical approach to detect the object features. These object features werecombined with a minimum distance classifier, thereby obtaining objectrecognition system accuracies of 93% using the Amsterdam library of objectimages (ALOI) database, 92% using the Columbia object image library (COIL)-100database, and 69% using the PASCAL visual object challenge 2007 database. Theobject recognition performance was shown to be robust to variations in noise,object scaling, and object shifts. Finally, a comparison with eight existingobject recognition methods indicated that our new method improved therecognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10%with the PASCAL visual object challenge 2007 database.
arxiv-1411-5140 | Attentional Neural Network: Feature Selection Using Cognitive Feedback |  http://arxiv.org/abs/1411.5140  | author:Qian Wang, Jiaxing Zhang, Sen Song, Zheng Zhang category:cs.CV cs.NE published:2014-11-19 summary:Attentional Neural Network is a new framework that integrates top-downcognitive bias and bottom-up feature extraction in one coherent architecture.The top-down influence is especially effective when dealing with high noise ordifficult segmentation problems. Our system is modular and extensible. It isalso easy to train and cheap to run, and yet can accommodate complex behaviors.We obtain classification accuracy better than or competitive with state of artresults on the MNIST variation dataset, and successfully disentangle overlaiddigits with high success rates. We view such a general purpose framework as anessential foundation for a larger system emulating the cognitive abilities ofthe whole brain.
arxiv-1411-5260 | Large-Margin Classification with Multiple Decision Rules |  http://arxiv.org/abs/1411.5260  | author:Patrick K. Kimes, D. Neil Hayes, J. S. Marron, Yufeng Liu category:stat.ML cs.LG published:2014-11-19 summary:Binary classification is a common statistical learning problem in which amodel is estimated on a set of covariates for some outcome indicating themembership of one of two classes. In the literature, there exists a distinctionbetween hard and soft classification. In soft classification, the conditionalclass probability is modeled as a function of the covariates. In contrast, hardclassification methods only target the optimal prediction boundary. While hardand soft classification methods have been studied extensively, not much workhas been done to compare the actual tasks of hard and soft classification. Inthis paper we propose a spectrum of statistical learning problems which spanthe hard and soft classification tasks based on fitting multiple decision rulesto the data. By doing so, we reveal a novel collection of learning tasks ofincreasing complexity. We study the problems using the framework oflarge-margin classifiers and a class of piecewise linear convex surrogates, forwhich we derive statistical properties and a corresponding sub-gradient descentalgorithm. We conclude by applying our approach to simulation settings and amagnetic resonance imaging (MRI) dataset from the Alzheimer's DiseaseNeuroimaging Initiative (ADNI) study.
arxiv-1411-5172 | Learning nonparametric differential equations with operator-valued kernels and gradient matching |  http://arxiv.org/abs/1411.5172  | author:Markus Heinonen, Florence d'Alché-Buc category:cs.LG stat.ML published:2014-11-19 summary:Modeling dynamical systems with ordinary differential equations implies amechanistic view of the process underlying the dynamics. However in many cases,this knowledge is not available. To overcome this issue, we introduce a generalframework for nonparametric ODE models using penalized regression inReproducing Kernel Hilbert Spaces (RKHS) based on operator-valued kernels.Moreover, we extend the scope of gradient matching approaches to nonparametricODE. A smooth estimate of the solution ODE is built to provide an approximationof the derivative of the ODE solution which is in turn used to learn thenonparametric ODE model. This approach benefits from the flexibility ofpenalized regression in RKHS allowing for ridge or (structured) sparseregression as well. Very good results are shown on 3 different ODE systems.
arxiv-1411-5190 | A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation |  http://arxiv.org/abs/1411.5190  | author:Mateusz Malinowski, Mario Fritz category:cs.CV published:2014-11-19 summary:Over the last two decades we have witnessed strong progress on modelingvisual object classes, scenes and attributes that have significantlycontributed to automated image understanding. On the other hand, surprisinglylittle progress has been made on incorporating a spatial representation andreasoning in the inference process. In this work, we propose a poolinginterpretation of spatial relations and show how it improves image retrievaland annotations tasks involving spatial language. Due to the complexity of thespatial language, we argue for a learning-based approach that acquires arepresentation of spatial relations by learning parameters of the poolingoperator. We show improvements on previous work on two datasets and twodifferent tasks as well as provide additional insights on a new dataset with anexplicit focus on spatial relations.
arxiv-1411-5271 | Quantifying error in estimates of human brain fiber directions using Earth Mover's Distance |  http://arxiv.org/abs/1411.5271  | author:Charles Zheng, Franco Pestilli, Ariel Rokem category:stat.ML published:2014-11-19 summary:Diffusion-weighted MR imaging (DWI) is the only method we currently have tomeasure connections between different parts of the human brain in vivo. Toelucidate the structure of these connections, algorithms for tracking bundlesof axonal fibers through the subcortical white matter rely on local estimatesof the fiber orientation distribution function (fODF) in different parts of thebrain. These functions describe the relative abundance of populations of axonalfibers crossing each other in each location. Multiple models exist forestimating fODFs. The quality of the resulting estimates can be quantified bymeans of a suitable measure of distance on the space of fODFs. However, thereare multiple distance metrics that can be applied for this purpose, includingsmoothed $L_p$ distances and the Wasserstein metrics. Here, we give fourreasons for the use of the Earth Mover's Distance (EMD) equipped with thearc-length, as a distance metric. (continued)
arxiv-1411-4702 | Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal Memory in Light of Frontal Cortical Function |  http://arxiv.org/abs/1411.4702  | author:Michael R. Ferrier category:q-bio.NC cs.AI cs.NE I.2.6 published:2014-11-18 summary:A wide range of evidence points toward the existence of a common algorithmunderlying the processing of information throughout the cerebral cortex.Several hypothesized features of this cortical algorithm are reviewed,including sparse distributed representation, Bayesian inference, hierarchicalorganization composed of alternating template matching and pooling layers,temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) isa family of learning algorithms and corresponding theories of cortical functionthat embodies these principles. HTM has previously been applied mainly toperceptual tasks typical of posterior cortex. In order to evaluate HTM as acandidate model of cortical function, it is necessary also to investigate itscompatibility with the requirements of frontal cortical function. To this end,a variety of models of frontal cortical function are reviewed and integrated,to arrive at the hypothesis that frontal functions including attention, workingmemory and action selection depend largely upon the same basic algorithms as doposterior functions, with the notable additions of a mechanism for the activemaintenance of representations and of multiple cortico-striato-thalamo-corticalloops that allow communication between regions of frontal cortex to be gated inan adaptive manner. Computational models of this system are reviewed. Finally,there is a discussion of how HTM can contribute to the understanding of frontalcortical function, and of what the requirements of frontal cortical functionmean for the future development of HTM.
arxiv-1411-4701 | Structured Hough Voting for Vision-based Highway Border Detection |  http://arxiv.org/abs/1411.4701  | author:Zhiding Yu, Wende Zhang, B. V. K. Vijaya Kumar, Dan Levi category:cs.CV cs.RO published:2014-11-18 summary:We propose a vision-based highway border detection algorithm using structuredHough voting. Our approach takes advantage of the geometric relationshipbetween highway road borders and highway lane markings. It uses a strategywhere a number of trained road border and lane marking detectors are triggered,followed by Hough voting to generate corresponding detection of the border andlane marking. Since the initially triggered detectors usually result in largenumber of positives, conventional frame-wise Hough voting is not able to alwaysgenerate robust border and lane marking results. Therefore, we formulate thisproblem as a joint detection-and-tracking problem under the structured Houghvoting model, where tracking refers to exploiting inter-frame structuralinformation to stabilize the detection results. Both qualitative andquantitative evaluations show the superiority of the proposed structured Houghvoting model over a number of baseline methods.
arxiv-1411-5879 | A Unified Semantic Embedding: Relating Taxonomies and Attributes |  http://arxiv.org/abs/1411.5879  | author:Sung Ju Hwang, Leonid Sigal category:cs.CV published:2014-11-18 summary:We propose a method that learns a discriminative yet semantic space forobject categorization, where we also embed auxiliary semantic entities such assupercategories and attributes. Contrary to prior work which only utilized themas side information, we explicitly embed the semantic entities into the samespace where we embed categories, which enables us to represent a category astheir linear combination. By exploiting such a unified model for semantics, weenforce each category to be represented by a supercategory + sparse combinationof attributes, with an additional exclusive regularization to learndiscriminative composition.
arxiv-1411-5065 | SIRF: Simultaneous Image Registration and Fusion in A Unified Framework |  http://arxiv.org/abs/1411.5065  | author:Chen Chen, Yeqing Li, Wei Liu, Junzhou Huang category:cs.CV published:2014-11-18 summary:In this paper, we propose a novel method for image fusion with ahigh-resolution panchromatic image and a low-resolution multispectral image atthe same geographical location. The fusion is formulated as a convexoptimization problem which minimizes a linear combination of a least-squaresfitting term and a dynamic gradient sparsity regularizer. The former is topreserve accurate spectral information of the multispectral image, while thelatter is to keep sharp edges of the high-resolution panchromatic image. Wefurther propose to simultaneously register the two images during the fusingprocess, which is naturally achieved by virtue of the dynamic gradient sparsityproperty. An efficient algorithm is then devised to solve the optimizationproblem, accomplishing a linear computational complexity in the size of theoutput image in each iteration. We compare our method against sevenstate-of-the-art image fusion methods on multispectral image datasets from foursatellites. Extensive experimental results demonstrate that the proposed methodsubstantially outperforms the others in terms of both spatial and spectralqualities. We also show that our method can provide high-quality products fromcoarsely registered real-world datasets. Finally, a MATLAB implementation isprovided to facilitate future research.
arxiv-1411-4834 | The NLMS algorithm with time-variant optimum stepsize derived from a Bayesian network perspective |  http://arxiv.org/abs/1411.4834  | author:Christian Huemmer, Roland Maas, Walter Kellermann category:stat.ML published:2014-11-18 summary:In this article, we derive a new stepsize adaptation for the normalized leastmean square algorithm (NLMS) by describing the task of linear acoustic echocancellation from a Bayesian network perspective. Similar to the well-knownKalman filter equations, we model the acoustic wave propagation from theloudspeaker to the microphone by a latent state vector and define a linearobservation equation (to model the relation between the state vector and theobservation) as well as a linear process equation (to model the temporalprogress of the state vector). Based on additional assumptions on thestatistics of the random variables in observation and process equation, weapply the expectation-maximization (EM) algorithm to derive an NLMS-like filteradaptation. By exploiting the conditional independence rules for Bayesiannetworks, we reveal that the resulting EM-NLMS algorithm has a stepsize updateequivalent to the optimal-stepsize calculation proposed by Yamamoto andKitayama in 1982, which has been adopted in many textbooks. As main difference,the instantaneous stepsize value is estimated in the M step of the EM algorithm(instead of being approximated by artificially extending the acoustic echopath). The EM-NLMS algorithm is experimentally verified for synthesizedscenarios with both, white noise and male speech as input signal.
arxiv-1411-4734 | Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture |  http://arxiv.org/abs/1411.4734  | author:David Eigen, Rob Fergus category:cs.CV published:2014-11-18 summary:In this paper we address three different computer vision tasks using a singlebasic architecture: depth prediction, surface normal estimation, and semanticlabeling. We use a multiscale convolutional network that is able to adapteasily to each task using only small modifications, regressing from the inputimage to the output map directly. Our method progressively refines predictionsusing a sequence of scales, and captures many image details without anysuperpixels or low-level segmentation. We achieve state-of-the-art performanceon benchmarks for all three tasks.
arxiv-1412-3717 | Unsupervised Neural Architecture for Saliency Detection: Extended Version |  http://arxiv.org/abs/1412.3717  | author:Natalia Efremova, Sergey Tarasenko category:cs.CV cs.NE published:2014-11-18 summary:We propose a novel neural network architecture for visual saliencydetections, which utilizes neurophysiologically plausible mechanisms forextraction of salient regions. The model has been significantly inspired byrecent findings from neurophysiology and aimed to simulate the bottom-upprocesses of human selective attention. Two types of features were analyzed:color and direction of maximum variance. The mechanism we employ for processingthose features is PCA, implemented by means of normalized Hebbian learning andthe waves of spikes. To evaluate performance of our model we have conductedpsychological experiment. Comparison of simulation results with those ofexperiment indicates good performance of our model.
arxiv-1411-4738 | Cross-Modal Similarity Learning : A Low Rank Bilinear Formulation |  http://arxiv.org/abs/1411.4738  | author:Cuicui Kang, Shengcai Liao, Yonghao He, Jian Wang, Wenjia Niu, Shiming Xiang, Chunhong Pan category:cs.MM cs.IR cs.LG published:2014-11-18 summary:The cross-media retrieval problem has received much attention in recent yearsdue to the rapid increasing of multimedia data on the Internet. A new approachto the problem has been raised which intends to match features of differentmodalities directly. In this research, there are two critical issues: how toget rid of the heterogeneity between different modalities and how to match thecross-modal features of different dimensions. Recently metric learning methodsshow a good capability in learning a distance metric to explore therelationship between data points. However, the traditional metric learningalgorithms only focus on single-modal features, which suffer difficulties inaddressing the cross-modal features of different dimensions. In this paper, wepropose a cross-modal similarity learning algorithm for the cross-modal featurematching. The proposed method takes a bilinear formulation, and with thenuclear-norm penalization, it achieves low-rank representation. Accordingly,the accelerated proximal gradient algorithm is successfully imported to findthe optimal solution with a fast convergence rate O(1/t^2). Experiments onthree well known image-text cross-media retrieval databases show that theproposed method achieves the best performance compared to the state-of-the-artalgorithms.
arxiv-1411-5014 | Music Data Analysis: A State-of-the-art Survey |  http://arxiv.org/abs/1411.5014  | author:Shubhanshu Gupta category:cs.DB cs.LG cs.SD 97M80 H.5.5; J.5 published:2014-11-18 summary:Music accounts for a significant chunk of interest among various onlineactivities. This is reflected by wide array of alternatives offered in musicrelated web/mobile apps, information portals, featuring millions of artists,songs and events attracting user activity at similar scale. Availability oflarge scale structured and unstructured data has attracted similar level ofattention by data science community. This paper attempts to offer currentstate-of-the-art in music related analysis. Various approaches involvingmachine learning, information theory, social network analysis, semantic web andlinked open data are represented in the form of taxonomy along with datasources and use cases addressed by the research community.
arxiv-1411-5878 | Salient Object Detection: A Survey |  http://arxiv.org/abs/1411.5878  | author:Ali Borji, Ming-Ming Cheng, Huaizu Jiang, Jia Li category:cs.CV cs.AI q-bio.NC published:2014-11-18 summary:Detecting and segmenting salient objects in natural scenes, also known assalient object detection, has attracted a lot of focused research in computervision and has resulted in many applications. However, while many such modelsexist, a deep understanding of achievements and issues is lacking. We aim toprovide a comprehensive review of the recent progress in this field. We situatesalient object detection among other closely related areas such as genericscene segmentation, object proposal generation, and saliency for fixationprediction. Covering 256 publications we survey i) roots, key concepts, andtasks, ii) core techniques and main modeling trends, and iii) datasets andevaluation metrics in salient object detection. We also discuss open problemssuch as evaluation metrics and dataset bias in model performance and suggestfuture research directions.
arxiv-1411-5935 | Towards Scene Understanding with Detailed 3D Object Representations |  http://arxiv.org/abs/1411.5935  | author:M. Zeeshan Zia, Michael Stark, Konrad Schindler category:cs.CV published:2014-11-18 summary:Current approaches to semantic image and scene understanding typically employrather simple object representations such as 2D or 3D bounding boxes. Whilesuch coarse models are robust and allow for reliable object detection, theydiscard much of the information about objects' 3D shape and pose, and thus donot lend themselves well to higher-level reasoning. Here, we propose to basescene understanding on a high-resolution object representation. An object class- in our case cars - is modeled as a deformable 3D wireframe, which enablesfine-grained modeling at the level of individual vertices and faces. We augmentthat model to explicitly include vertex-level occlusion, and embed allinstances in a common coordinate frame, in order to infer and exploitobject-object interactions. Specifically, from a single view we jointlyestimate the shapes and poses of multiple objects in a common 3D frame. Aground plane in that frame is estimated by consensus among different objects,which significantly stabilizes monocular 3D pose estimation. The fine-grainedmodel, in conjunction with the explicit 3D scene model, further allows one toinfer part-level occlusions between the modeled objects, as well as occlusionsby other, unmodeled scene elements. To demonstrate the benefits of suchdetailed object class models in the context of scene understanding wesystematically evaluate our approach on the challenging KITTI street scenedataset. The experiments show that the model's ability to utilize imageevidence at the level of individual parts improves monocular 3D pose estimationw.r.t. both location and (continuous) viewpoint.
arxiv-1411-5057 | Fast Iteratively Reweighted Least Squares Algorithms for Analysis-Based Sparsity Reconstruction |  http://arxiv.org/abs/1411.5057  | author:Chen Chen, Junzhou Huang, Lei He, Hongsheng Li category:cs.CV published:2014-11-18 summary:In this paper, we propose a novel algorithm for analysis-based sparsityreconstruction. It can solve the generalized problem by structured sparsityregularization with an orthogonal basis and total variation regularization. Theproposed algorithm is based on the iterative reweighted least squares (IRLS)model, which is further accelerated by the preconditioned conjugate gradientmethod. The convergence rate of the proposed algorithm is almost the same asthat of the traditional IRLS algorithms, that is, exponentially fast. Moreover,with the specifically devised preconditioner, the computational cost for eachiteration is significantly less than that of traditional IRLS algorithms, whichenables our approach to handle large scale problems. In addition to the fastconvergence, it is straightforward to apply our method to standard sparsity,group sparsity, overlapping group sparsity and TV based problems. Experimentsare conducted on a practical application: compressive sensing magneticresonance imaging. Extensive results demonstrate that the proposed algorithmachieves superior performance over 14 state-of-the-art algorithms in terms ofboth accuracy and computational cost.
arxiv-1411-4894 | Low-level Vision by Consensus in a Spatial Hierarchy of Regions |  http://arxiv.org/abs/1411.4894  | author:Ayan Chakrabarti, Ying Xiong, Steven J. Gortler, Todd Zickler category:cs.CV published:2014-11-18 summary:We introduce a multi-scale framework for low-level vision, where the goal isestimating physical scene values from image data---such as depth from stereoimage pairs. The framework uses a dense, overlapping set of image regions atmultiple scales and a "local model," such as a slanted-plane model for stereodisparity, that is expected to be valid piecewise across the visual field.Estimation is cast as optimization over a dichotomous mixture of variables,simultaneously determining which regions are inliers with respect to the localmodel (binary variables) and the correct co-ordinates in the local model spacefor each inlying region (continuous variables). When the regions are organizedinto a multi-scale hierarchy, optimization can occur in an efficient andparallel architecture, where distributed computational units iterativelyperform calculations and share information through sparse connections betweenparents and children. The framework performs well on a standard benchmark forbinocular stereo, and it produces a distributional scene representation that isappropriate for combining with higher-level reasoning and other low-level cues.
arxiv-1411-4952 | From Captions to Visual Concepts and Back |  http://arxiv.org/abs/1411.4952  | author:Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig category:cs.CV cs.CL published:2014-11-18 summary:This paper presents a novel approach for automatically generating imagedescriptions: visual detectors, language models, and multimodal similaritymodels learnt directly from a dataset of image captions. We use multipleinstance learning to train visual detectors for words that commonly occur incaptions, including many different parts of speech such as nouns, verbs, andadjectives. The word detector outputs serve as conditional inputs to amaximum-entropy language model. The language model learns from a set of over400,000 image descriptions to capture the statistics of word usage. We captureglobal semantics by re-ranking caption candidates using sentence-level featuresand a deep multimodal similarity model. Our system is state-of-the-art on theofficial Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. Whenhuman judges compare the system captions to ones written by other people on ourheld-out test set, the system captions have equal or better quality 34% of thetime.
arxiv-1411-4925 | Linguistic Descriptions for Automatic Generation of Textual Short-Term Weather Forecasts on Real Prediction Data |  http://arxiv.org/abs/1411.4925  | author:A. Ramos-Soto, A. Bugarín, S. Barro, J. Taboada category:cs.AI cs.CL published:2014-11-18 summary:We present in this paper an application which automatically generates textualshort-term weather forecasts for every municipality in Galicia (NW Spain),using the real data provided by the Galician Meteorology Agency (MeteoGalicia).This solution combines in an innovative way computing with perceptionstechniques and strategies for linguistic description of data together with anatural language generation (NLG) system. The application, named GALiWeather,extracts relevant information from weather forecast input data and encodes itinto intermediate descriptions using linguistic variables and temporalreferences. These descriptions are later translated into natural language textsby the natural language generation system. The obtained forecast results havebeen thoroughly validated by an expert meteorologist from MeteoGalicia using aquality assessment methodology which covers two key dimensions of a text: theaccuracy of its content and the correctness of its form. Following thisvalidation GALiWeather will be released as a real service offering customforecasts for a wide public.
arxiv-1411-4958 | Designing Deep Networks for Surface Normal Estimation |  http://arxiv.org/abs/1411.4958  | author:Xiaolong Wang, David F. Fouhey, Abhinav Gupta category:cs.CV published:2014-11-18 summary:In the past few years, convolutional neural nets (CNN) have shown incrediblepromise for learning visual representations. In this paper, we use CNNs for thetask of predicting surface normals from a single image. But what is the rightarchitecture we should use? We propose to build upon the decades of hard workin 3D scene understanding, to design new CNN architecture for the task ofsurface normal estimation. We show by incorporating several constraints(man-made, manhattan world) and meaningful intermediate representations (roomlayout, edge labels) in the architecture leads to state of the art performanceon surface normal estimation. We also show that our network is quite robust andshow state of the art results on other datasets as well without anyfine-tuning.
arxiv-1411-4960 | Network Motifs Analysis of Croatian Literature |  http://arxiv.org/abs/1411.4960  | author:Hana Rizvić, Sanda Martinčić-Ipšić, Ana Meštrović category:cs.CL published:2014-11-18 summary:In this paper we analyse network motifs in the co-occurrence directednetworks constructed from five different texts (four books and one portal) inthe Croatian language. After preparing the data and network construction, weperform the network motif analysis. We analyse the motif frequencies andZ-scores in the five networks. We present the triad significance profile forfive datasets. Furthermore, we compare our results with the existing resultsfor the linguistic networks. Firstly, we show that the triad significanceprofile for the Croatian language is very similar with the other languages andall the networks belong to the same family of networks. However, there arecertain differences between the Croatian language and other analysed languages.We conclude that this is due to the free word-order of the Croatian language.
arxiv-1411-5010 | Nonnegative Tensor Factorization for Directional Blind Audio Source Separation |  http://arxiv.org/abs/1411.5010  | author:Noah D. Stein category:stat.ML cs.LG published:2014-11-18 summary:We augment the nonnegative matrix factorization method for audio sourceseparation with cues about directionality of sound propagation. This improvesseparation quality greatly and removes the need for training data, but doublesthe computation.
arxiv-1411-5053 | Model of Interaction between Learning and Evolution |  http://arxiv.org/abs/1411.5053  | author:Vladimir G. Red'ko category:cs.NE published:2014-11-18 summary:The model of interaction between learning and evolutionary optimization isdesigned and investigated. The evolving population of modeled organisms isconsidered. The mechanism of the genetic assimilation of the acquired featuresduring a number of generations of Darwinian evolution is studied. It is shownthat the genetic assimilation takes place as follows: phenotypes of modeledorganisms move towards the optimum at learning; then the selection takes place;genotypes of selected organisms also move towards the optimum. The hidingeffect is also studied; this effect means that strong learning can inhibit theevolutionary search for the optimal genotype. The mechanism of influence of thelearning load on the interaction between learning and evolution is analyzed. Itis shown that the learning load can lead to a significant acceleration ofevolution.
arxiv-1411-4798 | Memcomputing NP-complete problems in polynomial time using polynomial resources and collective states |  http://arxiv.org/abs/1411.4798  | author:Fabio L. Traversa, Chiara Ramella, Fabrizio Bonani, Massimiliano Di Ventra category:cs.ET cs.NE published:2014-11-18 summary:Memcomputing is a novel non-Turing paradigm of computation that usesinteracting memory cells (memprocessors for short) to store and processinformation on the same physical platform. It was recently provedmathematically that memcomputing machines have the same computational power ofnon-deterministic Turing machines. Therefore, they can solve NP-completeproblems in polynomial time and, using the appropriate architecture, withresources that only grow polynomially with the input size. The reason for thiscomputational power stems from properties inspired by the brain and shared byany universal memcomputing machine, in particular intrinsic parallelism andinformation overhead, namely the capability of compressing information in thecollective state of the memprocessor network. Here, we show an experimentaldemonstration of an actual memcomputing architecture that solves theNP-complete version of the subset-sum problem in only one step and is composedof a number of memprocessors that scales linearly with the size of the problem.We have fabricated this architecture using standard microelectronic technologyso that it can be easily realized in any laboratory setting. Even though theparticular machine presented here is eventually limited by noise--and will thusrequire error-correcting codes to scale to an arbitrary number ofmemprocessors--it represents the first proof-of-concept of a machine capable ofworking with the collective state of interacting memory cells, unlike thepresent-day single-state machines built using the von Neumann architecture.
arxiv-1411-4825 | Cognitive Systems and Question Answering |  http://arxiv.org/abs/1411.4825  | author:Ulrich Furbach, Claudia Schon, Frieder Stolzenburg category:cs.AI cs.CL published:2014-11-18 summary:This paper briefly characterizes the field of cognitive computing. As anexemplification, the field of natural language question answering is introducedtogether with its specific challenges. A possibility to master these challengesis illustrated by a detailed presentation of the LogAnswer system, which is asuccessful representative of the field of natural language question answering.
arxiv-1411-4464 | Fully Convolutional Neural Networks for Crowd Segmentation |  http://arxiv.org/abs/1411.4464  | author:Kai Kang, Xiaogang Wang category:cs.CV published:2014-11-17 summary:In this paper, we propose a fast fully convolutional neural network (FCNN)for crowd segmentation. By replacing the fully connected layers in CNN with 1by 1 convolution kernels, FCNN takes whole images as inputs and directlyoutputs segmentation maps by one pass of forward propagation. It has theproperty of translation invariance like patch-by-patch scanning but with muchlower computation cost. Once FCNN is learned, it can process input images ofany sizes without warping them to a standard size. These attractive propertiesmake it extendable to other general image segmentation problems. Based on FCNN,a multi-stage deep learning is proposed to integrate appearance and motion cuesfor crowd segmentation. Both appearance filters and motion filers arepretrained stage-by-stage and then jointly optimized. Different combinationmethods are investigated. The effectiveness of our approach and component-wiseanalysis are evaluated on two crowd segmentation datasets created by us, whichinclude image frames from 235 and 11 scenes, respectively. They are currentlythe largest crowd segmentation datasets and will be released to the public.
arxiv-1411-4378 | Robust Kernel Density Estimation by Scaling and Projection in Hilbert Space |  http://arxiv.org/abs/1411.4378  | author:Robert A. Vandermeulen, Clayton D. Scott category:stat.ML published:2014-11-17 summary:While robust parameter estimation has been well studied in parametric densityestimation, there has been little investigation into robust density estimationin the nonparametric setting. We present a robust version of the popular kerneldensity estimator (KDE). As with other estimators, a robust version of the KDEis useful since sample contamination is a common issue with datasets. What"robustness" means for a nonparametric density estimate is not straightforwardand is a topic we explore in this paper. To construct a robust KDE we scale thetraditional KDE and project it to its nearest weighted KDE in the $L^2$ norm.This yields a scaled and projected KDE (SPKDE). Because the squared $L^2$ normpenalizes point-wise errors superlinearly this causes the weighted KDE toallocate more weight to high density regions. We demonstrate the robustness ofthe SPKDE with numerical experiments and a consistency result which shows thatasymptotically the SPKDE recovers the uncontaminated density under sufficientconditions on the contamination.
arxiv-1411-4555 | Show and Tell: A Neural Image Caption Generator |  http://arxiv.org/abs/1411.4555  | author:Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan category:cs.CV published:2014-11-17 summary:Automatically describing the content of an image is a fundamental problem inartificial intelligence that connects computer vision and natural languageprocessing. In this paper, we present a generative model based on a deeprecurrent architecture that combines recent advances in computer vision andmachine translation and that can be used to generate natural sentencesdescribing an image. The model is trained to maximize the likelihood of thetarget description sentence given the training image. Experiments on severaldatasets show the accuracy of the model and the fluency of the language itlearns solely from image descriptions. Our model is often quite accurate, whichwe verify both qualitatively and quantitatively. For instance, while thecurrent state-of-the-art BLEU-1 score (the higher the better) on the Pascaldataset is 25, our approach yields 59, to be compared to human performancearound 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66,and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, weachieve a BLEU-4 of 27.7, which is the current state-of-the-art.
arxiv-1411-4510 | Parallel Gaussian Process Regression for Big Data: Low-Rank Representation Meets Markov Approximation |  http://arxiv.org/abs/1411.4510  | author:Kian Hsiang Low, Jiangbo Yu, Jie Chen, Patrick Jaillet category:stat.ML cs.DC cs.LG published:2014-11-17 summary:The expressive power of a Gaussian process (GP) model comes at a cost of poorscalability in the data size. To improve its scalability, this paper presents alow-rank-cum-Markov approximation (LMA) of the GP model that is novel inleveraging the dual computational advantages stemming from complementing alow-rank approximate representation of the full-rank GP based on a support setof inputs with a Markov approximation of the resulting residual process; thelatter approximation is guaranteed to be closest in the Kullback-Leiblerdistance criterion subject to some constraint and is considerably more refinedthan that of existing sparse GP models utilizing low-rank representations dueto its more relaxed conditional independence assumption (especially with largerdata). As a result, our LMA method can trade off between the size of thesupport set and the order of the Markov property to (a) incur lowercomputational cost than such sparse GP models while achieving predictiveperformance comparable to them and (b) accurately represent features/patternsof any scale. Interestingly, varying the Markov order produces a spectrum ofLMAs with PIC approximation and full-rank GP at the two extremes. An advantageof our LMA method is that it is amenable to parallelization on multiplemachines/cores, thereby gaining greater scalability. Empirical evaluation onthree real-world datasets in clusters of up to 32 computing nodes shows thatour centralized and parallel LMA methods are significantly more time-efficientand scalable than state-of-the-art sparse and full-rank GP regression methodswhile achieving comparable predictive performances.
arxiv-1411-4521 | Implicitly Constrained Semi-Supervised Linear Discriminant Analysis |  http://arxiv.org/abs/1411.4521  | author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG published:2014-11-17 summary:Semi-supervised learning is an important and active topic of research inpattern recognition. For classification using linear discriminant analysisspecifically, several semi-supervised variants have been proposed. Using anyone of these methods is not guaranteed to outperform the supervised classifierwhich does not take the additional unlabeled data into account. In this work wecompare traditional Expectation Maximization type approaches forsemi-supervised linear discriminant analysis with approaches based on intrinsicconstraints and propose a new principled approach for semi-supervised lineardiscriminant analysis, using so-called implicit constraints. We explore therelationships between these methods and consider the question if and in whatsense we can expect improvement in performance over the supervised procedure.The constraint based approaches are more robust to misspecification of themodel, and may outperform alternatives that make more assumptions on the data,in terms of the log-likelihood of unseen objects.
arxiv-1411-4419 | Automatic Subspace Learning via Principal Coefficients Embedding |  http://arxiv.org/abs/1411.4419  | author:Xi Peng, Jiwen Lu, Rui Yan, Zhang Yi category:cs.CV published:2014-11-17 summary:In this paper, we address two problems in unsupervised subspace learning: 1)how to automatically identify the feature dimension of the learned subspace,and 2) how to learn the underlying subspace in the presence of grosscorruptions such as Gaussian noise. We show that these two problems are twosides of one coin, i.e. they can be solved by removing possible errors fromtraining data $\mathbf{D}\in \mathds{R}^{m\times n}$. To achieve this, wepropose a new method (called Principal Coefficients Embedding, PCE) that cansimultaneously learn a clean data set $\mathbf{D}_{0}\in \mathds{R}^{m\timesn}$ and a linear representation (denoted by $\mathbf{C}$) from $\mathbf{D}$. Byembedding $\mathbf{C}$ into a $k$-dimensional space, PCE obtains a projectionmatrix that preserves some desirable properties of inputs, where $k\ll m$ isexactly the rank of $\mathbf{C}$. PCE has three advantages: 1) it canautomatically determine the feature dimension even though data are sampled froma union of multiple linear subspaces; 2) it is robust to various noises andreal disguises; 3) it has a closed-form solution and can be calculated veryfast. Extensive experimental results show the superiority of PCE on a range ofdatabases with respect to classification accuracy, robustness and efficiency.
arxiv-1411-4423 | A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis |  http://arxiv.org/abs/1411.4423  | author:Sotirios P. Chatzis category:cs.CV published:2014-11-17 summary:Unsupervised feature learning algorithms based on convolutional formulationsof independent components analysis (ICA) have been demonstrated to yieldstate-of-the-art results in several action recognition benchmarks. However,existing approaches do not allow for the number of latent components (features)to be automatically inferred from the data in an unsupervised manner. This is asignificant disadvantage of the state-of-the-art, as it results in considerableburden imposed on researchers and practitioners, who must resort to tediouscross-validation procedures to obtain the optimal number of latent features. Toresolve these issues, in this paper we introduce a convolutional nonparametricBayesian sparse ICA architecture for overcomplete feature learning fromhigh-dimensional data. Our method utilizes an Indian buffet process prior tofacilitate inference of the appropriate number of latent features under ahybrid variational inference algorithm, scalable to massive datasets. As weshow, our model can be naturally used to obtain deep unsupervised hierarchicalfeature extractors, by greedily stacking successive model layers, similar toexisting approaches. In addition, inference for this model is completelyheuristics-free; thus, it obviates the need of tedious parameter tuning, whichis a major challenge most deep learning approaches are faced with. We evaluateour method on several action recognition benchmarks, and exhibit its advantagesover the state-of-the-art.
arxiv-1411-4389 | Long-term Recurrent Convolutional Networks for Visual Recognition and Description |  http://arxiv.org/abs/1411.4389  | author:Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell category:cs.CV published:2014-11-17 summary:Models based on deep convolutional networks have dominated recent imageinterpretation tasks; we investigate whether models which are also recurrent,or "temporally deep", are effective for tasks involving sequences, visual andotherwise. We develop a novel recurrent convolutional architecture suitable forlarge-scale visual learning which is end-to-end trainable, and demonstrate thevalue of these models on benchmark video recognition tasks, image descriptionand retrieval problems, and video narration challenges. In contrast to currentmodels which assume a fixed spatio-temporal receptive field or simple temporalaveraging for sequential processing, recurrent convolutional models are "doublydeep"' in that they can be compositional in spatial and temporal "layers". Suchmodels may have advantages when target concepts are complex and/or trainingdata are limited. Learning long-term dependencies is possible whennonlinearities are incorporated into the network state updates. Long-term RNNmodels are appealing in that they directly can map variable-length inputs(e.g., video frames) to variable length outputs (e.g., natural language text)and can model complex temporal dynamics; yet they can be optimized withbackpropagation. Our recurrent long-term models are directly connected tomodern visual convnet models and can be jointly trained to simultaneously learntemporal dynamics and convolutional perceptual representations. Our resultsshow such models have distinct advantages over state-of-the-art models forrecognition or generation which are separately defined and/or optimized.
arxiv-1411-4503 | Outlier-Robust Convex Segmentation |  http://arxiv.org/abs/1411.4503  | author:Itamar Katz, Koby Crammer category:cs.LG stat.ML published:2014-11-17 summary:We derive a convex optimization problem for the task of segmenting sequentialdata, which explicitly treats presence of outliers. We describe two algorithmsfor solving this problem, one exact and one a top-down novel approach, and wederive a consistency results for the case of two segments and no outliers.Robustness to outliers is evaluated on two real-world tasks related to speechsegmentation. Our algorithms outperform baseline segmentation algorithms.
arxiv-1411-4342 | Influence Functions for Machine Learning: Nonparametric Estimators for Entropies, Divergences and Mutual Informations |  http://arxiv.org/abs/1411.4342  | author:Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry Wasserman, James M. Robins category:stat.ML cs.AI cs.LG published:2014-11-17 summary:We propose and analyze estimators for statistical functionals of one or moredistributions under nonparametric assumptions. Our estimators are based on thetheory of influence functions, which appear in the semiparametric statisticsliterature. We show that estimators based either on data-splitting or aleave-one-out technique enjoy fast rates of convergence and other favorabletheoretical properties. We apply this framework to derive estimators forseveral popular information theoretic quantities, and via empirical evaluation,show the advantage of this approach over existing estimators.
arxiv-1411-4565 | A Parallel Genetic Algorithm for Three Dimensional Bin Packing with Heterogeneous Bins |  http://arxiv.org/abs/1411.4565  | author:Drona Pratap Chandu category:cs.DC cs.NE published:2014-11-17 summary:This paper presents a parallel genetic algorithm for three dimensional binpacking with heterogeneous bins using Hadoop Map-Reduce framework. The mostcommon three dimensional bin packing problem which packs given set of boxesinto minimum number of equal sized bins is proven to be NP Hard. The variationof three dimensional bin packing problem that allows heterogeneous bin sizesand rotation of boxes is computationally more harder than common threedimensional bin packing problem. The proposed Map-Reduce implementation helpsto run the genetic algorithm for three dimensional bin packing withheterogeneous bins on multiple machines parallely and computes the solution inrelatively short time.
arxiv-1411-4598 | Joint Association Graph Screening and Decomposition for Large-scale Linear Dynamical Systems |  http://arxiv.org/abs/1411.4598  | author:Yiyuan She, Yuejia He, Shijie Li, Dapeng Wu category:stat.CO stat.ML published:2014-11-17 summary:This paper studies large-scale dynamical networks where the current state ofthe system is a linear transformation of the previous state, contaminated by amultivariate Gaussian noise. Examples include stock markets, human brains andgene regulatory networks. We introduce a transition matrix to describe theevolution, which can be translated to a directed Granger transition graph, anduse the concentration matrix of the Gaussian noise to capture the second-orderrelations between nodes, which can be translated to an undirected conditionaldependence graph. We propose regularizing the two graphs jointly in topologyidentification and dynamics estimation. Based on the notion of jointassociation graph (JAG), we develop a joint graphical screening and estimation(JGSE) framework for efficient network learning in big data. In particular, ourmethod can pre-determine and remove unnecessary edges based on the jointgraphical structure, referred to as JAG screening, and can decompose a largenetwork into smaller subnetworks in a robust manner, referred to as JAGdecomposition. JAG screening and decomposition can reduce the problem size andsearch space for fine estimation at a later stage. Experiments on bothsynthetic data and real-world applications show the effectiveness of theproposed framework in large-scale network topology identification and dynamicsestimation.
arxiv-1411-4618 | Relations World: A Possibilistic Graphical Model |  http://arxiv.org/abs/1411.4618  | author:Christopher J. C. Burges, Erin Renshaw, Andrzej Pastusiak category:cs.CL cs.AI published:2014-11-17 summary:We explore the idea of using a "possibilistic graphical model" as the basisfor a world model that drives a dialog system. As a first step we havedeveloped a system that uses text-based dialog to derive a model of the user'sfamily relations. The system leverages its world model to infer relationaltriples, to learn to recover from upstream coreference resolution errors andambiguities, and to learn context-dependent paraphrase models. We also exploresome theoretical aspects of the underlying graphical model.
arxiv-1411-4379 | FGPGA: An Efficient Genetic Approach for Producing Feasible Graph Partitions |  http://arxiv.org/abs/1411.4379  | author:Md. Lisul Islam, Novia Nurain, Swakkhar Shatabda, M Sohel Rahman category:cs.NE cs.AI cs.DC published:2014-11-17 summary:Graph partitioning, a well studied problem of parallel computing has manyapplications in diversified fields such as distributed computing, socialnetwork analysis, data mining and many other domains. In this paper, weintroduce FGPGA, an efficient genetic approach for producing feasible graphpartitions. Our method takes into account the heterogeneity and capacityconstraints of the partitions to ensure balanced partitioning. Such approachhas various applications in mobile cloud computing that include feasibledeployment of software applications on the more resourceful infrastructure inthe cloud instead of mobile hand set. Our proposed approach is light weight andhence suitable for use in cloud architecture. We ensure feasibility of thepartitions generated by not allowing over-sized partitions to be generatedduring the initialization and search. Our proposed method tested on standardbenchmark datasets significantly outperforms the state-of-the-art methods interms of quality of partitions and feasibility of the solutions.
arxiv-1411-4695 | Feedback Solution to Optimal Switching Problems with Switching Cost |  http://arxiv.org/abs/1411.4695  | author:Ali Heydari category:cs.SY math.OC stat.ML published:2014-11-17 summary:The problem of optimal switching between nonlinear autonomous subsystems isinvestigated in this study where the objective is not only bringing the statesto close to the desired point, but also adjusting the switching pattern, in thesense of penalizing switching occurrences and assigning different preferencesto utilization of different modes. The mode sequence is unspecified and aswitching cost term is used in the cost function for penalizing each switching.It is shown that once a switching cost is incorporated, the optimal cost-to-gofunction depends on the already active subsystem, i.e., the subsystem which wasengaged in the previous time step. Afterwards, an approximate dynamicprogramming based method is developed which provides an approximation of theoptimal solution to the problem in a feedback form and for different initialconditions. Finally, the performance of the method is analyzed throughnumerical examples.
arxiv-1411-4648 | A unifying framework for relaxations of the causal assumptions in Bell's theorem |  http://arxiv.org/abs/1411.4648  | author:Rafael Chaves, Richard Kueng, Jonatan Bohr Brask, David Gross category:quant-ph stat.ML published:2014-11-17 summary:Bell's Theorem shows that quantum mechanical correlations can violate theconstraints that the causal structure of certain experiments impose on anyclassical explanation. It is thus natural to ask to which degree the causalassumptions -- e.g. locality or measurement independence -- have to be relaxedin order to allow for a classical description of such experiments. Here, wedevelop a conceptual and computational framework for treating this problem. Weemploy the language of Bayesian networks to systematically constructalternative causal structures and bound the degree of relaxation usingquantitative measures that originate from the mathematical theory of causality.The main technical insight is that the resulting problems can often beexpressed as computationally tractable linear programs. We demonstrate theversatility of the framework by applying it to a variety of scenarios, rangingfrom relaxations of the measurement independence, locality and bilocalityassumptions, to a novel causal interpretation of CHSH inequality violations.
arxiv-1411-4670 | AlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline Arabic Handwriting Recognition |  http://arxiv.org/abs/1411.4670  | author:Mohamed E. Hussein, Marwan Torki, Ahmed Elsallamy, Mahmoud Fayyaz category:cs.CV I.5.2; I.7.5 published:2014-11-17 summary:In this paper, we introduce the first phase of a new dataset for offlineArabic handwriting recognition. The aim is to collect a very large dataset ofisolated Arabic words that covers all letters of the alphabet in all possibleshapes using a small number of simple words. The end goal is to collect a verylarge dataset of segmented letter images, which can be used to build andevaluate Arabic handwriting recognition systems that are based on segmentedletter recognition. The current version of the dataset contains $25114$ samplesof $109$ unique Arabic words that cover all possible shapes of all alphabetletters. The samples were collected from $907$ writers. In its current form,the dataset can be used for the problem of closed-vocabulary word recognition.We evaluated a number of window-based descriptors and classifiers on this taskand obtained an accuracy of $92.16\%$ using a SIFT-based descriptor and ANN.
arxiv-1411-4691 | Group Regularized Estimation under Structural Hierarchy |  http://arxiv.org/abs/1411.4691  | author:Yiyuan She, He Jiang category:math.ST stat.CO stat.ML stat.TH published:2014-11-17 summary:In high-dimensional models that involve interactions, statisticians usuallyfavor variable selection obeying certain logical hierarchical constraints. Thispaper focuses on structural hierarchy which means that the existence of aninteraction term implies that at least one or both associated main effects mustbe present. Lately this problem has attracted a lot of attentions fromstatisticians, but existing computational algorithms converge slow and cannotmeet the challenge of big data computation. More importantly, theoreticalstudies of hierarchical variable selection are extremely scarce, largely due tothe difficulty that multiple sparsity-promoting penalties are enforced on thesame subject. This work investigates a new type of estimator based on groupmulti-regularization to capture various types of structural parsimonysimultaneously. We present non-asymptotic results based on combined statisticaland computational analysis, and reveal the minimax optimal rate. Ageneral-purpose algorithm is developed with a theoretical guarantee of strictiterate convergence and global optimality. Simulations and real dataexperiments demonstrate the efficiency and efficacy of the proposed approach.
arxiv-1411-4568 | TILDE: A Temporally Invariant Learned DEtector |  http://arxiv.org/abs/1411.4568  | author:Yannick Verdie, Kwang Moo Yi, Pascal Fua, Vincent Lepetit category:cs.CV published:2014-11-17 summary:We introduce a learning-based approach to detect repeatable keypoints underdrastic imaging changes of weather and lighting conditions to whichstate-of-the-art keypoint detectors are surprisingly sensitive. We firstidentify good keypoint candidates in multiple training images taken from thesame viewpoint. We then train a regressor to predict a score map whose maximaare those points so that they can be found by simple non-maximum suppression.As there are no standard datasets to test the influence of these kinds ofchanges, we created our own, which we will make publicly available. We willshow that our method significantly outperforms the state-of-the-art methods insuch challenging conditions, while still achieving state-of-the-art performanceon the untrained standard Oxford dataset.
arxiv-1411-4455 | Errata: Distant Supervision for Relation Extraction with Matrix Completion |  http://arxiv.org/abs/1411.4455  | author:Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu, Thomas Fang Zheng, Edward Y. Chang category:cs.CL cs.LG published:2014-11-17 summary:The essence of distantly supervised relation extraction is that it is anincomplete multi-label classification problem with sparse and noisy features.To tackle the sparsity and noise challenges, we propose solving theclassification problem using matrix completion on factorized matrix ofminimized rank. We formulate relation classification as completing the unknownlabels of testing items (entity pairs) in a sparse matrix that concatenatestraining and testing textual features with training labels. Our algorithmicframework is based on the assumption that the rank of item-by-feature anditem-by-label joint matrix is low. We apply two optimization models to recoverthe underlying low-rank matrix leveraging the sparsity of feature-label matrix.The matrix completion problem is then solved by the fixed point continuation(FPC) algorithm, which can find the global optimum. Experiments on two widelyused datasets with different dimensions of textual features demonstrate thatour low-rank matrix completion approach significantly outperforms the baselineand the state-of-the-art methods.
arxiv-1411-4679 | Pseudo Dynamic Transitional Modeling of Building Heating Energy Demand Using Artificial Neural Network |  http://arxiv.org/abs/1411.4679  | author:S. Paudel, M. Elmtiri, W. L. Kling, O. Le Corre, B. Lacarriere category:cs.CE cs.NE published:2014-11-17 summary:This paper presents the building heating demand prediction model withoccupancy profile and operational heating power level characteristics in shorttime horizon (a couple of days) using artificial neural network. In addition,novel pseudo dynamic transitional model is introduced, which consider timedependent attributes of operational power level characteristics and its effectin the overall model performance is outlined. Pseudo dynamic model is appliedto a case study of French Institution building and compared its results withstatic and other pseudo dynamic neural network models. The results show thecoefficients of correlation in static and pseudo dynamic neural network modelof 0.82 and 0.89 (with energy consumption error of 0.02%) during the learningphase, and 0.61 and 0.85 during the prediction phase respectively. Further,orthogonal array design is applied to the pseudo dynamic model to check theschedule of occupancy profile and operational heating power levelcharacteristics. The results show the new schedule and provide the robustdesign for pseudo dynamic model. Due to prediction in short time horizon, itfinds application for Energy Services Company (ESCOs) to manage the heatingload for dynamic control of heat production system.
arxiv-1411-4491 | Joint cross-domain classification and subspace learning for unsupervised adaptation |  http://arxiv.org/abs/1411.4491  | author:Basura Fernando, Tatiana Tommasi, Tinne Tuytelaars category:cs.CV cs.LG published:2014-11-17 summary:Domain adaptation aims at adapting the knowledge acquired on a source domainto a new different but related target domain. Several approaches havebeenproposed for classification tasks in the unsupervised scenario, where nolabeled target data are available. Most of the attention has been dedicated tosearching a new domain-invariant representation, leaving the definition of theprediction function to a second stage. Here we propose to learn both jointly.Specifically we learn the source subspace that best matches the target subspacewhile at the same time minimizing a regularized misclassification loss. Weprovide an alternating optimization technique based on stochastic sub-gradientdescent to solve the learning problem and we demonstrate its performance onseveral domain adaptation tasks.
arxiv-1411-4472 | Opinion mining of text documents written in Macedonian language |  http://arxiv.org/abs/1411.4472  | author:Andrej Gajduk, Ljupco Kocarev category:cs.CL published:2014-11-17 summary:The ability to extract public opinion from web portals such as review sites,social networks and blogs will enable companies and individuals to form a view,an attitude and make decisions without having to do lengthy and costlyresearches and surveys. In this paper machine learning techniques are used fordetermining the polarity of forum posts on kajgana which are written inMacedonian language. The posts are classified as being positive, negative orneutral. We test different feature metrics and classifiers and provide detailedevaluation of their participation in improving the overall performance on amanually generated dataset. By achieving 92% accuracy, we show that theperformance of systems for automated opinion mining is comparable to a humanevaluator, thus making it a viable option for text data analysis. Finally, wepresent a few statistics derived from the forum posts using the developedsystem.
arxiv-1411-4246 | GreMuTRRR: A Novel Genetic Algorithm to Solve Distance Geometry Problem for Protein Structures |  http://arxiv.org/abs/1411.4246  | author:Md. Lisul Islam, Swakkhar Shatabda, M. Sohel Rahman category:cs.NE cs.CE published:2014-11-16 summary:Nuclear Magnetic Resonance (NMR) Spectroscopy is a widely used technique topredict the native structure of proteins. However, NMR machines are only ableto report approximate and partial distances between pair of atoms. To build theprotein structure one has to solve the Euclidean distance geometry problemgiven the incomplete interval distance data produced by NMR machines. In thispaper, we propose a new genetic algorithm for solving the Euclidean distancegeometry problem for protein structure prediction given sparse NMR data. Ourgenetic algorithm uses a greedy mutation operator to intensify the search, atwin removal technique for diversification in the population and a randomrestart method to recover stagnation. On a standard set of benchmark dataset,our algorithm significantly outperforms standard genetic algorithms.
arxiv-1411-4286 | HIPAD - A Hybrid Interior-Point Alternating Direction algorithm for knowledge-based SVM and feature selection |  http://arxiv.org/abs/1411.4286  | author:Zhiwei Qin, Xiaocheng Tang, Ioannis Akrotirianakis, Amit Chakraborty category:stat.ML cs.LG published:2014-11-16 summary:We consider classification tasks in the regime of scarce labeled trainingdata in high dimensional feature space, where specific expert knowledge is alsoavailable. We propose a new hybrid optimization algorithm that solves theelastic-net support vector machine (SVM) through an alternating directionmethod of multipliers in the first phase, followed by an interior-point methodfor the classical SVM in the second phase. Both SVM formulations are adapted toknowledge incorporation. Our proposed algorithm addresses the challenges ofautomatic feature selection, high optimization accuracy, and algorithmicflexibility for taking advantage of prior knowledge. We demonstrate theeffectiveness and efficiency of our algorithm and compare it with existingmethods on a collection of synthetic and real-world data.
arxiv-1411-4296 | Combining contextual and local edges for line segment extraction in cluttered images |  http://arxiv.org/abs/1411.4296  | author:Rui F. C. Guerreiro category:cs.CV published:2014-11-16 summary:Automatic extraction methods typically assume that line segments arepronounced, thin, few and far between, do not cross each other, and are noiseand clutter-free. Since these assumptions often fail in realistic scenarios,many line segments are not detected or are fragmented. In more severe cases,i.e., many who use the Hough Transform, extraction can fail entirely. In thispaper, we propose a method that tackles these issues. Its key aspect is thecombination of thresholded image derivatives obtained with filters of large andsmall footprints, which we denote as contextual and local edges, respectively.Contextual edges are robust to noise and we use them to select valid localedges, i.e., local edges that are of the same type as contextual ones:dark-to-bright transition of vice-versa. If the distance between valid localedges does not exceed a maximum distance threshold, we enforce connectivity bymarking them and the pixels in between as edge points. This originatesconnected edge maps that are robust and well localized. We use a powerfultwo-sample statistical test to compute contextual edges, which we introducebriefly, as they are unfamiliar to the image processing community. Finally, wepresent experiments that illustrate, with synthetic and real images, how ourmethod is efficient in extracting complete segments of all lengths and widthsin several situations where current methods fail.
arxiv-1411-4304 | Ten Years of Pedestrian Detection, What Have We Learned? |  http://arxiv.org/abs/1411.4304  | author:Rodrigo Benenson, Mohamed Omran, Jan Hosang, Bernt Schiele category:cs.CV published:2014-11-16 summary:Paper-by-paper results make it easy to miss the forest for the trees.Weanalyse the remarkable progress of the last decade by discussing the main ideasexplored in the 40+ detectors currently present in the Caltech pedestriandetection benchmark. We observe that there exist three families of approaches,all currently reaching similar detection quality. Based on our analysis, westudy the complementarity of the most promising ideas by combining multiplepublished strategies. This new decision forest detector achieves the currentbest known performance on the challenging Caltech-USA dataset.
arxiv-1411-4280 | Efficient Object Localization Using Convolutional Networks |  http://arxiv.org/abs/1411.4280  | author:Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, Christopher Bregler category:cs.CV published:2014-11-16 summary:Recent state-of-the-art performance on human-body pose estimation has beenachieved with Deep Convolutional Networks (ConvNets). Traditional ConvNetarchitectures include pooling and sub-sampling layers which reducecomputational requirements, introduce invariance and prevent over-training.These benefits of pooling come at the cost of reduced localization accuracy. Weintroduce a novel architecture which includes an efficient `positionrefinement' model that is trained to estimate the joint offset location withina small region of the image. This refinement model is jointly trained incascade with a state-of-the-art ConvNet model to achieve improved accuracy inhuman joint location estimation. We show that the variance of our detectorapproaches the variance of human annotations on the FLIC dataset andoutperforms all existing approaches on the MPII-human-pose dataset.
arxiv-1411-4331 | A Latent Clothing Attribute Approach for Human Pose Estimation |  http://arxiv.org/abs/1411.4331  | author:Weipeng Zhang, Jie Shen, Guangcan Liu, Yong Yu category:cs.CV published:2014-11-16 summary:As a fundamental technique that concerns several vision tasks such as imageparsing, action recognition and clothing retrieval, human pose estimation (HPE)has been extensively investigated in recent years. To achieve accurate andreliable estimation of the human pose, it is well-recognized that the clothingattributes are useful and should be utilized properly. Most previousapproaches, however, require to manually annotate the clothing attributes andare therefore very costly. In this paper, we shall propose and explore a\emph{latent} clothing attribute approach for HPE. Unlike previous approaches,our approach models the clothing attributes as latent variables and thusrequires no explicit labeling for the clothing attributes. The inference of thelatent variables are accomplished by utilizing the framework of latentstructured support vector machines (LSSVM). We employ the strategy of\emph{alternating direction} to train the LSSVM model: In each iteration, onekind of variables (e.g., human pose or clothing attribute) are fixed and theothers are optimized. Our extensive experiments on two real-world benchmarksshow the state-of-the-art performance of our proposed approach.
arxiv-1411-4229 | Efficient and Accurate Approximations of Nonlinear Convolutional Networks |  http://arxiv.org/abs/1411.4229  | author:Xiangyu Zhang, Jianhua Zou, Xiang Ming, Kaiming He, Jian Sun category:cs.CV published:2014-11-16 summary:This paper aims to accelerate the test-time computation of deep convolutionalneural networks (CNNs). Unlike existing methods that are designed forapproximating linear filters or linear responses, our method takes thenonlinear units into account. We minimize the reconstruction error of thenonlinear responses, subject to a low-rank constraint which helps to reduce thecomplexity of filters. We develop an effective solution to this constrainednonlinear optimization problem. An algorithm is also presented for reducing theaccumulated error when multiple layers are approximated. A whole-model speedupratio of 4x is demonstrated on a large network trained for ImageNet, while thetop-5 error rate is only increased by 0.9%. Our accelerated model has acomparably fast speed as the "AlexNet", but is 4.7% more accurate.
arxiv-1411-4199 | Revisiting Kernelized Locality-Sensitive Hashing for Improved Large-Scale Image Retrieval |  http://arxiv.org/abs/1411.4199  | author:Ke Jiang, Qichao Que, Brian Kulis category:cs.CV cs.LG stat.ML published:2014-11-16 summary:We present a simple but powerful reinterpretation of kernelizedlocality-sensitive hashing (KLSH), a general and popular method developed inthe vision community for performing approximate nearest-neighbor searches in anarbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is basedon viewing the steps of the KLSH algorithm in an appropriately projected space,and has several key theoretical and practical benefits. First, it eliminatesthe problematic conceptual difficulties that are present in the existingmotivation of KLSH. Second, it yields the first formal retrieval performancebounds for KLSH. Third, our analysis reveals two techniques for boosting theempirical performance of KLSH. We evaluate these extensions on severallarge-scale benchmark image retrieval data sets, and show that our analysisleads to improved recall performance of at least 12%, and sometimes muchhigher, over the standard KLSH method.
arxiv-1411-4098 | GASP : Geometric Association with Surface Patches |  http://arxiv.org/abs/1411.4098  | author:Rahul Sawhney, Fuxin Li, Henrik I. Christensen category:cs.CV cs.GR cs.RO published:2014-11-15 summary:A fundamental challenge to sensory processing tasks in perception androbotics is the problem of obtaining data associations across views. We presenta robust solution for ascertaining potentially dense surface patch (superpixel)associations, requiring just range information. Our approach involvesdecomposition of a view into regularized surface patches. We represent them assequences expressing geometry invariantly over their superpixel neighborhoods,as uniquely consistent partial orderings. We match these representationsthrough an optimal sequence comparison metric based on the Damerau-Levenshteindistance - enabling robust association with quadratic complexity (in contrastto hitherto employed joint matching formulations which are NP-complete). Theapproach is able to perform under wide baselines, heavy rotations, partialoverlaps, significant occlusions and sensor noise. The technique does not require any priors -- motion or otherwise, and doesnot make restrictive assumptions on scene structure and sensor movement. Itdoes not require appearance -- is hence more widely applicable than appearancereliant methods, and invulnerable to related ambiguities such as textureless oraliased content. We present promising qualitative and quantitative resultsunder diverse settings, along with comparatives with popular approaches basedon range as well as RGB-D data.
arxiv-1411-4101 | Deep Deconvolutional Networks for Scene Parsing |  http://arxiv.org/abs/1411.4101  | author:Rahul Mohan category:stat.ML cs.CV cs.LG published:2014-11-15 summary:Scene parsing is an important and challenging prob- lem in computer vision.It requires labeling each pixel in an image with the category it belongs to.Tradition- ally, it has been approached with hand-engineered features fromcolor information in images. Recently convolutional neural networks (CNNs),which automatically learn hierar- chies of features, have achieved recordperformance on the task. These approaches typically include a post-processingtechnique, such as superpixels, to produce the final label- ing. In this paper,we propose a novel network architecture that combines deep deconvolutionalneural networks with CNNs. Our experiments show that deconvolutional neu- ralnetworks are capable of learning higher order image structure beyond edgeprimitives in comparison to CNNs. The new network architecture is employed formulti-patch training, introduced as part of this work. Multi-patch train- ingmakes it possible to effectively learn spatial priors from scenes. The proposedapproach yields state-of-the-art per- formance on four scene parsing datasets,namely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, oursystem has the added advantage of having a training system that can becompletely automated end-to-end with- out requiring any post-processing.
arxiv-1411-4102 | Anisotropic Agglomerative Adaptive Mean-Shift |  http://arxiv.org/abs/1411.4102  | author:Rahul Sawhney, Henrik I. Christensen, Gary R. Bradski category:cs.CV cs.LG published:2014-11-15 summary:Mean Shift today, is widely used for mode detection and clustering. Thetechnique though, is challenged in practice due to assumptions of isotropicityand homoscedasticity. We present an adaptive Mean Shift methodology that allowsfor full anisotropic clustering, through unsupervised local bandwidthselection. The bandwidth matrices evolve naturally, adapting locally throughagglomeration, and in turn guiding further agglomeration. The onlinemethodology is practical and effecive for low-dimensional feature spaces,preserving better detail and clustering salience. Additionally, conventionalMean Shift either critically depends on a per instance choice of bandwidth, orrelies on offline methods which are inflexible and/or again data instancespecific. The presented approach, due to its adaptive design, also alleviatesthis issue - with a default form performing generally well. The methodologythough, allows for effective tuning of results.
arxiv-1411-4109 | Resolution of Difficult Pronouns Using the ROSS Method |  http://arxiv.org/abs/1411.4109  | author:Glenn R. Hofford category:cs.CL cs.AI published:2014-11-15 summary:A new natural language understanding method for disambiguation of difficultpronouns is described. Difficult pronouns are those pronouns for which a levelof world or domain knowledge is needed in order to perform anaphoral or othertypes of resolution. Resolution of difficult pronouns may in some cases requirea prior step involving the application of inference to a situation that isrepresented by the natural language text. A general method is described: itperforms entity resolution and pronoun resolution. An extension to the generalpronoun resolution method performs inference as an embedded commonsensereasoning method. The general method and the embedded method utilize featuresof the ROSS representational scheme; in particular the methods use ROSSontology classes and the ROSS situation model. The overall method is a workingsolution that solves the following Winograd schemas: a) trophy and suitcase, b)person lifts person, c) person pays detective, and d) councilmen anddemonstrators.
arxiv-1411-4114 | Definition of Visual Speech Element and Research on a Method of Extracting Feature Vector for Korean Lip-Reading |  http://arxiv.org/abs/1411.4114  | author:Ha Jong Won, Li Gwang Chol, Kim Hyok Chol, Li Kum Song category:cs.CL cs.CV cs.LG published:2014-11-15 summary:In this paper, we defined the viseme (visual speech element) and describedabout the method of extracting visual feature vector. We defined the 10 visemesbased on vowel by analyzing of Korean utterance and proposed the method ofextracting the 20-dimensional visual feature vector, combination of staticfeatures and dynamic features. Lastly, we took an experiment in recognizingwords based on 3-viseme HMM and evaluated the efficiency.
arxiv-1411-4116 | Investigating the Role of Prior Disambiguation in Deep-learning Compositional Models of Meaning |  http://arxiv.org/abs/1411.4116  | author:Jianpeng Cheng, Dimitri Kartsaklis, Edward Grefenstette category:cs.CL cs.LG cs.NE published:2014-11-15 summary:This paper aims to explore the effect of prior disambiguation on neuralnetwork- based compositional models, with the hope that better semanticrepresentations for text compounds can be produced. We disambiguate the inputword vectors before they are fed into a compositional deep net. A series ofevaluations shows the positive effect of prior disambiguation for such deepmodels.
arxiv-1411-4148 | Diversity Handling In Evolutionary Landscape |  http://arxiv.org/abs/1411.4148  | author:Maumita Bhattacharya category:cs.NE 68T05 published:2014-11-15 summary:The search ability of an Evolutionary Algorithm (EA) depends on the variationamong the individuals in the population. Maintaining an optimal level ofdiversity in the EA population is imperative to ensure that progress of the EAsearch is unhindered by premature convergence to suboptimal solutions. Clearerunderstanding of the concept of population diversity, in the context ofevolutionary search and premature convergence in particular, is the key todesigning efficient EAs. To this end, this paper first presents a comprehensiveanalysis of the EA population diversity issues. Next we present aninvestigation on a counter-niching EA technique that introduces and maintainsconstructive diversity in the population. The proposed approach uses informedgenetic operations to reach promising, but un-explored or under-explored areasof the search space, while discouraging premature local convergence. Simulationruns on a number of standard benchmark test functions with Genetic Algorithm(GA) implementation shows promising results.
arxiv-1411-4194 | ROSS User's Guide and Reference Manual (Version 1.0) |  http://arxiv.org/abs/1411.4194  | author:Glenn R. Hofford category:cs.AI cs.CL published:2014-11-15 summary:The ROSS method is a new approach in the area of knowledge representationthat is useful for many artificial intelligence and natural languageunderstanding representation and reasoning tasks. (ROSS stands for"Representation", "Ontology", "Structure", "Star" language). ROSS is a physicalsymbol-based representational scheme. ROSS provides a complex model for thedeclarative representation of physical structure and for the representation ofprocesses and causality. From the metaphysical perspective, the ROSS view ofexternal reality involves a 4D model, wherein discrete single-time-pointunit-sized locations with states are the basis for all objects, processes andaspects that can be modeled. ROSS includes a language called "Star" for thespecification of ontology classes. The ROSS method also includes a formalscheme called the "instance model". Instance models are used in the area ofnatural language meaning representation to represent situations. This documentis an in-depth specification of the ROSS method.
arxiv-1411-4166 | Retrofitting Word Vectors to Semantic Lexicons |  http://arxiv.org/abs/1411.4166  | author:Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, Noah A. Smith category:cs.CL published:2014-11-15 summary:Vector space word representations are learned from distributional informationof words in large corpora. Although such statistics are semanticallyinformative, they disregard the valuable information that is contained insemantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. Thispaper proposes a method for refining vector space representations usingrelational information from semantic lexicons by encouraging linked words tohave similar vector representations, and it makes no assumptions about how theinput vectors were constructed. Evaluated on a battery of standard lexicalsemantic evaluation tasks in several languages, we obtain substantialimprovements starting with a variety of word vector models. Our refinementmethod outperforms prior techniques for incorporating semantic lexicons intothe word vector training algorithms.
arxiv-1411-4086 | Error Rate Bounds and Iterative Weighted Majority Voting for Crowdsourcing |  http://arxiv.org/abs/1411.4086  | author:Hongwei Li, Bin Yu category:stat.ML cs.HC cs.LG math.PR math.ST stat.TH published:2014-11-15 summary:Crowdsourcing has become an effective and popular tool for human-poweredcomputation to label large datasets. Since the workers can be unreliable, it iscommon in crowdsourcing to assign multiple workers to one task, and toaggregate the labels in order to obtain results of high quality. In this paper,we provide finite-sample exponential bounds on the error rate (in probabilityand in expectation) of general aggregation rules under the Dawid-Skenecrowdsourcing model. The bounds are derived for multi-class labeling, and canbe used to analyze many aggregation methods, including majority voting,weighted majority voting and the oracle Maximum A Posteriori (MAP) rule. Weshow that the oracle MAP rule approximately optimizes our upper bound on themean error rate of weighted majority voting in certain setting. We propose aniterative weighted majority voting (IWMV) method that optimizes the error ratebound and approximates the oracle MAP rule. Its one step version has a provabletheoretical guarantee on the error rate. The IWMV method is intuitive andcomputationally simple. Experimental results on simulated and real data showthat IWMV performs at least on par with the state-of-the-art methods, and ithas a much lower computational cost (around one hundred times faster) than thestate-of-the-art methods.
arxiv-1411-4033 | Sparse And Low Rank Decomposition Based Batch Image Alignment for Speckle Reduction of retinal OCT Images |  http://arxiv.org/abs/1411.4033  | author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV published:2014-11-14 summary:Optical Coherence Tomography (OCT) is an emerging technique in the field ofbiomedical imaging, with applications in ophthalmology, dermatology, coronaryimaging etc. Due to the underlying physics, OCT images usually suffer from agranular pattern, called speckle noise, which restricts the process ofinterpretation. Here, a sparse and low rank decomposition based method is usedfor speckle reduction in retinal OCT images. This technique works on input datathat consists of several B-scans of the same location. The next step is thebatch alignment of the images using a sparse and low-rank decomposition basedtechnique. Finally the denoised image is created by median filtering of thelow-rank component of the processed data. Simultaneous decomposition andalignment of the images result in better performance in comparison to simpleregistration-based methods that are used in the literature for noise reductionof OCT images.
arxiv-1411-4070 | A unified view of generative models for networks: models, methods, opportunities, and challenges |  http://arxiv.org/abs/1411.4070  | author:Abigail Z. Jacobs, Aaron Clauset category:stat.ML cs.LG cs.SI physics.soc-ph published:2014-11-14 summary:Research on probabilistic models of networks now spans a wide variety offields, including physics, sociology, biology, statistics, and machinelearning. These efforts have produced a diverse ecology of models and methods.Despite this diversity, many of these models share a common underlyingstructure: pairwise interactions (edges) are generated with probabilityconditional on latent vertex attributes. Differences between models generallystem from different philosophical choices about how to learn from data ordifferent empirically-motivated goals. The highly interdisciplinary nature ofwork on these generative models, however, has inhibited the development of aunified view of their similarities and differences. For instance, noveltheoretical models and optimization techniques developed in machine learningare largely unknown within the social and biological sciences, which haveinstead emphasized model interpretability. Here, we describe a unified view ofgenerative models for networks that draws together many of these disparatethreads and highlights the fundamental similarities and differences that spanthese fields. We then describe a number of opportunities and challenges forfuture work that are revealed by this view.
arxiv-1411-4000 | How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets |  http://arxiv.org/abs/1411.4000  | author:Zhiyun Lu, Avner May, Kuan Liu, Alireza Bagheri Garakani, Dong Guo, Aurélien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, Fei Sha category:cs.LG stat.ML published:2014-11-14 summary:The computational complexity of kernel methods has often been a major barrierfor applying them to large-scale learning problems. We argue that this barriercan be effectively overcome. In particular, we develop methods to scale upkernel models to successfully tackle large-scale learning problems that are sofar only approachable by deep learning architectures. Based on the seminal workby Rahimi and Recht on approximating kernel functions with features derivedfrom random projections, we advance the state-of-the-art by proposing methodsthat can efficiently train models with hundreds of millions of parameters, andlearn optimal representations from multiple kernels. We conduct extensiveempirical studies on problems from image recognition and automatic speechrecognition, and show that the performance of our kernel models matches that ofwell-engineered deep neural nets (DNNs). To the best of our knowledge, this isthe first time that a direct comparison between these two methods onlarge-scale problems is reported. Our kernel methods have several appealingproperties: training with convex optimization, cost for training a single modelcomparable to DNNs, and significantly reduced total cost due to fewerhyperparameters to tune for model selection. Our contrastive study betweenthese two very different but equally competitive models sheds light onfundamental questions such as how to learn good representations.
arxiv-1411-4072 | Learning Multi-Relational Semantics Using Neural-Embedding Models |  http://arxiv.org/abs/1411.4072  | author:Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng category:cs.CL cs.LG stat.ML published:2014-11-14 summary:In this paper we present a unified framework for modeling multi-relationalrepresentations, scoring, and learning, and conduct an empirical study ofseveral recent multi-relational embedding models under the framework. Weinvestigate the different choices of relation operators based on linear andbilinear transformations, and also the effects of entity representations byincorporating unsupervised vectors pre-trained on extra textual resources. Ourresults show several interesting findings, enabling the design of a simpleembedding model that achieves the new state-of-the-art performance on a popularknowledge base completion task evaluated on Freebase.
arxiv-1411-3919 | Sample-targeted clinical trial adaptation |  http://arxiv.org/abs/1411.3919  | author:Ognjen Arandjelovic category:cs.LG published:2014-11-14 summary:Clinical trial adaptation refers to any adjustment of the trial protocolafter the onset of the trial. The main goal is to make the process ofintroducing new medical interventions to patients more efficient by reducingthe cost and the time associated with evaluating their safety and efficacy. Theprincipal question is how should adaptation be performed so as to minimize thechance of distorting the outcome of the trial. We propose a novel method forachieving this. Unlike previous work our approach focuses on trial adaptationby sample size adjustment. We adopt a recently proposed stratificationframework based on collected auxiliary data and show that this informationtogether with the primary measured variables can be used to make aprobabilistically informed choice of the particular sub-group a sample shouldbe removed from. Experiments on simulated data are used to illustrate theeffectiveness of our method and its application in practice.
arxiv-1411-4046 | Deep Belief Network Training Improvement Using Elite Samples Minimizing Free Energy |  http://arxiv.org/abs/1411.4046  | author:Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour category:cs.LG cs.CV published:2014-11-14 summary:Nowadays this is very popular to use deep architectures in machine learning.Deep Belief Networks (DBNs) are deep architectures that use stack of RestrictedBoltzmann Machines (RBM) to create a powerful generative model using trainingdata. In this paper we present an improvement in a common method that isusually used in training of RBMs. The new method uses free energy as acriterion to obtain elite samples from generative model. We argue that thesesamples can more accurately compute gradient of log probability of trainingdata. According to the results, an error rate of 0.99% was achieved on MNISTtest set. This result shows that the proposed method outperforms the methodpresented in the first paper introducing DBN (1.25% error rate) and generalclassification methods such as SVM (1.4% error rate) and KNN (with 1.6% errorrate). In another test using ISOLET dataset, letter classification errordropped to 3.59% compared to 5.59% error rate achieved in those papers usingthis dataset. The implemented method is available online at"http://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html".
arxiv-1411-4076 | Association Rule Based Flexible Machine Learning Module for Embedded System Platforms like Android |  http://arxiv.org/abs/1411.4076  | author:Amiraj Dhawan, Shruti Bhave, Amrita Aurora, Vishwanathan Iyer category:cs.CY cs.HC cs.LG published:2014-11-14 summary:The past few years have seen a tremendous growth in the popularity ofsmartphones. As newer features continue to be added to smartphones to increasetheir utility, their significance will only increase in future. Combiningmachine learning with mobile computing can enable smartphones to become'intelligent' devices, a feature which is hitherto unseen in them. Also, thecombination of machine learning and context aware computing can enablesmartphones to gauge user's requirements proactively, depending upon theirenvironment and context. Accordingly, necessary services can be provided tousers. In this paper, we have explored the methods and applications of integratingmachine learning and context aware computing on the Android platform, toprovide higher utility to the users. To achieve this, we define a MachineLearning (ML) module which is incorporated in the basic Android architecture.Firstly, we have outlined two major functionalities that the ML module shouldprovide. Then, we have presented three architectures, each of whichincorporates the ML module at a different level in the Android architecture.The advantages and shortcomings of each of these architectures have beenevaluated. Lastly, we have explained a few applications in which our proposedsystem can be incorporated such that their functionality is improved.
arxiv-1411-4068 | Dynamic Programming for Instance Annotation in Multi-instance Multi-label Learning |  http://arxiv.org/abs/1411.4068  | author:Anh T. Pham, Raviv Raich, Xiaoli Z. Fern category:stat.ML cs.LG published:2014-11-14 summary:Labeling data for classification requires significant human effort. To reducelabeling cost, instead of labeling every instance, a group of instances (bag)is labeled by a single bag label. Computer algorithms are then used to inferthe label for each instance in a bag, a process referred to as instanceannotation. This task is challenging due to the ambiguity regarding theinstance labels. We propose a discriminative probabilistic model for theinstance annotation problem and introduce an expectation maximization frameworkfor inference, based on the maximum likelihood approach. For many probabilisticapproaches, brute-force computation of the instance label posterior probabilitygiven its bag label is exponential in the number of instances in the bag. Ourkey contribution is a dynamic programming method for computing the posteriorthat is linear in the number of instances. We evaluate our methods using bothbenchmark and real world data sets, in the domain of bird song, imageannotation, and activity recognition. In many cases, the proposed frameworkoutperforms, sometimes significantly, the current state-of-the-art MIMLlearning methods, both in instance label prediction and bag label prediction.
arxiv-1411-4005 | A convex formulation for hyperspectral image superresolution via subspace-based regularization |  http://arxiv.org/abs/1411.4005  | author:Miguel Simões, José Bioucas-Dias, Luis B. Almeida, Jocelyn Chanussot category:cs.CV stat.ML published:2014-11-14 summary:Hyperspectral remote sensing images (HSIs) usually have high spectralresolution and low spatial resolution. Conversely, multispectral images (MSIs)usually have low spectral and high spatial resolutions. The problem ofinferring images which combine the high spectral and high spatial resolutionsof HSIs and MSIs, respectively, is a data fusion problem that has been thefocus of recent active research due to the increasing availability of HSIs andMSIs retrieved from the same geographical area. We formulate this problem as the minimization of a convex objective functioncontaining two quadratic data-fitting terms and an edge-preserving regularizer.The data-fitting terms account for blur, different resolutions, and additivenoise. The regularizer, a form of vector Total Variation, promotespiecewise-smooth solutions with discontinuities aligned across thehyperspectral bands. The downsampling operator accounting for the different spatial resolutions,the non-quadratic and non-smooth nature of the regularizer, and the very largesize of the HSI to be estimated lead to a hard optimization problem. We dealwith these difficulties by exploiting the fact that HSIs generally "live" in alow-dimensional subspace and by tailoring the Split Augmented LagrangianShrinkage Algorithm (SALSA), which is an instance of the Alternating DirectionMethod of Multipliers (ADMM), to this optimization problem, by means of aconvenient variable splitting. The spatial blur and the spectral linearoperators linked, respectively, with the HSI and MSI acquisition processes arealso estimated, and we obtain an effective algorithm that outperforms thestate-of-the-art, as illustrated in a series of experiments with simulated andreal-life data.
arxiv-1411-3806 | Integrating Fuzzy and Ant Colony System for Fuzzy Vehicle Routing Problem with Time Windows |  http://arxiv.org/abs/1411.3806  | author:Sandhya Bansal, V. Katiyar category:cs.AI cs.CE cs.NE published:2014-11-14 summary:In this paper fuzzy VRPTW with an uncertain travel time is considered.Credibility theory is used to model the problem and specifies a preferenceindex at which it is desired that the travel times to reach the customers fallinto their time windows. We propose the integration of fuzzy and ant colonysystem based evolutionary algorithm to solve the problem while preserving theconstraints. Computational results for certain benchmark problems having shortand long time horizons are presented to show the effectiveness of thealgorithm. Comparison between different preferences indexes have been obtainedto help the user in making suitable decisions.
arxiv-1411-4064 | A Faster Method for Tracking and Scoring Videos Corresponding to Sentences |  http://arxiv.org/abs/1411.4064  | author:Haonan Yu, Daniel P. Barrett, Jeffrey Mark Siskind category:cs.CV published:2014-11-14 summary:Prior work presented the sentence tracker, a method for scoring how well asentence describes a video clip or alternatively how well a video clip depictsa sentence. We present an improved method for optimizing the same cost functionemployed by this prior work, reducing the space complexity from exponential inthe sentence length to polynomial, as well as producing a qualitativelyidentical result in time polynomial in the sentence length instead ofexponential. Since this new method is plug-compatible with the prior method, itcan be used for the same applications: video retrieval with sentential queries,generating sentential descriptions of video clips, and focusing the attentionof a tracker with a sentence, while allowing these applications to scale withsignificantly larger numbers of object detections, word meanings modeled withHMMs with significantly larger numbers of states, and significantly longersentences, with no appreciable degradation in quality of results.
arxiv-1411-3972 | Causal Inference by Identification of Vector Autoregressive Processes with Hidden Components |  http://arxiv.org/abs/1411.3972  | author:Philipp Geiger, Kun Zhang, Mingming Gong, Dominik Janzing, Bernhard Schölkopf category:stat.ML published:2014-11-14 summary:A widely applied approach to causal inference from a non-experimental timeseries $X$, often referred to as "(linear) Granger causal analysis", is toregress present on past and interpret the regression matrix $\hat{B}$ causally.However, if there is an unmeasured time series $Z$ that influences $X$, thenthis approach can lead to wrong causal conclusions, i.e., distinct from thoseone would draw if one had additional information such as $Z$. In this paper wetake a different approach: We assume that $X$ together with some hidden $Z$forms a first order vector autoregressive (VAR) process with transition matrix$A$, and argue why it is more valid to interpret $A$ causally instead of$\hat{B}$. Then we examine under which conditions the most important parts of$A$ are identifiable or almost identifiable from only $X$. Essentially,sufficient conditions are (1) non-Gaussian, independent noise or (2) noinfluence from $X$ to $Z$. We present two estimation algorithms that aretailored towards conditions (1) and (2), respectively, and evaluate them onsynthetic and real-world data. We discuss how to check the model using $X$.
arxiv-1411-4006 | A Discriminative CNN Video Representation for Event Detection |  http://arxiv.org/abs/1411.4006  | author:Zhongwen Xu, Yi Yang, Alexander G. Hauptmann category:cs.CV published:2014-11-14 summary:In this paper, we propose a discriminative video representation for eventdetection over a large scale video dataset when only limited hardware resourcesare available. The focus of this paper is to effectively leverage deepConvolutional Neural Networks (CNNs) to advance event detection, where onlyframe level static descriptors can be extracted by the existing CNN toolkit.This paper makes two contributions to the inference of CNN videorepresentation. First, while average pooling and max pooling have long been thestandard approaches to aggregating frame level static features, we show thatperformance can be significantly improved by taking advantage of an appropriateencoding method. Second, we propose using a set of latent concept descriptorsas the frame descriptor, which enriches visual information while keeping itcomputationally affordable. The integration of the two contributions results ina new state-of-the-art performance in event detection over the largest videodatasets. Compared to improved Dense Trajectories, which has been recognized asthe best video representation for event detection, our new representationimproves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVIDMEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset.This work is the core part of the winning solution of our CMU-Informedia teamin TRECVID MED 2014 competition.
arxiv-1411-3784 | Deep Narrow Boltzmann Machines are Universal Approximators |  http://arxiv.org/abs/1411.3784  | author:Guido Montufar category:stat.ML cs.LG math.PR published:2014-11-14 summary:We show that deep narrow Boltzmann machines are universal approximators ofprobability distributions on the activities of their visible units, providedthey have sufficiently many hidden layers, each containing the same number ofunits as the visible layer. We show that, within certain parameter domains,deep Boltzmann machines can be studied as feedforward networks. We provideupper and lower bounds on the sufficient depth and width of universalapproximators. These results settle various intuitions regarding undirectednetworks and, in particular, they show that deep narrow Boltzmann machines areat least as compact universal approximators as narrow sigmoid belief networksand restricted Boltzmann machines, with respect to the currently availablebounds for those models.
arxiv-1411-3815 | Predictive Encoding of Contextual Relationships for Perceptual Inference, Interpolation and Prediction |  http://arxiv.org/abs/1411.3815  | author:Mingmin Zhao, Chengxu Zhuang, Yizhou Wang, Tai Sing Lee category:cs.LG cs.CV cs.NE published:2014-11-14 summary:We propose a new neurally-inspired model that can learn to encode the globalrelationship context of visual events across time and space and to use thecontextual information to modulate the analysis by synthesis process in apredictive coding framework. The model learns latent contextual representationsby maximizing the predictability of visual events based on local and globalcontextual information through both top-down and bottom-up processes. Incontrast to standard predictive coding models, the prediction error in thismodel is used to update the contextual representation but does not alter thefeedforward input for the next layer, and is thus more consistent withneurophysiological observations. We establish the computational feasibility ofthis model by demonstrating its ability in several aspects. We show that ourmodel can outperform state-of-art performances of gated Boltzmann machines(GBM) in estimation of contextual information. Our model can also interpolatemissing events or predict future events in image sequences while simultaneouslyestimating contextual information. We show it achieves state-of-artperformances in terms of prediction accuracy in a variety of tasks andpossesses the ability to interpolate missing frames, a function that is lackingin GBM.
arxiv-1411-4077 | A framework for studying synaptic plasticity with neural spike train data |  http://arxiv.org/abs/1411.4077  | author:Scott W. Linderman, Christopher H. Stock, Ryan P. Adams category:stat.ML q-bio.NC published:2014-11-14 summary:Learning and memory in the brain are implemented by complex, time-varyingchanges in neural circuitry. The computational rules according to whichsynaptic weights change over time are the subject of much research, and are notprecisely understood. Until recently, limitations in experimental methods havemade it challenging to test hypotheses about synaptic plasticity on a largescale. However, as such data become available and these barriers are lifted, itbecomes necessary to develop analysis techniques to validate plasticity models.Here, we present a highly extensible framework for modeling arbitrary synapticplasticity rules on spike train data in populations of interconnected neurons.We treat synaptic weights as a (potentially nonlinear) dynamical systemembedded in a fully-Bayesian generalized linear model (GLM). In addition, weprovide an algorithm for inferring synaptic weight trajectories alongside theparameters of the GLM and of the learning rules. Using this method, we performmodel comparison of two proposed variants of the well-knownspike-timing-dependent plasticity (STDP) rule, where nonlinear effects play asubstantial role. On synthetic data generated from the biophysical simulatorNEURON, we show that we can recover the weight trajectories, the pattern ofconnectivity, and the underlying learning rules.
arxiv-1411-3827 | Autonomization of Monoidal Categories |  http://arxiv.org/abs/1411.3827  | author:Antonin Delpeuch category:math.CT cs.CL 18D10 published:2014-11-14 summary:We define the free autonomous category generated by a monoidal category andstudy some of its properties. From a linguistic perspective, this expands therange of possible models of meaning within the distributional compositionalframework, by allowing nonlinearities in maps. From a categorical point ofview, this provides a factorization of the construction in [Preller and Lambek,2007] of the free autonomous category generated by a category.
arxiv-1411-4080 | 6 Seconds of Sound and Vision: Creativity in Micro-Videos |  http://arxiv.org/abs/1411.4080  | author:Miriam Redi, Neil O Hare, Rossano Schifanella, Michele Trevisiol, Alejandro Jaimes category:cs.MM cs.CV cs.HC published:2014-11-14 summary:The notion of creativity, as opposed to related concepts such as beauty orinterestingness, has not been studied from the perspective of automaticanalysis of multimedia content. Meanwhile, short online videos shared on socialmedia platforms, or micro-videos, have arisen as a new medium for creativeexpression. In this paper we study creative micro-videos in an effort tounderstand the features that make a video creative, and to address the problemof automatic detection of creative content. Defining creative videos as thosethat are novel and have aesthetic value, we conduct a crowdsourcing experimentto create a dataset of over 3,800 micro-videos labelled as creative andnon-creative. We propose a set of computational features that we map to thecomponents of our definition of creativity, and conduct an analysis todetermine which of these features correlate most with creative video. Finally,we evaluate a supervised approach to automatically detect creative video, withpromising results, showing that it is necessary to model both aesthetic valueand novelty to achieve optimal classification accuracy.
arxiv-1411-3803 | Stochastic Compositional Gradient Descent: Algorithms for Minimizing Compositions of Expected-Value Functions |  http://arxiv.org/abs/1411.3803  | author:Mengdi Wang, Ethan X. Fang, Han Liu category:stat.ML published:2014-11-14 summary:Classical stochastic gradient methods are well suited for minimizingexpected-value objective functions. However, they do not apply to theminimization of a nonlinear function involving expected values or a compositionof two expected-value functions, i.e., problems of the form $\min_x\mathbf{E}_v [f_v\big(\mathbf{E}_w [g_w(x)]\big)]$. In order to solve thisstochastic composition problem, we propose a class of stochastic compositionalgradient descent (SCGD) algorithms that can be viewed as stochastic versions ofquasi-gradient method. SCGD update the solutions based on noisy samplegradients of $f_v,g_{w}$ and use an auxiliary variable to track the unknownquantity $\mathbf{E}_w[g_w(x)]$. We prove that the SCGD converge almost surelyto an optimal solution for convex optimization problems, as long as such asolution exists. The convergence involves the interplay of two iterations withdifferent time scales. For nonsmooth convex problems, the SCGD achieve aconvergence rate of $O(k^{-1/4})$ in the general case and $O(k^{-2/3})$ in thestrongly convex case, after taking $k$ samples. For smooth convex problems, theSCGD can be accelerated to converge at a rate of $O(k^{-2/7})$ in the generalcase and $O(k^{-4/5})$ in the strongly convex case. For nonconvex problems, weprove that any limit point generated by SCGD is a stationary point, for whichwe also provide the convergence rate analysis. Indeed, the stochastic settingwhere one wants to optimize compositions of expected-value functions is verycommon in practice. The proposed SCGD methods find wide applications inlearning, estimation, dynamic programming, etc.
arxiv-1411-3787 | Asymmetric Minwise Hashing |  http://arxiv.org/abs/1411.3787  | author:Anshumali Shrivastava, Ping Li category:stat.ML cs.DB cs.DS cs.IR cs.LG published:2014-11-14 summary:Minwise hashing (Minhash) is a widely popular indexing scheme in practice.Minhash is designed for estimating set resemblance and is known to besuboptimal in many applications where the desired measure is set overlap (i.e.,inner product between binary vectors) or set containment. Minhash has inherentbias towards smaller sets, which adversely affects its performance inapplications where such a penalization is not desirable. In this paper, wepropose asymmetric minwise hashing (MH-ALSH), to provide a solution to thisproblem. The new scheme utilizes asymmetric transformations to cancel the biasof traditional minhash towards smaller sets, making the final "collisionprobability" monotonic in the inner product. Our theoretical comparisons showthat for the task of retrieving with binary inner products asymmetric minhashis provably better than traditional minhash and other recently proposed hashingalgorithms for general inner products. Thus, we obtain an algorithmicimprovement over existing approaches in the literature. Experimentalevaluations on four publicly available high-dimensional datasets validate ourclaims and the proposed scheme outperforms, often significantly, other hashingalgorithms on the task of near neighbor retrieval with set containment. Ourproposal is simple and easy to implement in practice.
arxiv-1411-3825 | Statistical Models for Degree Distributions of Networks |  http://arxiv.org/abs/1411.3825  | author:Kayvan Sadeghi, Alessandro Rinaldo category:math.ST stat.ML stat.TH published:2014-11-14 summary:We define and study the statistical models in exponential family form whosesufficient statistics are the degree distributions and the bi-degreedistributions of undirected labelled simple graphs. Graphs that are constrainedby the joint degree distributions are called $dK$-graphs in the computerscience literature and this paper attempts to provide the first statisticallygrounded analysis of this type of models. In addition to formalizing thesemodels, we provide some preliminary results for the parameter estimation andthe asymptotic behaviour of the model for degree distribution, and discuss theparameter estimation for the model for bi-degree distribution.
arxiv-1411-4038 | Fully Convolutional Networks for Semantic Segmentation |  http://arxiv.org/abs/1411.4038  | author:Jonathan Long, Evan Shelhamer, Trevor Darrell category:cs.CV published:2014-11-14 summary:Convolutional networks are powerful visual models that yield hierarchies offeatures. We show that convolutional networks by themselves, trainedend-to-end, pixels-to-pixels, exceed the state-of-the-art in semanticsegmentation. Our key insight is to build "fully convolutional" networks thattake input of arbitrary size and produce correspondingly-sized output withefficient inference and learning. We define and detail the space of fullyconvolutional networks, explain their application to spatially dense predictiontasks, and draw connections to prior models. We adapt contemporaryclassification networks (AlexNet, the VGG net, and GoogLeNet) into fullyconvolutional networks and transfer their learned representations byfine-tuning to the segmentation task. We then define a novel architecture thatcombines semantic information from a deep, coarse layer with appearanceinformation from a shallow, fine layer to produce accurate and detailedsegmentations. Our fully convolutional network achieves state-of-the-artsegmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012),NYUDv2, and SIFT Flow, while inference takes one third of a second for atypical image.
arxiv-1411-3895 | Learning Fuzzy Controllers in Mobile Robotics with Embedded Preprocessing |  http://arxiv.org/abs/1411.3895  | author:I. Rodríguez-Fdez, M. Mucientes, A. Bugarín category:cs.RO cs.AI cs.LG published:2014-11-14 summary:The automatic design of controllers for mobile robots usually requires twostages. In the first stage,sensorial data are preprocessed or transformed intohigh level and meaningful values of variables whichare usually defined fromexpert knowledge. In the second stage, a machine learning technique is appliedtoobtain a controller that maps these high level variables to the controlcommands that are actually sent tothe robot. This paper describes an algorithmthat is able to embed the preprocessing stage into the learningstage in orderto get controllers directly starting from sensorial raw data with no expertknowledgeinvolved. Due to the high dimensionality of the sensorial data, thisapproach uses Quantified Fuzzy Rules(QFRs), that are able to transformlow-level input variables into high-level input variables, reducingthedimensionality through summarization. The proposed learning algorithm, calledIterative QuantifiedFuzzy Rule Learning (IQFRL), is based on geneticprogramming. IQFRL is able to learn rules with differentstructures, and canmanage linguistic variables with multiple granularities. The algorithm has beentestedwith the implementation of the wall-following behavior both in severalrealistic simulated environmentswith different complexity and on a Pioneer 3-ATrobot in two real environments. Results have beencompared with severalwell-known learning algorithms combined with different datapreprocessingtechniques, showing that IQFRL exhibits a better and statisticallysignificant performance. Moreover,three real world applications for which IQFRLplays a central role are also presented: path and objecttracking with staticand moving obstacles avoidance.
arxiv-1411-3715 | Acoustic Scene Classification |  http://arxiv.org/abs/1411.3715  | author:Daniele Barchiesi, Dimitrios Giannoulis, Dan Stowell, Mark D. Plumbley category:cs.SD cs.LG published:2014-11-13 summary:In this article we present an account of the state-of-the-art in acousticscene classification (ASC), the task of classifying environments from thesounds they produce. Starting from a historical review of previous research inthis area, we define a general framework for ASC and present different imple-mentations of its components. We then describe a range of different algorithmssubmitted for a data challenge that was held to provide a general and fairbenchmark for ASC techniques. The dataset recorded for this purpose ispresented, along with the performance metrics that are used to evaluate thealgorithms and statistical significance tests to compare the submitted methods.We use a baseline method that employs MFCCS, GMMS and a maximum likelihoodcriterion as a benchmark, and only find sufficient evidence to conclude thatthree algorithms significantly outperform it. We also evaluate the humanclassification accuracy in performing a similar classification task. The bestperforming algorithm achieves a mean accuracy that matches the median accuracyobtained by humans, and common pairs of classes are misclassified by bothcomputers and humans. However, all acoustic scenes are correctly classified byat least some individuals, while there are scenes that are misclassified by allalgorithms.
arxiv-1411-3561 | A Text to Speech (TTS) System with English to Punjabi Conversion |  http://arxiv.org/abs/1411.3561  | author:Prabhsimran Singh, Amritpal Singh category:cs.CL published:2014-11-13 summary:The paper aims to show how an application can be developed that converts theEnglish language into the Punjabi Language, and the same application canconvert the Text to Speech(TTS) i.e. pronounce the text. This application canbe really beneficial for those with special needs.
arxiv-1411-3553 | Greedy metrics in orthogonal greedy learning |  http://arxiv.org/abs/1411.3553  | author:Lin Xu, Shaobo Lin, Jinshan Zeng, Zongben Xu category:cs.LG F.2.2 published:2014-11-13 summary:Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds anew atom from a dictionary via the steepest gradient descent and build theestimator via orthogonal projecting the target function to the space spanned bythe selected atoms in each greedy step. Here, "greed" means choosing a new atomaccording to the steepest gradient descent principle. OGL then avoids theoverfitting/underfitting by selecting an appropriate iteration number. In thispaper, we point out that the overfitting/underfitting can also be avoided viaredefining "greed" in OGL. To this end, we introduce a new greedy metric,called $\delta$-greedy thresholds, to refine "greed" and theoretically verifiesits feasibility. Furthermore, we reveals that such a greedy metric can bring anadaptive termination rule on the premise of maintaining the prominent learningperformance of OGL. Our results show that the steepest gradient descent is notthe unique greedy metric of OGL and some other more suitable metric may lessenthe hassle of model-selection of OGL.
arxiv-1411-3519 | Window-Based Descriptors for Arabic Handwritten Alphabet Recognition: A Comparative Study on a Novel Dataset |  http://arxiv.org/abs/1411.3519  | author:Marwan Torki, Mohamed E. Hussein, Ahmed Elsallamy, Mahmoud Fayyaz, Shehab Yaser category:cs.CV I.5.2; I.7.5 published:2014-11-13 summary:This paper presents a comparative study for window-based descriptors on theapplication of Arabic handwritten alphabet recognition. We show a detailedexperimental evaluation of different descriptors with several classifiers. Theobjective of the paper is to evaluate different window-based descriptors on theproblem of Arabic letter recognition. Our experiments clearly show that theyperform very well. Moreover, we introduce a novel spatial pyramid partitioningscheme that enhances the recognition accuracy for most descriptors. Inaddition, we introduce a novel dataset for Arabic handwritten isolated alphabetletters, which can serve as a benchmark for future research.
arxiv-1411-3525 | Gaze Stabilization for Humanoid Robots: a Comprehensive Framework |  http://arxiv.org/abs/1411.3525  | author:Alessandro Roncone, Ugo Pattacini, Giorgio Metta, Lorenzo Natale category:cs.RO cs.CV published:2014-11-13 summary:Gaze stabilization is an important requisite for humanoid robots. Previouswork on this topic has focused on the integration of inertial and visualinformation. Little attention has been given to a third component, which is theknowledge that the robot has about its own movement. In this work we propose acomprehensive framework for gaze stabilization in a humanoid robot. We focus onthe problem of compensating for disturbances induced in the cameras due toself-generated movements of the robot. In this work we employ two separatesignals for stabilization: (1) an anticipatory term obtained from the velocitycommands sent to the joints while the robot moves autonomously; (2) a feedbackterm from the on board gyroscope, which compensates unpredicted externaldisturbances. We first provide the mathematical formulation to derive theforward and the differential kinematics of the fixation point of the stereosystem. We finally test our method on the iCub robot. We show that thestabilization consistently reduces the residual optical flow during themovement of the robot and in presence of external disturbances. We alsodemonstrate that proper integration of the neck DoF is crucial to achievecorrect stabilization.
arxiv-1411-3436 | SelfieBoost: A Boosting Algorithm for Deep Learning |  http://arxiv.org/abs/1411.3436  | author:Shai Shalev-Shwartz category:stat.ML cs.LG published:2014-11-13 summary:We describe and analyze a new boosting algorithm for deep learning calledSelfieBoost. Unlike other boosting algorithms, like AdaBoost, which constructensembles of classifiers, SelfieBoost boosts the accuracy of a single network.We prove a $\log(1/\epsilon)$ convergence rate for SelfieBoost under some "SGDsuccess" assumption which seems to hold in practice.
arxiv-1411-3423 | A Comparative Study of Techniques of Distant Reconstruction of Displacement Fields by using DISTRESS Simulator |  http://arxiv.org/abs/1411.3423  | author:Ghulam Mubashar Hassan, Arcady V. Dyskin, Cara K. MacNish category:cs.CV published:2014-11-13 summary:Reconstruction and monitoring of displacement and strain fields is animportant problem in engineering. We analyze the remote and non-obtrusivemethods of strain measurement based on photogrammetry and Digital ImageCorrelation (DIC). The method is based on covering the photographed surfacewith a pattern of speckles and comparing the images taken before and after thedeformation. In this study, a comprehensive literature review and comparativeanalysis of photogrammetric solutions is presented. The analysis is based on aspecially developed Digital Image Synthesizer To Reconstruct Strain in Solids(DISTRESS) Simulator to generate synthetic images of displacement and stressfields in order to investigate the intrinsic accuracy of the existing variantsof DIC. We investigated the Basic DIC and a commercial software VIC 2D, bothbased on displacement field reconstruction with post processing straindetermination based on numerical differentiation. We also investigated what wecall the Extended DIC where the strain field is determined independently of thedisplacement field. While the Basic DIC and VIC 2D are faster, the Extended DICdelivers the best accuracy of strain reconstruction. The speckle pattern isfound to be playing a critical role in achieving high accuracy for DIC.Increase in subset size for DIC does not significantly improves the accuracy,while the smallest subset size depends on the speckle pattern and speckle size.Increase in the overall image size provides more details but does not playsignificant role in improving the accuracy, while significantly increasing thecomputation cost.
arxiv-1411-3413 | Multi-view Anomaly Detection via Probabilistic Latent Variable Models |  http://arxiv.org/abs/1411.3413  | author:Tomoharu Iwata, Makoto Yamada category:stat.ML cs.LG published:2014-11-13 summary:We propose a nonparametric Bayesian probabilistic latent variable model formulti-view anomaly detection, which is the task of finding instances that haveinconsistent views. With the proposed model, all views of a non-anomalousinstance are assumed to be generated from a single latent vector. On the otherhand, an anomalous instance is assumed to have multiple latent vectors, and itsdifferent views are generated from different latent vectors. By inferring thenumber of latent vectors used for each instance with Dirichlet process priors,we obtain multi-view anomaly scores. The proposed model can be seen as a robustextension of probabilistic canonical correlation analysis for noisy multi-viewdata. We present Bayesian inference procedures for the proposed model based ona stochastic EM algorithm. The effectiveness of the proposed model isdemonstrated in terms of performance when detecting multi-view anomalies andimputing missing values in multi-view data with anomalies.
arxiv-1411-3410 | Person Re-identification Based on Color Histogram and Spatial Configuration of Dominant Color Regions |  http://arxiv.org/abs/1411.3410  | author:Kwangchol Jang, Sokmin Han, Insong Kim category:cs.CV published:2014-11-13 summary:There is a requirement to determine whether a given person of interest hasalready been observed over a network of cameras in video surveillance systems.A human appearance obtained in one camera is usually different from the onesobtained in another camera due to difference in illumination, pose andviewpoint, camera parameters. Being related to appearance-based approaches forperson re-identification, we propose a novel method based on the dominant colorhistogram and spatial configuration of dominant color regions on human bodyparts. Dominant color histogram and spatial configuration of the dominant colorregions based on dominant color descriptor(DCD) can be considered to be robustto illumination and pose, viewpoint changes. The proposed method is evaluatedusing benchmark video datasets. Experimental results using the cumulativematching characteristic(CMC) curve demonstrate the effectiveness of ourapproach for person re-identification.
arxiv-1411-3698 | Minimal Realization Problems for Hidden Markov Models |  http://arxiv.org/abs/1411.3698  | author:Qingqing Huang, Rong Ge, Sham Kakade, Munther Dahleh category:cs.LG published:2014-11-13 summary:Consider a stationary discrete random process with alphabet size d, which isassumed to be the output process of an unknown stationary Hidden Markov Model(HMM). Given the joint probabilities of finite length strings of the process,we are interested in finding a finite state generative model to describe theentire process. In particular, we focus on two classes of models: HMMs andquasi-HMMs, which is a strictly larger class of models containing HMMs. In themain theorem, we show that if the random process is generated by an HMM oforder less or equal than k, and whose transition and observation probabilitymatrix are in general position, namely almost everywhere on the parameterspace, both the minimal quasi-HMM realization and the minimal HMM realizationcan be efficiently computed based on the joint probabilities of all the lengthN strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim tocompare and connect the two lines of literature: realization theory of HMMs,and the recent development in learning latent variable models with tensordecomposition techniques.
arxiv-1411-3409 | A Randomized Algorithm for CCA |  http://arxiv.org/abs/1411.3409  | author:Paul Mineiro, Nikos Karampatziakis category:stat.ML cs.LG published:2014-11-13 summary:We present RandomizedCCA, a randomized algorithm for computing canonicalanalysis, suitable for large datasets stored either out of core or on adistributed file system. Accurate results can be obtained in as few as two datapasses, which is relevant for distributed processing frameworks in whichiteration is expensive (e.g., Hadoop). The strategy also provides an excellentinitializer for standard iterative solutions.
arxiv-1411-3685 | A warped kernel improving robustness in Bayesian optimization via random embeddings |  http://arxiv.org/abs/1411.3685  | author:Mickaël Binois, David Ginsbourger, Olivier Roustant category:math.OC stat.ML published:2014-11-13 summary:This works extends the Random Embedding Bayesian Optimization approach byintegrating a warping of the high dimensional subspace within the covariancekernel. The proposed warping, that relies on elementary geometricconsiderations, allows mitigating the drawbacks of the high extrinsicdimensionality while avoiding the algorithm to evaluate points giving redundantinformation. It also alleviates constraints on bound selection for the embeddeddomain, thus improving the robustness, as illustrated with a test case with 25variables and intrinsic dimension 6.
arxiv-1411-3652 | Jamming Bandits |  http://arxiv.org/abs/1411.3652  | author:SaiDhiraj Amuru, Cem Tekin, Mihaela van der Schaar, R. Michael Buehrer category:cs.IT cs.LG math.IT published:2014-11-13 summary:Can an intelligent jammer learn and adapt to unknown environments in anelectronic warfare-type scenario? In this paper, we answer this question in thepositive, by developing a cognitive jammer that adaptively and optimallydisrupts the communication between a victim transmitter-receiver pair. Weformalize the problem using a novel multi-armed bandit framework where thejammer can choose various physical layer parameters such as the signalingscheme, power level and the on-off/pulsing duration in an attempt to obtainpower efficient jamming strategies. We first present novel online learningalgorithms to maximize the jamming efficacy against static transmitter-receiverpairs and prove that our learning algorithm converges to the optimal (in termsof the error rate inflicted at the victim and the energy used) jammingstrategy. Even more importantly, we prove that the rate of convergence to theoptimal jamming strategy is sub-linear, i.e. the learning is fast in comparisonto existing reinforcement learning algorithms, which is particularly importantin dynamically changing wireless environments. Also, we characterize theperformance of the proposed bandit-based learning algorithm against multiplestatic and adaptive transmitter-receiver pairs.
arxiv-1411-3650 | DUM: Diversity-Weighted Utility Maximization for Recommendations |  http://arxiv.org/abs/1411.3650  | author:Azin Ashkan, Branislav Kveton, Shlomo Berkovsky, Zheng Wen category:cs.IR stat.ML published:2014-11-13 summary:The need for diversification of recommendation lists manifests in a number ofrecommender systems use cases. However, an increase in diversity may underminethe utility of the recommendations, as relevant items in the list may bereplaced by more diverse ones. In this work we propose a novel method formaximizing the utility of the recommended items subject to the diversity ofuser's tastes, and show that an optimal solution to this problem can be foundgreedily. We evaluate the proposed method in two online user studies as well asin an offline analysis incorporating a number of evaluation metrics. Theresults of evaluations show the superiority of our method over a number ofbaselines.
arxiv-1411-3251 | Identification of Helicopter Dynamics based on Flight Data using Nature Inspired Techniques |  http://arxiv.org/abs/1411.3251  | author:S. N. Omkar, Dheevatsa Mudigere, J Senthilnath, M. Vijaya Kumar category:cs.CE cs.NE published:2014-11-12 summary:The complexity of helicopter flight dynamics makes modeling and helicoptersystem identification a very difficult task. Most of the traditional techniquesrequire a model structure to be defined apriori and in case of helicopterdynamics, this is difficult due to its complexity and the interplay betweenvarious subsystems.To overcome this difficulty, non-parametric approaches arecommonly adopted for helicopter system identification. Artificial NeuralNetwork are a widely used class of algorithms for non-parametric systemidentification, among them, the Nonlinear Auto Regressive eXogeneous inputnetwork (NARX) model is very popular, but it also necessitates some in depthknowledge regarding the system being modeled. There have been many approachesproposed to circumvent this and yet still retain the advantageouscharacteristics. In this paper we carry out an extensive study of one suchnewly proposed approach using a modified NARX model with a two tiered,externally driven recurrent neural network architecture. This is coupled withan outer optimization routine for evolving the order of the system. Thisgeneric architecture is comprehensively explored to ascertain its usability andcritically asses its potential. Different instantiations of this architecture,based on nature inspired computational techniques (Artificial Bee Colony,Artificial Immune System and Particle Swarm Optimization) are evaluated andcritically compared in this paper. Simulations have been carried out foridentifying the longitudinally uncoupled dynamics. Results of identificationindicate a quite close correlation between the actual and the predictedresponse of the helicopter for all the models.
arxiv-1411-3315 | Statistically Significant Detection of Linguistic Change |  http://arxiv.org/abs/1411.3315  | author:Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, Steven Skiena category:cs.CL cs.IR cs.LG H.3.3; I.2.6 published:2014-11-12 summary:We propose a new computational approach for tracking and detectingstatistically significant linguistic shifts in the meaning and usage of words.Such linguistic shifts are especially prevalent on the Internet, where therapid exchange of ideas can quickly change a word's meaning. Our meta-analysisapproach constructs property time series of word usage, and then usesstatistically sound change point detection algorithms to identify significantlinguistic shifts. We consider and analyze three approaches of increasing complexity to generatesuch linguistic property time series, the culmination of which usesdistributional characteristics inferred from word co-occurrences. Usingrecently proposed deep neural language models, we first train vectorrepresentations of words for each time period. Second, we warp the vectorspaces into one unified coordinate system. Finally, we construct adistance-based distributional time series for each word to track it'slinguistic displacement over time. We demonstrate that our approach is scalable by tracking linguistic changeacross years of micro-blogging using Twitter, a decade of product reviews usinga corpus of movie reviews from Amazon, and a century of written books using theGoogle Book-ngrams. Our analysis reveals interesting patterns of language usagechange commensurate with each medium.
arxiv-1411-3169 | On Coarse Graining of Information and Its Application to Pattern Recognition |  http://arxiv.org/abs/1411.3169  | author:Ali Ghaderi category:cs.CV stat.ML published:2014-11-12 summary:We propose a method based on finite mixture models for classifying a set ofobservations into number of different categories. In order to demonstrate themethod, we show how the component densities for the mixture model can bederived by using the maximum entropy method in conjunction with conservation ofPythagorean means. Several examples of distributions belonging to thePythagorean family are derived. A discussion on estimation of model parametersand the number of categories is also given.
arxiv-1411-3159 | Part Detector Discovery in Deep Convolutional Neural Networks |  http://arxiv.org/abs/1411.3159  | author:Marcel Simon, Erik Rodner, Joachim Denzler category:cs.CV I.4.8 published:2014-11-12 summary:Current fine-grained classification approaches often rely on a robustlocalization of object parts to extract localized feature representationssuitable for discrimination. However, part localization is a challenging taskdue to the large variation of appearance and pose. In this paper, we show howpre-trained convolutional neural networks can be used for robust and efficientobject part discovery and localization without the necessity to actually trainthe network on the current dataset. Our approach called "part detectordiscovery" (PDD) is based on analyzing the gradient maps of the network outputsand finding activation centers spatially related to annotated semantic parts orbounding boxes. This allows us not just to obtain excellent performance on the CUB200-2011dataset, but in contrast to previous approaches also to perform detection andbird classification jointly without requiring a given bounding box annotationduring testing and ground-truth parts during training. The code is available athttp://www.inf-cv.uni-jena.de/part_discovery andhttps://github.com/cvjena/PartDetectorDisovery.
arxiv-1411-3146 | Distributed Representations for Compositional Semantics |  http://arxiv.org/abs/1411.3146  | author:Karl Moritz Hermann category:cs.CL published:2014-11-12 summary:The mathematical representation of semantics is a key issue for NaturalLanguage Processing (NLP). A lot of research has been devoted to finding waysof representing the semantics of individual words in vector spaces.Distributional approaches --- meaning distributed representations that exploitco-occurrence statistics of large corpora --- have proved popular andsuccessful across a number of tasks. However, natural language usually comes instructures beyond the word level, with meaning arising not only from theindividual words but also the structure they are contained in at the phrasal orsentential level. Modelling the compositional process by which the meaning ofan utterance arises from the meaning of its parts is an equally fundamentaltask of NLP. This dissertation explores methods for learning distributed semanticrepresentations and models for composing these into representations for largerlinguistic units. Our underlying hypothesis is that neural models are asuitable vehicle for learning semantically rich representations and that suchrepresentations in turn are suitable vehicles for solving important tasks innatural language processing. The contribution of this thesis is a thoroughevaluation of our hypothesis, as part of which we introduce several newapproaches to representation learning and compositional semantics, as well asmultiple state-of-the-art models which apply distributed semanticrepresentations to various tasks in NLP.
arxiv-1411-3041 | Collecting Image Description Datasets using Crowdsourcing |  http://arxiv.org/abs/1411.3041  | author:Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh category:cs.CV published:2014-11-12 summary:We describe our two new datasets with images described by humans. Both thedatasets were collected using Amazon Mechanical Turk, a crowdsourcing platform.The two datasets contain significantly more descriptions per image than otherexisting datasets. One is based on a popular image description dataset calledthe UIUC Pascal Sentence Dataset, whereas the other is based on the AbstractScenes dataset con- taining images made from clipart objects. In this paper wedescribe our interfaces, analyze some properties of and show exampledescriptions from our two datasets.
arxiv-1411-3285 | Amoeba Techniques for Shape and Texture Analysis |  http://arxiv.org/abs/1411.3285  | author:Martin Welk category:cs.CV published:2014-11-12 summary:Morphological amoebas are image-adaptive structuring elements formorphological and other local image filters introduced by Lerallut et al. Theirconstruction is based on combining spatial distance with contrast informationinto an image-dependent metric. Amoeba filters show interesting parallels toimage filtering methods based on partial differential equations (PDEs), whichcan be confirmed by asymptotic equivalence results. In computing amoebas, graphstructures are generated that hold information about local image texture. Thispaper reviews and summarises the work of the author and his coauthors onmorphological amoebas, particularly their relations to PDE filters and textureanalysis. It presents some extensions and points out directions for futureinvestigation on the subject.
arxiv-1411-3224 | On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence |  http://arxiv.org/abs/1411.3224  | author:Nathaniel Korda, L. A. Prashanth category:cs.LG math.OC stat.ML published:2014-11-12 summary:We provide non-asymptotic bounds for the well-known temporal differencelearning algorithm TD(0) with linear function approximators. These includehigh-probability bounds as well as bounds in expectation. Our analysis suggeststhat a step-size inversely proportional to the number of iterations cannotguarantee optimal rate of convergence unless we assume (partial) knowledge ofthe stationary distribution for the Markov chain underlying the policyconsidered. We also provide bounds for the iterate averaged TD(0) variant,which gets rid of the step-size dependency while exhibiting the optimal rate ofconvergence. Furthermore, we propose a variant of TD(0) with linearapproximators that incorporates a centering sequence, and establish that itexhibits an exponential rate of convergence in expectation. We demonstrate theusefulness of our bounds on two synthetic experimental settings.
arxiv-1411-3302 | Using Gaussian Measures for Efficient Constraint Based Clustering |  http://arxiv.org/abs/1411.3302  | author:Chandrima Sarkar, Atanu Roy category:cs.LG cs.IR published:2014-11-12 summary:In this paper we present a novel iterative multiphase clustering techniquefor efficiently clustering high dimensional data points. For this purpose weimplement clustering feature (CF) tree on a real data set and a Gaussiandensity distribution constraint on the resultant CF tree. The post processingby the application of Gaussian density distribution function on themicro-clusters leads to refinement of the previously formed clusters thusimproving their quality. This algorithm also succeeds in overcoming theinherent drawbacks of conventional hierarchical methods of clustering likeinability to undo the change made to the dendogram of the data points.Moreover, the constraint measure applied in the algorithm makes this clusteringtechnique suitable for need driven data analysis. We provide veracity of ourclaim by evaluating our algorithm with other similar clustering algorithms.Introduction
arxiv-1411-3277 | Using Ants as a Genetic Crossover Operator in GLS to Solve STSP |  http://arxiv.org/abs/1411.3277  | author:Hassan Ismkhan category:cs.NE published:2014-11-12 summary:Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are twooptimization algorithms that have been successfully applied to the TravelingSalesman Problem (TSP). In this paper we define new crossover operator thenredefine ACAs ants as operate according to defined crossover operator then putforward our GLS that uses these ants to solve Symmetric TSP (STSP) instances.
