arxiv-15600-1 | Model-Coupled Autoencoder for Time Series Visualisation | http://arxiv.org/abs/1601.05654 | author:Nikolaos Gianniotis, Sven D. Kügler, Peter Tiňo, Kai L. Polsterer category:astro-ph.IM cs.NE published:2016-01-21 summary:We present an approach for the visualisation of a set of time series thatcombines an echo state network with an autoencoder. For each time series in thedataset we train an echo state network, using a common and fixed reservoir ofhidden neurons, and use the optimised readout weights as the newrepresentation. Dimensionality reduction is then performed via an autoencoderon the readout weight representations. The crux of the work is to equip theautoencoder with a loss function that correctly interprets the reconstructedreadout weights by associating them with a reconstruction error measured in thedata space of sequences. This essentially amounts to measuring the predictiveperformance that the reconstructed readout weights exhibit on theircorresponding sequences when plugged back into the echo state network with thesame fixed reservoir. We demonstrate that the proposed visualisation frameworkcan deal both with real valued sequences as well as binary sequences. We derivemagnification factors in order to analyse distance preservations anddistortions in the visualisation space. The versatility and advantages of theproposed method are demonstrated on datasets of time series that originate fromdiverse domains.
arxiv-15600-2 | RGB-D-based Action Recognition Datasets: A Survey | http://arxiv.org/abs/1601.05511 | author:Jing Zhang, Wanqing Li, Philip O. Ogunbona, Pichao Wang, Chang Tang category:cs.CV published:2016-01-21 summary:Human action recognition from RGB-D (Red, Green, Blue and Depth) data hasattracted increasing attention since the first work reported in 2010. Over thisperiod, many benchmark datasets have been created to facilitate the developmentand evaluation of new algorithms. This raises the question of which dataset toselect and how to use it in providing a fair and objective comparativeevaluation against state-of-the-art methods. To address this issue, this paperprovides a comprehensive review of the most commonly used action recognitionrelated RGB-D video datasets, including 27 single-view datasets, 10 multi-viewdatasets, and 7 multi-person datasets. The detailed information and analysis ofthese datasets is a useful resource in guiding insightful selection of datasetsfor future research. In addition, the issues with current algorithm evaluationvis-\'{a}-vis limitations of the available datasets and evaluation protocolsare also highlighted; resulting in a number of recommendations for collectionof new datasets and use of evaluation protocols.
arxiv-15600-3 | B-spline Shape from Motion & Shading: An Automatic Free-form Surface Modeling for Face Reconstruction | http://arxiv.org/abs/1601.05644 | author:Weilong Peng, Zhiyong Feng, Chao Xu category:cs.CV cs.GR published:2016-01-21 summary:Recently, many methods have been proposed for face reconstruction frommultiple images, most of which involve fundamental principles of Shape fromShading and Structure from motion. However, a majority of the methods justgenerate discrete surface model of face. In this paper, B-spline Shape fromMotion and Shading (BsSfMS) is proposed to reconstruct continuous B-splinesurface for multi-view face images, according to an assumption that shading andmotion information in the images contain 1st- and 0th-order derivative ofB-spline face respectively. Face surface is expressed as a B-spline surfacethat can be reconstructed by optimizing B-spline control points. Therefore,normals and 3D feature points computed from shading and motion of imagesrespectively are used as the 1st- and 0th- order derivative information, to bejointly applied in optimizing the B-spline face. Additionally, an IMLS(iterative multi-least-square) algorithm is proposed to handle the difficultcontrol point optimization. Furthermore, synthetic samples and LFW dataset areintroduced and conducted to verify the proposed approach, and the experimentalresults demonstrate the effectiveness with different poses, illuminations,expressions etc., even with wild images.
arxiv-15600-4 | On the Diagnostic of Road Pathway Visibility | http://arxiv.org/abs/1601.05535 | author:Pierre Charbonnier, Jean-Philippe Tarel, Francois Goulette category:cs.CV published:2016-01-21 summary:Visibility distance on the road pathway plays a significant role in roadsafety and in particular, has a clear impact on the choice of speed limits.Visibility distance is thus of importance for road engineers and authorities.While visibility distance criteria are routinely taken into account in roaddesign, only a few systems exist for estimating it on existing road networks.Most existing systems comprise a target vehicle followed at a constant distanceby an observer vehicle, which only allows to check if a given, fixed visibilitydistance is available. We propose two new approaches that allow estimating themaximum available visibility distance, involving only one vehicle and based ondifferent sensor technologies, namely binocular stereovision and 3D rangesensing (LIDAR). The first approach is based on the processing of two viewstaken by digital cameras onboard the diagnostic vehicle. The main stages of theprocess are: road segmentation, edge registration between the two views, roadprofile 3D reconstruction and finally, maximal road visibility distanceestimation. The second approach involves the use of a Terrestrial LIDAR MobileMapping System. The triangulated 3D model of the road and its surroundingsprovided by the system is used to simulate targets at different distances,which allows estimating the maximum geometric visibility distance along thepathway. These approaches were developed in the context of the SARI-VIZIRPREDIT project. Both approaches are described, evaluated and compared. Theirpros and cons with respect to vehicle following systems are also discussed.
arxiv-15600-5 | Local community detection by seed expansion: from conductance to weighted kernel 1-mean optimization | http://arxiv.org/abs/1601.05775 | author:Twan van Laarhoven, Elena Marchiori category:cs.SI cs.LG stat.ML published:2016-01-21 summary:In local community detection by seed expansion a single cluster concentratedaround few given query nodes in a graph is discovered in a localized way.Conductance is a popular objective function used in many algorithms for localcommunity detection. Algorithms that directly optimize conductance usually addor remove one node at a time to find a local optimum. This amounts to fix aspecific neighborhood structure over clusters. A natural way to avoid theproblem of choosing a specific neighborhood structure is to use a continuousrelaxation of conductance. This paper studies such a continuous relaxation ofconductance. We show that in this setting continuous optimization leads to hardclusters. We investigate the relation of conductance with weighted kernelk-means for a single cluster, which leads to the introduction of a weightedkernel 1-mean objective function, called \sigma-conductance, where {\sigma} isa parameter which influences the size of the community. Conductance is obtainedby setting {\sigma} to 0. Two algorithms for local optimization of\sigma-conductance based on the expectation maximization and the projectedgradient descend method are developed, called EMc and PGDc, respectively. Weshow that for \sigma=0 EMc corresponds to gradient descend with an infinitestep size at each iteration. We design a procedure to automatically select avalue for {\sigma}. Performance guarantee for these algorithms is proven for aclass of dense communities centered around the seeds and well separated fromthe rest of the network. On this class we also prove that our algorithms staylocalized. A comparative experimental analysis on networks with ground-truthcommunities is performed using state-of-the-art algorithms based on the graphdiffusion method. Our experiments indicate that EMc and PGDc stay localized andproduce communities most similar to the ground.
arxiv-15600-6 | Automatic 3D modelling of craniofacial form | http://arxiv.org/abs/1601.05593 | author:Nick Pears, Christian Duncan category:cs.CV I.4.8 published:2016-01-21 summary:Three-dimensional models of craniofacial variation over the generalpopulation are useful for assessing pre- and post-operative head shape whentreating various craniofacial conditions, such as craniosynostosis. We presenta new method of automatically building both sagittal profile models and full 3Dsurface models of the human head using a range of techniques in 3D surfaceimage analysis; in particular, automatic facial landmarking using supervisedmachine learning, global and local symmetry plane detection using a variant oftrimmed iterative closest points, locally-affine template warping (for full 3Dmodels) and a novel pose normalisation using robust iterative ellipse fitting.The PCA-based models built using the new pose normalisation are more compactthan those using Generalised Procrustes Analysis and we demonstrate theirutility in a clinical case study.
arxiv-15600-7 | Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs | http://arxiv.org/abs/1601.05610 | author:Hui Li, Chunhua Shen category:cs.CV published:2016-01-21 summary:In this work, we tackle the problem of car license plate detection andrecognition in natural scene images. Inspired by the success of deep neuralnetworks (DNNs) in various vision applications, here we leverage DNNs to learnhigh-level features in a cascade framework, which lead to improved performanceon both detection and recognition. Firstly, we train a $37$-class convolutional neural network (CNN) to detectall characters in an image, which results in a high recall, compared withconventional approaches such as training a binary text/non-text classifier.False positives are then eliminated by the second plate/non-plate CNNclassifier. Bounding box refinement is then carried out based on the edgeinformation of the license plates, in order to improve theintersection-over-union (IoU) ratio. The proposed cascade framework extractslicense plates effectively with both high recall and precision. Last, wepropose to recognize the license characters as a {sequence labelling} problem.A recurrent neural network (RNN) with long short-term memory (LSTM) is trainedto recognize the sequential features extracted from the whole license plate viaCNNs. The main advantage of this approach is that it is segmentation free. Byexploring context information and avoiding errors caused by segmentation, theRNN method performs better than a baseline method of combining segmentation anddeep CNN classification; and achieves state-of-the-art recognition accuracy.
arxiv-15600-8 | A Confidence-Based Approach for Balancing Fairness and Accuracy | http://arxiv.org/abs/1601.05764 | author:Benjamin Fish, Jeremy Kun, Ádám D. Lelkes category:cs.LG cs.CY published:2016-01-21 summary:We study three classical machine learning algorithms in the context ofalgorithmic fairness: adaptive boosting, support vector machines, and logisticregression. Our goal is to maintain the high accuracy of these learningalgorithms while reducing the degree to which they discriminate againstindividuals because of their membership in a protected group. Our first contribution is a method for achieving fairness by shifting thedecision boundary for the protected group. The method is based on the theory ofmargins for boosting. Our method performs comparably to or outperforms previousalgorithms in the fairness literature in terms of accuracy and lowdiscrimination, while simultaneously allowing for a fast and transparentquantification of the trade-off between bias and error. Our second contribution addresses the shortcomings of the bias-errortrade-off studied in most of the algorithmic fairness literature. Wedemonstrate that even hopelessly naive modifications of a biased algorithm,which cannot be reasonably said to be fair, can still achieve low bias and highaccuracy. To help to distinguish between these naive algorithms and moresensible algorithms we propose a new measure of fairness, called resilience torandom bias (RRB). We demonstrate that RRB distinguishes well between our naiveand sensible fairness algorithms. RRB together with bias and accuracy providesa more complete picture of the fairness of an algorithm.
arxiv-15600-9 | Spatial Scaling of Satellite Soil Moisture using Temporal Correlations and Ensemble Learning | http://arxiv.org/abs/1601.05767 | author:Subit Chakrabarti, Jasmeet Judge, Tara Bongiovanni, Anand Rangarajan, Sanjay Ranka category:cs.CV published:2016-01-21 summary:A novel algorithm is developed to downscale soil moisture (SM), obtained atsatellite scales of 10-40 km by utilizing its temporal correlations tohistorical auxiliary data at finer scales. Including such correlationsdrastically reduces the size of the training set needed, accounts fortime-lagged relationships, and enables downscaling even in the presence ofshort gaps in the auxiliary data. The algorithm is based upon bagged regressiontrees (BRT) and uses correlations between high-resolution remote sensingproducts and SM observations. The algorithm trains multiple regression treesand automatically chooses the trees that generate the best downscaledestimates. The algorithm was evaluated using a multi-scale synthetic dataset innorth central Florida for two years, including two growing seasons of corn andone growing season of cotton per year. The time-averaged error across theregion was found to be 0.01 $\mathrm{m}^3/\mathrm{m}^3$, with a standarddeviation of 0.012 $\mathrm{m}^3/\mathrm{m}^3$ when 0.02% of the data were usedfor training in addition to temporal correlations from the past seven days, andall available data from the past year. The maximum spatially averaged errorsobtained using this algorithm in downscaled SM were 0.005$\mathrm{m}^3/\mathrm{m}^3$, for pixels with cotton land-cover. When landsurface temperature~(LST) on the day of downscaling was not included in thealgorithm to simulate "data gaps", the spatially averaged error increasedminimally by 0.015 $\mathrm{m}^3/\mathrm{m}^3$ when LST is unavailable on theday of downscaling. The results indicate that the BRT-based algorithm provideshigh accuracy for downscaling SM using complex non-linear spatio-temporalcorrelations, under heterogeneous micro meteorological conditions.
arxiv-15600-10 | Generalized optimal sub-pattern assignment metric | http://arxiv.org/abs/1601.05585 | author:Abu Sajana Rahmathullah, Ángel F. García-Fernández, Lennart Svensson category:cs.SY cs.CV published:2016-01-21 summary:In this paper, we present the generalized optimal sub-pattern assignment(GOSPA) metric on the space of sets of targets. This metric is a generalizedversion of the unnormalized optimal sub-pattern assignment (OSPA) metric. Thedifference between unnormalized OSPA and GOSPA is that, in the proposed metric,we can choose a range of values for the cardinality mismatch penalty for agiven cut-off distance c. We argue that in multiple target tracking, we shouldselect the cardinality mismatch of GOSPA in a specific way, which is differentfrom OSPA. In this case, the metric can be viewed as sum of target localizationerror and error due to missed and false targets. We also extend the GOSPAmetric to the space of random finite sets, and show that both mean GOSPA androot mean squared GOSPA are metrics, which are useful for performanceevaluation.
arxiv-15600-11 | Partial Sum Minimization of Singular Values Representation on Grassmann Manifolds | http://arxiv.org/abs/1601.05613 | author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV published:2016-01-21 summary:As a significant subspace clustering method, low rank representation (LRR)has attracted great attention in recent years. To further improve theperformance of LRR and extend its applications, there are several issues to beresolved. The nuclear norm in LRR does not sufficiently use the prior knowledgeof the rank which is known in many practical problems. The LRR is designed forvectorial data from linear spaces, thus not suitable for high dimensional datawith intrinsic non-linear manifold structure. This paper proposes an extendedLRR model for manifold-valued Grassmann data which incorporates prior knowledgeby minimizing partial sum of singular values instead of the nuclear norm,namely Partial Sum minimization of Singular Values Representation (GPSSVR). Thenew model not only enforces the global structure of data in low rank, but alsoretains important information by minimizing only smaller singular values. Tofurther maintain the local structures among Grassmann points, we also integratethe Laplacian penalty with GPSSVR. An effective algorithm is proposed to solvethe optimization problem based on the GPSSVR model. The proposed model andalgorithms are assessed on some widely used human action video datasets and areal scenery dataset. The experimental results show that the proposed methodsobviously outperform other state-of-the-art methods.
arxiv-15600-12 | On Structured Sparsity of Phonological Posteriors for Linguistic Parsing | http://arxiv.org/abs/1601.05647 | author:Milos Cernak, Afsaneh Asaei, Hervé Bourlard category:cs.CL published:2016-01-21 summary:The speech signal conveys information on different time scales from shorttime scale or segmental, associated to phonological and phonetic information tolong time scale or supra segmental, associated to syllabic and prosodicinformation. Linguistic and neurocognitive studies recognize the phonologicalclasses at segmental level as the essential and invariant representations usedin speech temporal organization. In the context of speech processing, a deepneural network (DNN) is an effective computational method to infer theprobability of individual phonological classes from a short segment of speechsignal. A vector of all phonological class probabilities is referred to asphonological posterior. There are only very few classes comprising a short termspeech signal; hence, the phonological posterior is a sparse vector. Althoughthe phonological posteriors are estimated at segmental level, we claim thatthey convey supra-segmental information. Specifically, we demonstrate thatphonological posteriors are indicative of syllabic and prosodic events.Building on findings from converging linguistic evidence on the gestural modelof Articulatory Phonology as well as the neural basis of speech perception, wehypothesize that phonological posteriors convey properties of linguisticclasses at multiple time scales, and this information is embedded in theirsupport (index) of active coefficients. To verify this hypothesis, we obtain abinary representation of phonological posteriors at the segmental level whichis referred to as first-order sparsity structure; the high-order structures areobtained by the concatenation of first-order binary vectors. It is thenconfirmed that the classification of supra-segmental linguistic events, theproblem known as linguistic parsing, can be achieved with high accuracy usingasimple binary pattern matching of first-order or high-order structures.
arxiv-15600-13 | Habits vs Environment: What really causes asthma? | http://arxiv.org/abs/1601.05141 | author:Mengfan Tang, Pranav Agrawal, Ramesh Jain category:cs.CY cs.LG H.4; D.2.8 published:2016-01-20 summary:Despite considerable number of studies on risk factors for asthma onset, verylittle is known about their relative importance. To have a full picture ofthese factors, both categories, personal and environmental data, have to betaken into account simultaneously, which is missing in previous studies. Wepropose a framework to rank the risk factors from heterogeneous data sources ofthe two categories. Established on top of EventShop and Personal EventShop,this framework extracts about 400 features, and analyzes them by employing agradient boosting tree. The features come from sources including personalprofile and life-event data, and environmental data on air pollution, weatherand PM2.5 emission sources. The top ranked risk factors derived from ourframework agree well with the general medical consensus. Thus, our framework isa reliable approach, and the discovered rankings of relative importance of riskfactors can provide insights for the prevention of asthma.
arxiv-15600-14 | Improved Spoken Document Summarization with Coverage Modeling Techniques | http://arxiv.org/abs/1601.05194 | author:Kuan-Yu Chen, Shih-Hung Liu, Berlin Chen, Hsin-Min Wang category:cs.CL cs.IR published:2016-01-20 summary:Extractive summarization aims at selecting a set of indicative sentences froma source document as a summary that can express the major theme of thedocument. A general consensus on extractive summarization is that bothrelevance and coverage are critical issues to address. The existing methodsdesigned to model coverage can be characterized by either reducing redundancyor increasing diversity in the summary. Maximal margin relevance (MMR) is awidely-cited method since it takes both relevance and redundancy into accountwhen generating a summary for a given document. In addition to MMR, there isonly a dearth of research concentrating on reducing redundancy or increasingdiversity for the spoken document summarization task, as far as we are aware.Motivated by these observations, two major contributions are presented in thispaper. First, in contrast to MMR, which considers coverage by reducingredundancy, we propose two novel coverage-based methods, which directlyincrease diversity. With the proposed methods, a set of representativesentences, which not only are relevant to the given document but also covermost of the important sub-themes of the document, can be selectedautomatically. Second, we make a step forward to plug in severaldocument/sentence representation methods into the proposed framework to furtherenhance the summarization performance. A series of empirical evaluationsdemonstrate the effectiveness of our proposed methods.
arxiv-15600-15 | Factors in Finetuning Deep Model for object detection | http://arxiv.org/abs/1601.05150 | author:Wanli Ouyang, Xiaogang Wang, Cong Zhang, Xiaokang Yang category:cs.CV published:2016-01-20 summary:Finetuning from a pretrained deep model is found to yield state-of-the-artperformance for many vision tasks. This paper investigates many factors thatinfluence the performance in finetuning for object detection. There is along-tailed distribution of sample numbers for classes in object detection. Ouranalysis and empirical results show that classes with more samples have higherimpact on the feature learning. And it is better to make the sample number moreuniform across classes. Generic object detection can be considered as multipleequally important tasks. Detection of each class is a task. These classes/taskshave their individuality in discriminative visual appearance representation.Taking this individuality into account, we cluster objects into visuallysimilar class groups and learn deep representations for these groupsseparately. A hierarchical feature learning scheme is proposed. In this scheme,the knowledge from the group with large number of classes is transferred forlearning features in its sub-groups. Finetuned on the GoogLeNet model,experimental results show 4.7% absolute mAP improvement of our approach on theImageNet object detection dataset without increasing much computational cost atthe testing stage.
arxiv-15600-16 | Nonlinear variable selection with continuous outcome: a nonparametric incremental forward stagewise approach | http://arxiv.org/abs/1601.05285 | author:Tianwei Yu category:stat.ML published:2016-01-20 summary:We present a method of variable selection for the situation where somepredictors are nonlinearly associated with a continuous outcome variable. Themethod doesn't assume any specific functional form, and can select from a largenumber of candidates. It takes the form of incremental forward stagewiseregression, in which very small steps are taken to select the variables. Givenno functional form is assumed, we devised an approach termed roughening toadjust the residuals in the iterations. In simulations, we show the new methodis competitive against popular machine learning approaches. We also demonstrateits performance using some real datasets.
arxiv-15600-17 | Hierarchical Latent Word Clustering | http://arxiv.org/abs/1601.05472 | author:Halid Ziya Yerebakan, Fitsum Reda, Yiqiang Zhan, Yoshihisa Shinagawa category:cs.CL published:2016-01-20 summary:This paper presents a new Bayesian non-parametric model by extending theusage of Hierarchical Dirichlet Allocation to extract tree structured wordclusters from text data. The inference algorithm of the model collects words ina cluster if they share similar distribution over documents. In ourexperiments, we observed meaningful hierarchical structures on NIPS corpus andradiology reports collected from public repositories.
arxiv-15600-18 | Detecting Temporally Consistent Objects in Videos through Object Class Label Propagation | http://arxiv.org/abs/1601.05447 | author:Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen category:cs.CV published:2016-01-20 summary:Object proposals for detecting moving or static video objects need to addressissues such as speed, memory complexity and temporal consistency. We propose anefficient Video Object Proposal (VOP) generation method and show its efficacyin learning a better video object detector. A deep-learning based video objectdetector learned using the proposed VOP achieves state-of-the-art detectionperformance on the Youtube-Objects dataset. We further propose a clustering ofVOPs which can efficiently be used for detecting objects in video in astreaming fashion. As opposed to applying per-frame convolutional neuralnetwork (CNN) based object detection, our proposed method called Objects inVideo Enabler thRough LAbel Propagation (OVERLAP) needs to classify only asmall fraction of all candidate proposals in every video frame throughstreaming clustering of object proposals and class-label propagation. Sourcecode will be made available soon.
arxiv-15600-19 | Deep Perceptual Mapping for Cross-Modal Face Recognition | http://arxiv.org/abs/1601.05347 | author:M. Saquib Sarfraz, Rainer Stiefelhagen category:cs.CV published:2016-01-20 summary:Cross modal face matching between the thermal and visible spectrum is a muchdesired capability for night-time surveillance and security applications. Dueto a very large modality gap, thermal-to-visible face recognition is one of themost challenging face matching problem. In this paper, we present an approachto bridge this modality gap by a significant margin. Our approach captures thehighly non-linear relationship between the two modalities by using a deepneural network. Our model attempts to learn a non-linear mapping from visibleto thermal spectrum while preserving the identity information. We showsubstantive performance improvement on three difficult thermal-visible facedatasets. The presented approach improves the state-of-the-art by more than10\% on UND-X1 dataset and by more than 15-30\% on NVESD dataset in terms ofRank-1 identification. Our method bridges the drop in performance due to themodality gap by more than 40\%.
arxiv-15600-20 | QUOTE: "Querying" Users as Oracles in Tag Engines - A Semi-Supervised Learning Approach to Personalized Image Tagging | http://arxiv.org/abs/1601.06440 | author:Amandianeze O. Nwana, Tsuhan Chen category:cs.IR cs.LG cs.MM cs.SI published:2016-01-20 summary:One common trend in image tagging research is to focus on visually relevanttags, and this tends to ignore the personal and social aspect of tags,especially on photoblogging websites such as Flickr. Previous work hascorrectly identified that many of the tags that users provide on images are notvisually relevant (i.e. representative of the salient content in the image) andthey go on to treat such tags as noise, ignoring that the users chose toprovide those tags over others that could have been more visually relevant.Another common assumption about user generated tags for images is that theorder of these tags provides no useful information for the prediction of tagson future images. This assumption also tends to define usefulness in terms ofwhat is visually relevant to the image. For general tagging or labelingapplications that focus on providing visual information about image content,these assumptions are reasonable, but when considering personalized imagetagging applications, these assumptions are at best too rigid, ignoring userchoice and preferences. We challenge the aforementioned assumptions, and provide a machine learningapproach to the problem of personalized image tagging with the followingcontributions: 1.) We reformulate the personalized image tagging problem as asearch/retrieval ranking problem, 2.) We leverage the order of tags, which doesnot always reflect visual relevance, provided by the user in the past as a cueto their tag preferences, similar to click data, 3.) We propose a technique toaugment sparse user tag data (semi-supervision), and 4.) We demonstrate theefficacy of our method on a subset of Flickr images, showing improvement overprevious state-of-art methods.
arxiv-15600-21 | Disaggregation of SMAP L3 Brightness Temperatures to 9km using Kernel Machines | http://arxiv.org/abs/1601.05350 | author:Subit Chakrabarti, Tara Bongiovanni, Jasmeet Judge, Anand Rangarajan, Sanjay Ranka category:cs.CV published:2016-01-20 summary:In this study, a machine learning algorithm is used for disaggregation ofSMAP brightness temperatures (T$_{\textrm{B}}$) from 36km to 9km. It uses imagesegmentation to cluster the study region based on meteorological and land coversimilarity, followed by a support vector machine based regression that computesthe value of the disaggregated T$_{\textrm{B}}$ at all pixels. High resolutionremote sensing products such as land surface temperature, normalized differencevegetation index, enhanced vegetation index, precipitation, soil texture, andland-cover were used for disaggregation. The algorithm was implemented in Iowa,United States, from April to July 2015, and compared with the SMAP L3_SM_APT$_{\textrm{B}}$ product at 9km. It was found that the disaggregatedT$_{\textrm{B}}$ were very similar to the SMAP-T$_{\textrm{B}}$ product, evenfor vegetated areas with a mean difference $\leq$ 5K. However, the standarddeviation of the disaggregation was lower by 7K than that of the AP product.The probability density functions of the disaggregated T$_{\textrm{B}}$ weresimilar to the SMAP-T$_{\textrm{B}}$. The results indicate that this algorithmmay be used for disaggregating T$_{\textrm{B}}$ using complex non-linearcorrelations on a grid.
arxiv-15600-22 | Selecting Efficient Features via a Hyper-Heuristic Approach | http://arxiv.org/abs/1601.05409 | author:Mitra Montazeri, Mahdieh Soleymani Baghshah, Aliakbar Niknafs category:cs.CV cs.NE published:2016-01-20 summary:By Emerging huge databases and the need to efficient learning algorithms onthese datasets, new problems have appeared and some methods have been proposedto solve these problems by selecting efficient features. Feature selection is aproblem of finding efficient features among all features in which the finalfeature set can improve accuracy and reduce complexity. One way to solve thisproblem is to evaluate all possible feature subsets. However, evaluating allpossible feature subsets is an exhaustive search and thus it has highcomputational complexity. Until now many heuristic algorithms have been studiedfor solving this problem. Hyper-heuristic is a new heuristic approach which cansearch the solution space effectively by applying local searches appropriately.Each local search is a neighborhood searching algorithm. Since each region ofthe solution space can have its own characteristics, it should be chosen anappropriate local search and apply it to current solution. This task is tackledto a supervisor. The supervisor chooses a local search based on the functionalhistory of local searches. By doing this task, it can trade of betweenexploitation and exploration. Since the existing heuristic cannot trade ofbetween exploration and exploitation appropriately, the solution space has notbeen searched appropriately in these methods and thus they have low convergencerate. For the first time, in this paper use a hyper-heuristic approach to findan efficient feature subset. In the proposed method, genetic algorithm is usedas a supervisor and 16 heuristic algorithms are used as local searches.Empirical study of the proposed method on several commonly used data sets fromUCI data sets indicates that it outperforms recent existing methods in theliterature for feature selection.
arxiv-15600-23 | Semantic Word Clusters Using Signed Normalized Graph Cuts | http://arxiv.org/abs/1601.05403 | author:João Sedoc, Jean Gallier, Lyle Ungar, Dean Foster category:cs.CL cs.AI published:2016-01-20 summary:Vector space representations of words capture many aspects of wordsimilarity, but such methods tend to make vector spaces in which antonyms (aswell as synonyms) are close to each other. We present a new signed spectralnormalized graph cut algorithm, signed clustering, that overlays existingthesauri upon distributionally derived vector representations of words, so thatantonym relationships between word pairs are represented by negative weights.Our signed clustering algorithm produces clusters of words which simultaneouslycapture distributional and synonym relations. We evaluate these clustersagainst the SimLex-999 dataset (Hill et al.,2014) of human judgments of wordpair similarities, and also show the benefit of using our clusters to predictthe sentiment of a given text.
arxiv-15600-24 | Sparsity in Dynamics of Spontaneous Subtle Emotions: Analysis \& Application | http://arxiv.org/abs/1601.04805 | author:Anh Cat Le Ngo, John See, Raphael Chung-Wei Phan category:cs.CV published:2016-01-19 summary:Spontaneous subtle emotions are expressed through micro-expressions, whichare tiny, sudden and short-lived dynamics of facial muscles; thus poses a greatchallenge for visual recognition. The abrupt but significant dynamics for therecognition task are temporally sparse while the rest, irrelevant dynamics, aretemporally redundant. In this work, we analyze and enforce sparsity constrainsto learn significant temporal and spectral structures while eliminateirrelevant facial dynamics of micro-expressions, which would ease the challengein the visual recognition of spontaneous subtle emotions. The hypothesis isconfirmed through experimental results of automatic spontaneous subtle emotionrecognition with several sparsity levels on CASME II and SMIC, the only twopublicly available spontaneous subtle emotion databases. The overallperformances of the automatic subtle emotion recognition are boosted when onlysignificant dynamics are preserved from the original sequences.
arxiv-15600-25 | Scalability in Neural Control of Musculoskeletal Robots | http://arxiv.org/abs/1601.04862 | author:Christoph Richter, Sören Jentzsch, Rafael Hostettler, Jesús A. Garrido, Eduardo Ros, Alois C. Knoll, Florian Röhrbein, Patrick van der Smagt, Jörg Conradt category:cs.RO cs.DC cs.NE cs.SY published:2016-01-19 summary:Anthropomimetic robots are robots that sense, behave, interact and feel likehumans. By this definition, anthropomimetic robots require human-like physicalhardware and actuation, but also brain-like control and sensing. The mostself-evident realization to meet those requirements would be a human-likemusculoskeletal robot with a brain-like neural controller. While bothmusculoskeletal robotic hardware and neural control software have existed fordecades, a scalable approach that could be used to build and control ananthropomimetic human-scale robot has not been demonstrated yet. CombiningMyorobotics, a framework for musculoskeletal robot development, with SpiNNaker,a neuromorphic computing platform, we present the proof-of-principle of asystem that can scale to dozens of neurally-controlled, physically compliantjoints. At its core, it implements a closed-loop cerebellar model whichprovides real-time low-level neural control at minimal power consumption andmaximal extensibility: higher-order (e.g., cortical) neural networks andneuromorphic sensors like silicon-retinae or -cochleae can naturally beincorporated.
arxiv-15600-26 | Variable projection without smoothness | http://arxiv.org/abs/1601.05011 | author:Aleksandr Aravkin, Dmitriy Drusvyatskiy, Tristan van Leeuwen category:math.OC stat.CO stat.ML published:2016-01-19 summary:Variable projection is a powerful technique in optimization. Over the last 30years, it has been applied broadly, with empirical and theoretical resultsdemonstrating both greater efficacy and greater stability than competingapproaches. In this paper, we illustrate the technique on a large class ofstructured nonsmooth optimization problems, with numerical examples in sparsedeconvolution and machine learning applications.
arxiv-15600-27 | PupilNet: Convolutional Neural Networks for Robust Pupil Detection | http://arxiv.org/abs/1601.04902 | author:Wolfgang Fuhl, Thiago Santini, Gjergji Kasneci, Enkelejda Kasneci category:cs.CV published:2016-01-19 summary:Real-time, accurate, and robust pupil detection is an essential prerequisitefor pervasive video-based eye-tracking. However, automated pupil detection inreal-world scenarios has proven to be an intricate challenge due to fastillumination changes, pupil occlusion, non centered and off-axis eye recording,and physiological eye characteristics. In this paper, we propose and evaluate amethod based on a novel dual convolutional neural network pipeline. In itsfirst stage the pipeline performs coarse pupil position identification using aconvolutional neural network and subregions from a downscaled input image todecrease computational costs. Using subregions derived from a small windowaround the initial pupil position estimate, the second pipeline stage employsanother convolutional neural network to refine this position, resulting in anincreased pupil detection rate up to 25% in comparison with the best performingstate-of-the-art algorithm. Annotated data sets can be made available uponrequest.
arxiv-15600-28 | Adaptive Image Denoising by Mixture Adaptation | http://arxiv.org/abs/1601.04770 | author:Enming Luo, Stanley H. Chan, Truong Q. Nguyen category:cs.CV stat.ME published:2016-01-19 summary:We propose an adaptive learning procedure to learn effective image priors.The new algorithm, called the Expectation-Maximization (EM) adaptation, takes ageneric prior learned from a generic external database and adapts it to theimage of interest to generate a specific prior. Different from existing methodswhich combine internal and external statistics in an ad-hoc way, the proposedalgorithm learns a single unified prior through an adaptive process. There aretwo major contributions in this paper. First, we rigorously derive the EMadaptation algorithm from the Bayesian hyper-prior perspective and show that itcan be further simplified to improve the computational complexity. Second, inthe absence of the latent clean image, we show how EM adaptation can bemodified and applied on pre-filtered images. We discuss how to estimateinternal parameters and demonstrate how to improve the denoising performance byrunning EM adaptation iteratively. Experimental results show that the adaptedprior is consistently better than the originally un-adapted prior, and issuperior than some state-of-the-art algorithms.
arxiv-15600-29 | PN-Net: Conjoined Triple Deep Network for Learning Local Image Descriptors | http://arxiv.org/abs/1601.05030 | author:Vassileios Balntas, Edward Johns, Lilian Tang, Krystian Mikolajczyk category:cs.CV published:2016-01-19 summary:In this paper we propose a new approach for learning local descriptors formatching image patches. It has recently been demonstrated that descriptorsbased on convolutional neural networks (CNN) can significantly improve thematching performance. Unfortunately their computational complexity isprohibitive for any practical application. We address this problem and proposea CNN based descriptor with improved matching performance, significantlyreduced training and execution time, as well as low dimensionality. We propose to train the network with triplets of patches that include apositive and negative pairs. To that end we introduce a new loss function thatexploits the relations within the triplets. We compare our approach to recentlyintroduced MatchNet and DeepCompare and demonstrate the advantages of ourdescriptor in terms of performance, memory footprint and speed i.e. when run inGPU, the extraction time of our 128 dimensional feature is comparable to thefastest available binary descriptors such as BRIEF and ORB.
arxiv-15600-30 | Top-N Recommender System via Matrix Completion | http://arxiv.org/abs/1601.04800 | author:Zhao Kang, Chong Peng, Qiang Cheng category:cs.IR cs.AI cs.LG stat.ML published:2016-01-19 summary:Top-N recommender systems have been investigated widely both in industry andacademia. However, the recommendation quality is far from satisfactory. In thispaper, we propose a simple yet promising algorithm. We fill the user-itemmatrix based on a low-rank assumption and simultaneously keep the originalinformation. To do that, a nonconvex rank relaxation rather than the nuclearnorm is adopted to provide a better rank approximation and an efficientoptimization strategy is designed. A comprehensive set of experiments on realdatasets demonstrates that our method pushes the accuracy of Top-Nrecommendation to a new level.
arxiv-15600-31 | A Theory of Local Matching: SIFT and Beyond | http://arxiv.org/abs/1601.05116 | author:Hossein Mobahi, Stefano Soatto category:cs.CV cs.LG published:2016-01-19 summary:Why has SIFT been so successful? Why its extension, DSP-SIFT, can furtherimprove SIFT? Is there a theory that can explain both? How can such theorybenefit real applications? Can it suggest new algorithms with reducedcomputational complexity or new descriptors with better accuracy for matching?We construct a general theory of local descriptors for visual matching. Ourtheory relies on concepts in energy minimization and heat diffusion. We showthat SIFT and DSP-SIFT approximate the solution the theory suggests. Inparticular, DSP-SIFT gives a better approximation to the theoretical solution;justifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derivenew descriptors that have fewer parameters and are potentially better inhandling affine deformations.
arxiv-15600-32 | Understanding Deep Convolutional Networks | http://arxiv.org/abs/1601.04920 | author:Stéphane Mallat category:stat.ML cs.CV cs.LG published:2016-01-19 summary:Deep convolutional networks provide state of the art classifications andregressions results over many high-dimensional problems. We review theirarchitecture, which scatters data with a cascade of linear filter weights andnon-linearities. A mathematical framework is introduced to analyze theirproperties. Computations of invariants involve multiscale contractions, thelinearization of hierarchical symmetries, and sparse separations. Applicationsare discussed.
arxiv-15600-33 | Graded Entailment for Compositional Distributional Semantics | http://arxiv.org/abs/1601.04908 | author:Desislava Bankova, Bob Coecke, Martha Lewis, Daniel Marsden category:cs.CL cs.AI cs.LO math.CT quant-ph published:2016-01-19 summary:The categorical compositional distributional model of natural languageprovides a conceptually motivated procedure to compute the meaning ofsentences, given grammatical structure and the meanings of its words. Thisapproach has outperformed other models in mainstream empirical languageprocessing tasks. However, until recently it has lacked the crucial feature oflexical entailment -- as do other distributional models of meaning. In this paper we solve the problem of entailment for categoricalcompositional distributional semantics. Taking advantage of the abstractcategorical framework allows us to vary our choice of model. This enables theintroduction of a notion of entailment, exploiting ideas from the categoricalsemantics of partial knowledge in quantum computation. The new model of language uses density matrices, on which we introduce anovel robust graded order capturing the entailment strength between concepts.This graded measure emerges from a general framework for approximateentailment, induced by any commutative monoid. Quantum logic embeds in ourgraded order. Our main theorem shows that entailment strength lifts compositionally to thesentence level, giving a lower bound on sentence entailment. We describe theessential properties of graded entailment such as continuity, and provide aprocedure for calculating entailment strength.
arxiv-15600-34 | Eye detection in digital images: challenges and solutions | http://arxiv.org/abs/1601.04871 | author:Mitra Montazeri, Mahdieh Montazeri, Saeid Saryazdi category:cs.CV published:2016-01-19 summary:Eye Detection has an important role in the field of biometric identificationand known as one method of person's identification. In recent years, manyefforts have been done which can detect eye automatically and with differentimage conditions. However, each method has its own drawbacks which can controlsome of these conditions. In this paper, different methods of eye detectionwill be categorized and explained. In each category, the advantages anddisadvantages of each method will be presented.
arxiv-15600-35 | Scale-aware Pixel-wise Object Proposal Networks | http://arxiv.org/abs/1601.04798 | author:Zequn Jie, Xiaodan Liang, Jiashi Feng, Wen Feng Lu, Eng Hock Francis Tay, Shuicheng Yan category:cs.CV published:2016-01-19 summary:Object proposal is essential for current state-of-the-art object detectionpipelines. However, the existing proposal methods generally fail in producingresults with satisfying localization accuracy. The case is even worse for smallobjects which however are quite common in practice. In this paper we propose anovel Scale-aware Pixel-wise Object Proposal (SPOP) network to tackle thechallenges. The SPOP network can generate proposals with high recall rate andaverage best overlap (ABO), even for small objects. In particular, in order toimprove the localization accuracy, a fully convolutional network is employedwhich predicts locations of object proposals for each pixel. The producedensemble of pixel-wise object proposals enhances the chance of hitting theobject significantly without incurring heavy extra computational cost. To solvethe challenge of localizing objects at small scale, two localization networkswhich are specialized for localizing objects with different scales areintroduced, following the divide-and-conquer philosophy. Location outputs ofthese two networks are then adaptively combined to generate the final proposalsby a large-/small-size weighting network. Extensive evaluations on PASCAL VOC2007 show the SPOP network is superior over the state-of-the-art models. Thehigh-quality proposals from SPOP network also significantly improve the meanaverage precision (mAP) of object detection with Fast-RCNN framework. Finally,the SPOP network (trained on PASCAL VOC) shows great generalization performancewhen testing it on ILSVRC 2013 validation set.
arxiv-15600-36 | Modeling Coverage for Neural Machine Translation | http://arxiv.org/abs/1601.04811 | author:Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li category:cs.CL published:2016-01-19 summary:Attention mechanism advanced state-of-the-art neural machine translation(NMT) by jointly learning to align and translate. However, attention-based NMTignores past alignment information, which often leads to over-translation andunder-translation. In response to this problem, we maintain a coverage vectorto keep track of the attention history. The coverage vector is fed to theattention model to help adjust future attention, which guides NMT to considermore about the untranslated source words. Experiments show that the proposedapproach significantly improves both translation quality and alignment qualityover traditional attention-based NMT.
arxiv-15600-37 | A Closed-Form Solution to Tensor Voting: Theory and Applications | http://arxiv.org/abs/1601.04888 | author:Tai-Pang Wu, Sai-Kit Yeung, Jiaya Jia, Chi-Keung Tang, Gerard Medioni category:cs.CV published:2016-01-19 summary:We prove a closed-form solution to tensor voting (CFTV): given a point set inany dimensions, our closed-form solution provides an exact, continuous andefficient algorithm for computing a structure-aware tensor that simultaneouslyachieves salient structure detection and outlier attenuation. Using CFTV, weprove the convergence of tensor voting on a Markov random field (MRF), thustermed as MRFTV, where the structure-aware tensor at each input site reaches astationary state upon convergence in structure propagation. We then embedstructure-aware tensor into expectation maximization (EM) for optimizing asingle linear structure to achieve efficient and robust parameter estimation.Specifically, our EMTV algorithm optimizes both the tensor and fittingparameters and does not require random sampling consensus typically used inexisting robust statistical techniques. We performed quantitative evaluation onits accuracy and robustness, showing that EMTV performs better than theoriginal TV and other state-of-the-art techniques in fundamental matrixestimation for multiview stereo matching. The extensions of CFTV and EMTV forextracting multiple and nonlinear structures are underway. An addendum isincluded in this arXiv version.
arxiv-15600-38 | Content Aware Neural Style Transfer | http://arxiv.org/abs/1601.04568 | author:Rujie Yin category:cs.CV 68T10 published:2016-01-18 summary:This paper presents a content-aware style transfer algorithm for paintingsand photos of similar content using pre-trained neural network, obtainingbetter results than the previous work. In addition, the numerical experimentsshow that the style pattern and the content information is not completelyseparated by neural network.
arxiv-15600-39 | Incremental Semiparametric Inverse Dynamics Learning | http://arxiv.org/abs/1601.04549 | author:Raffaello Camoriano, Silvio Traversaro, Lorenzo Rosasco, Giorgio Metta, Francesco Nori category:stat.ML cs.LG cs.RO published:2016-01-18 summary:This paper presents a novel approach for incremental semiparametric inversedynamics learning. In particular, we consider the mixture of two approaches:Parametric modeling based on rigid body dynamics equations and nonparametricmodeling based on incremental kernel methods, with no prior information on themechanical properties of the system. This yields to an incrementalsemiparametric approach, leveraging the advantages of both the parametric andnonparametric models. We validate the proposed technique learning the dynamicsof one arm of the iCub humanoid robot.
arxiv-15600-40 | Domain based classification | http://arxiv.org/abs/1601.04530 | author:Robert P. W. Duin, Elzbieta Pekalska category:stat.ML cs.LG published:2016-01-18 summary:The majority of traditional classification ru les minimizing the expectedprobability of error (0-1 loss) are inappropriate if the class probabilitydistributions are ill-defined or impossible to estimate. We argue that in suchcases class domains should be used instead of class distributions or densitiesto construct a reliable decision function. Proposals are presented for someevaluation criteria and classifier learning schemes, illustrated by an example.
arxiv-15600-41 | Bandit Structured Prediction for Learning from Partial Feedback in Statistical Machine Translation | http://arxiv.org/abs/1601.04468 | author:Artem Sokolov, Stefan Riezler, Tanguy Urvoy category:cs.CL cs.LG published:2016-01-18 summary:We present an approach to structured prediction from bandit feedback, calledBandit Structured Prediction, where only the value of a task loss function at asingle predicted point, instead of a correct structure, is observed inlearning. We present an application to discriminative reranking in StatisticalMachine Translation (SMT) where the learning algorithm only has access to a1-BLEU loss evaluation of a predicted translation instead of obtaining a goldstandard reference translation. In our experiment bandit feedback is obtainedby evaluating BLEU on reference translations without revealing them to thealgorithm. This can be thought of as a simulation of interactive machinetranslation where an SMT system is personalized by a user who provides singlepoint feedback to predicted translations. Our experiments show that ourapproach improves translation quality and is comparable to approaches thatemploy more informative feedback in learning.
arxiv-15600-42 | Zero-error dissimilarity based classifiers | http://arxiv.org/abs/1601.04451 | author:Robert P. W. Duin, Elzbieta Pekalska category:stat.ML cs.LG published:2016-01-18 summary:We consider general non-Euclidean distance measures between real worldobjects that need to be classified. It is assumed that objects are representedby distances to other objects only. Conditions for zero-error dissimilaritybased classifiers are derived. Additional conditions are given under which thezero-error decision boundary is a continues function of the distances to afinite set of training samples. These conditions affect the objects as well asthe distance measure used. It is argued that they can be met in practice.
arxiv-15600-43 | Detecting the Age of Twitter Users | http://arxiv.org/abs/1601.04621 | author:Benjamin Paul Chamberlain, Clive Humby, Marc Peter Deisenroth category:cs.SI stat.ML published:2016-01-18 summary:Twitter provides an extremely rich and open source of data for studying humanbehaviour at scale. It has been used to advance our understanding of socialnetwork structure, the viral flow of information and how new ideas develop.Enriching Twitter with demographic information would permit more precisescience and better generalisation to the real world. The only demographicindicators associated with a Twitter account are the free text name, locationand description fields. We show how the age of most Twitter accounts can beinferred with high accuracy using the structure of the social graph. Besidesclassical social science applications, there are obvious privacy and childprotection implications to this discovery. Previous work on Twitter agedetection has focussed on either user-name or linguistic features of tweets. Ashortcoming of the user-name approach is that it requires real names (Twitternames are often false) and census data from each user's (unknown) birthcountry. Problems with linguistic approaches are that most Twitter users do nottweet (the median number of Tweets is 4) and a different model must be learntfor each language. To address these issues, we devise a language-independentmethodology for determining the age of Twitter users from data that is nativeto the Twitter ecosystem. Roughly 150,000 Twitter users specify an age in theirfree text description field. We generalize this to the entire Twitter networkby showing that age can be predicted based on what or whom they follow. Weadopt a Bayesian classification paradigm, which offers a consistent frameworkfor handling uncertainty in our data, e.g., inaccurate age descriptions orspurious edges in the graph. Working within this paradigm we have successfullyapplied age detection to 700 million Twitter accounts with an F1 Score of 0.86.
arxiv-15600-44 | Discovering Picturesque Highlights from Egocentric Vacation Videos | http://arxiv.org/abs/1601.04406 | author:Vinay Bettadapura, Daniel Castro, Irfan Essa category:cs.CV published:2016-01-18 summary:We present an approach for identifying picturesque highlights from largeamounts of egocentric video data. Given a set of egocentric videos capturedover the course of a vacation, our method analyzes the videos and looks forimages that have good picturesque and artistic properties. We introduce noveltechniques to automatically determine aesthetic features such as composition,symmetry and color vibrancy in egocentric videos and rank the video framesbased on their photographic qualities to generate highlights. Our approach alsouses contextual information such as GPS, when available, to assess the relativeimportance of each geographic location where the vacation videos were shot.Furthermore, we specifically leverage the properties of egocentric videos toimprove our highlight detection. We demonstrate results on a new egocentricvacation dataset which includes 26.5 hours of videos taken over a 14 dayvacation that spans many famous tourist destinations and also provide resultsfrom a user-study to access our results.
arxiv-15600-45 | Nonparametric Bayesian Storyline Detection from Microtexts | http://arxiv.org/abs/1601.04580 | author:Vinodh Krishnan, Jacob Eisenstein category:cs.CL cs.LG published:2016-01-18 summary:News events and social media are composed of evolving storylines, whichcapture public attention for a limited period of time. Identifying thesestorylines would enable many high-impact applications, such as tracking publicinterest and opinion in ongoing crisis events. However, this requiresintegrating temporal and linguistic information, and prior work takes a largelyheuristic approach. We present a novel online non-parametric Bayesian frameworkfor storyline detection, using the distance-dependent Chinese RestaurantProcess (dd-CRP). To ensure efficient linear-time inference, we employ afixed-lag Gibbs sampling procedure, which is novel for the dd-CRP. We evaluateour baseline and proposed models on the TREC Twitter Timeline Generation taskand show strong results.
arxiv-15600-46 | A Comparative Study of Object Trackers for Infrared Flying Bird Tracking | http://arxiv.org/abs/1601.04386 | author:Ying Huang, Hong Zheng, Haibin Ling, Erik Blasch, Hao Yang category:cs.CV published:2016-01-18 summary:Bird strikes present a huge risk for aircraft, especially since traditionalairport bird surveillance is mainly dependent on inefficient human observation.Computer vision based technology has been proposed to automatically detectbirds, determine bird flying trajectories, and predict aircraft takeoff delays.However, the characteristics of bird flight using imagery and the performanceof existing methods applied to flying bird task are not well known. Therefore,we perform infrared flying bird tracking experiments using 12 state-of-the-artalgorithms on a real BIRDSITE-IR dataset to obtain useful clues and recommendfeature analysis. We also develop a Struck-scale method to demonstrate theeffectiveness of multiple scale sampling adaption in handling the object offlying bird with varying shape and scale. The general analysis can be used todevelop specialized bird tracking methods for airport safety, wildness andurban bird population studies.
arxiv-15600-47 | Combining Markov Random Fields and Convolutional Neural Networks for Image Synthesis | http://arxiv.org/abs/1601.04589 | author:Chuan Li, Michael Wand category:cs.CV published:2016-01-18 summary:This paper studies a combination of generative Markov random field (MRF)models and discriminatively trained deep convolutional neural networks (dCNNs)for synthesizing 2D images. The generative MRF acts on higher-levels of a dCNNfeature pyramid, controling the image layout at an abstract level. We apply themethod to both photographic and non-photo-realistic (artwork) synthesis tasks.The MRF regularizer prevents over-excitation artifacts and reduces implausiblefeature mixtures common to previous dCNN inversion approaches, permittingsynthezing photographic content with increased visual plausibility. Unlikestandard MRF-based texture synthesis, the combined system can both match andadapt local features with considerable variability, yielding results far out ofreach of classic generative MRF methods.
arxiv-15600-48 | Comparison-based Image Quality Assessment for Parameter Selection | http://arxiv.org/abs/1601.04619 | author:Haoyi Liang, Daniel S. Weller category:cs.CV published:2016-01-18 summary:Image quality assessment (IQA) is traditionally classified intofull-reference (FR) IQA and no-reference (NR) IQA according to whether theoriginal image is required. Although NR-IQA is widely used in practicalapplications, room for improvement still remains because of the lack of thereference image. Inspired by the fact that in many applications, such asparameter selection, a series of distorted images are available, the authorspropose a novel comparison-based image quality assessment (C-IQA) method. Thenew comparison-based framework parallels FR-IQA by requiring two input images,and resembles NR-IQA by not using the original image. As a result, the newcomparison-based approach has more application scenarios than FR-IQA does, andtakes greater advantage of the accessible information than the traditionalsingle-input NR-IQA does. Further, C-IQA is compared with otherstate-of-the-art NR-IQA methods on two widely used IQA databases. Experimentalresults show that C-IQA outperforms the other NR-IQA methods for parameterselection, and the parameter trimming framework combined with C-IQA saves thecomputation of iterative image reconstruction up to 80%.
arxiv-15600-49 | Sub-Sampled Newton Methods II: Local Convergence Rates | http://arxiv.org/abs/1601.04738 | author:Farbod Roosta-Khorasani, Michael W. Mahoney category:math.OC cs.LG stat.ML published:2016-01-18 summary:Many data-fitting applications require the solution of an optimizationproblem involving a sum of large number of functions of high dimensionalparameter. Here, we consider the problem of minimizing a sum of $n$ functionsover a convex constraint set $\mathcal{X} \subseteq \mathbb{R}^{p}$ where both$n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$can offer great amount of computational efficiency. Within the context of second order methods, we first give quantitative localconvergence results for variants of Newton's method where the Hessian isuniformly sub-sampled. Using random matrix concentration inequalities, one cansub-sample in a way that the curvature information is preserved. Using suchsub-sampling strategy, we establish locally Q-linear and Q-superlinearconvergence rates. We also give additional convergence results for when thesub-sampled Hessian is regularized by modifying its spectrum or Levenberg-typeregularization. Finally, in addition to Hessian sub-sampling, we consider sub-sampling thegradient as way to further reduce the computational complexity per iteration.We use approximate matrix multiplication results from randomized numericallinear algebra (RandNLA) to obtain the proper sampling strategy and weestablish locally R-linear convergence rates. In such a setting, we also showthat a very aggressive sample size increase results in a R-superlinearlyconvergent algorithm. While the sample size depends on the condition number of the problem, ourconvergence rates are problem-independent, i.e., they do not depend on thequantities related to the problem. Hence, our analysis here can be used tocomplement the results of our basic framework from the companion paper, [38],by exploring algorithmic trade-offs that are important in practice.
arxiv-15600-50 | Improved Sampling Techniques for Learning an Imbalanced Data Set | http://arxiv.org/abs/1601.04756 | author:Maureen Lyndel C. Lauron, Jaderick P. Pabico category:cs.LG published:2016-01-18 summary:This paper presents the performance of a classifier built using the stackingCalgorithm in nine different data sets. Each data set is generated using asampling technique applied on the original imbalanced data set. Five newsampling techniques are proposed in this paper (i.e., SMOTERandRep, Lax RandomOversampling, Lax Random Undersampling, Combined-Lax Random OversamplingUndersampling, and Combined-Lax Random Undersampling Oversampling) that werebased on the three sampling techniques (i.e., Random Undersampling, RandomOversampling, and Synthetic Minority Oversampling Technique) usually used assolutions in imbalance learning. The metrics used to evaluate the classifier'sperformance were F-measure and G-mean. F-measure determines the performance ofthe classifier for every class, while G-mean measures the overall performanceof the classifier. The results using F-measure showed that for the data withouta sampling technique, the classifier's performance is good only for themajority class. It also showed that among the eight sampling techniques, RU andLRU have the worst performance while other techniques (i.e., RO, C-LRUO andC-LROU) performed well only on some classes. The best performing techniques inall data sets were SMOTE, SMOTERandRep, and LRO having the lowest F-measurevalues between 0.5 and 0.65. The results using G-mean showed that theoversampling technique that attained the highest G-mean value is LRO (0.86),next is C-LROU (0.85), then SMOTE (0.84) and finally is SMOTERandRep (0.83).Combining the result of the two metrics (F-measure and G-mean), only the threesampling techniques are considered as good performing (i.e., LRO, SMOTE, andSMOTERandRep).
arxiv-15600-51 | A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure | http://arxiv.org/abs/1601.04674 | author:Peter Schulam, Suchi Saria category:stat.ML published:2016-01-18 summary:For many complex diseases, there is a wide variety of ways in which anindividual can manifest the disease. The challenge of personalized medicine isto develop tools that can accurately predict the trajectory of an individual'sdisease, which can in turn enable clinicians to optimize treatments. Werepresent an individual's disease trajectory as a continuous-valuedcontinuous-time function describing the severity of the disease over time. Wepropose a hierarchical latent variable model that individualizes predictions ofdisease trajectories. This model shares statistical strength acrossobservations at different resolutions--the population, subpopulation and theindividual level. We describe an algorithm for learning population andsubpopulation parameters offline, and an online procedure for dynamicallylearning individual-specific parameters. Finally, we validate our model on thetask of predicting the course of interstitial lung disease, a leading cause ofdeath among patients with the autoimmune disease scleroderma. We compare ourapproach against state-of-the-art and demonstrate significant improvements inpredictive accuracy.
arxiv-15600-52 | Proactive Message Passing on Memory Factor Networks | http://arxiv.org/abs/1601.04667 | author:Patrick Eschenfeldt, Dan Schmidt, Stark Draper, Jonathan Yedidia category:cs.AI cs.CV published:2016-01-18 summary:We introduce a new type of graphical model that we call a "memory factornetwork" (MFN). We show how to use MFNs to model the structure inherent in manytypes of data sets. We also introduce an associated message-passing stylealgorithm called "proactive message passing"' (PMP) that performs inference onMFNs. PMP comes with convergence guarantees and is efficient in comparison tocompeting algorithms such as variants of belief propagation. We specialize MFNsand PMP to a number of distinct types of data (discrete, continuous, labelled)and inference problems (interpolation, hypothesis testing), provide examples,and discuss approaches for efficient implementation.
arxiv-15600-53 | Sub-Sampled Newton Methods I: Globally Convergent Algorithms | http://arxiv.org/abs/1601.04737 | author:Farbod Roosta-Khorasani, Michael W. Mahoney category:math.OC cs.LG stat.ML published:2016-01-18 summary:Large scale optimization problems are ubiquitous in machine learning and dataanalysis and there is a plethora of algorithms for solving such problems. Manyof these algorithms employ sub-sampling, as a way to either speed up thecomputations and/or to implicitly implement a form of statisticalregularization. In this paper, we consider second-order iterative optimizationalgorithms and we provide bounds on the convergence of the variants of Newton'smethod that incorporate uniform sub-sampling as a means to estimate thegradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Ouralgorithms are global and are guaranteed to converge from any initial iterate. Using random matrix concentration inequalities, one can sub-sample theHessian to preserve the curvature information. Our first algorithm incorporatesHessian sub-sampling while using the full gradient. We also give additionalconvergence results for when the sub-sampled Hessian is regularized bymodifying its spectrum or ridge-type regularization. Next, in addition toHessian sub-sampling, we also consider sub-sampling the gradient as a way tofurther reduce the computational complexity per iteration. We use approximatematrix multiplication results from randomized numerical linear algebra toobtain the proper sampling strategy. In all these algorithms, computing theupdate boils down to solving a large scale linear system, which can becomputationally expensive. As a remedy, for all of our algorithms, we also giveglobal convergence results for the case of inexact updates where such linearsystem is solved only approximately. This paper has a more advanced companion paper, [42], in which we demonstratethat, by doing a finer-grained analysis, we can get problem-independent boundsfor local convergence of these algorithms and explore trade-offs to improveupon the basic results of the present paper.
arxiv-15600-54 | Statistical Mechanics of High-Dimensional Inference | http://arxiv.org/abs/1601.04650 | author:Madhu Advani, Surya Ganguli category:stat.ML math.ST q-bio.QM stat.TH published:2016-01-18 summary:To model modern large-scale datasets, we need efficient algorithms to infer aset of $P$ unknown model parameters from $N$ noisy measurements. What arefundamental limits on the accuracy of parameter inference, given finitesignal-to-noise ratios, limited measurements, prior information, andcomputational tractability requirements? How can we combine prior informationwith measurements to achieve these limits? Classical statistics gives incisiveanswers to these questions as the measurement density $\alpha =\frac{N}{P}\rightarrow \infty$. However, these classical results are notrelevant to modern high-dimensional inference problems, which instead occur atfinite $\alpha$. We formulate and analyze high-dimensional inference as aproblem in the statistical physics of quenched disorder. Our analysis uncoversfundamental limits on the accuracy of inference in high dimensions, and revealsthat widely cherished inference algorithms like maximum likelihood (ML) andmaximum-a posteriori (MAP) inference cannot achieve these limits. We furtherfind optimal, computationally tractable algorithms that can achieve theselimits. Intriguingly, in high dimensions, these optimal algorithms becomecomputationally simpler than MAP and ML, while still outperforming them. Forexample, such optimal algorithms can lead to as much as a 20% reduction in theamount of data to achieve the same performance relative to MAP. Moreover, ouranalysis reveals simple relations between optimal high dimensional inferenceand low dimensional scalar Bayesian inference, insights into the nature ofgeneralization and predictive power in high dimensions, information theoreticlimits on compressed sensing, phase transitions in quadratic inference, andconnections to central mathematical objects in convex optimization theory andrandom matrix theory.
arxiv-15600-55 | The Image Torque Operator for Contour Processing | http://arxiv.org/abs/1601.04669 | author:Morimichi Nishigaki, Cornelia Fermüller category:cs.CV published:2016-01-18 summary:Contours are salient features for image description, but the detection andlocalization of boundary contours is still considered a challenging problem.This paper introduces a new tool for edge processing implementing theGestaltism idea of edge grouping. This tool is a mid-level image operator,called the Torque operator, that is designed to help detect closed contours inimages. The torque operator takes as input the raw image and creates an imagemap by computing from the image gradients within regions of multiple sizes ameasure of how well the edges are aligned to form closed convex contours.Fundamental properties of the torque are explored and illustrated throughexamples. Then it is applied in pure bottom-up processing in a variety ofapplications, including edge detection, visual attention and segmentation andexperimentally demonstrated a useful tool that can improve existing techniques.Finally, its extension as a more general grouping mechanism and application inobject recognition is discussed.
arxiv-15600-56 | Sparse Convex Clustering | http://arxiv.org/abs/1601.04586 | author:Binhuan Wang, Yilong Zhang, Wei Sun, Yixin Fang category:stat.ME cs.LG stat.ML published:2016-01-18 summary:Convex clustering, a convex relaxation of k-means clustering and hierarchicalclustering, has drawn recent attentions since it nicely addresses theinstability issue of traditional nonconvex clustering methods. Although itscomputational and statistical properties have been recently studied, theperformance of convex clustering has not yet been investigated in thehigh-dimensional clustering scenario, where the data contains a large number offeatures and many of them carry no information about the clustering structure.In this paper, we demonstrate that the performance of convex clustering couldbe distorted when the uninformative features are included in the clustering. Toovercome it, we introduce a new clustering method, referred to as Sparse ConvexClustering, to simultaneously cluster observations and conduct featureselection. The key idea is to formulate convex clustering in a form ofregularization, with an adaptive group-lasso penalty term on cluster centers.In order to optimally balance the tradeoff between the cluster fitting andsparsity, a tuning criterion based on clustering stability is developed. Intheory, we provide an unbiased estimator for the degrees of freedom of theproposed sparse convex clustering method. Finally, the effectiveness of thesparse convex clustering is examined through a variety of numerical experimentsand a real data application.
arxiv-15600-57 | Spectral Theory of Unsigned and Signed Graphs. Applications to Graph Clustering: a Survey | http://arxiv.org/abs/1601.04692 | author:Jean Gallier category:cs.LG cs.DS 68 published:2016-01-18 summary:This is a survey of the method of graph cuts and its applications to graphclustering of weighted unsigned and signed graphs. I provide a fairly thoroughtreatment of the method of normalized graph cuts, a deeply original method dueto Shi and Malik, including complete proofs. The main thrust of this paper isthe method of normalized cuts. I give a detailed account for K = 2 clusters,and also for K > 2 clusters, based on the work of Yu and Shi. I also show howboth graph drawing and normalized cut K-clustering can be easily generalized tohandle signed graphs, which are weighted graphs in which the weight matrix Wmay have negative coefficients. Intuitively, negative coefficients indicatedistance or dissimilarity. The solution is to replace the degree matrix by thematrix in which absolute values of the weights are used, and to replace theLaplacian by the Laplacian with the new degree matrix of absolute values. Asfar as I know, the generalization of K-way normalized clustering to signedgraphs is new. Finally, I show how the method of ratio cuts, in which a cut isnormalized by the size of the cluster rather than its volume, is just a specialcase of normalized cuts.
arxiv-15600-58 | Learning the kernel matrix via predictive low-rank approximations | http://arxiv.org/abs/1601.04366 | author:Martin Stražar, Tomaž Curk category:cs.LG stat.ML published:2016-01-17 summary:Efficient and accurate low-rank approximations of multiple data sources areessential in the era of big data. The scaling of kernel-based learningalgorithms to large datasets is limited by the O(n^2) computation and storagecomplexity of the full kernel matrix, which is required by most of the recentkernel learning algorithms. We present the Mklaren algorithm to approximate multiple kernel matriceslearn a regression model, which is entirely based on geometrical concepts. Thealgorithm does not require access to full kernel matrices yet it accounts forthe correlations between all kernels. It uses Incomplete Choleskydecomposition, where pivot selection is based on least-angle regression in thecombined, low-dimensional feature space. The algorithm has linear complexity inthe number of data points and kernels. When explicit feature space induced bythe kernel can be constructed, a mapping from the dual to the primal Ridgeregression weights is used for model interpretation. The Mklaren algorithm was tested on eight standard regression datasets. Itoutperforms contemporary kernel matrix approximation approaches when learningwith multiple kernels. It identifies relevant kernels, achieving highestexplained variance than other multiple kernel learning methods for the samenumber of iterations. Test accuracy, equivalent to the one using full kernelmatrices, was achieved with at significantly lower approximation ranks. Adifference in run times of two orders of magnitude was observed when either thenumber of samples or kernels exceeds 3000.
arxiv-15600-59 | On-line Bayesian System Identification | http://arxiv.org/abs/1601.04251 | author:Diego Romeres, Giulia Prando, Gianluigi Pillonetto, Alessandro Chiuso category:cs.SY cs.LG stat.AP stat.ML published:2016-01-17 summary:We consider an on-line system identification setting, in which new databecome available at given time steps. In order to meet real-time estimationrequirements, we propose a tailored Bayesian system identification procedure,in which the hyper-parameters are still updated through Marginal Likelihoodmaximization, but after only one iteration of a suitable iterative optimizationalgorithm. Both gradient methods and the EM algorithm are considered for theMarginal Likelihood optimization. We compare this "1-step" procedure with thestandard one, in which the optimization method is run until convergence to alocal minimum. The experiments we perform confirm the effectiveness of theapproach we propose.
arxiv-15600-60 | Face-space Action Recognition by Face-Object Interactions | http://arxiv.org/abs/1601.04293 | author:Amir Rosenfeld, Shimon Ullman category:cs.CV published:2016-01-17 summary:Action recognition in still images has seen major improvement in recent yearsdue to advances in human pose estimation, object recognition and strongerfeature representations. However, there are still many cases in whichperformance remains far from that of humans. In this paper, we approach theproblem by learning explicitly, and then integrating three components oftransitive actions: (1) the human body part relevant to the action (2) theobject being acted upon and (3) the specific form of interaction between theperson and the object. The process uses class-specific features and relationsnot used in the past for action recognition and which use inherently two cyclesin the process unlike most standard approaches. We focus on face-relatedactions (FRA), a subset of actions that includes several currently challengingcategories. We present an average relative improvement of 52% over state-of-theart. We also make a new benchmark publicly available.
arxiv-15600-61 | Building a Learning Database for the Neural Network Retrieval of Sea Surface Salinity from SMOS Brightness Temperatures | http://arxiv.org/abs/1601.04296 | author:Adel Ammar, Sylvie Labroue, Estelle Obligis, Michel Crépon, Sylvie Thiria category:cs.NE physics.ao-ph published:2016-01-17 summary:This article deals with an important aspect of the neural network retrievalof sea surface salinity (SSS) from SMOS brightness temperatures (TBs). Theneural network retrieval method is an empirical approach that offers thepossibility of being independent from any theoretical emissivity model, duringthe in-flight phase. A Previous study [1] has proven that this approach isapplicable to all pixels on ocean, by designing a set of neural networks withdifferent inputs. The present study focuses on the choice of the learningdatabase and demonstrates that a judicious distribution of the geophysicalparameters allows to markedly reduce the systematic regional biases of theretrieved SSS, which are due to the high noise on the TBs. An equalization ofthe distribution of the geophysical parameters, followed by a new technique forboosting the learning process, makes the regional biases almost disappear forlatitudes between 40{\deg}S and 40{\deg}N, while the global standard deviationremains between 0.6 psu (at the center of the of the swath) and 1 psu (at theedges).
arxiv-15600-62 | Conversion of Artificial Recurrent Neural Networks to Spiking Neural Networks for Low-power Neuromorphic Hardware | http://arxiv.org/abs/1601.04187 | author:Peter U. Diehl, Guido Zarrella, Andrew Cassidy, Bruno U. Pedroni, Emre Neftci category:cs.NE published:2016-01-16 summary:In recent years the field of neuromorphic low-power systems that consumeorders of magnitude less power gained significant momentum. However, theirwider use is still hindered by the lack of algorithms that can harness thestrengths of such architectures. While neuromorphic adaptations ofrepresentation learning algorithms are now emerging, efficient processing oftemporal sequences or variable length-inputs remain difficult. Recurrent neuralnetworks (RNN) are widely used in machine learning to solve a variety ofsequence learning tasks. In this work we present a train-and-constrainmethodology that enables the mapping of machine learned (Elman) RNNs on asubstrate of spiking neurons, while being compatible with the capabilities ofcurrent and near-future neuromorphic systems. This "train-and-constrain" methodconsists of first training RNNs using backpropagation through time, thendiscretizing the weights and finally converting them to spiking RNNs bymatching the responses of artificial neurons with those of the spiking neurons.We demonstrate our approach by mapping a natural language processing task(question classification), where we demonstrate the entire mapping process ofthe recurrent layer of the network on IBM's Neurosynaptic System "TrueNorth", aspike-based digital neuromorphic hardware architecture. TrueNorth imposesspecific constraints on connectivity, neural and synaptic parameters. Tosatisfy these constraints, it was necessary to discretize the synaptic weightsand neural activities to 16 levels, and to limit fan-in to 64 inputs. We findthat short synaptic delays are sufficient to implement the dynamical (temporal)aspect of the RNN in the question classification task. The hardware-constrainedmodel achieved 74% accuracy in question classification while using less than0.025% of the cores on one TrueNorth chip, resulting in an estimated powerconsumption of ~17 uW.
arxiv-15600-63 | Training Recurrent Neural Networks by Diffusion | http://arxiv.org/abs/1601.04114 | author:Hossein Mobahi category:cs.LG published:2016-01-16 summary:This work presents a new algorithm for training recurrent neural networks(although ideas are applicable to feedforward networks as well). The algorithmis derived from a theory in nonconvex optimization related to the diffusionequation. The contributions made in this work are two fold. First, we show howsome seemingly disconnected mechanisms used in deep learning such as smartinitialization, annealed learning rate, layerwise pretraining, and noiseinjection (as done in dropout and SGD) arise naturally and automatically fromthis framework, without manually crafting them into the algorithms. Second, wepresent some preliminary results on comparing the proposed method against SGD.It turns out that the new algorithm can achieve similar level of generalizationaccuracy of SGD in much fewer number of epochs.
arxiv-15600-64 | TrueHappiness: Neuromorphic Emotion Recognition on TrueNorth | http://arxiv.org/abs/1601.04183 | author:Peter U. Diehl, Bruno U. Pedroni, Andrew Cassidy, Paul Merolla, Emre Neftci, Guido Zarrella category:q-bio.NC cs.NE published:2016-01-16 summary:We present an approach to constructing a neuromorphic device that responds tolanguage input by producing neuron spikes in proportion to the strength of theappropriate positive or negative emotional response. Specifically, we perform afine-grained sentiment analysis task with implementations on two differentsystems: one using conventional spiking neural network (SNN) simulators and theother one using IBM's Neurosynaptic System TrueNorth. Input words are projectedinto a high-dimensional semantic space and processed through a fully-connectedneural network (FCNN) containing rectified linear units trained viabackpropagation. After training, this FCNN is converted to a SNN bysubstituting the ReLUs with integrate-and-fire neurons. We show that there ispractically no performance loss due to conversion to a spiking network on asentiment analysis test set, i.e. correlations between predictions and humanannotations differ by less than 0.02 comparing the original DNN and its spikingequivalent. Additionally, we show that the SNN generated with this techniquecan be mapped to existing neuromorphic hardware -- in our case, the TrueNorthchip. Mapping to the chip involves 4-bit synaptic weight discretization andadjustment of the neuron thresholds. The resulting end-to-end system can take auser input, i.e. a word in a vocabulary of over 300,000 words, and estimate itssentiment on TrueNorth with a power consumption of approximately 50 uW.
arxiv-15600-65 | $\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of JPEG-Compressed Images | http://arxiv.org/abs/1601.04149 | author:Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, Thomas S. Huang category:cs.CV cs.AI cs.LG published:2016-01-16 summary:In this paper, we design a Deep Dual-Domain ($\mathbf{D^3}$) based fastrestoration model to remove artifacts of JPEG compressed images. It leveragesthe large learning capacity of deep networks, as well as the problem-specificexpertise that was hardly incorporated in the past design of deeparchitectures. For the latter, we take into consideration both the priorknowledge of the JPEG compression scheme, and the successful practice of thesparsity-based dual-domain approach. We further design the One-Step SparseInference (1-SI) module, as an efficient and light-weighted feed-forwardapproximation of sparse coding. Extensive experiments verify the superiority ofthe proposed $D^3$ model over several state-of-the-art methods. Specifically,our best model is capable of outperforming the latest deep model for around 1dB in PSNR, and is 30 times faster.
arxiv-15600-66 | Estimation of Fiber Orientations Using Neighborhood Information | http://arxiv.org/abs/1601.04115 | author:Chuyang Ye, Jiachen Zhuo, Rao P. Gullapalli, Jerry L. Prince category:cs.CV published:2016-01-16 summary:Data from diffusion magnetic resonance imaging (dMRI) can be used toreconstruct fiber tracts, for example, in muscle and white matter. Estimationof fiber orientations (FOs) is a crucial step in the reconstruction process andthese estimates can be corrupted by noise. In this paper, a new method calledFiber Orientation Reconstruction using Neighborhood Information (FORNI) isdescribed and shown to reduce the effects of noise and improve FO estimationperformance by incorporating spatial consistency. FORNI uses a fixed tensorbasis to model the diffusion weighted signals, which has the advantage ofproviding an explicit relationship between the basis vectors and the FOs. FOspatial coherence is encouraged using weighted l1-norm regularization terms,which contain the interaction of directional information between neighborvoxels. Data fidelity is encouraged using a squared error between the observedand reconstructed diffusion weighted signals. After appropriate weighting ofthese competing objectives, the resulting objective function is minimized usinga block coordinate descent algorithm, and a straightforward parallelizationstrategy is used to speed up processing. Experiments were performed on adigital crossing phantom, ex vivo tongue dMRI data, and in vivo brain dMRI datafor both qualitative and quantitative evaluation. The results demonstrate thatFORNI improves the quality of FO estimation over other state of the artalgorithms.
arxiv-15600-67 | Brain-Inspired Deep Networks for Image Aesthetics Assessment | http://arxiv.org/abs/1601.04155 | author:Zhangyang Wang, Shiyu Chang, Florin Dolcos, Diane Beck, Ding Liu, Thomas S. Huang category:cs.CV cs.LG cs.NE published:2016-01-16 summary:Image aesthetics assessment has been challenging due to its subjectivenature. Inspired by the scientific advances in the human visual perception andneuroaesthetics, we design Brain-Inspired Deep Networks (BDN) for this task.BDN first learns attributes through the parallel supervised pathways, on avariety of selected feature dimensions. A high-level synthesis network istrained to associate and transform those attributes into the overall aestheticsrating. We then extend BDN to predicting the distribution of human ratings,since aesthetics ratings are often subjective. Another highlight is ourfirst-of-its-kind study of label-preserving transformations in the context ofaesthetics assessment, which leads to an effective data augmentation approach.Experimental results on the AVA dataset show that our biological inspired andtask-specific BDN model gains significantly performance improvement, comparedto other state-of-the-art models with the same or higher parameter capacity.
arxiv-15600-68 | Engineering Safety in Machine Learning | http://arxiv.org/abs/1601.04126 | author:Kush R. Varshney category:stat.ML cs.AI cs.CY cs.LG published:2016-01-16 summary:Machine learning algorithms are increasingly influencing our decisions andinteracting with us in all parts of our daily lives. Therefore, just like forpower plants, highways, and myriad other engineered sociotechnical systems, wemust consider the safety of systems involving machine learning. In this paper,we first discuss the definition of safety in terms of risk, epistemicuncertainty, and the harm incurred by unwanted outcomes. Then we examinedimensions, such as the choice of cost function and the appropriateness ofminimizing the empirical average training cost, along which certain real-worldapplications may not be completely amenable to the foundational principle ofmodern statistical machine learning: empirical risk minimization. Inparticular, we note an emerging dichotomy of applications: ones in which safetyis important and risk minimization is not the complete story (we name theseType A applications), and ones in which safety is not so critical and riskminimization is sufficient (we name these Type B applications). Finally, wediscuss how four different strategies for achieving safety in engineering(inherently safe design, safety reserves, safe fail, and procedural safeguards)can be mapped to the machine learning context through interpretability andcausality of predictive models, objectives beyond expected prediction accuracy,human involvement for labeling difficult or rare examples, and user experiencedesign of software.
arxiv-15600-69 | Compositional Model based Fisher Vector Coding for Image Classification | http://arxiv.org/abs/1601.04143 | author:Lingqiao Liu, Peng Wang, Chunhua Shen, Lei Wang, Anton van den Hengel, Chao Wang, Heng Tao Shen category:cs.CV published:2016-01-16 summary:Deriving from the gradient vector of a generative model of local features,Fisher vector coding (FVC) has been identified as an effective coding methodfor image classification. Most, if not all, FVC implementations employ theGaussian mixture model (GMM) to depict the generation process of localfeatures. However, the representative power of the GMM could be limited becauseit essentially assumes that local features can be characterized by a fixednumber of feature prototypes and the number of prototypes is usually small inFVC. To handle this limitation, in this paper we break the convention whichassumes that a local feature is drawn from one of few Gaussian distributions.Instead, we adopt a compositional mechanism which assumes that a local featureis drawn from a Gaussian distribution whose mean vector is composed as thelinear combination of multiple key components and the combination weight is alatent random variable. In this way, we can greatly enhance the representativepower of the generative model of FVC. To implement our idea, we designed twoparticular generative models with such a compositional mechanism.
arxiv-15600-70 | Studying Very Low Resolution Recognition Using Deep Networks | http://arxiv.org/abs/1601.04153 | author:Zhangyang Wang, Shiyu Chang, Yingzhen Yang, Ding Liu, Thomas S. Huang category:cs.CV cs.AI cs.LG published:2016-01-16 summary:Visual recognition research often assumes a sufficient resolution of theregion of interest (ROI). That is usually violated in practice, inspiring us toexplore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROIin a VLRR problem can be smaller than $16 \times 16$ pixels, and is challengingto be recognized even by human experts. We attempt to solve the VLRR problemusing deep learning methods. Taking advantage of techniques primarily in superresolution, domain adaptation and robust regression, we formulate a dedicateddeep learning method and demonstrate how these techniques are incorporated stepby step. Any extra complexity, when introduced, is fully justified by bothanalysis and simulation results. The resulting \textit{Robust Partially CoupledNetworks} achieves feature enhancement and recognition simultaneously. Itallows for both the flexibility to combat the LR-HR domain mismatch, and therobustness to outliers. Finally, the effectiveness of the proposed models isevaluated on three different VLRR tasks, including face identification, digitrecognition and font recognition, all of which obtain very impressiveperformances.
arxiv-15600-71 | Tightening the Sample Complexity of Empirical Risk Minimization via Preconditioned Stability | http://arxiv.org/abs/1601.04011 | author:Alon Gonen, Shai Shalev-Shwartz category:cs.LG published:2016-01-15 summary:We tighten the sample complexity of empirical risk minimization (ERM)associated with a class of generalized linear models that include linear andlogistic regression. In particular, we conclude that ERM attains the optimalsample complexity for linear regression. Our analysis relies on a new notion ofstability, called preconditioned stability, which may be of independentinterest.
arxiv-15600-72 | Towards Turkish ASR: Anatomy of a rule-based Turkish g2p | http://arxiv.org/abs/1601.03783 | author:Duygu Altinok category:cs.CL published:2016-01-15 summary:This paper describes the architecture and implementation of a rule-basedgrapheme to phoneme converter for Turkish. The system accepts surface form asinput, outputs SAMPA mapping of the all parallel pronounciations according tothe morphological analysis together with stress positions. The system has beenimplemented in Python
arxiv-15600-73 | Learning Binary Features Online from Motion Dynamics for Incremental Loop-Closure Detection and Place Recognition | http://arxiv.org/abs/1601.03821 | author:Guangcong Zhang, Mason J. Lilly, Patricio A. Vela category:cs.CV published:2016-01-15 summary:This paper proposes a simple yet effective approach to learn visual featuresonline for improving loop-closure detection and place recognition, based onbag-of-words frameworks. The approach learns a codeword in bag-of-words modelfrom a pair of matched features from two consecutive frames, such that thecodeword has temporally-derived perspective invariance to camera motion. Thelearning algorithm is efficient: the binary descriptor is generated from themean image patch, and the mask is learned based on discriminative projection byminimizing the intra-class distances among the learned feature and the twooriginal features. A codeword for bag-of-words models is generated by packagingthe learned descriptor and mask, with a masked Hamming distance defined tomeasure the distance between two codewords. The geometric properties of thelearned codewords are then mathematically justified. In addition, hypothesisconstraints are imposed through temporal consistency in matched codewords,which improves precision. The approach, integrated in an incrementalbag-of-words system, is validated on multiple benchmark data sets and comparedto state-of-the-art methods. Experiments demonstrate improved precision/recalloutperforming state of the art with little loss in runtime.
arxiv-15600-74 | Modification of Question Writing Style Influences Content Popularity in a Social Q&A System | http://arxiv.org/abs/1601.04075 | author:Igor A. Podgorny category:cs.IR cs.CL cs.SI published:2016-01-15 summary:TurboTax AnswerXchange is a social Q&A system supporting users working onfederal and state tax returns. Using 2015 data, we demonstrate that contentpopularity (or number of views per AnswerXchange question) can be predictedwith reasonable accuracy based on attributes of the question alone. We alsoemploy probabilistic topic analysis and uplift modeling to identify questionfeatures with the highest impact on popularity. We demonstrate that contentpopularity is driven by behavioral attributes of AnswerXchange users anddepends on complex interactions between search ranking algorithms,psycholinguistic factors and question writing style. Our findings provide arationale for employing popularity predictions to guide the users intoformulating better questions and editing the existing ones. For example,starting question title with a question word or adding details to the questionincrease number of views per question. Similar approach can be applied topromoting AnswerXchange content indexed by Google to drive organic traffic toTurboTax.
arxiv-15600-75 | Faster Asynchronous SGD | http://arxiv.org/abs/1601.04033 | author:Augustus Odena category:stat.ML cs.LG published:2016-01-15 summary:Asynchronous distributed stochastic gradient descent methods have troubleconverging because of stale gradients. A gradient update sent to a parameterserver by a client is stale if the parameters used to calculate that gradienthave since been updated on the server. Approaches have been proposed tocircumvent this problem that quantify staleness in terms of the number ofelapsed updates. In this work, we propose a novel method that quantifiesstaleness in terms of moving averages of gradient statistics. We show that thismethod outperforms previous methods with respect to convergence speed andscalability to many clients. We also discuss how an extension to this methodcan be used to dramatically reduce bandwidth costs in a distributed trainingcontext. In particular, our method allows reduction of total bandwidth usage bya factor of 5 with little impact on cost convergence. We also describe (andlink to) a software library that we have used to simulate these algorithmsdeterministically on a single machine.
arxiv-15600-76 | Detecting and Extracting Events from Text Documents | http://arxiv.org/abs/1601.04012 | author:Jugal Kalita category:cs.CL published:2016-01-15 summary:Events of various kinds are mentioned and discussed in text documents,whether they are books, news articles, blogs or microblog feeds. The paperstarts by giving an overview of how events are treated in linguistics andphilosophy. We follow this discussion by surveying how events and associatedinformation are handled in computationally. In particular, we look at howtextual documents can be mined to extract events and ancillary information.These days, it is mostly through the application of various machine learningtechniques. We also discuss applications of event detection and extractionsystems, particularly in summarization, in the medical domain and in thecontext of Twitter posts. We end the paper with a discussion of challenges andfuture directions.
arxiv-15600-77 | ActiveClean: Interactive Data Cleaning While Learning Convex Loss Models | http://arxiv.org/abs/1601.03797 | author:Sanjay Krishnan, Jiannan Wang, Eugene Wu, Michael J. Franklin, Ken Goldberg category:cs.DB cs.LG published:2016-01-15 summary:Data cleaning is often an important step to ensure that predictive models,such as regression and classification, are not affected by systematic errorssuch as inconsistent, out-of-date, or outlier data. Identifying dirty data isoften a manual and iterative process, and can be challenging on large datasets.However, many data cleaning workflows can introduce subtle biases into thetraining processes due to violation of independence assumptions. We proposeActiveClean, a progressive cleaning approach where the model is updatedincrementally instead of re-training and can guarantee accuracy on partiallycleaned data. ActiveClean supports a popular class of models called convex lossmodels (e.g., linear regression and SVMs). ActiveClean also leverages thestructure of a user's model to prioritize cleaning those records likely toaffect the results. We evaluate ActiveClean on five real-world datasets UCIAdult, UCI EEG, MNIST, Dollars For Docs, and WorldBank with both real andsynthetic errors. Our results suggest that our proposed optimizations canimprove model accuracy by up-to 2.5x for the same amount of data cleaned.Furthermore for a fixed cleaning budget and on all real dirty datasets,ActiveClean returns more accurate models than uniform sampling and ActiveLearning.
arxiv-15600-78 | On the consistency of inversion-free parameter estimation for Gaussian random fields | http://arxiv.org/abs/1601.03822 | author:Hossein Keshavarz, Clayton Scott, XuanLong Nguyen category:math.ST cs.LG stat.ML stat.TH published:2016-01-15 summary:Gaussian random fields are a powerful tool for modeling environmentalprocesses. For high dimensional samples, classical approaches for estimatingthe covariance parameters require highly challenging and massive computations,such as the evaluation of the Cholesky factorization or solving linear systems.Recently, Anitescu, Chen and Stein \cite{M.Anitescu} proposed a fast andscalable algorithm which does not need such burdensome computations. The mainfocus of this article is to study the asymptotic behavior of the algorithm ofAnitescu et al. (ACS) for regular and irregular grids in the increasing domainsetting. Consistency, minimax optimality and asymptotic normality of thisalgorithm are proved under mild differentiability conditions on the covariancefunction. Despite the fact that ACS's method entails a non-concavemaximization, our results hold for any stationary point of the objectivefunction. A numerical study is presented to evaluate the efficiency of thisalgorithm for large data sets.
arxiv-15600-79 | Real-Time Association Mining in Large Social Networks | http://arxiv.org/abs/1601.03958 | author:Benjamin Paul Chamberlain, Josh Levy-Kramer, Clive Humby, Marc Peter Deisenroth category:cs.SI stat.ML published:2016-01-15 summary:There is a growing realisation that to combat the waning effectiveness oftraditional marketing, social media platform owners need to find new ways tomonetise their data. Social media data contains rich information describing howreal world entities relate to each other. Understanding the allegiances,communities and structure of key entities is of vital importance for decisionsupport in a swathe of industries that have hitherto relied on expensive, smallscale survey data. In this paper, we present a real-time method to query andvisualise regions of networks that are closely related to a set of inputvertices. The input vertices can define an industry, political party, sportetc. The key idea is that in large digital social networks measuring similarityvia direct connections between nodes is not robust, but that robustsimilarities between nodes can be attained through the similarity of theirneighbourhood graphs. We are able to achieve real-time performance bycompressing the neighbourhood graphs using minhash signatures and facilitaterapid queries through Locality Sensitive Hashing. These techniques reduce querytimes from hours using industrial desktop machines to milliseconds on standardlaptops. Our method allows analysts to interactively explore stronglyassociated regions of large networks in real time. Our work has been deployedin software that is actively used by analysts to understand social networkstructure.
arxiv-15600-80 | A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits | http://arxiv.org/abs/1601.03855 | author:Pratik Gajane, Tanguy Urvoy, Fabrice Clérot category:cs.LG published:2016-01-15 summary:We study the K-armed dueling bandit problem which is a variation of theclassical Multi-Armed Bandit (MAB) problem in which the learner receives onlyrelative feedback about the selected pairs of arms. We propose a new algorithmcalled Relative Exponential-weight algorithm for Exploration and Exploitation(REX3) to handle the adversarial utility-based formulation of this problem.This algorithm is a non-trivial extension of the Exponential-weight algorithmfor Exploration and Exploitation (EXP3) algorithm. We prove a finite timeexpected regret upper bound of order O(sqrt(K ln(K)T)) for this algorithm and ageneral lower bound of order omega(sqrt(KT)). At the end, we provideexperimental results using real data from information retrieval applications.
arxiv-15600-81 | Multimodal Pivots for Image Caption Translation | http://arxiv.org/abs/1601.03916 | author:Julian Hitschler, Shigehiko Schamoni, Stefan Riezler category:cs.CL published:2016-01-15 summary:We present an approach to improve statistical machine translation of imagedescriptions by multimodal pivots defined in visual space. The key idea is todisambiguate and ground the translation of an image desription by involving theimage as a pivot into the translation process. We compute image similarity by aconvolutional neural network, and use descriptions of most similar pivot imagesfor crosslingual reranking of translation outputs. Our approach does not dependon the availability of large amounts of in-domain parallel data and achievesimprovements of 1 BLEU point over strong baselines.
arxiv-15600-82 | Stereo Matching by Joint Energy Minimization | http://arxiv.org/abs/1601.03890 | author:Hongyang Xue, Deng Cai category:cs.CV published:2016-01-15 summary:In [18], Mozerov et al. propose to perform stereo matching as a two-stepenergy minimization problem. For the first step they solve a fully connectedMRF model. And in the next step the marginal output is employed as the unarycost for a locally connected MRF model. In this paper we intend to combine the two steps of energy minimization inorder to improve stereo matching results. We observe that the fully connectedMRF leads to smoother disparity maps, while the locally connected MRF achievessuperior results in fine-structured regions. Thus we propose to jointly solvethe fully connected and locally connected models, taking both their advantagesinto account. The joint model is solved by mean field approximations. Whileremaining efficient, our joint model outperforms the two-step energyminimization approach in both time and estimation error on the Middleburystereo benchmark v3.
arxiv-15600-83 | Improved graph-based SFA: Information preservation complements the slowness principle | http://arxiv.org/abs/1601.03945 | author:Alberto N. Escalante-B., Laurenz Wiskott category:cs.CV cs.LG stat.ML published:2016-01-15 summary:Slow feature analysis (SFA) is an unsupervised-learning algorithm thatextracts slowly varying features from a multi-dimensional time series. Asupervised extension to SFA for classification and regression is graph-basedSFA (GSFA). GSFA is based on the preservation of similarities, which arespecified by a graph structure derived from the labels. It has been shown thathierarchical GSFA (HGSFA) allows learning from images and otherhigh-dimensional data. The feature space spanned by HGSFA is complex due to thecomposition of the nonlinearities of the nodes in the network. However, we showthat the network discards useful information prematurely before it reacheshigher nodes, resulting in suboptimal global slowness and an under-exploitedfeature space. To counteract these problems, we propose an extension called hierarchicalinformation-preserving GSFA (HiGSFA), where information preservationcomplements the slowness-maximization goal. We build a 10-layer HiGSFA networkto estimate human age from facial photographs of the MORPH-II database,achieving a mean absolute error of 3.50 years, improving the state-of-the-artperformance. HiGSFA and HGSFA support multiple-labels and offer a rich featurespace, feed-forward training, and linear complexity in the number of samplesand dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of featureslowness, estimation accuracy and input reconstruction, giving rise to apromising hierarchical supervised-learning approach.
arxiv-15600-84 | Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures | http://arxiv.org/abs/1601.03896 | author:Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis, Frank Keller, Adrian Muscat, Barbara Plank category:cs.CL cs.CV published:2016-01-15 summary:Automatic description generation from natural images is a challenging problemthat has recently received a large amount of interest from the computer visionand natural language processing communities. In this survey, we classify theexisting approaches based on how they conceptualize this problem, viz., modelsthat cast description as either generation problem or as a retrieval problemover a visual or multimodal representational space. We provide a detailedreview of existing models, highlighting their advantages and disadvantages.Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality ofmachine-generated image descriptions. Finally we extrapolate future directionsin the area of automatic image description generation.
arxiv-15600-85 | Matrix Neural Networks | http://arxiv.org/abs/1601.03805 | author:Junbin Gao, Yi Guo, Zhiyong Wang category:cs.LG published:2016-01-15 summary:Traditional neural networks assume vectorial inputs as the network isarranged as layers of single line of computing units called neurons. Thisspecial structure requires the non-vectorial inputs such as matrices to beconverted into vectors. This process can be problematic. Firstly, the spatialinformation among elements of the data may be lost during vectorisation.Secondly, the solution space becomes very large which demands very specialtreatments to the network parameters and high computational cost. To addressthese issues, we propose matrix neural networks (MatNet), which takes matricesdirectly as inputs. Each neuron senses summarised information through bilinearmapping from lower layer units in exactly the same way as the classic feedforward neural networks. Under this structure, back prorogation and gradientdescent combination can be utilised to obtain network parameters efficiently.Furthermore, it can be conveniently extended for multimodal inputs. We applyMatNet to MNIST handwritten digits classification and image super resolutiontasks to show its effectiveness. Without too much tweaking MatNet achievescomparable performance as the state-of-the-art methods in both tasks withconsiderably reduced complexity.
arxiv-15600-86 | Dynamic Privacy For Distributed Machine Learning Over Network | http://arxiv.org/abs/1601.03466 | author:Tao Zhang, Quanyan Zhu category:cs.LG published:2016-01-14 summary:Privacy-preserving distributed machine learning becomes increasinglyimportant due to the recent rapid growth of data. This paper focuses on a classof regularized empirical risk minimization (ERM) machine learning problems, anddevelops two methods to provide differential privacy to distributed learningalgorithms over a network. We first decentralize the learning algorithm usingthe alternating direction method of multipliers (ADMM), and propose the methodsof dual variable perturbation and primal variable perturbation to providedynamic differential privacy. The two mechanisms lead to algorithms that canprovide privacy guarantees under mild conditions of the convexity anddifferentiability of the loss function and the regularizer. We study theperformance of the algorithms, and show that the dual variable perturbationoutperforms its primal counterpart. To design an optimal privacy mechanisms, weanalyze the fundamental tradeoff between privacy and accuracy, and provideguidelines to choose privacy parameters. Numerical experiments using customerinformation database are performed to corroborate the results on privacy andutility tradeoffs and design.
arxiv-15600-87 | Quantification of Ultrasonic Texture heterogeneity via Volumetric Stochastic Modeling for Tissue Characterization | http://arxiv.org/abs/1601.03531 | author:O. S. Al-Kadi, Daniel Y. F. Chung, Robert C. Carlisle, Constantin C. Coussios, J. Alison Noble category:cs.CV published:2016-01-14 summary:Intensity variations in image texture can provide powerful quantitativeinformation about physical properties of biological tissue. However, tissuepatterns can vary according to the utilized imaging system and areintrinsically correlated to the scale of analysis. In the case of ultrasound,the Nakagami distribution is a general model of the ultrasonic backscatteringenvelope under various scattering conditions and densities where it can beemployed for characterizing image texture, but the subtle intra-heterogeneitieswithin a given mass are difficult to capture via this model as it works at asingle spatial scale. This paper proposes a locally adaptive 3Dmulti-resolution Nakagami-based fractal feature descriptor that extendsNakagami-based texture analysis to accommodate subtle speckle spatial frequencytissue intensity variability in volumetric scans. Local textural fractaldescriptors - which are invariant to affine intensity changes - are extractedfrom volumetric patches at different spatial resolutions from voxellattice-based generated shape and scale Nakagami parameters. Using ultrasoundradio-frequency datasets we found that after applying an adaptive fractaldecomposition label transfer approach on top of the generated Nakagami voxels,tissue characterization results were superior to the state of art. Experimentalresults on real 3D ultrasonic pre-clinical and clinical datasets suggest thatdescribing tumor intra-heterogeneity via this descriptor may facilitateimproved prediction of therapy response and disease characterization.
arxiv-15600-88 | Trust from the past: Bayesian Personalized Ranking based Link Prediction in Knowledge Graphs | http://arxiv.org/abs/1601.03778 | author:Baichuan Zhang, Sutanay Choudhury, Mohammad Al Hasan, Xia Ning, Khushbu Agarwal, Sumit Purohit, Paola Pesntez Cabrera category:cs.LG cs.AI cs.IR published:2016-01-14 summary:Link prediction, or predicting the likelihood of a link in a knowledge graphbased on its existing state is a key research task. It differs from atraditional link prediction task in that the links in a knowledge graph arecategorized into different predicates and the link prediction performance ofdifferent predicates in a knowledge graph generally varies widely. In thiswork, we propose a latent feature embedding based link prediction model whichconsiders the prediction task for each predicate disjointly. To learn the modelparameters it utilizes a Bayesian personalized ranking based optimizationtechnique. Experimental results on large-scale knowledge bases such as YAGO2show that our link prediction approach achieves substantially higherperformance than several state-of-art approaches. We also show that for a givenpredicate the topological properties of the knowledge graph induced by thegiven predicate edges are key indicators of the link prediction performance ofthat predicate in the knowledge graph.
arxiv-15600-89 | Smoothing parameter estimation framework for IBM word alignment models | http://arxiv.org/abs/1601.03650 | author:Vuong Van Bui, Cuong Anh Le category:cs.CL published:2016-01-14 summary:IBM models are very important word alignment models in Machine Translation.Following the Maximum Likelihood Estimation principle to estimate theirparameters, the models will easily overfit the training data when the data aresparse. While smoothing is a very popular solution in Language Model, therestill lacks studies on smoothing for word alignment. In this paper, we proposea framework which generalizes the notable work Moore [2004] of applyingadditive smoothing to word alignment models. The framework allows developers tocustomize the smoothing amount for each pair of word. The added amount will bescaled appropriately by a common factor which reflects how much the frameworktrusts the adding strategy according to the performance on data. We alsocarefully examine various performance criteria and propose a smoothened versionof the error count, which generally gives the best result.
arxiv-15600-90 | Optimal Supervised Learning in Spiking Neural Networks for Precise Temporal Encoding | http://arxiv.org/abs/1601.03649 | author:Brian Gardner, André Grüning category:cs.NE q-bio.NC published:2016-01-14 summary:Precise spike timing as a means to encode information in neural networks isbiologically supported, and is advantageous over frequency-based codes byprocessing input features on a much shorter time-scale. For these reasons, muchrecent attention has been focused on the development of supervised learningrules for spiking neural networks that utilise a temporal coding scheme.However, despite significant progress in this area, there still lack rules thatare theoretically justified, and yet can be considered biologically relevant.Here we examine the general conditions under which optimal synaptic plasticitytakes place to support the supervised learning of a precise temporal code. Aspart of our analysis we introduce two analytically derived learning rules, oneof which relies on an instantaneous error signal to optimise synaptic weightsin a network (INST rule), and the other one relying on a filtered error signalto minimise the variance of synaptic weight modifications (FILT rule). We testthe optimality of the solutions provided by each rule with respect to theirtemporal encoding precision, and then measure the maximum number of inputpatterns they can learn to memorise using the precise timings of individualspikes. Our results demonstrate the optimality of the FILT rule in most cases,underpinned by the rule's error-filtering mechanism which provides smoothconvergence during learning. We also find the FILT rule to be most efficient atperforming input pattern memorisations, and most noticeably when patterns areidentified using spikes with sub-millisecond temporal precision. In comparisonwith existing work, we determine the performance of the FILT rule to beconsistent with that of the highly efficient E-learning Chronotron rule, butwith the distinct advantage that our FILT rule is also implementable as anonline method for increased biological realism.
arxiv-15600-91 | Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation | http://arxiv.org/abs/1601.03651 | author:Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen, Yangyang Lu, Zhi Jin category:cs.CL cs.LG published:2016-01-14 summary:Nowadays, neural networks play an important role in the task of relationclassification. By designing different neural architectures, researchers haveimproved the performance to a large extent, compared with traditional methods.However, existing neural networks for relation classification are usually ofshallow architectures (e.g., one-layer convolution neural networks or recurrentnetworks). They may fail to explore the potential representation space indifferent abstraction levels. In this paper, we propose deep recurrent neuralnetworks (DRNNs) to tackle this challenge. Further, we propose a dataaugmentation method by leveraging the directionality of relations. We evaluateour DRNNs on the SemEval-2010 Task 8, and achieve an $F_1$-score of 85.81%,outperforming state-of-the-art recorded results.
arxiv-15600-92 | Dynamic Concept Composition for Zero-Example Event Detection | http://arxiv.org/abs/1601.03679 | author:Xiaojun Chang, Yi Yang, Guodong Long, Chengqi Zhang, Alexander G. Hauptmann category:cs.CV published:2016-01-14 summary:In this paper, we focus on automatically detecting events in unconstrainedvideos without the use of any visual training exemplars. In principle,zero-shot learning makes it possible to train an event detection model based onthe assumption that events (e.g. \emph{birthday party}) can be described bymultiple mid-level semantic concepts (e.g. "blowing candle", "birthday cake").Towards this goal, we first pre-train a bundle of concept classifiers usingdata from other sources. Then we evaluate the semantic correlation of eachconcept \wrt the event of interest and pick up the relevant conceptclassifiers, which are applied on all test videos to get multiple predictionscore vectors. While most existing systems combine the predictions of theconcept classifiers with fixed weights, we propose to learn the optimal weightsof the concept classifiers for each testing video by exploring a set of onlineavailable videos with free-form text descriptions of their content. To validatethe effectiveness of the proposed approach, we have conducted extensiveexperiments on the latest TRECVID MEDTest 2014, MEDTest 2013 and CCV dataset.The experimental results confirm the superiority of the proposed approach.
arxiv-15600-93 | Dual-tree $k$-means with bounded iteration runtime | http://arxiv.org/abs/1601.03754 | author:Ryan R. Curtin category:cs.DS cs.LG published:2016-01-14 summary:k-means is a widely used clustering algorithm, but for $k$ clusters and adataset size of $N$, each iteration of Lloyd's algorithm costs $O(kN)$ time.Although there are existing techniques to accelerate single Lloyd iterations,none of these are tailored to the case of large $k$, which is increasinglycommon as dataset sizes grow. We propose a dual-tree algorithm that gives theexact same results as standard $k$-means; when using cover trees, we useadaptive analysis techniques to, under some assumptions, bound thesingle-iteration runtime of the algorithm as $O(N + k log k)$. To our knowledgethese are the first sub-$O(kN)$ bounds for exact Lloyd iterations. We then showthat this theoretically favorable algorithm performs competitively in practice,especially for large $N$ and $k$ in low dimensions. Further, the algorithm istree-independent, so any type of tree may be used.
arxiv-15600-94 | Linear Algebraic Structure of Word Senses, with Applications to Polysemy | http://arxiv.org/abs/1601.03764 | author:Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski category:cs.CL cs.LG stat.ML published:2016-01-14 summary:Word embeddings are ubiquitous in NLP and information retrieval, but it'sunclear what they represent when the word is polysemous, i.e., has multiplesenses. Here it is shown that multiple word senses reside in linearsuperposition within the word embedding and can be recovered by simple sparsecoding. The success of the method ---which applies to several embedding methodsincluding word2vec--- is mathematically explained using the random walk ondiscourses model (Arora et al., 2015). A novel aspect of our technique is thateach word sense is also accompanied by one of about 2000 "discourse atoms" thatgive a succinct description of which other words co-occur with that word sense.Discourse atoms seem of independent interest, and make the method potentiallymore useful than the traditional clustering-based approaches to polysemy.
arxiv-15600-95 | Generation of a Supervised Classification Algorithm for Time-Series Variable Stars with an Application to the LINEAR Dataset | http://arxiv.org/abs/1601.03769 | author:Kyle B Johnston, Hakeem M Oluseyi category:astro-ph.IM cs.LG published:2016-01-14 summary:With the advent of digital astronomy, new benefits and new problems have beenpresented to the modern day astronomer. While data can be captured in a moreefficient and accurate manor using digital means, the efficiency of dataretrieval has led to an overload of scientific data for processing and storage.This paper will focus on the construction and application of a supervisedpattern classification algorithm for the identification of variable stars.Given the reduction of a survey of stars into a standard feature space, theproblem of using prior patterns to identify new observed patterns can bereduced to time tested classification methodologies and algorithms. Suchsupervised methods, so called because the user trains the algorithms prior toapplication using patterns with known classes or labels, provide a means toprobabilistically determine the estimated class type of new observations. Thispaper will demonstrate the construction and application of a supervisedclassification algorithm on variable star data. The classifier is applied to aset of 192,744 LINEAR data points. Of the original samples, 34,451 unique starswere classified with high confidence (high level of probability of being thetrue class).
arxiv-15600-96 | Predicting the Effectiveness of Self-Training: Application to Sentiment Classification | http://arxiv.org/abs/1601.03288 | author:Vincent Van Asch, Walter Daelemans category:cs.CL published:2016-01-13 summary:The goal of this paper is to investigate the connection between theperformance gain that can be obtained by selftraining and the similaritybetween the corpora used in this approach. Self-training is a semi-supervisedtechnique designed to increase the performance of machine learning algorithmsby automatically classifying instances of a task and adding these as additionaltraining material to the same classifier. In the context of language processingtasks, this training material is mostly an (annotated) corpus. Unfortunatelyself-training does not always lead to a performance increase and whether itwill is largely unpredictable. We show that the similarity between corpora canbe used to identify those setups for which self-training can be beneficial. Weconsider this research as a step in the process of developing a classifier thatis able to adapt itself to each new test corpus that it is presented with.
arxiv-15600-97 | Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model | http://arxiv.org/abs/1601.03317 | author:Shi Feng, Shujie Liu, Mu Li, Ming Zhou category:cs.CL published:2016-01-13 summary:Neural machine translation has shown very promising results lately. Most NMTmodels follow the encoder-decoder framework. To make encoder-decoder modelsmore flexible, attention mechanism was introduced to machine translation andalso other tasks like speech recognition and image captioning. We observe thatthe quality of translation by attention-based encoder-decoder can besignificantly damaged when the alignment is incorrect. We attribute theseproblems to the lack of distortion and fertility models. Aiming to resolvethese problems, we propose new variations of attention-based encoder-decoderand compare them with other models on machine translation. Our proposed methodachieved an improvement of 2 BLEU points over the original attention-basedencoder-decoder.
arxiv-15600-98 | Political Speech Generation | http://arxiv.org/abs/1601.03313 | author:Valentin Kassarnig category:cs.CL published:2016-01-13 summary:In this report we present a system that can generate political speeches for adesired political party. Furthermore, the system allows to specify whether aspeech should hold a supportive or opposing opinion. The system relies on acombination of several state-of-the-art NLP methods which are discussed in thisreport. These include n-grams, Justeson & Katz POS tag filter, recurrent neuralnetworks, and latent Dirichlet allocation. Sequences of words are generatedbased on probabilities obtained from two underlying models: A language modeltakes care of the grammatical correctness while a topic model aims for textualconsistency. Both models were trained on the Convote dataset which containstranscripts from US congressional floor debates. Furthermore, we present amanual and an automated approach to evaluate the quality of generated speeches.In an experimental evaluation generated speeches have shown very high qualityin terms of grammatical correctness and sentence transitions.
arxiv-15600-99 | Document image classification, with a specific view on applications of patent images | http://arxiv.org/abs/1601.03295 | author:Gabriela Csurka category:cs.CV published:2016-01-13 summary:The main focus of this paper is document image classification and retrieval,where we analyze and compare different parameters for the RunLeght Histogram(RL) and Fisher Vector (FV) based image representations. We do an exhaustiveexperimental study using different document image datasets, including the MARGbenchmarks, two datasets built on customer data and the images from the PatentImage Classification task of the Clef-IP 2011. The aim of the study is to giveguidelines on how to best choose the parameters such that the same featuresperform well on different tasks. As an example of such need, we describe theImage-based Patent Retrieval task's of Clef-IP 2011, where we used the sameimage representation to predict the image type and retrieve relevant patents.
arxiv-15600-100 | Digital Image Forensics vs. Image Composition: An Indirect Arms Race | http://arxiv.org/abs/1601.03239 | author:Victor Schetinger, Massimo Iuliani, Alessandro Piva, Manuel M. Oliveira category:cs.CV cs.MM published:2016-01-13 summary:The field of image composition is constantly trying to improve the ways inwhich an image can be altered and enhanced. While this is usually done in thename of aesthetics and practicality, it also provides tools that can be used tomaliciously alter images. In this sense, the field of digital image forensicshas to be prepared to deal with the influx of new technology, in a constantarms-race. In this paper, the current state of this arms-race is analyzed,surveying the state-of-the-art and providing means to compare both sides. Anovel scale to classify image forensics assessments is proposed, andexperiments are performed to test composition techniques in regards todifferent forensics traces. We show that even though research in forensicsseems unaware of the advanced forms of image composition, it possesses thebasic tools to detect it.
arxiv-15600-101 | The scarcity of crossing dependencies: a direct outcome of a specific constraint? | http://arxiv.org/abs/1601.03210 | author:Carlos Gómez-Rodríguez, Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph published:2016-01-13 summary:Crossing syntactic dependencies have been observed to be infrequent innatural language, to the point that some syntactic theories and formalismsdisregard them entirely. This leads to the question of whether the scarcity ofcrossings in languages arises from an independent and specific constraint oncrossings. We provide statistical evidence suggesting that this is not thecase, as the proportion of dependency crossings in a wide range of naturallanguage treebanks can be accurately estimated by a simple predictor based onthe local probability that two dependencies cross given their lengths. Therelative error of this predictor never exceeds 5% on average, whereas abaseline predictor assuming a random ordering of the words of a sentence incursa relative error that is at least 6 times greater. Our results suggest that thelow frequency of crossings in natural languages is neither originated by hiddenknowledge of language nor by the undesirability of crossings per se, but as amere side effect of the principle of dependency length minimization.
arxiv-15600-102 | Localized Dictionary design for Geometrically Robust Sonar ATR | http://arxiv.org/abs/1601.03323 | author:John McKay, Vishal Monga, Raghu Raj category:cs.CV published:2016-01-13 summary:Advancements in Sonar image capture have opened the door to powerfulclassification schemes for automatic target recognition (ATR. Recent work hasparticularly seen the application of sparse reconstruction-based classification(SRC) to sonar ATR, which provides compelling accuracy rates even in thepresence of noise and blur. Existing sparsity based sonar ATR techniqueshowever assume that the test images exhibit geometric pose that is consistentwith respect to the training set. This work addresses the outstanding openchallenge of handling inconsistently posed test sonar images relative totraining. We develop a new localized block-based dictionary design that canenable geometric, i.e. pose robustness. Further, a dictionary learning methodis incorporated to increase performance and efficiency. The proposed SRC withLocalized Pose Management (LPM), is shown to outperform the state of the artSIFT feature and SVM approach, due to its power to discern background clutterin Sonar images.
arxiv-15600-103 | A Score-level Fusion Method for Eye Movement Biometrics | http://arxiv.org/abs/1601.03333 | author:Anjith George, Aurobinda Routray category:cs.CV published:2016-01-13 summary:This paper proposes a novel framework for the use of eye movement patternsfor biometric applications. Eye movements contain abundant information aboutcognitive brain functions, neural pathways, etc. In the proposed method, eyemovement data is classified into fixations and saccades. Features extractedfrom fixations and saccades are used by a Gaussian Radial Basis FunctionNetwork (GRBFN) based method for biometric authentication. A score fusionapproach is adopted to classify the data in the output layer. In the evaluationstage, the algorithm has been tested using two types of stimuli: random dotfollowing on a screen and text reading. The results indicate the strength ofeye movement pattern as a biometric modality. The algorithm has been evaluatedon BioEye 2015 database and found to outperform all the other methods. Eyemovements are generated by a complex oculomotor plant which is very hard tospoof by mechanical replicas. Use of eye movement dynamics along with irisrecognition technology may lead to a robust counterfeit-resistant personidentification system.
arxiv-15600-104 | Enhancing Energy Minimization Framework for Scene Text Recognition with Top-Down Cues | http://arxiv.org/abs/1601.03128 | author:Anand Mishra, Karteek Alahari, C. V. Jawahar category:cs.CV published:2016-01-13 summary:Recognizing scene text is a challenging problem, even more so than therecognition of scanned documents. This problem has gained significant attentionfrom the computer vision community in recent years, and several methods basedon energy minimization frameworks and deep learning approaches have beenproposed. In this work, we focus on the energy minimization framework andpropose a model that exploits both bottom-up and top-down cues for recognizingcropped words extracted from street images. The bottom-up cues are derived fromindividual character detections from an image. We build a conditional randomfield model on these detections to jointly model the strength of the detectionsand the interactions between them. These interactions are top-down cuesobtained from a lexicon-based prior, i.e., language statistics. The optimalword represented by the text image is obtained by minimizing the energyfunction corresponding to the random field model. We evaluate our proposedalgorithm extensively on a number of cropped scene text benchmark datasets,namely Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word,and show better performance than comparable methods. We perform a rigorousanalysis of all the steps in our approach and analyze the results. We also showthat state-of-the-art convolutional neural network features can be integratedin our framework to further improve the recognition performance.
arxiv-15600-105 | Online Prediction of Dyadic Data with Heterogeneous Matrix Factorization | http://arxiv.org/abs/1601.03124 | author:Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng category:cs.LG stat.ML published:2016-01-13 summary:Dyadic Data Prediction (DDP) is an important problem in many research areas.This paper develops a novel fully Bayesian nonparametric framework whichintegrates two popular and complementary approaches, discrete mixed membershipmodeling and continuous latent factor modeling into a unified HeterogeneousMatrix Factorization~(HeMF) model, which can predict the unobserved dyadicsaccurately. The HeMF can determine the number of communities automatically andexploit the latent linear structure for each bicluster efficiently. We proposea Variational Bayesian method to estimate the parameters and missing data. Wefurther develop a novel online learning approach for Variational inference anduse it for the online learning of HeMF, which can efficiently cope with theimportant large-scale DDP problem. We evaluate the performance of our method onthe EachMoive, MovieLens and Netflix Prize collaborative filtering datasets.The experiment shows that, our model outperforms state-of-the-art methods onall benchmarks. Compared with Stochastic Gradient Method (SGD), our onlinelearning approach achieves significant improvement on the estimation accuracyand robustness.
arxiv-15600-106 | Blind Image Denoising via Dependent Dirichlet Process Tree | http://arxiv.org/abs/1601.03117 | author:Fengyuan Zhu, Guangyong Chen, Jianye Hao, Pheng-Ann Heng category:cs.CV stat.ML published:2016-01-13 summary:Most existing image denoising approaches assumed the noise to be homogeneouswhite Gaussian distributed with known intensity. However, in real noisy images,the noise models are usually unknown beforehand and can be much more complex.This paper addresses this problem and proposes a novel blind image denoisingalgorithm to recover the clean image from noisy one with the unknown noisemodel. To model the empirical noise of an image, our method introduces themixture of Gaussian distribution, which is flexible enough to approximatedifferent continuous distributions. The problem of blind image denoising isreformulated as a learning problem. The procedure is to first build a two-layerstructural model for noisy patches and consider the clean ones as latentvariable. To control the complexity of the noisy patch model, this workproposes a novel Bayesian nonparametric prior called "Dependent DirichletProcess Tree" to build the model. Then, this study derives a variationalinference algorithm to estimate model parameters and recover clean patches. Weapply our method on synthesis and real noisy images with different noisemodels. Comparing with previous approaches, ours achieves better performance.The experimental results indicate the efficiency of the proposed algorithm tocope with practical image denoising tasks.
arxiv-15600-107 | EvoGrader: an online formative assessment tool for automatically evaluating written evolutionary explanations | http://arxiv.org/abs/1601.03348 | author:Kayhan Moharreri, Minsu Ha, Ross H Nehm category:cs.CL published:2016-01-13 summary:EvoGrader is a free, online, on-demand formative assessment service designedfor use in undergraduate biology classrooms. EvoGrader's web portal is poweredby Amazon's Elastic Cloud and run with LightSIDE Lab's open-sourcemachine-learning tools. The EvoGrader web portal allows biology instructors toupload a response file (.csv) containing unlimited numbers of evolutionaryexplanations written in response to 86 different ACORNS (Assessing COntextualReasoning about Natural Selection) instrument items. The system automaticallyanalyzes the responses and provides detailed information about the scientificand naive concepts contained within each student's response, as well as overallstudent (and sample) reasoning model types. Graphs and visual models providedby EvoGrader summarize class-level responses; downloadable files of raw scores(in .csv format) are also provided for more detailed analyses. Although thecomputational machinery that EvoGrader employs is complex, using the system iseasy. Users only need to know how to use spreadsheets to organize studentresponses, upload files to the web, and use a web browser. A series ofexperiments using new samples of 2,200 written evolutionary explanationsdemonstrate that EvoGrader scores are comparable to those of trained humanraters, although EvoGrader scoring takes 99% less time and is free. EvoGraderwill be of interest to biology instructors teaching large classes who seek toemphasize scientific practices such as generating scientific explanations, andto teach crosscutting ideas such as evolution and natural selection. Thesoftware architecture of EvoGrader is described as it may serve as a templatefor developing machine-learning portals for other core concepts within biologyand across other disciplines.
arxiv-15600-108 | Multi-Atlas Segmentation with Joint Label Fusion of Osteoporotic Vertebral Compression Fractures on CT | http://arxiv.org/abs/1601.03375 | author:Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, Ronald M. Summers category:cs.CV published:2016-01-13 summary:The precise and accurate segmentation of the vertebral column is essential inthe diagnosis and treatment of various orthopedic, neurological, andoncological traumas and pathologies. Segmentation is especially challenging inthe presence of pathology such as vertebral compression fractures. In thispaper, we propose a method to produce segmentations for osteoporoticcompression fractured vertebrae by applying a multi-atlas joint label fusiontechnique for clinical CT images. A total of 170 thoracic and lumbar vertebraewere evaluated using atlases from five patients with varying degrees of spinaldegeneration. In an osteoporotic cohort of bundled atlases, registrationprovided an average Dice coefficient and mean absolute surface distance of2.7$\pm$4.5% and 0.32$\pm$0.13mm for osteoporotic vertebrae, respectively, and90.9$\pm$3.0% and 0.36$\pm$0.11mm for compression fractured vertebrae.
arxiv-15600-109 | Using Filter Banks in Convolutional Neural Networks for Texture Classification | http://arxiv.org/abs/1601.02919 | author:Vincent Andrearczyk, Paul F. Whelan category:cs.CV cs.NE published:2016-01-12 summary:Deep learning has established many new state of the art solutions in the lastdecade in areas such as object, scene and speech recognition. In particularConvolutional Neural Network (CNN) is a category of deep learning which obtainsexcellent results in object detection and recognition tasks. Its architectureis indeed well suited to object analysis by learning and classifying complex(deep) features that represent parts of an object or the object itself.However, some of its features are very similar to texture analysis methods. CNNlayers can be thought of as filter banks of complexity increasing with thedepth. Filter banks are powerful tools to extract texture features and havebeen widely used in texture analysis. In this paper we develop a simple networkarchitecture named Texture CNN (T-CNN) which explores this observation. It isbuilt on the idea that the overall shape information extracted by the fullyconnected layers of a classic CNN is of minor importance in texture analysis.Therefore, we pool an energy measure from the last convolution layer which weconnect to a fully connected layer. We show that our approach can improve theperformance of a network while greatly reducing the memory usage andcomputation.
arxiv-15600-110 | Image Annotation combining Subspace Clustering , Matrix Completion and Inhomogeneous Errors | http://arxiv.org/abs/1601.03055 | author:Yuqing Hou category:cs.CV published:2016-01-12 summary:Image annotation methods have greatly facilitated image managementapplications. However, existing methods are still suffering from thedegradation of the missing and noisy tags provided by users. In this study, wepropose an image annotation method which performs tag completion and refinementsequentially. We assume that images are sampled from a union of subspaces.Images sampled from the same subspace, as well as their corresponding tags,should form a compatible image-tag sub-matrix. Thus we segment the subspaces bythe Sparse Subspace Clustering (SSC) method and share tags in each subspace. Anovel matrix completion model is designed for tag refinement, taking visual-tagcorrelation, semantic-tag correlation and the inhomogeneous errors property,which is explored in this field for the first time, into consideration. Weexploit CNN features to improve the model. The proposed algorithm outperformsstate-of-the-art approaches when handling missing and noisy tags on multiplebenchmark datasets.
arxiv-15600-111 | Creativity in Machine Learning | http://arxiv.org/abs/1601.03642 | author:Martin Thoma category:cs.CV cs.LG published:2016-01-12 summary:Recent machine learning techniques can be modified to produce creativeresults. Those results did not exist before; it is not a trivial combination ofthe data which was fed into the machine learning system. The obtained resultscome in multiple forms: As images, as text and as audio. This paper gives a high level overview of how they are created and gives someexamples. It is meant to be a summary of the current work and give people whoare new to machine learning some starting points.
arxiv-15600-112 | A metric for sets of trajectories that is practical and mathematically consistent | http://arxiv.org/abs/1601.03094 | author:José Bento category:cs.CV cs.SY math.OC published:2016-01-12 summary:Metrics on the space of sets of trajectories are important for scientists inthe field of computer vision, machine learning, robotics and general artificialintelligence. Yet existing notions of closeness are either mathematicallyinconsistent or of limited practical use. In this paper we outline thelimitations in the existing mathematically-consistent metrics, which are basedon Schuhmacher et al. 2008, and the inconsistencies in the heuristic notions ofcloseness used in practice, whose main ideas are common to the CLEAR MOTmeasures widely used in computer vision. In two steps we then propose a newintuitive metric between sets of trajectories and address these problems. Firstwe explain a natural solution that leads to a metric that is hard to compute.Then we modify this formulation to obtain a metric that is easy to compute andkeeps all the good properties of the previous metric. In particular, our notionof closeness is the first that has the following three properties: it can bequickly computed, it incorporates confusion of trajectories' identity in anoptimal way and it is a metric in the mathematical sense.
arxiv-15600-113 | Infomax strategies for an optimal balance between exploration and exploitation | http://arxiv.org/abs/1601.03073 | author:Gautam Reddy, Antonio Celani, Massimo Vergassola category:cs.LG cs.IT math.IT q-bio.PE stat.ML published:2016-01-12 summary:Proper balance between exploitation and exploration is what makes gooddecisions, which achieve high rewards like payoff or evolutionary fitness. TheInfomax principle postulates that maximization of information directs thefunction of diverse systems, from living systems to artificial neural networks.While specific applications are successful, the validity of information as aproxy for reward remains unclear. Here, we consider the multi-armed banditdecision problem, which features arms (slot-machines) of unknown probabilitiesof success and a player trying to maximize cumulative payoff by choosing thesequence of arms to play. We show that an Infomax strategy (Info-p) whichoptimally gathers information on the highest mean reward among the armssaturates known optimal bounds and compares favorably to existing policies. Thehighest mean reward considered by Info-p is not the quantity actually neededfor the choice of the arm to play, yet it allows for optimal tradeoffs betweenexploration and exploitation.
arxiv-15600-114 | Deep Neural Networks predict Hierarchical Spatio-temporal Cortical Dynamics of Human Visual Object Recognition | http://arxiv.org/abs/1601.02970 | author:Radoslaw M. Cichy, Aditya Khosla, Dimitrios Pantazis, Antonio Torralba, Aude Oliva category:cs.CV q-bio.NC published:2016-01-12 summary:The complex multi-stage architecture of cortical visual pathways provides theneural basis for efficient visual object recognition in humans. However, thestage-wise computations therein remain poorly understood. Here, we comparedtemporal (magnetoencephalography) and spatial (functional MRI) visual brainrepresentations with representations in an artificial deep neural network (DNN)tuned to the statistics of real-world visual recognition. We showed that theDNN captured the stages of human visual processing in both time and space fromearly visual areas towards the dorsal and ventral streams. Furtherinvestigation of crucial DNN parameters revealed that while model architecturewas important, training on real-world categorization was necessary to enforcespatio-temporal hierarchical relationships with the brain. Together our resultsprovide an algorithmically informed view on the spatio-temporal dynamics ofvisual object recognition in the human visual brain.
arxiv-15600-115 | Learning Subclass Representations for Visually-varied Image Classification | http://arxiv.org/abs/1601.02913 | author:Xinchao Li, Peng Xu, Yue Shi, Martha Larson, Alan Hanjalic category:cs.MM cs.CV published:2016-01-12 summary:In this paper, we present a subclass-representation approach that predictsthe probability of a social image belonging to one particular class. We explorethe co-occurrence of user-contributed tags to find subclasses with a strongconnection to the top level class. We then project each image on to theresulting subclass space to generate a subclass representation for the image.The novelty of the approach is that subclass representations make use of notonly the content of the photos themselves, but also information on theco-occurrence of their tags, which determines membership in both subclasses andtop-level classes. The novelty is also that the images are classified intosmaller classes, which have a chance of being more visually stable and easierto model. These subclasses are used as a latent space and images arerepresented in this space by their probability of relatedness to all of thesubclasses. In contrast to approaches directly modeling each top-level classbased on the image content, the proposed method can exploit more informationfor visually diverse classes. The approach is evaluated on a set of $2$ millionphotos with 10 classes, released by the Multimedia 2013 Yahoo! Large-scaleFlickr-tag Image Classification Grand Challenge. Experiments show that theproposed system delivers sound performance for visually diverse classescompared with methods that directly model top classes.
arxiv-15600-116 | Human Attention Estimation for Natural Images: An Automatic Gaze Refinement Approach | http://arxiv.org/abs/1601.02852 | author:Jinsoo Choi, Tae-Hyun Oh, In So Kweon category:cs.CV cs.HC cs.MM published:2016-01-12 summary:Photo collections and its applications today attempt to reflect userinteractions in various forms. Moreover, photo collections aim to capture theusers' intention with minimum effort through applications capturing userintentions. Human interest regions in an image carry powerful information aboutthe user's behavior and can be used in many photo applications. Research onhuman visual attention has been conducted in the form of gaze tracking andcomputational saliency models in the computer vision community, and has shownconsiderable progress. This paper presents an integration between implicit gazeestimation and computational saliency model to effectively estimate humanattention regions in images on the fly. Furthermore, our method estimates humanattention via implicit calibration and incremental model updating without anyactive participation from the user. We also present extensive analysis andpossible applications for personal photo collections.
arxiv-15600-117 | Weightless neural network parameters and architecture selection in a quantum computer | http://arxiv.org/abs/1601.03277 | author:Adenilton J. da Silva, Wilson R. de Oliveira, Teresa B. Ludermir category:quant-ph cs.NE published:2016-01-12 summary:Training artificial neural networks requires a tedious empirical evaluationto determine a suitable neural network architecture. To avoid this empiricalprocess several techniques have been proposed to automatise the architectureselection process. In this paper, we propose a method to perform parameter andarchitecture selection for a quantum weightless neural network (qWNN). Thearchitecture selection is performed through the learning procedure of a qWNNwith a learning algorithm that uses the principle of quantum superposition anda non-linear quantum operator. The main advantage of the proposed method isthat it performs a global search in the space of qWNN architecture andparameters rather than a local search.
arxiv-15600-118 | Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation | http://arxiv.org/abs/1601.02828 | author:Pawel Swietojanski, Jinyu Li, Steve Renals category:cs.CL cs.LG cs.SD published:2016-01-12 summary:This work presents a broad study on the adaptation of neural network acousticmodels by means of learning hidden unit contributions (LHUC) -- a method thatlinearly re-combines hidden units in a speaker- or environment-dependent mannerusing small amounts of unsupervised adaptation data. We also extend LHUC to aspeaker adaptive training (SAT) framework that leads to more adaptable DNNacoustic model, which can work in both a speaker-dependent and aspeaker-independent manner, without the requirement to maintain auxiliaryspeaker-dependent feature extractors or to introduce significantspeaker-dependent changes to the DNN structure. Through a series of experimentson four different speech recognition benchmarks (TED talks, Switchboard, AMImeetings and Aurora4) and over 270 test speakers we show that LHUC in both itstest-only and SAT variants results in consistent word error rate reductionsranging from 5% to 23% relative depending on the task and the degree ofmismatch between training and test data. In addition we have investigated theeffect of the amount of adaptation data per speaker, the quality of adaptationtargets when estimating transforms in an unsupervised manner, thecomplementarity to other adaptation techniques, one-shot adaptation, and anextension to adapting DNNs trained in a sequence discriminative manner.
arxiv-15600-119 | Comparison and Adaptation of Automatic Evaluation Metrics for Quality Assessment of Re-Speaking | http://arxiv.org/abs/1601.02789 | author:Krzysztof Wołk, Danijel Koržinek category:cs.CL stat.AP stat.ML published:2016-01-12 summary:Re-speaking is a mechanism for obtaining high quality subtitles for use inlive broadcast and other public events. Because it relies on humans performingthe actual re-speaking, the task of estimating the quality of the results isnon-trivial. Most organisations rely on humans to perform the actual qualityassessment, but purely automatic methods have been developed for other similarproblems, like Machine Translation. This paper will try to compare several ofthese methods: BLEU, EBLEU, NIST, METEOR, METEOR-PL, TER and RIBES. These willthen be matched to the human-derived NER metric, commonly used in re-speaking.
arxiv-15600-120 | Robust Lineage Reconstruction from High-Dimensional Single-Cell Data | http://arxiv.org/abs/1601.02748 | author:Gregory Giecold, Eugenio Marco, Lorenzo Trippa, Guo-Cheng Yuan category:q-bio.QM stat.AP stat.CO stat.ML published:2016-01-12 summary:Single-cell gene expression data provide invaluable resources for systematiccharacterization of cellular hierarchy in multi-cellular organisms. However,cell lineage reconstruction is still often associated with significantuncertainty due to technological constraints. Such uncertainties have not beentaken into account in current methods. We present ECLAIR, a novel computationalmethod for the statistical inference of cell lineage relationships fromsingle-cell gene expression data. ECLAIR uses an ensemble approach to improvethe robustness of lineage predictions, and provides a quantitative estimate ofthe uncertainty of lineage branchings. We show that the application of ECLAIRto published datasets successfully reconstructs known lineage relationships andsignificantly improves the robustness of predictions. In conclusion, ECLAIR isa powerful bioinformatics tool for single-cell data analysis. It can be usedfor robust lineage reconstruction with quantitative estimate of predictionaccuracy.
arxiv-15600-121 | Deep Learning of Part-based Representation of Data Using Sparse Autoencoders with Nonnegativity Constraints | http://arxiv.org/abs/1601.02733 | author:Ehsan Hosseini-Asl, Jacek M. Zurada, Olfa Nasraoui category:cs.LG stat.ML published:2016-01-12 summary:We demonstrate a new deep learning autoencoder network, trained by anonnegativity constraint algorithm (NCAE), that learns features which showpart-based representation of data. The learning algorithm is based onconstraining negative weights. The performance of the algorithm is assessedbased on decomposing data into parts and its prediction performance is testedon three standard image data sets and one text dataset. The results indicatethat the nonnegativity constraint forces the autoencoder to learn features thatamount to a part-based representation of data, while improving sparsity andreconstruction quality in comparison with the traditional sparse autoencoderand Nonnegative Matrix Factorization. It is also shown that this newly acquiredrepresentation improves the prediction performance of a deep neural network.
arxiv-15600-122 | IRLS and Slime Mold: Equivalence and Convergence | http://arxiv.org/abs/1601.02712 | author:Damian Straszak, Nisheeth K. Vishnoi category:cs.DS cs.ET cs.IT math.IT math.NA math.OC stat.ML published:2016-01-12 summary:In this paper we present a connection between two dynamical systems arisingin entirely different contexts: one in signal processing and the other inbiology. The first is the famous Iteratively Reweighted Least Squares (IRLS)algorithm used in compressed sensing and sparse recovery while the second isthe dynamics of a slime mold (Physarum polycephalum). Both of these dynamicsare geared towards finding a minimum l1-norm solution in an affine subspace.Despite its simplicity the convergence of the IRLS method has been shown onlyfor a certain regularization of it and remains an important open problem. Ourfirst result shows that the two dynamics are projections of the same dynamicalsystem in higher dimensions. As a consequence, and building on the recent workon Physarum dynamics, we are able to prove convergence and obtain complexitybounds for a damped version of the IRLS algorithm.
arxiv-15600-123 | Robobarista: Learning to Manipulate Novel Objects via Deep Multimodal Embedding | http://arxiv.org/abs/1601.02705 | author:Jaeyong Sung, Seok Hyun Jin, Ian Lenz, Ashutosh Saxena category:cs.RO cs.AI cs.LG published:2016-01-12 summary:There is a large variety of objects and appliances in human environments,such as stoves, coffee dispensers, juice extractors, and so on. It ischallenging for a roboticist to program a robot for each of these object typesand for each of their instantiations. In this work, we present a novel approachto manipulation planning based on the idea that many household objects sharesimilarly-operated object parts. We formulate the manipulation planning as astructured prediction problem and learn to transfer manipulation strategyacross different objects by embedding point-cloud, natural language, andmanipulation trajectory data into a shared embedding space using a deep neuralnetwork. In order to learn semantically meaningful spaces throughout ournetwork, we introduce a method for pre-training its lower layers for multimodalfeature embedding and a method for fine-tuning this embedding space using aloss-based margin. In order to collect a large number of manipulationdemonstrations for different objects, we develop a new crowd-sourcing platformcalled Robobarista. We test our model on our dataset consisting of 116 objectsand appliances with 249 parts along with 250 language instructions, for whichthere are 1225 crowd-sourced manipulation demonstrations. We further show thatour robot with our model can even prepare a cup of a latte with appliances ithas never seen before.
arxiv-15600-124 | Environmental Noise Embeddings for Robust Speech Recognition | http://arxiv.org/abs/1601.02553 | author:Suyoun Kim, Bhiksha Raj, Ian Lane category:cs.CL published:2016-01-11 summary:We propose a novel deep neural network architecture for speech recognitionthat explicitly employs knowledge of the background environmental noise withina deep neural network acoustic model. A deep neural network is used to predictthe acoustic environment in which the system in being used. The discriminativeembedding generated at the bottleneck layer of this network is thenconcatenated with traditional acoustic features as input to a deep neuralnetwork acoustic model. Using simulated acoustic environments we show that theproposed approach significantly improves speech recognition accuracy in noisyand highly reverberant environments, outperforming multi-condition training andmulti-task learning for this task.
arxiv-15600-125 | Evaluating the Performance of a Speech Recognition based System | http://arxiv.org/abs/1601.02543 | author:Vinod Kumar Pandey, Sunil Kumar Kopparapu category:cs.CL cs.AI cs.HC published:2016-01-11 summary:Speech based solutions have taken center stage with growth in the servicesindustry where there is a need to cater to a very large number of people fromall strata of the society. While natural language speech interfaces are thetalk in the research community, yet in practice, menu based speech solutionsthrive. Typically in a menu based speech solution the user is required torespond by speaking from a closed set of words when prompted by the system. Asequence of human speech response to the IVR prompts results in the completionof a transaction. A transaction is deemed successful if the speech solution cancorrectly recognize all the spoken utterances of the user whenever prompted bythe system. The usual mechanism to evaluate the performance of a speechsolution is to do an extensive test of the system by putting it to actualpeople use and then evaluating the performance by analyzing the logs forsuccessful transactions. This kind of evaluation could lead to dissatisfiedtest users especially if the performance of the system were to result in a poortransaction completion rate. To negate this the Wizard of Oz approach isadopted during evaluation of a speech system. Overall this kind of evaluationsis an expensive proposition both in terms of time and cost. In this paper, wepropose a method to evaluate the performance of a speech solution withoutactually putting it to people use. We first describe the methodology and thenshow experimentally that this can be used to identify the performancebottlenecks of the speech solution even before the system is actually used thussaving evaluation time and expenses.
arxiv-15600-126 | Investigating gated recurrent neural networks for speech synthesis | http://arxiv.org/abs/1601.02539 | author:Zhizheng Wu, Simon King category:cs.CL cs.NE published:2016-01-11 summary:Recently, recurrent neural networks (RNNs) as powerful sequence models havere-emerged as a potential acoustic model for statistical parametric speechsynthesis (SPSS). The long short-term memory (LSTM) architecture isparticularly attractive because it addresses the vanishing gradient problem instandard RNNs, making them easier to train. Although recent studies havedemonstrated that LSTMs can achieve significantly better performance on SPSSthan deep feed-forward neural networks, little is known about why. Here weattempt to answer two questions: a) why do LSTMs work well as a sequence modelfor SPSS; b) which component (e.g., input gate, output gate, forget gate) ismost important. We present a visual analysis alongside a series of experiments,resulting in a proposal for a simplified architecture. The simplifiedarchitecture has significantly fewer parameters than an LSTM, thus reducinggeneration complexity considerably without degrading quality.
arxiv-15600-127 | How to learn a graph from smooth signals | http://arxiv.org/abs/1601.02513 | author:Vassilis Kalofolias category:stat.ML cs.LG published:2016-01-11 summary:We propose a framework that learns the graph structure underlying a set ofsmooth signals. Given $X\in\mathbb{R}^{m\times n}$ whose rows reside on thevertices of an unknown graph, we learn the edge weights$w\in\mathbb{R}_+^{m(m-1)/2}$ under the smoothness assumption that$\text{tr}{X^\top LX}$ is small. We show that the problem is a weighted$\ell$-1 minimization that leads to naturally sparse solutions. We point outhow known graph learning or construction techniques fall within our frameworkand propose a new model that performs better than the state of the art in manysettings. We present efficient, scalable primal-dual based algorithms for bothour model and the previous state of the art, and evaluate their performance onartificial and real data.
arxiv-15600-128 | Trans-gram, Fast Cross-lingual Word-embeddings | http://arxiv.org/abs/1601.02502 | author:Jocelyn Coulmance, Jean-Marc Marty, Guillaume Wenzek, Amine Benhalloum category:cs.CL published:2016-01-11 summary:We introduce Trans-gram, a simple and computationally-efficient method tosimultaneously learn and align wordembeddings for a variety of languages, usingonly monolingual data and a smaller set of sentence-aligned data. We use ournew method to compute aligned wordembeddings for twenty-one languages usingEnglish as a pivot language. We show that some linguistic features are alignedacross languages for which we do not have aligned data, even though thoseproperties do not exist in the pivot language. We also achieve state of the artresults on standard cross-lingual text classification and word translationtasks.
arxiv-15600-129 | Facial Expression Recognition in the Wild using Rich Deep Features | http://arxiv.org/abs/1601.02487 | author:Abubakrelsedik Karali, Ahmad Bassiouny, Motaz El-Saban category:cs.CV published:2016-01-11 summary:Facial Expression Recognition is an active area of research in computervision with a wide range of applications. Several approaches have beendeveloped to solve this problem for different benchmark datasets. However,Facial Expression Recognition in the wild remains an area where much work isstill needed to serve real-world applications. To this end, in this paper wepresent a novel approach towards facial expression recognition. We fuse richdeep features with domain knowledge through encoding discriminant facialpatches. We conduct experiments on two of the most popular benchmark datasets;CK and TFE. Moreover, we present a novel dataset that, unlike its precedents,consists of natural - not acted - expression images. Experimental results showthat our approach achieves state-of-the-art results over standard benchmarksand our own dataset
arxiv-15600-130 | The Effects of Age, Gender and Region on Non-standard Linguistic Variation in Online Social Networks | http://arxiv.org/abs/1601.02431 | author:Claudia Peersman, Walter Daelemans, Reinhild Vandekerckhove, Bram Vandekerckhove, Leona Van Vaerenbergh category:cs.CL published:2016-01-11 summary:We present a corpus-based analysis of the effects of age, gender and regionof origin on the production of both "netspeak" or "chatspeak" features andregional speech features in Flemish Dutch posts that were collected from aBelgian online social network platform. The present study shows that combiningquantitative and qualitative approaches is essential for understandingnon-standard linguistic variation in a CMC corpus. It also presents amethodology that enables the systematic study of this variation by includingall non-standard words in the corpus. The analyses resulted in a convincingillustration of the Adolescent Peak Principle. In addition, our approachrevealed an intriguing correlation between the use of regional speech featuresand chatspeak features.
arxiv-15600-131 | Implicit Look-alike Modelling in Display Ads: Transfer Collaborative Filtering to CTR Estimation | http://arxiv.org/abs/1601.02377 | author:Weinan Zhang, Lingxi Chen, Jun Wang category:cs.LG cs.IR published:2016-01-11 summary:User behaviour targeting is essential in online advertising. Compared withsponsored search keyword targeting and contextual advertising page contenttargeting, user behaviour targeting builds users' interest profiles viatracking their online behaviour and then delivers the relevant ads according toeach user's interest, which leads to higher targeting accuracy and thus moreimproved advertising performance. The current user profiling methods includebuilding keywords and topic tags or mapping users onto a hierarchical taxonomy.However, to our knowledge, there is no previous work that explicitlyinvestigates the user online visits similarity and incorporates such similarityinto their ad response prediction. In this work, we propose a general frameworkwhich learns the user profiles based on their online browsing behaviour, andtransfers the learned knowledge onto prediction of their ad response.Technically, we propose a transfer learning model based on the probabilisticlatent factor graphic models, where the users' ad response profiles aregenerated from their online browsing profiles. The large-scale experimentsbased on real-world data demonstrate significant improvement of our solutionover some strong baselines.
arxiv-15600-132 | 3D Gaze Estimation from 2D Pupil Positions on Monocular Head-Mounted Eye Trackers | http://arxiv.org/abs/1601.02644 | author:Mohsen Mansouryar, Julian Steil, Yusuke Sugano, Andreas Bulling category:cs.HC cs.CV published:2016-01-11 summary:3D gaze information is important for scene-centric attention analysis butaccurate estimation and analysis of 3D gaze in real-world environments remainschallenging. We present a novel 3D gaze estimation method for monocularhead-mounted eye trackers. In contrast to previous work, our method does notaim to infer 3D eyeball poses but directly maps 2D pupil positions to 3D gazedirections in scene camera coordinate space. We first provide a detaileddiscussion of the 3D gaze estimation task and summarize different methods,including our own. We then evaluate the performance of different 3D gazeestimation approaches using both simulated and real data. Through experimentalvalidation, we demonstrate the effectiveness of our method in reducing parallaxerror, and we identify research challenges for the design of 3D calibrationprocedures.
arxiv-15600-133 | Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction | http://arxiv.org/abs/1601.02376 | author:Weinan Zhang, Tianming Du, Jun Wang category:cs.LG cs.IR published:2016-01-11 summary:Predicting user responses, such as click-through rate and conversion rate,are critical in many web applications including web search, personalisedrecommendation, and online advertising. Different from continuous raw featuresthat we usually found in the image and audio domains, the input features in webspace are always of multi-field and are mostly discrete and categorical whiletheir dependencies are little known. Major user response prediction models haveto either limit themselves to linear models or require manually building uphigh-order combination features. The former loses the ability of exploringfeature interactions, while the latter results in a heavy computation in thelarge feature space. To tackle the issue, we propose two novel models usingdeep neural networks (DNNs) to automatically learn effective patterns fromcategorical feature interactions and make predictions of users' ad clicks. Toget our DNNs efficiently work, we propose to leverage three featuretransformation methods, i.e., factorisation machines (FMs), restrictedBoltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paperpresents the structure of our models and their efficient training algorithms.The large-scale experiments with real-world data demonstrate that our methodswork better than major state-of-the-art models.
arxiv-15600-134 | Temporal Multinomial Mixture for Instance-Oriented Evolutionary Clustering | http://arxiv.org/abs/1601.02300 | author:Young-Min Kim, Julien Velcin, Stéphane Bonnevay, Marian-Andrei Rizoiu category:cs.IR cs.LG stat.ML published:2016-01-11 summary:Evolutionary clustering aims at capturing the temporal evolution of clusters.This issue is particularly important in the context of social media data thatare naturally temporally driven. In this paper, we propose a new probabilisticmodel-based evolutionary clustering technique. The Temporal Multinomial Mixture(TMM) is an extension of classical mixture model that optimizes featureco-occurrences in the trade-off with temporal smoothness. Our model isevaluated for two recent case studies on opinion aggregation over time. Wecompare four different probabilistic clustering models and we show thesuperiority of our proposal in the task of instance-oriented clustering.
arxiv-15600-135 | How to Use Temporal-Driven Constrained Clustering to Detect Typical Evolutions | http://arxiv.org/abs/1601.02603 | author:Marian-Andrei Rizoiu, Julien Velcin, Stéphane Lallich category:cs.LG cs.DS published:2016-01-11 summary:In this paper, we propose a new time-aware dissimilarity measure that takesinto account the temporal dimension. Observations that are close in thedescription space, but distant in time are considered as dissimilar. We alsopropose a method to enforce the segmentation contiguity, by introducing, in theobjective function, a penalty term inspired from the Normal DistributionFunction. We combine the two propositions into a novel time-driven constrainedclustering algorithm, called TDCK-Means, which creates a partition of coherentclusters, both in the multidimensional space and in the temporal space. Thisalgorithm uses soft semi-supervised constraints, to encourage adjacentobservations belonging to the same entity to be assigned to the same cluster.We apply our algorithm to a Political Studies dataset in order to detecttypical evolution phases. We adapt the Shannon entropy in order to measure theentity contiguity, and we show that our proposition consistently improvestemporal cohesion of clusters, without any significant loss in themultidimensional variance.
arxiv-15600-136 | Argumentation Mining in User-Generated Web Discourse | http://arxiv.org/abs/1601.02403 | author:Ivan Habernal, Iryna Gurevych category:cs.CL published:2016-01-11 summary:The goal of argumentation mining, an evolving research field in computationallinguistics, is to design methods capable of analyzing people's argumentation.In this article, we go beyond the state of the art in several ways. (i) We dealwith actual Web data and take up the challenges given by the variety ofregisters, multiple domains, and unrestricted noisy user-generated Webdiscourse. (ii) We bridge the gap between normative argumentation theories andargumentation phenomena encountered in actual data by adapting an argumentationmodel tested in an extensive annotation study. (iii) We create a new goldstandard corpus (90k tokens in 340 documents) and experiment with severalmachine learning methods to identify argument components. We offer the data,source codes, and annotation guidelines to the community under free licenses.Our findings show that argumentation mining in user-generated Web discourse isa feasible but challenging task.
arxiv-15600-137 | Stationary signal processing on graphs | http://arxiv.org/abs/1601.02522 | author:Nathanaël Perraudin, Pierre Vandergheynst category:cs.DS stat.AP stat.ML published:2016-01-11 summary:Graphs are a central tool in machine learning and information processing asthey allow to conveniently capture the structure of complex datasets. In thiscontext, it is of high importance to develop flexible models of signals definedover graphs or networks. In this paper, we generalize the traditional conceptof wide sense stationarity to signals defined over the vertices of arbitraryweighted undirected graphs. We show that stationarity is intimately linked tostatistical invariance under a localization operator reminiscent oftranslation. We prove that stationary graph signals are characterized by awell-defined Power Spectral Density that can be efficiently estimated even forlarge graphs. We leverage this new concept to derive Wiener-type estimationprocedures of noisy and partially observed signals and illustrate theperformance of this new model for denoising and regression.
arxiv-15600-138 | On Clustering Time Series Using Euclidean Distance and Pearson Correlation | http://arxiv.org/abs/1601.02213 | author:Michael R. Berthold, Frank Höppner category:cs.LG cs.AI stat.ML published:2016-01-10 summary:For time series comparisons, it has often been observed that z-scorenormalized Euclidean distances far outperform the unnormalized variant. In thispaper we show that a z-score normalized, squared Euclidean Distance is, infact, equal to a distance based on Pearson Correlation. This has profoundimpact on many distance-based classification or clustering methods. In additionto this theoretically sound result we also show that the often used k-Meansalgorithm formally needs a mod ification to keep the interpretation as Pearsoncorrelation strictly valid. Experimental results demonstrate that in many casesthe standard k-Means algorithm generally produces the same results.
arxiv-15600-139 | Joint Object-Material Category Segmentation from Audio-Visual Cues | http://arxiv.org/abs/1601.02220 | author:Anurag Arnab, Michael Sapienza, Stuart Golodetz, Julien Valentin, Ondrej Miksik, Shahram Izadi, Philip Torr category:cs.CV cs.SD published:2016-01-10 summary:It is not always possible to recognise objects and infer material propertiesfor a scene from visual cues alone, since objects can look visually similarwhilst being made of very different materials. In this paper, we thereforepresent an approach that augments the available dense visual cues with sparseauditory cues in order to estimate dense object and material labels. Sinceestimates of object class and material properties are mutually informative, weoptimise our multi-output labelling jointly using a random-field framework. Weevaluate our system on a new dataset with paired visual and auditory data thatwe make publicly available. We demonstrate that this joint estimation of objectand material labels significantly outperforms the estimation of either categoryin isolation.
arxiv-15600-140 | Parallel Stroked Multi Line: a model-based method for compressing large fingerprint databases | http://arxiv.org/abs/1601.02225 | author:Hamid Mansouri, Hamid-Reza Pourreza category:cs.CV cs.DS published:2016-01-10 summary:With increasing usage of fingerprints as an important biometric data, theneed to compress the large fingerprint databases has become essential. The mostrecommended compression algorithm, even by standards, is JPEG2K. But at highcompression rates, this algorithm is ineffective. In this paper, a model isproposed which is based on parallel lines with same orientations, arbitrarywidths and same gray level values located on rectangle with constant gray levelvalue as background. We refer to this algorithm as Parallel Stroked Multi Line(PSML). By using Adaptive Geometrical Wavelet and employing PSML, a compressionalgorithm is developed. This compression algorithm can preserve fingerprintstructure and minutiae. The exact algorithm of computing the PSML model takeexponential time. However, we have proposed an alternative approximationalgorithm, which reduces the time complexity to $O(n^3)$. The proposed PSMLalg. has significant advantage over Wedgelets Transform in PSNR value andvisual quality in compressed images. The proposed method, despite the lowerPSNR values than JPEG2K algorithm in common range of compression rates, in allcompression rates have nearly equal or greater advantage over JPEG2K when usedby Automatic Fingerprint Identification Systems (AFIS). At high compressionrates, according to PSNR values, mean EER rate and visual quality, the encodedimages with JPEG2K can not be identified from each other after compression.But, images encoded by the PSML alg. retained the sufficient information tomaintain fingerprint identification performances similar to the ones obtainedby raw images without compression. One the U.are.U 400 database, the mean EERrate for uncompressed images is 4.54%, while at 267:1 compression ratio, thisvalue becomes 49.41% and 6.22% for JPEG2K and PSML, respectively. This resultshows a significant improvement over the standard JPEG2K algorithm.
arxiv-15600-141 | A Sufficient Statistics Construction of Bayesian Nonparametric Exponential Family Conjugate Models | http://arxiv.org/abs/1601.02257 | author:Robert Finn, Brian Kulis category:cs.LG stat.ML published:2016-01-10 summary:Conjugate pairs of distributions over infinite dimensional spaces areprominent in statistical learning theory, particularly due to the widespreadadoption of Bayesian nonparametric methodologies for a host of models andapplications. Much of the existing literature in the learning community focuseson processes possessing some form of computationally tractable conjugacy as isthe case for the beta and gamma processes (and, via normalization, theDirichlet process). For these processes, proofs of conjugacy and requisitederivation of explicit computational formulae for posterior density parametersare idiosyncratic to the stochastic process in question. As such, BayesianNonparametric models are currently available for a limited number of conjugatepairs, e.g. the Dirichlet-multinomial and beta-Bernoulli process pairs. In eachof these above cases the likelihood process belongs to the class of discreteexponential family distributions. The exclusion of continuous likelihooddistributions from the known cases of Bayesian Nonparametric Conjugate modelsstands as a disparity in the researcher's toolbox. In this paper we first address the problem of obtaining a generalconstruction of prior distributions over infinite dimensional spaces possessingdistributional properties amenable to conjugacy. Second, we bridge the dividebetween the discrete and continuous likelihoods by illustrating a canonicalconstruction for stochastic processes whose Levy measure densities are frompositive exponential families, and then demonstrate that these processes infact form the prior, likelihood, and posterior in a conjugate family. Ourcanonical construction subsumes known computational formulae for posteriordensity parameters in the cases where the likelihood is from a discretedistribution belonging to an exponential family.
arxiv-15600-142 | Supervised multiview learning based on simultaneous learning of multiview intact and single view classifier | http://arxiv.org/abs/1601.02098 | author:Qingjun Wang, Haiyan Lv, Jun Yue, Eugene Mitchell category:cs.CV published:2016-01-09 summary:Multiview learning problem refers to the problem of learning a classifierfrom multiple view data. In this data set, each data points is presented bymultiple different views. In this paper, we propose a novel method for thisproblem. This method is based on two assumptions. The first assumption is thateach data point has an intact feature vector, and each view is obtained by alinear transformation from the intact vector. The second assumption is that theintact vectors are discriminative, and in the intact space, we have a linearclassifier to separate the positive class from the negative class. We define anintact vector for each data point, and a view-conditional transformation matrixfor each view, and propose to reconstruct the multiple view feature vectors bythe product of the corresponding intact vectors and transformation matrices.Moreover, we also propose a linear classifier in the intact space, and learn itjointly with the intact vectors. The learning problem is modeled by aminimization problem, and the objective function is composed of a Cauchy errorestimator-based view-conditional reconstruction term over all data points andviews, and a classification error term measured by hinge loss over all theintact vectors of all the data points. Some regularization terms are alsoimposed to different variables in the objective function. The minimizationproblem is solve by an iterative algorithm using alternate optimizationstrategy and gradient descent algorithm. The proposed algorithm shows itadvantage in the compression to other multiview learning algorithms onbenchmark data sets.
arxiv-15600-143 | Group Invariant Deep Representations for Image Instance Retrieval | http://arxiv.org/abs/1601.02093 | author:Olivier Morère, Antoine Veillard, Jie Lin, Julie Petta, Vijay Chandrasekhar, Tomaso Poggio category:cs.CV cs.IR published:2016-01-09 summary:Most image instance retrieval pipelines are based on comparison of vectorsknown as global image descriptors between a query image and the databaseimages. Due to their success in large scale image classification,representations extracted from Convolutional Neural Networks (CNN) are quicklygaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptorsfor image instance retrieval. While CNN-based descriptors are generallyremarked for good retrieval performance at lower bitrates, they neverthelesspresent a number of drawbacks including the lack of robustness to common objecttransformations such as rotations compared with their interest point based FVcounterparts. In this paper, we propose a method for computing invariant global descriptorsfrom CNNs. Our method implements a recently proposed mathematical theory forinvariance in a sensory cortex modeled as a feedforward neural network. Theresulting global descriptors can be made invariant to multiple arbitrarytransformation groups while retaining good discriminativeness. Based on a thorough empirical evaluation using several publicly availabledatasets, we show that our method is able to significantly and consistentlyimprove retrieval results every time a new type of invariance is incorporated.We also show that our method which has few parameters is not prone tooverfitting: improvements generalize well across datasets with differentproperties with regard to invariances. Finally, we show that our descriptorsare able to compare favourably to other state-of-the-art compact descriptors insimilar bitranges, exceeding the highest retrieval results reported in theliterature on some datasets. A dedicated dimensionality reduction step--quantization or hashing-- may be able to further improve the competitivenessof the descriptors.
arxiv-15600-144 | Multicuts and Perturb & MAP for Probabilistic Graph Clustering | http://arxiv.org/abs/1601.02088 | author:Jörg Hendrik Kappes, Paul Swoboda, Bogdan Savchynskyy, Tamir Hazan, Christoph Schnörr category:cs.CV published:2016-01-09 summary:We present a probabilistic graphical model formulation for the graphclustering problem. This enables to locally represent uncertainty of imagepartitions by approximate marginal distributions in a mathematicallysubstantiated way, and to rectify local data term cues so as to close contoursand to obtain valid partitions. We exploit recent progress on globally optimal MAP inference by integerprogramming and on perturbation-based approximations of the log-partitionfunction, in order to sample clusterings and to estimate marginal distributionsof node-pairs both more accurately and more efficiently than state-of-the-artmethods. Our approach works for any graphically represented problem instance.This is demonstrated for image segmentation and social network clusteranalysis. Our mathematical ansatz should be relevant also for othercombinatorial problems.
arxiv-15600-145 | Kernelized LRR on Grassmann Manifolds for Subspace Clustering | http://arxiv.org/abs/1601.02124 | author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV published:2016-01-09 summary:Low rank representation (LRR) has recently attracted great interest due toits pleasing efficacy in exploring low-dimensional sub- space structuresembedded in data. One of its successful applications is subspace clustering, bywhich data are clustered according to the subspaces they belong to. In thispaper, at a higher level, we intend to cluster subspaces into classes ofsubspaces. This is naturally described as a clustering problem on Grassmannmanifold. The novelty of this paper is to generalize LRR on Euclidean spaceonto an LRR model on Grassmann manifold in a uniform kernelized LRR framework.The new method has many applications in data analysis in computer vision tasks.The proposed models have been evaluated on a number of practical data analysisapplications. The experimental results show that the proposed models outperforma number of state-of-the-art subspace clustering methods.
arxiv-15600-146 | Empirical Gaussian priors for cross-lingual transfer learning | http://arxiv.org/abs/1601.02166 | author:Anders Søgaard category:cs.CL published:2016-01-09 summary:Sequence model learning algorithms typically maximize log-likelihood minusthe norm of the model (or minimize Hamming loss + norm). In cross-lingualpart-of-speech (POS) tagging, our target language training data consists ofsequences of sentences with word-by-word labels projected from translations in$k$ languages for which we have labeled data, via word alignments. Our trainingdata is therefore very noisy, and if Rademacher complexity is high, learningalgorithms are prone to overfit. Norm-based regularization assumes a constantwidth and zero mean prior. We instead propose to use the $k$ source languagemodels to estimate the parameters of a Gaussian prior for learning new POStaggers. This leads to significantly better performance in multi-sourcetransfer set-ups. We also present a drop-out version that injects (empirical)Gaussian noise during online learning. Finally, we note that using empiricalGaussian priors leads to much lower Rademacher complexity, and is superior tooptimally weighted model interpolation.
arxiv-15600-147 | Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs | http://arxiv.org/abs/1601.02129 | author:Zheng Shou, Dongang Wang, Shih-Fu Chang category:cs.CV published:2016-01-09 summary:We address temporal action localization in untrimmed long videos. This isimportant because videos in real applications are usually unconstrained andcontain multiple action instances plus video content of background scenes orother activities. To address this challenging issue, we exploit theeffectiveness of deep networks in temporal action localization via threesegment-based 3D ConvNets: (1) a proposal network identifies candidate segmentsin a long video that may contain actions; (2) a classification network learnsone-vs-all action classification model to serve as initialization for thelocalization network; and (3) a localization network fine-tunes on the learnedclassification network to localize each action instance. We propose a novelloss function for the localization network to explicitly consider temporaloverlap and therefore achieve high temporal localization accuracy. Only theproposal network and the localization network are used during prediction. Ontwo large-scale benchmarks, our approach achieves significantly superiorperformances compared with other state-of-the-art systems: mAP increases from1.7% to 7.4% on MEXaction2 and increases from 15.0% to 19.0% on THUMOS 2014,when the overlap threshold for evaluation is set to 0.5.
arxiv-15600-148 | Minimax Subsampling for Estimation and Prediction in Low-Dimensional Linear Regression | http://arxiv.org/abs/1601.02068 | author:Yining Wang, Aarti Singh category:stat.ML cs.LG math.ST stat.TH published:2016-01-09 summary:Subsampling strategies are derived to sample a small portion of design (data)points in a low-dimensional linear regression model $y=X\beta+\varepsilon$ withnear-optimal statistical rates. Our results apply to both problems ofestimation of the underlying linear model $\beta$ and predicting thereal-valued response $y$ of a new data point $x$. The derived subsamplingstrategies are minimax optimal under the fixed design setting, up to a small$(1+\epsilon)$ relative factor. We also give interpretable subsamplingprobabilities for the random design setting and demonstrate explicit gaps instatistial rates between optimal and baseline (e.g., uniform) subsamplingmethods.
arxiv-15600-149 | Numerical Coding of Nominal Data | http://arxiv.org/abs/1601.01966 | author:Zenon Gniazdowski, Michal Grabowski category:stat.ML published:2016-01-08 summary:In this paper, a novel approach for coding nominal data is proposed. For thegiven nominal data, a rank in a form of complex number is assigned. Theproposed method does not lose any information about the attribute and bringsother properties previously unknown. The approach based on these knewproperties can been used for classification. The analyzed example shows thatclassification with the use of coded nominal data or both numerical as well ascoded nominal data is more effective than the classification, which uses onlynumerical data.
arxiv-15600-150 | Nonparametric semi-supervised learning of class proportions | http://arxiv.org/abs/1601.01944 | author:Shantanu Jain, Martha White, Michael W. Trosset, Predrag Radivojac category:stat.ML cs.LG published:2016-01-08 summary:The problem of developing binary classifiers from positive and unlabeled datais often encountered in machine learning. A common requirement in this settingis to approximate posterior probabilities of positive and negative classes fora previously unseen data point. This problem can be decomposed into two steps:(i) the development of accurate predictors that discriminate between positiveand unlabeled data, and (ii) the accurate estimation of the prior probabilitiesof positive and negative examples. In this work we primarily focus on thelatter subproblem. We study nonparametric class prior estimation and formulatethis problem as an estimation of mixing proportions in two-component mixturemodels, given a sample from one of the components and another sample from themixture itself. We show that estimation of mixing proportions is generallyill-defined and propose a canonical form to obtain identifiability whilemaintaining the flexibility to model any distribution. We use insights fromthis theory to elucidate the optimization surface of the class priors andpropose an algorithm for estimating them. To address the problems ofhigh-dimensional density estimation, we provide practical transformations tolow-dimensional spaces that preserve class priors. Finally, we demonstrate theefficacy of our method on univariate and multivariate data.
arxiv-15600-151 | Research Project: Text Engineering Tool for Ontological Scientometry | http://arxiv.org/abs/1601.01887 | author:Rustam Tagiew category:cs.CL cs.DL published:2016-01-08 summary:The number of scientific papers grows exponentially in many disciplines. Theshare of online available papers grows as well. At the same time, the period oftime for a paper to loose at chance to be cited anymore shortens. The decay ofthe citing rate shows similarity to ultradiffusional processes as for otheronline contents in social networks. The distribution of papers per author showssimilarity to the distribution of posts per user in social networks. The rateof uncited papers for online available papers grows while some papers 'goviral' in terms of being cited. Summarized, the practice of scientificpublishing moves towards the domain of social networks. The goal of thisproject is to create a text engineering tool, which can semi-automaticallycategorize a paper according to its type of contribution and extractrelationships between them into an ontological database. Semi-automaticcategorization means that the mistakes made by automatic pre-categorization andrelationship-extraction will be corrected through a wikipedia-like front-end byvolunteers from general public. This tool should not only help researchers andthe general public to find relevant supplementary material and peers faster,but also provide more information for research funding agencies.
arxiv-15600-152 | Visual Script and Language Identification | http://arxiv.org/abs/1601.01885 | author:Anguelos Nicolaou, Andrew Bagdanov, Lluis Gomez-Bigorda, Dimosthenis Karatzas category:cs.CV published:2016-01-08 summary:In this paper we introduce a script identification method based onhand-crafted texture features and an artificial neural network. The proposedpipeline achieves near state-of-the-art performance for script identificationof video-text and state-of-the-art performance on visual languageidentification of handwritten text. More than using the deep network as aclassifier, the use of its intermediary activations as a learned metricdemonstrates remarkable results and allows the use of discriminative models onunknown classes. Comparative experiments in video-text and text in the wilddatasets provide insights on the internals of the proposed deep network.
arxiv-15600-153 | Facial age estimation using BSIF and LBP | http://arxiv.org/abs/1601.01876 | author:Salah Eddine Bekhouche, Abdelkrim Ouafi, Abdelmalik Taleb-Ahmed, Abdenour Hadid, Azeddine Benlamoudi category:cs.CV published:2016-01-08 summary:Human face aging is irreversible process causing changes in human facecharacteristics such us hair whitening, muscles drop and wrinkles. Due to theimportance of human face aging in biometrics systems, age estimation became anattractive area for researchers. This paper presents a novel method to estimatethe age from face images, using binarized statistical image features (BSIF) andlocal binary patterns (LBP)histograms as features performed by support vectorregression (SVR) and kernel ridge regression (KRR). We applied our method onFG-NET and PAL datasets. Our proposed method has shown superiority to that ofthe state-of-the-art methods when using the whole PAL database.
arxiv-15600-154 | Learning to Remove Multipath Distortions in Time-of-Flight Range Images for a Robotic Arm Setup | http://arxiv.org/abs/1601.01750 | author:Kilho Son, Ming-Yu Liu, Yuichi Taguchi category:cs.CV cs.RO published:2016-01-08 summary:Range images captured by Time-of-Flight (ToF) cameras are corrupted withmultipath distortions due to interaction between modulated light signals andscenes. The interaction is often complicated, which makes a model-basedsolution elusive. We propose a learning-based approach for removing themultipath distortions for a ToF camera in a robotic arm setup. Our approach isbased on deep learning. We use the robotic arm to automatically collect a largeamount of ToF range images containing various multipath distortions. Thetraining images are automatically labeled by leveraging a high precisionstructured light sensor available only in the training time. In the test time,we apply the learned model to remove the multipath distortions. This allows ourrobotic arm setup to enjoy the speed and compact form of the ToF camera withoutcompromising with its range measurement errors. We conduct extensiveexperimental validations and compare the proposed method to several baselinealgorithms. The experiment results show that our method achieves 55% errorreduction in range estimation and largely outperforms the baseline algorithms.
arxiv-15600-155 | Dense Bag-of-Temporal-SIFT-Words for Time Series Classification | http://arxiv.org/abs/1601.01799 | author:Adeline Bailly, Simon Malinowski, Romain Tavenard, Thomas Guyet, Laetitia Chapel category:cs.LG published:2016-01-08 summary:Time series classification is an application of particular interest with theincrease of data to monitor. Classical techniques for time seriesclassification rely on point-to-point distances. Recently, Bag-of-Wordsapproaches have been used in this context. Words are quantized versions ofsimple features extracted from sliding windows. The SIFT framework has provedefficient for image classification. In this paper, we design a time seriesclassification scheme that builds on the SIFT framework adapted to time seriesto feed a Bag-of-Words. We then refine our method by studying the impact ofnormalized Bag-of-Words, as well as densely extract point descriptors. Proposedadjustements achieve better performance. The evaluation shows that our methodoutperforms classical techniques in terms of classification.
arxiv-15600-156 | Cox process representation and inference for stochastic reaction-diffusion processes | http://arxiv.org/abs/1601.01972 | author:David Schnoerr, Ramon Grima, Guido Sanguinetti category:math.ST q-bio.QM stat.ML stat.TH published:2016-01-08 summary:Complex behaviour in many systems arises from the stochastic interactions ofspatially distributed particles or agents. Stochastic reaction-diffusionprocesses are widely used to model such behaviour in disciplines ranging frombiology to the social sciences, yet they are notoriously difficult to simulateand calibrate to observational data. Here we use ideas from statistical physicsand machine learning to provide a solution to the inverse problem of learning astochastic reaction diffusion process to data. Our solution relies on a novel,non-trivial connection between stochastic reaction-diffusion processes andspatio-temporal Cox processes, a well-studied class of models fromcomputational statistics. We develop an efficient and flexible algorithm whichshows excellent accuracy on numeric and real data examples from systems biologyand epidemiology. By using ideas from multiple disciplines, our approachprovides both new and fundamental insights into spatio-temporal stochasticsystems, and a practical solution to a long-standing problem in computationalmodelling.
arxiv-15600-157 | Scale-Free Online Learning | http://arxiv.org/abs/1601.01974 | author:Francesco Orabona, Dávid Pál category:cs.LG published:2016-01-08 summary:We design algorithms for online linear optimization that have optimal regretand at the same time do not need to know any upper or lower bounds on the normof the loss vectors. We achieve adaptiveness to the norms of the loss vectorsby scale invariance, i.e., our algorithms make exactly the same decisions ifthe sequence of loss vectors is multiplied by any positive constant. One of ouralgorithms works for any decision set, bounded or unbounded. For unboundeddecisions sets, this is the first adaptive algorithm for online linearoptimization with a non-vacuous regret bound. We also study a popular scale-free variant of online mirror descentalgorithm, and we show that in two natural settings it has linear or worseregret.
arxiv-15600-158 | A note on the sample complexity of the Er-SpUD algorithm by Spielman, Wang and Wright for exact recovery of sparsely used dictionaries | http://arxiv.org/abs/1601.02049 | author:Radosław Adamczak category:math.PR cs.LG math.ST stat.TH published:2016-01-08 summary:We consider the problem of recovering an invertible $n \times n$ matrix $A$and a sparse $n \times p$ random matrix $X$ based on the observation of $Y =AX$ (up to a scaling and permutation of columns of $A$ and rows of $X$). Usingonly elementary tools from the theory of empirical processes we show that aversion of the Er-SpUD algorithm by Spielman, Wang and Wright with highprobability recovers $A$ and $X$ exactly, provided that $p \ge Cn\log n$, whichis optimal up to the constant $C$.
arxiv-15600-159 | Song Recommendation with Non-Negative Matrix Factorization and Graph Total Variation | http://arxiv.org/abs/1601.01892 | author:Kirell Benzi, Vassilis Kalofolias, Xavier Bresson, Pierre Vandergheynst category:stat.ML cs.IR cs.LG published:2016-01-08 summary:This work formulates a novel song recommender system as a matrix completionproblem that benefits from collaborative filtering through Non-negative MatrixFactorization (NMF) and content-based filtering via total variation (TV) ongraphs. The graphs encode both playlist proximity information and songsimilarity, using a rich combination of audio, meta-data and social features.As we demonstrate, our hybrid recommendation system is very versatile andincorporates several well-known methods while outperforming them. Particularly,we show on real-world data that our model overcomes w.r.t. two evaluationmetrics the recommendation of models solely based on low-rank information,graph-based information or a combination of both.
arxiv-15600-160 | On Some Properties of Calibrated Trifocal Tensors | http://arxiv.org/abs/1601.01467 | author:Evgeniy Martyushev category:cs.CV published:2016-01-07 summary:In two-view geometry, the essential matrix describes the relative positionand orientation of two calibrated images. In three views, a similar role isassigned to the calibrated trifocal tensor. It is a particular case of the(uncalibrated) trifocal tensor and thus it inherits all its properties but, dueto the smaller degrees of freedom, satisfies a number of additional algebraicconstraints. Some of them are described in this paper. More specifically, wedefine a new notion --- the trifocal essential matrix. On the one hand, it is ageneralization of the ordinary (bifocal) essential matrix, and, on the otherhand, it is closely related to the calibrated trifocal tensor. We prove the twonecessary and sufficient conditions that characterize the set of trifocalessential matrices. Based on these characterizations, we propose threenecessary conditions on a calibrated trifocal tensor. They have a form of 15quartic and 99 quintic polynomial equations. We show that in the practicallysignificant real case the 15 quartic constraints are also sufficient.
arxiv-15600-161 | Measuring and Discovering Correlations in Large Data Sets | http://arxiv.org/abs/1602.07960 | author:Lijue Liu, Ming Li, Sha Wen category:stat.ME stat.ML published:2016-01-07 summary:In this paper, a class of statistics named ART (the alternant recursivetopology statistics) is proposed to measure the properties of correlationbetween two variables. A wide range of bi-variable correlations both linear andnonlinear can be evaluated by ART efficiently and equitably even if nothing isknown about the specific types of those relationships. ART compensates thedisadvantages of Reshef's model in which no polynomial time precise algorithmexists and the "local random" phenomenon can not be identified. As a class ofnonparametric exploration statistics, ART is applied for analyzing a dataset of10 American classical indexes, as a result, lots of bi-variable correlationsare discovered.
arxiv-15600-162 | Fast Kronecker product kernel methods via sampled vec trick | http://arxiv.org/abs/1601.01507 | author:Antti Airola, Tapio Pahikkala category:stat.ML cs.LG published:2016-01-07 summary:Kronecker product kernel provides the standard approach in the kernel methodsliterature for learning from pair-input data, where both data points andprediction tasks have their own feature representations. The methods allowsimultaneous generalization to both new tasks and data unobserved in thetraining set, a setting known as zero-shot or zero-data learning. Such asetting occurs in numerous applications, including drug-target interactionprediction, collaborative filtering and information retrieval. Efficienttraining algorithms based on the so-called vec trick, that makes use of thespecial structure of the Kronecker product, are known for the case where theoutput matrix for the training set is fully observed, i.e. the correct outputfor each data point-task combination is available. In this work we generalizethese results, proposing an efficient algorithm for sampled Kronecker productmultiplication, where only a subset of the full Kronecker product is computed.This allows us to derive a general framework for training Kronecker kernelmethods, as specific examples we implement Kronecker ridge regression andsupport vector machine algorithms. Experimental results demonstrate that theproposed approach leads to accurate models, while allowing order of magnitudeimprovements in training and prediction time.
arxiv-15600-163 | Stochastic Dykstra Algorithms for Metric Learning on Positive Semi-Definite Cone | http://arxiv.org/abs/1601.01422 | author:Tomoki Matsuzawa, Raissa Relator, Jun Sese, Tsuyoshi Kato category:cs.CV published:2016-01-07 summary:Recently, covariance descriptors have received much attention as powerfulrepresentations of set of points. In this research, we present a new metriclearning algorithm for covariance descriptors based on the Dykstra algorithm,in which the current solution is projected onto a half-space at each iteration,and runs at O(n^3) time. We empirically demonstrate that randomizing the orderof half-spaces in our Dykstra-based algorithm significantly accelerates theconvergence to the optimal solution. Furthermore, we show that our approachyields promising experimental results on pattern recognition tasks.
arxiv-15600-164 | Ensemble Methods of Classification for Power Systems Security Assessment | http://arxiv.org/abs/1601.01675 | author:Alexei Zhukov, Victor Kurbatsky, Nikita Tomin, Denis Sidorov, Daniil Panasetsky, Aoife Foley category:cs.AI cs.LG 68T05 published:2016-01-07 summary:One of the most promising approaches for complex technical systems analysisemploys ensemble methods of classification. Ensemble methods enable to build areliable decision rules for feature space classification in the presence ofmany possible states of the system. In this paper, novel techniques based ondecision trees are used for evaluation of the reliability of the regime ofelectric power systems. We proposed hybrid approach based on random forestsmodels and boosting models. Such techniques can be applied to predict theinteraction of increasing renewable power, storage devices and swiching ofsmart loads from intelligent domestic appliances, heaters and air-conditioningunits and electric vehicles with grid for enhanced decision making. Theensemble classification methods were tested on the modified 118-bus IEEE powersystem showing that proposed technique can be employed to examine whether thepower system is secured under steady-state operating conditions.
arxiv-15600-165 | Leveraging Sentence-level Information with Encoder LSTM for Natural Language Understanding | http://arxiv.org/abs/1601.01530 | author:Gakuto Kurata, Bing Xiang, Bowen Zhou, Mo Yu category:cs.CL published:2016-01-07 summary:Recurrent Neural Network (RNN) and one of its specific architectures, LongShort-Term Memory (LSTM), have been widely used for sequence labeling. In thispaper, we first enhance LSTM-based sequence labeling to explicitly model labeldependencies. Then we propose another enhancement to incorporate the globalinformation spanning over the whole input sequence. The latter proposed method,encoder-labeler LSTM, first encodes the whole input sequence into a fixedlength vector with the encoder LSTM, and then uses this encoded vector as theinitial state of another LSTM for sequence labeling. Combining these methods,we can predict the label sequence with considering label dependencies andinformation of whole input sequence. In the experiments of a slot filling task,which is an essential component of natural language understanding, with usingthe standard ATIS corpus, we achieved the state-of-the-art F1-score of 95.66%.
arxiv-15600-166 | From Word Embeddings to Item Recommendation | http://arxiv.org/abs/1601.01356 | author:Makbule Gulcin Ozsoy category:cs.LG cs.CL cs.IR cs.SI published:2016-01-07 summary:Social network platforms can archive data produced by their users. Then, thearchived data is used to provide better services to the users. One of theservices that these platforms provide is the recommendation service.Recommendation systems can predict the future preferences of users usingvarious different techniques. One of the most popular technique forrecommendation is matrix-factorization, which uses low-rank approximation ofinput data. Similarly, word embedding methods from natural language processingliterature learn low-dimensional vector space representation of input elements.Noticing the similarities among word embedding and matrix factorizationtechniques and based on the previous works that apply techniques from textprocessing to recommendation, Word2Vec's skip-gram technique is employed tomake recommendations. The aim of this work is to make recommendation on nextcheck-in venues. Unlike previous works that use Word2Vec for recommendation, inthis work non-textual features are used. For the experiments, a Foursquarecheck-in dataset is used. The results show that use of vector spacerepresentations of items modeled by skip-gram technique is promising for makingrecommendations.
arxiv-15600-167 | Mixture of Bilateral-Projection Two-dimensional Probabilistic Principal Component Analysis | http://arxiv.org/abs/1601.01431 | author:Fujiao Ju, Yanfeng Sun, Junbin Gao, Simeng Liu, Yongli Hu category:cs.CV published:2016-01-07 summary:The probabilistic principal component analysis (PPCA) is built upon a globallinear mapping, with which it is insufficient to model complex data variation.This paper proposes a mixture of bilateral-projection probabilistic principalcomponent analysis model (mixB2DPPCA) on 2D data. With multi-components in themixture, this model can be seen as a soft cluster algorithm and has capabilityof modeling data with complex structures. A Bayesian inference scheme has beenproposed based on the variational EM (Expectation-Maximization) approach forlearning model parameters. Experiments on some publicly available databasesshow that the performance of mixB2DPPCA has been largely improved, resulting inmore accurate reconstruction errors and recognition rates than the existingPCA-based algorithms.
arxiv-15600-168 | Block-Diagonal Sparse Representation by Learning a Linear Combination Dictionary for Recognition | http://arxiv.org/abs/1601.01432 | author:Xinglin Piao, Yongli Hu, Yanfeng Sun, Junbin Gao, Baocai Yin category:cs.CV published:2016-01-07 summary:In a sparse representation based recognition scheme, it is critical to learna desired dictionary, aiming both good representational power anddiscriminative performance. In this paper, we propose a new dictionary learningmodel for recognition applications, in which three strategies are adopted toachieve these two objectives simultaneously. First, a block-diagonal constraintis introduced into the model to eliminate the correlation between classes andenhance the discriminative performance. Second, a low-rank term is adopted tomodel the coherence within classes for refining the sparse representation ofeach class. Finally, instead of using the conventional over-completedictionary, a specific dictionary constructed from the linear combination ofthe training samples is proposed to enhance the representational power of thedictionary and to improve the robustness of the sparse representation model.The proposed method is tested on several public datasets. The experimentalresults show the method outperforms most state-of-the-art methods.
arxiv-15600-169 | Learning Kernels for Structured Prediction using Polynomial Kernel Transformations | http://arxiv.org/abs/1601.01411 | author:Chetan Tonde, Ahmed Elgammal category:cs.LG stat.ML published:2016-01-07 summary:Learning the kernel functions used in kernel methods has been a vastlyexplored area in machine learning. It is now widely accepted that to obtain'good' performance, learning a kernel function is the key challenge. In thiswork we focus on learning kernel representations for structured regression. Wepropose use of polynomials expansion of kernels, referred to as Schoenbergtransforms and Gegenbaur transforms, which arise from the seminal result ofSchoenberg (1938). These kernels can be thought of as polynomial combination ofinput features in a high dimensional reproducing kernel Hilbert space (RKHS).We learn kernels over input and output for structured data, such that,dependency between kernel features is maximized. We use Hilbert-SchmidtIndependence Criterion (HSIC) to measure this. We also give an efficient,matrix decomposition-based algorithm to learn these kernel transformations, anddemonstrate state-of-the-art results on several real-world datasets.
arxiv-15600-170 | An Automaton Learning Approach to Solving Safety Games over Infinite Graphs | http://arxiv.org/abs/1601.01660 | author:Daniel Neider, Ufuk Topcu category:cs.FL cs.LG cs.LO 05C57 F.1.1; I.2.6 published:2016-01-07 summary:We propose a method to construct finite-state reactive controllers forsystems whose interactions with their adversarial environment are modeled byinfinite-duration two-player games over (possibly) infinite graphs. Theproposed method targets safety games with infinitely many states or with such alarge number of states that it would be impractical---if not impossible---forconventional synthesis techniques that work on the entire state space. Weresort to constructing finite-state controllers for such systems through anautomata learning approach, utilizing a symbolic representation of theunderlying game that is based on finite automata. Throughout the learningprocess, the learner maintains an approximation of the winning region(represented as a finite automaton) and refines it using different types ofcounterexamples provided by the teacher until a satisfactory controller can bederived (if one exists). We present a symbolic representation of safety games(inspired by regular model checking), propose implementations of the learnerand teacher, and evaluate their performance on examples motivated by roboticmotion planning in dynamic environments.
arxiv-15600-171 | Learning to Compose Neural Networks for Question Answering | http://arxiv.org/abs/1601.01705 | author:Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein category:cs.CL cs.CV cs.NE published:2016-01-07 summary:We describe a question answering model that applies to both images andstructured knowledge bases. The model uses natural language strings toautomatically assemble neural networks from a collection of composable modules.Parameters for these modules are learned jointly with network-assemblyparameters via reinforcement learning, with only (world, question, answer)triples as supervision. Our approach, which we term a dynamic neural modelnetwork, achieves state-of-the-art results on benchmark datasets in both visualand structured domains.
arxiv-15600-172 | Large Collection of Diverse Gene Set Search Queries Recapitulate Known Protein-Protein Interactions and Gene-Gene Functional Associations | http://arxiv.org/abs/1601.01653 | author:Avi Ma'ayan, Neil R. Clark category:q-bio.MN cs.AI cs.SI q-bio.GN stat.ML published:2016-01-07 summary:Popular online enrichment analysis tools from the field of molecular systemsbiology provide users with the ability to submit their experimental results asgene sets for individual analysis. Such queries are kept private, and havenever before been considered as a resource for integrative analysis. Byharnessing gene set query submissions from thousands of users, we aim todiscover biological knowledge beyond the scope of an individual study. In thiswork, we investigated a large collection of gene sets submitted to the toolEnrichr by thousands of users. Based on co-occurrence, we constructed a globalgene-gene association network. We interpret this inferred network as providinga summary of the structure present in this crowdsourced gene set library, andshow that this network recapitulates known protein-protein interactions andfunctional associations between genes. This finding implies that this networkalso offers predictive value. Furthermore, we visualize this gene-geneassociation network using a new edge-pruning algorithm that retains both thelocal and global structures of large-scale networks. Our ability to makepredictions for currently unknown gene associations, that may not be capturedby individual researchers and data sources, is a demonstration of the potentialof harnessing collective knowledge from users of popular tools in the field ofmolecular systems biology.
arxiv-15600-173 | NodIO, a JavaScript framework for volunteer-based evolutionary algorithms : first results | http://arxiv.org/abs/1601.01607 | author:Juan-J. Merelo, Mario García-Valdez, Pedro A. Castillo, Pablo García-Sánchez, P. de las Cuevas, Nuria Rico category:cs.DC cs.NE published:2016-01-07 summary:JavaScript is an interpreted language mainly known for its inclusion in webbrowsers, making them a container for rich Internet based applications. Thishas inspired its use, for a long time, as a tool for evolutionary algorithms,mainly so in browser-based volunteer computing environments. Several librarieshave also been published so far and are in use. However, the last years haveseen a resurgence of interest in the language, becoming one of the most popularand thus spawning the improvement of its implementations, which are now thefoundation of many new client-server applications. We present such anapplication for running distributed volunteer-based evolutionary algorithmexperiments, and we make a series of measurements to establish the speed ofJavaScript in evolutionary algorithms that can serve as a baseline forcomparison with other distributed computing experiments. These experiments usedifferent integer and floating point problems, and prove that the speed ofJavaScript is actually competitive with other languages commonly used by theevolutionary algorithm practitioner.
arxiv-15600-174 | State Space representation of non-stationary Gaussian Processes | http://arxiv.org/abs/1601.01544 | author:Alessio Benavoli, Marco Zaffalon category:cs.LG stat.ML published:2016-01-07 summary:The state space (SS) representation of Gaussian processes (GP) has recentlygained a lot of interest. The main reason is that it allows to compute GPsbased inferences in O(n), where $n$ is the number of observations. Thisimplementation makes GPs suitable for Big Data. For this reason, it isimportant to provide a SS representation of the most important kernels used inmachine learning. The aim of this paper is to show how to exploit the transientbehaviour of SS models to map non-stationary kernels to SS models.
arxiv-15600-175 | A pragmatic approach to multi-class classification | http://arxiv.org/abs/1601.01121 | author:Thomas Kopinski, Stéphane Magand, Uwe Handmann, Alexander Gepperth category:cs.LG published:2016-01-06 summary:We present a novel hierarchical approach to multi-class classification whichis generic in that it can be applied to different classification models (e.g.,support vector machines, perceptrons), and makes no explicit assumptions aboutthe probabilistic structure of the problem as it is usually done in multi-classclassification. By adding a cascade of additional classifiers, each of whichreceives the previous classifier's output in addition to regular input data,the approach harnesses unused information that manifests itself in the form of,e.g., correlations between predicted classes. Using multilayer perceptrons as aclassification model, we demonstrate the validity of this approach by testingit on a complex ten-class 3D gesture recognition task.
arxiv-15600-176 | Streaming Gibbs Sampling for LDA Model | http://arxiv.org/abs/1601.01142 | author:Yang Gao, Jianfei Chen, Jun Zhu category:cs.LG stat.ML published:2016-01-06 summary:Streaming variational Bayes (SVB) is successful in learning LDA models in anonline manner. However previous attempts toward developing online Monte-Carlomethods for LDA have little success, often by having much worse perplexity thantheir batch counterparts. We present a streaming Gibbs sampling (SGS) method,an online extension of the collapsed Gibbs sampling (CGS). Our empirical studyshows that SGS can reach similar perplexity as CGS, much better than SVB. Ourdistributed version of SGS, DSGS, is much more scalable than SVB mainly becausethe updates' communication complexity is small.
arxiv-15600-177 | Vehicle Classification using Transferable Deep Neural Network Features | http://arxiv.org/abs/1601.01145 | author:Yiren Zhou, Ngai-Man Cheung category:cs.CV published:2016-01-06 summary:We address vehicle detection on rear view vehicle images captured from adistance along multi-lane highways, and vehicle classification usingtransferable features from Deep Neural Network. We address the followingproblems that are specific to our application: how to utilize dash lanemarkings to assist vehicle detection, what features are useful forclassification on vehicle categories, and how to utilize Deep Neural Networkwhen the size of the labelled data is limited. Experiment results suggest ourapproach outperforms other state-of-the-art.
arxiv-15600-178 | A simple technique for improving multi-class classification with neural networks | http://arxiv.org/abs/1601.01157 | author:Thomas Kopinski, Alexander Gepperth, Uwe Handmann category:cs.LG published:2016-01-06 summary:We present a novel method to perform multi-class pattern classification withneural networks and test it on a challenging 3D hand gesture recognitionproblem. Our method consists of a standard one-against-all (OAA)classification, followed by another network layer classifying the resultingclass scores, possibly augmented by the original raw input vector. This allowsthe network to disambiguate hard-to-separate classes as the distribution ofclass scores carries considerable information as well, and is in fact oftenused for assessing the confidence of a decision. We show that by this approachwe are able to significantly boost our results, overall as well as forparticular difficult cases, on the hard 10-class gesture classification task.
arxiv-15600-179 | On Bayesian index policies for sequential resource allocation | http://arxiv.org/abs/1601.01190 | author:Emilie Kaufmann category:stat.ML published:2016-01-06 summary:This paper is about index policies for minimizing (frequentist) regret in astochastic multi-armed bandit model, that are inspired by a Bayesian view onthe problem. Our main contribution is to prove the asymptotic optimality ofBayes-UCB, an algorithm based on quantiles of posterior distributions, when therewards distributions belong to a one-dimensional exponential family, for alarge class of prior distributions. We also show that the Bayesian literaturegives new insight on what kind of exploration rates could be used infrequentist, UCB-type algorithms. Indeed, approximations of the Bayesianoptimal solution or the Finite Horizon Gittins indices suggest the introductionof two algorithms, KL-UCB + and KL-UCB-H + , whose asymptotic optimality isalso established.
arxiv-15600-180 | Part-of-Speech Tagging for Code-mixed Indian Social Media Text at ICON 2015 | http://arxiv.org/abs/1601.01195 | author:Kamal Sarkar category:cs.CL 68T50 published:2016-01-06 summary:This paper discusses the experiments carried out by us at Jadavpur Universityas part of the participation in ICON 2015 task: POS Tagging for Code-mixedIndian Social Media Text. The tool that we have developed for the task is basedon Trigram Hidden Markov Model that utilizes information from dictionary aswell as some other word level features to enhance the observation probabilitiesof the known tokens as well as unknown tokens. We submitted runs forBengali-English, Hindi-English and Tamil-English Language pairs. Our system hasbeen trained and tested on the datasets released for ICON 2015 shared task: POSTagging For Code-mixed Indian Social Media Text. In constrained mode, oursystem obtains average overall accuracy (averaged over all three languagepairs) of 75.60% which is very close to other participating two systems (76.79%for IIITH and 75.79% for AMRITA_CEN) ranked higher than our system. Inunconstrained mode, our system obtains average overall accuracy of 70.65% whichis also close to the system (72.85% for AMRITA_CEN) which obtains the highestaverage overall accuracy.
arxiv-15600-181 | Automatic 3D object detection of Proteins in Fluorescent labeled microscope images with spatial statistical analysis | http://arxiv.org/abs/1601.01216 | author:Ramin Norousi, Volker J. Schmid category:cs.CV published:2016-01-06 summary:Since manual object detection is very inaccurate and time consuming, someautomatic object detection tools have been developed in recent years. At themoment, there is no image analysis software available which provides anautomatic, objective assessment of 3D foci which is generally applicable.Complications arise from discrete foci which are very close or even come incontact to other foci, moreover they are of variable sizes and show variablesignal-to-noise, and must be analyzed fully in 3D. Therefore we introduce the3D-OSCOS (3D-Object Segmentation and Colocalization Analysis based on Spatialstatistics) algorithm which is implemented as a user-friendly toolbox forinteractive detection of 3D objects and visualization of labeled images.
arxiv-15600-182 | Memory Matters: Convolutional Recurrent Neural Network for Scene Text Recognition | http://arxiv.org/abs/1601.01100 | author:Guo Qiang, Tu Dan, Li Guohui, Lei Jun category:cs.CV published:2016-01-06 summary:Text recognition in natural scene is a challenging problem due to the manyfactors affecting text appearance. In this paper, we presents a method thatdirectly transcribes scene text images to text without needing of sophisticatedcharacter segmentation. We leverage recent advances of deep neural networks tomodel the appearance of scene text images with temporal dynamics. Specifically,we integrates convolutional neural network (CNN) and recurrent neural network(RNN) which is motivated by observing the complementary modeling capabilitiesof the two models. The main contribution of this work is investigating howtemporal memory helps in an segmentation free fashion for this specificproblem. By using long short-term memory (LSTM) blocks as hidden units, ourmodel can retain long-term memory compared with HMMs which only maintainshort-term state dependences. We conduct experiments on Street View HouseNumber dataset containing highly variable number images. The resultsdemonstrate the superiority of the proposed method over traditional HMM basedmethods.
arxiv-15600-183 | Incorporating Structural Alignment Biases into an Attentional Neural Translation Model | http://arxiv.org/abs/1601.01085 | author:Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vymolova, Kaisheng Yao, Chris Dyer, Gholamreza Haffari category:cs.CL published:2016-01-06 summary:Neural encoder-decoder models of machine translation have achieved impressiveresults, rivalling traditional translation models. However their modellingformulation is overly simplistic, and omits several key inductive biases builtinto traditional models. In this paper we extend the attentional neuraltranslation model to include structural biases from word based alignmentmodels, including positional bias, Markov conditioning, fertility and agreementover translation directions. We show improvements over a baseline attentionalmodel and standard phrase-based model over several language pairs, evaluatingon difficult languages in a low resource setting.
arxiv-15600-184 | Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism | http://arxiv.org/abs/1601.01073 | author:Orhan Firat, Kyunghyun Cho, Yoshua Bengio category:cs.CL stat.ML published:2016-01-06 summary:We propose multi-way, multilingual neural machine translation. The proposedapproach enables a single neural translation model to translate betweenmultiple languages, with a number of parameters that grows only linearly withthe number of languages. This is made possible by having a single attentionmechanism that is shared across all language pairs. We train the proposedmulti-way, multilingual model on ten language pairs from WMT'15 simultaneouslyand observe clear performance improvements over models trained on only onelanguage pair. In particular, we observe that the proposed model significantlyimproves the translation quality of low-resource language pairs.
arxiv-15600-185 | A Survey on Social Media Anomaly Detection | http://arxiv.org/abs/1601.01102 | author:Rose Yu, Huida Qiu, Zhen Wen, Ching-Yung Lin, Yan Liu category:cs.LG cs.SI published:2016-01-06 summary:Social media anomaly detection is of critical importance to prevent maliciousactivities such as bullying, terrorist attack planning, and fraud informationdissemination. With the recent popularity of social media, new types ofanomalous behaviors arise, causing concerns from various parties. While a largeamount of work have been dedicated to traditional anomaly detection problems,we observe a surge of research interests in the new realm of social mediaanomaly detection. In this paper, we present a survey on existing approaches toaddress this problem. We focus on the new type of anomalous phenomena in thesocial media and review the recent developed techniques to detect those specialtypes of anomalies. We provide a general overview of the problem domain, commonformulations, existing methodologies and potential directions. With this work,we hope to call out the attention from the research community on thischallenging problem and open up new directions that we can contribute in thefuture.
arxiv-15600-186 | Low-rank Matrix Factorization under General Mixture Noise Distributions | http://arxiv.org/abs/1601.01060 | author:Xiangyong Cao, Qian Zhao, Deyu Meng, Yang Chen, Zongben Xu category:cs.CV published:2016-01-06 summary:Many computer vision problems can be posed as learning a low-dimensionalsubspace from high dimensional data. The low rank matrix factorization (LRMF)represents a commonly utilized subspace learning strategy. Most of the currentLRMF techniques are constructed on the optimization problems using L1-norm andL2-norm losses, which mainly deal with Laplacian and Gaussian noises,respectively. To make LRMF capable of adapting more complex noise, this paperproposes a new LRMF model by assuming noise as Mixture of Exponential Power(MoEP) distributions and proposes a penalized MoEP (PMoEP) model by combiningthe penalized likelihood method with MoEP distributions. Such settingfacilitates the learned LRMF model capable of automatically fitting the realnoise through MoEP distributions. Each component in this mixture is adaptedfrom a series of preliminary super- or sub-Gaussian candidates. Moreover, byfacilitating the local continuity of noise components, we embed Markov randomfield into the PMoEP model and further propose the advanced PMoEP-MRF model. AnExpectation Maximization (EM) algorithm and a variational EM (VEM) algorithmare also designed to infer the parameters involved in the proposed PMoEP andthe PMoEP-MRF model, respectively. The superseniority of our methods isdemonstrated by extensive experiments on synthetic data, face modeling,hyperspectral image restoration and background subtraction.
arxiv-15600-187 | Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation | http://arxiv.org/abs/1601.01343 | author:Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji category:cs.CL published:2016-01-06 summary:Named Entity Disambiguation (NED) refers to the task of resolving multiplenamed entity mentions in a document to their correct references in a knowledgebase (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding methodspecifically designed for NED. The proposed method jointly maps words andentities into the same continuous vector space. We extend the skip-gram modelby using two models. The KB graph model learns the relatedness of entitiesusing the link structure of the KB, whereas the anchor context model aims toalign vectors such that similar words and entities occur close to one anotherin the vector space by leveraging KB anchors and their context words. Bycombining contexts based on the proposed embedding with standard NED features,we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL datasetand 85.2% on the TAC 2010 dataset.
arxiv-15600-188 | Recurrent Memory Networks for Language Modeling | http://arxiv.org/abs/1601.01272 | author:Ke Tran, Arianna Bisazza, Christof Monz category:cs.CL published:2016-01-06 summary:Recurrent Neural Networks (RNN) have obtained excellent result in manynatural language processing (NLP) tasks. However, understanding andinterpreting the source of this success remains a challenge. In this paper, wepropose Recurrent Memory Network (RMN), a novel RNN architecture, that not onlyamplifies the power of RNN but also facilitates our understanding of itsinternal functioning and allows us to discover underlying patterns in data. Wedemonstrate the power of RMN on language modeling and sentence completiontasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM)network on three large German, Italian, and English dataset. Additionally weperform in-depth analysis of various linguistic dimensions that RMN captures.On Sentence Completion Challenge, for which it is essential to capture sentencecoherence, our RMN obtains 69.2% accuracy, surpassing the previousstate-of-the-art by a large margin.
arxiv-15600-189 | Adaptive and Efficient Nonlinear Channel Equalization for Underwater Acoustic Communication | http://arxiv.org/abs/1601.01218 | author:Dariush Kari, Nuri Denizcan Vanli, Suleyman Serdar Kozat category:cs.LG cs.IT cs.SD math.IT published:2016-01-06 summary:We investigate underwater acoustic (UWA) channel equalization and introducehierarchical and adaptive nonlinear channel equalization algorithms that arehighly efficient and provide significantly improved bit error rate (BER)performance. Due to the high complexity of nonlinear equalizers and poorperformance of linear ones, to equalize highly difficult underwater acousticchannels, we employ piecewise linear equalizers. However, in order to achievethe performance of the best piecewise linear model, we use a tree structure tohierarchically partition the space of the received signal. Furthermore, theequalization algorithm should be completely adaptive, since due to the highlynon-stationary nature of the underwater medium, the optimal MSE equalizer aswell as the best piecewise linear equalizer changes in time. To this end, weintroduce an adaptive piecewise linear equalization algorithm that not onlyadapts the linear equalizer at each region but also learns the completehierarchical structure with a computational complexity only polynomial in thenumber of nodes of the tree. Furthermore, our algorithm is constructed todirectly minimize the final squared error without introducing any ad-hocparameters. We demonstrate the performance of our algorithms through highlyrealistic experiments performed on accurately simulated underwater acousticchannels.
arxiv-15600-190 | Shape Animation with Combined Captured and Simulated Dynamics | http://arxiv.org/abs/1601.01232 | author:Benjamin Allain, Li Wang, Jean-Sebastien Franco, Franck Hetroy, Edmond Boyer category:cs.GR cs.CV published:2016-01-06 summary:We present a novel volumetric animation generation framework to create newtypes of animations from raw 3D surface or point cloud sequence of capturedreal performances. The framework considers as input time incoherent 3Dobservations of a moving shape, and is thus particularly suitable for theoutput of performance capture platforms. In our system, a suitable virtualrepresentation of the actor is built from real captures that allows seamlesscombination and simulation with virtual external forces and objects, in whichthe original captured actor can be reshaped, disassembled or reassembled fromuser-specified virtual physics. Instead of using the dominant surface-basedgeometric representation of the capture, which is less suitable for volumetriceffects, our pipeline exploits Centroidal Voronoi tessellation decompositionsas unified volumetric representation of the real captured actor, which we showcan be used seamlessly as a building block for all processing stages, fromcapture and tracking to virtual physic simulation. The representation makes nohuman specific assumption and can be used to capture and re-simulate the actorwith props or other moving scenery elements. We demonstrate the potential ofthis pipeline for virtual reanimation of a real captured event with variousunprecedented volumetric visual effects, such as volumetric distortion,erosion, morphing, gravity pull, or collisions.
arxiv-15600-191 | Language to Logical Form with Neural Attention | http://arxiv.org/abs/1601.01280 | author:Li Dong, Mirella Lapata category:cs.CL published:2016-01-06 summary:Semantic parsing aims at mapping natural language to machine interpretablemeaning representations. Traditional approaches rely on high-quality lexicons,manually-built templates, and linguistic features which are either domain- orrepresentation-specific. In this paper, we present a general method based on anattention-enhanced sequence-to-sequence model. We encode input sentences intovector representations using recurrent neural networks, and generate theirlogical forms by conditioning the output on the encoding vectors. The model istrained in an end-to-end fashion to maximize the likelihood of target logicalforms given the natural language inputs. Experimental results on four datasetsshow that our approach performs competitively without using hand-engineeredfeatures and is easy to adapt across domains and meaning representations.
arxiv-15600-192 | Quality Adaptive Low-Rank Based JPEG Decoding with Applications | http://arxiv.org/abs/1601.01339 | author:Xiao Shu, Xiaolin Wu category:cs.CV published:2016-01-06 summary:Small compression noises, despite being transparent to human eyes, canadversely affect the results of many image restoration processes, if leftunaccounted for. Especially, compression noises are highly detrimental toinverse operators of high-boosting (sharpening) nature, such as deblurring andsuperresolution against a convolution kernel. By incorporating the non-linearDCT quantization mechanism into the formulation for image restoration, wepropose a new sparsity-based convex programming approach for joint compressionnoise removal and image restoration. Experimental results demonstratesignificant performance gains of the new approach over existing imagerestoration methods.
arxiv-15600-193 | Bayesian Non-Negative Matrix Factorization | http://arxiv.org/abs/1601.01345 | author:Pierre Alquier, Benjamin Guedj category:stat.ML math.ST stat.TH published:2016-01-06 summary:The aim of this paper is to provide some theoretical understanding ofBayesian non-negative matrix factorization methods, along with practicalimplementations. We provide a sharp oracle inequality for a quasi-Bayesianestimator, also known as the exponentially weighted aggregate (Dalalyan andTsybakov, 2008). This result holds for a very general class of priordistributions and shows how the prior affects the rate of convergence. We thendiscuss possible algorithms. A natural choice in Bayesian statistics is theGibbs sampler, used for example in Salakhutdinov and Mnih (2008). Thisalgorithm is asymptotically exact, yet it suffers from the fact that theconvergence might be very slow on large datasets. When faced with massivedatasets, a more efficient path is to use approximate methods based onoptimisation algorithms: we here describe a blockwise gradient descent which isa Bayesian version of the algorithm in Xu et al. (2012). Here again, thegeneral form of the algorithm helps to understand the role of the prior, andsome priors will clearly lead to more efficient (i.e., faster) implementations.We end the paper with a short simulation study and an application to finance.These numerical studies support our claim that the reconstruction of the matrixis usually not very sensitive to the choice of the hyperparameters whereas rankidentification is.
arxiv-15600-194 | Angrier Birds: Bayesian reinforcement learning | http://arxiv.org/abs/1601.01297 | author:Imanol Arrieta Ibarra, Bernardo Ramos, Lars Roemheld category:cs.AI cs.LG published:2016-01-06 summary:We train a reinforcement learner to play a simplified version of the gameAngry Birds. The learner is provided with a game state in a manner similar tothe output that could be produced by computer vision algorithms. We improve onthe efficiency of regular {\epsilon}-greedy Q-Learning with linear functionapproximation through more systematic exploration in Randomized Least SquaresValue Iteration (RLSVI), an algorithm that samples its policy from a posteriordistribution on optimal policies. With larger state-action spaces, efficientexploration becomes increasingly important, as evidenced by the faster learningin RLSVI.
arxiv-15600-195 | Open challenges in understanding development and evolution of speech forms: The roles of embodied self-organization, motivation and active exploration | http://arxiv.org/abs/1601.00816 | author:Pierre-Yves Oudeyer category:cs.AI cs.CL cs.CY cs.LG published:2016-01-05 summary:This article discusses open scientific challenges for understandingdevelopment and evolution of speech forms, as a commentary to Moulin-Frier etal. (Moulin-Frier et al., 2015). Based on the analysis of mathematical modelsof the origins of speech forms, with a focus on their assumptions , we studythe fundamental question of how speech can be formed out of non--speech, atboth developmental and evolutionary scales. In particular, we emphasize theimportance of embodied self-organization , as well as the role of mechanisms ofmotivation and active curiosity-driven exploration in speech formation. Finally, we discuss an evolutionary-developmental perspective of the origins ofspeech.
arxiv-15600-196 | Robust Method of Vote Aggregation and Proposition Verification for Invariant Local Features | http://arxiv.org/abs/1601.00781 | author:Grzegorz Kurzejamski, Jacek Zawistowski, Grzegorz Sarwas category:cs.CV published:2016-01-05 summary:This paper presents a method for analysis of the vote space created from thelocal features extraction process in a multi-detection system. The method isopposed to the classic clustering approach and gives a high level of controlover the clusters composition for further verification steps. Proposed methodcomprises of the graphical vote space presentation, the proposition generation,the two-pass iterative vote aggregation and the cascade filters forverification of the propositions. Cascade filters contain all of the minoralgorithms needed for effective object detection verification. The new approachdoes not have the drawbacks of the classic clustering approaches and gives asubstantial control over process of detection. Method exhibits an exceptionallyhigh detection rate in conjunction with a low false detection chance incomparison to alternative methods.
arxiv-15600-197 | Forecasting Social Navigation in Crowded Complex Scenes | http://arxiv.org/abs/1601.00998 | author:Alexandre Robicquet, Alexandre Alahi, Amir Sadeghian, Bryan Anenberg, John Doherty, Eli Wu, Silvio Savarese category:cs.CV cs.RO cs.SI published:2016-01-05 summary:When humans navigate a crowed space such as a university campus or thesidewalks of a busy street, they follow common sense rules based on socialetiquette. In this paper, we argue that in order to enable the design of newalgorithms that can take fully advantage of these rules to better solve taskssuch as target tracking or trajectory forecasting, we need to have access tobetter data in the first place. To that end, we contribute the very first largescale dataset (to the best of our knowledge) that collects images and videos ofvarious types of targets (not just pedestrians, but also bikers, skateboarders,cars, buses, golf carts) that navigate in a real-world outdoor environment suchas a university campus. We present an extensive evaluation where differentmethods for trajectory forecasting are evaluated and compared. Moreover, wepresent a new algorithm for trajectory prediction that exploits the complexityof our new dataset and allows to: i) incorporate inter-class interactions intotrajectory prediction models (e.g, pedestrian vs bike) as opposed to justintra-class interactions (e.g., pedestrian vs pedestrian); ii) model the degreeto which the social forces are regulating an interaction. We call the latter"social sensitivity"and it captures the sensitivity to which a target isresponding to a certain interaction. An extensive experimental evaluationdemonstrates the effectiveness of our novel approach.
arxiv-15600-198 | Learning Preferences for Manipulation Tasks from Online Coactive Feedback | http://arxiv.org/abs/1601.00741 | author:Ashesh Jain, Shikhar Sharma, Thorsten Joachims, Ashutosh Saxena category:cs.RO cs.AI cs.LG published:2016-01-05 summary:We consider the problem of learning preferences over trajectories for mobilemanipulators such as personal robots and assembly line robots. The preferenceswe learn are more intricate than simple geometric constraints on trajectories;they are rather governed by the surrounding context of various objects andhuman interactions in the environment. We propose a coactive online learningframework for teaching preferences in contextually rich environments. The keynovelty of our approach lies in the type of feedback expected from the user:the human user does not need to demonstrate optimal trajectories as trainingdata, but merely needs to iteratively provide trajectories that slightlyimprove over the trajectory currently proposed by the system. We argue thatthis coactive preference feedback can be more easily elicited thandemonstrations of optimal trajectories. Nevertheless, theoretical regret boundsof our algorithm match the asymptotic rates of optimal trajectory algorithms. We implement our algorithm on two high degree-of-freedom robots, PR2 andBaxter, and present three intuitive mechanisms for providing such incrementalfeedback. In our experimental evaluation we consider two context rich settings-- household chores and grocery store checkout -- and show that users are ableto train the robot with just a few feedbacks (taking only a fewminutes).\footnote{Parts of this work has been published at NIPS and ISRRconferences~\citep{Jain13,Jain13b}. This journal submission presents aconsistent full paper, and also includes the proof of regret bounds, moredetails of the robotic system, and a thorough related work.}
arxiv-15600-199 | Low-Rank Representation over the Manifold of Curves | http://arxiv.org/abs/1601.00732 | author:Stephen Tierney, Junbin Gao, Yi Guo, Zhengwu Zhang category:cs.CV cs.LG published:2016-01-05 summary:In machine learning it is common to interpret each data point as a vector inEuclidean space. However the data may actually be functional i.e.\ each datapoint is a function of some variable such as time and the function isdiscretely sampled. The naive treatment of functional data as traditionalmultivariate data can lead to poor performance since the algorithms areignoring the correlation in the curvature of each function. In this paper wepropose a method to analyse subspace structure of the functional data by usingthe state of the art Low-Rank Representation (LRR). Experimental evaluation onsynthetic and real data reveals that this method massively outperformsconventional LRR in tasks concerning functional data.
arxiv-15600-200 | Space-Time Representation of People Based on 3D Skeletal Data: A Review | http://arxiv.org/abs/1601.01006 | author:Fei Han, Brian Reily, William Hoff, Hao Zhang category:cs.CV published:2016-01-05 summary:Spatiotemporal human representation based on 3D visual perception data is arapidly growing research area. Based on the information sources, theserepresentations can be broadly categorized into two groups based on RGB-Dinformation or 3D skeleton data. Recently, skeleton-based human representationshave been intensively studied and kept attracting an increasing attention, dueto their robustness to variations of viewpoint, human body scale and motionspeed as well as the realtime, online performance. This paper presents acomprehensive survey of existing space-time representations of people based on3D skeletal data, and provides an informative categorization and analysis ofthese methods from the perspectives, including information modality,representation encoding, structure and transition, and feature engineering. Wealso provide a brief overview of skeleton acquisition devices and constructionmethods, enlist a number of public benchmark datasets with skeleton data, anddiscuss potential future research directions.
arxiv-15600-201 | Coordinate Friendly Structures, Algorithms and Applications | http://arxiv.org/abs/1601.00863 | author:Zhimin Peng, Tianyu Wu, Yangyang Xu, Ming Yan, Wotao Yin category:math.OC cs.CE cs.DC math.NA stat.ML published:2016-01-05 summary:This paper focuses on coordinate update methods, which are useful for solvingproblems involving large or high-dimensional datasets. They decompose a probleminto simple subproblems, where each updates one, or a small block of, variableswhile fixing others. These methods can deal with linear and nonlinear mappings,smooth and nonsmooth functions, as well as convex and nonconvex problems. Inaddition, they are easy to parallelize. The great performance of coordinate update methods depends on solving simplesubproblems. To derive simple subproblems for several new classes ofapplications, this paper systematically studies coordinate friendly operatorsthat perform low-cost coordinate updates. Based on the discovered coordinate friendly operators, as well as operatorsplitting techniques, we obtain new coordinate update algorithms for a varietyof problems in machine learning, image processing, as well as sub-areas ofoptimization. Several problems are treated with coordinate update for the firsttime in history. The obtained algorithms are scalable to large instancesthrough parallel and even asynchronous computing. We present numerical examplesto illustrate how effective these algorithms are.
arxiv-15600-202 | Crater Detection via Convolutional Neural Networks | http://arxiv.org/abs/1601.00978 | author:Joseph Paul Cohen, Henry Z. Lo, Tingting Lu, Wei Ding category:cs.CV published:2016-01-05 summary:Craters are among the most studied geomorphic features in the Solar Systembecause they yield important information about the past and present geologicalprocesses and provide information about the relative ages of observed geologicformations. We present a method for automatic crater detection using advancedmachine learning to deal with the large amount of satellite imagery collected.The challenge of automatically detecting craters comes from their is complexsurface because their shape erodes over time to blend into the surface.Bandeira provided a seminal dataset that embodied this challenge that is stillan unsolved pattern recognition problem to this day. There has been work tosolve this challenge based on extracting shape and contrast features and thenapplying classification models on those features. The limiting factor in thisexisting work is the use of hand crafted filters on the image such as Gabor orSobel filters or Haar features. These hand crafted methods rely on domainknowledge to construct. We would like to learn the optimal filters and featuresbased on training examples. In order to dynamically learn filters and featureswe look to Convolutional Neural Networks (CNNs) which have shown theirdominance in computer vision. The power of CNNs is that they can learn imagefilters which generate features for high accuracy classification.
arxiv-15600-203 | Optimally Pruning Decision Tree Ensembles With Feature Cost | http://arxiv.org/abs/1601.00955 | author:Feng Nan, Joseph Wang, Venkatesh Saligrama category:stat.ML cs.LG published:2016-01-05 summary:We consider the problem of learning decision rules for prediction withfeature budget constraint. In particular, we are interested in pruning anensemble of decision trees to reduce expected feature cost while maintaininghigh prediction accuracy for any test example. We propose a novel 0-1 integerprogram formulation for ensemble pruning. Our pruning formulation is general -it takes any ensemble of decision trees as input. By explicitly accounting forfeature-sharing across trees together with accuracy/cost trade-off, our methodis able to significantly reduce feature cost by pruning subtrees that introducemore loss in terms of feature cost than benefit in terms of prediction accuracygain. Theoretically, we prove that a linear programming relaxation produces theexact solution of the original integer program. This allows us to use efficientconvex optimization tools to obtain an optimally pruned ensemble for any givenbudget. Empirically, we see that our pruning algorithm significantly improvesthe performance of the state of the art ensemble method BudgetRF.
arxiv-15600-204 | End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures | http://arxiv.org/abs/1601.00770 | author:Makoto Miwa, Mohit Bansal category:cs.CL cs.LG published:2016-01-05 summary:We present a novel end-to-end neural model to extract entities and relationsbetween them. Our recurrent neural network based model captures both wordsequence and dependency tree substructure information by stacking bidirectionaltree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allowsour model to jointly represent both entities and relations with sharedparameters in a single model. We further encourage detection of entities duringtraining and use of entity information in relation extraction via entitypretraining and scheduled sampling. Our model improves over thestate-of-the-art feature-based model on end-to-end relation extraction,achieving 3.5% and 4.8% relative error reductions in F1-score on ACE2004 andACE2005, respectively. We also show a 2.5% relative error reduction in F1-scoreover the state-of-the-art convolutional neural network based model on nominalrelation classification (SemEval-2010 Task 8).
arxiv-15600-205 | Complex Decomposition of the Negative Distance kernel | http://arxiv.org/abs/1601.00925 | author:Tim vor der Brück, Steffen Eger, Alexander Mehler category:cs.LG published:2016-01-05 summary:A Support Vector Machine (SVM) has become a very popular machine learningmethod for text classification. One reason for this relates to the range ofexisting kernels which allow for classifying data that is not linearlyseparable. The linear, polynomial and RBF (Gaussian Radial Basis Function)kernel are commonly used and serve as a basis of comparison in our study. Weshow how to derive the primal form of the quadratic Power Kernel (PK) -- alsocalled the Negative Euclidean Distance Kernel (NDK) -- by means of complexnumbers. We exemplify the NDK in the framework of text categorization using theDewey Document Classification (DDC) as the target scheme. Our evaluation showsthat the power kernel produces F-scores that are comparable to the referencekernels, but is -- except for the linear kernel -- faster to compute. Finally,we show how to extend the NDK-approach by including the Mahalanobis distance.
arxiv-15600-206 | The high-conductance state enables neural sampling in networks of LIF neurons | http://arxiv.org/abs/1601.00909 | author:Mihai A. Petrovici, Ilja Bytschok, Johannes Bill, Johannes Schemmel, Karlheinz Meier category:q-bio.NC cs.NE physics.bio-ph published:2016-01-05 summary:The apparent stochasticity of in-vivo neural circuits has long beenhypothesized to represent a signature of ongoing stochastic inference in thebrain. More recently, a theoretical framework for neural sampling has beenproposed, which explains how sample-based inference can be performed bynetworks of spiking neurons. One particular requirement of this approach isthat the neural response function closely follows a logistic curve. Analytical approaches to calculating neural response functions have been thesubject of many theoretical studies. In order to make the problem tractable,particular assumptions regarding the neural or synaptic parameters are usuallymade. However, biologically significant activity regimes exist which are notcovered by these approaches: Under strong synaptic bombardment, as is often thecase in cortex, the neuron is shifted into a high-conductance state (HCS)characterized by a small membrane time constant. In this regime, synaptic timeconstants and refractory periods dominate membrane dynamics. The core idea of our approach is to separately consider two different "modes"of spiking dynamics: burst spiking and transient quiescence, in which theneuron does not spike for longer periods. We treat the former by propagatingthe PDF of the effective membrane potential from spike to spike within a burst,while using a diffusion approximation for the latter. We find that ourprediction of the neural response function closely matches simulation data.Moreover, in the HCS scenario, we show that the neural response functionbecomes symmetric and can be well approximated by a logistic function, therebyproviding the correct dynamics in order to perform neural sampling. We herebyprovide not only a normative framework for Bayesian inference in cortex, butalso powerful applications of low-power, accelerated neuromorphic systems torelevant machine learning tasks.
arxiv-15600-207 | Joint learning of ontology and semantic parser from text | http://arxiv.org/abs/1601.00901 | author:Janez Starc, Dunja Mladenić category:cs.AI cs.CL published:2016-01-05 summary:Semantic parsing methods are used for capturing and representing semanticmeaning of text. Meaning representation capturing all the concepts in the textmay not always be available or may not be sufficiently complete. Ontologiesprovide a structured and reasoning-capable way to model the content of acollection of texts. In this work, we present a novel approach to jointlearning of ontology and semantic parser from text. The method is based onsemi-automatic induction of a context-free grammar from semantically annotatedtext. The grammar parses the text into semantic trees. Both, the grammar andthe semantic trees are used to learn the ontology on several levels -- classes,instances, taxonomic and non-taxonomic relations. The approach was evaluated onthe first sentences of Wikipedia pages describing people.
arxiv-15600-208 | Gamifying Video Object Segmentation | http://arxiv.org/abs/1601.00825 | author:Simone Palazzo, Concetto Spampinato, Daniela Giordano category:cs.CV published:2016-01-05 summary:Video object segmentation can be considered as one of the most challengingcomputer vision problems. Indeed, so far, no existing solution is able toeffectively deal with the peculiarities of real-world videos, especially incases of articulated motion and object occlusions; limitations that appear moreevident when we compare their performance with the human one. However, manuallysegmenting objects in videos is largely impractical as it requires a lot ofhuman time and concentration. To address this problem, in this paper we proposean interactive video object segmentation method, which exploits, on one hand,the capability of humans to identify correctly objects in visual scenes, and onthe other hand, the collective human brainpower to solve challenging tasks. Inparticular, our method relies on a web game to collect human inputs on objectlocations, followed by an accurate segmentation phase achieved by optimizing anenergy function encoding spatial and temporal constraints between objectregions as well as human-provided input. Performance analysis carried out onchallenging video datasets with some users playing the game demonstrated thatour method shows a better trade-off between annotation times and segmentationaccuracy than interactive video annotation and automated video objectsegmentation approaches.
arxiv-15600-209 | DrMAD: Distilling Reverse-Mode Automatic Differentiation for Optimizing Hyperparameters of Deep Neural Networks | http://arxiv.org/abs/1601.00917 | author:Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, Tat-Seng Chua category:cs.LG cs.NE published:2016-01-05 summary:The performance of deep neural networks is well-known to be sensitive to thesetting of their hyperparameters. Recent advances in reverse-mode automaticdifferentiation allow for optimizing hyperparameters with gradients. Thestandard way of computing these gradients involves a forward and backward passof computations. However, the backward pass usually needs to consumeunaffordable memory to store all the intermediate variables to exactly reversethe forward training procedure. In this work we propose a simple but effectivemethod, DrMAD, to distill the knowledge of the forward pass into a shortcutpath, through which we approximately reverse the training trajectory.Experiments on several image benchmark datasets show that DrMAD is at least 45times faster and consumes 100 times less memory compared to state-of-the-artmethods for optimizing hyperparameters with minimal compromise to itseffectiveness. To the best of our knowledge, DrMAD is the first researchattempt to make it practical to automatically tune thousands of hyperparametersof deep neural networks. The code can be downloaded fromhttps://github.com/bigaidream-projects/drmad
arxiv-15600-210 | Brain4Cars: Car That Knows Before You Do via Sensory-Fusion Deep Learning Architecture | http://arxiv.org/abs/1601.00740 | author:Ashesh Jain, Hema S Koppula, Shane Soh, Bharad Raghavan, Avi Singh, Ashutosh Saxena category:cs.RO cs.CV cs.LG published:2016-01-05 summary:Advanced Driver Assistance Systems (ADAS) have made driving safer over thelast decade. They prepare vehicles for unsafe road conditions and alert driversif they perform a dangerous maneuver. However, many accidents are unavoidablebecause by the time drivers are alerted, it is already too late. Anticipatingmaneuvers beforehand can alert drivers before they perform the maneuver andalso give ADAS more time to avoid or prepare for the danger. In this work we propose a vehicular sensor-rich platform and learningalgorithms for maneuver anticipation. For this purpose we equip a car withcameras, Global Positioning System (GPS), and a computing device to capture thedriving context from both inside and outside of the car. In order to anticipatemaneuvers, we propose a sensory-fusion deep learning architecture which jointlylearns to anticipate and fuse multiple sensory streams. Our architectureconsists of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory(LSTM) units to capture long temporal dependencies. We propose a novel trainingprocedure which allows the network to predict the future given only a partialtemporal context. We introduce a diverse data set with 1180 miles of naturalfreeway and city driving, and show that we can anticipate maneuvers 3.5 secondsbefore they occur in real-time with a precision and recall of 90.5\% and 87.4\%respectively.
arxiv-15600-211 | Matrix Variate RBM and Its Applications | http://arxiv.org/abs/1601.00722 | author:Guanglei Qi, Yanfeng Sun, Junbin Gao, Yongli Hu, Jinghua Li category:cs.CV published:2016-01-05 summary:Restricted Boltzmann Machine (RBM) is an importan- t generative modelmodeling vectorial data. While applying an RBM in practice to images, the datahave to be vec- torized. This results in high-dimensional data and valu- ablespatial information has got lost in vectorization. In this paper, aMatrix-Variate Restricted Boltzmann Machine (MVRBM) model is proposed bygeneralizing the classic RBM to explicitly model matrix data. In the new RBMmodel, both input and hidden variables are in matrix forms which are connectedby bilinear transforms. The MVRBM has much less model parameters, resulting ina faster train- ing algorithm while retaining comparable performance as theclassic RBM. The advantages of the MVRBM have been demonstrated on tworeal-world applications: Image super- resolution and handwritten digitrecognition.
arxiv-15600-212 | The Role of Context Types and Dimensionality in Learning Word Embeddings | http://arxiv.org/abs/1601.00893 | author:Oren Melamud, David McClosky, Siddharth Patwardhan, Mohit Bansal category:cs.CL published:2016-01-05 summary:We provide the first extensive evaluation of how using different types ofcontext to learn skip-gram word embeddings affects performance on a wide rangeof intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsictasks tend to exhibit a clear preference to particular types of contexts andhigher dimensionality, more careful tuning is required for finding the optimalsettings for most of the extrinsic tasks that we considered. Furthermore, forthese extrinsic tasks, we find that once the benefit from increasing theembedding dimensionality is mostly exhausted, simple concatenation of wordembeddings, learned with different context types, can yield further performancegains. As an additional contribution, we propose a new variant of the skip-grammodel that learns word embeddings from weighted contexts of substitute words.
arxiv-15600-213 | Multi-Source Neural Translation | http://arxiv.org/abs/1601.00710 | author:Barret Zoph, Kevin Knight category:cs.CL published:2016-01-05 summary:We build a multi-source machine translation model and train it to maximizethe probability of a target English string given French and German sources.Using the neural encoder-decoder framework, we explore several combinationmethods and report up to +4.8 Bleu increases on top of a very strongattention-based neural translation model.
arxiv-15600-214 | Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis | http://arxiv.org/abs/1601.00706 | author:Jimei Yang, Scott Reed, Ming-Hsuan Yang, Honglak Lee category:cs.LG cs.AI cs.CV published:2016-01-05 summary:An important problem for both graphics and vision is to synthesize novelviews of a 3D object from a single image. This is particularly challenging dueto the partial observability inherent in projecting a 3D object onto the imagespace, and the ill-posedness of inferring object shape and pose. However, wecan train a neural network to address the problem if we restrict our attentionto specific object categories (in our case faces and chairs) for which we cangather ample training data. In this paper, we propose a novel recurrentconvolutional encoder-decoder network that is trained end-to-end on the task ofrendering rotated objects starting from a single image. The recurrent structureallows our model to capture long-term dependencies along a sequence oftransformations. We demonstrate the quality of its predictions for human faceson the Multi-PIE dataset and for a dataset of 3D chair models, and also showits ability to disentangle latent factors of variation (e.g., identity andpose) without using full supervision.
arxiv-15600-215 | Mutual Information and Diverse Decoding Improve Neural Machine Translation | http://arxiv.org/abs/1601.00372 | author:Jiwei Li, Dan Jurafsky category:cs.CL cs.AI published:2016-01-04 summary:Sequence-to-sequence neural translation models learn semantic and syntacticrelations between sentence pairs by optimizing the likelihood of the targetgiven the source, i.e., $p(yx)$, an objective that ignores other potentiallyuseful sources of information. We introduce an alternative objective functionfor neural MT that maximizes the mutual information between the source andtarget sentences, modeling the bi-directional dependency of sources andtargets. We implement the model with a simple re-ranking method, and alsointroduce a decoding algorithm that increases diversity in the N-best listproduced by the first pass. Applied to the WMT German/English andFrench/English tasks, the proposed models offers a consistent performance booston both standard LSTM and attention-based neural MT architectures.
arxiv-15600-216 | Fitting Spectral Decay with the $k$-Support Norm | http://arxiv.org/abs/1601.00449 | author:Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos category:cs.LG stat.ML published:2016-01-04 summary:The spectral $k$-support norm enjoys good estimation properties in low rankmatrix learning problems, empirically outperforming the trace norm. Its unitball is the convex hull of rank $k$ matrices with unit Frobenius norm. In thispaper we generalize the norm to the spectral $(k,p)$-support norm, whoseadditional parameter $p$ can be used to tailor the norm to the decay of thespectrum of the underlying model. We characterize the unit ball and weexplicitly compute the norm. We further provide a conditional gradient methodto solve regularization problems with the norm, and we derive an efficientalgorithm to compute the Euclidean projection on the unit ball in the case$p=\infty$. In numerical experiments, we show that allowing $p$ to varysignificantly improves performance over the spectral $k$-support norm onvarious matrix completion benchmarks, and better captures the spectral decay ofthe underlying model.
arxiv-15600-217 | Nonparametric Modeling of Dynamic Functional Connectivity in fMRI Data | http://arxiv.org/abs/1601.00496 | author:Søren F. V. Nielsen, Kristoffer H. Madsen, Rasmus Røge, Mikkel N. Schmidt, Morten Mørup category:stat.AP q-bio.NC stat.ML published:2016-01-04 summary:Dynamic functional connectivity (FC) has in recent years become a topic ofinterest in the neuroimaging community. Several models and methods exist forboth functional magnetic resonance imaging (fMRI) and electroencephalography(EEG), and the results point towards the conclusion that FC exhibits dynamicchanges. The existing approaches modeling dynamic connectivity have primarilybeen based on time-windowing the data and k-means clustering. We propose anon-parametric generative model for dynamic FC in fMRI that does not rely onspecifying window lengths and number of dynamic states. Rooted in Bayesianstatistical modeling we use the predictive likelihood to investigate if themodel can discriminate between a motor task and rest both within and acrosssubjects. We further investigate what drives dynamic states using the model onthe entire data collated across subjects and task/rest. We find that the numberof states extracted are driven by subject variability and preprocessingdifferences while the individual states are almost purely defined by eithertask or rest. This questions how we in general interpret dynamic FC and pointsto the need for more research on what drives dynamic FC.
arxiv-15600-218 | Learning relationships between data obtained independently | http://arxiv.org/abs/1601.00504 | author:Alexandra Carpentier, Teresa Schlueter category:stat.ML published:2016-01-04 summary:The aim of this paper is to provide a new method for learning therelationships between data that have been obtained independently. Unlikeexisting methods like matching, the proposed technique does not require anycontextual information, provided that the dependency between the variables ofinterest is monotone. It can therefore be easily combined with matching inorder to exploit the advantages of both methods. This technique can bedescribed as a mix between quantile matching, and deconvolution. We provide forit a theoretical and an empirical validation.
arxiv-15600-219 | Kernel Sparse Subspace Clustering on Symmetric Positive Definite Manifolds | http://arxiv.org/abs/1601.00414 | author:Ming Yin, Yi Guo, Junbin Gao, Zhaoshui He, Shengli Xie category:cs.CV published:2016-01-04 summary:Sparse subspace clustering (SSC), as one of the most successful subspaceclustering methods, has achieved notable clustering accuracy in computer visiontasks. However, SSC applies only to vector data in Euclidean space. As such,there is still no satisfactory approach to solve subspace clustering by ${\itself-expressive}$ principle for symmetric positive definite (SPD) matriceswhich is very useful in computer vision. In this paper, by embedding the SPDmatrices into a Reproducing Kernel Hilbert Space (RKHS), a kernel subspaceclustering method is constructed on the SPD manifold through an appropriateLog-Euclidean kernel, termed as kernel sparse subspace clustering on the SPDRiemannian manifold (KSSCR). By exploiting the intrinsic Riemannian geometrywithin data, KSSCR can effectively characterize the geodesic distance betweenSPD matrices to uncover the underlying subspace structure. Experimental resultson two famous database demonstrate that the proposed method achieves betterclustering results than the state-of-the-art approaches.
arxiv-15600-220 | Variational Inference: A Review for Statisticians | http://arxiv.org/abs/1601.00670 | author:David M. Blei, Alp Kucukelbir, Jon D. McAuliffe category:stat.CO cs.LG stat.ML published:2016-01-04 summary:One of the core problems of modern statistics is to approximatedifficult-to-compute probability distributions. This problem is especiallyimportant in Bayesian statistics, which frames all inference about unknownquantities as a calculation about the posterior. In this paper, we reviewvariational inference (VI), a method from machine learning that approximatesprobability distributions through optimization. VI has been used in myriadapplications and tends to be faster than classical methods, such as Markovchain Monte Carlo sampling. The idea behind VI is to first posit a family ofdistributions and then to find the member of that family which is close to thetarget. Closeness is measured by Kullback-Leibler divergence. We review theideas behind mean-field variational inference, discuss the special case of VIapplied to exponential family models, present a full example with a Bayesianmixture of Gaussians, and derive a variant that uses stochastic optimization toscale up to massive data. We discuss modern research in VI and highlightimportant open problems. VI is powerful, but it is not yet well understood. Ourhope in writing this paper is to catalyze statistical research on thiswidely-used class of algorithms.
arxiv-15600-221 | Approximate Message Passing with Nearest Neighbor Sparsity Pattern Learning | http://arxiv.org/abs/1601.00543 | author:Xiangming Meng, Sheng Wu, Linling Kuang, Defeng, Huang, Jianhua Lu category:cs.IT cs.LG math.IT published:2016-01-04 summary:We consider the problem of recovering clustered sparse signals with no priorknowledge of the sparsity pattern. Beyond simple sparsity, signals of interestoften exhibits an underlying sparsity pattern which, if leveraged, can improvethe reconstruction performance. However, the sparsity pattern is usuallyunknown a priori. Inspired by the idea of k-nearest neighbor (k-NN) algorithm,we propose an efficient algorithm termed approximate message passing withnearest neighbor sparsity pattern learning (AMP-NNSPL), which learns thesparsity pattern adaptively. AMP-NNSPL specifies a flexible spike and slabprior on the unknown signal and, after each AMP iteration, sets the sparseratios as the average of the nearest neighbor estimates via expectationmaximization (EM). Experimental results on both synthetic and real datademonstrate the superiority of our proposed algorithm both in terms ofreconstruction performance and computational complexity.
arxiv-15600-222 | NFL Play Prediction | http://arxiv.org/abs/1601.00574 | author:Brendan Teich, Roman Lutz, Valentin Kassarnig category:cs.LG published:2016-01-04 summary:Based on NFL game data we try to predict the outcome of a play in multipledifferent ways. An application of this is the following: by plugging in variousplay options one could determine the best play for a given situation in realtime. While the outcome of a play can be described in many ways we had the mostpromising results with a newly defined measure that we call "progress". We seethis work as a first step to include predictive analysis into NFL playcalling.
arxiv-15600-223 | Multi-task CNN Model for Attribute Prediction | http://arxiv.org/abs/1601.00400 | author:Abrar H. Abdulnabi, Gang Wang, Jiwen Lu, Kui Jia category:cs.CV published:2016-01-04 summary:This paper proposes a joint multi-task learning algorithm to better predictattributes in images using deep convolutional neural networks (CNN). Weconsider learning binary semantic attributes through a multi-task CNN model,where each CNN will predict one binary attribute. The multi-task learningallows CNN models to simultaneously share visual knowledge among differentattribute categories. Each CNN will generate attribute-specific featurerepresentations, and then we apply multi-task learning on the features topredict their attributes. In our multi-task framework, we propose a method todecompose the overall model's parameters into a latent task matrix andcombination matrix. Furthermore, under-sampled classifiers can leverage sharedstatistics from other classifiers to improve their performance. Naturalgrouping of attributes is applied such that attributes in the same group areencouraged to share more knowledge. Meanwhile, attributes in different groupswill generally compete with each other, and consequently share less knowledge.We show the effectiveness of our method on two popular attribute datasets.
arxiv-15600-224 | Robust non-linear regression analysis: A greedy approach employing kernels and application to image denoising | http://arxiv.org/abs/1601.00595 | author:George Papageorgiou, Pantelis Bouboulis, Sergios Theodoridis category:cs.LG stat.ML published:2016-01-04 summary:We consider the task of robust non-linear estimation in the presence of bothbounded noise and outliers. Assuming that the unknown non-linear functionbelongs to a Reproducing Kernel Hilbert Space (RKHS), our goal is to accuratelyestimate the coefficients of the kernel regression matrix. Due to the existenceof outliers, common techniques such as the Kernel Ridge Regression (KRR), orthe Support Vector Regression (SVR) turn out to be inadequate. Instead, weemploy sparse modeling arguments to model and estimate the outliers, adopting agreedy approach. In particular, the proposed robust scheme, i.e., Kernel GreedyAlgorithm for Robust Denoising (KGARD), is a modification of the classicalOrthogonal Matching Pursuit (OMP) algorithm. In a nutshell, the proposed schemealternates between a KRR task and an OMP-like selection step. Convergenceproperties as well as theoretical results concerning the identification of theoutliers are provided. Moreover, KGARD is compared against other cutting edgemethods (using toy examples) to demonstrate its performance and verify theaforementioned theoretical results. Finally, the proposed robust estimationframework is applied to the task of image denoising, showing that it canenhance the denoising process significantly, when outliers are present.
arxiv-15600-225 | Automatic Detection and Decoding of Photogrammetric Coded Targets | http://arxiv.org/abs/1601.00396 | author:Udaya Wijenayake, Sung-In Choi, Soon-Yong Park category:cs.CV published:2016-01-04 summary:Close-range Photogrammetry is widely used in many industries because of thecost effectiveness and efficiency of the technique. In this research, weintroduce an automated coded target detection method which can be used toenhance the efficiency of the Photogrammetry.
arxiv-15600-226 | On the Reducibility of Submodular Functions | http://arxiv.org/abs/1601.00393 | author:Jincheng Mei, Hao Zhang, Bao-Liang Lu category:cs.LG stat.ML published:2016-01-04 summary:The scalability of submodular optimization methods is critical for theirusability in practice. In this paper, we study the reducibility of submodularfunctions, a property that enables us to reduce the solution space ofsubmodular optimization problems without performance loss. We introduce theconcept of reducibility using marginal gains. Then we show that by addingperturbation, we can endow irreducible functions with reducibility, based onwhich we propose the perturbation-reduction optimization framework. Ourtheoretical analysis proves that given the perturbation scales, thereducibility gain could be computed, and the performance loss has additiveupper bounds. We further conduct empirical studies and the results demonstratethat our proposed framework significantly accelerates existing optimizationmethods for irreducible submodular functions with a cost of only smallperformance losses.
arxiv-15600-227 | Multimodal Classification of Events in Social Media | http://arxiv.org/abs/1601.00599 | author:Matthias Zeppelzauer, Daniel Schopfhauser category:cs.CV cs.IR cs.MM published:2016-01-04 summary:A large amount of social media hosted on platforms like Flickr and Instagramis related to social events. The task of social event classification refers tothe distinction of event and non-event-related content as well as theclassification of event types (e.g. sports events, concerts, etc.). In thispaper, we provide an extensive study of textual, visual, as well as multimodalrepresentations for social event classification. We investigate strengths andweaknesses of the modalities and study synergy effects between the modalities.Experimental results obtained with our multimodal representation outperformstate-of-the-art methods and provide a new baseline for future research.
arxiv-15600-228 | Distant IE by Bootstrapping Using Lists and Document Structure | http://arxiv.org/abs/1601.00620 | author:Lidong Bing, Mingyang Ling, Richard C. Wang, William W. Cohen category:cs.CL published:2016-01-04 summary:Distant labeling for information extraction (IE) suffers from noisy trainingdata. We describe a way of reducing the noise associated with distant IE byidentifying coupling constraints between potential instance labels. As oneexample of coupling, items in a list are likely to have the same label. Asecond example of coupling comes from analysis of document structure: in somecorpora, sections can be identified such that items in the same section arelikely to have the same label. Such sections do not exist in all corpora, butwe show that augmenting a large corpus with coupling constraints from even asmall, well-structured corpus can improve performance substantially, doublingF1 on one task.
arxiv-15600-229 | Scalable Models for Computing Hierarchies in Information Networks | http://arxiv.org/abs/1601.00626 | author:Baoxu Shi, Tim Weninger category:cs.AI cs.DL cs.LG published:2016-01-04 summary:Information hierarchies are organizational structures that often used toorganize and present large and complex information as well as provide amechanism for effective human navigation. Fortunately, many statistical andcomputational models exist that automatically generate hierarchies; however,the existing approaches do not consider linkages in information {\em networks}that are increasingly common in real-world scenarios. Current approaches alsotend to present topics as an abstract probably distribution over words, etcrather than as tangible nodes from the original network. Furthermore, thestatistical techniques present in many previous works are not yet capable ofprocessing data at Web-scale. In this paper we present the HierarchicalDocument Topic Model (HDTM), which uses a distributed vertex-programmingprocess to calculate a nonparametric Bayesian generative model. Experiments onthree medium size data sets and the entire Wikipedia dataset show that HDTM caninfer accurate hierarchies even over large information networks.
arxiv-15600-230 | Nonlinear Hebbian learning as a unifying principle in receptive field formation | http://arxiv.org/abs/1601.00701 | author:Carlos S. N. Brito, Wulfram Gerstner category:q-bio.NC cs.LG published:2016-01-04 summary:The development of sensory receptive fields has been modeled in the past by avariety of models including normative models such as sparse coding orindependent component analysis and bottom-up models such as spike-timingdependent plasticity or the Bienenstock-Cooper-Munro model of synapticplasticity. Here we show that the above variety of approaches can all beunified into a single common principle, namely Nonlinear Hebbian Learning. WhenNonlinear Hebbian Learning is applied to natural images, receptive field shapeswere strongly constrained by the input statistics and preprocessing, butexhibited only modest variation across different choices of nonlinearities inneuron models or synaptic plasticity rules. Neither overcompleteness nor sparsenetwork activity are necessary for the development of localized receptivefields. The analysis of alternative sensory modalities such as auditory modelsor V2 development lead to the same conclusions. In all examples, receptivefields can be predicted a priori by reformulating an abstract model asnonlinear Hebbian learning. Thus nonlinear Hebbian learning and naturalstatistics can account for many aspects of receptive field formation acrossmodels and sensory modalities.
arxiv-15600-231 | Contrastive Entropy: A new evaluation metric for unnormalized language models | http://arxiv.org/abs/1601.00248 | author:Kushal Arora, Anand Rangarajan category:cs.CL published:2016-01-03 summary:Perplexity (per word) is the most widely used metric for evaluating languagemodels. Despite this, there has been no dearth of criticism for this metric.Most of these criticisms center around lack of correlation with extrinsicmetrics like word error rate (WER), dependence upon shared vocabulary for modelcomparison and unsuitability for unnormalized language model evaluation. Inthis paper, we address the last problem and propose a new discriminativeentropy based intrinsic metric that works for both traditional word levelmodels and unnormalized language models like sentence level models. We alsopropose a discriminatively trained sentence level interpretation of recurrentneural network based language model (RNN) as an example of unnormalizedsentence level model. We demonstrate that for word level models, contrastiveentropy shows a strong correlation with perplexity. We also observe that whentrained at lower distortion levels, sentence level RNN considerably outperformstraditional RNNs on this new metric.
arxiv-15600-232 | Dimensionality-Dependent Generalization Bounds for $k$-Dimensional Coding Schemes | http://arxiv.org/abs/1601.00238 | author:Tongliang Liu, Dacheng Tao, Dong Xu category:stat.ML cs.LG published:2016-01-03 summary:The $k$-dimensional coding schemes refer to a collection of methods thatattempt to represent data using a set of representative $k$-dimensionalvectors, and include non-negative matrix factorization, dictionary learning,sparse coding, $k$-means clustering and vector quantization as special cases.Previous generalization bounds for the reconstruction error of the$k$-dimensional coding schemes are mainly dimensionality independent. A majoradvantage of these bounds is that they can be used to analyze thegeneralization error when data is mapped into an infinite- or high-dimensionalfeature space. However, many applications use finite-dimensional data features.Can we obtain dimensionality-dependent generalization bounds for$k$-dimensional coding schemes that are tighter than dimensionality-independentbounds when data is in a finite-dimensional feature space? The answer ispositive. In this paper, we address this problem and derive adimensionality-dependent generalization bound for $k$-dimensional codingschemes by bounding the covering number of the loss function class induced bythe reconstruction error. The bound is of order$\mathcal{O}\left(\left(mk\ln(mkn)/n\right)^{\lambda_n}\right)$, where $m$ isthe dimension of features, $k$ is the number of the columns in the linearimplementation of coding schemes, $n$ is the size of sample, $\lambda_n>0.5$when $n$ is finite and $\lambda_n=0.5$ when $n$ is infinite. We show that ourbound can be tighter than previous results, because it avoids inducing theworst-case upper bound on $k$ of the loss function and converges faster. Theproposed generalization bound is also applied to some specific coding schemesto demonstrate that the dimensionality-dependent bound is an indispensablecomplement to these dimensionality-independent generalization bounds.
arxiv-15600-233 | Sparse Diffusion Steepest-Descent for One Bit Compressed Sensing in Wireless Sensor Networks | http://arxiv.org/abs/1601.00350 | author:Hadi Zayyani, Mehdi Korki, Farrokh Marvasti category:stat.ML cs.IT cs.LG math.IT published:2016-01-03 summary:This letter proposes a sparse diffusion steepest-descent algorithm for onebit compressed sensing in wireless sensor networks. The approach exploits thediffusion strategy from distributed learning in the one bit compressed sensingframework. To estimate a common sparse vector cooperatively from only the signof measurements, steepest-descent is used to minimize the suitable global andlocal convex cost functions. A diffusion strategy is suggested for distributivelearning of the sparse vector. Simulation results show the effectiveness of theproposed distributed algorithm compared to the state-of-the-art nondistributive algorithms in the one bit compressed sensing framework.
arxiv-15600-234 | A simple and low redundancy method of image compressed sampling | http://arxiv.org/abs/1601.00311 | author:Leonid Yaroslavsky category:cs.CV physics.optics published:2016-01-03 summary:A problem is addressed of minimization of the number of measurements neededfor image acquisition and reconstruction with a given accuracy. In last severalyears, the compressed sensing approach to solving this problem was advanced,which promises reducing the number of required measurements by means ofobtaining sparse approximations of images. However, the number of measurementsrequired by compressive sensing substantially exceeds the theoretical minimumdefined by sparsity of the image sparse approximation. In the paper, a samplingtheory based method of image sampling is suggested that represents a practicaland substantially more economical alternative to the compressed sensingapproach. Presented and discussed are also results of experimental verificationof the method, its possible applicability extensions and some its limitations.
arxiv-15600-235 | Image Resolution Enhancement by Using Interpolation Followed by Iterative Back Projection | http://arxiv.org/abs/1601.00260 | author:Pejman Rasti, Hasan Demirel, Gholamreza Anbarjafari category:cs.CV published:2016-01-03 summary:In this paper, we propose a new super resolution technique based on theinterpolation followed by registering them using iterative back projection(IBP). Low resolution images are being interpolated and then the interpolatedimages are being registered in order to generate a sharper high resolutionimage. The proposed technique has been tested on Lena, Elaine, Pepper, andBaboon. The quantitative peak signal-to-noise ratio (PSNR) and structuralsimilarity index (SSIM) results as well as the visual results show thesuperiority of the proposed technique over the conventional and state-of-artimage super resolution techniques. For Lena's image, the PSNR is 6.52 dB higherthan the bicubic interpolation.
arxiv-15600-236 | Supervised Dimensionality Reduction via Distance Correlation Maximization | http://arxiv.org/abs/1601.00236 | author:Praneeth Vepakomma, Chetan Tonde, Ahmed Elgammal category:cs.LG stat.ML published:2016-01-03 summary:In our work, we propose a novel formulation for supervised dimensionalityreduction based on a nonlinear dependency criterion called Statistical DistanceCorrelation, Szekely et. al. (2007). We propose an objective which is free ofdistributional assumptions on regression variables and regression modelassumptions. Our proposed formulation is based on learning a low-dimensionalfeature representation $\mathbf{z}$, which maximizes the squared sum ofDistance Correlations between low dimensional features $\mathbf{z}$ andresponse $y$, and also between features $\mathbf{z}$ and covariates$\mathbf{x}$. We propose a novel algorithm to optimize our proposed objectiveusing the Generalized Minimization Maximizaiton method of \Parizi et. al.(2015). We show superior empirical results on multiple datasets proving theeffectiveness of our proposed approach over several relevant state-of-the-artsupervised dimensionality reduction methods.
arxiv-15600-237 | A Unified Approach for Learning the Parameters of Sum-Product Networks | http://arxiv.org/abs/1601.00318 | author:Han Zhao, Pascal Poupart category:cs.LG cs.AI published:2016-01-03 summary:We present a unified approach for learning the parameters of Sum-Productnetworks (SPNs). We prove that any complete and decomposable SPN is equivalentto a mixture of trees where each tree corresponds to a product of univariatedistributions. Based on the mixture model perspective, we characterize theobjective function when learning SPNs based on the maximum likelihoodestimation (MLE) principle and show that the optimization problem can beformulated as a signomial program. Both the projected gradient descent (PGD)and the exponentiated gradient (EG) in this setting can be viewed as firstorder approximations of the signomial program after proper transformation ofthe objective function. Based on the signomial program formulation, weconstruct two parameter learning algorithms for SPNs by using sequentialmonomial approximations (SMA) and the concave-convex procedure (CCCP),respectively. The two proposed methods naturally admit multiplicative updates,hence effectively avoiding the projection operation. With the help of theunified framework, we also show that, in the case of SPNs, CCCP leads to thesame algorithm as Expectation Maximization (EM) despite the fact that they aredifferent in general. Extensive experiments on 20 data sets demonstrate theeffectiveness and efficiency of the two proposed approaches for learning SPNs.We also show that the proposed methods can improve the performance of structurelearning and yield state-of-the-art results.
arxiv-15600-238 | A fractal dimension based optimal wavelet packet analysis technique for classification of meningioma brain tumours | http://arxiv.org/abs/1601.00211 | author:Omar S. Al-Kadi category:cs.CV published:2016-01-02 summary:With the heterogeneous nature of tissue texture, using a single resolutionapproach for optimum classification might not suffice. In contrast, amultiresolution wavelet packet analysis can decompose the input signal into aset of frequency subbands giving the opportunity to characterise the texture atthe appropriate frequency channel. An adaptive best bases algorithm for optimalbases selection for meningioma histopathological images is proposed, viaapplying the fractal dimension (FD) as the bases selection criterion in atree-structured manner. Thereby, the most significant subband that betteridentifies texture discontinuities will only be chosen for furtherdecomposition, and its fractal signature would represent the extracted featurevector for classification. The best basis selection using the FD outperformedthe energy based selection approaches, achieving an overall classificationaccuracy of 91.25% as compared to 83.44% and 73.75% for the co-occurrencematrix and energy texture signatures; respectively.
arxiv-15600-239 | Supervised Texture Segmentation: A Comparative Study | http://arxiv.org/abs/1601.00212 | author:Omar S. Al-Kadi category:cs.CV published:2016-01-02 summary:This paper aims to compare between four different types of feature extractionapproaches in terms of texture segmentation. The feature extraction methodsthat were used for segmentation are Gabor filters (GF), Gaussian Markov randomfields (GMRF), run-length matrix (RLM) and co-occurrence matrix (GLCM). It wasshown that the GF performed best in terms of quality of segmentation while theGLCM localises the texture boundaries better as compared to the other methods.
arxiv-15600-240 | A Submodule Clustering Method for Multi-way Data by Sparse and Low-Rank Representation | http://arxiv.org/abs/1601.00149 | author:Xinglin Piao, Yongli Hu, Junbin Gao, Yanfeng Sun, Zhouchen Lin category:cs.CV published:2016-01-02 summary:A new submodule clustering method via sparse and low-rank representation formulti-way data is proposed in this paper. Instead of reshaping multi-way datainto vectors, this method maintains their natural orders to preserve dataintrinsic structures, e.g., image data kept as matrices. To implementclustering, the multi-way data, viewed as tensors, are represented by theproposed tensor sparse and low-rank model to obtain its submodulerepresentation, called a free module, which is finally used for spectralclustering. The proposed method extends the conventional subspace clusteringmethod based on sparse and low-rank representation to multi-way data submoduleclustering by combining t-product operator. The new method is tested on severalpublic datasets, including synthetical data, video sequences and toy images.The experiments show that the new method outperforms the state-of-the-artmethods, such as Sparse Subspace Clustering (SSC), Low-Rank Representation(LRR), Ordered Subspace Clustering (OSC), Robust Latent Low Rank Representation(RobustLatLRR) and Sparse Submodule Clustering method (SSmC).
arxiv-15600-241 | Joint Estimation of Precision Matrices in Heterogeneous Populations | http://arxiv.org/abs/1601.00142 | author:Takumi Saegusa, Ali Shojaie category:stat.ML published:2016-01-02 summary:We introduce a general framework for estimation of inverse covariance, orprecision, matrices from heterogeneous populations. The proposed framework usesa Laplacian shrinkage penalty to encourage similarity among estimates fromdisparate, but related, subpopulations, while allowing for differences amongmatrices. We propose an efficient alternating direction method of multipliers(ADMM) algorithm for parameter estimation, as well as its extension for fastercomputation in high dimensions by thresholding the empirical covariance matrixto identify the joint block diagonal structure in the estimated precisionmatrices. We establish both variable selection and norm consistency of theproposed estimator for distributions with exponential or polynomial tails.Further, to extend the applicability of the method to the settings with unknownpopulations structure, we propose a Laplacian penalty based on hierarchicalclustering, and discuss conditions under which this data-driven choice resultsin consistent estimation of precision matrices in heterogenous populations.Extensive numerical studies and applications to gene expression data fromsubtypes of cancer with distinct clinical outcomes indicate the potentialadvantages of the proposed method over existing approaches.
arxiv-15600-242 | Susceptibility of texture measures to noise: an application to lung tumor CT images | http://arxiv.org/abs/1601.00210 | author:O. S. Al-Kadi, D. Watson category:cs.CV published:2016-01-02 summary:Five different texture methods are used to investigate their susceptibilityto subtle noise occurring in lung tumor Computed Tomography (CT) images causedby acquisition and reconstruction deficiencies. Noise of Gaussian and Rayleighdistributions with varying mean and variance was encountered in the analyzed CTimages. Fisher and Bhattacharyya distance measures were used to differentiatebetween an original extracted lung tumor region of interest (ROI) with afiltered and noisy reconstructed versions. Through examining the texturecharacteristics of the lung tumor areas by five different texture measures, itwas determined that the autocovariance measure was least affected and the graylevel co-occurrence matrix was the most affected by noise. Depending on theselected ROI size, it was concluded that the number of extracted features fromeach texture measure increases susceptibility to noise.
arxiv-15600-243 | A Unified Framework for Compositional Fitting of Active Appearance Models | http://arxiv.org/abs/1601.00199 | author:Joan Alabort-i-Medina, Stefanos Zafeiriou category:cs.CV published:2016-01-02 summary:Active Appearance Models (AAMs) are one of the most popular andwell-established techniques for modeling deformable objects in computer vision.In this paper, we study the problem of fitting AAMs using CompositionalGradient Descent (CGD) algorithms. We present a unified and complete view ofthese algorithms and classify them with respect to three main characteristics:i) cost function; ii) type of composition; and iii) optimization method.Furthermore, we extend the previous view by: a) proposing a novel Bayesian costfunction that can be interpreted as a general probabilistic formulation of thewell-known project-out loss; b) introducing two new types of composition,asymmetric and bidirectional, that combine the gradients of both image andappearance model to derive better conver- gent and more robust CGD algorithms;and c) providing new valuable insights into existent CGD algorithms byreinterpreting them as direct applications of the Schur complement and theWiberg method. Finally, in order to encourage open research and facilitatefuture comparisons with our work, we make the implementa- tion of thealgorithms studied in this paper publicly available as part of the MenpoProject.
arxiv-15600-244 | An Improved Intelligent Agent for Mining Real-Time Databases Using Modified Cortical Learning Algorithms | http://arxiv.org/abs/1601.00191 | author:N. E. Osegi category:cs.NE published:2016-01-02 summary:Cortical Learning Algorithms based on the Hierarchical Temporal Memory, HTMhave been developed by Numenta Incorporation from which variations andmodifications are currently being investigated upon. HTM offers better promisesas a future computational model of the neocortex the seat of intelligence inthe brain. Currently, intelligent agents are embedded in almost every modernday electronic system found in homes, offices and industries worldwide. In thispaper, we present a first step in realising useful HTM like applicationsspecifically for mining a synthetic and real time dataset based on a novelintelligent agent framework, and demonstrate how a modified version of thisvery important computational technique will lead to improved recognition.
arxiv-15600-245 | Sentiment/Subjectivity Analysis Survey for Languages other than English | http://arxiv.org/abs/1601.00087 | author:Mohammed Korayem category:cs.CL published:2016-01-01 summary:Subjective and sentiment analysis has gained considerable attention recently.Most of the resources and systems built so far are done for English. The needfor designing systems for other languages is increasing. This paper surveysdifferent ways used for building systems for subjective and sentiment analysisfor languages other than English. There are three different types of systemsused for building these systems. The first (and the best) one is the languagespecific systems. The second type of systems involves reusing or transferringsentiment resources from English to the target language. The third type ofmethods is based on using language independent methods. The paper presents aseparate section devoted to Arabic sentiment analysis.
arxiv-15600-246 | GPU-Based Fuzzy C-Means Clustering Algorithm for Image Segmentation | http://arxiv.org/abs/1601.00072 | author:Mishal Almazrooie, Mogana Vadiveloo, Rosni Abdullah category:cs.DC cs.CV published:2016-01-01 summary:In this paper, a fast and practical GPU-based implementation of FuzzyC-Means(FCM) clustering algorithm for image segmentation is proposed. First, anextensive analysis is conducted to study the dependency among the image pixelsin the algorithm for parallelization. The proposed GPU-based FCM has beentested on digital brain simulated dataset to segment white matter(WM), graymatter(GM) and cerebrospinal fluid (CSF) soft tissue regions. The executiontime of the sequential FCM is 519 seconds for an image dataset with the size of1MB. While the proposed GPU-based FCM requires only 2.33 seconds for thesimilar size of image dataset. An estimated 245-fold speedup is measured forthe data size of 40 KB on a CUDA device that has 448 processors.
arxiv-15600-247 | Stochastic Neural Networks with Monotonic Activation Functions | http://arxiv.org/abs/1601.00034 | author:Siamak Ravanbakhsh, Barnabas Poczos, Jeff Schneider, Dale Schuurmans, Russell Greiner category:stat.ML cs.LG cs.NE published:2016-01-01 summary:We propose a Laplace approximation that creates a stochastic unit from anysmooth monotonic activation function, using only Gaussian noise. This paperinvestigates the application of this stochastic approximation in training afamily of Restricted Boltzmann Machines (RBM) that are closely linked toBregman divergences. This family, that we call exponential family RBM(Exp-RBM), is a subset of the exponential family Harmoniums that expressesfamily members through a choice of smooth monotonic non-linearity for eachneuron. Using contrastive divergence along with our Gaussian approximation, weshow that Exp-RBM can learn useful representations using novel stochasticunits.
arxiv-15600-248 | Discriminative Sparsity for Sonar ATR | http://arxiv.org/abs/1601.00119 | author:John McKay, Raghu Raj, Vishal Monga, Jason Isaacs category:cs.CV published:2016-01-01 summary:Advancements in Sonar image capture have enabled researchers to applysophisticated object identification algorithms in order to locate targets ofinterest in images such as mines. Despite progress in this field, modern sonarautomatic target recognition (ATR) approaches lack robustness to the amount ofnoise one would expect in real-world scenarios, the capability to handleblurring incurred from the physics of image capture, and the ability to excelwith relatively few training samples. We address these challenges by adaptingmodern sparsity-based techniques with dictionaries comprising of training fromeach class. We develop new discriminative (as opposed to generative) sparserepresentations which can help automatically classify targets in Sonar imaging.Using a simulated SAS data set from the Naval Surface Warfare Center (NSWC), weobtained compelling classification rates for multi-class problems even in caseswith considerable noise and sparsity in training samples.
arxiv-15600-249 | Demystifying Symmetric Smoothing Filters | http://arxiv.org/abs/1601.00088 | author:Stanley H. Chan, Todd Zickler, Yue M. Lu category:cs.CV published:2016-01-01 summary:Many patch-based image denoising algorithms can be formulated as applying asmoothing filter to the noisy image. Expressed as matrices, the smoothingfilters must be row normalized so that each row sums to unity. Surprisingly, ifwe apply a column normalization before the row normalization, the performanceof the smoothing filter can often be significantly improved. Prior works showedthat such performance gain is related to the Sinkhorn-Knopp balancingalgorithm, an iterative procedure that symmetrizes a row-stochastic matrix to adoubly-stochastic matrix. However, a complete understanding of the performancegain phenomenon is still lacking. In this paper, we study the performance gain phenomenon from a statisticallearning perspective. We show that Sinkhorn-Knopp is equivalent to anExpectation-Maximization (EM) algorithm of learning a Product of Gaussians(PoG) prior of the image patches. By establishing the correspondence betweenthe steps of Sinkhorn-Knopp and the EM algorithm, we provide a geometricalinterpretation of the symmetrization process. The new PoG model also allows usto develop a new denoising algorithm called Product of Gaussian Non-Local-Means(PoG-NLM). PoG-NLM is an extension of the Sinkhorn-Knopp and is ageneralization of the classical non-local means. Despite its simpleformulation, PoG-NLM outperforms many existing smoothing filters and has asimilar performance compared to BM3D.
arxiv-15600-250 | Practical Algorithms for Learning Near-Isometric Linear Embeddings | http://arxiv.org/abs/1601.00062 | author:Jerry Luo, Kayla Shapiro, Hao-Jun Michael Shi, Qi Yang, Kan Zhu category:stat.ML cs.LG math.OC 90C90 published:2016-01-01 summary:We propose two practical non-convex approaches for learning near-isometric,linear embeddings of finite sets of data points. Given a set of training points$\mathcal{X}$, we consider the secant set $S(\mathcal{X})$ that consists of allpairwise difference vectors of $\mathcal{X}$, normalized to lie on the unitsphere. The problem can be formulated as finding a symmetric and positivesemi-definite matrix $\boldsymbol{\Psi}$ that preserves the norms of all thevectors in $S(\mathcal{X})$ up to a distortion parameter $\delta$. Motivated bynon-negative matrix factorization, we reformulate our problem into a Frobeniusnorm minimization problem, which is solved by the Alternating Direction Methodof Multipliers (ADMM) and develop an algorithm, FroMax. Another method solvesfor a projection matrix $\boldsymbol{\Psi}$ by minimizing the restrictedisometry property (RIP) directly over the set of symmetric, postivesemi-definite matrices. Applying ADMM and a Moreau decomposition on a proximalmapping, we develop another algorithm, NILE-Pro, for dimensionality reduction.FroMax is shown to converge faster for smaller $\delta$ while NILE-Proconverges faster for larger $\delta$. Both non-convex approaches are thenempirically demonstrated to be more computationally efficient than prior convexapproaches for a number of applications in machine learning and signalprocessing.
arxiv-15600-251 | Solving the G-problems in less than 500 iterations: Improved efficient constrained optimization by surrogate modeling and adaptive parameter control | http://arxiv.org/abs/1512.09251 | author:Samineh Bagheri, Wolfgang Konen, Michael Emmerich, Thomas Bäck category:math.OC cs.NE stat.ML published:2015-12-31 summary:Constrained optimization of high-dimensional numerical problems plays animportant role in many scientific and industrial applications. Functionevaluations in many industrial applications are severely limited and noanalytical information about objective function and constraint functions isavailable. For such expensive black-box optimization tasks, the constraintoptimization algorithm COBRA was proposed, making use of RBF surrogate modelingfor both the objective and the constraint functions. COBRA has shown remarkablesuccess in solving reliably complex benchmark problems in less than 500function evaluations. Unfortunately, COBRA requires careful adjustment ofparameters in order to do so. In this work we present a new self-adjusting algorithm SACOBRA, which isbased on COBRA and capable to achieve high-quality results with very fewfunction evaluations and no parameter tuning. It is shown with the help ofperformance profiles on a set of benchmark problems (G-problems, MOPTA08) thatSACOBRA consistently outperforms any COBRA algorithm with fixed parametersetting. We analyze the importance of the several new elements in SACOBRA andfind that each element of SACOBRA plays a role to boost up the overalloptimization performance. We discuss the reasons behind and get in this way abetter understanding of high-quality RBF surrogate modeling.
arxiv-15600-252 | Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions | http://arxiv.org/abs/1512.09272 | author:B G Vijay Kumar, Gustavo Carneiro, Ian Reid category:cs.CV published:2015-12-31 summary:Recent innovations in training deep convolutional neural network (ConvNet)models have motivated the design of new methods to automatically learn localimage descriptors. The latest deep ConvNets proposed for this task consist of asiamese network that is trained by penalising misclassification of pairs oflocal image patches. Current results from machine learning show that replacingthis siamese by a triplet network can improve the classification accuracy inseveral problems, but this has yet to be demonstrated for local imagedescriptor learning. Moreover, current siamese and triplet networks have beentrained with stochastic gradient descent that computes the gradient fromindividual pairs or triplets of local image patches, which can make them proneto overfitting. In this paper, we first propose the use of triplet networks forthe problem of local image descriptor learning. Furthermore, we also proposethe use of a global loss that minimises the overall classification error in thetraining set, which can improve the generalisation capability of the model.Using the UBC benchmark dataset for comparing local image descriptors, we showthat the triplet network produces a more accurate embedding than the siamesenetwork in terms of the UBC dataset errors. Moreover, we also demonstrate thata combination of the triplet and global losses produces the best embedding inthe field, using this triplet network. Finally, we also show that the use ofthe central-surround siamese network trained with the global loss produces thebest result of the field on the UBC dataset.
arxiv-15600-253 | Strategies and Principles of Distributed Machine Learning on Big Data | http://arxiv.org/abs/1512.09295 | author:Eric P. Xing, Qirong Ho, Pengtao Xie, Wei Dai category:stat.ML cs.DC cs.LG published:2015-12-31 summary:The rise of Big Data has led to new demands for Machine Learning (ML) systemsto learn complex models with millions to billions of parameters, that promiseadequate capacity to digest massive datasets and offer powerful predictiveanalytics thereupon. In order to run ML algorithms at such scales, on adistributed cluster with 10s to 1000s of machines, it is often the case thatsignificant engineering efforts are required --- and one might fairly ask ifsuch engineering truly falls within the domain of ML research or not. Takingthe view that Big ML systems can benefit greatly from ML-rooted statistical andalgorithmic insights --- and that ML researchers should therefore not shy awayfrom such systems design --- we discuss a series of principles and strategiesdistilled from our recent efforts on industrial-scale ML solutions. Theseprinciples and strategies span a continuum from application, to engineering,and to theoretical research and development of Big ML systems andarchitectures, with the goal of understanding how to make them efficient,generally-applicable, and supported with convergence and scaling guarantees.They concern four key questions which traditionally receive little attention inML research: How to distribute an ML program over a cluster? How to bridge MLcomputation with inter-machine communication? How to perform suchcommunication? What should be communicated between machines? By exposingunderlying statistical and algorithmic characteristics unique to ML programsbut not typically seen in traditional computer programs, and by dissectingsuccessful cases to reveal how we have harnessed these principles to design anddevelop both high-performance distributed ML software as well asgeneral-purpose ML frameworks, we present opportunities for ML researchers andpractitioners to further shape and grow the area that lies between ML andsystems.
arxiv-15600-254 | Linear Convergence of Proximal Gradient Algorithm with Extrapolation for a Class of Nonconvex Nonsmooth Minimization Problems | http://arxiv.org/abs/1512.09302 | author:Bo Wen, Xiaojun Chen, Ting Kei Pong category:math.OC stat.ML published:2015-12-31 summary:In this paper, we study the proximal gradient algorithm with extrapolationfor minimizing the sum of a Lipschitz differentiable function and a properclosed convex function. Under the error bound condition used in [19] foranalyzing the convergence of the proximal gradient algorithm, we show thatthere exists a threshold such that if the extrapolation coefficients are chosenbelow this threshold, then the sequence generated converges $R$-linearly to astationary point of the problem. Moreover, the corresponding sequence ofobjective values is also $R$-linearly convergent. In addition, the thresholdreduces to $1$ for convex problems and, as a consequence, we obtain the$R$-linear convergence of the sequence generated by the FISTA with the fixedrestart. Finally, again for convex problems, we show that the successivechanges of the iterates vanish for many choices of sequences of extrapolationcoefficients that approach the threshold. In particular, this conclusion can beshown to hold for the sequence generated by the FISTA.
arxiv-15600-255 | Denoising and Completion of 3D Data via Multidimensional Dictionary Learning | http://arxiv.org/abs/1512.09227 | author:Zemin Zhang, Shuchin Aeron category:cs.LG cs.CV cs.DS published:2015-12-31 summary:In this paper a new dictionary learning algorithm for multidimensional datais proposed. Unlike most conventional dictionary learning methods which arederived for dealing with vectors or matrices, our algorithm, named KTSVD,learns a multidimensional dictionary directly via a novel algebraic approachfor tensor factorization as proposed in [3, 12, 13]. Using this approach onecan define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-Ddata to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, basedon the idea of sparse coding (using group-sparsity over multidimensionalcoefficient vectors), alternates between estimating a compact representationand dictionary learning. We analyze our KTSVD algorithm and demonstrate itsresult on video completion and multispectral image denoising.
arxiv-15600-256 | Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server | http://arxiv.org/abs/1512.09327 | author:Yee Whye Teh, Leonard Hasenclever, Thibaut Lienart, Sebastian Vollmer, Stefan Webb, Balaji Lakshminarayanan, Charles Blundell category:cs.LG stat.ML published:2015-12-31 summary:This paper makes two contributions to Bayesian machine learning algorithms.Firstly, we propose stochastic natural gradient expectation propagation (SNEP),a novel alternative to expectation propagation (EP), a popular variationalinference algorithm. SNEP is a black box variational algorithm, in that it doesnot require any simplifying assumptions on the distribution of interest, beyondthe existence of some Monte Carlo sampler for estimating the moments of the EPtilted distributions. Further, as opposed to EP which has no guarantee ofconvergence, SNEP can be shown to be convergent, even when using Monte Carlomoment estimates. Secondly, we propose a novel architecture for distributedBayesian learning which we call the posterior server. The posterior serverallows scalable and robust Bayesian learning in cases where a dataset is storedin a distributed manner across a cluster, with each compute node containing adisjoint subset of data. An independent Markov chain Monte Carlo (MCMC) sampleris run on each compute node, with direct access only to the local data subset,but which targets an approximation to the global posterior distribution givenall data across the whole cluster. This is achieved by using a distributedasynchronous implementation of SNEP to pass messages across the cluster. Wedemonstrate SNEP and the posterior server on distributed Bayesian learning oflogistic regression and neural networks.
arxiv-15600-257 | Selecting Near-Optimal Learners via Incremental Data Allocation | http://arxiv.org/abs/1601.00024 | author:Ashish Sabharwal, Horst Samulowitz, Gerald Tesauro category:cs.LG stat.ML published:2015-12-31 summary:We study a novel machine learning (ML) problem setting of sequentiallyallocating small subsets of training data amongst a large set of classifiers.The goal is to select a classifier that will give near-optimal accuracy whentrained on all data, while also minimizing the cost of misallocated samples.This is motivated by large modern datasets and ML toolkits with manycombinations of learning algorithms and hyper-parameters. Inspired by theprinciple of "optimism under uncertainty," we propose an innovative strategy,Data Allocation using Upper Bounds (DAUB), which robustly achieves theseobjectives across a variety of real-world datasets. We further develop substantial theoretical support for DAUB in an idealizedsetting where the expected accuracy of a classifier trained on $n$ samples canbe known exactly. Under these conditions we establish a rigorous sub-linearbound on the regret of the approach (in terms of misallocated data), as well asa rigorous bound on suboptimality of the selected classifier. Our accuracyestimates using real-world datasets only entail mild violations of thetheoretical scenario, suggesting that the practical behavior of DAUB is likelyto approach the idealized behavior.
arxiv-15600-258 | A single hidden layer feedforward network with only one neuron in the hidden layer can approximate any univariate function | http://arxiv.org/abs/1601.00013 | author:Namig J. Guliyev, Vugar E. Ismailov category:cs.NE cs.IT math.IT math.NA published:2015-12-31 summary:The possibility of approximating a continuous function on a compact subset ofthe real line by a feedforward single hidden layer neural network with asigmoidal activation function has been studied in many papers. Such networkscan approximate an arbitrary continuous function provided that an unlimitednumber of neurons in a hidden layer is permitted. In this paper, we considerconstructive approximation on any finite interval of $\mathbb{R}$ by neuralnetworks with only one neuron in the hidden layer. We construct algorithmicallya smooth, sigmoidal, almost monotone activation function $\sigma$ providingapproximation to an arbitrary continuous function within any degree ofaccuracy. This algorithm is implemented in a computer program, which computesthe value of $\sigma$ at any reasonable point of the real axis.
arxiv-15600-259 | Write a Classifier: Predicting Visual Classifiers from Unstructured Text Descriptions | http://arxiv.org/abs/1601.00025 | author:Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh category:cs.CV cs.CL cs.LG published:2015-12-31 summary:People typically learn through exposure to visual concepts associated withlinguistic descriptions. For instance, teaching visual object categories tochildren is often accompanied by descriptions in text or speech. In a machinelearning context, these observations motivates us to ask whether this learningprocess could be computationally modeled to learn visual classifiers. Morespecifically, the main question of this work is how to utilize purely textualdescription of visual classes with no training images, to learn explicit visualclassifiers for them. We propose and investigate two baseline formulations,based on regression and domain transfer that predict a linear classifier. Then,we propose a new constrained optimization formulation that combines aregression function and a knowledge transfer function with additionalconstraints to predict the linear classifier parameters for new classes. Wealso propose a generic kernelized models where a kernel classifier, in the formdefined by the representer theorem, is predicted. The kernelized models allowdefining any two RKHS kernel functions in the visual space and text space,respectively, and could be useful for other applications. We finally propose akernel function between unstructured text descriptions that builds ondistributional semantics, which shows an advantage in our setting and could beuseful for other applications. We applied all the studied models to predictvisual classifiers for two fine-grained categorization datasets, and theresults indicate successful predictions of our final model against severalbaselines that we designed.
arxiv-15600-260 | Computational Pathology: Challenges and Promises for Tissue Analysis | http://arxiv.org/abs/1601.00027 | author:Thomas J. Fuchs, Joachim M. Buhmann category:cs.CV cs.AI published:2015-12-31 summary:The histological assessment of human tissue has emerged as the key challengefor detection and treatment of cancer. A plethora of different data sourcesranging from tissue microarray data to gene expression, proteomics ormetabolomics data provide a detailed overview of the health status of apatient. Medical doctors need to assess these information sources and they relyon data driven automatic analysis tools. Methods for classification, groupingand segmentation of heterogeneous data sources as well as regression of noisydependencies and estimation of survival probabilities enter the processingworkflow of a pathology diagnosis system at various stages. This paper reportson state-of-the-art of the design and effectiveness of computational pathologyworkflows and it discusses future research directions in this emergent field ofmedical informatics and diagnostic machine learning.
arxiv-15600-261 | Nonparametric mixture of Gaussian graphical models | http://arxiv.org/abs/1512.09206 | author:Kevin Lee, Lingzhou Xue category:stat.ME stat.ML published:2015-12-31 summary:Graphical model has been widely used to investigate the complex dependencestructure of high-dimensional data, and it is common to assume that observeddata follow a homogeneous graphical model. However, observations usually comefrom different resources and have heterogeneous hidden commonality inreal-world applications. Thus, it is of great importance to estimateheterogeneous dependencies and discover subpopulation with certain commonalityacross the whole population. In this work, we introduce a novel regularizedestimation scheme for learning nonparametric mixture of Gaussian graphicalmodels, which extends the methodology and applicability of Gaussian graphicalmodels and mixture models. We propose a unified penalized likelihood approachto effectively estimate nonparametric functional parameters and heterogeneousgraphical parameters. We further design an efficient generalized effective EMalgorithm to address three significant challenges: high-dimensionality,non-convexity, and label switching. Theoretically, we study both thealgorithmic convergence of our proposed algorithm and the asymptotic propertiesof our proposed estimators. Numerically, we demonstrate the performance of ourmethod in simulation studies and a real application to estimate human brainfunctional connectivity from ADHD imaging data, where two heterogeneousconditional dependencies are explained through profiling demographic variablesand supported by existing scientific findings.
arxiv-15600-262 | Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index Policies | http://arxiv.org/abs/1512.09204 | author:Weici Hu, Peter I. Frazier category:cs.LG cs.AI stat.ML published:2015-12-31 summary:We consider effort allocation in crowdsourcing, where we wish to assignlabeling tasks to imperfect homogeneous crowd workers to maximize overallaccuracy in a continuous-time Bayesian setting, subject to budget and timeconstraints. The Bayes-optimal policy for this problem is the solution to apartially observable Markov decision process, but the curse of dimensionalityrenders the computation infeasible. Based on the Lagrangian Relaxationtechnique in Adelman & Mersereau (2008), we provide a computationally tractableinstance-specific upper bound on the value of this Bayes-optimal policy, whichcan in turn be used to bound the optimality gap of any other sub-optimalpolicy. In an approach similar in spirit to the Whittle index for restlessmultiarmed bandits, we provide an index policy for effort allocation incrowdsourcing and demonstrate numerically that it outperforms other stateof-arts and performs close to optimal solution.
arxiv-15600-263 | Event Specific Multimodal Pattern Mining with Image-Caption Pairs | http://arxiv.org/abs/1601.00022 | author:Hongzhi Li, Joseph G. Ellis, Shih-Fu Chang category:cs.CV published:2015-12-31 summary:In this paper we describe a novel framework and algorithms for discoveringimage patch patterns from a large corpus of weakly supervised image-captionpairs generated from news events. Current pattern mining techniques attempt tofind patterns that are representative and discriminative, we stipulate that ourdiscovered patterns must also be recognizable by humans and preferably withmeaningful names. We propose a new multimodal pattern mining approach thatleverages the descriptive captions often accompanying news images to learnsemantically meaningful image patch patterns. The mutltimodal patterns are thennamed using words mined from the associated image captions for each pattern. Anovel evaluation framework is provided that demonstrates our patterns are 26.2%more semantically meaningful than those discovered by the state of the artvision only pipeline, and that we can provide tags for the discovered imagespatches with 54.5% accuracy with no direct supervision. Our methods alsodiscover named patterns beyond those covered by the existing image datasetslike ImageNet. To the best of our knowledge this is the first algorithmdeveloped to automatically mine image patch patterns that have strong semanticmeaning specific to high-level news events, and then evaluate these patternsbased on that criteria.
arxiv-15600-264 | Autoencoding beyond pixels using a learned similarity metric | http://arxiv.org/abs/1512.09300 | author:Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, Ole Winther category:cs.LG cs.CV stat.ML published:2015-12-31 summary:We present an autoencoder that leverages learned representations to bettermeasure similarities in data space. By combining a variational autoencoder witha generative adversarial network we can use learned feature representations inthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,we replace element-wise errors with feature-wise errors to better capture thedata distribution while offering invariance towards e.g. translation. We applyour method to images of faces and show that it outperforms VAEs withelement-wise similarity measures in terms of visual fidelity. Moreover, we showthat the method learns an embedding in which high-level abstract visualfeatures (e.g. wearing glasses) can be modified using simple arithmetic.
arxiv-15600-265 | Exploiting Local Structures with the Kronecker Layer in Convolutional Networks | http://arxiv.org/abs/1512.09194 | author:Shuchang Zhou, Jia-Nan Wu, Yuxin Wu, Xinyu Zhou category:cs.CV published:2015-12-31 summary:In this paper, we propose and study a technique to reduce the number ofparameters and computation time in convolutional neural networks. We useKronecker product to exploit the local structures within convolution andfully-connected layers, by replacing the large weight matrices by combinationsof multiple Kronecker products of smaller matrices. Just as the Kroneckerproduct is a generalization of the outer product from vectors to matrices, ourmethod is a generalization of the low rank approximation method for convolutionneural networks. We also introduce combinations of different shapes ofKronecker product to increase modeling capacity. Experiments on SVHN, scenetext recognition and ImageNet dataset demonstrate that we can achieve $3.3\times$ speedup or $3.6 \times$ parameter reduction with less than 1\% drop inaccuracy, showing the effectiveness and efficiency of our method. Moreover, thecomputation efficiency of Kronecker layer makes using larger feature mappossible, which in turn enables us to outperform the previous state-of-the-arton both SVHN(digit recognition) and CASIA-HWDB (handwritten Chinese characterrecognition) datasets.
arxiv-15600-266 | Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices | http://arxiv.org/abs/1512.08996 | author:Ayan Acharya, Joydeep Ghosh, Mingyuan Zhou category:stat.ML stat.AP stat.ME published:2015-12-30 summary:A gamma process dynamic Poisson factor analysis model is proposed tofactorize a dynamic count matrix, whose columns are sequentially observed countvectors. The model builds a novel Markov chain that sends the latent gammarandom variables at time $(t-1)$ as the shape parameters of those at time $t$,which are linked to observed or latent counts under the Poisson likelihood. Thesignificant challenge of inferring the gamma shape parameters is fullyaddressed, using unique data augmentation and marginalization techniques forthe negative binomial distribution. The same nonparametric Bayesian model alsoapplies to the factorization of a dynamic binary matrix, via aBernoulli-Poisson link that connects a binary observation to a latent count,with closed-form conditional posteriors for the latent counts and efficientcomputation for sparse observations. We apply the model to text and musicanalysis, with state-of-the-art results.
arxiv-15600-267 | Technical Report: a tool for measuring Prosodic Accommodation | http://arxiv.org/abs/1512.08982 | author:Sucheta Ghosh category:cs.SD cs.CL published:2015-12-30 summary:Social interaction is a dynamic and joint activity where all participants areengaged and coordinate their behaviour in the co-construction of meaning. It isobserved that conversational partners adapt their pitch, intensity and timingbehaviour to their in- terlocutors. The majority of research has focused on itslinear manifestation over the course of an interaction. De Looze et alhypothesised that it evolves dynamically with functional social aspects. In thework of De Looze et al, they proposed through their praat based featureextraction and matlab based visualisation that one can visualise prosodicaccommodation at the positive correlation threshold values. and the capture ofits dynamic manifestation. Here we seek to build a complete system formeasuring prosodic accommodation with matlab. This work uses data collected ina pilot training scenario where senior pilot and co-pilot are engaged inconversation during the flight. Additionally, we use the data from a ship wrecktask by two pilots. We also attempt to evaluate this measures of accommodationwith ground truth labels given by a trainer of Crew (or sometimes Crisis)Resource Management (CRM).
arxiv-15600-268 | Personalized Course Sequence Recommendations | http://arxiv.org/abs/1512.09176 | author:Jie Xu, Tianwei Xing, Mihaela van der Schaar category:cs.CY cs.LG published:2015-12-30 summary:Given the variability in student learning it is becoming increasinglyimportant to tailor courses as well as course sequences to student needs. Thispaper presents a systematic methodology for offering personalized coursesequence recommendations to students. First, a forward-searchbackward-induction algorithm is developed that can optimally select coursesequences to decrease the time required for a student to graduate. Thealgorithm accounts for prerequisite requirements (typically present in higherlevel education) and course availability. Second, using the tools ofmulti-armed bandits, an algorithm is developed that can optimally recommend acourse sequence that both reduces the time to graduate while also increasingthe overall GPA of the student. The algorithm dynamically learns how studentswith different contextual backgrounds perform for given course sequences andthen recommends an optimal course sequence for new students. Using real-worldstudent data from the UCLA Mechanical and Aerospace Engineering department, weillustrate how the proposed algorithms outperform other methods that do notinclude student contextual information when making course sequencerecommendations.
arxiv-15600-269 | Online Keyword Spotting with a Character-Level Recurrent Neural Network | http://arxiv.org/abs/1512.08903 | author:Kyuyeon Hwang, Minjae Lee, Wonyong Sung category:cs.CL cs.LG cs.NE published:2015-12-30 summary:In this paper, we propose a context-aware keyword spotting model employing acharacter-level recurrent neural network (RNN) for spoken term detection incontinuous speech. The RNN is end-to-end trained with connectionist temporalclassification (CTC) to generate the probabilities of character andword-boundary labels. There is no need for the phonetic transcription, senonemodeling, or system dictionary in training and testing. Also, keywords caneasily be added and modified by editing the text based keyword list withoutretraining the RNN. Moreover, the unidirectional RNN processes an infinitelylong input audio streams without pre-segmentation and keywords are detectedwith low-latency before the utterance is finished. Experimental results showthat the proposed keyword spotter significantly outperforms the deep neuralnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model evenwith less computations.
arxiv-15600-270 | Statistical Query Algorithms for Stochastic Convex Optimization | http://arxiv.org/abs/1512.09170 | author:Vitaly Feldman, Cristobal Guzman, Santosh Vempala category:cs.LG cs.DS published:2015-12-30 summary:Stochastic convex optimization, where the objective is the expectation of arandom convex function, is an important and widely used method with numerousapplications in machine learning, statistics, operations research and otherareas. We study the complexity of stochastic convex optimization given onlystatistical query (SQ) access to the objective function. We show thatwell-known and popular methods, including first-order iterative methods andpolynomial-time methods, can be implemented using only statistical queries. Formany cases of interest we derive nearly matching upper and lower bounds on theestimation (sample) complexity including linear optimization in the mostgeneral setting. We then present several consequences for machine learning,differential privacy and proving concrete lower bounds on the power of convexoptimization based methods. A new technical ingredient of our work is SQ algorithms for estimating themean vector of a distribution over vectors in $\mathbb{R}^d$ with optimalestimation complexity. This is a natural problem and we show that our solutionscan be used to get substantially improved SQ versions of Perceptron and otheronline algorithms for learning halfspaces.
arxiv-15600-271 | Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures, acyclic BP, and the information-computation gap | http://arxiv.org/abs/1512.09080 | author:Emmanuel Abbe, Colin Sandon category:math.PR cs.CC cs.IT cs.LG cs.SI math.IT published:2015-12-30 summary:In a paper that initiated the modern study of the stochastic block model,Decelle et al., backed up Mossel et al., made a fascinating conjecture: Denoteby $k$ the number of balanced communities, $a/n$ the probability of connectinginside communities and $b/n$ across, and set$\mathrm{SNR}=a-b/\sqrt{k(a+(k-1)b)}$; for any $k \geq 2$, it is possible todetect communities efficiently whenever $\mathrm{SNR}>1$ (the KS threshold),whereas for $k\geq 5$, it is possible to detect communitiesinformation-theoretically for some $\mathrm{SNR}<1$. Massouli\'e, Mossel etal.\ and Bordenave et al.\ succeeded in proving that the KS threshold isefficiently achievable for $k=2$, while Mossel et al.\ proved that it cannot becrossed information-theoretically for $k=2$. The above conjecture remained openfor $k \geq 3$. This paper proves this conjecture. For the efficient part, an acyclic beliefpropagation (ABP) algorithm is developed and proved to detect communities forany $k$ down the KS threshold in time $O(n \log n)$. Achieving this requiresshowing optimality of BP in the presence of cycles and random initialization, achallenge in the realm of graphical models. The paper also connects ABP to apower iteration method on a $r$-nonbacktracking operator, formalizing themessage passing and spectral interplay. Further, it shows that the model can belearned efficiently down the KS threshold, implying that ABP improves upon thestate-of-the-art both in terms of complexity and universality. For theinformation-theoretic (IT) part, a non-efficient algorithm sampling a typicalclustering is shown to break down the KS threshold at $k=5$. The emerging gapis shown to be large in some cases; if $a=0$, the KS threshold reads $b \gtrsimk^2$ whereas the IT bound reads $b \gtrsim k \ln(k)$. This thus makes the SBM agood study case for information-computation gaps. The results extend to generalSBMs.
arxiv-15600-272 | Low rank approximation and decomposition of large matrices using error correcting codes | http://arxiv.org/abs/1512.09156 | author:Shashanka Ubaru, Arya Mazumdar, Yousef Saad category:cs.IT cs.LG cs.NA math.IT published:2015-12-30 summary:Low rank approximation is an important tool used in many applications ofsignal processing and machine learning. Recently, randomized sketchingalgorithms were proposed to effectively construct low rank approximations andobtain approximate singular value decompositions of large matrices. Similarideas were used to solve least squares regression problems. In this paper, weshow how matrices from error correcting codes can be used to find such low rankapproximations and matrix decompositions, and extend the framework to linearleast squares regression problems. The benefits of using these code matricesare the following: (i) They are easy to generate and they reduce randomnesssignificantly. (ii) Code matrices with mild properties satisfy the subspaceembedding property, and have a better chance of preserving the geometry of anentire subspace of vectors. (iii) For parallel and distributed applications,code matrices have significant advantages over structured random matrices andGaussian random matrices. (iv) Unlike Fourier or Hadamard transform matrices,which require sampling $O(k\log k)$ columns for a rank-$k$ approximation, thelog factor is not necessary for certain types of code matrices. That is,$(1+\epsilon)$ optimal Frobenius norm error can be achieved for a rank-$k$approximation with $O(k/\epsilon)$ samples. (v) Fast multiplication is possiblewith structured code matrices, so fast approximations can be achieved forgeneral dense input matrices. (vi) For least squares regression problem$\min\Ax-b\_2$ where $A\in \mathbb{R}^{n\times d}$, the $(1+\epsilon)$relative error approximation can be achieved with $O(d/\epsilon)$ samples, withhigh probability, when certain code matrices are used.
arxiv-15600-273 | Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling | http://arxiv.org/abs/1512.09103 | author:Zeyuan Allen-Zhu, Zheng Qu, Peter Richtárik, Yang Yuan category:math.OC cs.DS math.NA stat.ML published:2015-12-30 summary:Accelerated coordinate descent methods are widely used in optimization andmachine learning. By taking cheap-to-compute coordinate gradients in eachiteration, they are usually faster than accelerated full gradient descent, andthus suitable for large-scale optimization problems. In this paper, we improve the running time of accelerated coordinate descent,using a clean and novel non-uniform sampling method. In particular, if afunction $f$ is coordinate-wise smooth with parameters $L_1,L_2,\dots,L_n$, weobtain an algorithm that converges in $O(\sum_i \sqrt{L_i} /\sqrt{\varepsilon})$ iterations for convex $f$ and in $\tilde{O}(\sum_i\sqrt{L_i} / \sqrt{\sigma})$ iterations for $\sigma$-strongly convex $f$. Thebest known result was $\tilde{O}(\sqrt{n \sum_i L_i} / \sqrt{\varepsilon})$ forconvex $f$ and $\tilde{O}(\sqrt{n \sum_i L_i} / \sqrt{\sigma})$ for$\sigma$-strongly convex $f$, due to Lee and Sidford. Thus, our algorithmimproves the running time by a factor up to $\sqrt{n}$. Our speed-up naturally applies to important problems such as empirical riskminimizations and solving linear systems.
arxiv-15600-274 | Joint limiting laws for high-dimensional independence tests | http://arxiv.org/abs/1512.08819 | author:Danning Li, Lingzhou Xue category:math.ST stat.ME stat.ML stat.OT stat.TH 62H12, 60F05 published:2015-12-30 summary:Testing independence is of significant interest in many important areas oflarge-scale inference. Using extreme-value form statistics to test againstsparse alternatives and using quadratic form statistics to test against densealternatives are two important testing procedures for high-dimensionalindependence. However, quadratic form statistics suffer from low power againstsparse alternatives, and extreme-value form statistics suffer from low poweragainst dense alternatives with small disturbances and may have sizedistortions due to its slow convergence. For real-world applications, it isimportant to derive powerful testing procedures against more generalalternatives. Based on intermediate limiting distributions, we derive(model-free) joint limiting laws of extreme-value form and quadratic formstatistics, and surprisingly, we prove that they are asymptoticallyindependent. Given such asymptotic independencies, we propose (model-free)testing procedures to boost the power against general alternatives and alsoretain the correct asymptotic size. Under the high-dimensional setting, wederive the closed-form limiting null distributions, and obtain their explicitrates of uniform convergence. We prove their consistent statistical powersagainst general alternatives. We demonstrate the performance of our proposedtest statistics in simulation studies. Our work provides very helpful insightsto high-dimensional independence tests, and fills an important gap.
arxiv-15600-275 | Learning to Filter with Predictive State Inference Machines | http://arxiv.org/abs/1512.08836 | author:Wen Sun, Arun Venkatraman, Byron Boots, J. Andrew Bagnell category:cs.LG published:2015-12-30 summary:Latent state space models are one of the most fundamental and widely usedtools for modeling dynamical systems. Traditional Maximum Likelihood Estimation(MLE) based approaches aim to maximize the likelihood objective, which isnon-convex due to latent states. While non-convex optimization methods like EMcan learn models that locally optimize the likelihood objective, using thelocally optimal model for an inference task such as Bayesian filtering usuallydoes not have performance guarantees. In this work, we propose a method thatconsiders the inference procedure on the dynamical system as a composition ofpredictors. Instead of optimizing a given parametrization of latent states, welearn predictors for inference in predictive belief space, where we can usesufficient features of observations for supervision of our learning algorithm.We further show that our algorithm, the Predictive State Inference Machine, hastheoretical performance guarantees on the inference task. Empiricalverification across several of dynamical system benchmarks ranging from asimulated helicopter to recorded telemetry traces from a robot showcase theabilities of training Inference Machines.
arxiv-15600-276 | Learning Natural Language Inference with LSTM | http://arxiv.org/abs/1512.08849 | author:Shuohang Wang, Jing Jiang category:cs.CL cs.AI cs.NE published:2015-12-30 summary:Natural language inference (NLI) is a fundamentally important task in naturallanguage processing that has many applications. The recently released StanfordNatural Language Inference (SNLI) corpus has made it possible to develop andevaluate learning-centered methods such as deep neural networks for the NLItask. In this paper, we propose a special long short-term memory (LSTM)architecture for NLI. Our model builds on top of a recently proposed neutralattention model for NLI but is based on a significantly different idea. Insteadof deriving sentence embeddings for the premise and the hypothesis to be usedfor classification, our solution uses a matching-LSTM that performsword-by-word matching of the hypothesis with the premise. This LSTM is able toplace more emphasis on important word-level matching results. In particular, weobserve that this LSTM remembers important mismatches that are critical forpredicting the contradiction or the neutral relationship label. Our experimentson the SNLI corpus show that our model outperforms the state of the art,achieving an accuracy of 86.1% on the test data.
arxiv-15600-277 | LIBSVX: A Supervoxel Library and Benchmark for Early Video Processing | http://arxiv.org/abs/1512.09049 | author:Chenliang Xu, Jason J. Corso category:cs.CV published:2015-12-30 summary:Supervoxel segmentation has strong potential to be incorporated into earlyvideo analysis as superpixel segmentation has in image analysis. However, thereare many plausible supervoxel methods and little understanding as to when andwhere each is most appropriate. Indeed, we are not aware of a singlecomparative study on supervoxel segmentation. To that end, we study sevensupervoxel algorithms, including both off-line and streaming methods, in thecontext of what we consider to be a good supervoxel: namely, spatiotemporaluniformity, object/region boundary detection, region compression and parsimony.For the evaluation we propose a comprehensive suite of seven quality metrics tomeasure these desirable supervoxel characteristics. In addition, we evaluatethe methods in a supervoxel classification task as a proxy for subsequenthigh-level uses of the supervoxels in video analysis. We use six existingbenchmark video datasets with a variety of content-types and dense humanannotations. Our findings have led us to conclusive evidence that thehierarchical graph-based (GBH), segmentation by weighted aggregation (SWA) andtemporal superpixels (TSP) methods are the top-performers among the sevenmethods. They all perform well in terms of segmentation accuracy, but vary inregard to the other desiderata: GBH captures object boundaries best; SWA hasthe best potential for region compression; and TSP achieves the bestundersegmentation error.
arxiv-15600-278 | Actor-Action Semantic Segmentation with Grouping Process Models | http://arxiv.org/abs/1512.09041 | author:Chenliang Xu, Jason J. Corso category:cs.CV published:2015-12-30 summary:Actor-action semantic segmentation made an important step toward advancedvideo understanding problems: what action is happening; who is performing theaction; and where is the action in space-time. Current models for this problemare local, based on layered CRFs, and are unable to capture long-ranginginteraction of video parts. We propose a new model that combines these locallabeling CRFs with a hierarchical supervoxel decomposition. The supervoxelsprovide cues for possible groupings of nodes, at various scales, in the CRFs toencourage adaptive, high-order groups for more effective labeling. Our model isdynamic and continuously exchanges information during inference: the local CRFsinfluence what supervoxels in the hierarchy are active, and these active nodesinfluence the connectivity in the CRF; we hence call it a grouping processmodel. The experimental results on a recent large-scale video dataset show alarge margin of 60% relative improvement over the state of the art, whichdemonstrates the effectiveness of the dynamic, bidirectional flow betweenlabeling and grouping.
arxiv-15600-279 | Simple, Robust and Optimal Ranking from Pairwise Comparisons | http://arxiv.org/abs/1512.08949 | author:Nihar B. Shah, Martin J. Wainwright category:cs.LG cs.AI cs.IT math.IT stat.ML published:2015-12-30 summary:We consider data in the form of pairwise comparisons of n items, with thegoal of precisely identifying the top k items for some value of k < n, oralternatively, recovering a ranking of all the items. We analyze the Copelandcounting algorithm that ranks the items in order of the number of pairwisecomparisons won, and show it has three attractive features: (a) itscomputational efficiency leads to speed-ups of several orders of magnitude incomputation time as compared to prior work; (b) it is robust in thattheoretical guarantees impose no conditions on the underlying matrix ofpairwise-comparison probabilities, in contrast to some prior work that appliesonly to the BTL parametric model; and (c) it is an optimal method up toconstant factors, meaning that it achieves the information-theoretic limits forrecovering the top k-subset. We extend our results to obtain sharp guaranteesfor approximate recovery under the Hamming distortion metric, and moregenerally, to any arbitrary error requirement that satisfies a simple andnatural monotonicity condition.
arxiv-15600-280 | Sharp Computational-Statistical Phase Transitions via Oracle Computational Model | http://arxiv.org/abs/1512.08861 | author:Zhaoran Wang, Quanquan Gu, Han Liu category:stat.ML published:2015-12-30 summary:We study the fundamental tradeoffs between computational tractability andstatistical accuracy for a general family of hypothesis testing problems withcombinatorial structures. Based upon an oracle model of computation, whichcaptures the interactions between algorithms and data, we establish a generallower bound that explicitly connects the minimum testing risk undercomputational budget constraints with the intrinsic probabilistic andcombinatorial structures of statistical problems. This lower bound mirrors theclassical statistical lower bound by Le Cam (1986) and allows us to quantifythe optimal statistical performance achievable given limited computationalbudgets in a systematic fashion. Under this unified framework, we sharplycharacterize the statistical-computational phase transition for two testingproblems, namely, normal mean detection and sparse principal componentdetection. For normal mean detection, we consider two combinatorial structures,namely, sparse set and perfect matching. For these problems we identifysignificant gaps between the optimal statistical accuracy that is achievableunder computational tractability constraints and the classical statisticallower bounds. Compared with existing works on computational lower bounds forstatistical problems, which consider general polynomial-time algorithms onTuring machines, and rely on computational hardness hypotheses on problems likeplanted clique detection, we focus on the oracle computational model, whichcovers a broad range of popular algorithms, and do not rely on unprovenhypotheses. Moreover, our result provides an intuitive and concreteinterpretation for the intrinsic computational intractability ofhigh-dimensional statistical problems. One byproduct of our result is a lowerbound for a strict generalization of the matrix permanent problem, which is ofindependent interest.
arxiv-15600-281 | Estimation of the sample covariance matrix from compressive measurements | http://arxiv.org/abs/1512.08887 | author:Farhad Pourkamali-Anaraki category:stat.ML cs.LG published:2015-12-30 summary:This paper focuses on the estimation of the sample covariance matrix fromlow-dimensional random projections of data known as compressive measurements.In particular, we present an unbiased estimator to extract the covariancestructure from compressive measurements obtained by a general class of randomprojection matrices consisting of i.i.d. zero-mean entries and finite firstfour moments. In contrast to previous works, we make no structural assumptionsabout the underlying covariance matrix such as being low-rank. In fact, ouranalysis is based on a non-Bayesian data setting which requires nodistributional assumptions on the set of data samples. Furthermore, inspired bythe generality of the projection matrices, we propose an approach to covarianceestimation that utilizes very sparse random projections with Bernoulli entries.Therefore, our algorithm can be used to estimate the covariance matrix inapplications with limited memory and computation power at the acquisitiondevices. Experimental results demonstrate that our approach allows for accurateestimation of the sample covariance matrix on several real-world data sets,including video data.
arxiv-15600-282 | Tight Bounds for Approximate Carathéodory and Beyond | http://arxiv.org/abs/1512.08602 | author:Vahab Mirrokni, Renato Paes Leme, Adrian Vladu, Sam Chiu-wai Wong category:cs.DS cs.LG math.OC published:2015-12-29 summary:We give a deterministic nearly-linear time algorithm for approximating anypoint inside a convex polytope with a sparse convex combination of thepolytope's vertices. Our result provides a constructive proof for theApproximate Carath\'{e}odory Problem, which states that any point inside apolytope contained in the $\ell_p$ ball of radius $D$ can be approximated towithin $\epsilon$ in $\ell_p$ norm by a convex combination of only $O\left(D^2p/\epsilon^2\right)$ vertices of the polytope for $p \geq 2$. We also show thatthis bound is tight, using an argument based on anti-concentration for thebinomial distribution. Along the way of establishing the upper bound, we develop a technique forminimizing norms over convex sets with complicated geometry; this is achievedby running Mirror Descent on a dual convex function obtained via Sion'sTheorem. As simple extensions of our method, we then provide new algorithms forsubmodular function minimization and SVM training. For submodular functionminimization we obtain a simplification and (provable) speed-up over Wolfe'salgorithm, the method commonly found to be the fastest in practice. For SVMtraining, we obtain $O(1/\epsilon^2)$ convergence for arbitrary kernels; eachiteration only requires matrix-vector operations involving the kernel matrix,so we overcome the obstacle of having to explicitly store the kernel or computeits Cholesky factorization.
arxiv-15600-283 | Hypothesis Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity | http://arxiv.org/abs/1512.08643 | author:Eugene Belilovsky, Gaël Varoquaux, Matthew B. Blaschko category:stat.ML published:2015-12-29 summary:Functional brain networks are well described and estimated from data withGaussian Graphical Models (GGMs), e.g. using sparse inverse covarianceestimators. Comparing functional connectivity of subjects in two populationcalls for comparing these estimated GGMs. We study the problem of identifyingdifferences in Gaussian Graphical Models (GGMs) known to have similarstructure. We aim to characterize the uncertainty of differences withconfidence intervals obtained using a para-metric distribution on parameters ofa sparse estimator. Sparse penalties enable statistical guarantees andinterpretable models even in high-dimensional and low-number-of-samplessettings. Quantifying the uncertainty of the parameters selected by the sparsepenalty is an important question in applications such as neuroimaging orbioinformatics. Indeed, selected variables can be interpreted to buildtheoretical understanding or to make therapeutic decisions. Characterizing thedistributions of sparse regression models is inherently challenging since thepenalties produce a biased estimator. Recent work has shown how one can invokethe sparsity assumptions to effectively remove the bias from a sparse estimatorsuch as the lasso. These distributions can be used to give us confidenceintervals on edges in GGMs, and by extension their differences. However, in thecase of comparing GGMs, these estimators do not make use of any assumed jointstructure among the GGMs. Inspired by priors from brain functional connectivitywe focus on deriving the distribution of parameter differences under a jointpenalty when parameters are known to be sparse in the difference. This leads usto introduce the debiased multi-task fused lasso. We show that we can debiasand characterize the distribution in an efficient manner. We then go on to showhow the debiased lasso and multi-task fused lasso can be used to obtainconfidence intervals on edge differences in Gaussian graphical models. Wevalidate the techniques proposed on a set of synthetic examples as well asneuro-imaging dataset created for the study of autism.
arxiv-15600-284 | Analyzing Walter Skeat's Forty-Five Parallel Extracts of William Langland's Piers Plowman | http://arxiv.org/abs/1512.08569 | author:Roger Bilisoly category:stat.AP cs.CL published:2015-12-29 summary:Walter Skeat published his critical edition of William Langland's 14thcentury alliterative poem, Piers Plowman, in 1886. In preparation for this helocated forty-five manuscripts, and to compare dialects, he published excerptsfrom each of these. This paper does three statistical analyses using theseexcerpts, each of which mimics a task he did in writing his critical edition.First, he combined multiple versions of a poetic line to create a best line,which is compared to the mean string that is computed by a generalization ofthe arithmetic mean that uses edit distance. Second, he claims that a certainsubset of manuscripts varies little. This is quantified by computing a stringvariance, which is closely related to the above generalization of the mean.Third, he claims that the manuscripts fall into three groups, which is aclustering problem that is addressed by using edit distance. The overall goalis to develop methodology that would be of use to a literary critic.
arxiv-15600-285 | Structured Pruning of Deep Convolutional Neural Networks | http://arxiv.org/abs/1512.08571 | author:Sajid Anwar, Kyuyeon Hwang, Wonyong Sung category:cs.NE cs.LG stat.ML published:2015-12-29 summary:Real time application of deep learning algorithms is often hindered by highcomputational complexity and frequent memory accesses. Network pruning is apromising technique to solve this problem. However, pruning usually results inirregular network connections that not only demand extra representation effortsbut also do not fit well on parallel computation. We introduce structuredsparsity at various scales for convolutional neural networks, which are channelwise, kernel wise and intra kernel strided sparsity. This structured sparsityis very advantageous for direct computational resource savings on embeddedcomputers, parallel computing environments and hardware based systems. Todecide the importance of network connections and paths, the proposed methoduses a particle filtering approach. The importance weight of each particle isassigned by computing the misclassification rate with correspondingconnectivity pattern. The pruned network is re-trained to compensate for thelosses due to pruning. While implementing convolutions as matrix products, weparticularly show that intra kernel strided sparsity with a simple constraintcan significantly reduce the size of kernel and feature map matrices. Thepruned network is finally fixed point optimized with reduced word lengthprecision. This results in significant reduction in the total storage sizeproviding advantages for on-chip memory based implementations of deep neuralnetworks.
arxiv-15600-286 | Error Bounds for Compressed Sensing Algorithms With Group Sparsity: A Unified Approach | http://arxiv.org/abs/1512.08673 | author:M. Eren Ahsen, M. Vidyasagar category:stat.ML 62J99 published:2015-12-29 summary:In compressed sensing, in order to recover a sparse or nearly sparse vectorfrom possibly noisy measurements, the most popular approach is $\ell_1$-normminimization. Upper bounds for the $\ell_2$- norm of the error between the trueand estimated vectors are given in [1] and reviewed in [2], while bounds forthe $\ell_1$-norm are given in [3]. When the unknown vector is notconventionally sparse but is "group sparse" instead, a variety of alternativesto the $\ell_1$-norm have been proposed in the literature, including the groupLASSO, sparse group LASSO, and group LASSO with tree structured overlappinggroups. However, no error bounds are available for any of these modifiedobjective functions. In the present paper, a unified approach is presented forderiving upper bounds on the error between the true vector and itsapproximation, based on the notion of decomposable and $\gamma$-decomposablenorms. The bounds presented cover all of the norms mentioned above, and alsoprovide a guideline for choosing norms in future to accommodate alternate formsof sparsity.
arxiv-15600-287 | Matrix Completion Under Monotonic Single Index Models | http://arxiv.org/abs/1512.08787 | author:Ravi Ganti, Laura Balzano, Rebecca Willett category:stat.ML cs.LG published:2015-12-29 summary:Most recent results in matrix completion assume that the matrix underconsideration is low-rank or that the columns are in a union of low-ranksubspaces. In real-world settings, however, the linear structure underlyingthese models is distorted by a (typically unknown) nonlinear transformation.This paper addresses the challenge of matrix completion in the face of suchnonlinearities. Given a few observations of a matrix that are obtained byapplying a Lipschitz, monotonic function to a low rank matrix, our task is toestimate the remaining unobserved entries. We propose a novel matrix completionmethod that alternates between low-rank matrix estimation and monotonicfunction estimation to estimate the missing matrix elements. Mean squared errorbounds provide insight into how well the matrix can be estimated based on thesize, rank of the matrix and properties of the nonlinear transformation.Empirical results on synthetic and real-world datasets demonstrate thecompetitiveness of the proposed approach.
arxiv-15600-288 | Combined statistical and model based texture features for improved image classification | http://arxiv.org/abs/1512.08814 | author:Omar Al-Kadi category:cs.CV published:2015-12-29 summary:This paper aims to improve the accuracy of texture classification based onextracting texture features using five different texture methods andclassifying the patterns using a naive Bayesian classifier. Threestatistical-based and two model-based methods are used to extract texturefeatures from eight different texture images, then their accuracy is rankedafter using each method individually and in pairs. The accuracy improved up to97.01% when model based -Gaussian Markov random field (GMRF) and fractionalBrownian motion (fBm) - were used together for classification as compared tothe highest achieved using each of the five different methods alone; and provedto be better in classifying as compared to statistical methods. Also, usingGMRF with statistical based methods, such as Gray level co-occurrence (GLCM)and run-length (RLM) matrices, improved the overall accuracy to 96.94% and96.55%; respectively.
arxiv-15600-289 | Robust Scene Text Recognition Using Sparse Coding based Features | http://arxiv.org/abs/1512.08669 | author:Da-Han Wang, Hanzi Wang, Dong Zhang, Jonathan Li, David Zhang category:cs.CV published:2015-12-29 summary:In this paper, we propose an effective scene text recognition method usingsparse coding based features, called Histograms of Sparse Codes (HSC) features.For character detection, we use the HSC features instead of using theHistograms of Oriented Gradients (HOG) features. The HSC features are extractedby computing sparse codes with dictionaries that are learned from data usingK-SVD, and aggregating per-pixel sparse codes to form local histograms. Forword recognition, we integrate multiple cues including character detectionscores and geometric contexts in an objective function. The final recognitionresults are obtained by searching for the words which correspond to the maximumvalue of the objective function. The parameters in the objective function arelearned using the Minimum Classification Error (MCE) training method.Experiments on several challenging datasets demonstrate that the proposedHSC-based scene text recognition method outperforms HOG-based methodssignificantly and outperforms most state-of-the-art methods.
arxiv-15600-290 | A framework for robust object multi-detection with a vote aggregation and a cascade filtering | http://arxiv.org/abs/1512.08648 | author:Grzegorz Kurzejamski, Jacek Zawistowski, Grzegorz Sarwas category:cs.CV published:2015-12-29 summary:This paper presents a framework designed for the multi-object detectionpurposes and adjusted for the application of product search on the marketshelves. The framework uses a single feedback loop and a pattern resizingmechanism to demonstrate the top effectiveness of the state-of-the-art localfeatures. A high detection rate with a low false detection chance can beachieved with use of only one pattern per object and no manual parametersadjustments. The method incorporates well known local features and a basicmatching process to create a reliable voting space. Further steps comprise ofmetric transformations, graphical vote space representation, two-phase voteaggregation process and a cascade of verifying filters.
arxiv-15600-291 | Optimal Selective Attention in Reactive Agents | http://arxiv.org/abs/1512.08575 | author:Roy Fox, Naftali Tishby category:cs.LG cs.IT math.IT published:2015-12-29 summary:In POMDPs, information about the hidden state, delivered throughobservations, is both valuable to the agent, allowing it to base its actions onbetter informed internal states, and a "curse", exploding the size anddiversity of the internal state space. One attempt to deal with this is tofocus on reactive policies, that only base their actions on the most recentobservation. However, even reactive policies can be demanding on resources, andagents need to pay selective attention to only some of the informationavailable to them in observations. In this report we present theminimum-information principle for selective attention in reactive agents. Wefurther motivate this approach by reducing the general problem of optimalcontrol in POMDPs, to reactive control with complex observations. Lastly, weexplore a newly discovered phenomenon of this optimization process - perioddoubling bifurcations. This necessitates periodic policies, and raises manymore questions regarding stability, periodicity and chaos in optimal control.
arxiv-15600-292 | Sparse group factor analysis for biclustering of multiple data sources | http://arxiv.org/abs/1512.08808 | author:Kerstin Bunte, Eemeli Leppäaho, Inka Saarinen, Samuel Kaski category:cs.LG cs.IR stat.ML published:2015-12-29 summary:Motivation: Modelling methods that find structure in data are necessary withthe current large volumes of genomic data, and there have been various effortsto find subsets of genes exhibiting consistent patterns over subsets oftreatments. These biclustering techniques have focused on one data source,often gene expression data. We present a Bayesian approach for jointbiclustering of multiple data sources, extending a recent method Group FactorAnalysis (GFA) to have a biclustering interpretation with additional sparsityassumptions. The resulting method enables data-driven detection of linearstructure present in parts of the data sources. Results: Our simulation studiesshow that the proposed method reliably infers bi-clusters from heterogeneousdata sources. We tested the method on data from the NCI-DREAM drug sensitivityprediction challenge, resulting in an excellent prediction accuracy. Moreover,the predictions are based on several biclusters which provide insight into thedata sources, in this case on gene expression, DNA methylation, proteinabundance, exome sequence, functional connectivity fingerprints and drugsensitivity.
arxiv-15600-293 | Common Variable Learning and Invariant Representation Learning using Siamese Neural Networks | http://arxiv.org/abs/1512.08806 | author:Uri Shaham, Roy Lederman category:stat.ML cs.LG cs.NE published:2015-12-29 summary:We consider the statistical problem of learning common source of variabilityin data which are synchronously captured by multiple sensors, and demonstratethat Siamese neural networks can be naturally applied to this problem. Thisapproach is useful in particular in exploratory, data-driven applications,where neither a model nor label information is available. In recent years, manyresearchers have successfully applied Siamese neural networks to obtain anembedding of data which corresponds to a "semantic similarity". We present aninterpretation of this "semantic similarity" as learning of equivalenceclasses. We discuss properties of the embedding obtained by Siamese networksand provide empirical results that demonstrate the ability of Siamese networksto learn common variability.
arxiv-15600-294 | Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems | http://arxiv.org/abs/1512.08756 | author:Colin Raffel, Daniel P. W. Ellis category:cs.LG cs.NE published:2015-12-29 summary:We propose a simplified model of attention which is applicable tofeed-forward neural networks and demonstrate that the resulting model can solvethe synthetic "addition" and "multiplication" long-term memory problems forsequence lengths which are both longer and more widely varying than the bestpublished results for these tasks.
arxiv-15600-295 | Post-Regularization Inference for Dynamic Nonparanormal Graphical Models | http://arxiv.org/abs/1512.08298 | author:Junwei Lu, Mladen Kolar, Han Liu category:stat.ML published:2015-12-28 summary:We propose a novel class of dynamic nonparanormal graphical models, whichallows us to model high dimensional heavy-tailed systems and the evolution oftheir latent network structures. Under this model we develop statistical testsfor presence of edges both locally at a fixed index value and globally over arange of values. The tests are developed for a high-dimensional regime, arerobust to model selection mistakes and do not require commonly assumed minimumsignal strength. The testing procedures are based on a high dimensional,debiasing-free moment estimator, which uses a novel kernel smoothed Kendall'stau correlation matrix as an input statistic. The estimator consistentlyestimates the latent inverse Pearson correlation matrix uniformly in both indexvariable and kernel bandwidth. Its rate of convergence is shown to be minimaxoptimal. Thorough numerical simulations and an application to a neural imagingdataset support the usefulness of our method.
arxiv-15600-296 | Feedforward Sequential Memory Networks: A New Structure to Learn Long-term Dependency | http://arxiv.org/abs/1512.08301 | author:Shiliang Zhang, Cong Liu, Hui Jiang, Si Wei, Lirong Dai, Yu Hu category:cs.NE published:2015-12-28 summary:In this paper, we propose a novel neural network structure, namely\emph{feedforward sequential memory networks (FSMN)}, to model long-termdependency in time series without using recurrent feedback. The proposed FSMNis a standard fully-connected feedforward neural network equipped with somelearnable memory blocks in its hidden layers. The memory blocks use atapped-delay line structure to encode the long context information into afixed-size representation as short-term memory mechanism. We have evaluated theproposed FSMNs in several standard benchmark tasks, including speechrecognition and language modelling. Experimental results have shown FSMNssignificantly outperform the conventional recurrent neural networks (RNN),including LSTMs, in modeling sequential signals like speech or language.Moreover, FSMNs can be learned much more reliably and faster than RNNs or LSTMsdue to the inherent non-recurrent model structure.
arxiv-15600-297 | G-Learning: Taming the Noise in Reinforcement Learning via Soft Updates | http://arxiv.org/abs/1512.08562 | author:Roy Fox, Ari Pakman, Naftali Tishby category:cs.LG cs.IT math.IT published:2015-12-28 summary:Model-free reinforcement learning algorithms such as Q-learning performpoorly in the early stages of learning in noisy environments, because mucheffort is spent on unlearning biased estimates of the state-action function.The bias comes from selecting, among several noisy estimates, the apparentoptimum, which may actually be suboptimal. We propose G-learning, a newoff-policy learning algorithm that regularizes the noise in the space ofoptimal actions by penalizing deterministic policies at the beginning of thelearning. Moreover, it enables naturally incorporating prior distributions overoptimal actions when available. The stochastic nature of G-learning also makesit more cost-effective than Q-learning in noiseless but exploration-riskydomains. We illustrate these ideas in several examples where G-learning resultsin significant improvements of the learning rate and the learning cost.
arxiv-15600-298 | Outlier Detection In Large-scale Traffic Data By Naïve Bayes Method and Gaussian Mixture Model Method | http://arxiv.org/abs/1512.08413 | author:Philip Lam, Lili Wang, Henry Y. T. Ngan, Nelson H. C. Yung, Anthony G. O. Yeh category:cs.CV published:2015-12-28 summary:It is meaningful to detect outliers in traffic data for traffic management.However, this is a massive task for people from large-scale database todistinguish outliers. In this paper, we present two methods: Kernel SmoothingNa\"ive Bayes (NB) method and Gaussian Mixture Model (GMM) method toautomatically detect any hardware errors as well as abnormal traffic events intraffic data collected at a four-arm junction in Hong Kong. Traffic data wasrecorded in a video format, and converted to spatial-temporal (ST) trafficsignals by statistics. The ST signals are then projected to a two-dimensional(2D) (x,y)-coordinate plane by Principal Component Analysis (PCA) for dimensionreduction. We assume that inlier data are normal distributed. As such, the NBand GMM methods are successfully applied in outlier detection (OD) for trafficdata. The kernel smooth NB method assumes the existence of kernel distributionsin traffic data and uses Bayes' Theorem to perform OD. In contrast, the GMMmethod believes the traffic data is formed by the mixture of Gaussiandistributions and exploits confidence region for OD. This paper would addressthe modeling of each method and evaluate their respective performances.Experimental results show that the NB algorithm with Triangle kernel and GMMmethod achieve up to 93.78% and 94.50% accuracies, respectively.
arxiv-15600-299 | Graph entropies in texture segmentation of images | http://arxiv.org/abs/1512.08424 | author:Martin Welk category:cs.CV published:2015-12-28 summary:We study the applicability of a set of texture descriptors introduced inrecent work by the author to texture-based segmentation of images. The texturedescriptors under investigation result from applying graph indices fromquantitative graph theory to graphs encoding the local structure of images. Theunderlying graphs arise from the computation of morphological amoebas asstructuring elements for adaptive morphology, either as weighted or unweightedDijkstra search trees or as edge-weighted pixel graphs within structuringelements. In the present paper we focus on texture descriptors in which thegraph indices are entropy-based, and use them in a geodesic active contourframework for image segmentation. Experiments on several synthetic and onereal-world image are shown to demonstrate texture segmentation by thisapproach. Forthermore, we undertake an attempt to analyse selectedentropy-based texture descriptors with regard to what information about texturethey actually encode. Whereas this analysis uses some heuristic assumptions, itindicates that the graph-based texture descriptors are related to fractaldimension measures that have been proven useful in texture analysis.
arxiv-15600-300 | Communicating with sentences: A multi-word naming game model | http://arxiv.org/abs/1512.08347 | author:Yang Lou, Guanrong Chen, Jianwei Hu category:cs.CL physics.soc-ph published:2015-12-28 summary:Naming game simulates the process of naming a single object by a single word,in which a population of communicating agents can reach global consensusasymptotically through iteratively pair-wise conversations. In this paper, wepropose an extension of the single-word naming game, to a multi-word naminggame (MWNG), which simulates the naming game process when agents name an objectby a sentence (i.e., a series of multiple words) for describing a complexobject such as an opinion or an event. We first define several categories ofwords, and then organize sentences by combining words from different wordcategories. We refer to a formatted combination of several words as a pattern.In such an MWNG, through a pair-wise conversation, it requires the hearer toachieve consensus with the speaker with respect to both every single word inthe sentence as well as the sentence pattern, so as to guarantee the correctmeaning of the saying; otherwise, they fail reaching consensus in theinteraction. We employ three typical topologies used for the underlyingcommunication network, namely random-graph, small-world and scale-freenetworks. We validate the model by using both conventional English languagepatterns and man-made test sentence patterns in performing the MWNG. Oursimulation results show that: 1) the new sentence sharing model is an extensionof the classical lexicon sharing model; 2) the propagating, learning andconverging processes are more complicated than that in the conventional naminggame; 3) the convergence time is non-decreasing as the network becomes betterconnected; 4) the agents are prone to accept short sentence patterns. These newfindings may help deepen our understanding of the human language developmentfrom a network science perspective.
