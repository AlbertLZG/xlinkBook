arxiv-15600-1 | We don't need no bounding-boxes: Training object class detectors using only human verification | http://arxiv.org/pdf/1602.08405v1.pdf | author:Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio Ferrari category:cs.CV published:2016-02-26 summary:Training object class detectors typically requires a large set of images inwhich objects are annotated by bounding-boxes. However, manually drawingbounding-boxes is very time consuming. We propose a new scheme for trainingobject detectors which only requires annotators to verify bounding-boxesproduced automatically by the learning algorithm. Our scheme iterates betweenre-training the detector, re-localizing objects in the training images, andhuman verification. We use the verification signal both to improve re-trainingand to reduce the search space for re-localisation, which makes these stepsdifferent to what is normally done in a weakly supervised setting. Extensiveexperiments on PASCAL VOC 2007 show that (1) using human verification to updatedetectors and reduce the search space leads to the rapid production ofhigh-quality bounding-box annotations; (2) our scheme delivers detectorsperforming almost as good as those trained in a fully supervised setting,without ever drawing any bounding-box; (3) as the verification task is veryquick, our scheme substantially reduces total annotation time by a factor6x-9x.
arxiv-15600-2 | Top-N Recommendation with Novel Rank Approximation | http://arxiv.org/pdf/1602.07783v2.pdf | author:Zhao Kang, Qiang Cheng category:cs.IR cs.AI stat.ML published:2016-02-25 summary:The importance of accurate recommender systems has been widely recognized byacademia and industry. However, the recommendation quality is still rather low.Recently, a linear sparse and low-rank representation of the user-item matrixhas been applied to produce Top-N recommendations. This approach uses thenuclear norm as a convex relaxation for the rank function and has achievedbetter recommendation accuracy than the state-of-the-art methods. In the pastseveral years, solving rank minimization problems by leveraging nonconvexrelaxations has received increasing attention. Some empirical resultsdemonstrate that it can provide a better approximation to original problemsthan convex relaxation. In this paper, we propose a novel rank approximation toenhance the performance of Top-N recommendation systems, where theapproximation error is controllable. Experimental results on real data showthat the proposed rank approximation improves the Top-$N$ recommendationaccuracy substantially.
arxiv-15600-3 | Multivariate response and parsimony for Gaussian cluster-weighted models | http://arxiv.org/pdf/1411.0560v2.pdf | author:Utkarsh J. Dang, Antonio Punzo, Paul D. McNicholas, Salvatore Ingrassia, Ryan P. Browne category:stat.CO stat.ME stat.ML published:2014-11-03 summary:A family of parsimonious Gaussian cluster-weighted models is presented. Thisfamily concerns a multivariate extension to cluster-weighted modelling that canaccount for correlations between multivariate responses. Parsimony is attainedby constraining parts of an eigen-decomposition imposed on the componentcovariance matrices. A sufficient condition for identifiability is provided andan expectation-maximization algorithm is presented for parameter estimation.Model performance is investigated on both synthetic and classical real datasets and compared with some popular approaches. Finally, accounting for lineardependencies in the presence of a linear regression structure is shown to offerbetter performance, vis-\`{a}-vis clustering, over existing methodologies.
arxiv-15600-4 | Cortical Computation via Iterative Constructions | http://arxiv.org/pdf/1602.08357v1.pdf | author:Christos Papadimitrou, Samantha Petti, Santosh Vempala category:cs.NE cs.DS published:2016-02-26 summary:We study Boolean functions of an arbitrary number of input variables that canbe realized by simple iterative constructions based on constant-sizeprimitives. This restricted type of construction needs little globalcoordination or control and thus is a candidate for neurally feasiblecomputation. Valiant's construction of a majority function can be realized inthis manner and, as we show, can be generalized to any uniform thresholdfunction. We study the rate of convergence, finding that while linearconvergence to the correct function can be achieved for any threshold using afixed set of primitives, for quadratic convergence, the size of the primitivesmust grow as the threshold approaches 0 or 1. We also study finite realizationsof this process and the learnability of the functions realized. We show thatthe constructions realized are accurate outside a small interval near thetarget threshold, where the size of the construction grows as the inversesquare of the interval width. This phenomenon, that errors are higher closer tothresholds (and thresholds closer to the boundary are harder to represent), isa well-known cognitive finding.
arxiv-15600-5 | Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets | http://arxiv.org/pdf/1602.08350v1.pdf | author:Patrick O. Glauner, Andre Boechat, Lautaro Dolberg, Radu State, Franck Bettinger, Yves Rangoni, Diogo Duarte category:cs.LG cs.AI published:2016-02-26 summary:Non-technical losses (NTL) such as electricity theft cause significant harmto our economies, as in some countries they may range up to 40% of the totalelectricity distributed. Detecting NTLs requires costly on-site inspections.Accurate prediction of NTLs for customers using machine learning is thereforecrucial. To date, related research largely ignore that the two classes ofregular and non-regular customers are highly imbalanced, that NTL proportionsmay change and mostly consider small data sets, often not allowing to deploythe results in production. In this paper, we present a comprehensive approachto assess three NTL detection models for different NTL proportions in largereal world data sets of 100Ks of customers: Boolean rules, fuzzy logic andSupport Vector Machine. This work has resulted in appreciable results that areabout to be deployed in a leading industry solution. We believe that theconsiderations and observations made in this contribution are necessary forfuture smart meter research in order to report their effectiveness onimbalanced and large real world data sets.
arxiv-15600-6 | Bounded Rational Decision-Making in Feedforward Neural Networks | http://arxiv.org/pdf/1602.08332v1.pdf | author:Felix Leibfried, Daniel Alexander Braun category:cs.AI cs.LG cs.NE published:2016-02-26 summary:Bounded rational decision-makers transform sensory input into motor outputunder limited computational resources. Mathematically, such decision-makers canbe modeled as information-theoretic channels with limited transmission rate.Here, we apply this formalism for the first time to multilayer feedforwardneural networks. We derive synaptic weight update rules for two scenarios,where either each neuron is considered as a bounded rational decision-maker orthe network as a whole. In the update rules, bounded rationality translatesinto information-theoretically motivated types of regularization in weightspace. In experiments on the MNIST benchmark classification task forhandwritten digits, we show that such information-theoretic regularizationsuccessfully prevents overfitting across different architectures and attainsstate-of-the-art results for both ordinary and convolutional neural networks.
arxiv-15600-7 | Victory Sign Biometric for Terrorists Identification | http://arxiv.org/pdf/1602.08325v1.pdf | author:Ahmad B. A. Hassanat, Mahmoud B. Alhasanat, Mohammad Ali Abbadi, Eman Btoush, Mouhammd Al-Awadi, Ahmad S. Tarawneh category:cs.CV published:2016-02-26 summary:Covering the face and all body parts, sometimes the only evidence to identifya person is their hand geometry, and not the whole hand- only two fingers (theindex and the middle fingers) while showing the victory sign, as seen in manyterrorists videos. This paper investigates for the first time a new way toidentify persons, particularly (terrorists) from their victory sign. We havecreated a new database in this regard using a mobile phone camera, imaging thevictory signs of 50 different persons over two sessions. Simple measurementsfor the fingers, in addition to the Hu Moments for the areas of the fingerswere used to extract the geometric features of the shown part of the hand shownafter segmentation. The experimental results using the KNN classifier wereencouraging for most of the recorded persons; with about 40% to 93% totalidentification accuracy, depending on the features, distance metric and K used.
arxiv-15600-8 | Deep Spiking Networks | http://arxiv.org/pdf/1602.08323v1.pdf | author:Peter O'Connor, Max Welling category:cs.NE published:2016-02-26 summary:We introduce the Spiking Multi-Layer Perceptron (SMLP). The SMLP is a spikingversion of a conventional Multi-Layer Perceptron with rectified-linear units.Our architecture is event-based, meaning that neurons in the networkcommunicate by sending "events" to downstream neurons, and that the state ofeach neuron is only updated when it receives an event. We show that the SMLPbehaves identically, during both prediction and training, to a conventionaldeep network of rectified-linear units in the limiting case where we run thespiking network for a long time. We apply this architecture to a conventionalclassification problem (MNIST) and achieve performance very close to that of aconventional MLP with the same architecture. Our network is a naturalarchitecture for learning based on streaming event-based data, and haspotential applications in robotic systems systems, which require low power andlow response latency.
arxiv-15600-9 | Enhancing Genetic Algorithms using Multi Mutations | http://arxiv.org/pdf/1602.08313v1.pdf | author:Ahmad B. A. Hassanat, Esra'a Alkafaween, Nedal A. Al-Nawaiseh, Mohammad A. Abbadi, Mouhammd Alkasassbeh, Mahmoud B. Alhasanat category:cs.AI cs.NE published:2016-02-26 summary:Mutation is one of the most important stages of the genetic algorithm becauseof its impact on the exploration of global optima, and to overcome prematureconvergence. There are many types of mutation, and the problem lies inselection of the appropriate type, where the decision becomes more difficultand needs more trial and error. This paper investigates the use of more thanone mutation operator to enhance the performance of genetic algorithms. Novelmutation operators are proposed, in addition to two selection strategies forthe mutation operators, one of which is based on selecting the best mutationoperator and the other randomly selects any operator. Several experiments onsome Travelling Salesman Problems (TSP) were conducted to evaluate the proposedmethods, and these were compared to the well-known exchange mutation andrearrangement mutation. The results show the importance of some of the proposedmethods, in addition to the significant enhancement of the genetic algorithm'sperformance, particularly when using more than one mutation operator.
arxiv-15600-10 | Theoretical Analysis of the $k$-Means Algorithm - A Survey | http://arxiv.org/pdf/1602.08254v1.pdf | author:Johannes Blömer, Christiane Lammersen, Melanie Schmidt, Christian Sohler category:cs.DS cs.LG published:2016-02-26 summary:The $k$-means algorithm is one of the most widely used clustering heuristics.Despite its simplicity, analyzing its running time and quality of approximationis surprisingly difficult and can lead to deep insights that can be used toimprove the algorithm. In this paper we survey the recent results in thisdirection as well as several extension of the basic $k$-means method.
arxiv-15600-11 | Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks | http://arxiv.org/pdf/1602.07868v2.pdf | author:Tim Salimans, Diederik P. Kingma category:cs.LG cs.AI cs.NE published:2016-02-25 summary:We present weight normalization: a reparameterization of the weight vectorsin a neural network that decouples the length of those weight vectors fromtheir direction. By reparameterizing the weights in this way we improve theconditioning of the optimization problem and we speed up convergence ofstochastic gradient descent. Our reparameterization is inspired by batchnormalization but does not introduce any dependencies between the examples in aminibatch. This means that our method can also be applied successfully torecurrent models such as LSTMs and to noise-sensitive applications such as deepreinforcement learning or generative models, for which batch normalization isless well suited. Although our method is much simpler, it still provides muchof the speed-up of full batch normalization. In addition, the computationaloverhead of our method is lower, permitting more optimization steps to be takenin the same amount of time. We demonstrate the usefulness of our method onapplications in supervised image recognition, generative modelling, and deepreinforcement learning.
arxiv-15600-12 | Multimodal Emotion Recognition Using Multimodal Deep Learning | http://arxiv.org/pdf/1602.08225v1.pdf | author:Wei Liu, Wei-Long Zheng, Bao-Liang Lu category:cs.HC cs.CV cs.LG published:2016-02-26 summary:To enhance the performance of affective models and reduce the cost ofacquiring physiological signals for real-world applications, we adoptmultimodal deep learning approach to construct affective models from multiplephysiological signals. For unimodal enhancement task, we indicate that the bestrecognition accuracy of 82.11% on SEED dataset is achieved with sharedrepresentations generated by Deep AutoEncoder (DAE) model. For multimodalfacilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE)achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets,respectively, which are much superior to the state-of-the-art approaches. Forcross-modal learning task, our experimental results demonstrate that the meanaccuracy of 66.34% is achieved on SEED dataset through shared representationsgenerated by EEG-based DAE as training samples and shared representationsgenerated by eye-based DAE as testing sample, and vice versa.
arxiv-15600-13 | Learning and Free Energy in Expectation Consistent Approximate Inference | http://arxiv.org/pdf/1602.08207v1.pdf | author:Alyson K. Fletcher category:cs.IT math.IT stat.ML published:2016-02-26 summary:Approximations of loopy belief propagation are commonly combined withexpectation-maximization (EM) for probabilistic inference problems when thedensities have unknown parameters. This work considers an approximate EMlearning method combined with Opper and Winther's Expectation ConsistentApproximate Inference method. The combined algorithm is called EM-EC and isshown to have a simple variational free energy interpretation. In addition, thealgorithm can provide a computationally efficient and general approach to anumber of learning problems with hidden states including empirical Bayesianforms of regression, classification, compressed sensing, and sparse Bayesianlearning. Systems with linear dynamics interconnected with non-Gaussian ornonlinear components can also be easily considered.
arxiv-15600-14 | Neural Networks with Few Multiplications | http://arxiv.org/pdf/1510.03009v3.pdf | author:Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, Yoshua Bengio category:cs.LG cs.NE published:2015-10-11 summary:For most deep learning algorithms training is notoriously time consuming.Since most of the computation in training neural networks is typically spent onfloating point multiplications, we investigate an approach to training thateliminates the need for most of these. Our method consists of two parts: Firstwe stochastically binarize weights to convert multiplications involved incomputing hidden states to sign changes. Second, while back-propagating errorderivatives, in addition to binarizing the weights, we quantize therepresentations at each layer to convert the remaining multiplications intobinary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10,SVHN) show that this approach not only does not hurt classification performancebut can result in even better performance than standard stochastic gradientdescent training, paving the way to fast, hardware-friendly training of neuralnetworks.
arxiv-15600-15 | Scalable and Sustainable Deep Learning via Randomized Hashing | http://arxiv.org/pdf/1602.08194v1.pdf | author:Ryan Spring, Anshumali Shrivastava category:stat.ML cs.LG cs.NE published:2016-02-26 summary:Current deep learning architectures are growing larger in order to learn fromenormous datasets.These architectures require giant matrix multiplicationoperations to train millions or billions of parameters during forward and backpropagation steps. These operations are very expensive from a computational andenergy standpoint. We present a novel technique to reduce the amount ofcomputation needed to train and test deep net-works drastically. Our approachcombines recent ideas from adaptive dropouts and randomized hashing for maximuminner product search to select only the nodes with the highest activationefficiently. Our new algorithm for training deep networks reduces the overallcomputational cost,of both feed-forward pass and backpropagation,by operatingon significantly fewer nodes. As a consequence, our algorithm only requires 5%of computations (multiplications) compared to traditional algorithms, withoutany loss in the accuracy. Furthermore, due to very sparse gradient updates, ouralgorithm is ideally suited for asynchronous training leading to near linearspeedup with increasing parallelism. We demonstrate the scalability andsustainability (energy efficiency) of our proposed algorithm via rigorousexperimental evaluations.
arxiv-15600-16 | Sub-Sampled Newton Methods II: Local Convergence Rates | http://arxiv.org/pdf/1601.04738v3.pdf | author:Farbod Roosta-Khorasani, Michael W. Mahoney category:math.OC cs.LG stat.ML published:2016-01-18 summary:Many data-fitting applications require the solution of an optimizationproblem involving a sum of large number of functions of high dimensionalparameter. Here, we consider the problem of minimizing a sum of $n$ functionsover a convex constraint set $\mathcal{X} \subseteq \mathbb{R}^{p}$ where both$n$ and $p$ are large. In such problems, sub-sampling as a way to reduce $n$can offer great amount of computational efficiency. Within the context of second order methods, we first give quantitative localconvergence results for variants of Newton's method where the Hessian isuniformly sub-sampled. Using random matrix concentration inequalities, one cansub-sample in a way that the curvature information is preserved. Using suchsub-sampling strategy, we establish locally Q-linear and Q-superlinearconvergence rates. We also give additional convergence results for when thesub-sampled Hessian is regularized by modifying its spectrum or Levenberg-typeregularization. Finally, in addition to Hessian sub-sampling, we consider sub-sampling thegradient as way to further reduce the computational complexity per iteration.We use approximate matrix multiplication results from randomized numericallinear algebra (RandNLA) to obtain the proper sampling strategy and weestablish locally R-linear convergence rates. In such a setting, we also showthat a very aggressive sample size increase results in a R-superlinearlyconvergent algorithm. While the sample size depends on the condition number of the problem, ourconvergence rates are problem-independent, i.e., they do not depend on thequantities related to the problem. Hence, our analysis here can be used tocomplement the results of our basic framework from the companion paper, [38],by exploring algorithmic trade-offs that are important in practice.
arxiv-15600-17 | Sub-Sampled Newton Methods I: Globally Convergent Algorithms | http://arxiv.org/pdf/1601.04737v3.pdf | author:Farbod Roosta-Khorasani, Michael W. Mahoney category:math.OC cs.LG stat.ML published:2016-01-18 summary:Large scale optimization problems are ubiquitous in machine learning and dataanalysis and there is a plethora of algorithms for solving such problems. Manyof these algorithms employ sub-sampling, as a way to either speed up thecomputations and/or to implicitly implement a form of statisticalregularization. In this paper, we consider second-order iterative optimizationalgorithms and we provide bounds on the convergence of the variants of Newton'smethod that incorporate uniform sub-sampling as a means to estimate thegradient and/or Hessian. Our bounds are non-asymptotic and quantitative. Ouralgorithms are global and are guaranteed to converge from any initial iterate. Using random matrix concentration inequalities, one can sub-sample theHessian to preserve the curvature information. Our first algorithm incorporatesHessian sub-sampling while using the full gradient. We also give additionalconvergence results for when the sub-sampled Hessian is regularized bymodifying its spectrum or ridge-type regularization. Next, in addition toHessian sub-sampling, we also consider sub-sampling the gradient as a way tofurther reduce the computational complexity per iteration. We use approximatematrix multiplication results from randomized numerical linear algebra toobtain the proper sampling strategy. In all these algorithms, computing theupdate boils down to solving a large scale linear system, which can becomputationally expensive. As a remedy, for all of our algorithms, we also giveglobal convergence results for the case of inexact updates where such linearsystem is solved only approximately. This paper has a more advanced companion paper, [42], in which we demonstratethat, by doing a finer-grained analysis, we can get problem-independent boundsfor local convergence of these algorithms and explore trade-offs to improveupon the basic results of the present paper.
arxiv-15600-18 | Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn | http://arxiv.org/pdf/1602.08186v1.pdf | author:Viet Ha-Thuc, Ye Xu, Satya Pradeep Kanduri, Xianren Wu, Vijay Dialani, Yan Yan, Abhishek Gupta, Shakti Sinha category:cs.IR cs.LG published:2016-02-26 summary:One key challenge in talent search is how to translate complex criteria of ahiring position into a search query. This typically requires deep knowledge onwhich skills are typically needed for the position, what are theiralternatives, which companies are likely to have such candidates, etc. However,listing examples of suitable candidates for a given position is a relativelyeasy job. Therefore, in order to help searchers overcome this challenge, wedesign a next generation of talent search paradigm at LinkedIn: Search by IdealCandidates. This new system only needs the searcher to input one or severalexamples of suitable candidates for the position. The system will generate aquery based on the input candidates and then retrieve and rank results based onthe query as well as the input candidates. The query is also shown to thesearcher to make the system transparent and to allow the searcher to interactwith it. As the searcher modifies the initial query and makes it deviate fromthe ideal candidates, the search ranking function dynamically adjusts anrefreshes the ranking results balancing between the roles of query and idealcandidates. As of writing this paper, the new system is being launched to ourcustomers.
arxiv-15600-19 | Unifying distillation and privileged information | http://arxiv.org/pdf/1511.03643v3.pdf | author:David Lopez-Paz, Léon Bottou, Bernhard Schölkopf, Vladimir Vapnik category:stat.ML cs.LG published:2015-11-11 summary:Distillation (Hinton et al., 2015) and privileged information (Vapnik &Izmailov, 2015) are two techniques that enable machines to learn from othermachines. This paper unifies these two techniques into generalizeddistillation, a framework to learn from multiple machines and datarepresentations. We provide theoretical and causal insight about the innerworkings of generalized distillation, extend it to unsupervised, semisupervisedand multitask learning scenarios, and illustrate its efficacy on a variety ofnumerical simulations on both synthetic and real-world data.
arxiv-15600-20 | Optimal Binary Classifier Aggregation for General Losses | http://arxiv.org/pdf/1510.00452v4.pdf | author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML published:2015-10-01 summary:We address the problem of aggregating an ensemble of binary classifiers in asemi-supervised setting. Recently, this problem was solved optimally using agame-theoretic approach, but that analysis was specific to the 0-1 loss. Inthis paper, we generalize the minimax optimal algorithm of the previous work toa very general, novel class of loss functions, including but not limited to allconvex surrogates, while extending its performance and efficiency guarantees. The result is a family of parameter-free ensemble aggregation algorithmswhich use labeled and unla- beled data; these are as efficient as linearlearning and prediction for convex risk minimization, but work without anyrelaxations on many non-convex loss functions. The prediction algorithms take aform familiar in decision theory, applying sigmoid functions to a generalizednotion of ensemble margin, but without the assumptions typically made inmargin-based learning.
arxiv-15600-21 | Harnessing disordered quantum dynamics for machine learning | http://arxiv.org/pdf/1602.08159v1.pdf | author:Keisuke Fujii, Kohei Nakajima category:quant-ph cs.AI cs.LG cs.NE nlin.CD published:2016-02-26 summary:Quantum computer has an amazing potential of fast information processing.However, realisation of a digital quantum computer is still a challengingproblem requiring highly accurate controls and key application strategies. Herewe propose a novel platform, quantum reservoir computing, to solve these issuessuccessfully by exploiting natural quantum dynamics, which is ubiquitous inlaboratories nowadays, for machine learning. In this framework, nonlineardynamics including classical chaos can be universally emulated in quantumsystems. A number of numerical experiments show that quantum systems consistingof at most seven qubits possess computational capabilities comparable toconventional recurrent neural networks of 500 nodes. This discovery opens up anew paradigm for information processing with artificial intelligence powered byquantum physics.
arxiv-15600-22 | Black-box $α$-divergence Minimization | http://arxiv.org/pdf/1511.03243v2.pdf | author:José Miguel Hernández-Lobato, Yingzhen Li, Mark Rowland, Daniel Hernández-Lobato, Thang Bui, Richard E. Turner category:stat.ML published:2015-11-10 summary:We present \emph{black-box alpha} (BB-$\alpha$), an approximate inferencemethod based on the minimization of $\alpha$-divergences between probabilitydistributions. BB-$\alpha$ scales to large datasets since it can be implementedusing stochastic gradient descent. BB-$\alpha$ can be applied to complexprobabilistic models with little effort since it only requires as input thelikelihood function and its gradients. These gradients can be easily obtainedusing automatic differentiation. By tuning divergence parameter $\alpha$, themethod is able to interpolate between variational Bayes and an expectationpropagation-like algorithm. Experiments on probit regression, neural networkregression and classification problems illustrate the accuracy of the posteriorapproximations obtained with BB-$\alpha$.
arxiv-15600-23 | Layered Adaptive Importance Sampling | http://arxiv.org/pdf/1505.04732v3.pdf | author:L. Martino, V. Elvira, D. Luengo, J. Corander category:stat.CO cs.LG stat.ML published:2015-05-18 summary:Monte Carlo methods represent the "de facto" standard for approximatingcomplicated integrals involving multidimensional target distributions. In orderto generate random realizations from the target distribution, Monte Carlotechniques use simpler proposal probability densities to draw candidatesamples. The performance of any such method is strictly related to thespecification of the proposal distribution, such that unfortunate choiceseasily wreak havoc on the resulting estimators. In this work, we introduce alayered (i.e., hierarchical) procedure to generate samples employed within aMonte Carlo scheme. This approach ensures that an appropriate equivalentproposal density is always obtained automatically (thus eliminating the risk ofa catastrophic performance), although at the expense of a moderate increase inthe complexity. Furthermore, we provide a general unified importance sampling(IS) framework, where multiple proposal densities are employed and several ISschemes are introduced by applying the so-called deterministic mixtureapproach. Finally, given these schemes, we also propose a novel class ofadaptive importance samplers using a population of proposals, where theadaptation is driven by independent parallel or interacting Markov Chain MonteCarlo (MCMC) chains. The resulting algorithms efficiently combine the benefitsof both IS and MCMC methods.
arxiv-15600-24 | Learning to Abstain from Binary Prediction | http://arxiv.org/pdf/1602.08151v1.pdf | author:Akshay Balsubramani category:cs.LG stat.ML published:2016-02-25 summary:We address how to learn a binary classifier capable of abstaining from makinga label prediction. Such a classifier hopes to abstain where it would be mostinaccurate if forced to predict, so it has two goals in tension with eachother: minimizing errors, and avoiding abstaining unnecessarily often. In this work, we exactly characterize the best achievable tradeoff betweenthese two goals in a general semi-supervised setting, given an ensemble ofclassifiers of varying competence as well as unlabeled data on which we wish topredict or abstain. We give an algorithm for learning a classifier which tradesoff its errors with abstentions in a minimax optimal manner. This algorithm isas efficient as linear learning and prediction, and comes with strong androbust theoretical guarantees. Our analysis extends to a large class of lossfunctions and other scenarios, including ensembles comprised of "specialist"classifiers that can themselves abstain.
arxiv-15600-25 | Autonomous navigation for low-altitude UAVs in urban areas | http://arxiv.org/pdf/1602.08141v1.pdf | author:Thomas Castelli, Aidean Sharghi, Don Harper, Alain Tremeau, Mubarak Shah category:cs.RO cs.CV published:2016-02-25 summary:In recent years, consumer Unmanned Aerial Vehicles have become very popular,everyone can buy and fly a drone without previous experience, which raisesconcern in regards to regulations and public safety. In this paper, we presenta novel approach towards enabling safe operation of such vehicles in urbanareas. Our method uses geodetically accurate dataset images with GeographicalInformation System (GIS) data of road networks and buildings provided by GoogleMaps, to compute a weighted A* shortest path from start to end locations of amission. Weights represent the potential risk of injuries for individuals inall categories of land-use, i.e. flying over buildings is considered safer thanabove roads. We enable safe UAV operation in regards to 1- land-use bycomputing a static global path dependent on environmental structures, and 2-avoiding flying over moving objects such as cars and pedestrians by dynamicallyoptimizing the path locally during the flight. As all input sources are firstgeo-registered, pixels and GPS coordinates are equivalent, it therefore allowsus to generate an automated and user-friendly mission with GPS waypointsreadable by consumer drones' autopilots. We simulated 54 missions and showsignificant improvement in maximizing UAV's standoff distance to moving objectswith a quantified safety parameter over 40 times better than the naive straightline navigation.
arxiv-15600-26 | Adaptive Frequency Cepstral Coefficients for Word Mispronunciation Detection | http://arxiv.org/pdf/1602.08132v1.pdf | author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.CV published:2016-02-25 summary:Systems based on automatic speech recognition (ASR) technology can provideimportant functionality in computer assisted language learning applications.This is a young but growing area of research motivated by the large number ofstudents studying foreign languages. Here we propose a Hidden Markov Model(HMM)-based method to detect mispronunciations. Exploiting the specific dialogscripting employed in language learning software, HMMs are trained fordifferent pronunciations. New adaptive features have been developed andobtained through an adaptive warping of the frequency scale prior to computingthe cepstral coefficients. The optimization criterion used for the warpingfunction is to maximize separation of two major groups of pronunciations(native and non-native) in terms of classification rate. Experimental resultsshow that the adaptive frequency scale yields a better coefficientrepresentation leading to higher classification rates in comparison withconventional HMMs using Mel-frequency cepstral coefficients.
arxiv-15600-27 | PCA Method for Automated Detection of Mispronounced Words | http://arxiv.org/pdf/1602.08128v1.pdf | author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.CL cs.LG published:2016-02-25 summary:This paper presents a method for detecting mispronunciations with the aim ofimproving Computer Assisted Language Learning (CALL) tools used by foreignlanguage learners. The algorithm is based on Principle Component Analysis(PCA). It is hierarchical with each successive step refining the estimate toclassify the test word as being either mispronounced or correct. Preprocessingbefore detection, like normalization and time-scale modification, isimplemented to guarantee uniformity of the feature vectors input to thedetection system. The performance using various features including spectrogramsand Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.Best results were obtained using MFCCs, achieving up to 99% accuracy in wordverification and 93% in native/non-native classification. Compared with HiddenMarkov Models (HMMs) which are used pervasively in recognition application,this particular approach is computational efficient and effective when trainingdata is limited.
arxiv-15600-28 | Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network | http://arxiv.org/pdf/1602.08118v1.pdf | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2016-02-25 summary:Recurrent neural networks (RNN) are capable of learning to encode and exploitactivation history over an arbitrary timescale. However, in practice, state ofthe art gradient descent based training methods are known to suffer fromdifficulties in learning long term dependencies. Here, we describe a noveltraining method that involves concurrent parallel cloned networks, each sharingthe same weights, each trained at different stimulus phase and each maintainingindependent activation histories. Training proceeds by recursively performingbatch-updates over the parallel clones as activation history is progressivelyincreased. This allows conflicts to propagate hierarchically from short-termcontexts towards longer-term contexts until they are resolved. We illustratethe parallel clones method and hierarchical conflict propagation with acharacter-level deep RNN tasked with memorizing a paragraph of Moby Dick (byHerman Melville).
arxiv-15600-29 | MuProp: Unbiased Backpropagation for Stochastic Neural Networks | http://arxiv.org/pdf/1511.05176v3.pdf | author:Shixiang Gu, Sergey Levine, Ilya Sutskever, Andriy Mnih category:cs.LG published:2015-11-16 summary:Deep neural networks are powerful parametric models that can be trainedefficiently using the backpropagation algorithm. Stochastic neural networkscombine the power of large parametric functions with that of graphical models,which makes it possible to learn very complex distributions. However, asbackpropagation is not directly applicable to stochastic networks that includediscrete sampling operations within their computational graph, training suchnetworks remains difficult. We present MuProp, an unbiased gradient estimatorfor stochastic networks, designed to make this task easier. MuProp improves onthe likelihood-ratio estimator by reducing its variance using a control variatebased on the first-order Taylor expansion of a mean-field network. Crucially,unlike prior attempts at using backpropagation for training stochasticnetworks, the resulting estimator is unbiased and well behaved. Our experimentson structured output prediction and discrete latent variable modelingdemonstrate that MuProp yields consistently good performance across a range ofdifficult tasks.
arxiv-15600-30 | PCA/LDA Approach for Text-Independent Speaker Recognition | http://arxiv.org/pdf/1602.08045v1.pdf | author:Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith category:cs.SD cs.LG published:2016-02-25 summary:Various algorithms for text-independent speaker recognition have beendeveloped through the decades, aiming to improve both accuracy and e?ciency.This paper presents a novel PCA/LDA-based approach that is faster thantraditional statistical model-based methods and achieves competitive results.First, the performance based on only PCA and only LDA is measured; then a mixedmodel, taking advantages of both methods, is introduced. A subset of the TIMITcorpus composed of 200 male speakers, is used for enrollment, validation andtesting. The best results achieve 100%; 96% and 95% classi?cation rate atpopulation level 50; 100 and 200, using 39-dimensional MFCC features with deltaand double delta. These results are based on 12-second text-independent speechfor training and 4-second data for test. These are comparable to theconventional MFCC-GMM methods, but require signi?cantly less time to train andoperate.
arxiv-15600-31 | Local entropy as a measure for sampling solutions in Constraint Satisfaction Problems | http://arxiv.org/pdf/1511.05634v2.pdf | author:Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, Riccardo Zecchina category:stat.ML G.1.6; I.2.M published:2015-11-18 summary:We introduce a novel Entropy-driven Monte Carlo (EdMC) strategy toefficiently sample solutions of random Constraint Satisfaction Problems (CSPs).First, we extend a recent result that, using a large-deviation analysis, showsthat the geometry of the space of solutions of the Binary Perceptron LearningProblem (a prototypical CSP), contains regions of very high-density ofsolutions. Despite being sub-dominant, these regions can be found by optimizinga local entropy measure. Building on these results, we construct a fast solverthat relies exclusively on a local entropy estimate, and can be applied togeneral CSPs. We describe its performance not only for the Perceptron LearningProblem but also for the random $K$-Satisfiabilty Problem (another prototypicalCSP with a radically different structure), and show numerically that a simplezero-temperature Metropolis search in the smooth local entropy landscape canreach sub-dominant clusters of optimal solutions in a small number of steps,while standard Simulated Annealing either requires extremely long coolingprocedures or just fails. We also discuss how the EdMC can heuristically bemade even more efficient for the cases we studied.
arxiv-15600-32 | Meta-learning within Projective Simulation | http://arxiv.org/pdf/1602.08017v1.pdf | author:Adi Makmal, Alexey A. Melnikov, Vedran Dunjko, Hans J. Briegel category:cs.AI cs.LG stat.ML published:2016-02-25 summary:Learning models of artificial intelligence can nowadays perform very well ona large variety of tasks. However, in practice different task environments arebest handled by different learning models, rather than a single, universal,approach. Most non-trivial models thus require the adjustment of several tomany learning parameters, which is often done on a case-by-case basis by anexternal party. Meta-learning refers to the ability of an agent to autonomouslyand dynamically adjust its own learning parameters, or meta-parameters. In thiswork we show how projective simulation, a recently developed model ofartificial intelligence, can naturally be extended to account for meta-learningin reinforcement learning settings. The projective simulation approach is basedon a random walk process over a network of clips. The suggested meta-learningscheme builds upon the same design and employs clip networks to monitor theagent's performance and to adjust its meta-parameters "on the fly". Wedistinguish between "reflexive adaptation" and "adaptation through learning",and show the utility of both approaches. In addition, a trade-off betweenflexibility and learning-time is addressed. The extended model is examined onthree different kinds of reinforcement learning tasks, in which the agent hasdifferent optimal values of the meta-parameters, and is shown to perform well,reaching near-optimal to optimal success rates in all of them, without everneeding to manually adjust any meta-parameter.
arxiv-15600-33 | Prioritized Experience Replay | http://arxiv.org/pdf/1511.05952v4.pdf | author:Tom Schaul, John Quan, Ioannis Antonoglou, David Silver category:cs.LG published:2015-11-18 summary:Experience replay lets online reinforcement learning agents remember andreuse experiences from the past. In prior work, experience transitions wereuniformly sampled from a replay memory. However, this approach simply replaystransitions at the same frequency that they were originally experienced,regardless of their significance. In this paper we develop a framework forprioritizing experience, so as to replay important transitions more frequently,and therefore learn more efficiently. We use prioritized experience replay inDeep Q-Networks (DQN), a reinforcement learning algorithm that achievedhuman-level performance across many Atari games. DQN with prioritizedexperience replay achieves a new state-of-the-art, outperforming DQN withuniform replay on 41 out of 49 games.
arxiv-15600-34 | Practical Riemannian Neural Networks | http://arxiv.org/pdf/1602.08007v1.pdf | author:Gaétan Marceau-Caron, Yann Ollivier category:cs.NE cs.LG stat.ML published:2016-02-25 summary:We provide the first experimental results on non-synthetic datasets for thequasi-diagonal Riemannian gradient descents for neural networks introduced in[Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as apreviously unpublished electroencephalogram dataset. The quasi-diagonalRiemannian algorithms consistently beat simple stochastic gradient gradientdescents by a varying margin. The computational overhead with respect to simplebackpropagation is around a factor $2$. Perhaps more interestingly, thesemethods also reach their final performance quickly, thus requiring fewertraining epochs and a smaller total computation time. We also present an implementation guide to these Riemannian gradient descentsfor neural networks, showing how the quasi-diagonal versions can be implementedwith minimal effort on top of existing routines which compute gradients.
arxiv-15600-35 | Sparse Estimation of Multivariate Poisson Log-Normal Model and Inverse Covariance for Count Data | http://arxiv.org/pdf/1602.07337v2.pdf | author:Hao Wu, Xinwei Deng, Naren Ramakrishnan category:stat.ME cs.LG published:2016-02-22 summary:Modeling data with multivariate count responses is a challenge problem due tothe discrete nature of the responses. Such data are commonly observed in manyapplications such as biology, epidemiology and social studies. The existingmethods for univariate count response cannot be easily extended to themultivariate situation since the dependency among multiple responses needs tobe properly accommodated. In this paper, we propose a multivariate PoissonLog-Normal regression model for multivariate data with count responses. Anefficient Monte Carlo EM algorithm is also developed to facilitate the modelestimation. By simultaneously estimating the regression coefficients andinverse covariance matrix over the latent variables, the proposed regressionmodel takes advantages of association among multiple count responses to improvethe model prediction performance. Simulation studies are conducted tosystematically evaluate the performance of the proposed method in comparisonwith conventional methods. The proposed method is further used to analyze areal influenza-like illness data, showing accurate prediction and forecastingwith meaningful interpretation.
arxiv-15600-36 | How effective can simple ordinal peer grading be? | http://arxiv.org/pdf/1602.07985v1.pdf | author:Ioannis Caragiannis, George A. Krimpas, Alexandros A. Voudouris category:cs.AI cs.DS cs.LG published:2016-02-25 summary:Ordinal peer grading has been proposed as a simple and scalable solution forcomputing reliable information about student performance in massive open onlinecourses. The idea is to outsource the grading task to the students themselvesas follows. After the end of an exam, each student is asked to rank ---in termsof quality--- a bundle of exam papers by fellow students. An aggregation rulewill then combine the individual rankings into a global one that contains allstudents. We define a broad class of simple aggregation rules and present atheoretical framework for assessing their effectiveness. When statisticalinformation about the grading behaviour of students is available, the frameworkcan be used to compute the optimal rule from this class with respect to aseries of performance objectives. For example, a natural rule known as Borda isproved to be optimal when students grade correctly. In addition, we presentextensive simulations and a field experiment that validate our theory and proveit to be extremely accurate in predicting the performance of aggregation ruleseven when only rough information about grading behaviour is available.
arxiv-15600-37 | Robust Convolutional Neural Networks under Adversarial Noise | http://arxiv.org/pdf/1511.06306v2.pdf | author:Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello category:cs.LG cs.CV published:2015-11-19 summary:Recent studies have shown that Convolutional Neural Networks (CNNs) arevulnerable to a small perturbation of input called "adversarial examples". Inthis work, we propose a new feedforward CNN that improves robustness in thepresence of adversarial noise. Our model uses stochastic additive noise addedto the input image and to the CNN models. The proposed model operates inconjunction with a CNN trained with either standard or adversarial objectivefunction. In particular, convolution, max-pooling, and ReLU layers are modifiedto benefit from the noise model. Our feedforward model is parameterized by onlya mean and variance per pixel which simplifies computations and makes ourmethod scalable to a deep architecture. From CIFAR-10 and ImageNet test, theproposed model outperforms other methods and the improvement is more evidentfor difficult classification tasks or stronger adversarial noise.
arxiv-15600-38 | Accurate Urban Road Centerline Extraction from VHR Imagery via Multiscale Segmentation and Tensor Voting | http://arxiv.org/pdf/1508.06163v2.pdf | author:Guangliang Cheng, Feiyun Zhu, Shiming Xiang, Chunhong Pan category:cs.CV published:2015-08-25 summary:It is very useful and increasingly popular to extract accurate roadcenterlines from very-high-resolution (VHR) re- mote sensing imagery forvarious applications, such as road map generation and updating etc. There arethree shortcomings of current methods: (a) Due to the noise and occlusions(owing to vehicles and trees), most road extraction methods bring inheterogeneous classification results; (b) Morphological thinning algorithm iswidely used to extract road centerlines, while it pro- duces small spurs aroundthe centerlines; (c) Many methods are ineffective to extract centerlines aroundthe road intersections. To address the above three issues, we propose a novelmethod to ex- tract smooth and complete road centerlines via three techniques:the multiscale joint collaborative representation (MJCR) & graph cuts (GC),tensor voting (TV) & non-maximum suppression (NMS) and fitting based connectionalgorithm. Specifically, a MJCR-GC based road area segmentation method isproposed by incorporating mutiscale features and spatial information. In thisway, a homogenous road segmentation result is achieved. Then, to obtain asmooth and correct road centerline network, a TV-NMS based centerlineextraction method is introduced. This method not only extracts smooth roadcenterlines, but also connects the discontinuous road centerlines. Finally, toovercome the ineffectiveness of current methods in the road intersection, afitting based road centerline connection algorithm is proposed. As a result, wecan get a complete road centerline network. Extensive experiments on twodatasets demonstrate that our method achieves higher quantitative results, aswell as more satisfactory visual performances by comparing with state-of-the-art methods.
arxiv-15600-39 | Thompson Sampling is Asymptotically Optimal in General Environments | http://arxiv.org/pdf/1602.07905v1.pdf | author:Jan Leike, Tor Lattimore, Laurent Orseau, Marcus Hutter category:cs.LG cs.AI stat.ML published:2016-02-25 summary:We discuss a variant of Thompson sampling for nonparametric reinforcementlearning in a countable classes of general stochastic environments. Theseenvironments can be non-Markov, non-ergodic, and partially observable. We showthat Thompson sampling learns the environment class in the sense that (1)asymptotically its value converges to the optimal value in mean and (2) given arecoverability assumption regret is sublinear.
arxiv-15600-40 | Expanded Parts Model for Semantic Description of Humans in Still Images | http://arxiv.org/pdf/1509.04186v2.pdf | author:Gaurav Sharma, Frederic Jurie, Cordelia Schmid category:cs.CV published:2015-09-14 summary:We introduce an Expanded Parts Model (EPM) for recognizing human attributes(e.g. young, short hair, wearing suit) and actions (e.g. running, jumping) instill images. An EPM is a collection of part templates which are learntdiscriminatively to explain specific scale-space regions in the images (inhuman centric coordinates). This is in contrast to current models which consistof a relatively few (i.e. a mixture of) 'average' templates. EPM uses only asubset of the parts to score an image and scores the image sparsely in space,i.e. it ignores redundant and random background in an image. To learn ourmodel, we propose an algorithm which automatically mines parts and learnscorresponding discriminative templates together with their respective locationsfrom a large number of candidate parts. We validate our method on three recentchallenging datasets of human attributes and actions. We obtain convincingqualitative and state-of-the-art quantitative results on the three datasets.
arxiv-15600-41 | Firefly Algorithm for optimization problems with non-continuous variables: A Review and Analysis | http://arxiv.org/pdf/1602.07884v1.pdf | author:Surafel Luleseged Tilahun, Jean Medard T Ngnotchouye category:cs.NE published:2016-02-25 summary:Firefly algorithm is a swarm based metaheuristic algorithm inspired by theflashing behavior of fireflies. It is an effective and an easy to implementalgorithm. It has been tested on different problems from different disciplinesand found to be effective. Even though the algorithm is proposed foroptimization problems with continuous variables, it has been modified and usedfor problems with non-continuous variables, including binary and integer valuedproblems. In this paper a detailed review of this modifications of fireflyalgorithm for problems with non-continuous variables will be discussed. Thestrength and weakness of the modifications along with possible future workswill be presented.
arxiv-15600-42 | CNN for License Plate Motion Deblurring | http://arxiv.org/pdf/1602.07873v1.pdf | author:Pavel Svoboda, Michal Hradis, Lukas Marsik, Pavel Zemcik category:cs.CV published:2016-02-25 summary:In this work we explore the previously proposed approach of direct blinddeconvolution and denoising with convolutional neural networks in a situationwhere the blur kernels are partially constrained. We focus on blurred imagesfrom a real-life traffic surveillance system, on which we, for the first time,demonstrate that neural networks trained on artificial data provide superiorreconstruction quality on real images compared to traditional blinddeconvolution methods. The training data is easy to obtain by blurring sharpphotos from a target system with a very rough approximation of the expectedblur kernels, thereby allowing custom CNNs to be trained for a specificapplication (image content and blur range). Additionally, we evaluate thebehavior and limits of the CNNs with respect to blur direction range andlength.
arxiv-15600-43 | Projected Estimators for Robust Semi-supervised Classification | http://arxiv.org/pdf/1602.07865v1.pdf | author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG published:2016-02-25 summary:For semi-supervised techniques to be applied safely in practice we at leastwant methods to outperform their supervised counterparts. We study thisquestion for classification using the well-known quadratic surrogate lossfunction. Using a projection of the supervised estimate onto a set ofconstraints imposed by the unlabeled data, we find we can safely improve overthe supervised solution in terms of this quadratic loss. Unlike otherapproaches to semi-supervised learning, the procedure does not rely onassumptions that are not intrinsic to the classifier at hand. It istheoretically demonstrated that, measured on the labeled and unlabeled trainingdata, this semi-supervised procedure never gives a lower quadratic loss thanthe supervised alternative. To our knowledge this is the first approach thatoffers such strong, albeit conservative, guarantees for improvement over thesupervised solution. The characteristics of our approach are explicated usingbenchmark datasets to further understand the similarities and differencesbetween the quadratic loss criterion used in the theoretical results and theclassification accuracy often considered in practice.
arxiv-15600-44 | Learning Gaussian Graphical Models With Fractional Marginal Pseudo-likelihood | http://arxiv.org/pdf/1602.07863v1.pdf | author:Janne Leppä-aho, Johan Pensar, Teemu Roos, Jukka Corander category:stat.ML cs.LG published:2016-02-25 summary:We propose a Bayesian approximate inference method for learning thedependence structure of a Gaussian graphical model. Using pseudo-likelihood, wederive an analytical expression to approximate the marginal likelihood for anarbitrary graph structure without invoking any assumptions aboutdecomposability. The majority of the existing methods for learning Gaussiangraphical models are either restricted to decomposable graphs or requirespecification of a tuning parameter that may have a substantial impact onlearned structures. By combining a simple sparsity inducing prior for the graphstructures with a default reference prior for the model parameters, we obtain afast and easily applicable scoring function that works well for evenhigh-dimensional data. We demonstrate the favourable performance of ourapproach by large-scale comparisons against the leading methods for learningnon-decomposable Gaussian graphical models. A theoretical justification for ourmethod is provided by showing that it yields a consistent estimator of thegraph structure.
arxiv-15600-45 | Probably Approximately Correct Greedy Maximization | http://arxiv.org/pdf/1602.07860v1.pdf | author:Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek category:cs.AI cs.LG stat.ML published:2016-02-25 summary:Submodular function maximization finds application in a variety of real-worlddecision-making problems. However, most existing methods, based on greedymaximization, assume it is computationally feasible to evaluate F, the functionbeing maximized. Unfortunately, in many realistic settings F is too expensiveto evaluate exactly even once. We present probably approximately correct greedymaximization, which requires access only to cheap anytime confidence bounds onF and uses them to prune elements. We show that, with high probability, ourmethod returns an approximately optimal set. We propose novel, cheap confidencebounds for conditional entropy, which appears in many common choices of F andfor which it is difficult to find unbiased or bounded estimates. Finally,results on a real-world dataset from a multi-camera tracking system in ashopping mall demonstrate that our approach performs comparably to existingmethods, but at a fraction of the computational cost.
arxiv-15600-46 | Kernel Mean Shrinkage Estimators | http://arxiv.org/pdf/1405.5505v3.pdf | author:Krikamol Muandet, Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf category:stat.ML cs.LG published:2014-05-21 summary:A mean function in a reproducing kernel Hilbert space (RKHS), or a kernelmean, is central to kernel methods in that it is used by many classicalalgorithms such as kernel principal component analysis, and it also forms thecore inference step of modern kernel methods that rely on embedding probabilitydistributions in RKHSs. Given a finite sample, an empirical average has beenused commonly as a standard estimator of the true kernel mean. Despite awidespread use of this estimator, we show that it can be improved thanks to thewell-known Stein phenomenon. We propose a new family of estimators calledkernel mean shrinkage estimators (KMSEs), which benefit from both theoreticaljustifications and good empirical performance. The results demonstrate that theproposed estimators outperform the standard one, especially in a "large d,small n" paradigm.
arxiv-15600-47 | Fast Nonsmooth Regularized Risk Minimization with Continuation | http://arxiv.org/pdf/1602.07844v1.pdf | author:Shuai Zheng, Ruiliang Zhang, James T. Kwok category:cs.LG published:2016-02-25 summary:In regularized risk minimization, the associated optimization problem becomesparticularly difficult when both the loss and regularizer are nonsmooth.Existing approaches either have slow or unclear convergence properties, arerestricted to limited problem subclasses, or require careful setting of asmoothing parameter. In this paper, we propose a continuation algorithm that isapplicable to a large class of nonsmooth regularized risk minimizationproblems, can be flexibly used with a number of existing solvers for theunderlying smoothed subproblem, and with convergence results on the wholealgorithm rather than just one of its subproblems. In particular, whenaccelerated solvers are used, the proposed algorithm achieves the fastest knownrates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convexproblems. Experiments on nonsmooth classification and regression tasksdemonstrate that the proposed algorithm outperforms the state-of-the-art.
arxiv-15600-48 | Automated Word Prediction in Bangla Language Using Stochastic Language Models | http://arxiv.org/pdf/1602.07803v1.pdf | author:Md. Masudul Haque, Md. Tarek Habib, Md. Mokhlesur Rahman category:cs.CL published:2016-02-25 summary:Word completion and word prediction are two important phenomena in typingthat benefit users who type using keyboard or other similar devices. They canhave profound impact on the typing of disable people. Our work is based on wordprediction on Bangla sentence by using stochastic, i.e. N-gram language modelsuch as unigram, bigram, trigram, deleted Interpolation and backoff models forauto completing a sentence by predicting a correct word in a sentence whichsaves time and keystrokes of typing and also reduces misspelling. We use largedata corpus of Bangla language of different word types to predict correct wordwith the accuracy as much as possible. We have found promising results. We hopethat our work will impact on the baseline for automated Bangla typing.
arxiv-15600-49 | Expectation Consistent Approximate Inference: Generalizations and Convergence | http://arxiv.org/pdf/1602.07795v1.pdf | author:Alyson Fletcher, Mojtaba Sahraee-Ardakan, Sundeep Rangan, Philip Schniter category:cs.IT math.IT stat.ML published:2016-02-25 summary:Approximations of loopy belief propagation, including expectation propagationand approximate message passing, have attracted considerable attention forprobabilistic inference problems. This paper proposes and analyzes ageneralization of Opper and Winther's expectation consistent (EC) approximateinference method. The proposed method, called Generalized ExpectationConsistency (GEC), can be applied to both maximum a posteriori (MAP) andminimum mean squared error (MMSE) estimation. Here we characterize its fixedpoints, convergence, and performance relative to the replica prediction ofoptimality.
arxiv-15600-50 | On Training Bi-directional Neural Network Language Model with Noise Contrastive Estimation | http://arxiv.org/pdf/1602.06064v3.pdf | author:Tianxing He, Yu Zhang, Jasha Droppo, Kai Yu category:cs.CL published:2016-02-19 summary:We propose to train bi-directional neural network language model(NNLM) withnoise contrastive estimation(NCE). Experiments are conducted on a rescore taskon the PTB data set. It is shown that NCE-trained bi-directional NNLMoutperformed the one trained by conventional maximum likelihood training. Butstill(regretfully), it did not out-perform the baseline uni-directional NNLM.
arxiv-15600-51 | Reinforcement Learning of POMDP's using Spectral Methods | http://arxiv.org/pdf/1602.07764v1.pdf | author:Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar category:cs.AI cs.LG cs.NA math.OC stat.ML published:2016-02-25 summary:We propose a new reinforcement learning algorithm for partially observableMarkov decision processes (POMDP) based on spectral decomposition methods.While spectral methods have been previously employed for consistent learning of(passive) latent variable models such as hidden Markov models, POMDPs are morechallenging since the learner interacts with the environment and possiblychanges the future observations in the process. We devise a learning algorithmrunning through episodes, in each episode we employ spectral techniques tolearn the POMDP parameters from a trajectory generated by a fixed policy. Atthe end of the episode, an optimization oracle returns the optimal memorylessplanning policy which maximizes the expected reward based on the estimatedPOMDP model. We prove an order-optimal regret bound w.r.t. the optimalmemoryless policy and efficient scaling with respect to the dimensionality ofobservation and action spaces.
arxiv-15600-52 | A Compressed Sensing Based Decomposition of Electro-Dermal Activity Signals | http://arxiv.org/pdf/1602.07754v1.pdf | author:Swayambhoo Jain, Urvashi Oswal, Kevin S. Xu, Brian Eriksson, Jarvis Haupt category:stat.ML published:2016-02-24 summary:The measurement and analysis of Electro-Dermal Activity (EDA) offersapplications in diverse areas ranging from market research, to seizuredetection, to human stress analysis. Unfortunately, the analysis of EDA signalsis made difficult by the superposition of numerous components which can obscurethe signal information related to a user's response to a stimulus. We show howsimple pre-processing followed by a novel compressed sensing baseddecomposition can mitigate the effects of these noise components and helpreveal the underlying physiological signal. The proposed framework allows forfast decomposition of EDA signals with provable bounds on the recovery of userresponses. We test our procedure on both synthetic and real-world EDA signalsfrom wearable sensors, and demonstrate that our approach allows for moreaccurate recovery of user responses as compared to the existing techniques.
arxiv-15600-53 | Toward Mention Detection Robustness with Recurrent Neural Networks | http://arxiv.org/pdf/1602.07749v1.pdf | author:Thien Huu Nguyen, Avirup Sil, Georgiana Dinu, Radu Florian category:cs.CL published:2016-02-24 summary:One of the key challenges in natural language processing (NLP) is to yieldgood performance across application domains and languages. In this work, weinvestigate the robustness of the mention detection systems, one of thefundamental tasks in information extraction, via recurrent neural networks(RNNs). The advantage of RNNs over the traditional approaches is their capacityto capture long ranges of context and implicitly adapt the word embeddings,trained on a large corpus, into a task-specific word representation, but stillpreserve the original semantic generalization to be helpful across domains. Oursystematic evaluation for RNN architectures demonstrates that RNNs not onlyoutperform the best reported systems (up to 9\% relative error reduction) inthe general setting but also achieve the state-of-the-art performance in thecross-domain setting for English. Regarding other languages, RNNs aresignificantly better than the traditional methods on the similar task of namedentity recognition for Dutch (up to 22\% relative error reduction).
arxiv-15600-54 | Adaptive Learning with Robust Generalization Guarantees | http://arxiv.org/pdf/1602.07726v1.pdf | author:Rachel Cummings, Katrina Ligett, Kobbi Nissim, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.LG published:2016-02-24 summary:The traditional notion of generalization --- i.e., learning a hypothesiswhose empirical error is close to its true error --- is surprisingly brittle.As has recently been noted [DFH+15b], even if several algorithms have thisguarantee in isolation, the guarantee need not hold if the algorithms arecomposed adaptively. In this paper, we study three notions of generalization---increasing in strength--- that are robust to post-processing and amenable toadaptive composition, and examine the relationships between them. We call the weakest such notion Robust Generalization. A second,intermediate, notion is the stability guarantee known as differential privacy.The strongest guarantee we consider we call Perfect Generalization. We provethat every hypothesis class that is PAC learnable is also PAC learnable in arobustly generalizing fashion, albeit with an exponential blowup in samplecomplexity. We conjecture that a stronger version of this theorem also holdsthat avoids any blowup in sample complexity (and, in fact, it would, subject toa longstanding conjecture [LW86, War03]). It was previously known thatdifferentially private algorithms satisfy robust generalization. In this paper,we show that robust generalization is a strictly weaker concept, and that thereis a learning task that can be carried out subject to robust generalizationguarantees, yet cannot be carried out subject to differential privacy,answering an open question of [DFH+15a]. We also show that perfectgeneralization is a strictly stronger guarantee than differential privacy, butthat, nevertheless, many learning tasks can be carried out subject to theguarantees of perfect generalization.
arxiv-15600-55 | Learning functions across many orders of magnitudes | http://arxiv.org/pdf/1602.07714v1.pdf | author:Hado van Hasselt, Arthur Guez, Matteo Hessel, David Silver category:cs.LG cs.AI cs.NE stat.ML published:2016-02-24 summary:Learning non-linear functions can be hard when the magnitude of the targetfunction is unknown beforehand, as most learning algorithms are not scaleinvariant. We propose an algorithm to adaptively normalize these targets. Thisis complementary to recent advances in input normalization. Importantly, theproposed method preserves the unnormalized outputs whenever the normalizationis updated to avoid instability caused by non-stationarity. It can be combinedwith any learning algorithm and any non-linear function approximation,including the important special case of deep learning. We empirically validatethe method in supervised learning and reinforcement learning and apply it tolearning how to play Atari 2600 games. Previous work on applying deep learningto this domain relied on clipping the rewards to make learning in differentgames more homogeneous, but this uses the domain-specific knowledge that inthese games counting rewards is often almost as informative as summing these.Using our adaptive normalization we can remove this heuristic withoutdiminishing overall performance, and even improve performance on some games,such as Ms. Pac-Man and Centipede, on which previous methods did not performwell.
arxiv-15600-56 | Recurrent Gaussian Processes | http://arxiv.org/pdf/1511.06644v6.pdf | author:César Lincoln C. Mattos, Zhenwen Dai, Andreas Damianou, Jeremy Forth, Guilherme A. Barreto, Neil D. Lawrence category:cs.LG stat.ML published:2015-11-20 summary:We define Recurrent Gaussian Processes (RGP) models, a general family ofBayesian nonparametric models with recurrent GP priors which are able to learndynamical patterns from sequential data. Similar to Recurrent Neural Networks(RNNs), RGPs can have different formulations for their internal states,distinct inference methods and be extended with deep structures. In suchcontext, we propose a novel deep RGP model whose autoregressive states arelatent, thereby performing representation and dynamical learningsimultaneously. To fully exploit the Bayesian nature of the RGP model wedevelop the Recurrent Variational Bayes (REVARB) framework, which enablesefficient inference and strong regularization through coherent propagation ofuncertainty across the RGP layers and states. We also introduce a RGP extensionwhere variational parameters are greatly reduced by being reparametrizedthrough RNN-based sequential recognition models. We apply our model to thetasks of nonlinear system identification and human motion modeling. Thepromising obtained results indicate that our RGP model maintains its highlyflexibility while being able to avoid overfitting and being applicable evenwhen larger datasets are not available.
arxiv-15600-57 | The Use of Machine Learning Algorithms in Recommender Systems: A Systematic Review | http://arxiv.org/pdf/1511.05263v4.pdf | author:Ivens Portugal, Paulo Alencar, Donald Cowan category:cs.SE cs.IR cs.LG published:2015-11-17 summary:Recommender systems use algorithms to provide users with product or servicerecommendations. Recently, these systems have been using machine learningalgorithms from the field of artificial intelligence. However, choosing asuitable machine learning algorithm for a recommender system is difficultbecause of the number of algorithms described in the literature. Researchersand practitioners developing recommender systems are left with littleinformation about the current approaches in algorithm usage. Moreover, thedevelopment of a recommender system using a machine learning algorithm oftenhas problems and open questions that must be evaluated, so software engineersknow where to focus research efforts. This paper presents a systematic reviewof the literature that analyzes the use of machine learning algorithms inrecommender systems and identifies research opportunities for softwareengineering research. The study concludes that Bayesian and decision treealgorithms are widely used in recommender systems because of their relativesimplicity, and that requirement and design phases of recommender systemdevelopment appear to offer opportunities for further research.
arxiv-15600-58 | Online Dual Coordinate Ascent Learning | http://arxiv.org/pdf/1602.07630v1.pdf | author:Bicheng Ying, Kun Yuan, Ali H. Sayed category:math.OC cs.LG stat.ML published:2016-02-24 summary:The stochastic dual coordinate-ascent (S-DCA) technique is a usefulalternative to the traditional stochastic gradient-descent algorithm forsolving large-scale optimization problems due to its scalability to large datasets and strong theoretical guarantees. However, the available S-DCAformulation is limited to finite sample sizes and relies on performing multiplepasses over the same data. This formulation is not well-suited for onlineimplementations where data keep streaming in. In this work, we develop an {\emonline} dual coordinate-ascent (O-DCA) algorithm that is able to respond tostreaming data and does not need to revisit the past data. This feature embedsthe resulting construction with continuous adaptation, learning, and trackingabilities, which are particularly attractive for online learning scenarios.
arxiv-15600-59 | Noisy population recovery in polynomial time | http://arxiv.org/pdf/1602.07616v1.pdf | author:Anindya De, Michael Saks, Sijian Tang category:cs.CC cs.DS cs.LG published:2016-02-24 summary:In the noisy population recovery problem of Dvir et al., the goal is to learnan unknown distribution $f$ on binary strings of length $n$ from noisy samples.For some parameter $\mu \in [0,1]$, a noisy sample is generated by flippingeach coordinate of a sample from $f$ independently with probability$(1-\mu)/2$. We assume an upper bound $k$ on the size of the support of thedistribution, and the goal is to estimate the probability of any string towithin some given error $\varepsilon$. It is known that the algorithmiccomplexity and sample complexity of this problem are polynomially related toeach other. We show that for $\mu > 0$, the sample complexity (and hence the algorithmiccomplexity) is bounded by a polynomial in $k$, $n$ and $1/\varepsilon$improving upon the previous best result of $\mathsf{poly}(k^{\log\logk},n,1/\varepsilon)$ due to Lovett and Zhang. Our proof combines ideas from Lovett and Zhang with a \emph{noise attenuated}version of M\"{o}bius inversion. In turn, the latter crucially uses theconstruction of \emph{robust local inverse} due to Moitra and Saks.
arxiv-15600-60 | Optimally Confident UCB: Improved Regret for Finite-Armed Bandits | http://arxiv.org/pdf/1507.07880v3.pdf | author:Tor Lattimore category:cs.LG math.OC published:2015-07-28 summary:I present the first algorithm for stochastic finite-armed bandits thatsimultaneously enjoys order-optimal problem-dependent regret and worst-caseregret. Besides the theoretical results, the new algorithm is simple, efficientand empirically superb. The approach is based on UCB, but with a carefullychosen confidence parameter that optimally balances the risk of failingconfidence intervals against the cost of excessive optimism.
arxiv-15600-61 | A data-driven method for syndrome type identification and classification in traditional Chinese medicine | http://arxiv.org/pdf/1410.7140v5.pdf | author:Nevin L. Zhang, Chen Fu, Teng Fei Liu, Bao Xin Chen, Kin Man Poon, Pei Xian Chen, Yun Ling Zhang category:cs.LG stat.AP published:2014-10-27 summary:Objective: The efficacy of traditional Chinese medicine (TCM) treatments forWestern medicine (WM) diseases relies heavily on the proper classification ofpatients into TCM syndrome types. We develop a data-driven method for solvingthe classification problem, where syndrome types are identified and quantifiedbased on patterns detected in unlabeled symptom survey data. Method: Latent class analysis (LCA) has been applied in WM research to solvea similar problem, i.e., to identify subtypes of a patient population in theabsence of a gold standard. A widely known weakness of LCA is that it makes anunrealistically strong independence assumption. We relax the assumption byfirst detecting symptom co-occurrence patterns from survey data and use thosepatterns instead of the symptoms as features for LCA. Results: The result ofthe investigation is a six-step method: Data collection, symptom co-occurrencepattern discovery, pattern interpretation, syndrome identification, syndrometype identification, and syndrome type classification. A software packagecalled Lantern is developed to support the application of the method. Themethod is illustrated using a data set on Vascular Mild Cognitive Impairment(VMCI). Conclusions: A data-driven method for TCM syndrome identification andclassification is presented. The method can be used to answer the followingquestions about a Western medicine disease: What TCM syndrome types are thereamong the patients with the disease? What is the prevalence of each syndrometype? What are the statistical characteristics of each syndrome type in termsof occurrence of symptoms? How can we determine the syndrome type(s) of apatient?
arxiv-15600-62 | Sparse Estimation in a Correlated Probit Model | http://arxiv.org/pdf/1507.04777v2.pdf | author:Stephan Mandt, Florian Wenzel, Shinichi Nakajima, John P. Cunningham, Christoph Lippert, Marius Kloft category:stat.ML cs.LG published:2015-07-16 summary:Among the goals of statistical genetics is to find associations betweengenetic data and binary phenotypes, such as heritable diseases. Often, the dataare obfuscated by confounders such as age, ethnicity, or population structure.Linear mixed models are linear regression models that correct for confoundingby means of correlated label noise; they are widely appreciated in the field ofstatistical genetics. We generalize this modeling paradigm to binaryclassification, where we face the problem that marginalizing over the noiseleads to an intractable, high-dimensional integral. We present a scalable,approximate inference algorithm that lets us fit the model to high-dimensionaldata sets. The algorithm selects features based on an $\ell_1$ norm regularizerwhich are up to 40% less confounded compared to the outcomes of uncorrectedfeature selection, as we show. The proposed method also outperforms Gaussianprocess classification and uncorrelated probit regression in terms ofprediction performance.
arxiv-15600-63 | Bayesian Exploration: Incentivizing Exploration in Bayesian Games | http://arxiv.org/pdf/1602.07570v1.pdf | author:Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, Zhiwei Steven Wu category:cs.GT cs.DS cs.LG published:2016-02-24 summary:We consider a ubiquitous scenario in the Internet economy when individualdecision-makers (henceforth, agents) both produce and consume information asthey make strategic choices in an uncertain environment. This creates athree-way tradeoff between exploration (trying out insufficiently exploredalternatives to help others in the future), exploitation (making optimaldecisions given the information discovered by other agents), and incentives ofthe agents (who are myopically interested in exploitation, while preferring theothers to explore). We posit a principal who controls the flow of informationfrom agents that came before, and strives to coordinate the agents towards asocially optimal balance between exploration and exploitation, not using anymonetary transfers. The goal is to design a recommendation policy for theprincipal which respects agents' incentives and minimizes a suitable notion ofregret. We extend prior work in this direction to allow the agents to interact withone another in a shared environment: at each time step, multiple agents arriveto play a Bayesian game, receive recommendations, choose their actions, receivetheir payoffs, and then leave the game forever. The agents now face two sourcesof uncertainty: the actions of the other agents and the parameters of theuncertain game environment. Our main contribution is to show that the principal can achieve constantregret when the utilities are deterministic (where the constant depends on theprior distribution, but not on the time horizon), and logarithmic regret whenthe utilities are stochastic. As a key technical tool, we introduce the conceptof explorable actions, the actions which some incentive-compatible policy canrecommend with non-zero probability. We show how the principal can identify(and explore) all explorable actions, and use the revealed information toperform optimally.
arxiv-15600-64 | Particular object retrieval with integral max-pooling of CNN activations | http://arxiv.org/pdf/1511.05879v2.pdf | author:Giorgos Tolias, Ronan Sicre, Hervé Jégou category:cs.CV published:2015-11-18 summary:Recently, image representation built upon Convolutional Neural Network (CNN)has been shown to provide effective descriptors for image search, outperformingpre-CNN features as short-vector representations. Yet such models are notcompatible with geometry-aware re-ranking methods and still outperformed, onsome particular object retrieval benchmarks, by traditional image searchsystems relying on precise descriptor matching, geometric re-ranking, or queryexpansion. This work revisits both retrieval stages, namely initial search andre-ranking, by employing the same primitive information derived from the CNN.We build compact feature vectors that encode several image regions without theneed to feed multiple inputs to the network. Furthermore, we extend integralimages to handle max-pooling on convolutional layer activations, allowing us toefficiently localize matching objects. The resulting bounding box is finallyused for image re-ranking. As a result, this paper significantly improvesexisting CNN-based recognition pipeline: We report for the first time resultscompeting with traditional methods on the challenging Oxford5k and Paris6kdatasets.
arxiv-15600-65 | On the Accuracy of Point Localisation in a Circular Camera-Array | http://arxiv.org/pdf/1602.07542v1.pdf | author:Alireza Ghasemi, Adam Scholefield, Martin Vetterli category:cs.CV published:2016-02-24 summary:Although many advances have been made in light-field and camera-array imageprocessing, there is still a lack of thorough analysis of the localisationaccuracy of different multi-camera systems. By considering the problem from aframe-quantisation perspective, we are able to quantify the point localisationerror of circular camera configurations. Specifically, we obtain closed formexpressions bounding the localisation error in terms of the parametersdescribing the acquisition setup. These theoretical results are independent of the localisation algorithm andthus provide fundamental limits on performance. Furthermore, the newframe-quantisation perspective is general enough to be extended to more complexcamera configurations.
arxiv-15600-66 | SHAPE: Linear-Time Camera Pose Estimation With Quadratic Error-Decay | http://arxiv.org/pdf/1602.07535v1.pdf | author:Alireza Ghasemi, Adam Scholefield, Martin Vetterli category:cs.CV published:2016-02-24 summary:We propose a novel camera pose estimation or perspective-n-point (PnP)algorithm, based on the idea of consistency regions and half-spaceintersections. Our algorithm has linear time-complexity and a squaredreconstruction error that decreases at least quadratically, as the number offeature point correspondences increase. Inspired by ideas from triangulation and frame quantisation theory, we defineconsistent reconstruction and then present SHAPE, our proposed consistent poseestimation algorithm. We compare this algorithm with state-of-the-art poseestimation techniques in terms of accuracy and error decay rate. Theexperimental results verify our hypothesis on the optimal worst-case quadraticdecay and demonstrate its promising performance compared to other approaches.
arxiv-15600-67 | A Bayesian Approach to the Data Description Problem | http://arxiv.org/pdf/1602.07507v1.pdf | author:Alireza Ghasemi, Hamid R. Rabiee, Mohammad T. Manzuri, M. H. Rohban category:cs.LG published:2016-02-24 summary:In this paper, we address the problem of data description using a Bayesianframework. The goal of data description is to draw a boundary around objects ofa certain class of interest to discriminate that class from the rest of thefeature space. Data description is also known as one-class learning and has awide range of applications. The proposed approach uses a Bayesian framework to precisely compute theclass boundary and therefore can utilize domain information in form of priorknowledge in the framework. It can also operate in the kernel space andtherefore recognize arbitrary boundary shapes. Moreover, the proposed methodcan utilize unlabeled data in order to improve accuracy of discrimination. We evaluate our method using various real-world datasets and compare it withother state of the art approaches of data description. Experiments showpromising results and improved performance over other data description andone-class learning algorithms.
arxiv-15600-68 | Active Learning from Positive and Unlabeled Data | http://arxiv.org/pdf/1602.07495v1.pdf | author:Alireza Ghasemi, Hamid R. Rabiee, Mohsen Fadaee, Mohammad T. Manzuri, Mohammad H. Rohban category:cs.LG published:2016-02-24 summary:During recent years, active learning has evolved into a popular paradigm forutilizing user's feedback to improve accuracy of learning algorithms. Activelearning works by selecting the most informative sample among unlabeled dataand querying the label of that point from user. Many different methods such asuncertainty sampling and minimum risk sampling have been utilized to select themost informative sample in active learning. Although many active learningalgorithms have been proposed so far, most of them work with binary ormulti-class classification problems and therefore can not be applied toproblems in which only samples from one class as well as a set of unlabeleddata are available. Such problems arise in many real-world situations and are known as theproblem of learning from positive and unlabeled data. In this paper we proposean active learning algorithm that can work when only samples of one class aswell as a set of unlabelled data are available. Our method works by separatelyestimating probability desnity of positive and unlabeled points and thencomputing expected value of informativeness to get rid of a hyper-parameter andhave a better measure of informativeness./ Experiments and empirical analysisshow promising results compared to other similar methods.
arxiv-15600-69 | Boosting patch-based scene text script identification with ensembles of conjoined networks | http://arxiv.org/pdf/1602.07480v1.pdf | author:Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas category:cs.CV published:2016-02-24 summary:This paper focuses on the problem of script identification in scene textimages. Facing this problem with state of the art CNN classifiers is notstraightforward, as they fail to address a key characteristic of scene textinstances: their extremely variable aspect ratio. Instead of resizing inputimages to a fixed aspect ratio as in the typical use of holistic CNNclassifiers, we propose here a patch-based classification framework in order topreserve discriminative parts of the image that are characteristic of itsclass. We describe a novel method based on the use of ensembles of conjoinednetworks to jointly learn discriminative stroke-parts representations and theirrelative importance in a patch-based classification scheme. Our experimentswith this learning procedure demonstrate state-of-the-art results in two publicscript identification datasets. In addition, we propose a new public benchmark dataset for the evaluation ofmulti-lingual scene text end-to-end reading systems. Experiments done in thisdataset demonstrate the key role of script identification in a completeend-to-end system that combines our script identification method with apreviously published text detector and an off-the-shelf OCR engine.
arxiv-15600-70 | A fine-grained approach to scene text script identification | http://arxiv.org/pdf/1602.07475v1.pdf | author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV published:2016-02-24 summary:This paper focuses on the problem of script identification in unconstrainedscenarios. Script identification is an important prerequisite to recognition,and an indispensable condition for automatic text understanding systemsdesigned for multi-language environments. Although widely studied for documentimages and handwritten documents, it remains an almost unexplored territory forscene text images. We detail a novel method for script identification in natural images thatcombines convolutional features and the Naive-Bayes Nearest Neighborclassifier. The proposed framework efficiently exploits the discriminativepower of small stroke-parts, in a fine-grained classification framework. In addition, we propose a new public benchmark dataset for the evaluation ofjoint text detection and script identification in natural scenes. Experimentsdone in this new dataset demonstrate that the proposed method yields state ofthe art results, while it generalizes well to different datasets and variablenumber of scripts. The evidence provided shows that multi-lingual scene textrecognition in the wild is a viable proposition. Source code of the proposedmethod is made available online.
arxiv-15600-71 | Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications | http://arxiv.org/pdf/1511.06530v2.pdf | author:Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, Dongjun Shin category:cs.CV cs.LG published:2015-11-20 summary:Although the latest high-end smartphone has powerful CPU and GPU, runningdeeper convolutional neural networks (CNNs) for complex tasks such as ImageNetclassification on mobile devices is challenging. To deploy deep CNNs on mobiledevices, we present a simple and effective scheme to compress the entire CNN,which we call one-shot whole network compression. The proposed scheme consistsof three steps: (1) rank selection with variational Bayesian matrixfactorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuningto recover accumulated loss of accuracy, and each step can be easilyimplemented using publicly available tools. We demonstrate the effectiveness ofthe proposed scheme by testing the performance of various compressed CNNs(AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significantreductions in model size, runtime, and energy consumption are obtained, at thecost of small loss in accuracy. In addition, we address the importantimplementation level issue on 1?1 convolution, which is a key operation ofinception module of GoogLeNet as well as CNNs compressed by our proposedscheme.
arxiv-15600-72 | Asymptotic consistency and order specification for logistic classifier chains in multi-label learning | http://arxiv.org/pdf/1602.07466v1.pdf | author:Paweł Teisseyre category:cs.LG stat.ML published:2016-02-24 summary:Classifier chains are popular and effective method to tackle a multi-labelclassification problem. The aim of this paper is to study the asymptoticproperties of the chain model in which the conditional probabilities are of thelogistic form. In particular we find conditions on the number of labels and thedistribution of feature vector under which the estimated mode of the jointdistribution of labels converges to the true mode. Best of our knowledge, thisimportant issue has not yet been studied in the context of multi-labellearning. We also investigate how the order of model building in a chaininfluences the estimation of the joint distribution of labels. We establish thelink between the problem of incorrect ordering in the chain and incorrect modelspecification. We propose a procedure of determining the optimal ordering oflabels in the chain, which is based on using measures of correct specificationand allows to find the ordering such that the consecutive logistic models arebest possibly specified. The other important question raised in this paper ishow accurately can we estimate the joint posterior probability when theordering of labels is wrong or the logistic models in the chain are incorrectlyspecified. The numerical experiments illustrate the theoretical results.
arxiv-15600-73 | Towards a Biologically Plausible Backprop | http://arxiv.org/pdf/1602.05179v2.pdf | author:Benjamin Scellier, Yoshua Bengio category:cs.LG published:2016-02-16 summary:This work follows Bengio and Fischer (2015) in which theoretical foundationswere laid to show how iterative inference can backpropagate error signals.Neurons move their activations towards configurations corresponding to lowerenergy and smaller prediction error: a new observation creates a perturbationat visible neurons that propagates into hidden layers, with these propagatedperturbations corresponding to the back-propagated gradient. This avoids theneed for a lengthy relaxation in the positive phase of training (when bothinputs and targets are observed), as was believed with previous work onfixed-point recurrent networks. We show experimentally that energy-based neuralnetworks with several hidden layers can be trained at discriminative tasks byusing iterative inference and an STDP-like learning rule. The main result ofthis paper is that we can train neural networks with 1, 2 and 3 hidden layerson the permutation-invariant MNIST task and get the training error down to0.00%. The results presented here make it more biologically plausible that amechanism similar to back-propagation may take place in brains in order toachieve credit assignment in deep networks. The paper also discusses some ofthe remaining open problems to achieve a biologically plausible implementationof backprop in brains.
arxiv-15600-74 | Feature ranking for multi-label classification using Markov Networks | http://arxiv.org/pdf/1602.07464v1.pdf | author:Paweł Teisseyre category:cs.LG stat.ML published:2016-02-24 summary:We propose a simple and efficient method for ranking features in multi-labelclassification. The method produces a ranking of features showing theirrelevance in predicting labels, which in turn allows to choose a final subsetof features. The procedure is based on Markov Networks and allows to model thedependencies between labels and features in a direct way. In the first step webuild a simple network using only labels and then we test how much adding asingle feature affects the initial network. More specifically, in the firststep we use the Ising model whereas the second step is based on the scorestatistic, which allows to test a significance of added features very quickly.The proposed approach does not require transformation of label space, givesinterpretable results and allows for attractive visualization of dependencystructure. We give a theoretical justification of the procedure by discussingsome theoretical properties of the Ising model and the score statistic. We alsodiscuss feature ranking procedure based on fitting Ising model using $l_1$regularized logistic regressions. Numerical experiments show that the proposedmethods outperform the conventional approaches on the considered artificial andreal datasets.
arxiv-15600-75 | Hand-Object Interaction and Precise Localization in Transitive Action Recognition | http://arxiv.org/pdf/1511.03814v2.pdf | author:Amir Rosenfeld, Shimon Ullman category:cs.CV published:2015-11-12 summary:Action recognition in still images has seen major improvement in recent yearsdue to advances in human pose estimation, object recognition and strongerfeature representations produced by deep neural networks. However, there arestill many cases in which performance remains far from that of humans. A majordifficulty arises in distinguishing between transitive actions in which theoverall actor pose is similar, and recognition therefore depends on details ofthe grasp and the object, which may be largely occluded. In this paper wedemonstrate how recognition is improved by obtaining precise localization ofthe action-object and consequently extracting details of the object shapetogether with the actor-object interaction. To obtain exact localization of theaction object and its interaction with the actor, we employ a coarse-to-fineapproach which combines semantic segmentation and contextual features, insuccessive stages. We focus on (but are not limited) to face-related actions, aset of actions that includes several currently challenging categories. Wepresent an average relative improvement of 35% over state-of-the art andvalidate through experimentation the effectiveness of our approach.
arxiv-15600-76 | Max-Margin Nonparametric Latent Feature Models for Link Prediction | http://arxiv.org/pdf/1602.07428v1.pdf | author:Jun Zhu, Jiaming Song, Bei Chen category:cs.LG cs.SI stat.ME stat.ML published:2016-02-24 summary:Link prediction is a fundamental task in statistical network analysis. Recentadvances have been made on learning flexible nonparametric Bayesian latentfeature models for link prediction. In this paper, we present a max-marginlearning method for such nonparametric latent feature relational models. Ourapproach attempts to unite the ideas of max-margin learning and Bayesiannonparametrics to discover discriminative latent features for link prediction.It inherits the advances of nonparametric Bayesian methods to infer the unknownlatent social dimension, while for discriminative link prediction, it adoptsthe max-margin learning principle by minimizing a hinge-loss using the linearexpectation operator, without dealing with a highly nonlinear link likelihoodfunction. For posterior inference, we develop an efficient stochasticvariational inference algorithm under a truncated mean-field assumption. Ourmethods can scale up to large-scale real networks with millions of entities andtens of millions of positive links. We also provide a full Bayesianformulation, which can avoid tuning regularization hyper-parameters.Experimental results on a diverse range of real datasets demonstrate thebenefits inherited from max-margin learning and Bayesian nonparametricinference.
arxiv-15600-77 | Sifting Robotic from Organic Text: A Natural Language Approach for Detecting Automation on Twitter | http://arxiv.org/pdf/1505.04342v5.pdf | author:Eric M. Clark, Jake Ryland Williams, Chris A. Jones, Richard A. Galbraith, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL published:2015-05-17 summary:Twitter, a popular social media outlet, has evolved into a vast source oflinguistic data, rich with opinion, sentiment, and discussion. Due to theincreasing popularity of Twitter, its perceived potential for exerting socialinfluence has led to the rise of a diverse community of automatons, commonlyreferred to as bots. These inorganic and semi-organic Twitter entities canrange from the benevolent (e.g., weather-update bots, help-wanted-alert bots)to the malevolent (e.g., spamming messages, advertisements, or radicalopinions). Existing detection algorithms typically leverage meta-data (timebetween tweets, number of followers, etc.) to identify robotic accounts. Here,we present a powerful classification scheme that exclusively uses the naturallanguage text from organic users to provide a criterion for identifyingaccounts posting automated messages. Since the classifier operates on textalone, it is flexible and may be applied to any textual data beyond theTwitter-sphere.
arxiv-15600-78 | Learning to Generate with Memory | http://arxiv.org/pdf/1602.07416v1.pdf | author:Chongxuan Li, Jun Zhu, Bo Zhang category:cs.LG cs.CV published:2016-02-24 summary:Memory units have been widely used to enrich the capabilities of deepnetworks on capturing long-term dependencies in reasoning and prediction tasks,but little investigation exists on deep generative models (DGMs) which are goodat inferring high-level invariant representations from unlabeled data. Thispaper presents a deep generative model with a possibly large external memoryand an attention mechanism to capture the local detail information that isoften lost in the bottom-up abstraction process in representation learning. Byadopting a smooth attention model, the whole network is trained end-to-end byoptimizing a variational bound of data likelihood via auto-encoding variationalBayesian methods, where an asymmetric recognition network is learnt jointly toinfer high-level invariant representations. The asymmetric architecture canreduce the competition between bottom-up invariant feature extraction andtop-down generation of instance details. Our experiments on several datasetsdemonstrate that memory can significantly boost the performance of DGMs andeven achieve state-of-the-art results on various tasks, including densityestimation, image generation, and missing value imputation.
arxiv-15600-79 | Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling | http://arxiv.org/pdf/1602.07415v1.pdf | author:Christopher De Sa, Kunle Olukotun, Christopher Ré category:cs.LG published:2016-02-24 summary:Gibbs sampling is a Markov Chain Monte Carlo technique commonly used forestimating marginal distributions. To speed up Gibbs sampling, there hasrecently been interest in parallelizing it by executing asynchronously. Whileempirical results suggest that many models can be efficiently sampledasynchronously, traditional Markov chain analysis does not apply to theasynchronous case, and thus asynchronous Gibbs sampling is poorly understood.In this paper, we derive a better understanding of the two main challenges ofasynchronous Gibbs: sampling bias and mixing time. We show experimentally thatour theoretical results match practical outcomes.
arxiv-15600-80 | Harder, Better, Faster, Stronger Convergence Rates for Least-Squares Regression | http://arxiv.org/pdf/1602.05419v2.pdf | author:Aymeric Dieuleveut, Nicolas Flammarion, Francis Bach category:math.OC cs.LG stat.ML published:2016-02-17 summary:We consider the optimization of a quadratic objective function whosegradients are only accessible through a stochastic oracle that returns thegradient at any given point plus a zero-mean finite variance random error. Wepresent the first algorithm that achieves jointly the optimal prediction errorrates for least-squares regression, both in terms of forgetting of initialconditions in O(1/n 2), and in terms of dependence on the noise and dimension dof the problem, as O(d/n). Our new algorithm is based on averaged acceleratedregularized gradient descent, and may also be analyzed through finerassumptions on initial conditions and the Hessian matrix, leading todimension-free quantities that may still be small while the " optimal " termsabove are large. In order to characterize the tightness of these new bounds, weconsider an application to non-parametric regression and use the known lowerbounds on the statistical performance (without computational limits), whichhappen to match our bounds obtained from a single pass on the data and thusshow optimality of our algorithm in a wide variety of particular trade-offsbetween bias and variance.
arxiv-15600-81 | Improved Accent Classification Combining Phonetic Vowels with Acoustic Features | http://arxiv.org/pdf/1602.07394v1.pdf | author:Zhenhao Ge category:cs.SD cs.CL published:2016-02-24 summary:Researches have shown accent classification can be improved by integratingsemantic information into pure acoustic approach. In this work, we combinephonetic knowledge, such as vowels, with enhanced acoustic features to build animproved accent classification system. The classifier is based on GaussianMixture Model-Universal Background Model (GMM-UBM), with normalized PerceptualLinear Predictive (PLP) features. The features are further optimized byPrinciple Component Analysis (PCA) and Hetroscedastic Linear DiscriminantAnalysis (HLDA). Using 7 major types of accented speech from the ForeignAccented English (FAE) corpus, the system achieves classification accuracy 54%with input test data as short as 20 seconds, which is competitive to the stateof the art in this field.
arxiv-15600-82 | Domain Specific Author Attribution Based on Feedforward Neural Network Language Models | http://arxiv.org/pdf/1602.07393v1.pdf | author:Zhenhao Ge, Yufang Sun category:cs.CL cs.LG cs.NE published:2016-02-24 summary:Authorship attribution refers to the task of automatically determining theauthor based on a given sample of text. It is a problem with a long history andhas a wide range of application. Building author profiles using language modelsis one of the most successful methods to automate this task. New languagemodeling methods based on neural networks alleviate the curse of dimensionalityand usually outperform conventional N-gram methods. However, there have notbeen much research applying them to authorship attribution. In this paper, wepresent a novel setup of a Neural Network Language Model (NNLM) and apply it toa database of text samples from different authors. We investigate how the NNLMperforms on a task with moderate author set size and relatively limitedtraining and test data, and how the topics of the text samples affect theaccuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement offitness of a trained language model to the test data. Given 5 random testsentences, it also increases the author classification accuracy by 3.43% onaverage, compared with the N-gram methods using SRILM tools. An open sourceimplementation of our methodology is freely available athttps://github.com/zge/authorship-attribution/.
arxiv-15600-83 | The Myopia of Crowds: A Study of Collective Evaluation on Stack Exchange | http://arxiv.org/pdf/1602.07388v1.pdf | author:Keith Burghardt, Emanuel F. Alsina, Michelle Girvan, William Rand, Kristina Lerman category:cs.HC cs.CY cs.LG physics.soc-ph published:2016-02-24 summary:Crowds can often make better decisions than individuals or small groups ofexperts by leveraging their ability to aggregate diverse information. Questionanswering sites, such as Stack Exchange, rely on the "wisdom of crowds" effectto identify the best answers to questions asked by users. We analyze data from250 communities on the Stack Exchange network to pinpoint factors affectingwhich answers are chosen as the best answers. Our results suggest that, ratherthan evaluate all available answers to a question, users rely on simplecognitive heuristics to choose an answer to vote for or accept. These cognitiveheuristics are linked to an answer's salience, such as the order in which it islisted and how much screen space it occupies. While askers appear to dependmore on heuristics, compared to voting users, when choosing an answer to acceptas the most helpful one, voters use acceptance itself as a heuristic: they aremore likely to choose the answer after it is accepted than before that verysame answer was accepted. These heuristics become more important in explainingand predicting behavior as the number of available answers increases. Ourfindings suggest that crowd judgments may become less reliable as the number ofanswers grow.
arxiv-15600-84 | Automatic Moth Detection from Trap Images for Pest Management | http://arxiv.org/pdf/1602.07383v1.pdf | author:Weiguang Ding, Graham Taylor category:cs.CV cs.LG cs.NE published:2016-02-24 summary:Monitoring the number of insect pests is a crucial component inpheromone-based pest management systems. In this paper, we propose an automaticdetection pipeline based on deep learning for identifying and counting pests inimages taken inside field traps. Applied to a commercial codling moth dataset,our method shows promising performance both qualitatively and quantitatively.Compared to previous attempts at pest detection, our approach uses nopest-specific engineering which enables it to adapt to other species andenvironments with minimal human effort. It is amenable to implementation onparallel hardware and therefore capable of deployment in settings wherereal-time performance is required.
arxiv-15600-85 | Accent Classification with Phonetic Vowel Representation | http://arxiv.org/pdf/1604.08095v1.pdf | author:Zhenhao Ge, Yingyi Tan, Aravind Ganapathiraju category:cs.SD cs.CL published:2016-02-24 summary:Previous accent classification research focused mainly on detecting accentswith pure acoustic information without recognizing accented speech. This workcombines phonetic knowledge such as vowels with acoustic information to buildGuassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP)features, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). Withinput about 20-second accented speech, this system achieves classification rateof 51% on a 7-way classification system focusing on the major types of accentsin English, which is competitive to the state-of-the-art results in this field.
arxiv-15600-86 | On Study of the Binarized Deep Neural Network for Image Classification | http://arxiv.org/pdf/1602.07373v1.pdf | author:Song Wang, Dongchun Ren, Li Chen, Wei Fan, Jun Sun, Satoshi Naoi category:cs.NE cs.CV cs.LG published:2016-02-24 summary:Recently, the deep neural network (derived from the artificial neuralnetwork) has attracted many researchers' attention by its outstandingperformance. However, since this network requires high-performance GPUs andlarge storage, it is very hard to use it on individual devices. In order toimprove the deep neural network, many trials have been made by refining thenetwork structure or training strategy. Unlike those trials, in this paper, wefocused on the basic propagation function of the artificial neural network andproposed the binarized deep neural network. This network is a pure binarysystem, in which all the values and calculations are binarized. As a result,our network can save a lot of computational resource and storage. Therefore, itis possible to use it on various devices. Moreover, the experimental resultsproved the feasibility of the proposed network.
arxiv-15600-87 | RAND-WALK: A Latent Variable Model Approach to Word Embeddings | http://arxiv.org/pdf/1502.03520v6.pdf | author:Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, Andrej Risteski category:cs.LG cs.CL stat.ML published:2015-02-12 summary:Semantic word embeddings represent the meaning of a word via a vector, andare created by diverse methods such as Latent Semantic Analysis (LSA),generative text models such as topic models, matrix factorization, neural nets,and energy-based models. Many methods use nonlinear operations ---such asPairwise Mutual Information or PMI--- on co-occurrence statistics, and havehand-tuned hyperparameters and reweightings. Often a {\em generative model} can help provide theoretical insight into suchmodeling choices, but there appears to be no such model to "explain" the abovenonlinear models. For example, we know of no generative model for which thecorrect solution is the usual (dimension-restricted) PMI model. This paper gives a new generative model, a dynamic version of the loglineartopic model of \citet{mnih2007three}. The methodological novelty is to use theprior to compute {\em closed form} expressions for word statistics. Theseprovide an explanation for nonlinear models like PMI, {\bf word2vec}, andGloVe, as well as some hyperparameter choices. Experimental support is provided for the generative model assumptions, themost important of which is that latent word vectors are fairly uniformlydispersed ("isotropic") in space. The model also helps explain why low-dimensional semantic embeddings containlinear algebraic structure that allows solution of word analogies, as shownby~\citet{mikolov2013efficient} and many subsequent papers.
arxiv-15600-88 | adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs | http://arxiv.org/pdf/1511.01169v5.pdf | author:Nitish Shirish Keskar, Albert S. Berahas category:cs.LG math.OC stat.ML published:2015-11-04 summary:Recurrent Neural Networks (RNNs) are powerful models that achieve exceptionalperformance on several pattern recognition problems. However, the training ofRNNs is a computationally difficult task owing to the well-known"vanishing/exploding" gradient problem. Algorithms proposed for training RNNseither exploit no (or limited) curvature information and have cheapper-iteration complexity, or attempt to gain significant curvature informationat the cost of increased per-iteration cost. The former set includesdiagonally-scaled first-order methods such as ADAGRAD and ADAM, while thelatter consists of second-order algorithms like Hessian-Free Newton and K-FAC.In this paper, we present adaQN, a stochastic quasi-Newton algorithm fortraining RNNs. Our approach retains a low per-iteration cost while allowing fornon-diagonal scaling through a stochastic L-BFGS updating scheme. The methoduses a novel L-BFGS scaling initialization scheme and is judicious in storingand retaining L-BFGS curvature pairs. We present numerical experiments on twolanguage modeling tasks and show that adaQN is competitive with popular RNNtraining algorithms.
arxiv-15600-89 | Parsimonious modeling with Information Filtering Networks | http://arxiv.org/pdf/1602.07349v1.pdf | author:Wolfram Barfuss, Guido Previde Massara, T. Di Matteo, Tomaso Aste category:cs.IT math.IT stat.ML published:2016-02-23 summary:We introduce a methodology to construct sparse models from data by usinginformation filtering networks as inference structure. This method iscomputationally very efficient and statistically robust because it is based{on} local, low-dimensional, inversions of the covariance matrix to generate aglobal sparse inverse. Compared with state-of-the-art methodologies such aslasso, our method is computationally more efficient producing in a fraction ofcomputation time models that have equivalent or better performances but with asparser and more meaningful inference structure. The local nature of thisapproach allow{s} dynamical partial updating when the properties of somevariables change without the need of recomputing the whole model. We discussperformances with financial data and financial applications to prediction,stress testing and risk allocation.
arxiv-15600-90 | Order Matters: Sequence to sequence for sets | http://arxiv.org/pdf/1511.06391v4.pdf | author:Oriol Vinyals, Samy Bengio, Manjunath Kudlur category:stat.ML cs.CL cs.LG published:2015-11-19 summary:Sequences have become first class citizens in supervised learning thanks tothe resurgence of recurrent neural networks. Many complex tasks that requiremapping from or to a sequence of observations can now be formulated with thesequence-to-sequence (seq2seq) framework which employs the chain rule toefficiently represent the joint probability of sequences. In many cases,however, variable sized inputs and/or outputs might not be naturally expressedas sequences. For instance, it is not clear how to input a set of numbers intoa model where the task is to sort them; similarly, we do not know how toorganize outputs when they correspond to random variables and the task is tomodel their unknown joint probability. In this paper, we first show usingvarious examples that the order in which we organize input and/or output datamatters significantly when learning an underlying model. We then discuss anextension of the seq2seq framework that goes beyond sequences and handles inputsets in a principled way. In addition, we propose a loss which, by searchingover possible orders during training, deals with the lack of structure ofoutput sets. We show empirical evidence of our claims regarding ordering, andon the modifications to the seq2seq framework on benchmark language modelingand parsing tasks, as well as two artificial tasks -- sorting numbers andestimating the joint probability of unknown graphical models.
arxiv-15600-91 | Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations | http://arxiv.org/pdf/1602.07332v1.pdf | author:Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, Fei-Fei Li category:cs.CV cs.AI published:2016-02-23 summary:Despite progress in perceptual tasks such as image classification, computersstill perform poorly on cognitive tasks such as image description and questionanswering. Cognition is core to tasks that involve not just recognizing, butreasoning about our visual world. However, models used to tackle the richcontent in images for cognitive tasks are still being trained using the samedatasets designed for perceptual tasks. To achieve success at cognitive tasks,models need to understand the interactions and relationships between objects inan image. When asked "What vehicle is the person riding?", computers will needto identify the objects in an image as well as the relationships riding(man,carriage) and pulling(horse, carriage) in order to answer correctly that "theperson is riding a horse-drawn carriage". In this paper, we present the Visual Genome dataset to enable the modeling ofsuch relationships. We collect dense annotations of objects, attributes, andrelationships within each image to learn these models. Specifically, ourdataset contains over 100K images where each image has an average of 21objects, 18 attributes, and 18 pairwise relationships between objects. Wecanonicalize the objects, attributes, relationships, and noun phrases in regiondescriptions and questions answer pairs to WordNet synsets. Together, theseannotations represent the densest and largest dataset of image descriptions,objects, attributes, relationships, and question answers.
arxiv-15600-92 | Stuck in a What? Adventures in Weight Space | http://arxiv.org/pdf/1602.07320v1.pdf | author:Zachary C. Lipton category:cs.LG published:2016-02-23 summary:Deep learning researchers commonly suggest that converged models are stuck inlocal minima. More recently, some researchers observed that under reasonableassumptions, the vast majority of critical points are saddle points, not trueminima. Both descriptions suggest that weights converge around a point inweight space, be it a local optima or merely a critical point. However, it'spossible that neither interpretation is accurate. As neural networks aretypically over-complete, it's easy to show the existence of vast continuousregions through weight space with equal loss. In this paper, we build on recentwork empirically characterizing the error surfaces of neural networks. Weanalyze training paths through weight space, presenting evidence that apparentconvergence of loss does not correspond to weights arriving at critical points,but instead to large movements through flat regions of weight space. While it'strivial to show that neural network error surfaces are globally non-convex, weshow that error surfaces are also locally non-convex, even after breakingsymmetry with a random initialization and also after partial training.
arxiv-15600-93 | Unsupervised Ensemble Learning with Dependent Classifiers | http://arxiv.org/pdf/1510.05830v2.pdf | author:Ariel Jaffe, Ethan Fetaya, Boaz Nadler, Tingting Jiang, Yuval Kluger category:cs.LG stat.ML published:2015-10-20 summary:In unsupervised ensemble learning, one obtains predictions from multiplesources or classifiers, yet without knowing the reliability and expertise ofeach source, and with no labeled data to assess it. The task is to combinethese possibly conflicting predictions into an accurate meta-learner. Mostworks to date assumed perfect diversity between the different sources, aproperty known as conditional independence. In realistic scenarios, however,this assumption is often violated, and ensemble learners based on it can beseverely sub-optimal. The key challenges we address in this paper are:\ (i) howto detect, in an unsupervised manner, strong violations of conditionalindependence; and (ii) construct a suitable meta-learner. To this end weintroduce a statistical model that allows for dependencies between classifiers.Our main contributions are the development of novel unsupervised methods todetect strongly dependent classifiers, better estimate their accuracies, andconstruct an improved meta-learner. Using both artificial and real datasets, weshowcase the importance of taking classifier dependencies into account and thecompetitive performance of our approach.
arxiv-15600-94 | The IBM 2016 Speaker Recognition System | http://arxiv.org/pdf/1602.07291v1.pdf | author:Seyed Omid Sadjadi, Sriram Ganapathy, Jason W. Pelecanos category:cs.SD cs.CL stat.ML published:2016-02-23 summary:In this paper we describe the recent advancements made in the IBM i-vectorspeaker recognition system for conversational speech. In particular, weidentify key techniques that contribute to significant improvements inperformance of our system, and quantify their contributions. The techniquesinclude: 1) a nearest-neighbor discriminant analysis (NDA) approach that isformulated to alleviate some of the limitations associated with theconventional linear discriminant analysis (LDA) that assumes Gaussianclass-conditional distributions, 2) the application of speaker- andchannel-adapted features, which are derived from an automatic speechrecognition (ASR) system, for speaker recognition, and 3) the use of a deepneural network (DNN) acoustic model with a large number of output units (~10ksenones) to compute the frame-level soft alignments required in the i-vectorestimation process. We evaluate these techniques on the NIST 2010 speakerrecognition evaluation (SRE) extended core conditions involving telephone andmicrophone trials. Experimental results indicate that: 1) the NDA is moreeffective (up to 35% relative improvement in terms of EER) than the traditionalparametric LDA for speaker recognition, 2) when compared to raw acousticfeatures (e.g., MFCCs), the ASR speaker-adapted features provide gains inspeaker recognition performance, and 3) increasing the number of output unitsin the DNN acoustic model (i.e., increasing the senone set size from 2k to 10k)provides consistent improvements in performance (for example from 37% to 57%relative EER gains over our baseline GMM i-vector system). To our knowledge,results reported in this paper represent the best performances published todate on the NIST SRE 2010 extended core tasks.
arxiv-15600-95 | A Simple Approach to Sparse Clustering | http://arxiv.org/pdf/1602.07277v1.pdf | author:Ery Arias-Castro, Xiao Pu category:stat.ML published:2016-02-23 summary:We consider the problem of sparse clustering, where it is assumed that only asubset of the features are useful for clustering purposes. In the framework ofthe COSA method of Friedman and Meulman (2004), subsequently improved in theform of the Sparse K-means method of Witten and Tibshirani (2010), we propose avery natural and simpler hill-climbing approach that is competitive with thesetwo methods.
arxiv-15600-96 | Submodular Functions: from Discrete to Continous Domains | http://arxiv.org/pdf/1511.00394v2.pdf | author:Francis Bach category:cs.LG math.OC published:2015-11-02 summary:Submodular set-functions have many applications in combinatorialoptimization, as they can be minimized and approximately maximized inpolynomial time. A key element in many of the algorithms and analyses is thepossibility of extending the submodular set-function to a convex function,which opens up tools from convex optimization. Submodularity goes beyondset-functions and has naturally been considered for problems with multiplelabels or for functions defined on continuous domains, where it correspondsessentially to cross second-derivatives being nonpositive. In this paper, weshow that most results relating submodularity and convexity for set-functionscan be extended to all submodular functions. In particular, (a) we naturallydefine a continuous extension in a set of probability measures, (b) show thatthe extension is convex if and only if the original function is submodular, (c)prove that the problem of minimizing a submodular function is equivalent to atypically non-smooth convex optimization problem, and (d) propose anotherconvex optimization problem with better computational properties (e.g., asmooth dual problem). Most of these extensions from the set-function situationare obtained by drawing links with the theory of multi-marginal optimaltransport, which provides also a new interpretation of existing results forset-functions. We then provide practical algorithms to minimize genericsubmodular functions on discrete domains, with associated convergence rates.
arxiv-15600-97 | Deep Learning in Finance | http://arxiv.org/pdf/1602.06561v2.pdf | author:J. B. Heaton, N. G. Polson, J. H. Witte category:cs.LG published:2016-02-21 summary:We explore the use of deep learning hierarchical models for problems infinancial prediction and classification. Financial prediction problems -- suchas those presented in designing and pricing securities, constructingportfolios, and risk management -- often involve large data sets with complexdata interactions that currently are difficult or impossible to specify in afull economic model. Applying deep learning methods to these problems canproduce more useful results than standard methods in finance. In particular,deep learning can detect and exploit interactions in the data that are, atleast currently, invisible to any existing financial economic theory.
arxiv-15600-98 | Search Improves Label for Active Learning | http://arxiv.org/pdf/1602.07265v1.pdf | author:Alina Beygelzimer, Daniel Hsu, John Langford, Chicheng Zhang category:cs.LG stat.ML published:2016-02-23 summary:We investigate active learning with access to two distinct oracles: Label(which is standard) and Search (which is not). The Search oracle models thesituation where a human searches a database to seed or counterexample anexisting solution. Search is stronger than Label while being natural toimplement in many situations. We show that an algorithm using both oracles canprovide exponentially large problem-dependent improvements over Label alone.
arxiv-15600-99 | Learning to Remove Multipath Distortions in Time-of-Flight Range Images for a Robotic Arm Setup | http://arxiv.org/pdf/1601.01750v3.pdf | author:Kilho Son, Ming-Yu Liu, Yuichi Taguchi category:cs.CV cs.RO published:2016-01-08 summary:Range images captured by Time-of-Flight (ToF) cameras are corrupted withmultipath distortions due to interaction between modulated light signals andscenes. The interaction is often complicated, which makes a model-basedsolution elusive. We propose a learning-based approach for removing themultipath distortions for a ToF camera in a robotic arm setup. Our approach isbased on deep learning. We use the robotic arm to automatically collect a largeamount of ToF range images containing various multipath distortions. Thetraining images are automatically labeled by leveraging a high precisionstructured light sensor available only in the training time. In the test time,we apply the learned model to remove the multipath distortions. This allows ourrobotic arm setup to enjoy the speed and compact form of the ToF camera withoutcompromising with its range measurement errors. We conduct extensiveexperimental validations and compare the proposed method to several baselinealgorithms. The experiment results show that our method achieves 55% errorreduction in range estimation and largely outperforms the baseline algorithms.
arxiv-15600-100 | Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning | http://arxiv.org/pdf/1602.07261v1.pdf | author:Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke category:cs.CV published:2016-02-23 summary:Very deep convolutional networks have been central to the largest advances inimage recognition performance in recent years. One example is the Inceptionarchitecture that has been shown to achieve very good performance at relativelylow computational cost. Recently, the introduction of residual connections inconjunction with a more traditional architecture has yielded state-of-the-artperformance in the 2015 ILSVRC challenge; its performance was similar to thelatest generation Inception-v3 network. This raises the question of whetherthere are any benefit in combining the Inception architecture with residualconnections. Here we give clear empirical evidence that training with residualconnections accelerates the training of Inception networks significantly. Thereis also some evidence of residual Inception networks outperforming similarlyexpensive Inception networks without residual connections by a thin margin. Wealso present several new streamlined architectures for both residual andnon-residual Inception networks. These variations improve the single-framerecognition performance on the ILSVRC 2012 classification task significantly.We further demonstrate how proper activation scaling stabilizes the training ofvery wide residual Inception networks. With an ensemble of three residual andone Inception-v4, we achieve 3.08 percent top-5 error on the test set of theImageNet classification (CLS) challenge
arxiv-15600-101 | Petrarch 2 : Petrarcher | http://arxiv.org/pdf/1602.07236v1.pdf | author:Clayton Norris category:cs.CL published:2016-02-23 summary:PETRARCH 2 is the fourth generation of a series of Event-Data coders stemmingfrom research by Phillip Schrodt. Each iteration has brought new functionalityand usability, and this is no exception.Petrarch 2 takes much of the power ofthe original Petrarch's dictionaries and redirects it into a faster and smartercore logic. Earlier iterations handled sentences largely as a list of words,incorporating some syntactic information here and there. Petrarch 2 now viewsthe sentence entirely on the syntactic level. It receives the syntactic parseof a sentence from the Stanford CoreNLP software, and stores this data as atree structure of linked nodes, where each node is a Phrase object.Prepositional, noun, and verb phrases each have their own version of thisPhrase class, which deals with the logic particular to those kinds of phrases.Since this is an event coder, the core of the logic focuses around the verbs:who is acting, who is being acted on, and what is happening. The theory behindthis new structure and its logic is founded in Generative Grammar, InformationTheory, and Lambda-Calculus Semantics.
arxiv-15600-102 | Lens depth function and k-relative neighborhood graph: versatile tools for ordinal data analysis | http://arxiv.org/pdf/1602.07194v1.pdf | author:Matthäus Kleindessner, Ulrike von Luxburg category:stat.ML cs.DS cs.LG published:2016-02-23 summary:In recent years it has become popular to study machine learning problemsbased on ordinal distance information rather than numerical distancemeasurements. By ordinal distance information we refer to binary answers todistance comparisons such as d(A,B) < d(C,D). For many problems in machinelearning and statistics it is unclear how to solve them in such a scenario. Upto now, the main approach is to explicitly construct an ordinal embedding ofthe data points in the Euclidean space, an approach that has a number ofdrawbacks. In this paper, we propose algorithms for the problems of medoidestimation, outlier identification, classification and clustering when givenonly ordinal distance comparisons. They are based on estimating the lens depthfunction and the k-relative neighborhood graph on a data set. Our algorithmsare simple, can easily be parallelized, avoid many of the drawbacks of anordinal embedding approach and are much faster.
arxiv-15600-103 | Explore First, Exploit Next: The True Shape of Regret in Bandit Problems | http://arxiv.org/pdf/1602.07182v1.pdf | author:Aurélien Garivier, Pierre Ménard, Gilles Stoltz category:math.ST cs.LG stat.TH published:2016-02-23 summary:We revisit lower bounds on the regret in the case of multi-armed banditproblems. We obtain non-asymptotic bounds and provide straightforward proofsbased only on well-known properties of Kullback-Leibler divergences. Thesebounds show that in an initial phase the regret grows almost linearly, and thatthe well-known logarithmic growth of the regret only holds in a final phase.The proof techniques come to the essence of the arguments used and they aredeprived of all unnecessary complications.
arxiv-15600-104 | Particle detection and tracking by a-contrario approach: application to fluorescence time-lapse imaging | http://arxiv.org/pdf/1507.06266v4.pdf | author:Mariella Dimiccoli, Jean-Pascal Jacob, Lionel Moisan category:cs.CV published:2015-07-22 summary:In this work, we propose a probabilistic approach for the detection and thetracking of particles on biological images. In presence of very noised and poorquality data, particles and trajectories can be characterized by an a-contrariomodel, that estimates the probability of observing the structures of interestin random data. This approach, first introduced in the modeling of human visualperception and then successfully applied in many image processing tasks, leadsto algorithms that do not require a previous learning stage, nor a tediousparameter tuning and are very robust to noise. Comparative evaluations againsta well established baseline show that the proposed approach outperforms thestate of the art.
arxiv-15600-105 | Permutational Rademacher Complexity: a New Complexity Measure for Transductive Learning | http://arxiv.org/pdf/1505.02910v2.pdf | author:Ilya Tolstikhin, Nikita Zhivotovskiy, Gilles Blanchard category:stat.ML cs.LG published:2015-05-12 summary:Transductive learning considers situations when a learner observes $m$labelled training points and $u$ unlabelled test points with the final goal ofgiving correct answers for the test points. This paper introduces a newcomplexity measure for transductive learning called Permutational RademacherComplexity (PRC) and studies its properties. A novel symmetrization inequalityis proved, which shows that PRC provides a tighter control over expectedsuprema of empirical processes compared to what happens in the standard i.i.d.setting. A number of comparison results are also provided, which show therelation between PRC and other popular complexity measures used in statisticallearning theory, including Rademacher complexity and Transductive RademacherComplexity (TRC). We argue that PRC is a more suitable complexity measure fortransductive learning. Finally, these results are combined with a standardconcentration argument to provide novel data-dependent risk bounds fortransductive learning.
arxiv-15600-106 | Submodular Learning and Covering with Response-Dependent Costs | http://arxiv.org/pdf/1602.07120v1.pdf | author:Sivan Sabato category:cs.LG stat.ML published:2016-02-23 summary:We consider interactive learning and covering problems, in a setting whereactions may incur different costs, depending on the outcomes of the action. Forinstance, in a clinical trial, selecting a patient for treatment might resultin improved health or adverse effects for this patients, and these two outcomeshave different costs. We consider a setting where these costs can be inferredfrom observable responses to the actions. We generalize previous analyses ofinteractive learning and covering to \emph{consistency aware} submodularobjectives, and propose a natural greedy algorithm for the setting ofresponse-dependent costs. We bound the approximation factor of this greedyalgorithm, for general submodular functions, as well as specifically for\emph{learning objectives}, and show that a different property of the costfunction controls the approximation factor in each of these scenarios. Wefurther show that in both settings, the approximation factor of this greedyalgorithm is near-optimal in the class of greedy algorithms. Experimentsdemonstrate the advantages of the proposed algorithm in the response-dependentcost setting.
arxiv-15600-107 | The ImageNet Shuffle: Reorganized Pre-training for Video Event Detection | http://arxiv.org/pdf/1602.07119v1.pdf | author:Pascal Mettes, Dennis C. Koelma, Cees G. M. Snoek category:cs.CV published:2016-02-23 summary:This paper strives for video event detection using a representation learnedfrom deep convolutional neural networks. Different from the leading approaches,who all learn from the 1,000 classes defined in the ImageNet Large Scale VisualRecognition Challenge, we investigate how to leverage the complete ImageNethierarchy for pre-training deep networks. To deal with the problems ofover-specific classes and classes with few images, we introduce a bottom-up andtop-down approach for reorganization of the ImageNet hierarchy based on all its21,814 classes and more than 14 million images. Experiments on the TRECVIDMultimedia Event Detection 2013 and 2015 datasets show that videorepresentations derived from the layers of a deep neural network pre-trainedwith our reorganized hierarchy i) improves over standard pre-training, ii) iscomplementary among different reorganizations, iii) maintains the benefits offusion with other modalities, and iv) leads to state-of-the-art event detectionresults. The reorganized hierarchies and their derived Caffe models arepublicly available at http://tinyurl.com/imagenetshuffle.
arxiv-15600-108 | Learning Efficient Algorithms with Hierarchical Attentive Memory | http://arxiv.org/pdf/1602.03218v2.pdf | author:Marcin Andrychowicz, Karol Kurach category:cs.LG published:2016-02-09 summary:In this paper, we propose and investigate a novel memory architecture forneural networks called Hierarchical Attentive Memory (HAM). It is based on abinary tree with leaves corresponding to memory cells. This allows HAM toperform memory access in O(log n) complexity, which is a significantimprovement over the standard attention mechanism that requires O(n)operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms forproblems like merging, sorting or binary searching from pure input-outputexamples. In particular, it learns to sort n numbers in time O(n log n) andgeneralizes well to input sequences much longer than the ones seen during thetraining. We also show that HAM can be trained to act like classic datastructures: a stack, a FIFO queue and a priority queue.
arxiv-15600-109 | A Streaming Algorithm for Crowdsourced Data Classification | http://arxiv.org/pdf/1602.07107v1.pdf | author:Thomas Bonald, Richard Combes category:stat.ML cs.LG published:2016-02-23 summary:We propose a streaming algorithm for the binary classification of data basedon crowdsourcing. The algorithm learns the competence of each labeller bycomparing her labels to those of other labellers on the same tasks and usesthis information to minimize the prediction error rate on each task. We provideperformance guarantees of our algorithm for a fixed population of independentlabellers. In particular, we show that our algorithm is optimal in the sensethat the cumulative regret compared to the optimal decision with known labellererror probabilities is finite, independently of the number of tasks to label.The complexity of the algorithm is linear in the number of labellers and thenumber of tasks, up to some logarithmic factors. Numerical experimentsillustrate the performance of our algorithm compared to existing algorithms,including simple majority voting and expectation-maximization algorithms, onboth synthetic and real datasets.
arxiv-15600-110 | Online Unmixing of Multitemporal Hyperspectral Images accounting for Spectral Variability | http://arxiv.org/pdf/1510.05893v2.pdf | author:Pierre-Antoine Thouvenin, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV stat.ME published:2015-10-20 summary:Hyperspectral unmixing is aimed at identifying the reference spectralsignatures composing an hyperspectral image and their relative abundancefractions in each pixel. In practice, the identified signatures may varyspectrally from an image to another due to varying acquisition conditions, thusinducing possibly significant estimation errors. Against this background,hyperspectral unmixing of several images acquired over the same area is ofconsiderable interest. Indeed, such an analysis enables the endmembers of thescene to be tracked and the corresponding endmember variability to becharacterized. Sequential endmember estimation from a set of hyperspectralimages is expected to provide improved performance when compared to methodsanalyzing the images independently. However, the significant size ofhyperspectral data precludes the use of batch procedures to jointly estimatethe mixture parameters of a sequence of hyperspectral images. Provided thateach elementary component is present in at least one image of the sequence, wepropose to perform an online hyperspectral unmixing accounting for temporalendmember variability. The online hyperspectral unmixing is formulated as atwo-stage stochastic program, which can be solved using a stochasticapproximation. The performance of the proposed method is evaluated on syntheticand real data. A comparison with independent unmixings of each image byestablished methods finally illustrates the interest of the proposed strategy.
arxiv-15600-111 | Object Learning and Convex Cardinal Shape Composition | http://arxiv.org/pdf/1602.07613v1.pdf | author:Alireza Aghasi, Justin Romberg category:cs.CV math.OC published:2016-02-23 summary:This work mainly focuses on a novel segmentation and partitioning scheme,based on learning the principal elements of the optimal partitioner in theimage. The problem of interest is characterizing the objects present in animage as a composition of matching elements from a dictionary of prototypeshapes. The composition model allows set union and difference among theselected elements, while regularizing the problem by restricting their count toa fixed level. This is a combinatorial problem addressing which is not ingeneral computationally tractable. Convex cardinal shape composition (CSC) is arecent relaxation scheme presented as a proxy to the original problem. From atheoretical standpoint, this paper improves the results presented in theoriginal work, by deriving the general sufficient conditions under which CSCidentifies a target composition. We also provide qualitative results on whowell the CSC outcome approximates the combinatorial solution. From acomputational standpoint, two convex solvers, one supporting distributedprocessing for large-scale problems, and one cast as a linear program arepresented. Applications such as multi-resolution segmentation and recovery ofthe principal shape components are presented as the experiments supporting theproposed ideas.
arxiv-15600-112 | Minimum Conditional Description Length Estimation for Markov Random Fields | http://arxiv.org/pdf/1602.03061v2.pdf | author:Matthew G. Reyes, David L. Neuhoff category:cs.IT cs.LG math.IT math.ST stat.TH published:2016-02-09 summary:In this paper we discuss a method, which we call Minimum ConditionalDescription Length (MCDL), for estimating the parameters of a subset of siteswithin a Markov random field. We assume that the edges are known for the entiregraph $G=(V,E)$. Then, for a subset $U\subset V$, we estimate the parametersfor nodes and edges in $U$ as well as for edges incident to a node in $U$, byfinding the exponential parameter for that subset that yields the bestcompression conditioned on the values on the boundary $\partial U$. Ourestimate is derived from a temporally stationary sequence of observations onthe set $U$. We discuss how this method can also be applied to estimate aspatially invariant parameter from a single configuration, and in so doing,derive the Maximum Pseudo-Likelihood (MPL) estimate.
arxiv-15600-113 | WordRank: Learning Word Embeddings via Robust Ranking | http://arxiv.org/pdf/1506.02761v3.pdf | author:Shihao Ji, Hyokun Yun, Pinar Yanardag, Shin Matsushima, S. V. N. Vishwanathan category:cs.CL cs.LG stat.ML published:2015-06-09 summary:Embedding words in a vector space has gained a lot of attention in recentyears. While state-of-the-art methods provide efficient computation of wordsimilarities via a low-dimensional matrix embedding, their motivation is oftenleft unclear. In this paper, we argue that word embedding can be naturallyviewed as a ranking problem due to the ranking nature of the evaluationmetrics. Then, based on this insight, we propose a novel framework WordRankthat efficiently estimates word representations via robust ranking, in whichthe attention mechanism and robustness to noise are readily achieved via theDCG-like ranking losses. The performance of WordRank is measured in wordsimilarity and word analogy benchmarks, and the results are compared to thestate-of-the-art word embedding techniques. Our algorithm is very competitiveto the state-of-the-arts on large corpora, while outperforms them by asignificant margin when the training set is limited (i.e., sparse and noisy).With 17 million tokens, WordRank performs almost as well as existing methodsusing 7.2 billion tokens on a popular word similarity benchmark. Ourmulti-machine distributed implementation of WordRank is open sourced forgeneral usage.
arxiv-15600-114 | An Improved Gap-Dependency Analysis of the Noisy Power Method | http://arxiv.org/pdf/1602.07046v1.pdf | author:Maria Florina Balcan, Simon S. Du, Yining Wang, Adams Wei Yu category:stat.ML cs.LG published:2016-02-23 summary:We consider the noisy power method algorithm, which has wide applications inmachine learning and statistics, especially those related to principalcomponent analysis (PCA) under resource (communication, memory or privacy)constraints. Existing analysis of the noisy power method shows anunsatisfactory dependency over the "consecutive" spectral gap$(\sigma_k-\sigma_{k+1})$ of an input data matrix, which could be very smalland hence limits the algorithm's applicability. In this paper, we present a newanalysis of the noisy power method that achieves improved gap dependency forboth sample complexity and noise tolerance bounds. More specifically, weimprove the dependency over $(\sigma_k-\sigma_{k+1})$ to dependency over$(\sigma_k-\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter andcould be much larger than the target rank $k$. Our proofs are built upon anovel characterization of proximity between two subspaces that differ fromcanonical angle characterizations analyzed in previous works. Finally, we applyour improved bounds to distributed private PCA and memory-efficient streamingPCA and obtain bounds that are superior to existing results in the literature.
arxiv-15600-115 | Auditing Black-box Models by Obscuring Features | http://arxiv.org/pdf/1602.07043v1.pdf | author:Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, Suresh Venkatasubramanian category:stat.ML cs.LG published:2016-02-23 summary:Data-trained predictive models are widely used to assist in decision making.But they are used as black boxes that output a prediction or score. It istherefore hard to acquire a deeper understanding of model behavior: and inparticular how different attributes influence the model prediction. This isvery important when trying to interpret the behavior of complex models, orensure that certain problematic attributes (like race or gender) are not undulyinfluencing decisions. In this paper, we present a technique for auditing black-box models: we canstudy the extent to which existing models take advantage of particular featuresin the dataset without knowing how the models work. We show how a class oftechniques originally developed for the detection and repair of disparateimpact in classification models can be used to study the sensitivity of anymodel with respect to any feature subsets. Our approach does not require the black-box model to be retrained. This isimportant if (for example) the model is only accessible via an API, andcontrasts our work with other methods that investigate feature influence likefeature selection. We present experimental evidence for the effectiveness ofour procedure using a variety of publicly available datasets and models. Wealso validate our procedure using techniques from interpretable learning andfeature selection.
arxiv-15600-116 | Computer Aided Restoration of Handwritten Character Strokes | http://arxiv.org/pdf/1602.07038v1.pdf | author:Barak Sober, David Levin category:cs.GR cs.CV math.NA published:2016-02-23 summary:This work suggests a new variational approach to the task of computer aidedrestoration of incomplete characters, residing in a highly noisy document. Wemodel character strokes as the movement of a pen with a varying radius.Following this model, a cubic spline representation is being utilized toperform gradient descent steps, while maintaining interpolation at some initial(manually sampled) points. The proposed algorithm was utilized in the processof restoring approximately 1000 ancient Hebrew characters (dating to ca.8th-7th century BCE), some of which are presented herein and show that thealgorithm yields plausible results when applied on deteriorated documents.
arxiv-15600-117 | Mobile Big Data Analytics Using Deep Learning and Apache Spark | http://arxiv.org/pdf/1602.07031v1.pdf | author:Mohammad Abu Alsheikh, Dusit Niyato, Shaowei Lin, Hwee-Pink Tan, Zhu Han category:cs.DC cs.LG cs.NE published:2016-02-23 summary:The proliferation of mobile devices, such as smartphones and Internet ofThings (IoT) gadgets, results in the recent mobile big data (MBD) era.Collecting MBD is unprofitable unless suitable analytics and learning methodsare utilized for extracting meaningful information and hidden patterns fromdata. This article presents an overview and brief tutorial of deep learning inMBD analytics and discusses a scalable learning framework over Apache Spark.Specifically, a distributed deep learning is executed as an iterative MapReducecomputing on many Spark workers. Each Spark worker learns a partial deep modelon a partition of the overall MBD, and a master deep model is then built byaveraging the parameters of all partial models. This Spark-based frameworkspeeds up the learning of deep models consisting of many hidden layers andmillions of parameters. We use a context-aware activity recognition applicationwith a real-world dataset containing millions of samples to validate ourframework and assess its speedup effectiveness.
arxiv-15600-118 | Latent Skill Embedding for Personalized Lesson Sequence Recommendation | http://arxiv.org/pdf/1602.07029v1.pdf | author:Siddharth Reddy, Igor Labutov, Thorsten Joachims category:cs.LG cs.AI cs.CY published:2016-02-23 summary:Students in online courses generate large amounts of data that can be used topersonalize the learning process and improve quality of education. In thispaper, we present the Latent Skill Embedding (LSE), a probabilistic model ofstudents and educational content that can be used to recommend personalizedsequences of lessons with the goal of helping students prepare for specificassessments. Akin to collaborative filtering for recommender systems, thealgorithm does not require students or content to be described by features, butit learns a representation using access traces. We formulate this problem as aregularized maximum-likelihood embedding of students, lessons, and assessmentsfrom historical student-content interactions. An empirical evaluation onlarge-scale data from Knewton, an adaptive learning technology company, showsthat this approach predicts assessment results competitively with benchmarkmodels and is able to discriminate between lesson sequences that lead tomastery and failure.
arxiv-15600-119 | Maximum margin classifier working in a set of strings | http://arxiv.org/pdf/1406.0597v3.pdf | author:Hitoshi Koyano, Morihiro Hayashida, Tatsuya Akutsu category:stat.ML published:2014-06-03 summary:Numbers and numerical vectors account for a large portion of data. However,recently the amount of string data generated has increased dramatically.Consequently, classifying string data is a common problem in many fields. Themost widely used approach to this problem is to convert strings into numericalvectors using string kernels and subsequently apply a support vector machinethat works in a numerical vector space. However, this non-one-to-one conversioninvolves a loss of information and makes it impossible to evaluate, usingprobability theory, the generalization error of a learning machine, consideringthat the given data to train and test the machine are strings generatedaccording to probability laws. In this study, we approach this classificationproblem by constructing a classifier that works in a set of strings. Toevaluate the generalization error of such a classifier theoretically,probability theory for strings is required. Therefore, we first extend a limittheorem on the asymptotic behavior of a consensus sequence of strings, which isthe counterpart of the mean of numerical vectors, as demonstrated in theprobability theory on a metric space of strings developed by one of the authorsand his colleague in a previous study [18]. Using the obtained result, we thendemonstrate that our learning machine classifies strings in an asymptoticallyoptimal manner. Furthermore, we demonstrate the usefulness of our machine inpractical data analysis by applying it to predicting protein--proteininteractions using amino acid sequences.
arxiv-15600-120 | Representing Meaning with a Combination of Logical Form and Vectors | http://arxiv.org/pdf/1505.06816v3.pdf | author:I. Beltagy, Stephen Roller, Pengxiang Cheng, Katrin Erk, Raymond J. Mooney category:cs.CL published:2015-05-26 summary:NLP tasks differ in the semantic information they require, and at this timeno single se- mantic representation fulfills all requirements. Logic-basedrepresentations characterize sentence structure, but do not capture the gradedaspect of meaning. Distributional models give graded similarity ratings forwords and phrases, but do not capture sentence structure in the same detail aslogic-based approaches. So it has been argued that the two are complementary.We adopt a hybrid approach that combines logic-based and distributionalsemantics through probabilistic logic inference in Markov Logic Networks(MLNs). In this paper, we focus on the three components of a practical systemintegrating logical and distributional models: 1) Parsing and taskrepresentation is the logic-based part where input problems are represented inprobabilistic logic. This is quite different from representing them in standardfirst-order logic. 2) For knowledge base construction we form weightedinference rules. We integrate and compare distributional information with othersources, notably WordNet and an existing paraphrase collection. In particular,we use our system to evaluate distributional lexical entailment approaches. Weuse a variant of Robinson resolution to determine the necessary inferencerules. More sources can easily be added by mapping them to logical rules; oursystem learns a resource-specific weight that corrects for scaling differencesbetween resources. 3) In discussing probabilistic inference, we show how tosolve the inference problems efficiently. To evaluate our approach, we use thetask of textual entailment (RTE), which can utilize the strengths of bothlogic-based and distributional representations. In particular we focus on theSICK dataset, where we achieve state-of-the-art results.
arxiv-15600-121 | Sentence Similarity Learning by Lexical Decomposition and Composition | http://arxiv.org/pdf/1602.07019v1.pdf | author:Zhiguo Wang, Haitao Mi, Abraham Ittycheriah category:cs.CL published:2016-02-23 summary:Most conventional sentence similarity methods only focus on similar parts oftwo input sentences, and simply ignore the dissimilar parts, which usually giveus some clues and semantic meanings about the sentences. In this work, wepropose a model to take into account both the similarities and dissimilaritiesby decomposing and composing lexical semantics over sentences. The modelrepresents each word as a vector, and calculates a semantic matching vector foreach word based on all words in the other sentence. Then, each word vector isdecomposed into a similar component and a dissimilar component based on thesemantic matching vector. After this, a two-channel CNN model is employed tocapture features by composing the similar and dissimilar components. Finally, asimilarity score is estimated over the composed feature vectors. Experimentalresults show that our model gets the state-of-the-art performance on the answersentence selection task, and achieves a comparable result on the paraphraseidentification task.
arxiv-15600-122 | A survey of sparse representation: algorithms and applications | http://arxiv.org/pdf/1602.07017v1.pdf | author:Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, David Zhang category:cs.CV cs.LG published:2016-02-23 summary:Sparse representation has attracted much attention from researchers in fieldsof signal processing, image processing, computer vision and patternrecognition. Sparse representation also has a good reputation in boththeoretical research and practical applications. Many different algorithms havebeen proposed for sparse representation. The main purpose of this article is toprovide a comprehensive study and an updated review on sparse representationand to supply a guidance for researchers. The taxonomy of sparse representationmethods can be studied from various viewpoints. For example, in terms ofdifferent norm minimizations used in sparsity constraints, the methods can beroughly categorized into five groups: sparse representation with $l_0$-normminimization, sparse representation with $l_p$-norm (0$<$p$<$1) minimization,sparse representation with $l_1$-norm minimization and sparse representationwith $l_{2,1}$-norm minimization. In this paper, a comprehensive overview ofsparse representation is provided. The available sparse representationalgorithms can also be empirically categorized into four groups: greedystrategy approximation, constrained optimization, proximity algorithm-basedoptimization, and homotopy algorithm-based sparse representation. Therationales of different algorithms in each category are analyzed and a widerange of sparse representation applications are summarized, which couldsufficiently reveal the potential nature of the sparse representation theory.Specifically, an experimentally comparative study of these sparserepresentation algorithms was presented. The Matlab code used in this paper canbe available at: http://www.yongxu.org/lunwen.html.
arxiv-15600-123 | Recovering the number of clusters in data sets with noise features using feature rescaling factors | http://arxiv.org/pdf/1602.06989v1.pdf | author:Renato Cordeiro de Amorim, Christian Hennig category:stat.ML cs.LG published:2016-02-22 summary:In this paper we introduce three methods for re-scaling data sets aiming atimproving the likelihood of clustering validity indexes to return the truenumber of spherical Gaussian clusters with additional noise features. Ourmethod obtains feature re-scaling factors taking into account the structure ofa given data set and the intuitive idea that different features may havedifferent degrees of relevance at different clusters. We experiment with the Silhouette (using squared Euclidean, Manhattan, andthe p$^{th}$ power of the Minkowski distance), Dunn's, Calinski-Harabasz andHartigan indexes on data sets with spherical Gaussian clusters with and withoutnoise features. We conclude that our methods indeed increase the chances ofestimating the true number of clusters in a data set.
arxiv-15600-124 | Empath: Understanding Topic Signals in Large-Scale Text | http://arxiv.org/pdf/1602.06979v1.pdf | author:Ethan Fast, Binbin Chen, Michael Bernstein category:cs.CL cs.AI published:2016-02-22 summary:Human language is colored by a broad range of topics, but existing textanalysis tools only focus on a small number of them. We present Empath, a toolthat can generate and validate new lexical categories on demand from a smallset of seed terms (like "bleed" and "punch" to generate the category violence).Empath draws connotations between words and phrases by deep learning a neuralembedding across more than 1.8 billion words of modern fiction. Given a smallset of seed words that characterize a category, Empath uses its neuralembedding to discover new related terms, then validates the category with acrowd-powered filter. Empath also analyzes text across 200 built-in,pre-validated categories we have generated from common topics in our webdataset, like neglect, government, and social media. We show that Empath'sdata-driven, human validated categories are highly correlated (r=0.906) withsimilar categories in LIWC.
arxiv-15600-125 | Blind score normalization method for PLDA based speaker recognition | http://arxiv.org/pdf/1602.06967v1.pdf | author:Danila Doroshin, Nikolay Lubimov, Marina Nastasenko, Mikhail Kotov category:cs.CL cs.LG cs.SD published:2016-02-22 summary:Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-artmethod for modeling $i$-vector space in speaker recognition task. However theperformance degradation is observed if enrollment data size differs from onespeaker to another. This paper presents a solution to such problem byintroducing new PLDA scoring normalization technique. Normalization parametersare derived in a blind way, so that, unlike traditional \textit{ZT-norm}, noextra development data is required. Moreover, proposed method has shown to beoptimal in terms of detection cost function. The experiments conducted on NISTSRE 2014 database demonstrate an improved accuracy in a mixed enrollment numbercondition.
arxiv-15600-126 | Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning | http://arxiv.org/pdf/1503.00949v3.pdf | author:Ramazan Gokberk Cinbis, Jakob Verbeek, Cordelia Schmid category:cs.CV published:2015-03-03 summary:Object category localization is a challenging problem in computer vision.Standard supervised training requires bounding box annotations of objectinstances. This time-consuming annotation process is sidestepped in weaklysupervised learning. In this case, the supervised information is restricted tobinary labels that indicate the absence/presence of object instances in theimage, without their locations. We follow a multiple-instance learning approachthat iteratively trains the detector and infers the object locations in thepositive training images. Our main contribution is a multi-fold multipleinstance learning procedure, which prevents training from prematurely lockingonto erroneous object locations. This procedure is particularly important whenusing high-dimensional representations, such as Fisher vectors andconvolutional neural network features. We also propose a window refinementmethod, which improves the localization accuracy by incorporating an objectnessprior. We present a detailed experimental evaluation using the PASCAL VOC 2007dataset, which verifies the effectiveness of our approach.
arxiv-15600-127 | Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning | http://arxiv.org/pdf/1511.06342v4.pdf | author:Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov category:cs.LG published:2015-11-19 summary:The ability to act in multiple environments and transfer previous knowledgeto new situations can be considered a critical aspect of any intelligent agent.Towards this goal, we define a novel method of multitask and transfer learningthat enables an autonomous agent to learn how to behave in multiple taskssimultaneously, and then generalize its knowledge to new domains. This method,termed "Actor-Mimic", exploits the use of deep reinforcement learning and modelcompression techniques to train a single policy network that learns how to actin a set of distinct tasks by using the guidance of several expert teachers. Wethen show that the representations learnt by the deep policy network arecapable of generalizing to new tasks with no prior expert guidance, speeding uplearning in novel environments. Although our method can in general be appliedto a wide range of problems, we use Atari games as a testing environment todemonstrate these methods.
arxiv-15600-128 | Sparse Linear Regression via Generalized Orthogonal Least-Squares | http://arxiv.org/pdf/1602.06916v1.pdf | author:Abolfazl Hashemi, Haris Vikalo category:stat.ML cs.IT cs.LG math.IT published:2016-02-22 summary:Sparse linear regression, which entails finding a sparse solution to anunderdetermined system of linear equations, can formally be expressed as an$l_0$-constrained least-squares problem. The Orthogonal Least-Squares (OLS)algorithm sequentially selects the features (i.e., columns of the coefficientmatrix) to greedily find an approximate sparse solution. In this paper, ageneralization of Orthogonal Least-Squares which relies on a recursive relationbetween the components of the optimal solution to select L features at eachstep and solve the resulting overdetermined system of equations is proposed.Simulation results demonstrate that the generalized OLS algorithm iscomputationally efficient and achieves performance superior to that of existinggreedy algorithms broadly used in the literature.
arxiv-15600-129 | Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model | http://arxiv.org/pdf/1511.01574v2.pdf | author:Ciprian Chelba, Fernando Pereira category:cs.CL published:2015-11-05 summary:We describe Sparse Non-negative Matrix (SNM) language model estimation usingmultinomial loss on held-out data. Being able to train on held-out data is important in practical situationswhere the training data is usually mismatched from the held-out/test data. Itis also less constrained than the previous training algorithm usingleave-one-out on training data: it allows the use of richer meta-features inthe adjustment model, e.g. the diversity counts used by Kneser-Ney smoothingwhich would be difficult to deal with correctly in leave-one-out training. In experiments on the one billion words language modeling benchmark, we areable to slightly improve on our previous results which use a different lossfunction, and employ leave-one-out training on a subset of the main trainingset. Surprisingly, an adjustment model with meta-features that discard alllexical information can perform as well as lexicalized meta-features. We findthat fairly small amounts of held-out data (on the order of 30-70 thousandwords) are sufficient for training the adjustment model. In a real-life scenario where the training data is a mix of data sources thatare imbalanced in size, and of different degrees of relevance to the held-outand test data, taking into account the data source for a given skip-/n-gramfeature and combining them for best performance on held-out/test data improvesover skip-/n-gram SNM models trained on pooled data by about 8% in the SMTsetup, or as much as 15% in the ASR/IME setup. The ability to mix various data sources based on how relevant they are to amismatched held-out set is probably the most attractive feature of the newestimation method for SNM LM.
arxiv-15600-130 | Clustering with a Reject Option: Interactive Clustering as Bayesian Prior Elicitation | http://arxiv.org/pdf/1602.06886v1.pdf | author:Akash Srivastava, James Zou, Ryan P. Adams, Charles Sutton category:stat.ML cs.LG published:2016-02-22 summary:A good clustering can help a data analyst to explore and understand a dataset, but what constitutes a good clustering may depend on domain-specific andapplication-specific criteria. These criteria can be difficult to formalize,even when it is easy for an analyst to know a good clustering when they seeone. We present a new approach to interactive clustering for data explorationcalled TINDER, based on a particularly simple feedback mechanism, in which ananalyst can reject a given clustering and request a new one, which is chosen tobe different from the previous clustering while fitting the data well. Weformalize this interaction in a Bayesian framework as a method for priorelicitation, in which each different clustering is produced by a priordistribution that is modified to discourage previously rejected clusterings. Weshow that TINDER successfully produces a diverse set of clusterings, each ofequivalent quality, that are much more diverse than would be obtained byrandomized restarts.
arxiv-15600-131 | Learning to classify with possible sensor failures | http://arxiv.org/pdf/1507.04540v3.pdf | author:Tianpei Xie, Nasser M. Nasrabadi, Alfred O. Hero category:cs.LG cs.IT math.IT stat.ML published:2015-07-16 summary:In this paper, we propose a general framework to learn a robust large-marginbinary classifier when corrupt measurements, called anomalies, caused by sensorfailure might be present in the training set. The goal is to minimize thegeneralization error of the classifier on non-corrupted measurements whilecontrolling the false alarm rate associated with anomalous samples. Byincorporating a non-parametric regularizer based on an empirical entropyestimator, we propose a Geometric-Entropy-Minimization regularized MaximumEntropy Discrimination (GEM-MED) method to learn to classify and detectanomalies in a joint manner. We demonstrate using simulated data and a realmultimodal data set. Our GEM-MED method can yield improved performance overprevious robust classification methods in terms of both classification accuracyand anomaly detection rate.
arxiv-15600-132 | 3D Pictorial Structures on RGB-D Data for Articulated Human Detection in Operating Rooms | http://arxiv.org/pdf/1602.03468v2.pdf | author:Abdolrahim Kadkhodamohammadi, Afshin Gangi, Michel de Mathelin, Nicolas Padoy category:cs.CV published:2016-02-10 summary:Reliable human pose estimation (HPE) is essential to many clinicalapplications, such as surgical workflow analysis, radiation safety monitoringand human-robot cooperation. Proposed methods for the operating room (OR) relyeither on foreground estimation using a multi-camera system, which is achallenge in real ORs due to color similarities and frequent illuminationchanges, or on wearable sensors or markers, which are invasive and thereforedifficult to introduce in the room. Instead, we propose a novel approach basedon Pictorial Structures (PS) and on RGB-D data, which can be easily deployed inreal ORs. We extend the PS framework in two ways. First, we build robust anddiscriminative part detectors using both color and depth images. We alsopresent a novel descriptor for depth images, called histogram of depthdifferences (HDD). Second, we extend PS to 3D by proposing 3D pairwiseconstraints and a new method for exact and tractable inference. Our approach isevaluated for pose estimation and clinician detection on a challenging RGB-Ddataset recorded in a busy operating room during live surgeries. We conductseries of experiments to study the different part detectors in conjunction withthe various 2D or 3D pairwise constraints. Our comparisons demonstrate that 3DPS with RGB-D part detectors significantly improves the results in a visuallychallenging operating environment.
arxiv-15600-133 | Temporal Network Analysis of Literary Texts | http://arxiv.org/pdf/1602.07275v1.pdf | author:Sandra D. Prado, Silvio R. Dahmen, Ana L. C. Bazzan, Padraig Mac Carron, Ralph Kenna category:physics.soc-ph cs.CL published:2016-02-22 summary:We study temporal networks of characters in literature focusing on "Alice'sAdventures in Wonderland" (1865) by Lewis Carroll and the anonymous "La Chansonde Roland" (around 1100). The former, one of the most influential pieces ofnonsense literature ever written, describes the adventures of Alice in afantasy world with logic plays interspersed along the narrative. The latter, asong of heroic deeds, depicts the Battle of Roncevaux in 778 A.D. duringCharlemagne's campaign on the Iberian Peninsula. We apply methods recentlydeveloped by Taylor and coworkers \cite{Taylor+2015} to find time-averagedeigenvector centralities, Freeman indices and vitalities of characters. We showthat temporal networks are more appropriate than static ones for studyingstories, as they capture features that the time-independent approaches fail toyield.
arxiv-15600-134 | Principal Component Projection Without Principal Component Analysis | http://arxiv.org/pdf/1602.06872v1.pdf | author:Roy Frostig, Cameron Musco, Christopher Musco, Aaron Sidford category:cs.DS cs.LG stat.ML published:2016-02-22 summary:We show how to efficiently project a vector onto the top principal componentsof a matrix, without explicitly computing these components. Specifically, weintroduce an iterative algorithm that provably computes the projection usingfew calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is thefirst with no runtime dependence on the number of top principal components. Weshow that it can be used to give a fast iterative method for the popularprincipal component regression problem, giving the first major runtimeimprovement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used toobtain a "smooth projection" onto the top principal components. We then sharpenthis approximation to true projection using a low-degree polynomialapproximation to the matrix step function. Step function approximation is atopic of long-term interest in scientific computing. We extend prior theory byconstructing polynomials with simple iterative structure and rigorouslyanalyzing their behavior under limited precision.
arxiv-15600-135 | Geodesics of learned representations | http://arxiv.org/pdf/1511.06394v4.pdf | author:Olivier J. Hénaff, Eero P. Simoncelli category:cs.CV cs.LG published:2015-11-19 summary:We develop a new method for visualizing and refining the invariances oflearned representations. Specifically, we test for a general form ofinvariance, linearization, in which the action of a transformation is confinedto a low-dimensional subspace. Given two reference images (typically, differingby some transformation), we synthesize a sequence of images lying on a pathbetween them that is of minimal length in the space of the representation (a"representational geodesic"). If the transformation relating the two referenceimages is linearized by the representation, this sequence should follow thegradual evolution of this transformation. We use this method to assess theinvariance properties of a state-of-the-art image classification network andfind that geodesics generated for image pairs differing by translation,rotation, and dilation do not evolve according to their associatedtransformations. Our method also suggests a remedy for these failures, andfollowing this prescription, we show that the modified representation is ableto linearize a variety of geometric image transformations.
arxiv-15600-136 | Higher-Order Low-Rank Regression | http://arxiv.org/pdf/1602.06863v1.pdf | author:Guillaume Rabusseau, Hachem Kadri category:cs.LG published:2016-02-22 summary:This paper proposes an efficient algorithm (HOLRR) to handle regression taskswhere the outputs have a tensor structure. We formulate the regression problemas the minimization of a least square criterion under a multilinear rankconstraint, a difficult non convex problem. HOLRR computes efficiently anapproximate solution of this problem, with solid theoretical guarantees. Akernel extension is also presented. Experiments on synthetic and real data showthat HOLRR outperforms multivariate and multilinear regression methods and isconsiderably faster than existing tensor methods.
arxiv-15600-137 | Dynamic Filtering of Time-Varying Sparse Signals via l1 Minimization | http://arxiv.org/pdf/1507.06145v2.pdf | author:Adam Charles, Aurele Balavoine, Christopher Rozell category:math.ST stat.ML stat.TH published:2015-07-22 summary:Despite the importance of sparsity signal models and the increasingprevalence of high-dimensional streaming data, there are relatively fewalgorithms for dynamic filtering of time-varying sparse signals. Of theexisting algorithms, fewer still provide strong performance guarantees. Thispaper examines two algorithms for dynamic filtering of sparse signals that arebased on efficient l1 optimization methods. We first present an analysis forone simple algorithm (BPDN-DF) that works well when the system dynamics areknown exactly. We then introduce a novel second algorithm (RWL1-DF) that ismore computationally complex than BPDN-DF but performs better in practice,especially in the case where the system dynamics model is inaccurate.Robustness to model inaccuracy is achieved by using a hierarchicalprobabilistic data model and propagating higher-order statistics from theprevious estimate (akin to Kalman filtering) in the sparse inference process.We demonstrate the properties of these algorithms on both simulated data aswell as natural video sequences. Taken together, the algorithms presented inthis paper represent the first strong performance analysis of dynamic filteringalgorithms for time-varying sparse signals as well as state-of-the-artperformance in this emerging application.
arxiv-15600-138 | From quantum foundations via natural language meaning to a theory of everything | http://arxiv.org/pdf/1602.07618v1.pdf | author:Bob Coecke category:cs.CL quant-ph published:2016-02-22 summary:In this paper we argue for a paradigmatic shift from `reductionism' to`togetherness'. In particular, we show how interaction between systems inquantum theory naturally carries over to modelling how word meanings interactin natural language. Since meaning in natural language, depending on thesubject domain, encompasses discussions within any scientific discipline, weobtain a template for theories such as social interaction, animal behaviour,and many others.
arxiv-15600-139 | Understanding Visual Concepts with Continuation Learning | http://arxiv.org/pdf/1602.06822v1.pdf | author:William F. Whitney, Michael Chang, Tejas Kulkarni, Joshua B. Tenenbaum category:cs.LG published:2016-02-22 summary:We introduce a neural network architecture and a learning algorithm toproduce factorized symbolic representations. We propose to learn these conceptsby observing consecutive frames, letting all the components of the hiddenrepresentation except a small discrete set (gating units) be predicted from theprevious frame, and let the factors of variation in the next frame berepresented entirely by these discrete gated units (corresponding to symbolicrepresentations). We demonstrate the efficacy of our approach on datasets offaces undergoing 3D transformations and Atari 2600 games.
arxiv-15600-140 | The p-filter: multi-layer FDR control for grouped hypotheses | http://arxiv.org/pdf/1512.03397v2.pdf | author:Rina Foygel Barber, Aaditya Ramdas category:stat.ME stat.ML published:2015-12-10 summary:In many practical applications of multiple hypothesis testing using the FalseDiscovery Rate (FDR), the given hypotheses can be naturally partitioned intogroups, and one may not only want to control the number of false discoveries(wrongly rejected null hypotheses), but also the number of falsely discoveredgroups of hypotheses (we say a group is falsely discovered if at least onehypothesis within that group is rejected, when in reality the group containsonly nulls). In this paper, we introduce the p-filter, a procedure whichunifies and generalizes the standard FDR procedure by Benjamini and Hochbergand global null testing procedure by Simes. We first prove that our proposedmethod can simultaneously control the overall FDR at the finest level(individual hypotheses treated separately) and the group FDR at coarser levels(when such groups are user-specified). We then generalize the p-filterprocedure even further to handle multiple partitions of hypotheses, since thatmight be natural in many applications. For example, in neuroscienceexperiments, we may have a hypothesis for every (discretized) location in thebrain, and at every (discretized) timepoint: does the stimulus correlate withactivity in location x at time t after the stimulus was presented? In thissetting, one might want to group hypotheses by location and by time.Importantly, our procedure can handle multiple partitions which arenonhierarchical (i.e. one partition may arrange p-values by voxel, and anotherpartition arranges them by time point; neither one is nested inside the other).We prove that our procedure controls FDR simultaneously across these multiplelay- ers, under assumptions that are standard in the literature: we do not needthe hypotheses to be independent, but require a nonnegative dependencecondition known as PRDS.
arxiv-15600-141 | Semi-supervised Clustering for Short Text via Deep Representation Learning | http://arxiv.org/pdf/1602.06797v1.pdf | author:Zhiguo Wang, Haitao Mi, Abraham Ittycheriah category:cs.CL published:2016-02-22 summary:In this work, we propose a semi-supervised method for short text clustering,where we represent texts as distributed vectors with neural networks, and use asmall amount of labeled data to specify our intention for clustering. We designa novel objective to combine the representation learning process and thek-means clustering process together, and optimize the objective with bothlabeled data and unlabeled data iteratively until convergence through threesteps: (1) assign each short text to its nearest centroid based on itsrepresentation from the current neural networks; (2) re-estimate the clustercentroids based on cluster assignments from step (1); (3) update neuralnetworks according to the objective by keeping centroids and clusterassignments fixed. Experimental results on four datasets show that our methodworks significantly better than several other text clustering methods.
arxiv-15600-142 | A Statistical Model for Stroke Outcome Prediction and Treatment Planning | http://arxiv.org/pdf/1602.07280v1.pdf | author:Abhishek Sengupta, Vaibhav Rajan, Sakyajit Bhattacharya, G R K Sarma category:stat.AP cs.LG published:2016-02-22 summary:Stroke is a major cause of mortality and long--term disability in the world.Predictive outcome models in stroke are valuable for personalized treatment,rehabilitation planning and in controlled clinical trials. In this paper wedesign a new model to predict outcome in the short-term, the putativetherapeutic window for several treatments. Our regression-based model has aparametric form that is designed to address many challenges common in medicaldatasets like highly correlated variables and class imbalance. Empirically ourmodel outperforms the best--known previous models in predicting short--termoutcomes and in inferring the most effective treatments that improve outcome.
arxiv-15600-143 | Convexification of Learning from Constraints | http://arxiv.org/pdf/1602.06746v1.pdf | author:Iaroslav Shcherbatyi, Bjoern Andres category:cs.LG math.OC stat.ML published:2016-02-22 summary:Regularized empirical risk minimization with constrained labels (in contrastto fixed labels) is a remarkably general abstraction of learning. For commonloss and regularization functions, this optimization problem assumes the formof a mixed integer program (MIP) whose objective function is non-convex. Inthis form, the problem is resistant to standard optimization techniques. Weconstruct MIPs with the same solutions whose objective functions are convex.Specifically, we characterize the tightest convex extension of the objectivefunction, given by the Legendre-Fenchel biconjugate. Computing values of thistightest convex extension is NP-hard. However, by applying our characterizationto every function in an additive decomposition of the objective function, weobtain a class of looser convex extensions that can be computed efficiently.For some decompositions, common loss and regularization functions, we derive aclosed form.
arxiv-15600-144 | Variational inference for Monte Carlo objectives | http://arxiv.org/pdf/1602.06725v1.pdf | author:Andriy Mnih, Danilo J. Rezende category:cs.LG stat.ML published:2016-02-22 summary:Recent progress in deep latent variable models has largely been driven by thedevelopment of flexible and scalable variational inference methods. Variationaltraining of this type involves maximizing a lower bound on the log-likelihood,using samples from the variational posterior to compute the required gradients.Recently, Burda et al. (2015) have derived a tighter lower bound using amulti-sample importance sampling estimate of the likelihood and showed thatoptimizing it yields models that use more of their capacity and achieve higherlikelihoods. This development showed the importance of such multi-sampleobjectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyzethe difficulty encountered when estimating the gradients involved. We thendevelop the first unbiased gradient estimator designed for importance-sampledobjectives and evaluate it at training generative and structured outputprediction models. The resulting estimator, which is based on low-varianceper-sample learning signals, is both simpler and more effective than the NVILestimator proposed for the single-sample variational objective, and iscompetitive with the currently used biased estimators.
arxiv-15600-145 | Distributed Deep Learning Using Synchronous Stochastic Gradient Descent | http://arxiv.org/pdf/1602.06709v1.pdf | author:Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan Vaidynathan, Srinivas Sridharan, Dhiraj Kalamkar, Bharat Kaul, Pradeep Dubey category:cs.DC cs.LG published:2016-02-22 summary:We design and implement a distributed multinode synchronous SGD algorithm,without altering hyper parameters, or compressing data, or altering algorithmicbehavior. We perform a detailed analysis of scaling, and identify optimaldesign points for different networks. We demonstrate scaling of CNNs on 100s ofnodes, and present what we believe to be record training throughputs. A 512minibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatchVGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64node cluster. We also demonstrate the generality of our approach viabest-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attemptto democratize deep-learning by training on an Ethernet based AWS cluster andshow ~14X scaling on 16 nodes.
arxiv-15600-146 | Inference Networks for Sequential Monte Carlo in Graphical Models | http://arxiv.org/pdf/1602.06701v1.pdf | author:Brooks Paige, Frank Wood category:stat.ML published:2016-02-22 summary:We introduce a new approach for amortizing inference in directed graphicalmodels by learning heuristic approximations to stochastic inverses, designedspecifically for use as proposal distributions in sequential Monte Carlomethods. We describe a procedure for constructing and learning a structuredneural network which represents an inverse factorization of the graphicalmodel, resulting in a conditional density estimator that takes as inputparticular values of the observed random variables, and returns anapproximation to the distribution of the latent variables. This recognitionmodel can be learned offline, independent from any particular dataset, prior toperforming inference. The output of these networks can be used asautomatically-learned high-quality proposal distributions to acceleratesequential Monte Carlo across a diverse range of problem settings.
arxiv-15600-147 | Regular expressions for decoding of neural network outputs | http://arxiv.org/pdf/1509.04438v2.pdf | author:Tobias Strauß, Gundram Leifert, Tobias Grüning, Roger Labahn category:cs.NE published:2015-09-15 summary:This article proposes a convenient tool for decoding the output of neuralnetworks trained by Connectionist Temporal Classification (CTC) for handwrittentext recognition. We use regular expressions to describe the complex structuresexpected in the writing. The corresponding finite automata are employed tobuild a decoder. We analyze theoretically which calculations are relevant andwhich can be avoided. A great speed-up results from an approximation. Weconclude that the approximation most likely fails if the regular expressiondoes not match the ground truth which is not harmful for many applicationssince the low probability will be even underestimated. The proposed decoder isvery efficient compared to other decoding methods. The variety of applicationsreaches from information retrieval to full text recognition. We refer toapplications where we integrated the proposed decoder successfully.
arxiv-15600-148 | Correlation Hashing Network for Efficient Cross-Modal Retrieval | http://arxiv.org/pdf/1602.06697v1.pdf | author:Yue Cao, Mingsheng Long, Jianmin Wang category:cs.CV published:2016-02-22 summary:Due to the storage and retrieval efficiency, hashing has been widely deployedto approximate nearest neighbor search for large-scale multimedia retrieval.Cross-modal hashing, which improves the quality of hash coding by exploitingthe semantic correlation across different modalities, has received increasingattention recently. For most existing cross-modal hashing methods, an object isfirst represented as of vector of hand-crafted or machine-learned features,followed by another separate quantization step that generates binary codes.However, suboptimal hash coding may be produced, because the quantization erroris not statistically minimized and the feature representation is not optimallycompatible with the binary coding. In this paper, we propose a novelCorrelation Hashing Network (CHN) architecture for cross-modal hashing, inwhich we jointly learn good data representation tailored to hash coding andformally control the quantization error. The CHN model is a hybrid deeparchitecture constituting four key components: (1) an image network withmultiple convolution-pooling layers to extract good image representations, anda text network with several fully-connected layers to extract good textrepresentations; (2) a fully-connected hashing layer to generatemodality-specific compact hash codes; (3) a squared cosine loss layer forcapturing both cross-modal correlation and within-modal correlation; and (4) anew cosine quantization loss for controlling the quality of the binarized hashcodes. Extensive experiments on standard cross-modal retrieval datasets showthe proposed CHN model yields substantial boosts over latest state-of-the-arthashing methods.
arxiv-15600-149 | Preconditioning Kernel Matrices | http://arxiv.org/pdf/1602.06693v1.pdf | author:Kurt Cutajar, Michael A. Osborne, John P. Cunningham, Maurizio Filippone category:stat.ML stat.CO stat.ME published:2016-02-22 summary:The computational and storage complexity of kernel machines presents theprimary barrier to their scaling to large, modern, datasets. A common way totackle the scalability issue is to use the conjugate gradient algorithm, whichrelieves the constraints on both storage (the kernel matrix need not be stored)and computation (both stochastic gradients and parallelization can be used).Even so, conjugate gradient is not without its own issues: the conditioning ofkernel matrices is often such that conjugate gradients will have poorconvergence in practice. Preconditioning is a common approach to alleviatingthis issue. Here we propose preconditioned conjugate gradients for kernelmachines, and develop a broad range of preconditioners particularly useful forkernel matrices. We describe a scalable approach to both solving kernelmachines and learning their hyperparameters. We show this approach is exact inthe limit of iterations and outperforms state-of-the-art approximations for agiven computational budget.
arxiv-15600-150 | MultiView Diffusion Maps | http://arxiv.org/pdf/1508.05550v4.pdf | author:Ofir Lindenbaum, Arie Yeredor, Moshe Salhov, Amir Averbuch category:cs.LG stat.ML published:2015-08-23 summary:In this study we consider learning a reduced dimensionality representationfrom datasets obtained under multiple views. Such multiple views of datasetscan be obtained, for example, when the same underlying process is observedusing several different modalities, or measured with different instrumentation.Our goal is to effectively exploit the availability of such multiple views forvarious purposes, such as non-linear embedding, manifold learning, spectralclustering, anomaly detection and non-linear system identification. Ourproposed method exploits the intrinsic relation within each view, as well asthe mutual relations between views. We do this by defining a cross-view model,in which an implied Random Walk process between objects is restrained to hopbetween the different views. Our method is robust to scaling of each dataset,and is insensitive to small structural changes in the data. Within thisframework, we define new diffusion distances and analyze the spectra of theimplied kernels. We demonstrate the applicability of the proposed approach onboth artificial and real data sets.
arxiv-15600-151 | An Effective and Efficient Approach for Clusterability Evaluation | http://arxiv.org/pdf/1602.06687v1.pdf | author:Margareta Ackerman, Andreas Adolfsson, Naomi Brownstein category:cs.LG stat.ML published:2016-02-22 summary:Clustering is an essential data mining tool that aims to discover inherentcluster structure in data. As such, the study of clusterability, whichevaluates whether data possesses such structure, is an integral part of clusteranalysis. Yet, despite their central role in the theory and application ofclustering, current notions of clusterability fall short in two crucial aspectsthat render them impractical; most are computationally infeasible and othersfail to classify the structure of real datasets. In this paper, we propose a novel approach to clusterability evaluation thatis both computationally efficient and successfully captures the structure ofreal data. Our method applies multimodality tests to the (one-dimensional) setof pairwise distances based on the original, potentially high-dimensional data.We present extensive analyses of our approach for both the Dip and Silvermanmultimodality tests on real data as well as 17,000 simulations, demonstratingthe success of our approach as the first practical notion of clusterability.
arxiv-15600-152 | Population size predicts lexical diversity, but so does the mean sea level - why it is important to correctly account for the structure of temporal data | http://arxiv.org/pdf/1511.02014v2.pdf | author:Alexander Koplenig, Carolin Mueller-Spitzer category:cs.CL published:2015-11-06 summary:In order to demonstrate why it is important to correctly account for the(serial dependent) structure of temporal data, we document an apparentlyspectacular relationship between population size and lexical diversity: forfive out of seven investigated languages, there is a strong relationshipbetween population size and lexical diversity of the primary language in thiscountry. We show that this relationship is the result of a misspecified modelthat does not consider the temporal aspect of the data by presenting a similarbut nonsensical relationship between the global annual mean sea level andlexical diversity. Given the fact that in the recent past, several studies werepublished that present surprising links between different economic, cultural,political and (socio-)demographical variables on the one hand and cultural orlinguistic characteristics on the other hand, but seem to suffer from exactlythis problem, we explain the cause of the misspecification and show that it hasprofound consequences. We demonstrate how simple transformation of the timeseries can often solve problems of this type and argue that the evaluation ofthe plausibility of a relationship is important in this context. We hope thatour paper will help both researchers and reviewers to understand why it isimportant to use special models for the analysis of data with a naturaltemporal ordering.
arxiv-15600-153 | Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) | http://arxiv.org/pdf/1511.07289v5.pdf | author:Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter category:cs.LG published:2015-11-23 summary:We introduce the "exponential linear unit" (ELU) which speeds up learning indeep neural networks and leads to higher classification accuracies. Likerectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs(PReLUs), ELUs alleviate the vanishing gradient problem via the identity forpositive values. However, ELUs have improved learning characteristics comparedto the units with other activation functions. In contrast to ReLUs, ELUs havenegative values which allows them to push mean unit activations closer to zerolike batch normalization but with lower computational complexity. Mean shiftstoward zero speed up learning by bringing the normal gradient closer to theunit natural gradient because of a reduced bias shift effect. While LReLUs andPReLUs have negative values, too, they do not ensure a noise-robustdeactivation state. ELUs saturate to a negative value with smaller inputs andthereby decrease the forward propagated variation and information. Therefore,ELUs code the degree of presence of particular phenomena in the input, whilethey do not quantitatively model the degree of their absence. In experiments,ELUs lead not only to faster learning, but also to significantly bettergeneralization performance than ReLUs and LReLUs on networks with more than 5layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks withbatch normalization while batch normalization does not improve ELU networks.ELU networks are among the top 10 reported CIFAR-10 results and yield the bestpublished result on CIFAR-100, without resorting to multi-view evaluation ormodel averaging. On ImageNet, ELU networks considerably speed up learningcompared to a ReLU network with the same architecture, obtaining less than 10%classification error for a single crop, single model network.
arxiv-15600-154 | Orthogonal RNNs and Long-Memory Tasks | http://arxiv.org/pdf/1602.06662v1.pdf | author:Mikael Henaff, Arthur Szlam, Yann LeCun category:cs.NE cs.AI cs.LG stat.ML published:2016-02-22 summary:Although RNNs have been shown to be powerful tools for processing sequentialdata, finding architectures or optimization strategies that allow them to modelvery long term dependencies is still an active area of research. In this work,we carefully analyze two synthetic datasets originally outlined in (Hochreiterand Schmidhuber, 1997) which are used to evaluate the ability of RNNs to storeinformation over many time steps. We explicitly construct RNN solutions tothese problems, and using these constructions, illuminate both the problemsthemselves and the way in which RNNs store different types of information intheir hidden states. These constructions furthermore explain the success ofrecent methods that specify unitary initializations or constraints on thetransition matrices.
arxiv-15600-155 | Eye-CU: Sleep Pose Classification for Healthcare using Multimodal Multiview Data | http://arxiv.org/pdf/1602.02343v2.pdf | author:Carlos Torres, Victor Fragoso, Scott D. Hammond, Jeffrey C. Fried, B. S. Manjunath category:cs.CV published:2016-02-07 summary:Manual analysis of body poses of bed-ridden patients requires staff tocontinuously track and record patient poses. Two limitations in thedissemination of pose-related therapies are scarce human resources andunreliable automated systems. This work addresses these issues by introducing anew method and a new system for robust automated classification of sleep posesin an Intensive Care Unit (ICU) environment. The new method,coupled-constrained Least-Squares (cc-LS), uses multimodal and multiview (MM)data and finds the set of modality trust values that minimizes the differencebetween expected and estimated labels. The new system, Eye-CU, is an affordablemulti-sensor modular system for unobtrusive data collection and analysis inhealthcare. Experimental results indicate that the performance of cc-LS matchesthe performance of existing methods in ideal scenarios. This method outperformsthe latest techniques in challenging scenarios by 13% for those with poorillumination and by 70% for those with both poor illumination and occlusions.Results also show that a reduced Eye-CU configuration can classify poseswithout pressure information with only a slight drop in its performance.
arxiv-15600-156 | Structured Learning of Binary Codes with Column Generation | http://arxiv.org/pdf/1602.06654v1.pdf | author:Guosheng Lin, Fayao Liu, Chunhua Shen, Jianxin Wu, Heng Tao Shen category:cs.LG published:2016-02-22 summary:Hashing methods aim to learn a set of hash functions which map the originalfeatures to compact binary codes with similarity preserving in the Hammingspace. Hashing has proven a valuable tool for large-scale informationretrieval. We propose a column generation based binary code learning frameworkfor data-dependent hash function learning. Given a set of triplets that encodethe pairwise similarity comparison information, our column generation basedmethod learns hash functions that preserve the relative comparison relationswithin the large-margin learning framework. Our method iteratively learns thebest hash functions during the column generation procedure. Existing hashingmethods optimize over simple objectives such as the reconstruction error orgraph Laplacian related loss functions, instead of the performance evaluationcriteria of interest---multivariate performance measures such as the AUC andNDCG. Our column generation based method can be further generalized from thetriplet loss to a general structured learning based framework that allows oneto directly optimize multivariate performance measures. For optimizing generalranking measures, the resulting optimization problem can involve exponentiallyor infinitely many variables and constraints, which is more challenging thanstandard structured output learning. We use a combination of column generationand cutting-plane techniques to solve the optimization problem. To speed-up thetraining we further explore stage-wise training and propose to use a simplifiedNDCG loss for efficient inference. We demonstrate the generality of our methodby applying it to ranking prediction and image retrieval, and show that itoutperforms a few state-of-the-art hashing methods.
arxiv-15600-157 | Planogram Compliance Checking Based on Detection of Recurring Patterns | http://arxiv.org/pdf/1602.06647v1.pdf | author:Song Liu, Wanqing Li, Stephen Davis, Christian Ritz, Hongda Tian category:cs.CV published:2016-02-22 summary:In this paper, a novel method for automatic planogram compliance checking inretail chains is proposed without requiring product template images fortraining. Product layout is extracted from an input image by means ofunsupervised recurring pattern detection and matched via graph matching withthe expected product layout specified by a planogram to measure the level ofcompliance. A divide and conquer strategy is employed to improve the speed.Specifically, the input image is divided into several regions based on theplanogram. Recurring patterns are detected in each region respectively and thenmerged together to estimate the product layout. Experimental results on realdata have verified the efficacy of the proposed method. Compared with atemplate-based method, higher accuracies are achieved by the proposed methodover a wide range of products.
arxiv-15600-158 | Creating Simplified 3D Models with High Quality Textures | http://arxiv.org/pdf/1602.06645v1.pdf | author:Song Liu, Wanqing Li, Philip Ogunbona, Yang-Wai Chow category:cs.GR cs.CV published:2016-02-22 summary:This paper presents an extension to the KinectFusion algorithm which allowscreating simplified 3D models with high quality RGB textures. This is achievedthrough (i) creating model textures using images from an HD RGB camera that iscalibrated with Kinect depth camera, (ii) using a modified scheme to updatemodel textures in an asymmetrical colour volume that contains a higher numberof voxels than that of the geometry volume, (iii) simplifying dense polygonmesh model using quadric-based mesh decimation algorithm, and (iv) creating andmapping 2D textures to every polygon in the output 3D model. The proposedmethod is implemented in real-time by means of GPU parallel processing.Visualization via ray casting of both geometry and colour volumes providesusers with a real-time feedback of the currently scanned 3D model. Experimentalresults show that the proposed method is capable of keeping the model texturequality even for a heavily decimated model and that, when reconstructing smallobjects, photorealistic RGB textures can still be reconstructed.
arxiv-15600-159 | Statistical Mechanics of High-Dimensional Inference | http://arxiv.org/pdf/1601.04650v2.pdf | author:Madhu Advani, Surya Ganguli category:stat.ML math.ST q-bio.QM stat.TH published:2016-01-18 summary:To model modern large-scale datasets, we need efficient algorithms to infer aset of $P$ unknown model parameters from $N$ noisy measurements. What arefundamental limits on the accuracy of parameter inference, given finitesignal-to-noise ratios, limited measurements, prior information, andcomputational tractability requirements? How can we combine prior informationwith measurements to achieve these limits? Classical statistics gives incisiveanswers to these questions as the measurement density $\alpha =\frac{N}{P}\rightarrow \infty$. However, these classical results are notrelevant to modern high-dimensional inference problems, which instead occur atfinite $\alpha$. We formulate and analyze high-dimensional inference as aproblem in the statistical physics of quenched disorder. Our analysis uncoversfundamental limits on the accuracy of inference in high dimensions, and revealsthat widely cherished inference algorithms like maximum likelihood (ML) andmaximum-a posteriori (MAP) inference cannot achieve these limits. We furtherfind optimal, computationally tractable algorithms that can achieve theselimits. Intriguingly, in high dimensions, these optimal algorithms becomecomputationally simpler than MAP and ML, while still outperforming them. Forexample, such optimal algorithms can lead to as much as a 20% reduction in theamount of data to achieve the same performance relative to MAP. Moreover, ouranalysis reveals simple relations between optimal high dimensional inferenceand low dimensional scalar Bayesian inference, insights into the nature ofgeneralization and predictive power in high dimensions, information theoreticlimits on compressed sensing, phase transitions in quadratic inference, andconnections to central mathematical objects in convex optimization theory andrandom matrix theory.
arxiv-15600-160 | Handling Class Imbalance in Link Prediction using Learning to Rank Techniques | http://arxiv.org/pdf/1511.04383v2.pdf | author:Bopeng Li, Sougata Chaudhuri, Ambuj Tewari category:stat.ML cs.LG cs.SI published:2015-11-13 summary:We consider the link prediction problem in a partially observed network,where the objective is to make predictions in the unobserved portion of thenetwork. Many existing methods reduce link prediction to binary classificationproblem. However, the dominance of absent links in real world networks makesmisclassification error a poor performance metric. Instead, researchers haveargued for using ranking performance measures, like AUC, AP and NDCG, forevaluation. Our main contribution is to recast the link prediction problem as alearning to rank problem and use effective learning to rank techniques directlyduring training. This is in contrast to existing work that uses rankingmeasures only during evaluation. Our approach is able to deal with the classimbalance problem by using effective, scalable learning to rank techniquesduring training. Furthermore, our approach allows us to combine networktopology and node features. As a demonstration of our general approach, wedevelop a link prediction method by optimizing the cross-entropy surrogate,originally used in the popular ListNet ranking algorithm. We conduct extensiveexperiments on publicly available co-authorship, citation and metabolicnetworks to demonstrate the merits of our method.
arxiv-15600-161 | On the geometry of output-code multi-class learning | http://arxiv.org/pdf/1511.03225v2.pdf | author:Maria Florina Balcan, Travis Dick, Yishay Mansour category:cs.LG published:2015-11-10 summary:We provide a new perspective on the popular multi-class algorithmictechniques one-vs-all and (error correcting) output-codes. We show that is thatin cases where they are successful (at learning from labeled data), thesetechniques implicitly assume structure on how the classes are related. We showthat by making that structure explicit, we can design algorithms to recover theclasses based on limited labeled data. We provide results for commonly studiedcases where the codewords of the classes are well separated: learning a linearone-vs-all classifier for data on the unit ball and learning a linear errorcorrecting output code when the Hamming distance between the codewords is large(at least $d+1$ in a $d$-dimensional problem). We additionally consider themore challenging case where the codewords are not well separated, but satisfy aboundary features condition.
arxiv-15600-162 | Regret Analysis of the Finite-Horizon Gittins Index Strategy for Multi-Armed Bandits | http://arxiv.org/pdf/1511.06014v2.pdf | author:Tor Lattimore category:cs.LG math.ST stat.ML stat.TH published:2015-11-18 summary:I analyse the frequentist regret of the famous Gittins index strategy formulti-armed bandits with Gaussian noise and a finite horizon. Remarkably itturns out that this approach leads to finite-time regret guarantees comparableto those available for the popular UCB algorithm. Along the way I derivefinite-time bounds on the Gittins index that are asymptotically exact and maybe of independent interest. I also discuss some computational issues andpresent experimental results suggesting that a particular version of theGittins index strategy is a modest improvement on existing algorithms withfinite-time regret guarantees such as UCB and Thompson sampling.
arxiv-15600-163 | Document Context Language Models | http://arxiv.org/pdf/1511.03962v4.pdf | author:Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, Jacob Eisenstein category:cs.CL cs.LG stat.ML published:2015-11-12 summary:Text documents are structured on multiple levels of detail: individual wordsare related by syntax, but larger units of text are related by discoursestructure. Existing language models generally fail to account for discoursestructure, but it is crucial if we are to have language models that rewardcoherence and generate coherent texts. We present and empirically evaluate aset of multi-level recurrent neural network language models, calledDocument-Context Language Models (DCLM), which incorporate contextualinformation both within and beyond the sentence. In comparison with word-levelrecurrent neural network language models, the DCLM models obtain slightlybetter predictive likelihoods, and considerably better assessments of documentcoherence.
arxiv-15600-164 | 2-Bit Random Projections, NonLinear Estimators, and Approximate Near Neighbor Search | http://arxiv.org/pdf/1602.06577v1.pdf | author:Ping Li, Michael Mitzenmacher, Anshumali Shrivastava category:stat.ML cs.DS cs.LG published:2016-02-21 summary:The method of random projections has become a standard tool for machinelearning, data mining, and search with massive data at Web scale. The effectiveuse of random projections requires efficient coding schemes for quantizing(real-valued) projected data into integers. In this paper, we focus on a simple2-bit coding scheme. In particular, we develop accurate nonlinear estimators ofdata similarity based on the 2-bit strategy. This work will have importantpractical applications. For example, in the task of near neighbor search, acrucial step (often called re-ranking) is to compute or estimate datasimilarities once a set of candidate data points have been identified by hashtable techniques. This re-ranking step can take advantage of the proposedcoding scheme and estimator. As a related task, in this paper, we also study a simple uniform quantizationscheme for the purpose of building hash tables with projected data. Ouranalysis shows that typically only a small number of bits are needed. Forexample, when the target similarity level is high, 2 or 3 bits might besufficient. When the target similarity level is not so high, it is preferableto use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a goodchoice for the task of sublinear time approximate near neighbor search via hashtables. Combining these results, we conclude that 2-bit random projections should berecommended for approximate near neighbor search and similarity estimation.Extensive experimental results are provided.
arxiv-15600-165 | Interactive Storytelling over Document Collections | http://arxiv.org/pdf/1602.06566v1.pdf | author:Dipayan Maiti, Mohammad Raihanul Islam, Scotland Leman, Naren Ramakrishnan category:cs.AI cs.LG stat.ML published:2016-02-21 summary:Storytelling algorithms aim to 'connect the dots' between disparate documentsby linking starting and ending documents through a series of intermediatedocuments. Existing storytelling algorithms are based on notions of coherenceand connectivity, and thus the primary way by which users can steer the storyconstruction is via design of suitable similarity functions. We present analternative approach to storytelling wherein the user can interactively anditeratively provide 'must use' constraints to preferentially support theconstruction of some stories over others. The three innovations in our approachare distance measures based on (inferred) topic distributions, the use ofconstraints to define sets of linear inequalities over paths, and theintroduction of slack and surplus variables to condition the topic distributionto preferentially emphasize desired terms over others. We describe experimentalresults to illustrate the effectiveness of our interactive storytellingapproach over multiple text datasets.
arxiv-15600-166 | Automatic Building Extraction in Aerial Scenes Using Convolutional Networks | http://arxiv.org/pdf/1602.06564v1.pdf | author:Jiangye Yuan category:cs.CV published:2016-02-21 summary:Automatic building extraction from aerial and satellite imagery is highlychallenging due to extremely large variations of building appearances. Toattack this problem, we design a convolutional network with a final stage thatintegrates activations from multiple preceding stages for pixel-wiseprediction, and introduce the signed distance function of building boundariesas the output representation, which has an enhanced representation power. Weleverage abundant building footprint data available from geographic informationsystems (GIS) to compile training data. The trained network achieves superiorperformance on datasets that are significantly larger and more complex thanthose used in prior work, demonstrating that the proposed method provides apromising and scalable solution for automating this labor-intensive task.
arxiv-15600-167 | Non-linear Causal Inference using Gaussianity Measures | http://arxiv.org/pdf/1409.4573v3.pdf | author:Daniel Hernández-Lobato, Pablo Morales-Mombiela, David Lopez-Paz, Alberto Suárez category:stat.ML published:2014-09-16 summary:We provide theoretical and empirical evidence for a type of asymmetry betweencauses and effects that is present when these are related via linear modelscontaminated with additive non-Gaussian noise. Assuming that the causes and theeffects have the same distribution, we show that the distribution of theresiduals of a linear fit in the anti-causal direction is closer to a Gaussianthan the distribution of the residuals in the causal direction. ThisGaussianization effect is characterized by reduction of the magnitude of thehigh-order cumulants and by an increment of the differential entropy of theresiduals. The problem of non-linear causal inference is addressed byperforming an embedding in an expanded feature space, in which the relationbetween causes and effects can be assumed to be linear. The effectiveness of amethod to discriminate between causes and effects based on this type ofasymmetry is illustrated in a variety of experiments using different measuresof Gaussianity. The proposed method is shown to be competitive withstate-of-the-art techniques for causal inference.
arxiv-15600-168 | Predictive Entropy Search for Multi-objective Bayesian Optimization | http://arxiv.org/pdf/1511.05467v3.pdf | author:Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Amar Shah, Ryan P. Adams category:stat.ML published:2015-11-17 summary:We present PESMO, a Bayesian method for identifying the Pareto set ofmulti-objective optimization problems, when the functions are expensive toevaluate. The central idea of PESMO is to choose evaluation points so as tomaximally reduce the entropy of the posterior distribution over the Pareto set.Critically, the PESMO multi-objective acquisition function can be decomposed asa sum of objective-specific acquisition functions, which enables the algorithmto be used in \emph{decoupled} scenarios in which the objectives can beevaluated separately and perhaps with different costs. This decouplingcapability also makes it possible to identify difficult objectives that requiremore evaluations. PESMO also offers gains in efficiency, as its cost scaleslinearly with the number of objectives, in comparison to the exponential costof other methods. We compare PESMO with other related methods formulti-objective Bayesian optimization on synthetic and real-world problems. Theresults show that PESMO produces better recommendations with a smaller numberof evaluations of the objectives, and that a decoupled evaluation can lead toimprovements in performance, particularly when the number of objectives islarge.
arxiv-15600-169 | Determining the best attributes for surveillance video keywords generation | http://arxiv.org/pdf/1602.06539v1.pdf | author:Liangchen Liu, Arnold Wiliem, Shaokang Chen, Kun Zhao, Brian C. Lovell category:cs.LG cs.AI published:2016-02-21 summary:Automatic video keyword generation is one of the key ingredients in reducingthe burden of security officers in analyzing surveillance videos. Keywords orattributes are generally chosen manually based on expert knowledge ofsurveillance. Most existing works primarily aim at either supervised learningapproaches relying on extensive manual labelling or hierarchical probabilisticmodels that assume the features are extracted using the bag-of-words approach;thus limiting the utilization of the other features. To address this, we turnour attention to automatic attribute discovery approaches. However, it is notclear which automatic discovery approach can discover the most meaningfulattributes. Furthermore, little research has been done on how to compare andchoose the best automatic attribute discovery methods. In this paper, wepropose a novel approach, based on the shared structure exhibited amongstmeaningful attributes, that enables us to compare between different automaticattribute discovery approaches.We then validate our approach by comparingvarious attribute discovery methods such as PiCoDeS on two attribute datasets.The evaluation shows that our approach is able to select the automaticdiscovery approach that discovers the most meaningful attributes. We thenemploy the best discovery approach to generate keywords for videos recordedfrom a surveillance system. This work shows it is possible to massively reducethe amount of manual work in generating video keywords without limitingourselves to a particular video feature descriptor.
arxiv-15600-170 | Multi-task and Lifelong Learning of Kernels | http://arxiv.org/pdf/1602.06531v1.pdf | author:Anastasia Pentina, Shai Ben-David category:stat.ML cs.LG published:2016-02-21 summary:We consider a problem of learning kernels for use in SVM classification inthe multi-task and lifelong scenarios and provide generalization bounds on theerror of a large margin classifier. Our results show that, under mildconditions on the family of kernels used for learning, solving several relatedtasks simultaneously is beneficial over single task learning. In particular, asthe number of observed tasks grows, assuming that in the considered family ofkernels there exists one that yields low approximation error on all tasks, theoverhead associated with learning such a kernel vanishes and the complexityconverges to that of learning when this good kernel is given to the learner.
arxiv-15600-171 | Machine learning meets network science: dimensionality reduction for fast and efficient embedding of networks in the hyperbolic space | http://arxiv.org/pdf/1602.06522v1.pdf | author:Josephine Maria Thomas, Alessandro Muscoloni, Sara Ciucci, Ginestra Bianconi, Carlo Vittorio Cannistraci category:cs.AI cs.LG published:2016-02-21 summary:Complex network topologies and hyperbolic geometry seem specularly connected,and one of the most fascinating and challenging problems of recent complexnetwork theory is to map a given network to its hyperbolic space. ThePopularity Similarity Optimization (PSO) model represents - at the moment - theclimax of this theory. It suggests that the trade-off between node popularityand similarity is a mechanism to explain how complex network topologies emerge- as discrete samples - from the continuous world of hyperbolic geometry. Thehyperbolic space seems appropriate to represent real complex networks. In fact,it preserves many of their fundamental topological properties, and can beexploited for real applications such as, among others, link prediction andcommunity detection. Here, we observe for the first time that atopological-based machine learning class of algorithms - for nonlinearunsupervised dimensionality reduction - can directly approximate the network'snode angular coordinates of the hyperbolic model into a two-dimensional space,according to a similar topological organization that we named angularcoalescence. On the basis of this phenomenon, we propose a new class ofalgorithms that offers fast and accurate coalescent embedding of networks inthe hyperbolic space even for graphs with thousands of nodes.
arxiv-15600-172 | Learning, Visualizing, and Exploiting a Model for the Intrinsic Value of a Batted Ball | http://arxiv.org/pdf/1603.00050v1.pdf | author:Glenn Healey category:stat.AP cs.LG I.2.6 published:2016-02-21 summary:We present an algorithm for learning the intrinsic value of a batted ball inbaseball. This work addresses the fundamental problem of separating the valueof a batted ball at contact from factors such as the defense, weather, andballpark that can affect its observed outcome. The algorithm uses a Bayesianmodel to construct a continuous mapping from a vector of batted ball parametersto an intrinsic measure defined as the expected value of a linear weightsrepresentation for run value. A kernel method is used to build nonparametricestimates for the component probability density functions in Bayes theorem froma set of over one hundred thousand batted ball measurements recorded by theHITf/x system during the 2014 major league baseball (MLB) season.Cross-validation is used to determine the optimal vector of smoothingparameters for the density estimates. Properties of the mapping are visualizedby considering reduced-dimension subsets of the batted ball parameter space. Weuse the mapping to derive statistics for intrinsic quality of contact forbatters and pitchers which have the potential to improve the accuracy of playermodels and forecasting systems. We also show that the new approach leads to asimple automated measure of contact-adjusted defense and provides insight intothe impact of environmental variables on batted balls.
arxiv-15600-173 | Efficient functional ANOVA through wavelet-domain Markov groves | http://arxiv.org/pdf/1602.03990v2.pdf | author:Li Ma, Jacopo Soriano category:stat.ME stat.CO stat.ML published:2016-02-12 summary:We introduce a wavelet-domain functional analysis of variance (fANOVA) methodbased on a Bayesian hierarchical model. The factor effects are modeled througha spike-and-slab mixture at each location-scale combination along with anormal-inverse-Gamma (NIG) conjugate setup for the coefficients and errors. Agraphical model called the Markov grove (MG) is designed to jointly model thespike-and-slab statuses at all location-scale combinations, which incorporatesthe clustering of each factor effect in the wavelet-domain thereby allowingborrowing of strength across location and scale. The posterior of this NIG-MGmodel is analytically available through a pyramid algorithm of the samecomputational complexity as Mallat's pyramid algorithm for discrete wavelettransform, i.e., linear in both the number of observations and the number oflocations. Posterior probabilities of factor contributions can also be computedthrough pyramid recursion, and exact samples from the posterior can be drawnwithout MCMC. We investigate the performance of our method through extensivesimulation and show that it outperforms existing wavelet-domain fANOVA methodsin a variety of common settings. We apply the method to analyzing the orthosisdata.
arxiv-15600-174 | Distributed Private Online Learning for Social Big Data Computing over Data Center Networks | http://arxiv.org/pdf/1602.06489v1.pdf | author:Chencheng Li, Pan Zhou, Yingxue Zhou, Kaigui Bian, Tao Jiang, Susanto Rahardja category:cs.DC cs.LG cs.SI published:2016-02-21 summary:With the rapid growth of Internet technologies, cloud computing and socialnetworks have become ubiquitous. An increasing number of people participate insocial networks and massive online social data are obtained. In order toexploit knowledge from copious amounts of data obtained and predict socialbehavior of users, we urge to realize data mining in social networks. Almostall online websites use cloud services to effectively process the large scaleof social data, which are gathered from distributed data centers. These dataare so large-scale, high-dimension and widely distributed that we propose adistributed sparse online algorithm to handle them. Additionally,privacy-protection is an important point in social networks. We should notcompromise the privacy of individuals in networks, while these social data arebeing learned for data mining. Thus we also consider the privacy problem inthis article. Our simulations shows that the appropriate sparsity of data wouldenhance the performance of our algorithm and the privacy-preserving method doesnot significantly hurt the performance of the proposed algorithm.
arxiv-15600-175 | Bio-Inspired Human Action Recognition using Hybrid Max-Product Neuro-Fuzzy Classifier and Quantum-Behaved PSO | http://arxiv.org/pdf/1509.03789v2.pdf | author:Bardia Yousefi, Chu Kiong Loo category:cs.AI cs.CV published:2015-09-13 summary:Studies on computational neuroscience through functional magnetic resonanceimaging (fMRI) and following biological inspired system stated that humanaction recognition in the brain of mammalian leads two distinct pathways in themodel, which are specialized for analysis of motion (optic flow) and forminformation. Principally, we have defined a novel and robust form featuresapplying active basis model as form extractor in form pathway in the biologicalinspired model. An unbalanced synergetic neural net-work classifies shapes andstructures of human objects along with tuning its attention parameter byquantum particle swarm optimization (QPSO) via initiation of Centroidal VoronoiTessellations. These tools utilized and justified as strong tools for followingbiological system model in form pathway. But the final decision has done bycombination of ultimate outcomes of both pathways via fuzzy inference whichincreases novality of proposed model. Combination of these two brain pathwaysis done by considering each feature sets in Gaussian membership functions withfuzzy product inference method. Two configurations have been proposed for formpathway: applying multi-prototype human action templates using two timesynergetic neural network for obtaining uniform template regarding eachactions, and second scenario that it uses abstracting human action in fourkey-frames. Experimental results showed promising accuracy performance ondifferent datasets (KTH and Weizmann).
arxiv-15600-176 | FLASH: Fast Bayesian Optimization for Data Analytic Pipelines | http://arxiv.org/pdf/1602.06468v1.pdf | author:Yuyu Zhang, Mohammad Taha Bahadori, Hang Su, Jimeng Sun category:cs.LG published:2016-02-20 summary:Modern data science relies on data analytic pipelines to organizeinterdependent computational steps. Such analytic pipelines often involvedifferent algorithms across multiple steps, each with its own hyperparameters.To get the best performance, it is often critical to select optimal algorithmsand set appropriate hyperparameters, which requires large computationalefforts. Bayesian optimization provides a principled way for searching optimalhyperparameters for a single algorithm. However, many challenges remain insolving pipeline optimization problems with high-dimensional and highlyconditional search space. In this work, we propose Fast LineAr SearcH (FLASH),an efficient method for tuning analytic pipelines. FLASH is a two-layerBayesian optimization framework, which firstly uses a parametric model toselect promising algorithms, then computes a nonparametric model to fine-tunehyperparameters of the promising algorithms. FLASH also includes an effectivecaching algorithm which can further accelerate the search process. Extensiveexperiments on a number of benchmark datasets have demonstrated that FLASHsignificantly outperforms previous state-of-the-art methods in both searchspeed and accuracy. Using 50% of the time budget, FLASH achieves up to 20%improvement on test error rate compared to the baselines. Our method alsoyields state-of-the-art performance on a real-world application for healthcarepredictive modeling.
arxiv-15600-177 | A Dataset for Improved RGBD-based Object Detection and Pose Estimation for Warehouse Pick-and-Place | http://arxiv.org/pdf/1509.01277v2.pdf | author:Colin Rennie, Rahul Shome, Kostas E. Bekris, Alberto F. De Souza category:cs.CV cs.RO published:2015-09-03 summary:An important logistics application of robotics involves manipulators thatpick-and-place objects placed in warehouse shelves. A critical aspect of thistask corre- sponds to detecting the pose of a known object in the shelf usingvisual data. Solving this problem can be assisted by the use of an RGB-Dsensor, which also provides depth information beyond visual data. Nevertheless,it remains a challenging problem since multiple issues need to be addressed,such as low illumination inside shelves, clutter, texture-less and reflectiveobjects as well as the limitations of depth sensors. This paper provides a newrich data set for advancing the state-of-the-art in RGBD- based 3D object poseestimation, which is focused on the challenges that arise when solvingwarehouse pick- and-place tasks. The publicly available data set includesthousands of images and corresponding ground truth data for the objects usedduring the first Amazon Picking Challenge at different poses and clutterconditions. Each image is accompanied with ground truth information to assistin the evaluation of algorithms for object detection. To show the utility ofthe data set, a recent algorithm for RGBD-based pose estimation is evaluated inthis paper. Based on the measured performance of the algorithm on the data set,various modifications and improvements are applied to increase the accuracy ofdetection. These steps can be easily applied to a variety of differentmethodologies for object pose detection and improve performance in the domainof warehouse pick-and-place.
arxiv-15600-178 | Context-guided diffusion for label propagation on graphs | http://arxiv.org/pdf/1602.06439v1.pdf | author:Kwang In Kim, James Tompkin, Hanspeter Pfister, Christian Theobalt category:cs.CV published:2016-02-20 summary:Existing approaches for diffusion on graphs, e.g., for label propagation, aremainly focused on isotropic diffusion, which is induced by the commonly-usedgraph Laplacian regularizer. Inspired by the success of diffusivity tensors foranisotropic diffusion in image processing, we presents anisotropic diffusion ongraphs and the corresponding label propagation algorithm. We develop positivedefinite diffusivity operators on the vector bundles of Riemannian manifolds,and discretize them to diffusivity operators on graphs. This enables us toeasily define new robust diffusivity operators which significantly improvesemi-supervised learning performance over existing diffusion algorithms.
arxiv-15600-179 | Burstiness Scale: a highly parsimonious model for characterizing random series of events | http://arxiv.org/pdf/1602.06431v1.pdf | author:Rodrigo A S Alves, Renato Assunção, Pedro O S Vaz de Melo category:stat.ML cs.SI H.2.8; G.3 published:2016-02-20 summary:The problem to accurately and parsimoniously characterize random series ofevents (RSEs) present in the Web, such as e-mail conversations or Twitterhashtags, is not trivial. Reports found in the literature reveal two apparentconflicting visions of how RSEs should be modeled. From one side, thePoissonian processes, of which consecutive events follow each other at arelatively regular time and should not be correlated. On the other side, theself-exciting processes, which are able to generate bursts of correlated eventsand periods of inactivities. The existence of many and sometimes conflictingapproaches to model RSEs is a consequence of the unpredictability of theaggregated dynamics of our individual and routine activities, which sometimesshow simple patterns, but sometimes results in irregular rising and fallingtrends. In this paper we propose a highly parsimonious way to characterizegeneral RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSEas a mix of two independent process: a Poissonian and a self-exciting one. Herewe describe a fast method to extract the two parameters of BuSca that,together, gives the burstyness scale, which represents how much of the RSE isdue to bursty and viral effects. We validated our method in eight diverse andlarge datasets containing real random series of events seen in Twitter, Yelp,e-mail conversations, Digg, and online forums. Results showed that, even usingonly two parameters, BuSca is able to accurately describe RSEs seen in thesediverse systems, what can leverage many applications.
arxiv-15600-180 | Semidefinite Programs for Exact Recovery of a Hidden Community | http://arxiv.org/pdf/1602.06410v1.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.IT cs.SI math.IT math.ST stat.TH published:2016-02-20 summary:We study a semidefinite programming (SDP) relaxation of the maximumlikelihood estimation for exactly recovering a hidden community of cardinality$K$ from an $n \times n$ symmetric data matrix $A$, where for distinct indices$i,j$, $A_{ij} \sim P$ if $i, j$ are both in the community and $A_{ij} \sim Q$otherwise, for two known probability distributions $P$ and $Q$. We identify asufficient condition and a necessary condition for the success of SDP for thegeneral model. For both the Bernoulli case ($P={\rm Bern}(p)$ and $Q={\rmBern}(q)$ with $p>q$) and the Gaussian case ($P=\mathcal{N}(\mu,1)$ and$Q=\mathcal{N}(0,1)$ with $\mu>0$), which correspond to the problem of planteddense subgraph recovery and submatrix localization respectively, the generalresults lead to the following findings: (1) If $K=\omega( n /\log n)$, SDPattains the information-theoretic recovery limits with sharp constants; (2) If$K=\Theta(n/\log n)$, SDP is order-wise optimal, but strictly suboptimal by aconstant factor; (3) If $K=o(n/\log n)$ and $K \to \infty$, SDP is order-wisesuboptimal. A key ingredient in the proof of the necessary condition is aconstruction of a primal feasible solution based on random perturbation of thetrue cluster matrix.
arxiv-15600-181 | Stratified Bayesian Optimization | http://arxiv.org/pdf/1602.02338v2.pdf | author:Saul Toscano-Palmerin, Peter I. Frazier category:cs.LG math.OC stat.ML published:2016-02-07 summary:We consider derivative-free black-box global optimization of expensive noisyfunctions, when most of the randomness in the objective is produced by a fewinfluential scalar random inputs. We present a new Bayesian global optimizationalgorithm, called Stratified Bayesian Optimization (SBO), which uses thisstrong dependence to improve performance. Our algorithm is similar in spirit tostratification, a technique from simulation, which uses strong dependence on acategorical representation of the random input to reduce variance. Wedemonstrate in numerical experiments that SBO outperforms state-of-the-artBayesian optimization benchmarks that do not leverage this dependence.
arxiv-15600-182 | Study of all the periods of a Neuronal Recurrence Equation | http://arxiv.org/pdf/1503.06866v4.pdf | author:Serge Alain Ebélé, Renè Ndoundam category:cs.NE 92B20 F.1.1 published:2015-03-23 summary:We characterize the structure of the periods of a neuronal recurrenceequation. Firstly, we give a characterization of k-chains in 0-1 periodicsequences. Secondly, we characterize the periods of all cycles of some neuronalrecurrence equation. Thirdly, we explain how these results can be used todeduce the existence of the generalized period-halving bifurcation.
arxiv-15600-183 | Text Matching as Image Recognition | http://arxiv.org/pdf/1602.06359v1.pdf | author:Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, Xueqi Cheng category:cs.CL cs.AI published:2016-02-20 summary:Matching two texts is a fundamental problem in many natural languageprocessing tasks. An effective way is to extract meaningful matching patternsfrom words, phrases, and sentences to produce the matching score. Inspired bythe success of convolutional neural network in image recognition, where neuronscan capture many complicated patterns based on the extracted elementary visualpatterns such as oriented edges and corners, we propose to model text matchingas the problem of image recognition. Firstly, a matching matrix whose entriesrepresent the similarities between words is constructed and viewed as an image.Then a convolutional neural network is utilized to capture rich matchingpatterns in a layer-by-layer way. We show that by resembling the compositionalhierarchies of patterns in image recognition, our model can successfullyidentify salient signals such as n-gram and n-term matchings. Experimentalresults demonstrate its superiority against the baselines.
arxiv-15600-184 | The Segmented iHMM: A Simple, Efficient Hierarchical Infinite HMM | http://arxiv.org/pdf/1602.06349v1.pdf | author:Ardavan Saeedi, Matthew Hoffman, Matthew Johnson, Ryan Adams category:stat.ML published:2016-02-20 summary:We propose the segmented iHMM (siHMM), a hierarchical infinite hidden Markovmodel (iHMM) that supports a simple, efficient inference scheme. The siHMM iswell suited to segmentation problems, where the goal is to identify points atwhich a time series transitions from one relatively stable regime to a newregime. Conventional iHMMs often struggle with such problems, since they haveno mechanism for distinguishing between high- and low-level dynamics.Hierarchical HMMs (HHMMs) can do better, but they require much more complex andexpensive inference algorithms. The siHMM retains the simplicity and efficiencyof the iHMM, but outperforms it on a variety of segmentation problems,achieving performance that matches or exceeds that of a more complicated HHMM.
arxiv-15600-185 | Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models | http://arxiv.org/pdf/1602.06346v1.pdf | author:Bernardo Ávila Pires, Csaba Szepesvári category:stat.ML cs.LG published:2016-02-19 summary:In this paper we study a model-based approach to calculating approximatelyoptimal policies in Markovian Decision Processes. In particular, we derivenovel bounds on the loss of using a policy derived from a factored linearmodel, a class of models which generalize virtually all previous models thatcome with strong computational guarantees. For the first time in theliterature, we derive performance bounds for model-based techniques where themodel inaccuracy is measured in weighted norms. Moreover, our bounds show adecreased sensitivity to the discount factor and, unlike similar bounds derivedfor other approaches, they are insensitive to measure mismatch. Similarly toprevious works, our proofs are also based on contraction arguments, but withthe main differences that we use carefully constructed norms building on Banachlattices, and the contraction property is only assumed for operators acting on"compressed" spaces, thus weakening previous assumptions, while strengtheningprevious results.
arxiv-15600-186 | Theoretical guarantees for approximate sampling from smooth and log-concave densities | http://arxiv.org/pdf/1412.7392v5.pdf | author:Arnak S. Dalalyan category:stat.CO math.ST stat.ML stat.TH published:2014-12-23 summary:Sampling from various kinds of distributions is an issue of paramountimportance in statistics since it is often the key ingredient for constructingestimators, test procedures or confidence intervals. In many situations, theexact sampling from a given distribution is impossible or computationallyexpensive and, therefore, one needs to resort to approximate samplingstrategies. However, there is no well-developed theory providing meaningfulnonasymptotic guarantees for the approximate sampling procedures, especially inthe high-dimensional problems. This paper makes some progress in this directionby considering the problem of sampling from a distribution having a smooth andlog-concave density defined on $\mathbb R^p$, for some integer $p>0$. Weestablish nonasymptotic bounds for the error of approximating the truedistribution by the one obtained by the Langevin Monte Carlo method and itsvariants. We illustrate the effectiveness of the established guarantees withvarious experiments. Underlying our analysis are insights from the theory ofcontinuous-time diffusion processes, which may be of interest beyond theframework of distributions with log-concave densities considered in the presentwork.
arxiv-15600-187 | Learning Laplacian Matrix in Smooth Graph Signal Representations | http://arxiv.org/pdf/1406.7842v3.pdf | author:Xiaowen Dong, Dorina Thanou, Pascal Frossard, Pierre Vandergheynst category:cs.LG cs.SI stat.ML published:2014-06-30 summary:The construction of a meaningful graph plays a crucial role in the success ofmany graph-based representations and algorithms for handling structured data,especially in the emerging field of graph signal processing. However, ameaningful graph is not always readily available from the data, nor easy todefine depending on the application domain. In particular, it is oftendesirable in graph signal processing applications that a graph is chosen suchthat the data admit certain regularity or smoothness on the graph. In thispaper, we address the problem of learning graph Laplacians, which is equivalentto learning graph topologies, such that the input data form graph signals withsmooth variations on the resulting topology. To this end, we adopt a factoranalysis model for the graph signals and impose a Gaussian probabilistic prioron the latent variables that control these signals. We show that the Gaussianprior leads to an efficient representation that favors the smoothness propertyof the graph signals. We then propose an algorithm for learning graphs thatenforces such property and is based on minimizing the variations of the signalson the learned graph. Experiments on both synthetic and real world datademonstrate that the proposed graph learning framework can efficiently infermeaningful graph topologies from signal observations under the smoothnessprior.
arxiv-15600-188 | Contextual LSTM (CLSTM) models for Large scale NLP tasks | http://arxiv.org/pdf/1602.06291v1.pdf | author:Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, Larry Heck category:cs.CL published:2016-02-19 summary:Documents exhibit sequential structure at multiple levels of abstraction(e.g., sentences, paragraphs, sections). These abstractions constitute anatural hierarchy for representing the context in which to infer the meaning ofwords and larger fragments of text. In this paper, we present CLSTM (ContextualLSTM), an extension of the recurrent neural network LSTM (Long-Short TermMemory) model, where we incorporate contextual features (e.g., topics) into themodel. We evaluate CLSTM on three specific NLP tasks: word prediction, nextsentence selection, and sentence topic prediction. Results from experiments runon two corpora, English documents in Wikipedia and a subset of articles from arecent snapshot of English Google News, indicate that using both words andtopics as features improves performance of the CLSTM models over baseline LSTMmodels for these tasks. For example on the next sentence selection task, we getrelative accuracy improvements of 21% for the Wikipedia dataset and 18% for theGoogle News dataset. This clearly demonstrates the significant benefit of usingcontext appropriately in natural language (NL) tasks. This has implications fora wide variety of NL applications like question answering, sentence completion,paraphrase generation, and next utterance prediction in dialog systems.
arxiv-15600-189 | Learning to SMILE(S) | http://arxiv.org/pdf/1602.06289v1.pdf | author:Stanisław Jastrzębski, Damian Leśniak, Wojciech Marian Czarnecki category:cs.CL published:2016-02-19 summary:This paper shows how one can directly apply natural language processing (NLP)methods to classification problems in cheminformatics. Connection between theseseemingly separate fields is shown by considering standard textualrepresentation of compound, SMILES. The problem of activity prediction againsta target protein is considered, which is a crucial part of computer aided drugdesign process. Conducted experiments show that this way one can not onlyoutrank state of the art results of hand crafted representations but also getsdirect structural insights into the way decisions are made.
arxiv-15600-190 | Semi-parametric Order-based Generalized Multivariate Regression | http://arxiv.org/pdf/1602.06276v1.pdf | author:Milad Kharratzadeh, Mark Coates category:stat.ML math.ST stat.TH published:2016-02-19 summary:In this paper, we consider a generalized multivariate regression problemwhere the responses are monotonic functions of linear transformations ofpredictors. We propose a semi-parametric algorithm based on the ordering of theresponses which is invariant to the functional form of the transformationfunction. We prove that our algorithm, which maximizes the rank correlation ofresponses and linear transformations of predictors, is a consistent estimatorof the true coefficient matrix. We also identify the rate of convergence andshow that the squared estimation error decays with a rate of $o(1/\sqrt{n})$.We then propose a greedy algorithm to maximize the highly non-smooth objectivefunction of our model and examine its performance through extensivesimulations. Finally, we compare our algorithm with traditional multivariateregression algorithms over synthetic and real data.
arxiv-15600-191 | A Mutual Contamination Analysis of Mixed Membership and Partial Label Models | http://arxiv.org/pdf/1602.06235v1.pdf | author:Julian Katz-Samuels, Clayton Scott category:stat.ML published:2016-02-19 summary:Many machine learning problems can be characterized by mutual contaminationmodels. In these problems, one observes several random samples from differentconvex combinations of a set of unknown base distributions. It is of interestto decontaminate mutual contamination models, i.e., to recover the basedistributions either exactly or up to a permutation. This paper considers thegeneral setting where the base distributions are defined on arbitraryprobability spaces. We examine the decontamination problem in two mutualcontamination models that describe popular machine learning tasks: recoveringthe base distributions up to a permutation in a mixed membership model, andrecovering the base distributions exactly in a partial label model forclassification. We give necessary and sufficient conditions for identifiabilityof both mutual contamination models, algorithms for both problems in theinfinite and finite sample cases, and introduce novel proof techniques based onaffine geometry.
arxiv-15600-192 | GAP Safe Screening Rules for Sparse-Group-Lasso | http://arxiv.org/pdf/1602.06225v1.pdf | author:Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon category:stat.ML cs.LG math.OC stat.CO published:2016-02-19 summary:In high dimensional settings, sparse structures are crucial for efficiency,either in term of memory, computation or performance. In some contexts, it isnatural to handle more refined structures than pure sparsity, such as forinstance group sparsity. Sparse-Group Lasso has recently been introduced in thecontext of linear regression to enforce sparsity both at the feature level andat the group level. We adapt to the case of Sparse-Group Lasso recent safescreening rules that discard early in the solver irrelevant features/groups.Such rules have led to important speed-ups for a wide range of iterativemethods. Thanks to dual gap computations, we provide new safe screening rulesfor Sparse-Group Lasso and show significant gains in term of computing time fora coordinate descent implementation.
arxiv-15600-193 | LCSTS: A Large Scale Chinese Short Text Summarization Dataset | http://arxiv.org/pdf/1506.05865v4.pdf | author:Baotian Hu, Qingcai Chen, Fangze Zhu category:cs.CL cs.IR cs.LG published:2015-06-19 summary:Automatic text summarization is widely regarded as the highly difficultproblem, partially because of the lack of large text summarization data set.Due to the great challenge of constructing the large scale summaries for fulltext, in this paper, we introduce a large corpus of Chinese short textsummarization dataset constructed from the Chinese microblogging website SinaWeibo, which is released to the public{http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over2 million real Chinese short texts with short summaries given by the author ofeach text. We also manually tagged the relevance of 10,666 short summaries withtheir corresponding short texts. Based on the corpus, we introduce recurrentneural network for the summary generation and achieve promising results, whichnot only shows the usefulness of the proposed corpus for short textsummarization research, but also provides a baseline for further research onthis topic.
arxiv-15600-194 | 8-Bit Approximations for Parallelism in Deep Learning | http://arxiv.org/pdf/1511.04561v4.pdf | author:Tim Dettmers category:cs.NE cs.LG published:2015-11-14 summary:The creation of practical deep learning data-products often requiresparallelization across processors and computers to make deep learning feasibleon large data sets, but bottlenecks in communication bandwidth make itdifficult to attain good speedups through parallelism. Here we develop and test8-bit approximation algorithms which make better use of the available bandwidthby compressing 32-bit gradients and nonlinear activations to 8-bitapproximations. We show that these approximations do not decrease predictiveperformance on MNIST, CIFAR10, and ImageNet for both model and data parallelismand provide a data transfer speedup of 2x relative to 32-bit parallelism. Webuild a predictive model for speedups based on our experimental data, verifyits validity on known speedup data, and show that we can obtain a speedup of50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. Wecompare our data types with other methods and show that 8-bit approximationsachieve state-of-the-art speedups for model parallelism. Thus 8-bitapproximation is an efficient method to parallelize convolutional networks onvery large systems of GPUs.
arxiv-15600-195 | Node-By-Node Greedy Deep Learning for Interpretable Features | http://arxiv.org/pdf/1602.06183v1.pdf | author:Ke Wu, Malik Magdon-Ismail category:cs.LG published:2016-02-19 summary:Multilayer networks have seen a resurgence under the umbrella of deeplearning. Current deep learning algorithms train the layers of the networksequentially, improving algorithmic performance as well as providing someregularization. We present a new training algorithm for deep networks whichtrains \emph{each node in the network} sequentially. Our algorithm is orders ofmagnitude faster, creates more interpretable internal representations at thenode level, while not sacrificing on the ultimate out-of-sample performance.
arxiv-15600-196 | TribeFlow: Mining & Predicting User Trajectories | http://arxiv.org/pdf/1511.01032v2.pdf | author:Flavio Figueiredo, Bruno Ribeiro, Jussara Almeida, Christos Faloutsos category:cs.SI physics.soc-ph stat.ML published:2015-11-03 summary:Which song will Smith listen to next? Which restaurant will Alice go totomorrow? Which product will John click next? These applications have in commonthe prediction of user trajectories that are in a constant state of flux over ahidden network (e.g. website links, geographic location). What users are doingnow may be unrelated to what they will be doing in an hour from now. Mindful ofthese challenges we propose TribeFlow, a method designed to cope with thecomplex challenges of learning personalized predictive models ofnon-stationary, transient, and time-heterogeneous user trajectories. TribeFlowis a general method that can perform next product recommendation, next songrecommendation, next location prediction, and general arbitrary-length usertrajectory prediction without domain-specific knowledge. TribeFlow is moreaccurate and up to 413x faster than top competitors.
arxiv-15600-197 | Real Time Video Quality Representation Classification of Encrypted HTTP Adaptive Video Streaming - the Case of Safari | http://arxiv.org/pdf/1602.00489v2.pdf | author:Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar, Itay Richman, Ofir Trabelsi category:cs.MM cs.CR cs.LG cs.NI published:2016-02-01 summary:The increasing popularity of HTTP adaptive video streaming services hasdramatically increased bandwidth requirements on operator networks, whichattempt to shape their traffic through Deep Packet Inspection (DPI). However,Google and certain content providers have started to encrypt their videoservices. As a result, operators often encounter difficulties in shaping theirencrypted video traffic via DPI. This highlights the need for new trafficclassification methods for encrypted HTTP adaptive video streaming to enablesmart traffic shaping. These new methods will have to effectively estimate thequality representation layer and playout buffer. We present a new method andshow for the first time that video quality representation classification for(YouTube) encrypted HTTP adaptive streaming is possible. We analyze theperformance of this classification method with Safari over HTTPS. Based on alarge number of offline and online traffic classification experiments, wedemonstrate that it can independently classify, in real time, every videosegment into one of the quality representation layers with 97.18% averageaccuracy.
arxiv-15600-198 | All you need is a good init | http://arxiv.org/pdf/1511.06422v7.pdf | author:Dmytro Mishkin, Jiri Matas category:cs.LG published:2015-11-19 summary:Layer-sequential unit-variance (LSUV) initialization - a simple method forweight initialization for deep net learning - is proposed. The method consistsof the two steps. First, pre-initialize weights of each convolution orinner-product layer with orthonormal matrices. Second, proceed from the firstto the final layer, normalizing the variance of the output of each layer to beequal to one. Experiment with different activation functions (maxout, ReLU-family, tanh)show that the proposed initialization leads to learning of very deep nets that(i) produces networks with test accuracy better or equal to standard methodsand (ii) is at least as fast as the complex schemes proposed specifically forvery deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastavaet al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual netsand the state-of-the-art, or very close to it, is achieved on the MNIST,CIFAR-10/100 and ImageNet datasets.
arxiv-15600-199 | Depth-Based Object Tracking Using a Robust Gaussian Filter | http://arxiv.org/pdf/1602.06157v1.pdf | author:Jan Issac, Manuel Wüthrich, Cristina Garcia Cifuentes, Jeannette Bohg, Sebastian Trimpe, Stefan Schaal category:cs.RO cs.CV published:2016-02-19 summary:We consider the problem of model-based 3D-tracking of objects given densedepth images as input. Two difficulties preclude the application of a standardGaussian filter to this problem. First of all, depth sensors are characterizedby fat-tailed measurement noise. To address this issue, we show how a recentlypublished robustification method for Gaussian filters can be applied to theproblem at hand. Thereby, we avoid using heuristic outlier detection methodsthat simply reject measurements if they do not match the model. Secondly, thecomputational cost of the standard Gaussian filter is prohibitive due to thehigh-dimensional measurement, i.e. the depth image. To address this problem, wepropose an approximation to reduce the computational complexity of the filter.In quantitative experiments on real data we show how our method clearlyoutperforms the standard Gaussian filter. Furthermore, we compare itsperformance to a particle-filter-based tracking method, and observe comparablecomputational efficiency and improved accuracy and smoothness of the estimates.
arxiv-15600-200 | Large age-gap face verification by feature injection in deep networks | http://arxiv.org/pdf/1602.06149v1.pdf | author:Simone Bianco category:cs.CV published:2016-02-19 summary:This paper introduces a new method for face verification across large agegaps and also a dataset containing variations of age in the wild, the LargeAge-Gap (LAG) dataset, with images ranging from child/young to adult/old. Theproposed method exploits a deep convolutional neural network (DCNN) pre-trainedfor the face recognition task on a large dataset and then fine-tuned for thelarge age-gap face verification task. Finetuning is performed in a Siamesearchitecture using a contrastive loss function. A feature injection layer isintroduced to boost verification accuracy, showing the ability of the DCNN tolearn a similarity metric leveraging external features. Experimental results onthe LAG dataset show that our method is able to outperform the faceverification solutions in the state of the art considered.
arxiv-15600-201 | Descriptors and regions of interest fusion for gender classification in the wild. Comparison and combination with Convolutional Neural Networks | http://arxiv.org/pdf/1507.06838v2.pdf | author:M. Castrillón-Santana, J. Lorenzo-Navarro, E. Ramón-Balmaseda category:cs.CV published:2015-07-24 summary:Gender classification (GC) has achieved high accuracy in differentexperimental evaluations based mostly on inner facial details. However, theseresults do not generalize well in unrestricted datasets and particularly incross-database experiments, where the performance drops drastically. In thispaper, we analyze the state-of-the-art GC accuracy on three large datasets:MORPH, LFW and GROUPS. We discuss their respective difficulties and bias,concluding that the most challenging and wildest complexity is present inGROUPS. This dataset covers hard conditions such as low resolution imagery andcluttered background. Firstly, we analyze in depth the performance of differentdescriptors extracted from the face and its local context on this dataset.Selecting the bests and studying their most suitable combination allows us todesign a solution that beats any previously published results for GROUPS withthe Dago's protocol, reaching an accuracy over 94.2%, reducing the gap withother simpler datasets. The chosen solution based on local descriptors is laterevaluated in a cross-database scenario with the three mentioned datasets, andfull dataset 5-fold cross validation. The achieved results are compared with aConvolutional Neural Network approach, achieving rather similar marks. Finally,a solution is proposed combining both focuses, exhibiting greatcomplementarity, boosting GC performance to beat previously published resultsin GC both cross-database, and full in-database evaluations.
arxiv-15600-202 | Stacking for machine learning redshifts applied to SDSS galaxies | http://arxiv.org/pdf/1602.06294v1.pdf | author:Roman Zitlau, Ben Hoyle, Kerstin Paech, Jochen Weller, Markus Michael Rau, Stella Seitz category:astro-ph.IM astro-ph.CO cs.LG published:2016-02-19 summary:We present an analysis of a general machine learning technique called'stacking' for the estimation of photometric redshifts. Stacking techniques canfeed the photometric redshift estimate, as output by a base algorithm, backinto the same algorithm as an additional input feature in a subsequent learninground. We shown how all tested base algorithms benefit from at least oneadditional stacking round (or layer). To demonstrate the benefit of stacking,we apply the method to both unsupervised machine learning techniques based onself-organising maps (SOMs), and supervised machine learning methods based ondecision trees. We explore a range of stacking architectures, such as thenumber of layers and the number of base learners per layer. Finally we explorethe effectiveness of stacking even when using a successful algorithm such asAdaBoost. We observe a significant improvement of between 1.9% and 21% on allcomputed metrics when stacking is applied to weak learners (such as SOMs anddecision trees). When applied to strong learning algorithms (such as AdaBoost)the ratio of improvement shrinks, but still remains positive and is between0.4% and 2.5% for the explored metrics and comes at almost no additionalcomputational cost.
arxiv-15600-203 | Least Mean Squares Estimation of Graph Signals | http://arxiv.org/pdf/1602.05703v2.pdf | author:Paolo Di Lorenzo, Sergio Barbarossa, Paolo Banelli, Stefania Sardellitti category:cs.LG cs.SY published:2016-02-18 summary:In many applications spanning from sensor to social networks, transportationsystems, gene regulatory networks or big data, the signals of interest aredefined over the vertices of a graph. The aim of this paper is to propose aleast mean square (LMS) strategy for adaptive estimation of signals definedover graphs. Assuming the graph signal to be band-limited, over a knownbandwidth, the method enables reconstruction, with guaranteed performance interms of mean-square error, and tracking from a limited number of observationsover a subset of vertices. A detailed mean square analysis provides theperformance of the proposed method, and leads to several insights for designinguseful sampling strategies for graph signals. Numerical results validate ourtheoretical findings, and illustrate the performance of the proposed method.Furthermore, to cope with the case where the bandwidth is not known beforehand,we propose a method that performs a sparse online estimation of the signalsupport in the (graph) frequency domain, which enables online adaptation of thegraph sampling strategy. Finally, we apply the proposed method to build thepower spatial density cartography of a given operational region in a cognitivenetwork environment.
arxiv-15600-204 | Alternative Markov and Causal Properties for Acyclic Directed Mixed Graphs | http://arxiv.org/pdf/1511.05835v4.pdf | author:Jose M. Peña category:stat.ML cs.AI published:2015-11-18 summary:We extend Andersson-Madigan-Perlman chain graphs by (i) relaxing thesemidirected acyclity constraint so that only directed cycles are forbidden,and (ii) allowing up to two edges between any pair of nodes. We introduceglobal, and ordered local and pairwise Markov properties for the new models. Weshow the equivalence of these properties for strictly positive probabilitydistributions. We also show that when the random variables are continuous, thenew models can be interpreted as systems of structural equations withcorrelated errors. This enables us to adapt Pearl's do-calculus to them.Finally, we describe an exact algorithm for learning the new models fromobservational and interventional data via answer set programming.
arxiv-15600-205 | Uniresolution representations of white-matter data from CoCoMac | http://arxiv.org/pdf/1602.06057v1.pdf | author:Raghavendra Singh category:cs.NE q-bio.NC published:2016-02-19 summary:Tracing data as collated by CoCoMac, a seminal neuroinformatics database, isat multiple resolutions -- white matter tracts were studied for areas and theirsubdivisions by different reports. Network theoretic analysis of thismulti-resolution data often assumes that the data at various resolutions isequivalent, which may not be correct. In this paper we propose three methods toresolve the multi-resolution issue such that the resultant networks haveconnectivity data at only one resolution. The different resultant networks arecompared in terms of their network analysis metrics and degree distributions.
arxiv-15600-206 | Modularity Component Analysis versus Principal Component Analysis | http://arxiv.org/pdf/1510.05492v2.pdf | author:Hansi Jiang, Carl Meyer category:stat.ML published:2015-10-19 summary:In this paper the exact linear relation between the leading eigenvectors ofthe modularity matrix and the singular vectors of an uncentered data matrix isdeveloped. Based on this analysis the concept of a modularity component isdefined, and its properties are developed. It is shown that modularitycomponent analysis can be used to cluster data similar to how traditionalprincipal component analysis is used except that modularity component analysisdoes not require data centering.
arxiv-15600-207 | First-order Methods for Geodesically Convex Optimization | http://arxiv.org/pdf/1602.06053v1.pdf | author:Hongyi Zhang, Suvrit Sra category:math.OC cs.LG stat.ML published:2016-02-19 summary:Geodesic convexity generalizes the notion of (vector space) convexity tononlinear metric spaces. But unlike convex optimization, geodesically convex(g-convex) optimization is much less developed. In this paper we contribute tothe understanding of g-convex optimization by developing iteration complexityanalysis for several first-order algorithms on Hadamard manifolds.Specifically, we prove upper bounds for the global complexity of deterministicand stochastic (sub)gradient methods for optimizing smooth and nonsmoothg-convex functions, both with and without strong g-convexity. Our analysis alsoreveals how the manifold geometry, especially \emph{sectional curvature},impacts convergence rates. To the best of our knowledge, our work is the firstto provide global complexity analysis for first-order algorithms for generalg-convex optimization.
arxiv-15600-208 | Asynchronous Distributed ADMM for Large-Scale Optimization- Part I: Algorithm and Convergence Analysis | http://arxiv.org/pdf/1509.02597v2.pdf | author:Tsung-Hui Chang, Mingyi Hong, Wei-Cheng Liao, Xiangfeng Wang category:cs.DC cs.LG cs.SY published:2015-09-09 summary:Aiming at solving large-scale learning problems, this paper studiesdistributed optimization methods based on the alternating direction method ofmultipliers (ADMM). By formulating the learning problem as a consensus problem,the ADMM can be used to solve the consensus problem in a fully parallel fashionover a computer network with a star topology. However, traditional synchronizedcomputation does not scale well with the problem size, as the speed of thealgorithm is limited by the slowest workers. This is particularly true in aheterogeneous network where the computing nodes experience differentcomputation and communication delays. In this paper, we propose an asynchronousdistributed ADMM (AD-AMM) which can effectively improve the time efficiency ofdistributed optimization. Our main interest lies in analyzing the convergenceconditions of the AD-ADMM, under the popular partially asynchronous model,which is defined based on a maximum tolerable delay of the network.Specifically, by considering general and possibly non-convex cost functions, weshow that the AD-ADMM is guaranteed to converge to the set ofKarush-Kuhn-Tucker (KKT) points as long as the algorithm parameters are chosenappropriately according to the network delay. We further illustrate that theasynchrony of the ADMM has to be handled with care, as slightly modifying theimplementation of the AD-ADMM can jeopardize the algorithm convergence, evenunder a standard convex setting.
arxiv-15600-209 | Scaling up Dynamic Topic Models | http://arxiv.org/pdf/1602.06049v1.pdf | author:Arnab Bhadury, Jianfei Chen, Jun Zhu, Shixia Liu category:stat.ML H.4; G.3 published:2016-02-19 summary:Dynamic topic models (DTMs) are very effective in discovering topics andcapturing their evolution trends in time series data. To do posterior inferenceof DTMs, existing methods are all batch algorithms that scan the full datasetbefore each update of the model and make inexact variational approximationswith mean-field assumptions. Due to a lack of a more scalable inferencealgorithm, despite the usefulness, DTMs have not captured large topic dynamics. This paper fills this research void, and presents a fast and parallelizableinference algorithm using Gibbs Sampling with Stochastic Gradient LangevinDynamics that does not make any unwarranted assumptions. We also present aMetropolis-Hastings based $O(1)$ sampler for topic assignments for each wordtoken. In a distributed environment, our algorithm requires very littlecommunication between workers during sampling (almost embarrassingly parallel)and scales up to large-scale applications. We are able to learn the largestDynamic Topic Model to our knowledge, and learned the dynamics of 1,000 topicsfrom 2.6 million documents in less than half an hour, and our empirical resultsshow that our algorithm is not only orders of magnitude faster than thebaselines but also achieves lower perplexity.
arxiv-15600-210 | Structured Sparse Regression via Greedy Hard-Thresholding | http://arxiv.org/pdf/1602.06042v1.pdf | author:Prateek Jain, Nikhil Rao, Inderjit Dhillon category:stat.ML cs.LG published:2016-02-19 summary:Several learning applications require solving high-dimensional regressionproblems where the relevant features belong to a small number of (overlapping)groups. For very large datasets, hard thresholding methods have proven to beextremely efficient under standard sparsity assumptions, but such methodsrequire NP hard projections when dealing with overlapping groups. In thispaper, we propose a simple and efficient method that avoids NP-hard projectionsby using greedy approaches. Our proposed methods come with strong theoreticalguarantees even in the presence of poorly conditioned data, exhibit aninteresting computation-accuracy trade-off and can be extended to significantlyharder problems such as sparse overlapping groups. Experiments on both real andsynthetic data validate our claims and demonstrate that the proposed methodsare significantly faster than the best known greedy and convex relaxationtechniques for learning with structured sparsity.
arxiv-15600-211 | Spectral Learning for Supervised Topic Models | http://arxiv.org/pdf/1602.06025v1.pdf | author:Yong Ren, Yining Wang, Jun Zhu category:cs.LG cs.CL cs.IR stat.ML published:2016-02-19 summary:Supervised topic models simultaneously model the latent topic structure oflarge collections of documents and a response variable associated with eachdocument. Existing inference methods are based on variational approximation orMonte Carlo sampling, which often suffers from the local minimum defect.Spectral methods have been applied to learn unsupervised topic models, such aslatent Dirichlet allocation (LDA), with provable guarantees. This paperinvestigates the possibility of applying spectral methods to recover theparameters of supervised LDA (sLDA). We first present a two-stage spectralmethod, which recovers the parameters of LDA followed by a power update methodto recover the regression model parameters. Then, we further present asingle-phase spectral algorithm to jointly recover the topic distributionmatrix as well as the regression weights. Our spectral algorithms are provablycorrect and computationally efficient. We prove a sample complexity bound foreach algorithm and subsequently derive a sufficient condition for theidentifiability of sLDA. Thorough experiments on synthetic and real-worlddatasets verify the theory and demonstrate the practical effectiveness of thespectral algorithms. In fact, our results on a large-scale review ratingdataset demonstrate that our single-phase spectral algorithm alone getscomparable or even better performance than state-of-the-art methods, whileprevious work on spectral methods has rarely reported such promisingperformance.
arxiv-15600-212 | Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples | http://arxiv.org/pdf/1602.02697v2.pdf | author:Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, Ananthram Swami category:cs.CR cs.LG published:2016-02-08 summary:Advances in deep learning have led to the broad adoption of Deep NeuralNetworks (DNNs) to a range of important machine learning problems, e.g.,guiding autonomous vehicles, speech recognition, malware detection. Yet,machine learning models, including DNNs, were shown to be vulnerable toadversarial samples-subtly (and often humanly indistinguishably) modifiedmalicious inputs crafted to compromise the integrity of their outputs.Adversarial examples thus enable adversaries to manipulate system behaviors.Potential attacks include attempts to control the behavior of vehicles, havespam content identified as legitimate content, or have malware identified aslegitimate software. Adversarial examples are known to transfer from one modelto another, even if the second model has a different architecture or wastrained on a different set. We introduce the first practical demonstration thatthis cross-model transfer phenomenon enables attackers to control a remotelyhosted DNN with no access to the model, its parameters, or its training data.In our demonstration, we only assume that the adversary can observe outputsfrom the target DNN given inputs chosen by the adversary. We introduce theattack strategy of fitting a substitute model to the input-output pairs in thismanner, then crafting adversarial examples based on this auxiliary model. Weevaluate the approach on existing DNN datasets and real-world settings. In oneexperiment, we force a DNN supported by MetaMind (one of the online APIs forDNN classifiers) to mis-classify inputs at a rate of 84.24%. We conclude withexperiments exploring why adversarial samples transfer between DNNs, and adiscussion on the applicability of our attack when targeting machine learningalgorithms distinct from DNNs.
arxiv-15600-213 | Structured illumination microscopy image reconstruction algorithm | http://arxiv.org/pdf/1602.06904v1.pdf | author:Amit Lal, Chunyan Shan, Peng Xi category:cs.CV published:2016-02-19 summary:Structured illumination microscopy (SIM) is a very important super-resolutionmicroscopy technique, which provides high speed super-resolution with abouttwo-fold spatial resolution enhancement. Several attempts aimed at improvingthe performance of SIM reconstruction algorithm have been reported. However,most of these highlight only one specific aspect of the SIM reconstruction --such as the determination of the illumination pattern phase shift accurately --whereas other key elements -- such as determination of modulation factor,estimation of object power spectrum, Wiener filtering frequency components withinclusion of object power spectrum information, translocating and the mergingof the overlapping frequency components -- are usually glossed oversuperficially. In addition, most of the work reported lie scattered throughoutthe literature and a comprehensive review of the theoretical background isfound lacking. The purpose of the present work is two-fold: 1) to collect theessential theoretical details of SIM algorithm at one place, thereby makingthem readily accessible to readers for the first time; and 2) to provide anopen source SIM reconstruction code (named OpenSIM), which enables users tointeractively vary the code parameters and study it's effect on reconstructedSIM image.
arxiv-15600-214 | Ground-truth dataset and baseline evaluations for image base-detail separation algorithms | http://arxiv.org/pdf/1511.06830v2.pdf | author:Xuan Dong, Boyan Bonev, Weixin Li, Weichao Qiu, Xianjie Chen, Alan Yuille category:cs.CV published:2015-11-21 summary:Base-detail separation is a fundamental computer vision problem consisting ofmodeling a smooth base layer with the coarse structures, and a detail layercontaining the texture-like structures. One of the challenges of estimating thebase is to preserve sharp boundaries between objects or parts to avoid haloartifacts. Many methods have been proposed to address this problem, but thereis no ground-truth dataset of real images for quantitative evaluation. Weproposed a procedure to construct such a dataset, and provide two datasets:Pascal Base-Detail and Fashionista Base-Detail, containing 1000 and 250 images,respectively. Our assumption is that the base is piecewise smooth and we labelthe appearance of each piece by a polynomial model. The pieces are objects andparts of objects, obtained from human annotations. Finally, we proposed a wayto evaluate methods with our base-detail ground-truth and we compared theperformances of seven state-of-the-art algorithms.
arxiv-15600-215 | A Nonparametric Framework for Quantifying Generative Inference on Neuromorphic Systems | http://arxiv.org/pdf/1602.05996v1.pdf | author:Ojash Neopane, Srinjoy Das, Ery Arias-Castro, Kenneth Kreutz-Delgado category:cs.NE published:2016-02-18 summary:Restricted Boltzmann Machines and Deep Belief Networks have been successfullyused in probabilistic generative model applications such as image occlusionremoval, pattern completion and motion synthesis. Generative inference in suchalgorithms can be performed very efficiently on hardware using a Markov ChainMonte Carlo procedure called Gibbs sampling, where stochastic samples are drawnfrom noisy integrate and fire neurons implemented on neuromorphic substrates.Currently, no satisfactory metrics exist for evaluating the generativeperformance of such algorithms implemented on high-dimensional data forneuromorphic platforms. This paper demonstrates the application ofnonparametric goodness-of-fit testing to both quantify the generativeperformance as well as provide decision-directed criteria for choosing theparameters of the neuromorphic Gibbs sampler and optimizing usage of hardwareresources used during sampling.
arxiv-15600-216 | Plücker Correction Problem: Analysis and Improvements in Efficiency | http://arxiv.org/pdf/1602.05990v1.pdf | author:João R. Cardoso, Pedro Miraldo, Helder Araujo category:cs.CV cs.RO published:2016-02-18 summary:A given six dimensional vector represents a 3D straight line in Pluckercoordinates if its coordinates satisfy the Klein quadric constraint. In manyproblems aiming to find the Plucker coordinates of lines, noise in the dataand other type of errors contribute for obtaining 6D vectors that do notcorrespond to lines, because of that constraint. A common procedure to overcomethis drawback is to find the Plucker coordinates of the lines that are closestto those vectors. This is known as the Plucker correction problem. In thisarticle we propose a simple, closed-form, and global solution for this problem.When compared with the state-of-the-art method, one can conclude that ouralgorithm is easier and requires much less operations than previous techniques(it does not require Singular Value Decomposition techniques).
arxiv-15600-217 | The Interaction of Memory and Attention in Novel Word Generalization: A Computational Investigation | http://arxiv.org/pdf/1602.05944v1.pdf | author:Erin Grant, Aida Nematzadeh, Suzanne Stevenson category:cs.CL published:2016-02-18 summary:People exhibit a tendency to generalize a novel noun to the basic-level in ahierarchical taxonomy -- a cognitively salient category such as "dog" -- withthe degree of generalization depending on the number and type of exemplars.Recently, a change in the presentation timing of exemplars has also been shownto have an effect, surprisingly reversing the prior observed pattern ofbasic-level generalization. We explore the precise mechanisms that could leadto such behavior by extending a computational model of word learning and wordgeneralization to integrate cognitive processes of memory and attention. Ourresults show that the interaction of forgetting and attention to novelty, aswell as sensitivity to both type and token frequencies of exemplars, enablesthe model to replicate the empirical results from different presentationtimings. Our results reinforce the need to incorporate general cognitiveprocesses within word learning models to better understand the range ofobserved behaviors in vocabulary acquisition.
arxiv-15600-218 | Multi-resolution Compressive Sensing Reconstruction | http://arxiv.org/pdf/1602.05941v1.pdf | author:Adriana Gonzalez, Hong Jiang, Gang Huang, Laurent Jacques category:cs.CV published:2016-02-18 summary:We consider the problem of reconstructing an image from compressivemeasurements using a multi-resolution grid. In this context, the reconstructedimage is divided into multiple regions, each one with a different resolution.This problem arises in situations where the image to reconstruct contains acertain region of interest (RoI) that is more important than the rest. Througha theoretical analysis and simulation experiments we show that themulti-resolution reconstruction provides a higher quality of the RoI comparedto the traditional single-resolution approach.
arxiv-15600-219 | Multimodal Transfer Deep Learning with Applications in Audio-Visual Recognition | http://arxiv.org/pdf/1412.3121v2.pdf | author:Seungwhan Moon, Suyoun Kim, Haohan Wang category:cs.NE cs.LG published:2014-12-09 summary:We propose a transfer deep learning (TDL) framework that can transfer theknowledge obtained from a single-modal neural network to a network with adifferent modality. Specifically, we show that we can leverage speech data tofine-tune the network trained for video recognition, given an initial set ofaudio-video parallel dataset within the same semantics. Our approach firstlearns the analogy-preserving embeddings between the abstract representationslearned from intermediate layers of each network, allowing for semantics-leveltransfer between the source and target modalities. We then apply our neuralnetwork operation that fine-tunes the target network with the additionalknowledge transferred from the source network, while keeping the topology ofthe target network unchanged. While we present an audio-visual recognition taskas an application of our approach, our framework is flexible and thus can workwith any multimodal dataset, or with any already-existing deep networks thatshare the common underlying semantics. In this work in progress report, we aimto provide comprehensive results of different configurations of the proposedapproach on two widely used audio-visual datasets, and we discuss potentialapplications of the proposed approach.
arxiv-15600-220 | Encoding Data for HTM Systems | http://arxiv.org/pdf/1602.05925v1.pdf | author:Scott Purdy category:cs.NE q-bio.NC published:2016-02-18 summary:Hierarchical Temporal Memory (HTM) is a biologically inspired machineintelligence technology that mimics the architecture and processes of theneocortex. In this white paper we describe how to encode data as SparseDistributed Representations (SDRs) for use in HTM systems. We explain severalexisting encoders, which are available through the open source project calledNuPIC, and we discuss requirements for creating encoders for new types of data.
arxiv-15600-221 | Weighted Unsupervised Learning for 3D Object Detection | http://arxiv.org/pdf/1602.05920v1.pdf | author:Kamran Kowsari, Manal H. Alassaf category:cs.CV published:2016-02-18 summary:This paper introduces a novel weighted unsupervised learning for objectdetection using an RGB-D camera. This technique is feasible for detecting themoving objects in the noisy environments that are captured by an RGB-D camera.The main contribution of this paper is a real-time algorithm for detecting eachobject using weighted clustering as a separate cluster. In a preprocessingstep, the algorithm calculates the pose 3D position X, Y, Z and RGB color ofeach data point and then it calculates each data point's normal vector usingthe point's neighbor. After preprocessing, our algorithm calculates k-weightsfor each data point; each weight indicates membership. Resulting in clusteredobjects of the scene.
arxiv-15600-222 | Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning | http://arxiv.org/pdf/1602.05916v1.pdf | author:Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi, Georgios Anagnastapolous category:cs.LG published:2016-02-18 summary:We show a Talagrand-type of concentration inequality for Multi-Task Learning(MTL), using which we establish sharp excess risk bounds for MTL in terms ofdistribution- and data-dependent versions of the Local Rademacher Complexity(LRC). We also give a new bound on the LRC for strongly convex hypothesisclasses, which applies not only to MTL but also to the standard i.i.d. setting.Combining both results, one can now easily derive fast-rate bounds on theexcess risk for many prominent MTL methods, including---as wedemonstrate---Schatten-norm, group-norm, and graph-regularized MTL. The derivedbounds reflect a relationship akeen to a conservation law of asymptoticconvergence rates. This very relationship allows for trading o? slower ratesw.r.t. the number of tasks for faster rates with respect to the number ofavailable samples per task, when compared to the rates obtained via atraditional, global Rademacher analysis.
arxiv-15600-223 | Mismatch in the Classification of Linear Subspaces: Sufficient Conditions for Reliable Classification | http://arxiv.org/pdf/1508.01720v2.pdf | author:Jure Sokolic, Francesco Renna, Robert Calderbank, Miguel R. D. Rodrigues category:cs.IT cs.CV math.IT stat.ML published:2015-08-07 summary:This paper considers the classification of linear subspaces with mismatchedclassifiers. In particular, we assume a model where one observes signals in thepresence of isotropic Gaussian noise and the distribution of the signalsconditioned on a given class is Gaussian with a zero mean and a low-rankcovariance matrix. We also assume that the classifier knows only a mismatchedversion of the parameters of input distribution in lieu of the true parameters.By constructing an asymptotic low-noise expansion of an upper bound to theerror probability of such a mismatched classifier, we provide sufficientconditions for reliable classification in the low-noise regime that are able tosharply predict the absence of a classification error floor. Such conditionsare a function of the geometry of the true signal distribution, the geometry ofthe mismatched signal distributions as well as the interplay between suchgeometries, namely, the principal angles and the overlap between the true andthe mismatched signal subspaces. Numerical results demonstrate that ourconditions for reliable classification can sharply predict the behavior of amismatched classifier both with synthetic data and in a motion segmentation anda hand-written digit classification applications.
arxiv-15600-224 | Efficient approaches for escaping higher order saddle points in non-convex optimization | http://arxiv.org/pdf/1602.05908v1.pdf | author:Anima Anandkumar, Rong Ge category:cs.LG stat.ML published:2016-02-18 summary:Local search heuristics for non-convex optimizations are popular in appliedmachine learning. However, in general it is hard to guarantee that suchalgorithms even converge to a local minimum, due to the existence ofcomplicated saddle point structures in high dimensions. Many functions havedegenerate saddle points such that the first and second order derivativescannot distinguish them with local optima. In this paper we use higher orderderivatives to escape these saddle points: we design the first efficientalgorithm guaranteed to converge to a third order local optimum (while existingtechniques are at most second order). We also show that it is NP-hard to extendthis further to finding fourth order local optima.
arxiv-15600-225 | Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity | http://arxiv.org/pdf/1602.05897v1.pdf | author:Amit Daniely, Roy Frostig, Yoram Singer category:cs.LG cs.AI cs.CC cs.DS stat.ML published:2016-02-18 summary:We develop a general duality between neural networks and compositionalkernels, striving towards a better understanding of deep learning. We show thatinitial representations generated by common random initializations aresufficiently rich to express all functions in the dual kernel space. Hence,though the training objective is hard to optimize in the worst case, theinitial weights form a good starting point for optimization. Our dual view alsoreveals a pragmatic and aesthetic perspective of neural networks andunderscores their expressive power.
arxiv-15600-226 | Noisy Tensor Completion via the Sum-of-Squares Hierarchy | http://arxiv.org/pdf/1501.06521v3.pdf | author:Boaz Barak, Ankur Moitra category:cs.LG cs.DS stat.ML published:2015-01-26 summary:In the noisy tensor completion problem we observe $m$ entries (whose locationis chosen uniformly at random) from an unknown $n_1 \times n_2 \times n_3$tensor $T$. We assume that $T$ is entry-wise close to being rank $r$. Our goalis to fill in its missing entries using as few observations as possible. Let $n= \max(n_1, n_2, n_3)$. We show that if $m = n^{3/2} r$ then there is apolynomial time algorithm based on the sixth level of the sum-of-squareshierarchy for completing it. Our estimate agrees with almost all of $T$'sentries almost exactly and works even when our observations are corrupted bynoise. This is also the first algorithm for tensor completion that works in theovercomplete case when $r > n$, and in fact it works all the way up to $r =n^{3/2-\epsilon}$. Our proofs are short and simple and are based on establishing a newconnection between noisy tensor completion (through the language of Rademachercomplexity) and the task of refuting random constant satisfaction problems.This connection seems to have gone unnoticed even in the context of matrixcompletion. Furthermore, we use this connection to show matching lower bounds.Our main technical result is in characterizing the Rademacher complexity of thesequence of norms that arise in the sum-of-squares relaxations to the tensornuclear norm. These results point to an interesting new direction: Can weexplore computational vs. sample complexity tradeoffs through thesum-of-squares hierarchy?
arxiv-15600-227 | Metric learning approach for graph-based label propagation | http://arxiv.org/pdf/1511.05789v6.pdf | author:Pauline Wauquier, Mikaela Keller category:cs.LG published:2015-11-18 summary:The efficiency of graph-based semi-supervised algorithms depends on the graphof instances on which they are applied. The instances are often in a vectorialform before a graph linking them is built. The construction of the graph relieson a metric over the vectorial space that help define the weight of theconnection between entities. The classic choice for this metric is usually adistance measure or a similarity measure based on the euclidean norm. We claimthat in some cases the euclidean norm on the initial vectorial space might notbe the more appropriate to solve the task efficiently. We propose an algorithmthat aims at learning the most appropriate vectorial representation forbuilding a graph on which the task at hand is solved efficiently.
arxiv-15600-228 | Scale-aware Pixel-wise Object Proposal Networks | http://arxiv.org/pdf/1601.04798v2.pdf | author:Zequn Jie, Xiaodan Liang, Jiashi Feng, Wen Feng Lu, Eng Hock Francis Tay, Shuicheng Yan category:cs.CV published:2016-01-19 summary:Object proposal is essential for current state-of-the-art object detectionpipelines. However, the existing proposal methods generally fail in producingresults with satisfying localization accuracy. The case is even worse for smallobjects which however are quite common in practice. In this paper we propose anovel Scale-aware Pixel-wise Object Proposal (SPOP) network to tackle thechallenges. The SPOP network can generate proposals with high recall rate andaverage best overlap (ABO), even for small objects. In particular, in order toimprove the localization accuracy, a fully convolutional network is employedwhich predicts locations of object proposals for each pixel. The producedensemble of pixel-wise object proposals enhances the chance of hitting theobject significantly without incurring heavy extra computational cost. To solvethe challenge of localizing objects at small scale, two localization networkswhich are specialized for localizing objects with different scales areintroduced, following the divide-and-conquer philosophy. Location outputs ofthese two networks are then adaptively combined to generate the final proposalsby a large-/small-size weighting network. Extensive evaluations on PASCAL VOC2007 show the SPOP network is superior over the state-of-the-art models. Thehigh-quality proposals from SPOP network also significantly improve the meanaverage precision (mAP) of object detection with Fast-RCNN framework. Finally,the SPOP network (trained on PASCAL VOC) shows great generalization performancewhen testing it on ILSVRC 2013 validation set.
arxiv-15600-229 | What is the distribution of the number of unique original items in a bootstrap sample? | http://arxiv.org/pdf/1602.05822v1.pdf | author:Alex F. Mendelson, Maria A. Zuluaga, Brian F. Hutton, Sébastien Ourselin category:stat.ML 62G09 published:2016-02-18 summary:Sampling with replacement occurs in many settings in machine learning,notably in the bagging ensemble technique and the .632+ validation scheme. Thenumber of unique original items in a bootstrap sample can have an importantrole in the behaviour of prediction models learned on it. Indeed, there areuncontrived examples where duplicate items have no effect. The purpose of thisreport is to present the distribution of the number of unique original items ina bootstrap sample clearly and concisely, with a view to enabling other machinelearning researchers to understand and control this quantity in existing andfuture resampling techniques. We describe the key characteristics of thisdistribution along with the generalisation for the case where items come fromdistinct categories, as in classification. In both cases we discuss the normallimit, and conduct an empirical investigation to derive a heuristic for when anormal approximation is permissible.
arxiv-15600-230 | PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization | http://arxiv.org/pdf/1505.07427v4.pdf | author:Alex Kendall, Matthew Grimes, Roberto Cipolla category:cs.CV cs.NE cs.RO published:2015-05-27 summary:We present a robust and real-time monocular six degree of freedomrelocalization system. Our system trains a convolutional neural network toregress the 6-DOF camera pose from a single RGB image in an end-to-end mannerwith no need of additional engineering or graph optimisation. The algorithm canoperate indoors and outdoors in real time, taking 5ms per frame to compute. Itobtains approximately 2m and 6 degree accuracy for large scale outdoor scenesand 0.5m and 10 degree accuracy indoors. This is achieved using an efficient 23layer deep convnet, demonstrating that convnets can be used to solvecomplicated out of image plane regression problems. This was made possible byleveraging transfer learning from large scale classification data. We show theconvnet localizes from high level features and is robust to difficult lighting,motion blur and different camera intrinsics where point based SIFT registrationfails. Furthermore we show how the pose feature that is produced generalizes toother scenes allowing us to regress pose with only a few dozen trainingexamples. PoseNet code, dataset and an online demonstration is available on ourproject webpage, at http://mi.eng.cam.ac.uk/projects/relocalisation/
arxiv-15600-231 | Modelling Uncertainty in Deep Learning for Camera Relocalization | http://arxiv.org/pdf/1509.05909v2.pdf | author:Alex Kendall, Roberto Cipolla category:cs.CV cs.RO published:2015-09-19 summary:We present a robust and real-time monocular six degree of freedom visualrelocalization system. We use a Bayesian convolutional neural network toregress the 6-DOF camera pose from a single RGB image. It is trained in anend-to-end manner with no need of additional engineering or graph optimisation.The algorithm can operate indoors and outdoors in real time, taking under 6msto compute. It obtains approximately 2m and 6 degrees accuracy for very largescale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesianconvolutional neural network implementation we obtain an estimate of themodel's relocalization uncertainty and improve state of the art localizationaccuracy on a large scale outdoor dataset. We leverage the uncertainty measureto estimate metric relocalization error and to detect the presence or absenceof the scene in the input image. We show that the model's uncertainty is causedby images being dissimilar to the training dataset in either pose orappearance.
arxiv-15600-232 | Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure | http://arxiv.org/pdf/1602.05772v1.pdf | author:Stefan Gerdjikov, Klaus U. Schulz category:cs.CL published:2016-02-18 summary:When looking at the structure of natural language, "phrases" and "words" arecentral notions. We consider the problem of identifying such "meaningfulsubparts" of language of any length and underlying composition principles in acompletely corpus-based and language-independent way without using any kind ofprior linguistic knowledge. Unsupervised methods for identifying "phrases",mining subphrase structure and finding words in a fully automated way aredescribed. This can be considered as a step towards automatically computing a"general dictionary and grammar of the corpus". We hope that in the long runvariants of our approach turn out to be useful for other kind of sequence dataas well, such as, e.g., speech, genom sequences, or music annotation. Even ifwe are not primarily interested in immediate applications, results obtained fora variety of languages show that our methods are interesting for many practicaltasks in text mining, terminology extraction and lexicography, search enginetechnology, and related fields.
arxiv-15600-233 | Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning | http://arxiv.org/pdf/1602.05765v1.pdf | author:Shoaib Jameel, Steven Schockaert category:cs.AI cs.CL published:2016-02-18 summary:Conceptual spaces are geometric representations of conceptual knowledge, inwhich entities correspond to points, natural properties correspond to convexregions, and the dimensions of the space correspond to salient features. Whileconceptual spaces enable elegant models of various cognitive phenomena, thelack of automated methods for constructing such representations have so farlimited their application in artificial intelligence. To address this issue, wepropose a method which learns a vector-space embedding of entities fromWikipedia and constrains this embedding such that entities of the same semantictype are located in some lower-dimensional subspace. We experimentallydemonstrate the usefulness of these subspaces as (approximate) conceptual spacerepresentations by showing, among others, that important features can bemodelled as directions and that natural properties tend to correspond to convexregions.
arxiv-15600-234 | Overview of Annotation Creation: Processes & Tools | http://arxiv.org/pdf/1602.05753v1.pdf | author:Mark A. Finlayson, Tomaž Erjavec category:cs.CL cs.HC published:2016-02-18 summary:Creating linguistic annotations requires more than just a reliable annotationscheme. Annotation can be a complex endeavour potentially involving manypeople, stages, and tools. This chapter outlines the process of creatingend-to-end linguistic annotations, identifying specific tasks that researchersoften perform. Because tool support is so central to achieving high quality,reusable annotations with low cost, the focus is on identifying capabilitiesthat are necessary or useful for annotation tools, as well as common problemsthese tools present that reduce their utility. Although examples of specifictools are provided in many cases, this chapter concentrates more on abstractcapabilities and problems because new tools appear continuously, while oldtools disappear into disuse or disrepair. The two core capabilities tools musthave are support for the chosen annotation scheme and the ability to work onthe language under study. Additional capabilities are organized into threecategories: those that are widely provided; those that often useful but foundin only a few tools; and those that have as yet little or no available toolsupport.
arxiv-15600-235 | An improved analysis of the ER-SpUD dictionary learning algorithm | http://arxiv.org/pdf/1602.05719v1.pdf | author:Jarosław Błasiok, Jelani Nelson category:cs.LG cs.DS cs.IT math.IT math.PR I.2.6; F.2.0 published:2016-02-18 summary:In "dictionary learning" we observe $Y = AX + E$ for some$Y\in\mathbb{R}^{n\times p}$, $A \in\mathbb{R}^{m\times n}$, and$X\in\mathbb{R}^{m\times p}$. The matrix $Y$ is observed, and $A, X, E$ areunknown. Here $E$ is "noise" of small norm, and $X$ is column-wise sparse. Thematrix $A$ is referred to as a {\em dictionary}, and its columns as {\ematoms}. Then, given some small number $p$ of samples, i.e.\ columns of $Y$, thegoal is to learn the dictionary $A$ up to small error, as well as $X$. Themotivation is that in many applications data is expected to sparse whenrepresented by atoms in the "right" dictionary $A$ (e.g.\ images in the Haarwavelet basis), and the goal is to learn $A$ from the data to then use it forother applications. Recently, [SWW12] proposed the dictionary learning algorithm ER-SpUD withprovable guarantees when $E = 0$ and $m = n$. They showed if $X$ hasindependent entries with an expected $s$ non-zeroes per column for $1 \lesssims \lesssim \sqrt{n}$, and with non-zero entries being subgaussian, then for$p\gtrsim n^2\log^2 n$ with high probability ER-SpUD outputs matrices $A', X'$which equal $A, X$ up to permuting and scaling columns (resp.\ rows) of $A$(resp.\ $X$). They conjectured $p\gtrsim n\log n$ suffices, which they showedwas information theoretically necessary for {\em any} algorithm to succeed when$s \simeq 1$. Significant progress was later obtained in [LV15]. We show that for a slight variant of ER-SpUD, $p\gtrsim n\log(n/\delta)$samples suffice for successful recovery with probability $1-\delta$. We alsoshow that for the unmodified ER-SpUD, $p\gtrsim n^{1.99}$ samples are requiredeven to learn $A, X$ with polynomially small success probability. This resolvesthe main conjecture of [SWW12], and contradicts the main result of [LV15],which claimed that $p\gtrsim n\log^4 n$ guarantees success whp.
arxiv-15600-236 | On the Use of Deep Learning for Blind Image Quality Assessment | http://arxiv.org/pdf/1602.05531v2.pdf | author:Simone Bianco, Luigi Celona, Paolo Napoletano, Raimondo Schettini category:cs.CV published:2016-02-17 summary:In this work we investigate the use of deep learning for distortion-genericblind image quality assessment. We report on different design choices, rangingfrom the use of features extracted from pre-trained Convolutional NeuralNetworks (CNNs) as a generic image description, to the use of featuresextracted from a CNN fine-tuned for the image quality task. Our best proposal,named DeepBIQ, estimates the image quality by aver- age pooling the scorespredicted on multiple sub-regions of the original image. The score of eachsub-region is computed using a Support Vector Regression (SVR) machine takingas input features extracted using a CNN fine-tuned for image qualityassessment. Experimental results on the LIVE In the Wild Image QualityChallenge Database show that DeepBIQ outperforms the state-of-the-art methodscompared, having a Linear Correlation Coefficient (LCC) with human subjectivescores of almost 0.91. Furthermore, in many cases, the quality scorepredictions of DeepBIQ are closer to the average observer than those of ageneric human observer.
arxiv-15600-237 | EEG-informed attended speaker extraction from recorded speech mixtures with application in neuro-steered hearing prostheses | http://arxiv.org/pdf/1602.05702v1.pdf | author:Simon Van Eyndhoven, Tom Francart, Alexander Bertrand category:cs.SD cs.SY stat.ML published:2016-02-18 summary:OBJECTIVE: We aim to extract and denoise the attended speaker in a noisy,two-speaker acoustic scenario, relying on microphone array recordings from abinaural hearing aid, which are complemented with electroencephalography (EEG)recordings to infer the speaker of interest. METHODS: In this study, we proposea modular processing flow that first extracts the two speech envelopes from themicrophone recordings, then selects the attended speech envelope based on theEEG, and finally uses this envelope to inform a multi-channel speech separationand denoising algorithm. RESULTS: Strong suppression of interfering(unattended) speech and background noise is achieved, while the attended speechis preserved. Furthermore, EEG-based auditory attention detection (AAD) isshown to be robust to the use of noisy speech signals. CONCLUSIONS: Our resultsshow that AAD-based speaker extraction from microphone array recordings isfeasible and robust, even in noisy acoustic environments, and without access tothe clean speech signals to perform EEG-based AAD. SIGNIFICANCE: Currentresearch on AAD always assumes the availability of the clean speech signals,which limits the applicability in real settings. We have extended this researchto detect the attended speaker even when only microphone recordings with noisyspeech mixtures are available. This is an enabling ingredient for newbrain-computer interfaces and effective filtering schemes in neuro-steeredhearing prostheses. Here, we provide a first proof of concept for EEG-informedattended speaker extraction and denoising.
arxiv-15600-238 | Multi-Sensor Slope Change Detection | http://arxiv.org/pdf/1509.00114v2.pdf | author:Yang Cao, Yao Xie, Nagi Gebraeel category:stat.ML cs.LG math.ST stat.TH published:2015-09-01 summary:We develop a mixture procedure for multi-sensor systems to monitor datastreams for a change-point that causes a gradual degradation to a subset of thestreams. Observations are assumed to be initially normal random variables withknown constant means and variances. After the change-point, observations in thesubset will have increasing or decreasing means. The subset and therate-of-changes are unknown. Our procedure uses a mixture statistics, whichassumes that each sensor is affected by the change-point with probability$p_0$. Analytic expressions are obtained for the average run length (ARL) andthe expected detection delay (EDD) of the mixture procedure, which aredemonstrated to be quite accurate numerically. We establish the asymptoticoptimality of the mixture procedure. Numerical examples demonstrate the goodperformance of the proposed procedure. We also discuss an adaptive mixtureprocedure using empirical Bayes. This paper extends our earlier work ondetecting an abrupt change-point that causes a mean-shift, by tackling thechallenges posed by the non-stationarity of the slope-change problem.
arxiv-15600-239 | Feature-Area Optimization: A Novel SAR Image Registration Method | http://arxiv.org/pdf/1602.05660v1.pdf | author:Fuqiang Liu, Fukun Bi, Liang Chen, Hao Shi, Wei Liu category:cs.CV published:2016-02-18 summary:This letter proposes a synthetic aperture radar (SAR) image registrationmethod named Feature-Area Optimization (FAO). First, the traditional area-basedoptimization model is reconstructed and decomposed into three key but uncertainfactors: initialization, slice set and regularization. Next, structuralfeatures are extracted by scale invariant feature transform (SIFT) indual-resolution space (SIFT-DRS), a novel SIFT-Like method dedicated to FAO.Then, the three key factors are determined based on these features. Finally,solving the factor-determined optimization model can get the registrationresult. A series of experiments demonstrate that the proposed method canregister multi-temporal SAR images accurately and efficiently.
arxiv-15600-240 | Unitary Evolution Recurrent Neural Networks | http://arxiv.org/pdf/1511.06464v3.pdf | author:Martin Arjovsky, Amar Shah, Yoshua Bengio category:cs.LG cs.NE published:2015-11-20 summary:Recurrent neural networks (RNNs) are notoriously difficult to train. When theeigenvalues of the hidden to hidden weight matrix deviate from absolute value1, optimization becomes difficult due to the well studied issue of vanishingand exploding gradients, especially when trying to learn long-termdependencies. To circumvent this problem, we propose a new architecture thatlearns a unitary weight matrix, with eigenvalues of absolute value exactly 1.The challenge we address is that of parametrizing unitary matrices in a waythat does not require expensive computations (such as eigendecomposition) aftereach weight update. We construct an expressive unitary weight matrix bycomposing several structured matrices that act as building blocks withparameters to be learned. Optimization with this parameterization becomesfeasible only when considering hidden states in the complex domain. Wedemonstrate the potential of this architecture by achieving state of the artresults in several hard tasks involving very long-term dependencies.
arxiv-15600-241 | Federated Learning of Deep Networks using Model Averaging | http://arxiv.org/pdf/1602.05629v1.pdf | author:H. Brendan McMahan, Eider Moore, Daniel Ramage, Blaise Agüera y Arcas category:cs.LG published:2016-02-17 summary:Modern mobile devices have access to a wealth of data suitable for learningmodels, which in turn can greatly improve the user experience on the device.For example, language models can improve speech recognition and text entry, andimage models can automatically select good photos. However, this rich data isoften privacy sensitive, large in quantity, or both, which may preclude loggingto the data-center and training there using conventional approaches. Weadvocate an alternative that leaves the training data distributed on the mobiledevices, and learns a shared model by aggregating locally-computed updates. Weterm this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networksthat proves robust to the unbalanced and non-IID data distributions thatnaturally arise. This method allows high-quality models to be trained inrelatively few rounds of communication, the principal constraint for federatedlearning. The key insight is that despite the non-convex loss functions weoptimize, parameter averaging over updates from multiple clients producessurprisingly good results, for example decreasing the communication needed totrain an LSTM language model by two orders of magnitude.
arxiv-15600-242 | Visual Tracking via Reliable Memories | http://arxiv.org/pdf/1602.01887v2.pdf | author:Shu Wang, Shaoting Zhang, Wei Liu, Dimitris N. Metaxas category:cs.CV published:2016-02-04 summary:In this paper, we propose a novel visual tracking framework thatintelligently discovers reliable patterns from a wide range of video to resistdrift error for long-term tracking tasks. First, we design a Discrete FourierTransform (DFT) based tracker which is able to exploit a large number oftracked samples while still ensures real-time performance. Second, we propose aclustering method with temporal constraints to explore and memorize consistentpatterns from previous frames, named as reliable memories. By virtue of thismethod, our tracker can utilize uncontaminated information to alleviatedrifting issues. Experimental results show that our tracker performs favorablyagainst other state of-the-art methods on benchmark datasets. Furthermore, itis significantly competent in handling drifts and able to robustly trackchallenging long videos over 4000 frames, while most of others lose track atearly frames.
arxiv-15600-243 | A Survey on Social Media Anomaly Detection | http://arxiv.org/pdf/1601.01102v2.pdf | author:Rose Yu, Huida Qiu, Zhen Wen, Ching-Yung Lin, Yan Liu category:cs.LG cs.SI published:2016-01-06 summary:Social media anomaly detection is of critical importance to prevent maliciousactivities such as bullying, terrorist attack planning, and fraud informationdissemination. With the recent popularity of social media, new types ofanomalous behaviors arise, causing concerns from various parties. While a largeamount of work have been dedicated to traditional anomaly detection problems,we observe a surge of research interests in the new realm of social mediaanomaly detection. In this paper, we present a survey on existing approaches toaddress this problem. We focus on the new type of anomalous phenomena in thesocial media and review the recent developed techniques to detect those specialtypes of anomalies. We provide a general overview of the problem domain, commonformulations, existing methodologies and potential directions. With this work,we hope to call out the attention from the research community on thischallenging problem and open up new directions that we can contribute in thefuture.
arxiv-15600-244 | Multi-layer Representation Learning for Medical Concepts | http://arxiv.org/pdf/1602.05568v1.pdf | author:Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coffey, Jimeng Sun category:cs.LG published:2016-02-17 summary:Learning efficient representations for concepts has been proven to be animportant basis for many applications such as machine translation or documentclassification. Proper representations of medical concepts such as diagnosis,medication, procedure codes and visits will have broad applications inhealthcare analytics. However, in Electronic Health Records (EHR) the visitsequences of patients include multiple concepts (diagnosis, procedure, andmedication codes) per visit. This structure provides two types of relationalinformation, namely sequential order of visits and co-occurrence of the codeswithin each visit. In this work, we propose Med2Vec, which not only learnsdistributed representations for both medical codes and visits from a large EHRdataset with over 3 million visits, but also allows us to interpret the learnedrepresentations confirmed positively by clinical experts. In the experiments,Med2Vec displays significant improvement in key medical applications comparedto popular baselines such as Skip-gram, GloVe and stacked autoencoder, whileproviding clinically meaningful interpretation.
arxiv-15600-245 | Robust Kernel (Cross-) Covariance Operators in Reproducing Kernel Hilbert Space toward Kernel Methods | http://arxiv.org/pdf/1602.05563v1.pdf | author:Md. Ashad Alam, Kenji Fukumizu, Yu-Ping Wang category:stat.ML published:2016-02-17 summary:To the best of our knowledge, there are no general well-founded robustmethods for statistical unsupervised learning. Most of the unsupervised methodsexplicitly or implicitly depend on the kernel covariance operator (kernel CO)or kernel cross-covariance operator (kernel CCO). They are sensitive tocontaminated data, even when using bounded positive definite kernels. First, wepropose robust kernel covariance operator (robust kernel CO) and robust kernelcrosscovariance operator (robust kernel CCO) based on a generalized lossfunction instead of the quadratic loss function. Second, we propose influencefunction of classical kernel canonical correlation analysis (classical kernelCCA). Third, using this influence function, we propose a visualization methodto detect influential observations from two sets of data. Finally, we propose amethod based on robust kernel CO and robust kernel CCO, called robust kernelCCA, which is designed for contaminated data and less sensitive to noise thanclassical kernel CCA. The principles we describe also apply to many kernelmethods which must deal with the issue of kernel CO or kernel CCO. Experimentson synthesized and imaging genetics analysis demonstrate that the proposedvisualization and robust kernel CCA can be applied effectively to both idealdata and contaminated data. The robust methods show the superior performanceover the state-of-the-art methods.
arxiv-15600-246 | Graying the black box: Understanding DQNs | http://arxiv.org/pdf/1602.02658v3.pdf | author:Tom Zahavy, Nir Ben Zrihem, Shie Mannor category:cs.LG cs.AI cs.NE published:2016-02-08 summary:In recent years there is a growing interest in using deep representations forreinforcement learning. In this paper, we present a methodology and tools toanalyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we revealthat the features learned by DQNs aggregate the state space in a hierarchicalfashion, explaining its success. Moreover we are able to understand anddescribe the policies learned by DQNs for three different Atari2600 games andsuggest ways to interpret, debug and optimize of deep neural networks inReinforcement Learning.
arxiv-15600-247 | High-Performance and Tunable Stereo Reconstruction | http://arxiv.org/pdf/1511.00758v2.pdf | author:Sudeep Pillai, Srikumar Ramalingam, John J. Leonard category:cs.RO cs.CV published:2015-11-03 summary:Traditional stereo algorithms have focused their efforts on reconstructionquality and have largely avoided prioritizing for run time performance. Robots,on the other hand, require quick maneuverability and effective computation toobserve its immediate environment and perform tasks within it. In this work, wepropose a high-performance and tunable stereo disparity estimation method, witha peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that canpotentially enable robots to quickly reconstruct their immediate surroundingsand maneuver at high-speeds. Our key contribution is a disparity estimationalgorithm that iteratively approximates the scene depth via a piece-wise planarmesh from stereo imagery, with a fast depth validation step for semi-densereconstruction. The mesh is initially seeded with sparsely matched keypoints,and is recursively tessellated and refined as needed (via a resampling stage),to provide the desired stereo disparity accuracy. The inherent simplicity andspeed of our approach, with the ability to tune it to a desired reconstructionquality and runtime performance makes it a compelling solution for applicationsin high-speed vehicles.
arxiv-15600-248 | Auxiliary Deep Generative Models | http://arxiv.org/pdf/1602.05473v1.pdf | author:Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, Ole Winther category:stat.ML cs.AI cs.LG published:2016-02-17 summary:Deep generative models parameterized by neural networks have recentlyachieved state-of-the-art performance in unsupervised and semi-supervisedlearning. We extend deep generative models with auxiliary variables whichimproves the variational approximation. The auxiliary variables leave thegenerative model unchanged but make the variational distribution moreexpressive. Inspired by the structure of the auxiliary variable we also proposea model with two stochastic layers and skip connections. Our findings suggestthat more expressive and properly specified deep generative models convergefaster with better results. We show state-of-the-art performance withinsemi-supervised learning on MNIST (0.96%), SVHN (16.61%) and NORB (9.40%)datasets.
arxiv-15600-249 | Lass-0: sparse non-convex regression by local search | http://arxiv.org/pdf/1511.04402v2.pdf | author:William Herlands, Maria De-Arteaga, Daniel Neill, Artur Dubrawski category:stat.ML published:2015-11-13 summary:We compute approximate solutions to L0 regularized linear regression using L1regularization, also known as the Lasso, as an initialization step. Ouralgorithm, the Lass-0 ("Lass-zero"), uses a computationally efficient stepwisesearch to determine a locally optimal L0 solution given any L1 regularizationsolution. We present theoretical results of consistency under orthogonality andappropriate handling of redundant features. Empirically, we use synthetic datato demonstrate that Lass-0 solutions are closer to the true sparse support thanL1 regularization models. Additionally, in real-world data Lass-0 finds moreparsimonious solutions than L1 regularization while maintaining similarpredictive accuracy.
arxiv-15600-250 | Inverse Reinforcement Learning in Swarm Systems | http://arxiv.org/pdf/1602.05450v1.pdf | author:Adrian Šošić, Wasiur R. KhudaBukhsh, Abdelhak M. Zoubir, Heinz Koeppl category:stat.ML cs.AI cs.MA cs.SY published:2016-02-17 summary:Inverse reinforcement learning (IRL) is the problem of recovering a system'slatent reward function from observed system behavior. In this paper, weconcentrate on IRL in homogeneous large-scale systems, which we refer to asswarms. We show that, by exploiting the inherent homogeneity of a swarm, theIRL objective can be reduced to an equivalent single-agent formulation ofconstant complexity, which allows us to decompose a global system objectiveinto local subgoals at the agent-level. Based on this finding, we reformulatethe corresponding optimal control problem as a fix-point problem pointingtowards a symmetric Nash equilibrium, which we solve using a novelheterogeneous learning scheme particularly tailored to the swarm setting.Results on the Vicsek model and the Ising model demonstrate that the proposedframework is able to produce meaningful reward models from which we can learnnear-optimal local controllers that replicate the observed system dynamics.
arxiv-15600-251 | Cell segmentation with random ferns and graph-cuts | http://arxiv.org/pdf/1602.05439v1.pdf | author:Arnaud Browet, Christophe De Vleeschouwer, Laurent Jacques, Navrita Mathiah, Bechara Saykali, Isabelle Migeotte category:cs.CV cs.LG published:2016-02-17 summary:The progress in imaging techniques have allowed the study of various aspectof cellular mechanisms. To isolate individual cells in live imaging data, weintroduce an elegant image segmentation framework that effectively extractscell boundaries, even in the presence of poor edge details. Our approach worksin two stages. First, we estimate pixel interior/border/exterior classprobabilities using random ferns. Then, we use an energy minimization frameworkto compute boundaries whose localization is compliant with the pixel classprobabilities. We validate our approach on a manually annotated dataset.
arxiv-15600-252 | Low-Rank Factorization of Determinantal Point Processes for Recommendation | http://arxiv.org/pdf/1602.05436v1.pdf | author:Mike Gartrell, Ulrich Paquet, Noam Koenigstein category:stat.ML cs.LG published:2016-02-17 summary:Determinantal point processes (DPPs) have garnered attention as an elegantprobabilistic model of set diversity. They are useful for a number of subsetselection tasks, including product recommendation. DPPs are parametrized by apositive semi-definite kernel matrix. In this work we present a new method forlearning the DPP kernel from observed data using a low-rank factorization ofthis kernel. We show that this low-rank factorization enables a learningalgorithm that is nearly an order of magnitude faster than previous approaches,while also providing for a method for computing product recommendationpredictions that is far faster (up to 20x faster or more for large itemcatalogs) than previous techniques that involve a full-rank DPP kernel.Furthermore, we show that our method provides equivalent or sometimes betterpredictive performance than prior full-rank DPP approaches, and betterperformance than several other competing recommendation methods in many cases.We conduct an extensive experimental evaluation using several real-worlddatasets in the domain of product recommendation to demonstrate the utility ofour method, along with its limitations.
arxiv-15600-253 | Online optimization and regret guarantees for non-additive long-term constraints | http://arxiv.org/pdf/1602.05394v1.pdf | author:Rodolphe Jenatton, Jim Huang, Cedric Archambeau category:stat.ML cs.LG math.OC math.ST stat.TH published:2016-02-17 summary:We consider online optimization in the 1-lookahead setting, where theobjective does not decompose additively over the rounds of the online game. Theresulting formulation enables us to deal with non-stationary and/or long-termconstraints , which arise, for example, in online display advertising problems.We propose an on-line primal-dual algorithm for which we obtain dynamiccumulative regret guarantees. They depend on the convexity and the smoothnessof the non-additive penalty, as well as terms capturing the smoothness withwhich the residuals of the non-stationary and long-term constraints vary overthe rounds. We conduct experiments on synthetic data to illustrate the benefitsof the non-additive penalty and show vanishing regret convergence on livetraffic data collected by a display advertising platform in production.
arxiv-15600-254 | Recommendations as Treatments: Debiasing Learning and Evaluation | http://arxiv.org/pdf/1602.05352v1.pdf | author:Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, Thorsten Joachims category:cs.LG cs.AI cs.IR published:2016-02-17 summary:Most data for evaluating and training recommender systems is subject toselection biases, either through self-selection by the users or through theactions of the recommendation system itself. In this paper, we provide aprincipled approach to handling selection biases, adapting models andestimation techniques from causal inference. The approach leads to unbiasedperformance estimators despite biased data, and to a matrix factorizationmethod that provides substantially improved prediction performance onreal-world data. We theoretically and empirically characterize the robustnessof the approach, finding that it is highly practical and scalable.
arxiv-15600-255 | Relative Error Embeddings for the Gaussian Kernel Distance | http://arxiv.org/pdf/1602.05350v1.pdf | author:Di Chen, Jeff M. Phillips category:cs.LG published:2016-02-17 summary:A reproducing kernel can define an embedding of a data point into an infinitedimensional reproducing kernel Hilbert space (RKHS). The norm in this spacedescribes a distance, which we call the kernel distance. The random Fourierfeatures (of Rahimi and Recht) describe an oblivious approximate mapping intofinite dimensional Euclidean space that behaves similar to the RKHS. We show inthis paper that for the Gaussian kernel the Euclidean norm between these mappedto features has $(1+\epsilon)$-relative error with respect to the kerneldistance. When there are $n$ data points, we show that $O((1/\epsilon^2)\log(n))$ dimensions of the approximate feature space are sufficient andnecessary. Without a bound on $n$, but when the original points lie in $\mathbb{R}^d$and have diameter bounded by $\mathcal{M}$, then we show that $O((d/\epsilon^2)\log(\mathcal{M}))$ dimensions are sufficient, and that this many are required,up to $\log(1/\epsilon)$ factors.
arxiv-15600-256 | Image Restoration: A General Wavelet Frame Based Model and Its Asymptotic Analysis | http://arxiv.org/pdf/1602.05332v1.pdf | author:Bin Dong, Zuowei Shen, Peichu Xie category:math.FA cs.CV published:2016-02-17 summary:Image restoration is one of the most important areas in imaging science.Mathematical tools have been widely used in image restoration, where waveletframe based approach is one of the successful examples. In this paper, weintroduce a generic wavelet frame based image restoration model, called the"general model", which includes most of the existing wavelet frame based modelsas special cases. Moreover, the general model also includes examples that arenew to the literature. Motivated by our earlier studies [1-3], We provide anasymptotic analysis of the general model as image resolution goes to infinity,which establishes a connection between the general model in discrete settingand a new variatonal model in continuum setting. The variational model alsoincludes some of the existing variational models as special cases, such as thetotal generalized variational model proposed by [4]. In the end, we introducean algorithm solving the general model and present one numerical simulation asan example.
arxiv-15600-257 | Deep Linear Discriminant Analysis | http://arxiv.org/pdf/1511.04707v5.pdf | author:Matthias Dorfer, Rainer Kelz, Gerhard Widmer category:cs.LG published:2015-11-15 summary:We introduce Deep Linear Discriminant Analysis (DeepLDA) which learnslinearly separable latent representations in an end-to-end fashion. Classic LDAextracts features which preserve class separability and is used fordimensionality reduction for many classification problems. The central idea ofthis paper is to put LDA on top of a deep neural network. This can be seen as anon-linear extension of classic LDA. Instead of maximizing the likelihood oftarget labels for individual samples, we propose an objective function thatpushes the network to produce feature distributions which: (a) have lowvariance within the same class and (b) high variance between different classes.Our objective is derived from the general LDA eigenvalue problem and stillallows to train with stochastic gradient descent and back-propagation. Forevaluation we test our approach on three different benchmark datasets (MNIST,CIFAR-10 and STL-10). DeepLDA produces competitive results on MNIST andCIFAR-10 and outperforms a network trained with categorical cross entropy (samearchitecture) on a supervised setting of STL-10.
arxiv-15600-258 | A variational approach to path estimation and parameter inference of hidden diffusion processes | http://arxiv.org/pdf/1508.00506v2.pdf | author:Tobias Sutter, Arnab Ganguly, Heinz Koeppl category:math.OC cs.LG cs.SY math.PR math.ST stat.TH published:2015-08-03 summary:We consider a hidden Markov model, where the signal process, given by adiffusion, is only indirectly observed through some noisy measurements. Thearticle develops a variational method for approximating the hidden states ofthe signal process given the full set of observations. This, in particular,leads to systematic approximations of the smoothing densities of the signalprocess. The paper then demonstrates how an efficient inference scheme, basedon this variational approach to the approximation of the hidden states, can bedesigned to estimate the unknown parameters of stochastic differentialequations. Two examples at the end illustrate the efficacy and the accuracy ofthe presented method.
arxiv-15600-259 | PlaNet - Photo Geolocation with Convolutional Neural Networks | http://arxiv.org/pdf/1602.05314v1.pdf | author:Tobias Weyand, Ilya Kostrikov, James Philbin category:cs.CV published:2016-02-17 summary:Is it possible to build a system to determine the location where a photo wastaken using just its pixels? In general, the problem seems exceptionallydifficult: it is trivial to construct situations where no location can beinferred. Yet images often contain informative cues such as landmarks, weatherpatterns, vegetation, road markings, and architectural details, which incombination may allow one to determine an approximate location and occasionallyan exact location. Websites such as GeoGuessr and View from your Window suggestthat humans are relatively good at integrating these cues to geolocate images,especially en-masse. In computer vision, the photo geolocation problem isusually approached using image retrieval methods. In contrast, we pose theproblem as one of classification by subdividing the surface of the earth intothousands of multi-scale geographic cells, and train a deep network usingmillions of geotagged images. While previous approaches only recognizelandmarks or perform approximate matching using global image descriptors, ourmodel is able to use and integrate multiple visible cues. We show that theresulting model, called PlaNet, outperforms previous approaches and evenattains superhuman levels of accuracy in some cases. Moreover, we extend ourmodel to photo albums by combining it with a long short-term memory (LSTM)architecture. By learning to exploit temporal coherence to geolocate uncertainphotos, we demonstrate that this model achieves a 50% performance improvementover the single-image model.
arxiv-15600-260 | Density-based Denoising of Point Cloud | http://arxiv.org/pdf/1602.05312v1.pdf | author:Faisal Zaman, Ya Ping Wong, Boon Yian Ng category:cs.CV published:2016-02-17 summary:Point cloud source data for surface reconstruction is usually contaminatedwith noise and outliers. To overcome this deficiency, a density-based pointcloud denoising method is presented to remove outliers and noisy points. First,particle-swam optimization technique is employed for automaticallyapproximating optimal bandwidth of multivariate kernel density estimation toensure the robust performance of density estimation. Then, mean-shift basedclustering technique is used to remove outliers through a thresholding scheme.After removing outliers from the point cloud, bilateral mesh filtering isapplied to smooth the remaining points. The experimental results show that thisapproach, comparably, is robust and efficient.
arxiv-15600-261 | Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning | http://arxiv.org/pdf/1509.01692v3.pdf | author:Ekaterina Vylomova, Laura Rimell, Trevor Cohn, Timothy Baldwin category:cs.CL published:2015-09-05 summary:Recent work on word embeddings has shown that simple vector subtraction overpre-trained embeddings is surprisingly effective at capturing different lexicalrelations, despite lacking explicit supervision. Prior work has evaluated thisintriguing result using a word analogy prediction formulation and hand-selectedrelations, but the generality of the finding over a broader range of lexicalrelation types and different learning settings has not been evaluated. In thispaper, we carry out such an evaluation in two learning settings: (1) spectralclustering to induce word relations, and (2) supervised learning to classifyvector differences into relation types. We find that word embeddings capture asurprising amount of information, and that, under suitable supervised training,vector subtraction generalises well to a broad range of relations, includingover unseen lexical items.
arxiv-15600-262 | Large Scale Kernel Learning using Block Coordinate Descent | http://arxiv.org/pdf/1602.05310v1.pdf | author:Stephen Tu, Rebecca Roelofs, Shivaram Venkataraman, Benjamin Recht category:cs.LG math.OC stat.ML published:2016-02-17 summary:We demonstrate that distributed block coordinate descent can quickly solvekernel regression and classification problems with millions of data points.Armed with this capability, we conduct a thorough comparison between the fullkernel, the Nystr\"om method, and random features on three large classificationtasks from various domains. Our results suggest that the Nystr\"om methodgenerally achieves better statistical accuracy than random features, but canrequire significantly more iterations of optimization. Lastly, we derive newrates for block coordinate descent which support our experimental findings whenspecialized to kernel methods.
arxiv-15600-263 | Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding | http://arxiv.org/pdf/1602.05307v1.pdf | author:Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Jiawei Han category:cs.CL cs.LG published:2016-02-17 summary:Current systems of fine-grained entity typing use distant supervision inconjunction with existing knowledge bases to assign categories (type labels) toentity mentions. However, the type labels so obtained from knowledge bases areoften noisy (i.e., incorrect for the entity mention's local context). We definea new task, Label Noise Reduction in Entity Typing (LNR), to be the automaticidentification of correct type labels (type-paths) for training examples, giventhe set of candidate type labels obtained by distant supervision with a giventype hierarchy. The unknown type labels for individual entity mentions and thesemantic similarity between entity types pose unique challenges for solving theLNR task. We propose a general framework, called PLE, to jointly embed entitymentions, text features and entity types into the same low-dimensional spacewhere, in that space, objects whose types are semantically close have similarrepresentations. Then we estimate the type-path for each training example in atop-down manner using the learned embeddings. We formulate a global objectivefor learning the embeddings from text corpora and knowledge bases, which adoptsa novel margin-based loss that is robust to noisy labels and faithfully modelstype correlation derived from knowledge bases. Our experiments on three publictyping datasets demonstrate the effectiveness and robustness of PLE, with anaverage of 25% improvement in accuracy compared to next best method.
arxiv-15600-264 | Multi-Source Domain Adaptation Using Approximate Label Matching | http://arxiv.org/pdf/1602.04889v2.pdf | author:Jordan T. Ash, Robert E. Schapire category:cs.LG cs.AI published:2016-02-16 summary:Domain adaptation, and transfer learning more generally, seeks to remedy theproblem created when training and testing datasets are generated by differentdistributions. In this work, we introduce a new unsupervised domain adaptationalgorithm for when there are multiple sources available to a learner. Ourtechnique assigns a rough labeling on the target samples, then uses it to learna transformation that aligns the two datasets before final classification. Inthis article we give a convenient implementation of our method, show severalexperiments using it, and compare it to other methods commonly used in thefield.
arxiv-15600-265 | Authorship Attribution Using a Neural Network Language Model | http://arxiv.org/pdf/1602.05292v1.pdf | author:Zhenhao Ge, Yufang Sun, Mark J. T. Smith category:cs.CL cs.AI published:2016-02-17 summary:In practice, training language models for individual authors is oftenexpensive because of limited data resources. In such cases, Neural NetworkLanguage Models (NNLMs), generally outperform the traditional non-parametricN-gram models. Here we investigate the performance of a feed-forward NNLM on anauthorship attribution problem, with moderate author set size and relativelylimited data. We also consider how the text topics impact performance. Comparedwith a well-constructed N-gram baseline method with Kneser-Ney smoothing, theproposed method achieves nearly 2:5% reduction in perplexity and increasesauthor classification accuracy by 3:43% on average, given as few as 5 testsentences. The performance is very competitive with the state of the art interms of accuracy and demand on test data. The source code, preprocesseddatasets, a detailed description of the methodology and results are availableat https://github.com/zge/authorship-attribution.
arxiv-15600-266 | Choice by Elimination via Deep Neural Networks | http://arxiv.org/pdf/1602.05285v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG published:2016-02-17 summary:We introduce Neural Choice by Elimination, a new framework that integratesdeep neural networks into probabilistic sequential choice models for learningto rank. Given a set of items to chose from, the elimination strategy startswith the whole item set and iteratively eliminates the least worthy item in theremaining subset. We prove that the choice by elimination is equivalent tomarginalizing out the random Gompertz latent utilities. Coupled with the choicemodel is the recently introduced Neural Highway Networks for approximatingarbitrarily complex rank functions. We evaluate the proposed framework on alarge-scale public dataset with over 425K items, drawn from the Yahoo! learningto rank challenge. It is demonstrated that the proposed method is competitiveagainst state-of-the-art learning to rank methods.
arxiv-15600-267 | High-dimensional Time Series Prediction with Missing Values | http://arxiv.org/pdf/1509.08333v3.pdf | author:Hsiang-Fu Yu, Nikhil Rao, Inderjit S. Dhillon category:cs.LG stat.ML published:2015-09-28 summary:High-dimensional time series prediction is needed in applications as diverseas demand forecasting and climatology. Often, such applications require methodsthat are both highly scalable, and deal with noisy data in terms of corruptionsor missing values. Classical time series methods usually fall short of handlingboth these issues. In this paper, we propose to adapt matrix matrix completionapproaches that have previously been successfully applied to large scale noisydata, but which fail to adequately model high-dimensional time series due totemporal dependencies. We present a novel temporal regularized matrixfactorization (TRMF) framework which supports data-driven temporal dependencylearning and enables forecasting ability to our new matrix factorizationapproach. TRMF is highly general, and subsumes many existing matrixfactorization approaches for time series data. We make interesting connectionsto graph regularized matrix factorization methods in the context of learningthe dependencies. Experiments on both real and synthetic data show that TRMFoutperforms several existing approaches for common time series tasks.
arxiv-15600-268 | A landmark-based algorithm for automatic pattern recognition and abnormality detection | http://arxiv.org/pdf/1602.05572v1.pdf | author:S. Huzurbazar, Long Lee, Dongyang Kuang category:cs.CV published:2016-02-17 summary:We study a class of mathematical and statistical algorithms with the aim ofestablishing a computer-based framework for fast and reliable automatic patternrecognition and abnormality detection. Under this framework, we propose anumerical algorithm for finding group averages where an average of a group isan estimator that is said to best represent the properties of interest of thatgroup. A novelty of the proposed landmark-based algorithm is that the algorithmtracks information of the momentum field through the geodesic shooting process.The momentum field provides a local template-based coordinate system and islinear in nature. It is also a dual of the velocity field with respect to anassigned base template, yielding advantages for statistical analyses. We applythis framework to a small brain image database for detecting structureabnormality. The brain structure changes identified by our framework are highlyconsistent with studies in the literature.
arxiv-15600-269 | Anomaly Detection in Clutter using Spectrally Enhanced Ladar | http://arxiv.org/pdf/1602.05264v1.pdf | author:Puneet S Chhabra, Andrew M Wallace, James R Hopgood category:physics.optics cs.LG stat.AP stat.ML published:2016-02-17 summary:Discrete return (DR) Laser Detection and Ranging (Ladar) systems provide aseries of echoes that reflect from objects in a scene. These can be first, lastor multi-echo returns. In contrast, Full-Waveform (FW)-Ladar systems measurethe intensity of light reflected from objects continuously over a period oftime. In a camouflaged scenario, e.g., objects hidden behind dense foliage, aFW-Ladar penetrates such foliage and returns a sequence of echoes includingburied faint echoes. The aim of this paper is to learn local-patterns ofco-occurring echoes characterised by their measured spectra. A deviation fromsuch patterns defines an abnormal event in a forest/tree depth profile. As faras the authors know, neither DR or FW-Ladar, along with several spectralmeasurements, has not been applied to anomaly detection. This work presents analgorithm that allows detection of spectral and temporal anomalies in FW-MultiSpectral Ladar (FW-MSL) data samples. An anomaly is defined as a full waveformtemporal and spectral signature that does not conform to a prior expectation,represented using a learnt subspace (dictionary) and set of coefficients thatcapture co-occurring local-patterns using an overlapping temporal window. Amodified optimization scheme is proposed for subspace learning based onstochastic approximations. The objective function is augmented with adiscriminative term that represents the subspace's separability properties andsupports anomaly characterisation. The algorithm detects several man-madeobjects and anomalous spectra hidden in a dense clutter of vegetation and alsoallows tree species classification.
arxiv-15600-270 | Doubly Robust Off-policy Value Evaluation for Reinforcement Learning | http://arxiv.org/pdf/1511.03722v2.pdf | author:Nan Jiang, Lihong Li category:cs.LG cs.AI cs.SY stat.ME stat.ML published:2015-11-11 summary:We study the problem of off-policy value evaluation in reinforcement learning(RL), where one aims to estimate the value of a new policy based on datacollected by a different policy. This problem is often a critical step whenapplying RL in real-world problems. Despite its importance, existing generalmethods either have uncontrolled bias or suffer high variance. In this work, weextend the doubly robust estimator for bandits to sequential decision-makingproblems, which gets the best of both worlds: it is guaranteed to be unbiasedand can have a much lower variance than the popular importance samplingestimators. We demonstrate the estimator's accuracy in several benchmarkproblems, and illustrate its use as a subroutine in safe policy improvement. Wealso provide theoretical results on the hardness of the problem, and show thatour estimator can match the lower bound in certain scenarios.
arxiv-15600-271 | 2D SEM images turn into 3D object models | http://arxiv.org/pdf/1602.05256v1.pdf | author:Wichai Shanklin category:cs.CV cs.GR published:2016-02-17 summary:The scanning electron microscopy (SEM) is probably one the most fascinatingexamination approach that has been used since more than two decades to detailedinspection of micro scale objects. Most of the scanning electron microscopescould only produce 2D images that could not assist operational analysis ofmicroscopic surface properties. Computer vision algorithms combined with veryadvanced geometry and mathematical approaches turn any SEM into a full 3Dmeasurement device. This work focuses on a methodical literature review forautomatic 3D surface reconstruction of scanning electron microscope images.
arxiv-15600-272 | Convolutional networks and learning invariant to homogeneous multiplicative scalings | http://arxiv.org/pdf/1506.08230v4.pdf | author:Mark Tygert, Arthur Szlam, Soumith Chintala, Marc'Aurelio Ranzato, Yuandong Tian, Wojciech Zaremba category:cs.LG cs.NE published:2015-06-26 summary:The conventional classification schemes -- notably multinomial logisticregression -- used in conjunction with convolutional networks (convnets) areclassical in statistics, designed without consideration for the usual couplingwith convnets, stochastic gradient descent, and backpropagation. In thespecific application to supervised learning for convnets, a simplescale-invariant classification stage turns out to be more robust thanmultinomial logistic regression, appears to result in slightly lower errors onseveral standard test sets, has similar computational costs, and featuresprecise control over the actual rate of learning. "Scale-invariant" means thatmultiplying the input values by any nonzero scalar leaves the output unchanged.
arxiv-15600-273 | A Sparse PCA Approach to Clustering | http://arxiv.org/pdf/1602.05236v1.pdf | author:T. Tony Cai, Linjun Zhang category:stat.ME stat.ML published:2016-02-16 summary:We discuss a clustering method for Gaussian mixture model based on the sparseprincipal component analysis (SPCA) method and compare it with the IF-PCAmethod. We also discuss the dependent case where the covariance matrix $\Sigma$is not necessarily diagonal.
arxiv-15600-274 | Primal-Dual Rates and Certificates | http://arxiv.org/pdf/1602.05205v1.pdf | author:Celestine Dünner, Simone Forte, Martin Takáč, Martin Jaggi category:cs.LG math.OC published:2016-02-16 summary:We propose an algorithm-independent framework to equip existing optimizationmethods with primal-dual certificates. Such certificates and corresponding rateof convergence guarantees are important for practitioners to diagnose progress,in particular in machine learning applications. We obtain new primal-dualconvergence rates e.g. for the Lasso as well as many L1, Elastic-Net andgroup-lasso-regularized problems. The theory applies to any norm-regularizedgeneralized linear model. Our approach provides efficiently computable dualitygaps which are globally defined, without modifying the original problems in theregion of interest.
arxiv-15600-275 | Support Consistency of Direct Sparse-Change Learning in Markov Networks | http://arxiv.org/pdf/1407.0581v10.pdf | author:Song Liu, Taiji Suzuki, Raissa Relator, Jun Sese, Masashi Sugiyama, Kenji Fukumizu category:stat.ML published:2014-07-02 summary:We study the problem of learning sparse structure changes between two Markovnetworks $P$ and $Q$. Rather than fitting two Markov networks separately to twosets of data and figuring out their differences, a recent work proposed tolearn changes \emph{directly} via estimating the ratio between two Markovnetwork models. In this paper, we give sufficient conditions for\emph{successful change detection} with respect to the sample size $n_p, n_q$,the dimension of data $m$, and the number of changed edges $d$. When using anunbounded density ratio model we prove that the true sparse changes can beconsistently identified for $n_p = \Omega(d^2 \log \frac{m^2+m}{2})$ and $n_q =\Omega({n_p^2})$, with an exponentially decaying upper-bound on learning error.Such sample complexity can be improved to $\min(n_p, n_q) = \Omega(d^2 \log\frac{m^2+m}{2})$ when the boundedness of the density ratio model is assumed.Our theoretical guarantee can be applied to a wide range of discrete/continuousMarkov networks.
arxiv-15600-276 | An Approach for Noise Removal on Depth Images | http://arxiv.org/pdf/1602.05168v1.pdf | author:Rashi Chaudhary, Himanshu Dasgupta category:cs.CV published:2016-02-16 summary:Image based rendering is a fundamental problem in computer vision andgraphics. Modern techniques often rely on depth image for the 3D construction.However for most of the existing depth cameras, the large and unpredictablenoises can be problematic, which can cause noticeable artifacts in the renderedresults. In this paper, we proposed an efficacious method for depth image noiseremoval that can be applied for most RGBD systems. The proposed solution willbenefit many subsequent vision problems such as 3D reconstruction, novel viewrendering, object recognition. Our experimental results demonstrate theefficacy and accuracy.
arxiv-15600-277 | Fast Learning Requires Good Memory: A Time-Space Lower Bound for Parity Learning | http://arxiv.org/pdf/1602.05161v1.pdf | author:Ran Raz category:cs.LG cs.CC cs.CR published:2016-02-16 summary:We prove that any algorithm for learning parities requires either a memory ofquadratic size or an exponential number of samples. This proves a recentconjecture of Steinhardt, Valiant and Wager and shows that for some learningproblems a large storage space is crucial. More formally, in the problem of parity learning, an unknown string $x \in\{0,1\}^n$ was chosen uniformly at random. A learner tries to learn $x$ from astream of samples $(a_1, b_1), (a_2, b_2) \ldots$, where each~$a_t$ isuniformly distributed over $\{0,1\}^n$ and $b_t$ is the inner product of $a_t$and $x$, modulo~2. We show that any algorithm for parity learning, that usesless than $\frac{n^2}{25}$ bits of memory, requires an exponential number ofsamples. Previously, there was no non-trivial lower bound on the number of samplesneeded, for any learning problem, even if the allowed memory size is $O(n)$(where $n$ is the space needed to store one sample). We also give an application of our result in the field of bounded-storagecryptography. We show an encryption scheme that requires a private key oflength $n$, as well as time complexity of $n$ per encryption/decription of eachbit, and is provenly and unconditionally secure as long as the attacker usesless than $\frac{n^2}{25}$ memory bits and the scheme is used at most anexponential number of times. Previous works on bounded-storage cryptographyassumed that the memory size used by the attacker is at most linear in the timeneeded for encryption/decription.
arxiv-15600-278 | Kernel Sequential Monte Carlo | http://arxiv.org/pdf/1510.03105v3.pdf | author:Ingmar Schuster, Heiko Strathmann, Brooks Paige, Dino Sejdinovic category:stat.CO stat.ML published:2015-10-11 summary:Bayesian posterior inference with Monte Carlo methods has a fundamental rolein statistics and probabilistic machine learning. Target posteriordistributions arising in increasingly complex models often exhibit high degreesof nonlinearity and multimodality and pose substantial challenges totraditional samplers. We propose the Kernel Sequential Monte Carlo (KSMC) framework for buildingemulator models of the current particle system in a Reproducing Kernel HilbertSpace and use the emulator's geometry to inform local proposals. KSMC isapplicable when gradients are unknown or prohibitively expensive and inheritsthe superior performance of SMC on multi-modal targets and its ability toestimate model evidence. Strengths of the proposed methodology are demonstratedon a series of challenging synthetic and real-world examples.
arxiv-15600-279 | Interacting Particle Markov Chain Monte Carlo | http://arxiv.org/pdf/1602.05128v1.pdf | author:Tom Rainforth, Christian A. Naesseth, Fredrik Lindsten, Brooks Paige, Jan-Willem van de Meent, Arnaud Doucet, Frank Wood category:stat.CO stat.ML published:2016-02-16 summary:We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMCmethod that introduces a coupling between multiple standard and conditionalsequential Monte Carlo samplers. Like related methods, iPMCMC is a Markov chainMonte Carlo sampler on an extended space. We present empirical results thatshow significant improvements in mixing rates relative to both non-interactingPMCMC samplers and a single PMCMC sampler with an equivalent totalcomputational budget. An additional advantage of the iPMCMC method is that itis suitable for distributed and multi-core architectures.
arxiv-15600-280 | A Harmonic Extension Approach for Collaborative Ranking | http://arxiv.org/pdf/1602.05127v1.pdf | author:Da Kuang, Zuoqiang Shi, Stanley Osher, Andrea Bertozzi category:cs.LG published:2016-02-16 summary:We present a new perspective on graph-based methods for collaborative rankingfor recommender systems. Unlike user-based or item-based methods that compute aweighted average of ratings given by the nearest neighbors, or low-rankapproximation methods using convex optimization and the nuclear norm, weformulate matrix completion as a series of semi-supervised learning problems,and propagate the known ratings to the missing ones on the user-user oritem-item graph globally. The semi-supervised learning problems are expressedas Laplace-Beltrami equations on a manifold, or namely, harmonic extension, andcan be discretized by a point integral method. We show that our approach doesnot impose a low-rank Euclidean subspace on the data points, but insteadminimizes the dimension of the underlying manifold. Our method, named LDM (lowdimensional manifold), turns out to be particularly effective in generatingrankings of items, showing decent computational efficiency and robust rankingquality compared to state-of-the-art methods.
arxiv-15600-281 | Practical Introduction to Clustering Data | http://arxiv.org/pdf/1602.05124v1.pdf | author:Alexander K. Hartmann category:astro-ph.IM cs.LG published:2016-02-16 summary:Data clustering is an approach to seek for structure in sets of complex data,i.e., sets of "objects". The main objective is to identify groups of objectswhich are similar to each other, e.g., for classification. Here, anintroduction to clustering is given and three basic approaches are introduced:the k-means algorithm, neighbour-based clustering, and an agglomerativeclustering method. For all cases, C source code examples are given, allowingfor an easy implementation.
arxiv-15600-282 | Convolutional Clustering for Unsupervised Learning | http://arxiv.org/pdf/1511.06241v2.pdf | author:Aysegul Dundar, Jonghoon Jin, Eugenio Culurciello category:cs.LG cs.CV published:2015-11-19 summary:The task of labeling data for training deep neural networks is daunting andtedious, requiring millions of labels to achieve the current state-of-the-artresults. Such reliance on large amounts of labeled data can be relaxed byexploiting hierarchical features via unsupervised learning techniques. In thiswork, we propose to train a deep convolutional network based on an enhancedversion of the k-means clustering algorithm, which reduces the number ofcorrelated parameters in the form of similar filters, and thus increases testcategorization accuracy. We call our algorithm convolutional k-meansclustering. We further show that learning the connection between the layers ofa deep convolutional neural network improves its ability to be trained on asmaller amount of labeled data. Our experiments show that the proposedalgorithm outperforms other techniques that learn filters unsupervised.Specifically, we obtained a test accuracy of 74.1% on STL-10 and a test errorof 0.5% on MNIST.
arxiv-15600-283 | Union of Low-Rank Subspaces Detector | http://arxiv.org/pdf/1307.7521v6.pdf | author:Mohsen Joneidi, Parvin Ahmadi, Mostafa Sadeghi, Nazanin Rahnavard category:cs.IT cs.CV math.IT published:2013-07-29 summary:The problem of signal detection using a flexible and general model isconsidered. Due to applicability and flexibility of sparse signalrepresentation and approximation, it has attracted a lot of attention in manysignal processing areas. In this paper, we propose a new detection method basedon sparse decomposition in a union of subspaces (UoS) model. Our proposeddetector uses a dictionary that can be interpreted as a bank of matchedsubspaces. This improves the performance of signal detection, as it is ageneralization for detectors. Low-rank assumption for the desired signalsimplies that the representations of these signals in terms of some proper baseswould be sparse. Our proposed detector exploits sparsity in its decision rule.We demonstrate the high efficiency of our method in the cases of voice activitydetection in speech processing.
arxiv-15600-284 | Deep Reinforcement Learning in Parameterized Action Space | http://arxiv.org/pdf/1511.04143v4.pdf | author:Matthew Hausknecht, Peter Stone category:cs.AI cs.LG cs.MA cs.NE published:2015-11-13 summary:Recent work has shown that deep neural networks are capable of approximatingboth value functions and policies in reinforcement learning domains featuringcontinuous state and action spaces. However, to the best of our knowledge noprevious work has succeeded at using deep neural networks in structured(parameterized) continuous action spaces. To fill this gap, this paper focuseson learning within the domain of simulated RoboCup soccer, which features asmall set of discrete action types, each of which is parameterized withcontinuous variables. The best learned agent can score goals more reliably thanthe 2012 RoboCup champion agent. As such, this paper represents a successfulextension of deep reinforcement learning to the class of parameterized actionspace MDPs.
arxiv-15600-285 | Limiting fitness distributions in evolutionary dynamics | http://arxiv.org/pdf/1511.00296v2.pdf | author:Matteo Smerlak, Ahmed Youssef category:q-bio.PE cs.NE published:2015-11-01 summary:Darwinian evolution can be modeled in general terms as a flow in the space offitness (i.e. reproductive rate) distributions. In the diffusion approximation,Tsimring et al. have showed that this flow admits "fitness wave" solutions:Gaussian-shape fitness distributions moving towards higher fitness values atconstant speed. Here we show more generally that evolving fitness distributionsare attracted to a one-parameter family of distributions with a fixed parabolicrelationship between skewness and kurtosis. Unlike fitness waves, thisstatistical pattern encompasses both positive and negative (a.k.a. purifying)selection and is not restricted to rapidly adapting populations. Moreover wefind that the mean fitness of a population under the selection of pre-existingvariation is a power-law function of time, as observed in microbiologicalevolution experiments but at variance with fitness wave theory. At theconceptual level, our results can be viewed as the resolution of the "dynamicinsufficiency" of Fisher's fundamental theorem of natural selection. Ourpredictions are in good agreement with numerical simulations.
arxiv-15600-286 | An Analytic Expression of Performance Rate, Fitness Value and Average Convergence Rate for a Class of Evolutionary Algorithms | http://arxiv.org/pdf/1511.03483v2.pdf | author:Jun He category:cs.NE published:2015-11-11 summary:An important theoretical question in evolutionary computation is how goodsolutions evolutionary algorithms can produce. This paper aims to provide ananalytic analysis of solution quality of evolutionary algorithms in terms ofthe performance rate, which is defined by the difference between 1 and theapproximation ratio of the best solution found in each generation. Theperformance rate can be represented by a function of time. With the help ofmatrix analysis, it is possible to obtain an exact expression of such afunction. For the first time, an analytic expression for calculating theperformance rate is presented in this paper for a class of evolutionaryalgorithms, that is, (1+1) strictly elitist evolution algorithms. Furthermore,analytic expressions for calculate the fitness value and the averageconvergence rate in each generation are also derived for this class ofevolutionary algorithms. The approach is promising, and it can be extended tonon-elitist or population-based algorithms too.
arxiv-15600-287 | A Subsequence Interleaving Model for Sequential Pattern Mining | http://arxiv.org/pdf/1602.05012v1.pdf | author:Jaroslav Fowkes, Charles Sutton category:stat.ML cs.AI cs.LG published:2016-02-16 summary:Recent sequential pattern mining methods have used the minimum descriptionlength (MDL) principle to define an encoding scheme which describes analgorithm for mining the most compressing patterns in a database. We present anovel subsequence interleaving model based on a probabilistic model of thesequence database, which allows us to search for the most compressing set ofpatterns without designing a specific encoding scheme. Our proposed algorithmis able to efficiently mine the most relevant sequential patterns and rank themusing an associated measure of interestingness. The efficient inference in ourmodel is a direct result of our use of a structural expectation-maximizationframework, in which the expectation-step takes the form of a submodularoptimization problem subject to a coverage constraint. We show on bothsynthetic and real world datasets that our model mines a set of sequentialpatterns with low spuriousness and redundancy, high interpretability andusefulness in real-world applications. Furthermore, we demonstrate that thequality of the patterns from our approach is comparable to, if not better than,existing state of the art sequential pattern mining algorithms.
arxiv-15600-288 | Cells in Multidimensional Recurrent Neural Networks | http://arxiv.org/pdf/1412.2620v2.pdf | author:G. Leifert, T. Strauß, T. Grüning, R. Labahn category:cs.AI cs.NE 68T10, 68T05 published:2014-12-08 summary:The transcription of handwritten text on images is one task in machinelearning and one solution to solve it is using multi-dimensional recurrentneural networks (MDRNN) with connectionist temporal classification (CTC). TheRNNs can contain special units, the long short-term memory (LSTM) cells. Theyare able to learn long term dependencies but they get unstable when thedimension is chosen greater than one. We defined some useful and necessaryproperties for the one-dimensional LSTM cell and extend them in themulti-dimensional case. Thereby we introduce several new cells with betterstability. We present a method to design cells using the theory of linear shiftinvariant systems. The new cells are compared to the LSTM cell on the IFN/ENITand Rimes database, where we can improve the recognition rate compared to theLSTM cell. So each application where the LSTM cells in MDRNNs are used could beimproved by substituting them by the new developed cells.
arxiv-15600-289 | Contextual Media Retrieval Using Natural Language Queries | http://arxiv.org/pdf/1602.04983v1.pdf | author:Sreyasi Nag Chowdhury, Mateusz Malinowski, Andreas Bulling, Mario Fritz category:cs.IR cs.AI cs.CL cs.CV cs.HC published:2016-02-16 summary:The widespread integration of cameras in hand-held and head-worn devices aswell as the ability to share content online enables a large and diverse visualcapture of the world that millions of users build up collectively every day. Weenvision these images as well as associated meta information, such as GPScoordinates and timestamps, to form a collective visual memory that can bequeried while automatically taking the ever-changing context of mobile usersinto account. As a first step towards this vision, in this work we presentXplore-M-Ego: a novel media retrieval system that allows users to query adynamic database of images and videos using spatio-temporal natural languagequeries. We evaluate our system using a new dataset of real user queries aswell as through a usability study. One key finding is that there is aconsiderable amount of inter-user variability, for example in the resolution ofspatial relations in natural language utterances. We show that our retrievalsystem can cope with this variability using personalisation through an onlinelearning-based retrieval formulation.
arxiv-15600-290 | Optimizing Gaze Direction in a Visual Navigation Task | http://arxiv.org/pdf/1602.04981v1.pdf | author:Tuomas Välimäki, Risto Ritala category:cs.RO cs.CV published:2016-02-16 summary:Navigation in an unknown environment consists of multiple separable subtasks,such as collecting information about the surroundings and navigating to thecurrent goal. In the case of pure visual navigation, all these subtasks need toutilize the same vision system, and therefore a way to optimally control thedirection of focus is needed. We present a case study, where we model theactive sensing problem of directing the gaze of a mobile robot with threemachine vision cameras as a partially observable Markov decision process(POMDP) using a mutual information (MI) based reward function. The key aspectof the solution is that the cameras are dynamically used either in monocular orstereo configuration. The benefits of using the proposed active sensingimplementation are demonstrated with simulations and experiments on a realrobot.
arxiv-15600-291 | Stochastic Process Bandits: Upper Confidence Bounds Algorithms via Generic Chaining | http://arxiv.org/pdf/1602.04976v1.pdf | author:Emile Contal, Nicolas Vayatis category:stat.ML cs.LG published:2016-02-16 summary:The paper considers the problem of global optimization in the setup ofstochastic process bandits. We introduce an UCB algorithm which builds acascade of discretization trees based on generic chaining in order to renderpossible his operability over a continuous domain. The theoretical frameworkapplies to functions under weak probabilistic smoothness assumptions and alsoextends significantly the spectrum of application of UCB strategies. Moreovergeneric regret bounds are derived which are then specialized to Gaussianprocesses indexed on infinite-dimensional spaces as well as to quadratic formsof Gaussian processes. Lower bounds are also proved in the case of Gaussianprocesses to assess the optimality of the proposed algorithm.
arxiv-15600-292 | Deep Learning-Based Image Kernel for Inductive Transfer | http://arxiv.org/pdf/1512.04086v3.pdf | author:Neeraj Kumar, Animesh Karmakar, Ranti Dev Sharma, Abhinav Mittal, Amit Sethi category:cs.CV published:2015-12-13 summary:We propose a method to classify images from target classes with a smallnumber of training examples based on transfer learning from non-target classes.Without using any more information than class labels for samples fromnon-target classes, we train a Siamese net to estimate the probability of twoimages to belong to the same class. With some post-processing, output of theSiamese net can be used to form a gram matrix of a Mercer kernel. Coupled witha support vector machine (SVM), such a kernel gave reasonable classificationaccuracy on target classes without any fine-tuning. When the Siamese net wasonly partially fine-tuned using a small number of samples from the targetclasses, the resulting classifier outperformed the state-of-the-art and otheralternatives. We share class separation capabilities and insights into thelearning process of such a kernel on MNIST, Dogs vs. Cats, and CIFAR-10datasets.
arxiv-15600-293 | Syntax Evolution: Problems and Recursion | http://arxiv.org/pdf/1508.03040v3.pdf | author:Ramón Casares category:cs.CL I.2.7; I.2.8 published:2015-08-12 summary:We are Turing complete, and natural language parsing is decidable, so oursyntactic abilities are in excess to those needed to speak a natural language.This is an anomaly, because evolution would not keep an overqualified featurefor long. We solve this anomaly by using a coincidence, both syntax and problemsolving are computing, and a difference, Turing completeness is not arequirement of syntax, but of problem solving. Then computing should have beenshaped by evolutionary requirements coming from both syntax and problemsolving, but the last one, Turing completeness, only from problem solving. Sowe propose and analyze a hypothesis: syntax and problem solving co-evolved inhumans towards Turing completeness. Finally, we argue that Turing completeness,also known as recursion, is our most singular feature.
arxiv-15600-294 | Q($λ$) with Off-Policy Corrections | http://arxiv.org/pdf/1602.04951v1.pdf | author:Anna Harutyunyan, Marc G. Bellemare, Tom Stepleton, Remi Munos category:cs.AI cs.LG stat.ML published:2016-02-16 summary:We propose and analyze an alternate approach to off-policy multi-steptemporal difference learning, in which off-policy returns are corrected withthe current Q-function in terms of rewards, rather than with the target policyin terms of transition probabilities. We prove that such approximatecorrections are sufficient for off-policy convergence both in policy evaluationand control, provided certain conditions. These conditions relate the distancebetween the target and behavior policies, the eligibility trace parameter andthe discount factor, and formalize an underlying tradeoff in off-policyTD($\lambda$). We illustrate this theoretical relationship empirically on acontinuous-state control task.
arxiv-15600-295 | "Why Should I Trust You?": Explaining the Predictions of Any Classifier | http://arxiv.org/pdf/1602.04938v1.pdf | author:Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin category:cs.LG cs.AI stat.ML published:2016-02-16 summary:Despite widespread adoption, machine learning models remain mostly blackboxes. Understanding the reasons behind predictions is, however, quiteimportant in assessing trust in a model. Trust is fundamental if one plans totake action based on a prediction, or when choosing whether or not to deploy anew model. Such understanding further provides insights into the model, whichcan be used to turn an untrustworthy model or prediction into a trustworthyone. In this work, we propose LIME, a novel explanation technique that explainsthe predictions of any classifier in an interpretable and faithful manner, bylearning an interpretable model locally around the prediction. We furtherpropose a method to explain models by presenting representative individualpredictions and their explanations in a non-redundant way, framing the task asa submodular optimization problem. We demonstrate the flexibility of thesemethods by explaining different models for text (e.g. random forests) and imageclassification (e.g. neural networks). The usefulness of explanations is shownvia novel experiments, both simulated and with human subjects. Our explanationsempower users in various scenarios that require trust: deciding if one shouldtrust a prediction, choosing between models, improving an untrustworthyclassifier, and detecting why a classifier should not be trusted.
arxiv-15600-296 | Greedy Ants Colony Optimization Strategy for Solving the Curriculum Based University Course Timetabling Problem | http://arxiv.org/pdf/1602.04933v1.pdf | author:Patrick Kenekayoro, Godswill Zipamone category:cs.NE published:2016-02-16 summary:Timetabling is a problem faced in all higher education institutions. TheInternational Timetabling Competition (ITC) has published a dataset that can beused to test the quality of methods used to solve this problem. A number ofmeta-heuristic approaches have obtained good results when tested on the ITCdataset, however few have used the ant colony optimization technique,particularly on the ITC 2007 curriculum based university course timetablingproblem. This study describes an ant system that solves the curriculum baseduniversity course timetabling problem and the quality of the algorithm istested on the ITC 2007 dataset. The ant system was able to find feasiblesolutions in all instances of the dataset and close to optimal solutions insome instances. The ant system performs better than some published approaches,however results obtained are not as good as those obtained by the bestpublished approaches. This study may be used as a benchmark for ant basedalgorithms that solve the curriculum based university course timetablingproblem.
arxiv-15600-297 | Loop-corrected belief propagation for lattice spin models | http://arxiv.org/pdf/1505.03504v3.pdf | author:Hai-Jun Zhou, Wei-Mou Zheng category:cs.CV published:2015-05-13 summary:Belief propagation (BP) is a message-passing method for solving probabilisticgraphical models. It is very successful in treating disordered models (such asspin glasses) on random graphs. On the other hand, finite-dimensional latticemodels have an abundant number of short loops, and the BP method is still farfrom being satisfactory in treating the complicated loop-induced correlationsin these systems. Here we propose a loop-corrected BP method to take intoaccount the effect of short loops in lattice spin models. We demonstrate,through an application to the square-lattice Ising model, that loop-correctedBP improves over the naive BP method significantly. We also implementloop-corrected BP at the coarse-grained region graph level to further boost itsperformance.
arxiv-15600-298 | Generalized minimum dominating set and application in automatic text summarization | http://arxiv.org/pdf/1602.04930v1.pdf | author:Yi-Zhi Xu, Hai-Jun Zhou category:cs.IR cs.CL physics.soc-ph published:2016-02-16 summary:For a graph formed by vertices and weighted edges, a generalized minimumdominating set (MDS) is a vertex set of smallest cardinality such that thesummed weight of edges from each outside vertex to vertices in this set isequal to or larger than certain threshold value. This generalized MDS problemreduces to the conventional MDS problem in the limiting case of all the edgeweights being equal to the threshold value. We treat the generalized MDSproblem in the present paper by a replica-symmetric spin glass theory andderive a set of belief-propagation equations. As a practical application weconsider the problem of extracting a set of sentences that best summarize agiven input text document. We carry out a preliminary test of the statisticalphysics-inspired method to this automatic text summarization problem.
arxiv-15600-299 | Efficient inference in occlusion-aware generative models of images | http://arxiv.org/pdf/1511.06362v2.pdf | author:Jonathan Huang, Kevin Murphy category:cs.LG cs.CV published:2015-11-19 summary:We present a generative model of images based on layering, in which imagelayers are individually generated, then composited from front to back. We arethus able to factor the appearance of an image into the appearance ofindividual objects within the image --- and additionally for each individualobject, we can factor content from pose. Unlike prior work on layered models,we learn a shape prior for each object/layer, allowing the model to tease outwhich object is in front by looking for a consistent shape, without needingaccess to motion cues or any labeled data. We show that ordinary stochasticgradient variational bayes (SGVB), which optimizes our fully differentiablelower-bound on the log-likelihood, is sufficient to learn an interpretablerepresentation of images. Finally we present experiments demonstrating theeffectiveness of the model for inferring foreground and background objects inimages.
arxiv-15600-300 | Personalized Federated Search at LinkedIn | http://arxiv.org/pdf/1602.04924v1.pdf | author:Dhruv Arya, Viet Ha-Thuc, Shakti Sinha category:cs.IR cs.LG published:2016-02-16 summary:LinkedIn has grown to become a platform hosting diverse sources ofinformation ranging from member profiles, jobs, professional groups, slideshowsetc. Given the existence of multiple sources, when a member issues a query like"software engineer", the member could look for software engineer profiles, jobsor professional groups. To tackle this problem, we exploit a data-drivenapproach that extracts searcher intents from their profile data and recentactivities at a large scale. The intents such as job seeking, hiring, contentconsuming are used to construct features to personalize federated searchexperience. We tested the approach on the LinkedIn homepage and A/B tests showsignificant improvements in member engagement. As of writing this paper, theapproach powers all of federated search on LinkedIn homepage.
