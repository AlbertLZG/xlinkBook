arxiv-9600-1 | Improved Error Bounds Based on Worst Likely Assignments | http://arxiv.org/pdf/1504.00052v1.pdf | author:Eric Bax category:stat.ML cs.IT cs.LG math.IT math.PR published:2015-03-31 summary:Error bounds based on worst likely assignments use permutation tests tovalidate classifiers. Worst likely assignments can produce effective boundseven for data sets with 100 or fewer training examples. This paper introduces astatistic for use in the permutation tests of worst likely assignments thatimproves error bounds, especially for accurate classifiers, which are typicallythe classifiers of interest.
arxiv-9600-2 | Weakly Supervised Learning of Objects, Attributes and their Associations | http://arxiv.org/pdf/1504.00045v1.pdf | author:Zhiyuan Shi, Yongxin Yang, Timothy M. Hospedales, Tao Xiang category:cs.CV published:2015-03-31 summary:When humans describe images they tend to use combinations of nouns andadjectives, corresponding to objects and their associated attributesrespectively. To generate such a description automatically, one needs to modelobjects, attributes and their associations. Conventional methods require strongannotation of object and attribute locations, making them less scalable. Inthis paper, we model object-attribute associations from weakly labelled images,such as those widely available on media sharing sites (e.g. Flickr), where onlyimage-level labels (either object or attributes) are given, without theirlocations and associations. This is achieved by introducing a novel weaklysupervised non-parametric Bayesian model. Once learned, given a new image, ourmodel can describe the image, including objects, attributes and theirassociations, as well as their locations and segmentation. Extensiveexperiments on benchmark datasets demonstrate that our weakly supervised modelperforms at par with strongly supervised models on tasks such as imagedescription and retrieval based on object-attribute associations.
arxiv-9600-3 | Real-World Font Recognition Using Deep Network and Domain Adaptation | http://arxiv.org/pdf/1504.00028v1.pdf | author:Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang category:cs.CV cs.LG published:2015-03-31 summary:We address a challenging fine-grain classification problem: recognizing afont style from an image of text. In this task, it is very easy to generatelots of rendered font examples but very hard to obtain real-world labeledimages. This real-to-synthetic domain gap caused poor generalization to newreal data in previous methods (Chen et al. (2014)). In this paper, we refer toConvolutional Neural Networks, and use an adaptation technique based on aStacked Convolutional Auto-Encoder that exploits unlabeled real-world imagescombined with synthetic data. The proposed method achieves an accuracy ofhigher than 80% (top-5) on a real-world dataset.
arxiv-9600-4 | Example Selection For Dictionary Learning | http://arxiv.org/pdf/1412.6177v3.pdf | author:Tomoki Tsuchida, Garrison W. Cottrell category:cs.LG cs.AI stat.ML published:2014-12-18 summary:In unsupervised learning, an unbiased uniform sampling strategy is typicallyused, in order that the learned features faithfully encode the statisticalstructure of the training data. In this work, we explore whether active exampleselection strategies - algorithms that select which examples to use, based onthe current estimate of the features - can accelerate learning. Specifically,we investigate effects of heuristic and saliency-inspired selection algorithmson the dictionary learning task with sparse activations. We show that someselection algorithms do improve the speed of learning, and we speculate on whythey might work.
arxiv-9600-5 | Towards Using Machine Translation Techniques to Induce Multilingual Lexica of Discourse Markers | http://arxiv.org/pdf/1503.09144v1.pdf | author:António Lopes, David Martins de Matos, Vera Cabarrão, Ricardo Ribeiro, Helena Moniz, Isabel Trancoso, Ana Isabel Mata category:cs.CL I.2.7 published:2015-03-31 summary:Discourse markers are universal linguistic events subject to languagevariation. Although an extensive literature has already reported languagespecific traits of these events, little has been said on their cross-languagebehavior and on building an inventory of multilingual lexica of discoursemarkers. This work describes new methods and approaches for the description,classification, and annotation of discourse markers in the specific domain ofthe Europarl corpus. The study of discourse markers in the context oftranslation is crucial due to the idiomatic nature of these structures.Multilingual lexica together with the functional analysis of such structuresare useful tools for the hard task of translating discourse markers intopossible equivalents from one language to another. Using Daniel Marcu'svalidated discourse markers for English, extracted from the Brown Corpus, ourpurpose is to build multilingual lexica of discourse markers for otherlanguages, based on machine translation techniques. The major assumption inthis study is that the usage of a discourse marker is independent of thelanguage, i.e., the rhetorical function of a discourse marker in a sentence inone language is equivalent to the rhetorical function of the same discoursemarker in another language.
arxiv-9600-6 | Encoding Spike Patterns in Multilayer Spiking Neural Networks | http://arxiv.org/pdf/1503.09129v1.pdf | author:Brian Gardner, Ioana Sporea, André Grüning category:cs.NE published:2015-03-31 summary:Information encoding in the nervous system is supported through the precisespike-timings of neurons; however, an understanding of the underlying processesby which such representations are formed in the first place remains unclear.Here we examine how networks of spiking neurons can learn to encode for inputpatterns using a fully temporal coding scheme. To this end, we introduce alearning rule for spiking networks containing hidden neurons which optimizesthe likelihood of generating desired output spiking patterns. We show theproposed learning rule allows for a large number of accurate input-output spikepattern mappings to be learnt, which outperforms other existing learning rulesfor spiking neural networks: both in the number of mappings that can be learntas well as the complexity of spike train encodings that can be utilised. Thelearning rule is successful even in the presence of input noise, isdemonstrated to solve the linearly non-separable XOR computation andgeneralizes well on an example dataset. We further present a biologicallyplausible implementation of backpropagated learning in multilayer spikingnetworks, and discuss the neural mechanisms that might underlie its function.Our approach contributes both to a systematic understanding of how patternencodings might take place in the nervous system, and a learning rule thatdisplays strong technical capability.
arxiv-9600-7 | Denoising autoencoder with modulated lateral connections learns invariant representations of natural images | http://arxiv.org/pdf/1412.7210v4.pdf | author:Antti Rasmus, Tapani Raiko, Harri Valpola category:cs.NE cs.CV cs.LG stat.ML published:2014-12-22 summary:Suitable lateral connections between encoder and decoder are shown to allowhigher layers of a denoising autoencoder (dAE) to focus on invariantrepresentations. In regular autoencoders, detailed information needs to becarried through the highest layers but lateral connections from encoder todecoder relieve this pressure. It is shown that abstract invariant features canbe translated to detailed reconstructions when invariant features are allowedto modulate the strength of the lateral connection. Three dAE structures withmodulated and additive lateral connections, and without lateral connectionswere compared in experiments using real-world images. The experiments verifythat adding modulated lateral connections to the model 1) improves the accuracyof the probability model for inputs, as measured by denoising performance; 2)results in representations whose degree of invariance grows faster towards thehigher layers; and 3) supports the formation of diverse invariant poolings.
arxiv-9600-8 | ROML: A Robust Feature Correspondence Approach for Matching Objects in A Set of Images | http://arxiv.org/pdf/1403.7877v2.pdf | author:Kui Jia, Tsung-Han Chan, Zinan Zeng, Shenghua Gao, Gang Wang, Tianzhu Zhang, Yi Ma category:cs.CV published:2014-03-31 summary:Feature-based object matching is a fundamental problem for many applicationsin computer vision, such as object recognition, 3D reconstruction, tracking,and motion segmentation. In this work, we consider simultaneously matchingobject instances in a set of images, where both inlier and outlier features areextracted. The task is to identify the inlier features and establish theirconsistent correspondences across the image set. This is a challengingcombinatorial problem, and the problem complexity grows exponentially with theimage number. To this end, we propose a novel framework, termed ROML, toaddress this problem. ROML optimizes simultaneously a partial permutationmatrix (PPM) for each image, and feature correspondences are established by theobtained PPMs. Two of our key contributions are summarized as follows. (1) Weformulate the problem as rank and sparsity minimization for PPM optimization,and treat simultaneous optimization of multiple PPMs as a regularized consensusproblem in the context of distributed optimization. (2) We use the ADMM methodto solve the thus formulated ROML problem, in which a subproblem associatedwith a single PPM optimization appears to be a difficult integer quadraticprogram (IQP). We prove that under wildly applicable conditions, this IQP isequivalent to a linear sum assignment problem (LSAP), which can be efficientlysolved to an exact solution. Extensive experiments on rigid/non-rigid objectmatching, matching instances of a common object category, and common objectlocalization show the efficacy of our proposed method.
arxiv-9600-9 | Algorithmic Robustness for Learning via $(ε, γ, τ)$-Good Similarity Functions | http://arxiv.org/pdf/1412.6452v3.pdf | author:Maria-Irina Nicolae, Marc Sebban, Amaury Habrard, Éric Gaussier, Massih-Reza Amini category:cs.LG published:2014-12-19 summary:The notion of metric plays a key role in machine learning problems such asclassification, clustering or ranking. However, it is worth noting that thereis a severe lack of theoretical guarantees that can be expected on thegeneralization capacity of the classifier associated to a given metric. Thetheoretical framework of $(\epsilon, \gamma, \tau)$-good similarity functions(Balcan et al., 2008) has been one of the first attempts to draw a link betweenthe properties of a similarity function and those of a linear classifier makinguse of it. In this paper, we extend and complete this theory by providing a newgeneralization bound for the associated classifier based on the algorithmicrobustness framework.
arxiv-9600-10 | Thompson Sampling for Learning Parameterized Markov Decision Processes | http://arxiv.org/pdf/1406.7498v3.pdf | author:Aditya Gopalan, Shie Mannor category:stat.ML cs.LG published:2014-06-29 summary:We consider reinforcement learning in parameterized Markov Decision Processes(MDPs), where the parameterization may induce correlation across transitionprobabilities or rewards. Consequently, observing a particular state transitionmight yield useful information about other, unobserved, parts of the MDP. Wepresent a version of Thompson sampling for parameterized reinforcement learningproblems, and derive a frequentist regret bound for priors over generalparameter spaces. The result shows that the number of instants where suboptimalactions are chosen scales logarithmically with time, with high probability. Itholds for prior distributions that put significant probability near the truemodel, without any additional, specific closed-form structure such as conjugateor product-form priors. The constant factor in the logarithmic scaling encodesthe information complexity of learning the MDP in terms of the Kullback-Leiblergeometry of the parameter space.
arxiv-9600-11 | Fast Label Embeddings for Extremely Large Output Spaces | http://arxiv.org/pdf/1503.08873v1.pdf | author:Paul Mineiro, Nikos Karampatziakis category:cs.LG published:2015-03-30 summary:Many modern multiclass and multilabel problems are characterized byincreasingly large output spaces. For these problems, label embeddings havebeen shown to be a useful primitive that can improve computational andstatistical efficiency. In this work we utilize a correspondence between rankconstrained estimation and low dimensional label embeddings that uncovers afast label embedding algorithm which works in both the multiclass andmultilabel settings. The result is a randomized algorithm for partial leastsquares, whose running time is exponentially faster than naive algorithms. Wedemonstrate our techniques on two large-scale public datasets, from the LargeScale Hierarchical Text Challenge and the Open Directory Project, where weobtain state of the art results.
arxiv-9600-12 | Decentralized learning for wireless communications and networking | http://arxiv.org/pdf/1503.08855v1.pdf | author:Georgios B. Giannakis, Qing Ling, Gonzalo Mateos, Ioannis D. Schizas, Hao Zhu category:math.OC cs.IT cs.LG cs.MA cs.SY math.IT stat.ML published:2015-03-30 summary:This chapter deals with decentralized learning algorithms for in-networkprocessing of graph-valued data. A generic learning problem is formulated andrecast into a separable form, which is iteratively minimized using thealternating-direction method of multipliers (ADMM) so as to gain the desireddegree of parallelization. Without exchanging elements from the distributedtraining sets and keeping inter-node communications at affordable levels, thelocal (per-node) learners consent to the desired quantity inferred globally,meaning the one obtained if the entire training data set were centrallyavailable. Impact of the decentralized learning framework to contemporarywireless communications and networking tasks is illustrated through casestudies including target tracking using wireless sensor networks, unveilingInternet traffic anomalies, power system state estimation, as well as spectrumcartography for wireless cognitive radio networks.
arxiv-9600-13 | Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations | http://arxiv.org/pdf/1503.08853v1.pdf | author:Ali Borji, James Tanner category:cs.CV published:2015-03-30 summary:Predicting where people look in natural scenes has attracted a lot ofinterest in computer vision and computational neuroscience over the past twodecades. Two seemingly contrasting categories of cues have been proposed toinfluence where people look: \textit{low-level image saliency} and\textit{high-level semantic information}. Our first contribution is to take adetailed look at these cues to confirm the hypothesis proposed byHenderson~\cite{henderson1993eye} and Nuthmann \&Henderson~\cite{nuthmann2010object} that observers tend to look at the centerof objects. We analyzed fixation data for scene free-viewing over 17 observerson 60 fully annotated images with various types of objects. Images containeddifferent types of scenes, such as natural scenes, line drawings, and 3Drendered scenes. Our second contribution is to propose a simple combined modelof low-level saliency and object center-bias that outperforms each individualcomponent significantly over our data, as well as on the OSIE dataset by Xu etal.~\cite{xu2014predicting}. The results reconcile saliency with objectcenter-bias hypotheses and highlight that both types of cues are important inguiding fixations. Our work opens new directions to understand strategies thathumans use in observing scenes and objects, and demonstrates the constructionof combined models of low-level saliency and high-level object-basedinformation.
arxiv-9600-14 | Globally Tuned Cascade Pose Regression via Back Propagation with Application in 2D Face Pose Estimation and Heart Segmentation in 3D CT Images | http://arxiv.org/pdf/1503.08843v1.pdf | author:Peng Sun, James K. Min, Guanglei Xiong category:cs.CV published:2015-03-30 summary:Recently, a successful pose estimation algorithm, called Cascade PoseRegression (CPR), was proposed in the literature. Trained over Pose IndexFeature, CPR is a regressor ensemble that is similar to Boosting. In this paperwe show how CPR can be represented as a Neural Network. Specifically, we adopta Graph Transformer Network (GTN) representation and accordingly train CPR withBack Propagation (BP) that permits globally tuning. In contrast, previous CPRliterature only took a layer wise training without any post fine tuning. Weempirically show that global training with BP outperforms layer-wise(pre-)training. Our CPR-GTN adopts a Multi Layer Percetron as the regressor,which utilized sparse connection to learn local image feature representation.We tested the proposed CPR-GTN on 2D face pose estimation problem as inprevious CPR literature. Besides, we also investigated the possibility ofextending CPR-GTN to 3D pose estimation by doing experiments using 3D ComputedTomography dataset for heart segmentation.
arxiv-9600-15 | Deep Hierarchical Parsing for Semantic Segmentation | http://arxiv.org/pdf/1503.02725v2.pdf | author:Abhishek Sharma, Oncel Tuzel, David W. Jacobs category:cs.CV published:2015-03-09 summary:This paper proposes a learning-based approach to scene parsing inspired bythe deep Recursive Context Propagation Network (RCPN). RCPN is a deepfeed-forward neural network that utilizes the contextual information from theentire image, through bottom-up followed by top-down context propagation viarandom binary parse trees. This improves the feature representation of everysuper-pixel in the image for better classification into semantic categories. Weanalyze RCPN and propose two novel contributions to further improve the model.We first analyze the learning of RCPN parameters and discover the presence ofbypass error paths in the computation graph of RCPN that can hinder contextualpropagation. We propose to tackle this problem by including the classificationloss of the internal nodes of the random parse trees in the original RCPN lossfunction. Secondly, we use an MRF on the parse tree nodes to model thehierarchical dependency present in the output. Both modifications provideperformance boosts over the original RCPN and the new system achievesstate-of-the-art performance on Stanford Background, SIFT-Flow and Daimlerurban datasets.
arxiv-9600-16 | Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High Dimensional Results | http://arxiv.org/pdf/1206.0823v2.pdf | author:Yudong Chen, Constantine Caramanis category:math.ST cs.IT math.IT stat.ML stat.TH published:2012-06-05 summary:Many models for sparse regression typically assume that the covariates areknown completely, and without noise. Particularly in high-dimensionalapplications, this is often not the case. This paper develops efficientOMP-like algorithms to deal with precisely this setting. Our algorithms are asefficient as OMP, and improve on the best-known results for missing and noisydata in regression, both in the high-dimensional setting where we seek torecover a sparse vector from only a few measurements, and in the classicallow-dimensional setting where we recover an unstructured regressor. In thehigh-dimensional setting, our support-recovery algorithm requires no knowledgeof even the statistics of the noise. Along the way, we also obtain improvedperformance guarantees for OMP for the standard sparse regression problem withGaussian noise.
arxiv-9600-17 | Online Algorithms for Factorization-Based Structure from Motion | http://arxiv.org/pdf/1309.6964v3.pdf | author:Ryan Kennedy, Laura Balzano, Stephen J. Wright, Camillo J. Taylor category:cs.CV published:2013-09-26 summary:We present a family of online algorithms for real-time factorization-basedstructure from motion, leveraging a relationship between incremental singularvalue decomposition and recently proposed methods for online matrix completion.Our methods are orders of magnitude faster than previous state of the art, canhandle missing data and a variable number of feature points, and are robust tonoise and sparse outliers. We demonstrate our methods on both real andsynthetic sequences and show that they perform well in both online and batchsettings. We also provide an implementation which is able to produce 3D modelsin real time using a laptop with a webcam.
arxiv-9600-18 | A Parzen-based distance between probability measures as an alternative of summary statistics in Approximate Bayesian Computation | http://arxiv.org/pdf/1503.08727v1.pdf | author:Carlos D. Zuluaga, Edgar A. Valencia, Mauricio A. Álvarez category:stat.ML published:2015-03-30 summary:Approximate Bayesian Computation (ABC) are likelihood-free Monte Carlomethods. ABC methods use a comparison between simulated data, using differentparameters drew from a prior distribution, and observed data. This comparisonprocess is based on computing a distance between the summary statistics fromthe simulated data and the observed data. For complex models, it is usuallydifficult to define a methodology for choosing or constructing the summarystatistics. Recently, a nonparametric ABC has been proposed, that uses adissimilarity measure between discrete distributions based on empirical kernelembeddings as an alternative for summary statistics. The nonparametric ABCoutperforms other methods including ABC, kernel ABC or synthetic likelihoodABC. However, it assumes that the probability distributions are discrete, andit is not robust when dealing with few observations. In this paper, we proposeto apply kernel embeddings using an smoother density estimator or Parzenestimator for comparing the empirical data distributions, and computing the ABCposterior. Synthetic data and real data were used to test the Bayesianinference of our method. We compare our method with respect to state-of-the-artmethods, and demonstrate that our method is a robust estimator of the posteriordistribution in terms of the number of observations.
arxiv-9600-19 | Regression with Linear Factored Functions | http://arxiv.org/pdf/1412.6286v3.pdf | author:Wendelin Böhmer, Klaus Obermayer category:cs.LG stat.ML published:2014-12-19 summary:Many applications that use empirically estimated functions face a curse ofdimensionality, because the integrals over most function classes must beapproximated by sampling. This paper introduces a novel regression-algorithmthat learns linear factored functions (LFF). This class of functions hasstructural properties that allow to analytically solve certain integrals and tocalculate point-wise products. Applications like belief propagation andreinforcement learning can exploit these properties to break the curse andspeed up computation. We derive a regularized greedy optimization scheme, thatlearns factored basis functions during training. The novel regression algorithmperforms competitively to Gaussian processes on benchmark tasks, and thelearned LFF functions are with 4-9 factored basis functions on average verycompact.
arxiv-9600-20 | Sparse plus low-rank autoregressive identification in neuroimaging time series | http://arxiv.org/pdf/1503.08639v1.pdf | author:Raphaël Liégeois, Bamdev Mishra, Mattia Zorzi, Rodolphe Sepulchre category:cs.LG cs.SY published:2015-03-30 summary:This paper considers the problem of identifying multivariate autoregressive(AR) sparse plus low-rank graphical models. Based on the corresponding problemformulation recently presented, we use the alternating direction method ofmultipliers (ADMM) to efficiently solve it and scale it to sizes encountered inneuroimaging applications. We apply this decomposition on synthetic and realneuroimaging datasets with a specific focus on the information encoded in thelow-rank structure of our model. In particular, we illustrate that thisinformation captures the spatio-temporal structure of the original data,generalizing classical component analysis approaches.
arxiv-9600-21 | Speech Recognition Front End Without Information Loss | http://arxiv.org/pdf/1312.6849v2.pdf | author:Matthew Ager, Zoran Cvetkovic, Peter Sollich category:cs.CL cs.CV cs.LG published:2013-12-24 summary:Speech representation and modelling in high-dimensional spaces of acousticwaveforms, or a linear transformation thereof, is investigated with the aim ofimproving the robustness of automatic speech recognition to additive noise. Themotivation behind this approach is twofold: (i) the information in acousticwaveforms that is usually removed in the process of extracting low-dimensionalfeatures might aid robust recognition by virtue of structured redundancyanalogous to channel coding, (ii) linear feature domains allow for exact noiseadaptation, as opposed to representations that involve non-linear processingwhich makes noise adaptation challenging. Thus, we develop a generativeframework for phoneme modelling in high-dimensional linear feature domains, anduse it in phoneme classification and recognition tasks. Results show thatclassification and recognition in this framework perform better than analogousPLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensionaland MFCC features at the likelihood level performs uniformly better than eitherof the individual representations across all noise levels.
arxiv-9600-22 | LSHTC: A Benchmark for Large-Scale Text Classification | http://arxiv.org/pdf/1503.08581v1.pdf | author:Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artieres, George Paliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza Amini, Patrick Galinari category:cs.IR cs.CL cs.LG published:2015-03-30 summary:LSHTC is a series of challenges which aims to assess the performance ofclassification systems in large-scale classification in a a large number ofclasses (up to hundreds of thousands). This paper describes the dataset thathave been released along the LSHTC series. The paper details the constructionof the datsets and the design of the tracks as well as the evaluation measuresthat we implemented and a quick overview of the results. All of these datasetsare available online and runs may still be submitted on the online server ofthe challenges.
arxiv-9600-23 | Normalization of Non-Standard Words in Croatian Texts | http://arxiv.org/pdf/1503.08167v2.pdf | author:Slobodan Beliga, Miran Pobar, Sanda Martinčić-Ipšić category:cs.CL published:2015-03-27 summary:This paper presents text normalization which is an integral part of anytext-to-speech synthesis system. Text normalization is a set of methods with atask to write non-standard words, like numbers, dates, times, abbreviations,acronyms and the most common symbols, in their full expanded form arepresented. The whole taxonomy for classification of non-standard words inCroatian language together with rule-based normalization methods combined witha lookup dictionary are proposed. Achieved token rate for normalization ofCroatian texts is 95%, where 80% of expanded words are in correct morphologicalform.
arxiv-9600-24 | Optimal Bounds on Approximation of Submodular and XOS Functions by Juntas | http://arxiv.org/pdf/1307.3301v3.pdf | author:Vitaly Feldman, Jan Vondrak category:cs.DS cs.CC cs.LG published:2013-07-12 summary:We investigate the approximability of several classes of real-valuedfunctions by functions of a small number of variables ({\em juntas}). Our mainresults are tight bounds on the number of variables required to approximate afunction $f:\{0,1\}^n \rightarrow [0,1]$ within $\ell_2$-error $\epsilon$ overthe uniform distribution: 1. If $f$ is submodular, then it is $\epsilon$-closeto a function of $O(\frac{1}{\epsilon^2} \log \frac{1}{\epsilon})$ variables.This is an exponential improvement over previously known results. We note that$\Omega(\frac{1}{\epsilon^2})$ variables are necessary even for linearfunctions. 2. If $f$ is fractionally subadditive (XOS) it is $\epsilon$-closeto a function of $2^{O(1/\epsilon^2)}$ variables. This result holds for allfunctions with low total $\ell_1$-influence and is a real-valued analogue ofFriedgut's theorem for boolean functions. We show that $2^{\Omega(1/\epsilon)}$variables are necessary even for XOS functions. As applications of these results, we provide learning algorithms over theuniform distribution. For XOS functions, we give a PAC learning algorithm thatruns in time $2^{poly(1/\epsilon)} poly(n)$. For submodular functions we givean algorithm in the more demanding PMAC learning model (Balcan and Harvey,2011) which requires a multiplicative $1+\gamma$ factor approximation withprobability at least $1-\epsilon$ over the target distribution. Our uniformdistribution algorithm runs in time $2^{poly(1/(\gamma\epsilon))} poly(n)$.This is the first algorithm in the PMAC model that over the uniformdistribution can achieve a constant approximation factor arbitrarily close to 1for all submodular functions. As follows from the lower bounds in (Feldman etal., 2013) both of these algorithms are close to optimal. We also giveapplications for proper learning, testing and agnostic learning with valuequeries of these classes.
arxiv-9600-25 | Nonparametric Relational Topic Models through Dependent Gamma Processes | http://arxiv.org/pdf/1503.08542v1.pdf | author:Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo category:stat.ML cs.CL cs.IR cs.LG published:2015-03-30 summary:Traditional Relational Topic Models provide a way to discover the hiddentopics from a document network. Many theoretical and practical tasks, such asdimensional reduction, document clustering, link prediction, benefit from thisrevealed knowledge. However, existing relational topic models are based on anassumption that the number of hidden topics is known in advance, and this isimpractical in many real-world applications. Therefore, in order to relax thisassumption, we propose a nonparametric relational topic model in this paper.Instead of using fixed-dimensional probability distributions in its generativemodel, we use stochastic processes. Specifically, a gamma process is assignedto each document, which represents the topic interest of this document.Although this method provides an elegant solution, it brings additionalchallenges when mathematically modeling the inherent network structure oftypical document network, i.e., two spatially closer documents tend to havemore similar topics. Furthermore, we require that the topics are shared by allthe documents. In order to resolve these challenges, we use a subsamplingstrategy to assign each document a different gamma process from the globalgamma process, and the subsampling probabilities of documents are assigned witha Markov Random Field constraint that inherits the document network structure.Through the designed posterior inference algorithm, we can discover the hiddentopics and its number simultaneously. Experimental results on both syntheticand real-world network datasets demonstrate the capabilities of learning thehidden topics and, more importantly, the number of topics.
arxiv-9600-26 | Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process | http://arxiv.org/pdf/1503.08535v1.pdf | author:Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo category:stat.ML cs.IR cs.LG published:2015-03-30 summary:Incorporating the side information of text corpus, i.e., authors, timestamps, and emotional tags, into the traditional text mining models has gainedsignificant interests in the area of information retrieval, statistical naturallanguage processing, and machine learning. One branch of these works is theso-called Author Topic Model (ATM), which incorporates the authors's interestsas side information into the classical topic model. However, the existing ATMneeds to predefine the number of topics, which is difficult and inappropriatein many real-world settings. In this paper, we propose an Infinite Author Topic(IAT) model to resolve this issue. Instead of assigning a discrete probabilityon fixed number of topics, we use a stochastic process to determine the numberof topics from the data itself. To be specific, we extend a gamma-negativebinomial process to three levels in order to capture theauthor-document-keyword hierarchical structure. Furthermore, each document isassigned a mixed gamma process that accounts for the multi-author'scontribution towards this document. An efficient Gibbs sampling inferencealgorithm with each conditional distribution being closed-form is developed forthe IAT model. Experiments on several real-world datasets show the capabilitiesof our IAT model to learn the hidden topics, authors' interests on these topicsand the number of topics simultaneously.
arxiv-9600-27 | EmoNets: Multimodal deep learning approaches for emotion recognition in video | http://arxiv.org/pdf/1503.01800v2.pdf | author:Samira Ebrahimi Kahou, Xavier Bouthillier, Pascal Lamblin, Caglar Gulcehre, Vincent Michalski, Kishore Konda, Sébastien Jean, Pierre Froumenty, Yann Dauphin, Nicolas Boulanger-Lewandowski, Raul Chandias Ferrari, Mehdi Mirza, David Warde-Farley, Aaron Courville, Pascal Vincent, Roland Memisevic, Christopher Pal, Yoshua Bengio category:cs.LG cs.CV published:2015-03-05 summary:The task of the emotion recognition in the wild (EmotiW) Challenge is toassign one of seven emotions to short video clips extracted from Hollywoodstyle movies. The videos depict acted-out emotions under realistic conditionswith a large degree of variation in attributes such as pose and illumination,making it worthwhile to explore approaches which consider combinations offeatures from multiple modalities for label assignment. In this paper wepresent our approach to learning several specialist models using deep learningtechniques, each focusing on one modality. Among these are a convolutionalneural network, focusing on capturing visual information in detected faces, adeep belief net focusing on the representation of the audio stream, a K-Meansbased "bag-of-mouths" model, which extracts visual features around the mouthregion and a relational autoencoder, which addresses spatio-temporal aspects ofvideos. We explore multiple methods for the combination of cues from thesemodalities into one common classifier. This achieves a considerably greateraccuracy than predictions from our strongest single-modality classifier. Ourmethod was the winning submission in the 2013 EmotiW challenge and achieved atest set accuracy of 47.67% on the 2014 dataset.
arxiv-9600-28 | Active Authentication on Mobile Devices via Stylometry, Application Usage, Web Browsing, and GPS Location | http://arxiv.org/pdf/1503.08479v1.pdf | author:Lex Fridman, Steven Weber, Rachel Greenstadt, Moshe Kam category:cs.CR stat.ML published:2015-03-29 summary:Active authentication is the problem of continuously verifying the identityof a person based on behavioral aspects of their interaction with a computingdevice. In this study, we collect and analyze behavioral biometrics data from200subjects, each using their personal Android mobile device for a period of atleast 30 days. This dataset is novel in the context of active authenticationdue to its size, duration, number of modalities, and absence of restrictions ontracked activity. The geographical colocation of the subjects in the study isrepresentative of a large closed-world environment such as an organizationwhere the unauthorized user of a device is likely to be an insider threat:coming from within the organization. We consider four biometric modalities: (1)text entered via soft keyboard, (2) applications used, (3) websites visited,and (4) physical location of the device as determined from GPS (when outdoors)or WiFi (when indoors). We implement and test a classifier for each modalityand organize the classifiers as a parallel binary decision fusion architecture.We are able to characterize the performance of the system with respect tointruder detection time and to quantify the contribution of each modality tothe overall performance.
arxiv-9600-29 | A simple coding for cross-domain matching with dimension reduction via spectral graph embedding | http://arxiv.org/pdf/1412.8380v2.pdf | author:Hidetoshi Shimodaira category:stat.ML cs.CV cs.LG published:2014-12-29 summary:Data vectors are obtained from multiple domains. They are feature vectors ofimages or vector representations of words. Domains may have different numbersof data vectors with different dimensions. These data vectors from multipledomains are projected to a common space by linear transformations in order tosearch closely related vectors across domains. We would like to find projectionmatrices to minimize distances between closely related data vectors. Thisformulation of cross-domain matching is regarded as an extension of thespectral graph embedding to multi-domain setting, and it includes severalmultivariate analysis methods of statistics such as multiset canonicalcorrelation analysis, correspondence analysis, and principal componentanalysis. Similar approaches are very popular recently in pattern recognitionand vision. In this paper, instead of proposing a novel method, we willintroduce an embarrassingly simple idea of coding the data vectors forexplaining all the above mentioned approaches. A data vector is concatenatedwith zero vectors from all other domains to make an augmented vector. Thecross-domain matching is solved by applying the single-domain version ofspectral graph embedding to these augmented vectors of all the domains. Aninteresting connection to the classical associative memory model of neuralnetworks is also discussed by noticing a coding for association. Across-validation method for choosing the dimension of the common space and aregularization parameter will be discussed in an illustrative numericalexample.
arxiv-9600-30 | Founding Digital Currency on Imprecise Commodity | http://arxiv.org/pdf/1503.08818v1.pdf | author:Zimu Yuan, Zhiwei Xu category:cs.CY cs.LG published:2015-03-29 summary:Current digital currency schemes provide instantaneous exchange on precisecommodity, in which "precise" means a buyer can possibly verify the function ofthe commodity without error. However, imprecise commodities, e.g. statisticaldata, with error existing are abundant in digital world. Existing digitalcurrency schemes do not offer a mechanism to help the buyer for paymentdecision on precision of commodity, which may lead the buyer to a dilemmabetween having to buy and being unconfident. In this paper, we design acurrency schemes IDCS for imprecise digital commodity. IDCS completes a tradein three stages of handshake between a buyer and providers. We present an IDCSprototype implementation that assigns weights on the trustworthy of theproviders, and calculates a confidence level for the buyer to decide thequality of a imprecise commodity. In experiment, we characterize theperformance of IDCS prototype under varying impact factors.
arxiv-9600-31 | Maximum Entropy, Word-Frequency, Chinese Characters, and Multiple Meanings | http://arxiv.org/pdf/1402.1939v2.pdf | author:Xiao-Yong Yan, Petter Minnhagen category:physics.soc-ph cs.CL published:2014-02-09 summary:The word-frequency distribution of a text written by an author is wellaccounted for by a maximum entropy distribution, the RGF (random groupformation)-prediction. The RGF-distribution is completely determined by the apriori values of the total number of words in the text (M), the number ofdistinct words (N) and the number of repetitions of the most common word(k_max). It is here shown that this maximum entropy prediction also describes atext written in Chinese characters. In particular it is shown that although thesame Chinese text written in words and Chinese characters have quitedifferently shaped distributions, they are nevertheless both well predicted bytheir respective three a priori characteristic values. It is pointed out thatthis is analogous to the change in the shape of the distribution whentranslating a given text to another language. Another consequence of theRGF-prediction is that taking a part of a long text will change the inputparameters (M, N, k_max) and consequently also the shape of the frequencydistribution. This is explicitly confirmed for texts written in Chinesecharacters. Since the RGF-prediction has no system-specific information beyondthe three a priori values (M, N, k_max), any specific language characteristichas to be sought in systematic deviations from the RGF-prediction and themeasured frequencies. One such systematic deviation is identified and, througha statistical information theoretical argument and an extended RGF-model, it isproposed that this deviation is caused by multiple meanings of Chinesecharacters. The effect is stronger for Chinese characters than for Chinesewords. The relation between Zipf's law, the Simon-model for texts and thepresent results are discussed.
arxiv-9600-32 | Mutual Information-Based Unsupervised Feature Transformation for Heterogeneous Feature Subset Selection | http://arxiv.org/pdf/1411.6400v2.pdf | author:Min Wei, Tommy W. S. Chow, Rosa H. M. Chan category:stat.ML cs.LG published:2014-11-24 summary:Conventional mutual information (MI) based feature selection (FS) methods areunable to handle heterogeneous feature subset selection properly because ofdata format differences or estimation methods of MI between feature subset andclass label. A way to solve this problem is feature transformation (FT). Inthis study, a novel unsupervised feature transformation (UFT) which cantransform non-numerical features into numerical features is developed andtested. The UFT process is MI-based and independent of class label. MI-based FSalgorithms, such as Parzen window feature selector (PWFS), minimum redundancymaximum relevance feature selection (mRMR), and normalized MI feature selection(NMIFS), can all adopt UFT for pre-processing of non-numerical features. Unliketraditional FT methods, the proposed UFT is unbiased while PWFS is utilized toits full advantage. Simulations and analyses of large-scale datasets showedthat feature subset selected by the integrated method, UFT-PWFS, outperformedother FT-FS integrated methods in classification accuracy.
arxiv-9600-33 | Towards Shockingly Easy Structured Classification: A Search-based Probabilistic Online Learning Framework | http://arxiv.org/pdf/1503.08381v1.pdf | author:Xu Sun category:cs.LG cs.AI published:2015-03-29 summary:There are two major approaches for structured classification. One is theprobabilistic gradient-based methods such as conditional random fields (CRF),which has high accuracy but with drawbacks: slow training, and no support ofsearch-based optimization (which is important in many cases). The other one isthe search-based learning methods such as perceptrons and margin infusedrelaxed algorithm (MIRA), which have fast training but also with drawbacks: lowaccuracy, no probabilistic information, and non-convergence in real-worldtasks. We propose a novel and "shockingly easy" solution, a search-basedprobabilistic online learning method, to address most of those issues. Thismethod searches the output candidates, derives probabilities, and conductefficient online learning. We show that this method is with fast training,support search-based optimization, very easy to implement, with top accuracy,with probabilities, and with theoretical guarantees of convergence. Experimentson well-known tasks show that our method has better accuracy than CRF andalmost as fast training speed as perceptron and MIRA. Results also show thatSAPO can easily beat the state-of-the-art systems on those highly-competitivetasks, achieving record-breaking accuracies.
arxiv-9600-34 | Global Bandits | http://arxiv.org/pdf/1503.08370v1.pdf | author:Onur Atan, Cem Tekin, Mihaela van der Schaar category:cs.LG published:2015-03-29 summary:Standard multi-armed bandits model decision problems in which theconsequences of each action choice are unknown and independent of each other.But in a wide variety of decision problems - from drug dosage to dynamicpricing - the consequences (rewards) of different actions are correlated, sothat selecting one action provides information about the consequences (rewards)of other actions as well. We propose and analyze a class of models of suchdecision problems; we call this class of models global bandits. When rewardsacross actions (arms) are sufficiently correlated we construct a greedy policythat achieves bounded regret, with a bound that depends on the true parametersof the problem. In the special case in which rewards of all arms aredeterministic functions of a single unknown parameter, we construct a (moresophisticated) greedy policy that achieves bounded regret, with a bound thatdepends on the single true parameter of the problem. For this special case wealso obtain a bound on regret that is independent of the true parameter; thisbound is sub-linear, with an exponent that depends on the informativeness ofthe arms (which measures the strength of correlation between arm rewards).
arxiv-9600-35 | Active Model Aggregation via Stochastic Mirror Descent | http://arxiv.org/pdf/1503.08363v1.pdf | author:Ravi Ganti category:stat.ML cs.AI cs.LG published:2015-03-28 summary:We consider the problem of learning convex aggregation of models, that is asgood as the best convex aggregation, for the binary classification problem.Working in the stream based active learning setting, where the active learnerhas to make a decision on-the-fly, if it wants to query for the label of thepoint currently seen in the stream, we propose a stochastic-mirror descentalgorithm, called SMD-AMA, with entropy regularization. We establish an excessrisk bounds for the loss of the convex aggregate returned by SMD-AMA to be ofthe order of $O\left(\sqrt{\frac{\log(M)}{{T^{1-\mu}}}}\right)$, where $\mu\in[0,1)$ is an algorithm dependent parameter, that trades-off the number oflabels queried, and excess risk.
arxiv-9600-36 | Sparse Linear Regression With Missing Data | http://arxiv.org/pdf/1503.08348v1.pdf | author:Ravi Ganti, Rebecca M. Willett category:stat.ML cs.LG stat.ME published:2015-03-28 summary:This paper proposes a fast and accurate method for sparse regression in thepresence of missing data. The underlying statistical model encapsulates thelow-dimensional structure of the incomplete data matrix and the sparsity of theregression coefficients, and the proposed algorithm jointly learns thelow-dimensional structure of the data and a linear regressor with sparsecoefficients. The proposed stochastic optimization method, Sparse LinearRegression with Missing Data (SLRM), performs an alternating minimizationprocedure and scales well with the problem size. Large deviation inequalitiesshed light on the impact of the various problem-dependent parameters on theexpected squared loss of the learned regressor. Extensive simulations on bothsynthetic and real datasets show that SLRM performs better than competingalgorithms in a variety of contexts.
arxiv-9600-37 | Selection Bias Correction and Effect Size Estimation under Dependence | http://arxiv.org/pdf/1405.4251v2.pdf | author:Kean Ming Tan, Noah Simon, Daniela Witten category:stat.ME stat.AP stat.ML published:2014-05-16 summary:We consider large-scale studies in which it is of interest to test a verylarge number of hypotheses, and then to estimate the effect sizes correspondingto the rejected hypotheses. For instance, this setting arises in the analysisof gene expression or DNA sequencing data. However, naive estimates of theeffect sizes suffer from selection bias, i.e., some of the largest naiveestimates are large due to chance alone. Many authors have proposed methods toreduce the effects of selection bias under the assumption that the naiveestimates of the effect sizes are independent. Unfortunately, when the effectsize estimates are dependent, these existing techniques can have very poorperformance, and in practice there will often be dependence. We propose anestimator that adjusts for selection bias under a recently-proposed frequentistframework, without the independence assumption. We study some properties of theproposed estimator, and illustrate that it outperforms past proposals in asimulation study and on two gene expression data sets.
arxiv-9600-38 | Some Further Evidence about Magnification and Shape in Neural Gas | http://arxiv.org/pdf/1503.08322v1.pdf | author:Giacomo Parigi, Andrea Pedrini, Marco Piastra category:cs.NE published:2015-03-28 summary:Neural gas (NG) is a robust vector quantization algorithm with a well-knownmathematical model. According to this, the neural gas samples the underlyingdata distribution following a power law with a magnification exponent thatdepends on data dimensionality only. The effects of shape in the input datadistribution, however, are not entirely covered by the NG model above, due tothe technical difficulties involved. The experimental work described here showsthat shape is indeed relevant in determining the overall NG behavior; inparticular, some experiments reveal richer and complex behaviors induced byshape that cannot be explained by the power law alone. Although a morecomprehensive analytical model remains to be defined, the evidence collected inthese experiments suggests that the NG algorithm has an interesting potentialfor detecting complex shapes in noisy datasets.
arxiv-9600-39 | Evolvability signatures of generative encodings: beyond standard performance benchmarks | http://arxiv.org/pdf/1410.4985v2.pdf | author:Danesh Tarapore, Jean-Baptiste Mouret category:cs.NE published:2014-10-18 summary:Evolutionary robotics is a promising approach to autonomously synthesizemachines with abilities that resemble those of animals, but the field suffersfrom a lack of strong foundations. In particular, evolutionary systems arecurrently assessed solely by the fitness score their evolved artifacts canachieve for a specific task, whereas such fitness-based comparisons providelimited insights about how the same system would evaluate on different tasks,and its adaptive capabilities to respond to changes in fitness (e.g., fromdamages to the machine, or in new situations). To counter these limitations, weintroduce the concept of "evolvability signatures", which picture thepost-mutation statistical distribution of both behavior diversity (howdifferent are the robot behaviors after a mutation?) and fitness values (howdifferent is the fitness after a mutation?). We tested the relevance of thisconcept by evolving controllers for hexapod robot locomotion using fivedifferent genotype-to-phenotype mappings (direct encoding, generative encodingof open-loop and closed-loop central pattern generators, generative encoding ofneural networks, and single-unit pattern generators (SUPG)). We observed apredictive relationship between the evolvability signature of each encoding andthe number of generations required by hexapods to adapt from incurred damages.Our study also reveals that, across the five investigated encodings, the SUPGscheme achieved the best evolvability signature, and was always foremost inrecovering an effective gait following robot damages. Overall, our evolvabilitysignatures neatly complement existing task-performance benchmarks, and pave theway for stronger foundations for research in evolutionary robotics.
arxiv-9600-40 | A Multi-signal Variant for the GPU-based Parallelization of Growing Self-Organizing Networks | http://arxiv.org/pdf/1503.08294v1.pdf | author:Giacomo Parigi, Angelo Stramieri, Danilo Pau, Marco Piastra category:cs.DC cs.NE published:2015-03-28 summary:Among the many possible approaches for the parallelization of self-organizingnetworks, and in particular of growing self-organizing networks, perhaps themost common one is producing an optimized, parallel implementation of thestandard sequential algorithms reported in the literature. In this paper weexplore an alternative approach, based on a new algorithm variant specificallydesigned to match the features of the large-scale, fine-grained parallelism ofGPUs, in which multiple input signals are processed at once. Comparative testshave been performed, using both parallel and sequential implementations of thenew algorithm variant, in particular for a growing self-organizing network thatreconstructs surfaces from point clouds. The experimental results show thatthis approach allows harnessing in a more effective way the intrinsicparallelism that the self-organizing networks algorithms seem intuitively tosuggest, obtaining better performances even with networks of smaller size.
arxiv-9600-41 | Robust Bayesian compressive sensing with data loss recovery for structural health monitoring signals | http://arxiv.org/pdf/1503.08272v1.pdf | author:Yong Huang, James L. Beck, Stephen Wu, Hui Li category:stat.AP stat.CO stat.ML published:2015-03-28 summary:The application of compressive sensing (CS) to structural health monitoringis an emerging research topic. The basic idea in CS is to use aspecially-designed wireless sensor to sample signals that are sparse in somebasis (e.g. wavelet basis) directly in a compressed form, and then toreconstruct (decompress) these signals accurately using some inversionalgorithm after transmission to a central processing unit. However, mostsignals in structural health monitoring are only approximately sparse, i.e.only a relatively small number of the signal coefficients in some basis aresignificant, but the other coefficients are usually not exactly zero. In thiscase, perfect reconstruction from compressed measurements is not expected. Anew Bayesian CS algorithm is proposed in which robust treatment of theuncertain parameters is explored, including integration over theprediction-error precision parameter to remove it as a "nuisance" parameter.The performance of the new CS algorithm is investigated using compressed datafrom accelerometers installed on a space-frame structure and on a cable-stayedbridge. Compared with other state-of-the-art CS methods including ourpreviously-published Bayesian method which uses MAP (maximum a posteriori)estimation of the prediction-error precision parameter, the new algorithm showssuperior performance in reconstruction robustness and posterior uncertaintyquantification. Furthermore, our method can be utilized for recovery of lostdata during wireless transmission, regardless of the level of sparseness in thesignal.
arxiv-9600-42 | CRF Learning with CNN Features for Image Segmentation | http://arxiv.org/pdf/1503.08263v1.pdf | author:Fayao Liu, Guosheng Lin, Chunhua Shen category:cs.CV published:2015-03-28 summary:Conditional Random Rields (CRF) have been widely applied in imagesegmentations. While most studies rely on hand-crafted features, we herepropose to exploit a pre-trained large convolutional neural network (CNN) togenerate deep features for CRF learning. The deep CNN is trained on theImageNet dataset and transferred to image segmentations here for constructingpotentials of superpixels. Then the CRF parameters are learnt using astructured support vector machine (SSVM). To fully exploit context informationin inference, we construct spatially related co-occurrence pairwise potentialsand incorporate them into the energy function. This prefers labelling of objectpairs that frequently co-occur in a certain spatial layout and at the same timeavoids implausible labellings during the inference. Extensive experiments onbinary and multi-class segmentation benchmarks demonstrate the promise of theproposed method. We thus provide new baselines for the segmentation performanceon the Weizmann horse, Graz-02, MSRC-21, Stanford Background and PASCAL VOC2011 datasets.
arxiv-9600-43 | A System View of the Recognition and Interpretation of Observed Human Shape, Pose and Action | http://arxiv.org/pdf/1503.08223v1.pdf | author:David W. Arathorn category:cs.CV published:2015-03-27 summary:There is physiological evidence that our ability to interpret human pose andaction from 2D visual imagery (binocular or monocular) engages the circuitry ofthe motor cortices as well as the visual areas of the brain. This implies thatthe capability of the motor cortices to solve inverse kinematics is flexibleenough to apply to both motion planning as well as serving as a generativemodel for the visual processing of human figures, despite the differingfunctional requirements of the two tasks. This paper provides a computationalmodel of the cooperation between visual and motor areas: in other words, asystem view of an important class of brain computations. The model unifies thesolution of the separate inverse problems involved in the task, visualtransformation discovery, inverse kinematics, and adaptation to morphologyvariations, using several instances of the Map-seeking Circuit algorithm. Whilethe paper is weighted toward the exposition of a neurobiological hypothesis,from mathematical formalization of the problem to neuronal circuitry, thealgorithmic expression of the solution is also a functional machine visionsystem for human figure recognition, and 3D pose and body morphologyreconstruction from monocular, perspective-less input imagery. With an inversekinematic generative model capable of imposing a variety of endogenous andexogenous constraints the machine vision implementation acquirescharacteristics currently unique among such systems.
arxiv-9600-44 | RankMap: A Platform-Aware Framework for Distributed Learning from Dense Datasets | http://arxiv.org/pdf/1503.08169v1.pdf | author:Azalia Mirhoseini, Eva. L. Dyer, Ebrahim. M. Songhori, Richard Baraniuk, Farinaz Koushanfar category:cs.DC cs.LG published:2015-03-27 summary:This paper introduces RankMap, a platform-aware end-to-end framework forefficient execution of a broad class of iterative learning algorithms formassive and dense datasets. In contrast to the existing dense (iterative) dataanalysis methods that are oblivious to the platform, for the first time, weintroduce novel scalable data transformation and mapping algorithms that enableoptimizing for the underlying computing platforms' cost/constraints. The costis defined by the number of arithmetic and (within-platform) message passingoperations incurred by the variable updates in each iteration, while theconstraints are set by the available memory resources. RankMap's transformationscalably factorizes data into an ensemble of lower dimensional subspaces, whileits mapping schedules the flow of iterative computation on the transformed dataonto the pertinent computing machine. We show a trade-off between the desiredlevel of accuracy for the learning algorithm and the achieved efficiency.RankMap provides two APIs, one matrix-based and one graph-based, whichfacilitate automated adoption of the framework for performing severalcontemporary iterative learning applications optimized to the platform. Todemonstrate the utility of RankMap, we solve sparse recovery and poweriteration problems on various real-world datasets with up to 1.8 billionnon-zeros. Our evaluations are performed on Amazon EC2 and IBM iDataPlexplatforms using up to 244 cores. The results demonstrate up to 2 orders ofmagnitude improvements in memory usage, execution speed, and bandwidth comparedwith the best reported prior work.
arxiv-9600-45 | Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories | http://arxiv.org/pdf/1503.08155v1.pdf | author:Miao Fan, Qiang Zhou, Thomas Fang Zheng category:cs.AI cs.CL published:2015-03-27 summary:This paper considers the problem of knowledge inference on large-scaleimperfect repositories with incomplete coverage by means of embedding entitiesand relations at the first attempt. We propose IIKE (Imperfect and IncompleteKnowledge Embedding), a probabilistic model which measures the probability ofeach belief, i.e. $\langle h,r,t\rangle$, in large-scale knowledge bases suchas NELL and Freebase, and our objective is to learn a better low-dimensionalvector representation for each entity ($h$ and $t$) and relation ($r$) in theprocess of minimizing the loss of fitting the corresponding confidence given bymachine learning (NELL) or crowdsouring (Freebase), so that we can use ${\bfh} + {\bf r} - {\bf t}$ to assess the plausibility of a belief whenconducting inference. We use subsets of those inexact knowledge bases to trainour model and test the performances of link prediction and tripletclassification on ground truth beliefs, respectively. The results of extensiveexperiments show that IIKE achieves significant improvement compared with thebaseline and state-of-the-art approaches.
arxiv-9600-46 | Hyper-Spectral Image Analysis with Partially-Latent Regression and Spatial Markov Dependencies | http://arxiv.org/pdf/1409.8500v2.pdf | author:Antoine Deleforge, Florence Forbes, Sileye Ba, Radu Horaud category:stat.AP cs.CV published:2014-09-30 summary:Hyper-spectral data can be analyzed to recover physical properties at largeplanetary scales. This involves resolving inverse problems which can beaddressed within machine learning, with the advantage that, once a relationshipbetween physical parameters and spectra has been established in a data-drivenfashion, the learned relationship can be used to estimate physical parametersfor new hyper-spectral observations. Within this framework, we propose aspatially-constrained and partially-latent regression method which mapshigh-dimensional inputs (hyper-spectral images) onto low-dimensional responses(physical parameters such as the local chemical composition of the soil). Theproposed regression model comprises two key features. Firstly, it combines aGaussian mixture of locally-linear mappings (GLLiM) with a partially-latentresponse model. While the former makes high-dimensional regression tractable,the latter enables to deal with physical parameters that cannot be observed or,more generally, with data contaminated by experimental artifacts that cannot beexplained with noise models. Secondly, spatial constraints are introduced inthe model through a Markov random field (MRF) prior which provides a spatialstructure to the Gaussian-mixture hidden variables. Experiments conducted on adatabase composed of remotely sensed observations collected from the Marsplanet by the Mars Express orbiter demonstrate the effectiveness of theproposed model.
arxiv-9600-47 | Variational Optimization of Annealing Schedules | http://arxiv.org/pdf/1502.05313v2.pdf | author:Taichi Kiwaki category:stat.ML published:2015-02-18 summary:Annealed importance sampling (AIS) is a common algorithm to estimatepartition functions of useful stochastic models. One important problem forobtaining accurate AIS estimates is the selection of an annealing schedule.Conventionally, an annealing schedule is often determined heuristically or issimply set as a linearly increasing sequence. In this paper, we propose analgorithm for the optimal schedule by deriving a functional that dominates theAIS estimation error and by numerically minimizing this functional. Weexperimentally demonstrate that the proposed algorithm mostly outperformsconventional scheduling schemes with large quantization numbers.
arxiv-9600-48 | Gibbs Sampling with Low-Power Spiking Digital Neurons | http://arxiv.org/pdf/1503.07793v2.pdf | author:Srinjoy Das, Bruno Umbria Pedroni, Paul Merolla, John Arthur, Andrew S. Cassidy, Bryan L. Jackson, Dharmendra Modha, Gert Cauwenberghs, Ken Kreutz-Delgado category:cs.NE published:2015-03-26 summary:Restricted Boltzmann Machines and Deep Belief Networks have been successfullyused in a wide variety of applications including image classification andspeech recognition. Inference and learning in these algorithms uses a MarkovChain Monte Carlo procedure called Gibbs sampling. A sigmoidal function formsthe kernel of this sampler which can be realized from the firing statistics ofnoisy integrate-and-fire neurons on a neuromorphic VLSI substrate. This paperdemonstrates such an implementation on an array of digital spiking neurons withstochastic leak and threshold properties for inference tasks and presents somekey performance metrics for such a hardware-based sampler in both thegenerative and discriminative contexts.
arxiv-9600-49 | Sparse graphs using exchangeable random measures | http://arxiv.org/pdf/1401.1137v3.pdf | author:François Caron, Emily B. Fox category:stat.ME cs.SI math.ST stat.ML stat.TH published:2014-01-06 summary:Statistical network modeling has focused on representing the graph as adiscrete structure, namely the adjacency matrix, and considering theexchangeability of this array. In such cases, the Aldous-Hoover representationtheorem (Aldous, 1981;Hoover, 1979} applies and informs us that the graph isnecessarily either dense or empty. In this paper, we instead considerrepresenting the graph as a measure on $\mathbb{R}_+^2$. For the associateddefinition of exchangeability in this continuous space, we rely on theKallenberg representation theorem (Kallenberg, 2005). We show that for certainchoices of such exchangeable random measures underlying our graph construction,our network process is sparse with power-law degree distribution. Inparticular, we build on the framework of completely random measures (CRMs) anduse the theory associated with such processes to derive important networkproperties, such as an urn representation for our analysis and networksimulation. Our theoretical results are explored empirically and compared tocommon network models. We then present a Hamiltonian Monte Carlo algorithm forefficient exploration of the posterior distribution and demonstrate that we areable to recover graphs ranging from dense to sparse--and perform associatedtests--based on our flexible CRM-based formulation. We explore networkproperties in a range of real datasets, including Facebook social circles, apolitical blogosphere, protein networks, citation networks, and world wide webnetworks, including networks with hundreds of thousands of nodes and millionsof edges.
arxiv-9600-50 | FitNets: Hints for Thin Deep Nets | http://arxiv.org/pdf/1412.6550v4.pdf | author:Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio category:cs.LG cs.NE published:2014-12-19 summary:While depth tends to improve network performances, it also makesgradient-based training more difficult since deeper networks tend to be morenon-linear. The recently proposed knowledge distillation approach is aimed atobtaining small and fast-to-execute models, and it has shown that a studentnetwork could imitate the soft output of a larger teacher network or ensembleof networks. In this paper, we extend this idea to allow the training of astudent that is deeper and thinner than the teacher, using not only the outputsbut also the intermediate representations learned by the teacher as hints toimprove the training process and final performance of the student. Because thestudent intermediate hidden layer will generally be smaller than the teacher'sintermediate hidden layer, additional parameters are introduced to map thestudent hidden layer to the prediction of the teacher hidden layer. This allowsone to train deeper students that can generalize better or run faster, atrade-off that is controlled by the chosen student capacity. For example, onCIFAR-10, a deep student network with almost 10.4 times less parametersoutperforms a larger, state-of-the-art teacher network.
arxiv-9600-51 | Real-time multi-view deconvolution | http://arxiv.org/pdf/1503.07998v1.pdf | author:Benjamin Schmid, Jan Huisken category:q-bio.QM cs.CV published:2015-03-27 summary:In light-sheet microscopy, overall image content and resolution are improvedby acquiring and fusing multiple views of the sample from different directions.State-of-the-art multi-view (MV) deconvolution employs the point spreadfunctions (PSF) of the different views to simultaneously fuse and deconvolvethe images in 3D, but processing takes a multiple of the acquisition time andconstitutes the bottleneck in the imaging pipeline. Here we show that MVdeconvolution in 3D can finally be achieved in real-time by reslicing theacquired data and processing cross-sectional planes individually on themassively parallel architecture of a graphics processing unit (GPU).
arxiv-9600-52 | Estimation of a common covariance matrix for multiple classes with applications in meta- and discriminant analysis | http://arxiv.org/pdf/1503.07990v1.pdf | author:Anders Ellern Bilgrau, Poul Svante Eriksen, Karen Dybkær, Martin Bøgsted category:stat.ML q-bio.GN stat.ME published:2015-03-27 summary:We propose a hierarchical random effects model for a common covariance matrixin cases where multiple classes are present. It is applicable where the classesare believed to share a common covariance matrix of interest obscured byclass-dependent noise. As such, it provides a basis for integrative ormeta-analysis of covariance matrices where the classes are formed by datasets.Our approach is inspired by traditional meta-analysis using random effectsmodels but the model is also shown to be applicable as an intermediate betweenlinear and quadratic discriminant analysis. We derive basic properties andestimators of the model and compare their properties. Simple inference andinterpretation of the introduced parameter measuring the inter-classhomogeneity is suggested.
arxiv-9600-53 | Discriminative Bayesian Dictionary Learning for Classification | http://arxiv.org/pdf/1503.07989v1.pdf | author:Naveed Akhtar, Faisal Shafait, Ajmal Mian category:cs.CV cs.LG 68T10 published:2015-03-27 summary:We propose a Bayesian approach to learn discriminative dictionaries forsparse representation of data. The proposed approach infers probabilitydistributions over the atoms of a discriminative dictionary using a BetaProcess. It also computes sets of Bernoulli distributions that associate classlabels to the learned dictionary atoms. This association signifies theselection probabilities of the dictionary atoms in the expansion ofclass-specific data. Furthermore, the non-parametric character of the proposedapproach allows it to infer the correct size of the dictionary. We exploit theaforementioned Bernoulli distributions in separately learning a linearclassifier. The classifier uses the same hierarchical Bayesian model as thedictionary, which we present along the analytical inference solution for Gibbssampling. For classification, a test instance is first sparsely encoded overthe learned dictionary and the codes are fed to the classifier. We performedexperiments for face and action recognition; and object and scene-categoryclassification using five public datasets and compared the results withstate-of-the-art discriminative sparse representation approaches. Experimentsshow that the proposed Bayesian approach consistently outperforms the existingapproaches.
arxiv-9600-54 | Bayesian Cross Validation and WAIC for Predictive Prior Design in Regular Asymptotic Theory | http://arxiv.org/pdf/1503.07970v1.pdf | author:Sumio Watanabe category:cs.LG stat.ML published:2015-03-27 summary:Prior design is one of the most important problems in both statistics andmachine learning. The cross validation (CV) and the widely applicableinformation criterion (WAIC) are predictive measures of the Bayesianestimation, however, it has been difficult to apply them to find the optimalprior because their mathematical properties in prior evaluation have beenunknown and the region of the hyperparameters is too wide to be examined. Inthis paper, we derive a new formula by which the theoretical relation among CV,WAIC, and the generalization loss is clarified and the optimal hyperparametercan be directly found. By the formula, three facts are clarified about predictive prior design.Firstly, CV and WAIC have the same second order asymptotic expansion, hencethey are asymptotically equivalent to each other as the optimizer of thehyperparameter. Secondly, the hyperparameter which minimizes CV or WAIC makesthe average generalization loss to be minimized asymptotically but does not therandom generalization loss. And lastly, by using the mathematical relationbetween priors, the variances of the optimized hyperparameters by CV and WAICare made smaller with small computational costs. Also we show that theoptimized hyperparameter by DIC or the marginal likelihood does not minimizethe average or random generalization loss in general.
arxiv-9600-55 | Competitive Distribution Estimation | http://arxiv.org/pdf/1503.07940v1.pdf | author:Alon Orlitsky, Ananda Theertha Suresh category:cs.IT cs.DS cs.LG math.IT math.ST stat.TH published:2015-03-27 summary:Estimating an unknown distribution from its samples is a fundamental problemin statistics. The common, min-max, formulation of this goal considers theperformance of the best estimator over all distributions in a class. It showsthat with $n$ samples, distributions over $k$ symbols can be learned to a KLdivergence that decreases to zero with the sample size $n$, but growsunboundedly with the alphabet size $k$. Min-max performance can be viewed as regret relative to an oracle that knowsthe underlying distribution. We consider two natural and modest limits on theoracle's power. One where it knows the underlying distribution only up tosymbol permutations, and the other where it knows the exact distribution but isrestricted to use natural estimators that assign the same probability tosymbols that appeared equally many times in the sample. We show that in both cases the competitive regret reduces to$\min(k/n,\tilde{\mathcal{O}}(1/\sqrt n))$, a quantity upper bounded uniformlyfor every alphabet size. This shows that distributions can be estimated nearlyas well as when they are essentially known in advance, and nearly as well aswhen they are completely known in advance but need to be estimated via anatural estimator. We also provide an estimator that runs in linear time andincurs competitive regret of $\tilde{\mathcal{O}}(\min(k/n,1/\sqrt n))$, andshow that for natural estimators this competitive regret is inevitable. We alsodemonstrate the effectiveness of competitive estimators using simulations.
arxiv-9600-56 | Generalized K-fan Multimodal Deep Model with Shared Representations | http://arxiv.org/pdf/1503.07906v1.pdf | author:Gang Chen, Sargur N. Srihari category:cs.LG stat.ML 68T10 I.2.6 published:2015-03-26 summary:Multimodal learning with deep Boltzmann machines (DBMs) is an generativeapproach to fuse multimodal inputs, and can learn the shared representation viaContrastive Divergence (CD) for classification and information retrieval tasks.However, it is a 2-fan DBM model, and cannot effectively handle multipleprediction tasks. Moreover, this model cannot recover the hiddenrepresentations well by sampling from the conditional distribution when morethan one modalities are missing. In this paper, we propose a K-fan deepstructure model, which can handle the multi-input and muti-output learningproblems effectively. In particular, the deep structure has K-branch fordifferent inputs where each branch can be composed of a multi-layer deep model,and a shared representation is learned in an discriminative manner to tacklemultimodal tasks. Given the deep structure, we propose two objective functionsto handle two multi-input and multi-output tasks: joint visual restoration andlabeling, and the multi-view multi-calss object recognition tasks. To estimatethe model parameters, we initialize the deep model parameters with CD tomaximize the joint distribution, and then we use backpropagation to update themodel according to specific objective function. The experimental resultsdemonstrate that the model can effectively leverages multi-source informationand predict multiple tasks well over competitive baselines.
arxiv-9600-57 | Transductive Multi-class and Multi-label Zero-shot Learning | http://arxiv.org/pdf/1503.07884v1.pdf | author:Yanwei Fu, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Shaogang Gong category:cs.LG cs.CV published:2015-03-26 summary:Recently, zero-shot learning (ZSL) has received increasing interest. The keyidea underpinning existing ZSL approaches is to exploit knowledge transfer viaan intermediate-level semantic representation which is assumed to be sharedbetween the auxiliary and target datasets, and is used to bridge between thesedomains for knowledge transfer. The semantic representation used in existingapproaches varies from visual attributes to semantic word vectors and semanticrelatedness. However, the overall pipeline is similar: a projection mappinglow-level features to the semantic representation is learned from the auxiliarydataset by either classification or regression models and applied directly tomap each instance into the same semantic representation space where a zero-shotclassifier is used to recognise the unseen target class instances with a singleknown 'prototype' of each target class. In this paper we discuss two relatedlines of work improving the conventional approach: exploiting transductivelearning ZSL, and generalising ZSL to the multi-label case.
arxiv-9600-58 | Multi-Labeled Classification of Demographic Attributes of Patients: a case study of diabetics patients | http://arxiv.org/pdf/1503.07795v1.pdf | author:Naveen Kumar Parachur Cotha, Marina Sokolova category:cs.LG published:2015-03-26 summary:Automated learning of patients demographics can be seen as multi-labelproblem where a patient model is based on different race and gender groups. Theresulting model can be further integrated into Privacy-Preserving Data Mining,where it can be used to assess risk of identification of different patientgroups. Our project considers relations between diabetes and demographics ofpatients as a multi-labelled problem. Most research in this area has been doneas binary classification, where the target class is finding if a person hasdiabetes or not. But very few, and maybe no work has been done in multi-labeledanalysis of the demographics of patients who are likely to be diagnosed withdiabetes. To identify such groups, we applied ensembles of several multi-labellearning algorithms.
arxiv-9600-59 | Transductive Multi-label Zero-shot Learning | http://arxiv.org/pdf/1503.07790v1.pdf | author:Yanwei Fu, Yongxin Yang, Tim Hospedales, Tao Xiang, Shaogang Gong category:cs.LG cs.CV published:2015-03-26 summary:Zero-shot learning has received increasing interest as a means to alleviatethe often prohibitive expense of annotating training data for large scalerecognition problems. These methods have achieved great success via learningintermediate semantic representations in the form of attributes and morerecently, semantic word vectors. However, they have thus far been constrainedto the single-label case, in contrast to the growing popularity and importanceof more realistic multi-label data. In this paper, for the first time, weinvestigate and formalise a general framework for multi-label zero-shotlearning, addressing the unique challenge therein: how to exploit multi-labelcorrelation at test time with no training data for those classes? Inparticular, we propose (1) a multi-output deep regression model to project animage into a semantic word space, which explicitly exploits the correlations inthe intermediate semantic layer of word vectors; (2) a novel zero-shot learningalgorithm for multi-label data that exploits the unique compositionalityproperty of semantic word vector representations; and (3) a transductivelearning strategy to enable the regression model learned from seen classes togeneralise well to unseen classes. Our zero-shot learning experiments on anumber of standard multi-label datasets demonstrate that our method outperformsa variety of baselines.
arxiv-9600-60 | Towards Learning free Naive Bayes Nearest Neighbor-based Domain Adaptation | http://arxiv.org/pdf/1503.07783v1.pdf | author:Faraz Saeedan, Barbara Caputo category:cs.CV published:2015-03-26 summary:As of today, object categorization algorithms are not able to achieve thelevel of robustness and generality necessary to work reliably in the realworld. Even the most powerful convolutional neural network we can train failsto perform satisfactorily when trained and tested on data from differentdatabases. This issue, known as domain adaptation and/or dataset bias in theliterature, is due to a distribution mismatch between data collections. Methodsaddressing it go from max-margin classifiers to learning how to modify thefeatures and obtain a more robust representation. Recent work showed that bycasting the problem into the image-to-class recognition framework, the domainadaptation problem is significantly alleviated \cite{danbnn}. Here we followthis approach, and show how a very simple, learning free Naive Bayes NearestNeighbor (NBNN)-based domain adaptation algorithm can significantly alleviatethe distribution mismatch among source and target data, especially when thenumber of classes and the number of sources grow. Experiments on standardbenchmarks used in the literature show that our approach (a) is competitivewith the current state of the art on small scale problems, and (b) achieves thecurrent state of the art as the number of classes and sources grows, withminimal computational requirements.
arxiv-9600-61 | A Unified Perspective on Multi-Domain and Multi-Task Learning | http://arxiv.org/pdf/1412.7489v3.pdf | author:Yongxin Yang, Timothy M. Hospedales category:stat.ML cs.LG cs.NE published:2014-12-23 summary:In this paper, we provide a new neural-network based perspective onmulti-task learning (MTL) and multi-domain learning (MDL). By introducing theconcept of a semantic descriptor, this framework unifies MDL and MTL as well asencompassing various classic and recent MTL/MDL algorithms by interpreting themas different ways of constructing semantic descriptors. Our interpretationprovides an alternative pipeline for zero-shot learning (ZSL), where a modelfor a novel class can be constructed without training data. Moreover, it leadsto a new and practically relevant problem setting of zero-shot domainadaptation (ZSDA), which is the analogous to ZSL but for novel domains: A modelfor an unseen domain can be generated by its semantic descriptor. Experimentsacross this range of problems demonstrate that our framework outperforms avariety of alternatives.
arxiv-9600-62 | On Gridless Sparse Methods for Line Spectral Estimation From Complete and Incomplete Data | http://arxiv.org/pdf/1407.2490v2.pdf | author:Zai Yang, Lihua Xie category:cs.IT math.IT stat.ML published:2014-07-09 summary:This paper is concerned about sparse, continuous frequency estimation in linespectral estimation, and focused on developing gridless sparse methods whichovercome grid mismatches and correspond to limiting scenarios of existinggrid-based approaches, e.g., $\ell_1$ optimization and SPICE, with aninfinitely dense grid. We generalize AST (atomic-norm soft thresholding) to thecase of nonconsecutively sampled data (incomplete data) inspired by recentatomic norm based techniques. We present a gridless version of SPICE (gridlessSPICE, or GLS), which is applicable to both complete and incomplete datawithout the knowledge of noise level. We further prove the equivalence betweenGLS and atomic norm-based techniques under different assumptions of noise.Moreover, we extend GLS to a systematic framework consisting of model orderselection and robust frequency estimation, and present feasible algorithms forAST and GLS. Numerical simulations are provided to validate our theoreticalanalysis and demonstrate performance of our methods compared to existing ones.
arxiv-9600-63 | Effective Use of Word Order for Text Categorization with Convolutional Neural Networks | http://arxiv.org/pdf/1412.1058v2.pdf | author:Rie Johnson, Tong Zhang category:cs.CL cs.LG stat.ML published:2014-12-01 summary:Convolutional neural network (CNN) is a neural network that can make use ofthe internal structure of data such as the 2D structure of image data. Thispaper studies CNN on text categorization to exploit the 1D structure (namely,word order) of text data for accurate prediction. Instead of usinglow-dimensional word vectors as input as is often done, we directly apply CNNto high-dimensional text data, which leads to directly learning embedding ofsmall text regions for use in classification. In addition to a straightforwardadaptation of CNN from image to text, a simple but new variation which employsbag-of-word conversion in the convolution layer is proposed. An extension tocombine multiple convolution layers is also explored for higher accuracy. Theexperiments demonstrate the effectiveness of our approach in comparison withstate-of-the-art methods.
arxiv-9600-64 | Pain Intensity Estimation by a Self--Taught Selection of Histograms of Topographical Features | http://arxiv.org/pdf/1503.07706v1.pdf | author:Corneliu Florea, Laura Florea, Raluca Boia, Alessandra Bandrabur, Constantin Vertan category:cs.CV published:2015-03-26 summary:Pain assessment through observational pain scales is necessary for specialcategories of patients such as neonates, patients with dementia, critically illpatients, etc. The recently introduced Prkachin-Solomon score allows painassessment directly from facial images opening the path for multiple assistiveapplications. In this paper, we introduce the Histograms of Topographical (HoT)features, which are a generalization of the topographical primal sketch, forthe description of the face parts contributing to the mentioned score. Wepropose a semi-supervised, clustering oriented self--taught learning proceduredeveloped on the emotion oriented Cohn-Kanade database. We use this procedureto improve the discrimination between different pain intensity levels and thegeneralization with respect to the monitored persons, while testing on the UNBCMcMaster Shoulder Pain database.
arxiv-9600-65 | Robust Eye Centers Localization with Zero--Crossing Encoded Image Projections | http://arxiv.org/pdf/1503.07697v1.pdf | author:Laura Florea, Corneliu Florea, Constantin Vertan category:cs.CV published:2015-03-26 summary:This paper proposes a new framework for the eye centers localization by thejoint use of encoding of normalized image projections and a Multi LayerPerceptron (MLP) classifier. The encoding is novel and it consists inidentifying the zero-crossings and extracting the relevant parameters from theresulting modes. The compressed normalized projections produce featuredescriptors that are inputs to a properly-trained MLP, for discriminating amongvarious categories of image regions. The proposed framework forms a fast andreliable system for the eye centers localization, especially in the context offace expression analysis in unconstrained environments. We successfully testthe proposed method on a wide variety of databases including BioID,Cohn-Kanade, Extended Yale B and Labelled Faces in the Wild (LFW) databases.
arxiv-9600-66 | Bayesian Reconstruction of Missing Observations | http://arxiv.org/pdf/1404.5793v2.pdf | author:Shun Kataoka, Muneki Yasuda, Kazuyuki Tanaka category:stat.ML published:2014-04-23 summary:We focus on an interpolation method referred to Bayesian reconstruction inthis paper. Whereas in standard interpolation methods missing data areinterpolated deterministically, in Bayesian reconstruction, missing data areinterpolated probabilistically using a Bayesian treatment. In this paper, weaddress the framework of Bayesian reconstruction and its application to thetraffic data reconstruction problem in the field of traffic engineering. In thelatter part of this paper, we describe the evaluation of the statisticalperformance of our Bayesian traffic reconstruction model using a statisticalmechanical approach and clarify its statistical behavior.
arxiv-9600-67 | Unsupervised authorship attribution | http://arxiv.org/pdf/1503.07613v1.pdf | author:David Fifield, Torbjørn Follan, Emil Lunde category:cs.CL published:2015-03-26 summary:We describe a technique for attributing parts of a written text to a set ofunknown authors. Nothing is assumed to be known a priori about the writingstyles of potential authors. We use multiple independent clusterings of aninput text to identify parts that are similar and dissimilar to one another. Wedescribe algorithms necessary to combine the multiple clusterings into ameaningful output. We show results of the application of the technique on textshaving multiple writing styles.
arxiv-9600-68 | An Evolutionary Algorithm for Error-Driven Learning via Reinforcement | http://arxiv.org/pdf/1503.07609v1.pdf | author:Yanping Liu, Erik D. Reichle category:cs.AI cs.NE published:2015-03-26 summary:Although different learning systems are coordinated to afford complexbehavior, little is known about how this occurs. This article describes atheoretical framework that specifies how complex behaviors that might bethought to require error-driven learning might instead be acquired throughsimple reinforcement. This framework includes specific assumptions about themechanisms that contribute to the evolution of (artificial) neural networks togenerate topologies that allow the networks to learn large-scale complexproblems using only information about the quality of their performance. Thepractical and theoretical implications of the framework are discussed, as arepossible biological analogs of the approach.
arxiv-9600-69 | On learning optimized reaction diffusion processes for effective image restoration | http://arxiv.org/pdf/1503.05768v2.pdf | author:Yunjin Chen, Wei Yu, Thomas Pock category:cs.CV published:2015-03-19 summary:For several decades, image restoration remains an active research topic inlow-level computer vision and hence new approaches are constantly emerging.However, many recently proposed algorithms achieve state-of-the-art performanceonly at the expense of very high computation time, which clearly limits theirpractical relevance. In this work, we propose a simple but effective approachwith both high computational efficiency and high restoration quality. We extendconventional nonlinear reaction diffusion models by several parametrized linearfilters as well as several parametrized influence functions. We propose totrain the parameters of the filters and the influence functions through a lossbased approach. Experiments show that our trained nonlinear reaction diffusionmodels largely benefit from the training of the parameters and finally lead tothe best reported performance on common test datasets for image restoration.Due to their structural simplicity, our trained models are highly efficient andare also well-suited for parallel computation on GPUs.
arxiv-9600-70 | Stable Feature Selection from Brain sMRI | http://arxiv.org/pdf/1503.07508v1.pdf | author:Bo Xin, Lingjing Hu, Yizhou Wang, Wen Gao category:cs.LG stat.ML published:2015-03-25 summary:Neuroimage analysis usually involves learning thousands or even millions ofvariables using only a limited number of samples. In this regard, sparsemodels, e.g. the lasso, are applied to select the optimal features and achievehigh diagnosis accuracy. The lasso, however, usually results in independentunstable features. Stability, a manifest of reproducibility of statisticalresults subject to reasonable perturbations to data and the model, is animportant focus in statistics, especially in the analysis of high dimensionaldata. In this paper, we explore a nonnegative generalized fused lasso model forstable feature selection in the diagnosis of Alzheimer's disease. In additionto sparsity, our model incorporates two important pathological priors: thespatial cohesion of lesion voxels and the positive correlation between thefeatures and the disease labels. To optimize the model, we propose an efficientalgorithm by proving a novel link between total variation and fast network flowalgorithms via conic duality. Experiments show that the proposed nonnegativemodel performs much better in exploring the intrinsic structure of data viaselecting stable features compared with other state-of-the-arts.
arxiv-9600-71 | DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation | http://arxiv.org/pdf/1401.1880v2.pdf | author:Elad Liebman, Maytal Saar-Tsechansky, Peter Stone category:cs.LG published:2014-01-09 summary:In recent years, there has been growing focus on the study of automatedrecommender systems. Music recommendation systems serve as a prominent domainfor such works, both from an academic and a commercial perspective. Afundamental aspect of music perception is that music is experienced in temporalcontext and in sequence. In this work we present DJ-MC, a novelreinforcement-learning framework for music recommendation that does notrecommend songs individually but rather song sequences, or playlists, based ona model of preferences for both songs and song transitions. The model islearned online and is uniquely adapted for each listener. To reduce explorationtime, DJ-MC exploits user feedback to initialize a model, which it subsequentlyupdates by reinforcement. We evaluate our framework with human participantsusing both real song and playlist data. Our results indicate that DJ-MC'sability to recommend sequences of songs provides a significant improvement overmore straightforward approaches, which do not take transitions into account.
arxiv-9600-72 | Poisson Matrix Completion | http://arxiv.org/pdf/1501.06243v6.pdf | author:Yang Cao, Yao Xie category:stat.ML cs.LG published:2015-01-26 summary:We extend the theory of matrix completion to the case where we make Poissonobservations for a subset of entries of a low-rank matrix. We consider the(now) usual matrix recovery formulation through maximum likelihood with properconstraints on the matrix $M$, and establish theoretical upper and lower boundson the recovery error. Our bounds are nearly optimal up to a factor on theorder of $\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by adaptingthe arguments used for one-bit matrix completion \cite{davenport20121}(although these two problems are different in nature) and the adaptationrequires new techniques exploiting properties of the Poisson likelihoodfunction and tackling the difficulties posed by the locally sub-Gaussiancharacteristic of the Poisson distribution. Our results highlight a fewimportant distinctions of Poisson matrix completion compared to the prior workin matrix completion including having to impose a minimum signal-to-noiserequirement on each observed entry. We also develop an efficient iterativealgorithm and demonstrate its good performance in recovering solar flareimages.
arxiv-9600-73 | A Survey of Classification Techniques in the Area of Big Data | http://arxiv.org/pdf/1503.07477v1.pdf | author:Praful Koturwar, Sheetal Girase, Debajyoti Mukhopadhyay category:cs.LG published:2015-03-25 summary:Big Data concern large-volume, growing data sets that are complex and havemultiple autonomous sources. Earlier technologies were not able to handlestorage and processing of huge data thus Big Data concept comes into existence.This is a tedious job for users unstructured data. So, there should be somemechanism which classify unstructured data into organized form which helps userto easily access required data. Classification techniques over bigtransactional database provide required data to the users from large datasetsmore simple way. There are two main classification techniques, supervised andunsupervised. In this paper we focused on to study of different supervisedclassification techniques. Further this paper shows a advantages andlimitations.
arxiv-9600-74 | Compressed sensing MRI using masked DCT and DFT measurements | http://arxiv.org/pdf/1503.07384v1.pdf | author:Elma Hot, Petar Sekulić category:cs.CV published:2015-03-25 summary:This paper presents modification of the TwIST algorithm for CompressiveSensing MRI images reconstruction. Compressive Sensing is new approach insignal processing whose basic idea is recovering signal form small set ofavailable samples. The application of the Compressive Sensing in biomedicalimaging has found great importance. It allows significant lowering of theacquisition time, and therefore, save the patient from the negative impact ofthe MR apparatus. TwIST is commonly used algorithm for 2D signalsreconstruction using Compressive Sensing principle. It is based on the TotalVariation minimization. Standard version of the TwIST uses masked 2D DiscreteFourier Transform coefficients as Compressive Sensing measurements. In thispaper, different masks and different transformation domains for coefficientsselection are tested. Certain percent of the measurements is used from themask, as well as small number of coefficients outside the mask. Comparativeanalysis using 2D DFT and 2D DCT coefficients, with different mask shapes isperformed. The theory is proved with experimental results.
arxiv-9600-75 | Quantized Nonparametric Estimation | http://arxiv.org/pdf/1503.07368v1.pdf | author:Yuancheng Zhu, John Lafferty category:math.ST stat.ML stat.TH published:2015-03-25 summary:We present an extension to Pinsker's theorem for nonparametric estimationover Sobolev ellipsoids when estimation is carried out under storage orcommunication constraints. Placing limits on the number of bits used to encodeany estimator, we give tight lower and upper bounds on the excess risk due toquantization in terms of the number of bits, the signal size, and the noiselevel. This establishes the Pareto optimal minimax tradeoff between storage andrisk under quantization constraints for Sobolev spaces. Our results and prooftechniques combine elements of rate distortion theory and minimax analysis.
arxiv-9600-76 | Adaptive Metric Dimensionality Reduction | http://arxiv.org/pdf/1302.2752v3.pdf | author:Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer category:cs.LG cs.DS stat.ML published:2013-02-12 summary:We study adaptive data-dependent dimensionality reduction in the context ofsupervised learning in general metric spaces. Our main statistical contributionis a generalization bound for Lipschitz functions in metric spaces that aredoubling, or nearly doubling. On the algorithmic front, we describe an analogueof PCA for metric spaces: namely an efficient procedure that approximates thedata's intrinsic dimension, which is often much lower than the ambientdimension. Our approach thus leverages the dual benefits of low dimensionality:(1) more efficient algorithms, e.g., for proximity search, and (2) moreoptimistic generalization bounds.
arxiv-9600-77 | Fast and Robust Fixed-Rank Matrix Recovery | http://arxiv.org/pdf/1503.03004v3.pdf | author:German Ros, Julio Guerrero category:cs.CV cs.NA published:2015-03-10 summary:We address the problem of efficient sparse fixed-rank (S-FR) matrixdecomposition, i.e., splitting a corrupted matrix $M$ into an uncorruptedmatrix $L$ of rank $r$ and a sparse matrix of outliers $S$. Fixed-rankconstraints are usually imposed by the physical restrictions of the systemunder study. Here we propose a method to perform accurate and very efficientS-FR decomposition that is more suitable for large-scale problems than existingapproaches. Our method is a grateful combination of geometrical and algebraicaltechniques, which avoids the bottleneck caused by the Truncated SVD (TSVD).Instead, a polar factorization is used to exploit the manifold structure offixed-rank problems as the product of two Stiefel and an SPD manifold, leadingto a better convergence and stability. Then, closed-form projectors help tospeed up each iteration of the method. We introduce a novel and fast projectorfor the $\text{SPD}$ manifold and a proof of its validity. Further accelerationis achieved using a Nystrom scheme. Extensive experiments with synthetic andreal data in the context of robust photometric stereo and spectral clusteringshow that our proposals outperform the state of the art.
arxiv-9600-78 | A Brief Survey of Recent Edge-Preserving Smoothing Algorithms on Digital Images | http://arxiv.org/pdf/1503.07297v1.pdf | author:Chandrajit Pal, Amlan Chakrabarti, Ranjan Ghosh category:cs.CV published:2015-03-25 summary:Edge preserving filters preserve the edges and its information while blurringan image. In other words they are used to smooth an image, while reducing theedge blurring effects across the edge like halos, phantom etc. They arenonlinear in nature. Examples are bilateral filter, anisotropic diffusionfilter, guided filter, trilateral filter etc. Hence these family of filters arevery useful in reducing the noise in an image making it very demanding incomputer vision and computational photography applications like denoising,video abstraction, demosaicing, optical-flow estimation, stereo matching, tonemapping, style transfer, relighting etc. This paper provides a concreteintroduction to edge preserving filters starting from the heat diffusionequation in olden to recent eras, an overview of its numerous applications, aswell as mathematical analysis, various efficient and optimized ways ofimplementation and their interrelationships, keeping focus on preserving theboundaries, spikes and canyons in presence of noise. Furthermore it provides arealistic notion for efficient implementation with a research scope forhardware realization for further acceleration.
arxiv-9600-79 | Using Latent Semantic Analysis to Identify Quality in Use (QU) Indicators from User Reviews | http://arxiv.org/pdf/1503.07294v1.pdf | author:Wendy Tan Wei Syn, Bong Chih How, Issa Atoum category:cs.CL cs.AI cs.IR published:2015-03-25 summary:The paper describes a novel approach to categorize users' reviews accordingto the three Quality in Use (QU) indicators defined in ISO: effectiveness,efficiency and freedom from risk. With the tremendous amount of reviewspublished each day, there is a need to automatically summarize user reviews toinform us if any of the software able to meet requirement of a companyaccording to the quality requirements. We implemented the method of LatentSemantic Analysis (LSA) and its subspace to predict QU indicators. We build areduced dimensionality universal semantic space from Information Systemjournals and Amazon reviews. Next, we projected set of indicators' measurementscales into the universal semantic space and represent them as subspace. In thesubspace, we can map similar measurement scales to the unseen reviews andpredict the QU indicators. Our preliminary study able to obtain the average ofF-measure, 0.3627.
arxiv-9600-80 | Morphological Analyzer and Generator for Russian and Ukrainian Languages | http://arxiv.org/pdf/1503.07283v1.pdf | author:Mikhail Korobov category:cs.CL published:2015-03-25 summary:pymorphy2 is a morphological analyzer and generator for Russian and Ukrainianlanguages. It uses large efficiently encoded lexi- cons built from OpenCorporaand LanguageTool data. A set of linguistically motivated rules is developed toenable morphological analysis and generation of out-of-vocabulary wordsobserved in real-world documents. For Russian pymorphy2 providesstate-of-the-arts morphological analysis quality. The analyzer is implementedin Python programming language with optional C++ extensions. Emphasis is put onease of use, documentation and extensibility. The package is distributed undera permissive open-source license, encouraging its use in both academic andcommercial setting.
arxiv-9600-81 | Initialization Strategies of Spatio-Temporal Convolutional Neural Networks | http://arxiv.org/pdf/1503.07274v1.pdf | author:Elman Mansimov, Nitish Srivastava, Ruslan Salakhutdinov category:cs.CV cs.LG published:2015-03-25 summary:We propose a new way of incorporating temporal information present in videosinto Spatial Convolutional Neural Networks (ConvNets) trained on images, thatavoids training Spatio-Temporal ConvNets from scratch. We describe severalinitializations of weights in 3D Convolutional Layers of Spatio-TemporalConvNet using 2D Convolutional Weights learned from ImageNet. We show that itis important to initialize 3D Convolutional Weights judiciously in order tolearn temporal representations of videos. We evaluate our methods on theUCF-101 dataset and demonstrate improvement over Spatial ConvNets.
arxiv-9600-82 | Accuracy of Latent-Variable Estimation in Bayesian Semi-Supervised Learning | http://arxiv.org/pdf/1308.2029v3.pdf | author:Keisuke Yamazaki category:stat.ML published:2013-08-09 summary:Hierarchical probabilistic models, such as Gaussian mixture models, arewidely used for unsupervised learning tasks. These models consist of observableand latent variables, which represent the observable data and the underlyingdata-generation process, respectively. Unsupervised learning tasks, such ascluster analysis, are regarded as estimations of latent variables based on theobservable ones. The estimation of latent variables in semi-supervisedlearning, where some labels are observed, will be more precise than that inunsupervised, and one of the concerns is to clarify the effect of the labeleddata. However, there has not been sufficient theoretical analysis of theaccuracy of the estimation of latent variables. In a previous study, adistribution-based error function was formulated, and its asymptotic form wascalculated for unsupervised learning with generative models. It has been shownthat, for the estimation of latent variables, the Bayes method is more accuratethan the maximum-likelihood method. The present paper reveals the asymptoticforms of the error function in Bayesian semi-supervised learning for bothdiscriminative and generative models. The results show that the generativemodel, which uses all of the given data, performs better when the model is wellspecified.
arxiv-9600-83 | Regularized Minimax Conditional Entropy for Crowdsourcing | http://arxiv.org/pdf/1503.07240v1.pdf | author:Dengyong Zhou, Qiang Liu, John C. Platt, Christopher Meek, Nihar B. Shah category:cs.LG stat.ML published:2015-03-25 summary:There is a rapidly increasing interest in crowdsourcing for data labeling. Bycrowdsourcing, a large number of labels can be often quickly gathered at lowcost. However, the labels provided by the crowdsourcing workers are usually notof high quality. In this paper, we propose a minimax conditional entropyprinciple to infer ground truth from noisy crowdsourced labels. Under thisprinciple, we derive a unique probabilistic labeling model jointlyparameterized by worker ability and item difficulty. We also propose anobjective measurement principle, and show that our method is the only methodwhich satisfies this objective measurement principle. We validate our methodthrough a variety of real crowdsourcing datasets with binary, multiclass orordinal labels.
arxiv-9600-84 | Universal Approximation of Markov Kernels by Shallow Stochastic Feedforward Networks | http://arxiv.org/pdf/1503.07211v1.pdf | author:Guido Montufar category:cs.LG stat.ML 82C32 published:2015-03-24 summary:We establish upper bounds for the minimal number of hidden units for which abinary stochastic feedforward network with sigmoid activation probabilities anda single hidden layer is a universal approximator of Markov kernels. We showthat each possible probabilistic assignment of the states of $n$ output units,given the states of $k\geq1$ input units, can be approximated arbitrarily wellby a network with $2^{k-1}(2^{n-1}-1)$ hidden units.
arxiv-9600-85 | Yara Parser: A Fast and Accurate Dependency Parser | http://arxiv.org/pdf/1503.06733v2.pdf | author:Mohammad Sadegh Rasooli, Joel Tetreault category:cs.CL published:2015-03-23 summary:Dependency parsers are among the most crucial tools in natural languageprocessing as they have many important applications in downstream tasks such asinformation retrieval, machine translation and knowledge acquisition. Weintroduce the Yara Parser, a fast and accurate open-source dependency parserbased on the arc-eager algorithm and beam search. It achieves an unlabeledaccuracy of 93.32 on the standard WSJ test set which ranks it among the topdependency parsers. At its fastest, Yara can parse about 4000 sentences persecond when in greedy mode (1 beam). When optimizing for accuracy (using 64beams and Brown cluster features), Yara can parse 45 sentences per second. Theparser can be trained on any syntactic dependency treebank and differentoptions are provided in order to make it more flexible and tunable for specifictasks. It is released with the Apache version 2.0 license and can be used forboth commercial and academic purposes. The parser can be found athttps://github.com/yahoo/YaraParser.
arxiv-9600-86 | Analysis of Spectrum Occupancy Using Machine Learning Algorithms | http://arxiv.org/pdf/1503.07104v1.pdf | author:Freeha Azmat, Yunfei Chen, Nigel Stocks category:cs.NI cs.LG published:2015-03-24 summary:In this paper, we analyze the spectrum occupancy using different machinelearning techniques. Both supervised techniques (naive Bayesian classifier(NBC), decision trees (DT), support vector machine (SVM), linear regression(LR)) and unsupervised algorithm (hidden markov model (HMM)) are studied tofind the best technique with the highest classification accuracy (CA). Adetailed comparison of the supervised and unsupervised algorithms in terms ofthe computational time and classification accuracy is performed. The classifiedoccupancy status is further utilized to evaluate the probability of secondaryuser outage for the future time slots, which can be used by system designers todefine spectrum allocation and spectrum sharing policies. Numerical resultsshow that SVM is the best algorithm among all the supervised and unsupervisedclassifiers. Based on this, we proposed a new SVM algorithm by combining itwith fire fly algorithm (FFA), which is shown to outperform all otheralgorithms.
arxiv-9600-87 | Rotation-invariant convolutional neural networks for galaxy morphology prediction | http://arxiv.org/pdf/1503.07077v1.pdf | author:Sander Dieleman, Kyle W. Willett, Joni Dambre category:astro-ph.IM astro-ph.GA cs.CV cs.LG cs.NE stat.ML published:2015-03-24 summary:Measuring the morphological parameters of galaxies is a key requirement forstudying their formation and evolution. Surveys such as the Sloan Digital SkySurvey (SDSS) have resulted in the availability of very large collections ofimages, which have permitted population-wide analyses of galaxy morphology.Morphological analysis has traditionally been carried out mostly via visualinspection by trained experts, which is time-consuming and does not scale tolarge ($\gtrsim10^4$) numbers of images. Although attempts have been made to build automated classification systems,these have not been able to achieve the desired level of accuracy. The GalaxyZoo project successfully applied a crowdsourcing strategy, inviting onlineusers to classify images by answering a series of questions. Unfortunately,even this approach does not scale well enough to keep up with the increasingavailability of galaxy images. We present a deep neural network model for galaxy morphology classificationwhich exploits translational and rotational symmetry. It was developed in thecontext of the Galaxy Challenge, an international competition to build the bestmodel for morphology classification based on annotated images from the GalaxyZoo project. For images with high agreement among the Galaxy Zoo participants, our modelis able to reproduce their consensus with near-perfect accuracy ($> 99\%$) formost questions. Confident model predictions are highly accurate, which makesthe model suitable for filtering large collections of images and forwardingchallenging images to experts for manual annotation. This approach greatlyreduces the experts' workload without affecting accuracy. The application ofthese algorithms to larger sets of training data will be critical for analysingresults from future surveys such as the LSST.
arxiv-9600-88 | From Visual Attributes to Adjectives through Decompositional Distributional Semantics | http://arxiv.org/pdf/1501.02714v2.pdf | author:Angeliki Lazaridou, Georgiana Dinu, Adam Liska, Marco Baroni category:cs.CL cs.CV published:2015-01-12 summary:As automated image analysis progresses, there is increasing interest inricher linguistic annotation of pictures, with attributes of objects (e.g.,furry, brown...) attracting most attention. By building on the recent"zero-shot learning" approach, and paying attention to the linguistic nature ofattributes as noun modifiers, and specifically adjectives, we show that it ispossible to tag images with attribute-denoting adjectives even when no trainingdata containing the relevant annotation are available. Our approach relies ontwo key observations. First, objects can be seen as bundles of attributes,typically expressed as adjectival modifiers (a dog is something furry, brown,etc.), and thus a function trained to map visual representations of objects tonominal labels can implicitly learn to map attributes to adjectives. Second,objects and attributes come together in pictures (the same thing is a dog andit is brown). We can thus achieve better attribute (and object) label retrievalby treating images as "visual phrases", and decomposing their linguisticrepresentation into an attribute-denoting adjective and an object-denotingnoun. Our approach performs comparably to a method exploiting manual attributeannotation, it outperforms various competitive alternatives in both attributeand object annotation, and it automatically constructs attribute-centricrepresentations that significantly improve performance in supervised objectrecognition.
arxiv-9600-89 | Contextual Online Learning for Multimedia Content Aggregation | http://arxiv.org/pdf/1502.02125v2.pdf | author:Cem Tekin, Mihaela van der Schaar category:cs.MM cs.LG cs.MA published:2015-02-07 summary:The last decade has witnessed a tremendous growth in the volume as well asthe diversity of multimedia content generated by a multitude of sources (newsagencies, social media, etc.). Faced with a variety of content choices,consumers are exhibiting diverse preferences for content; their preferencesoften depend on the context in which they consume content as well as variousexogenous events. To satisfy the consumers' demand for such diverse content,multimedia content aggregators (CAs) have emerged which gather content fromnumerous multimedia sources. A key challenge for such systems is to accuratelypredict what type of content each of its consumers prefers in a certaincontext, and adapt these predictions to the evolving consumers' preferences,contexts and content characteristics. We propose a novel, distributed, onlinemultimedia content aggregation framework, which gathers content generated bymultiple heterogeneous producers to fulfill its consumers' demand for content.Since both the multimedia content characteristics and the consumers'preferences and contexts are unknown, the optimal content aggregation strategyis unknown a priori. Our proposed content aggregation algorithm is able tolearn online what content to gather and how to match content and users byexploiting similarities between consumer types. We prove bounds for ourproposed learning algorithms that guarantee both the accuracy of thepredictions as well as the learning speed. Importantly, our algorithms operateefficiently even when feedback from consumers is missing or content andpreferences evolve over time. Illustrative results highlight the merits of theproposed content aggregation system in a variety of settings.
arxiv-9600-90 | Probabilistic Binary-Mask Cocktail-Party Source Separation in a Convolutional Deep Neural Network | http://arxiv.org/pdf/1503.06962v1.pdf | author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx published:2015-03-24 summary:Separation of competing speech is a key challenge in signal processing and afeat routinely performed by the human auditory brain. A long standing benchmarkof the spectrogram approach to source separation is known as the ideal binarymask. Here, we train a convolutional deep neural network, on a two-speakercocktail party problem, to make probabilistic predictions about binary masks.Our results approach ideal binary mask performance, illustrating thatrelatively simple deep neural networks are capable of robust binary maskprediction. We also illustrate the trade-off between prediction statistics andseparation quality.
arxiv-9600-91 | Fast keypoint detection in video sequences | http://arxiv.org/pdf/1503.06959v1.pdf | author:Luca Baroffio, Matteo Cesana, Alessandro Redondi, Marco Tagliasacchi category:cs.CV cs.MM published:2015-03-24 summary:A number of computer vision tasks exploit a succinct representation of thevisual content in the form of sets of local features. Given an input image,feature extraction algorithms identify a set of keypoints and assign to each ofthem a description vector, based on the characteristics of the visual contentsurrounding the interest point. Several tasks might require local features tobe extracted from a video sequence, on a frame-by-frame basis. Althoughtemporal downsampling has been proven to be an effective solution for mobileaugmented reality and visual search, high temporal resolution is a keyrequirement for time-critical applications such as object tracking, eventrecognition, pedestrian detection, surveillance. In recent years, more and morecomputationally efficient visual feature detectors and decriptors have beenproposed. Nonetheless, such approaches are tailored to still images. In thispaper we propose a fast keypoint detection algorithm for video sequences, thatexploits the temporal coherence of the sequence of keypoints. According to theproposed method, each frame is preprocessed so as to identify the parts of theinput frame for which keypoint detection and description need to be performed.Our experiments show that it is possible to achieve a reduction incomputational time of up to 40%, without significantly affecting the taskaccuracy.
arxiv-9600-92 | Comparing published multi-label classifier performance measures to the ones obtained by a simple multi-label baseline classifier | http://arxiv.org/pdf/1503.06952v1.pdf | author:Jean Metz, Newton Spolaôr, Everton A. Cherman, Maria C. Monard category:cs.LG published:2015-03-24 summary:In supervised learning, simple baseline classifiers can be constructed byonly looking at the class, i.e., ignoring any other information from thedataset. The single-label learning community frequently uses as a reference theone which always predicts the majority class. Although a classifier mightperform worse than this simple baseline classifier, this behaviour requires aspecial explanation. Aiming to motivate the community to compare experimentalresults with the ones provided by a multi-label baseline classifier, callingthe attention about the need of special explanations related to classifierswhich perform worse than the baseline, in this work we propose the use ofGeneral_B, a multi-label baseline classifier. General_B was evaluated incontrast to results published in the literature which were carefully selectedusing a systematic review process. It was found that a considerable number ofpublished results on 10 frequently used datasets are worse than or equal to theones obtained by General_B, and for one dataset it reaches up to 43% of thedataset published results. Moreover, although a simple baseline classifier wasnot considered in these publications, it was observed that even for very poorresults no special explanations were provided in most of them. We hope that thefindings of this work would encourage the multi-label community to consider theidea of using a simple baseline classifier, such that further explanations areprovided when a classifiers performs worse than a baseline.
arxiv-9600-93 | PAC-Bayesian Theorems for Domain Adaptation with Specialization to Linear Classifiers | http://arxiv.org/pdf/1503.06944v1.pdf | author:Pascal Germain, Amaury Habrard, François Laviolette, Emilie Morvant category:stat.ML cs.LG published:2015-03-24 summary:In this paper, we provide two main contributions in PAC-Bayesian theory fordomain adaptation where the objective is to learn, from a source distribution,a well-performing majority vote on a different target distribution. On the onehand, we propose an improvement of the previous approach proposed by Germain etal. (2013), that relies on a novel distribution pseudodistance based on adisagreement averaging, allowing us to derive a new tighter PAC-Bayesian domainadaptation bound for the stochastic Gibbs classifier. We specialize it tolinear classifiers, and design a learning algorithm which shows interestingresults on a synthetic problem and on a popular sentiment annotation task. Onthe other hand, we generalize these results to multisource domain adaptationallowing us to take into account different source domains. This study opens thedoor to tackle domain adaptation tasks by making use of all the PAC-Bayesiantools.
arxiv-9600-94 | Measuring Software Quality in Use: State-of-the-Art and Research Challenges | http://arxiv.org/pdf/1503.06934v1.pdf | author:Issa Atoum, Chih How Bong category:cs.SE cs.CL published:2015-03-24 summary:Software quality in use comprises quality from the user's perspective. It hasgained its importance in e-government applications, mobile-based applications,embedded systems, and even business process development. User's decisions onsoftware acquisitions are often ad hoc or based on preference due to difficultyin quantitatively measuring software quality in use. But, why is quality-in-usemeasurement difficult? Although there are many software quality models, to theauthors' knowledge no works survey the challenges related to softwarequality-in-use measurement. This article has two main contributions: 1) itidentifies and explains major issues and challenges in measuring softwarequality in use in the context of the ISO SQuaRE series and related softwarequality models and highlights open research areas; and 2) it sheds light on aresearch direction that can be used to predict software quality in use. Inshort, the quality-in-use measurement issues are related to the complexity ofthe current standard models and the limitations and incompleteness of thecustomized software quality models. A sentiment analysis of software reviews isproposed to deal with these issues.
arxiv-9600-95 | Unsupervised Video Analysis Based on a Spatiotemporal Saliency Detector | http://arxiv.org/pdf/1503.06917v1.pdf | author:Qiang Zhang, Yilin Wang, Baoxin Li category:cs.CV published:2015-03-24 summary:Visual saliency, which predicts regions in the field of view that draw themost visual attention, has attracted a lot of interest from researchers. It hasalready been used in several vision tasks, e.g., image classification, objectdetection, foreground segmentation. Recently, the spectrum analysis basedvisual saliency approach has attracted a lot of interest due to its simplicityand good performance, where the phase information of the image is used toconstruct the saliency map. In this paper, we propose a new approach fordetecting spatiotemporal visual saliency based on the phase spectrum of thevideos, which is easy to implement and computationally efficient. With theproposed algorithm, we also study how the spatiotemporal saliency can be usedin two important vision task, abnormality detection and spatiotemporal interestpoint detection. The proposed algorithm is evaluated on several commonly useddatasets with comparison to the state-of-art methods from the literature. Theexperiments demonstrate the effectiveness of the proposed approach tospatiotemporal visual saliency detection and its application to the abovevision tasks
arxiv-9600-96 | Penalty, Shrinkage, and Preliminary Test Estimators under Full Model Hypothesis | http://arxiv.org/pdf/1503.06910v1.pdf | author:Enayetur Raheem, A. K. Md. Ehsanes Saleh category:math.ST stat.CO stat.ME stat.ML stat.TH published:2015-03-24 summary:This paper considers a multiple regression model and compares, under fullmodel hypothesis, analytically as well as by simulation, the performancecharacteristics of some popular penalty estimators such as ridge regression,LASSO, adaptive LASSO, SCAD, and elastic net versus Least Squares Estimator,restricted estimator, preliminary test estimator, and Stein-type estimatorswhen the dimension of the parameter space is smaller than the sample spacedimension. We find that RR uniformly dominates LSE, RE, PTE, SE and PRSE whileLASSO, aLASSO, SCAD, and EN uniformly dominates LSE only. Further, it isobserved that neither penalty estimators nor Stein-type estimator dominate oneanother.
arxiv-9600-97 | A Note on Information-Directed Sampling and Thompson Sampling | http://arxiv.org/pdf/1503.06902v1.pdf | author:Li Zhou category:cs.LG cs.AI published:2015-03-24 summary:This note introduce three Bayesian style Multi-armed bandit algorithms:Information-directed sampling, Thompson Sampling and Generalized ThompsonSampling. The goal is to give an intuitive explanation for these threealgorithms and their regret bounds, and provide some derivations that areomitted in the original papers.
arxiv-9600-98 | DistancePPG: Robust non-contact vital signs monitoring using a camera | http://arxiv.org/pdf/1502.08040v2.pdf | author:Mayank Kumar, Ashok Veeraraghavan, Ashutosh Sabharval category:cs.CV published:2015-02-27 summary:Vital signs such as pulse rate and breathing rate are currently measuredusing contact probes. But, non-contact methods for measuring vital signs aredesirable both in hospital settings (e.g. in NICU) and for ubiquitous in-situhealth tracking (e.g. on mobile phone and computers with webcams). Recently,camera-based non-contact vital sign monitoring have been shown to be feasible.However, camera-based vital sign monitoring is challenging for people withdarker skin tone, under low lighting conditions, and/or during movement of anindividual in front of the camera. In this paper, we propose distancePPG, a newcamera-based vital sign estimation algorithm which addresses these challenges.DistancePPG proposes a new method of combining skin-color change signals fromdifferent tracked regions of the face using a weighted average, where theweights depend on the blood perfusion and incident light intensity in theregion, to improve the signal-to-noise ratio (SNR) of camera-based estimate.One of our key contributions is a new automatic method for determining theweights based only on the video recording of the subject. The gains in SNR ofcamera-based PPG estimated using distancePPG translate into reduction of theerror in vital sign estimation, and thus expand the scope of camera-based vitalsign monitoring to potentially challenging scenarios. Further, a dataset willbe released, comprising of synchronized video recordings of face and pulseoximeter based ground truth recordings from the earlobe for people withdifferent skin tones, under different lighting conditions and for variousmotion scenarios.
arxiv-9600-99 | The local low-dimensionality of natural images | http://arxiv.org/pdf/1412.6626v4.pdf | author:Olivier J. Hénaff, Johannes Ballé, Neil C. Rabinowitz, Eero P. Simoncelli category:cs.CV published:2014-12-20 summary:We develop a new statistical model for photographic images, in which thelocal responses of a bank of linear filters are described as jointly Gaussian,with zero mean and a covariance that varies slowly over spatial position. Weoptimize sets of filters so as to minimize the nuclear norms of matrices oftheir local activations (i.e., the sum of the singular values), thusencouraging a flexible form of sparsity that is not tied to any particulardictionary or coordinate system. Filters optimized according to this objectiveare oriented and bandpass, and their responses exhibit substantial localcorrelation. We show that images can be reconstructed nearly perfectly fromestimates of the local filter response covariances alone, and with minimaldegradation (either visual or MSE) from low-rank approximations of thesecovariances. As such, this representation holds much promise for use inapplications such as denoising, compression, and texture representation, andmay form a useful substrate for hierarchical decompositions.
arxiv-9600-100 | On Lower and Upper Bounds for Smooth and Strongly Convex Optimization Problems | http://arxiv.org/pdf/1503.06833v1.pdf | author:Yossi Arjevani, Shai Shalev-Shwartz, Ohad Shamir category:math.OC cs.LG published:2015-03-23 summary:We develop a novel framework to study smooth and strongly convex optimizationalgorithms, both deterministic and stochastic. Focusing on quadratic functionswe are able to examine optimization algorithms as a recursive application oflinear operators. This, in turn, reveals a powerful connection between a classof optimization algorithms and the analytic theory of polynomials whereby newlower and upper bounds are derived. Whereas existing lower bounds for thissetting are only valid when the dimensionality scales with the number ofiterations, our lower bound holds in the natural regime where thedimensionality is fixed. Lastly, expressing it as an optimal solution for thecorresponding optimization problem over polynomials, as formulated by ourframework, we present a novel systematic derivation of Nesterov's well-knownAccelerated Gradient Descent method. This rather natural interpretation of AGDcontrasts with earlier ones which lacked a simple, yet solid, motivation.
arxiv-9600-101 | ROCKET: Robust Confidence Intervals via Kendall's Tau for Transelliptical Graphical Models | http://arxiv.org/pdf/1502.07641v2.pdf | author:Rina Foygel Barber, Mladen Kolar category:math.ST cs.LG stat.TH published:2015-02-26 summary:Undirected graphical models are used extensively in the biological and socialsciences to encode a pattern of conditional independences between variables,where the absence of an edge between two nodes $a$ and $b$ indicates that thecorresponding two variables $X_a$ and $X_b$ are believed to be conditionallyindependent, after controlling for all other measured variables. In theGaussian case, conditional independence corresponds to a zero entry in theprecision matrix $\Omega$ (the inverse of the covariance matrix $\Sigma$). Realdata often exhibits heavy tail dependence between variables, which cannot becaptured by the commonly-used Gaussian or nonparanormal (Gaussian copula)graphical models. In this paper, we study the transelliptical model, anelliptical copula model that generalizes Gaussian and nonparanormal models to abroader family of distributions. We propose the ROCKET method, which constructsan estimator of $\Omega_{ab}$ that we prove to be asymptotically normal undermild assumptions. Empirically, ROCKET outperforms the nonparanormal andGaussian models in terms of achieving accurate inference on simulated data. Wealso compare the three methods on real data (daily stock returns), and findthat the ROCKET estimator is the only method whose behavior across subsamplesagrees with the distribution predicted by the theory.
arxiv-9600-102 | Non-contact transmittance photoplethysmographic imaging (PPGI) for long-distance cardiovascular monitoring | http://arxiv.org/pdf/1503.06775v1.pdf | author:Robert Amelard, Christian Scharfenberger, Farnoud Kazemzadeh, Kaylen J. Pfisterer, Bill S. Lin, Alexander Wong, David A. Clausi category:physics.optics cs.CV published:2015-03-23 summary:Photoplethysmography (PPG) devices are widely used for monitoringcardiovascular function. However, these devices require skin contact, whichrestrict their use to at-rest short-term monitoring using single-pointmeasurements. Photoplethysmographic imaging (PPGI) has been recently proposedas a non-contact monitoring alternative by measuring blood pulse signals acrossa spatial region of interest. Existing systems operate in reflectance mode, ofwhich many are limited to short-distance monitoring and are prone to temporalchanges in ambient illumination. This paper is the first study to investigatethe feasibility of long-distance non-contact cardiovascular monitoring at thesupermeter level using transmittance PPGI. For this purpose, a novel PPGIsystem was designed at the hardware and software level using ambient correctionvia temporally coded illumination (TCI) and signal processing for PPGI signalextraction. Experimental results show that the processing steps yield asubstantially more pulsatile PPGI signal than the raw acquired signal,resulting in statistically significant increases in correlation to ground-truthPPG in both short- ($p \in [<0.0001, 0.040]$) and long-distance ($p \in[<0.0001, 0.056]$) monitoring. The results support the hypothesis thatlong-distance heart rate monitoring is feasible using transmittance PPGI,allowing for new possibilities of monitoring cardiovascular function in anon-contact manner.
arxiv-9600-103 | Unsupervised POS Induction with Word Embeddings | http://arxiv.org/pdf/1503.06760v1.pdf | author:Chu-Cheng Lin, Waleed Ammar, Chris Dyer, Lori Levin category:cs.CL published:2015-03-23 summary:Unsupervised word embeddings have been shown to be valuable as features insupervised learning problems; however, their role in unsupervised problems hasbeen less thoroughly explored. In this paper, we show that embeddings canlikewise add value to the problem of unsupervised POS induction. In tworepresentative models of POS induction, we replace multinomial distributionsover the vocabulary with multivariate Gaussian distributions over wordembeddings and observe consistent improvements in eight languages. We alsoanalyze the effect of various choices while inducing word embeddings on"downstream" POS induction results.
arxiv-9600-104 | Continuous Localization and Mapping of a Pan Tilt Zoom Camera for Wide Area Tracking | http://arxiv.org/pdf/1401.6606v2.pdf | author:Giuseppe Lisanti, Iacopo Masi, Federico Pernici, Alberto Del Bimbo category:cs.CV published:2014-01-26 summary:Pan-tilt-zoom (PTZ) cameras are powerful to support object identification andrecognition in far-field scenes. However, the effective use of PTZ cameras inreal contexts is complicated by the fact that a continuous on-line cameracalibration is needed and the absolute pan, tilt and zoom positional valuesprovided by the camera actuators cannot be used because are not synchronizedwith the video stream. So, accurate calibration must be directly extracted fromthe visual content of the frames. Moreover, the large and abrupt scale changes,the scene background changes due to the camera operation and the need of cameramotion compensation make target tracking with these cameras extremelychallenging. In this paper, we present a solution that provides continuouson-line calibration of PTZ cameras which is robust to rapid camera motion,changes of the environment due to illumination or moving objects and scalesbeyond thousands of landmarks. The method directly derives the relationshipbetween the position of a target in the 3D world plane and the correspondingscale and position in the 2D image, and allows real-time tracking of multipletargets with high and stable degree of accuracy even at far distances and anyzooming level.
arxiv-9600-105 | Online classifier adaptation for cost-sensitive learning | http://arxiv.org/pdf/1503.06745v1.pdf | author:Junlin Zhang, Jose Garcia category:cs.LG published:2015-03-23 summary:In this paper, we propose the problem of online cost-sensitive clas- sifieradaptation and the first algorithm to solve it. We assume we have a baseclassifier for a cost-sensitive classification problem, but it is trained withrespect to a cost setting different to the desired one. Moreover, we also havesome training data samples streaming to the algorithm one by one. The prob- lemis to adapt the given base classifier to the desired cost setting using thesteaming training samples online. To solve this problem, we propose to learn anew classifier by adding an adaptation function to the base classifier, andupdate the adaptation function parameter according to the streaming datasamples. Given a input data sample and the cost of misclassifying it, we up-date the adaptation function parameter by minimizing cost weighted hinge lossand respecting previous learned parameter simultaneously. The proposedalgorithm is compared to both online and off-line cost-sensitive algorithms ontwo cost-sensitive classification problems, and the experiments show that itnot only outperforms them one classification performances, but also requiressignificantly less running time.
arxiv-9600-106 | GSNs : Generative Stochastic Networks | http://arxiv.org/pdf/1503.05571v2.pdf | author:Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, Pascal Vincent category:cs.LG published:2015-03-18 summary:We introduce a novel training principle for probabilistic models that is analternative to maximum likelihood. The proposed Generative Stochastic Networks(GSN) framework is based on learning the transition operator of a Markov chainwhose stationary distribution estimates the data distribution. Because thetransition distribution is a conditional distribution generally involving asmall move, it has fewer dominant modes, being unimodal in the limit of smallmoves. Thus, it is easier to learn, more like learning to perform supervisedfunction approximation, with gradients that can be obtained byback-propagation. The theorems provided here generalize recent work on theprobabilistic interpretation of denoising auto-encoders and provide aninteresting justification for dependency networks and generalizedpseudolikelihood (along with defining an appropriate joint distribution andsampling mechanism, even when the conditionals are not consistent). We studyhow GSNs can be used with missing inputs and can be used to sample subsets ofvariables given the rest. Successful experiments are conducted, validatingthese theoretical results, on two image datasets and with a particulararchitecture that mimics the Deep Boltzmann Machine Gibbs sampler but allowstraining to proceed with backprop, without the need for layerwise pretraining.
arxiv-9600-107 | A Context-aware Delayed Agglomeration Framework for Electron Microscopy Segmentation | http://arxiv.org/pdf/1406.1476v5.pdf | author:Toufiq Parag, Anirban Chakraborty, Stephen Plaza, Lou Scheffer category:cs.CV published:2014-06-05 summary:Electron Microscopy (EM) image (or volume) segmentation has becomesignificantly important in recent years as an instrument for connectomics. Thispaper proposes a novel agglomerative framework for EM segmentation. Inparticular, given an over-segmented image or volume, we propose a novelframework for accurately clustering regions of the same neuron. Unlike existingagglomerative methods, the proposed context-aware algorithm divides superpixels(over-segmented regions) of different biological entities into differentsubsets and agglomerates them separately. In addition, this paper describes a"delayed" scheme for agglomerative clustering that postpones some of the mergedecisions, pertaining to newly formed bodies, in order to generate a moreconfident boundary prediction. We report significant improvements attained bythe proposed approach in segmentation accuracy over existing standard methodson 2D and 3D datasets.
arxiv-9600-108 | Vehicle Local Position Estimation System | http://arxiv.org/pdf/1503.06648v1.pdf | author:Mrinal Haloi, Dinesh Babu Jayagopi category:cs.CV 68T45 published:2015-03-23 summary:In this paper, a robust vehicle local position estimation with the help ofsingle camera sensor and GPS is presented. A modified Inverse PerspectiveMapping, illuminant Invariant techniques and object detection based approach isused to localize the vehicle in the road. Vehicles current lane, its positionfrom road boundary and other cars are used to define its local position. Forthis purpose Lane markings are detected using a Laplacian edge feature, robustto shadowing. Effect of shadowing and extra sun light are removed using Labcolor space and illuminant invariant techniques. Lanes are assumed to be asparabolic model and fitted using robust RANSAC. This method can reliably detectall lanes of the road, estimate lane departure angle and local position ofvehicle relative to lanes, road boundary and other cars. Different type ofobstacle like pedestrians, vehicles are detected using HOG feature baseddeformable part model.
arxiv-9600-109 | A novel pLSA based Traffic Signs Classification System | http://arxiv.org/pdf/1503.06643v1.pdf | author:Mrinal Haloi category:cs.CV 68T45 published:2015-03-23 summary:In this work we developed a novel and fast traffic sign recognition system, avery important part for advanced driver assistance system and for autonomousdriving. Traffic signs play a very vital role in safe driving and avoidingaccident. We have used image processing and topic discovery model pLSA totackle this challenging multiclass classification problem. Our algorithm isconsist of two parts, shape classification and sign classification for improvedaccuracy. For processing and representation of image we have used bag offeatures model with SIFT local descriptor. Where a visual vocabulary of size300 words are formed using k-means codebook formation algorithm. We exploitedthe concept that every image is a collection of visual topics and images havingsame topics will belong to same category. Our algorithm is tested on Germantraffic sign recognition benchmark (GTSRB) and gives very promising result nearto existing state of the art techniques.
arxiv-9600-110 | Superpixelizing Binary MRF for Image Labeling Problems | http://arxiv.org/pdf/1503.06642v1.pdf | author:Junyan Wang, Sai-Kit Yeung category:cs.CV published:2015-03-23 summary:Superpixels have become prevalent in computer vision. They have been used toachieve satisfactory performance at a significantly smaller computational costfor various tasks. People have also combined superpixels with Markov randomfield (MRF) models. However, it often takes additional effort to formulate MRFon superpixel-level, and to the best of our knowledge there exists noprincipled approach to obtain this formulation. In this paper, we show howgeneric pixel-level binary MRF model can be solved in the superpixel space. Asthe main contribution of this paper, we show that a superpixel-level MRF can bederived from the pixel-level MRF by substituting the superpixel representationof the pixelwise label into the original pixel-level MRF energy. The resultantsuperpixel-level MRF energy also remains submodular for a submodularpixel-level MRF. The derived formula hence gives us a handy way to formulateMRF energy in superpixel-level. In the experiments, we demonstrate the efficacyof our approach on several computer vision problems.
arxiv-9600-111 | Distributed Online Learning via Cooperative Contextual Bandits | http://arxiv.org/pdf/1308.4568v4.pdf | author:Cem Tekin, Mihaela van der Schaar category:cs.LG stat.ML published:2013-08-21 summary:In this paper we propose a novel framework for decentralized, online learningby many learners. At each moment of time, an instance characterized by acertain context may arrive to each learner; based on the context, the learnercan select one of its own actions (which gives a reward and providesinformation) or request assistance from another learner. In the latter case,the requester pays a cost and receives the reward but the provider learns theinformation. In our framework, learners are modeled as cooperative contextualbandits. Each learner seeks to maximize the expected reward from its arrivals,which involves trading off the reward received from its own actions, theinformation learned from its own actions, the reward received from the actionsrequested of others and the cost paid for these actions - taking into accountwhat it has learned about the value of assistance from each other learner. Wedevelop distributed online learning algorithms and provide analytic bounds tocompare the efficiency of these with algorithms with the complete knowledge(oracle) benchmark (in which the expected reward of every action in everycontext is known by every learner). Our estimates show that regret - the lossincurred by the algorithm - is sublinear in time. Our theoretical framework canbe used in many practical applications including Big Data mining, eventdetection in surveillance sensor networks and distributed online recommendationsystems.
arxiv-9600-112 | A Probabilistic Interpretation of Sampling Theory of Graph Signals | http://arxiv.org/pdf/1503.06629v1.pdf | author:Akshay Gadde, Antonio Ortega category:cs.LG published:2015-03-23 summary:We give a probabilistic interpretation of sampling theory of graph signals.To do this, we first define a generative model for the data using a pairwiseGaussian random field (GRF) which depends on the graph. We show that, undercertain conditions, reconstructing a graph signal from a subset of its samplesby least squares is equivalent to performing MAP inference on an approximationof this GRF which has a low rank covariance matrix. We then show that asampling set of given size with the largest associated cut-off frequency, whichis optimal from a sampling theoretic point of view, minimizes the worst casepredictive covariance of the MAP estimate on the GRF. This interpretation alsogives an intuitive explanation for the superior performance of the samplingtheoretic approach to active semi-supervised classification.
arxiv-9600-113 | Stochastic continuum armed bandit problem of few linear parameters in high dimensions | http://arxiv.org/pdf/1312.0232v3.pdf | author:Hemant Tyagi, Sebastian Stich, Bernd Gärtner category:stat.ML cs.LG math.OC published:2013-12-01 summary:We consider a stochastic continuum armed bandit problem where the arms areindexed by the $\ell_2$ ball $B_{d}(1+\nu)$ of radius $1+\nu$ in$\mathbb{R}^d$. The reward functions $r :B_{d}(1+\nu) \rightarrow \mathbb{R}$are considered to intrinsically depend on $k \ll d$ unknown linear parametersso that $r(\mathbf{x}) = g(\mathbf{A} \mathbf{x})$ where $\mathbf{A}$ is a fullrank $k \times d$ matrix. Assuming the mean reward function to be smooth wemake use of results from low-rank matrix recovery literature and derive anefficient randomized algorithm which achieves a regret bound of $O(C(k,d)n^{\frac{1+k}{2+k}} (\log n)^{\frac{1}{2+k}})$ with high probability. Here$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of roundsor the sampling budget which is assumed to be known beforehand.
arxiv-9600-114 | Proficiency Comparison of LADTree and REPTree Classifiers for Credit Risk Forecast | http://arxiv.org/pdf/1503.06608v1.pdf | author:Lakshmi Devasena C category:cs.LG published:2015-03-23 summary:Predicting the Credit Defaulter is a perilous task of Financial Industrieslike Banks. Ascertaining non-payer before giving loan is a significant andconflict-ridden task of the Banker. Classification techniques are the betterchoice for predictive analysis like finding the claimant, whether he/she is anunpretentious customer or a cheat. Defining the outstanding classifier is arisky assignment for any industrialist like a banker. This allow computerscience researchers to drill down efficient research works through evaluatingdifferent classifiers and finding out the best classifier for such predictiveproblems. This research work investigates the productivity of LADTreeClassifier and REPTree Classifier for the credit risk prediction and comparestheir fitness through various measures. German credit dataset has been takenand used to predict the credit risk with a help of open source machine learningtool.
arxiv-9600-115 | Using Distance Estimation and Deep Learning to Simplify Calibration in Food Calorie Measurement | http://arxiv.org/pdf/1502.03302v2.pdf | author:Pallavi Kuhad, Abdulsalam Yassine, Shervin Shirmohammadi category:cs.CY cs.HC cs.LG published:2015-02-11 summary:High calorie intake in the human body on the one hand, has proved harmful innumerous occasions leading to several diseases and on the other hand, astandard amount of calorie intake has been deemed essential by dieticians tomaintain the right balance of calorie content in human body. As such,researchers have proposed a variety of automatic tools and systems to assistusers measure their calorie in-take. In this paper, we consider the category ofthose tools that use image processing to recognize the food, and we propose amethod for fully automatic and user-friendly calibration of the dimension ofthe food portion sizes, which is needed in order to measure food portion weightand its ensuing amount of calories. Experimental results show that our method,which uses deep learning, mobile cloud computing, distance estimation and sizecalibration inside a mobile device, leads to an accuracy improvement to 95% onaverage compared to previous work
arxiv-9600-116 | Patterns in the English Language: Phonological Networks, Percolation and Assembly Models | http://arxiv.org/pdf/1410.4445v3.pdf | author:Massimo Stella, Markus Brede category:cs.CL published:2014-10-16 summary:In this paper we provide a quantitative framework for the study ofphonological networks (PNs) for the English language by carrying out principledcomparisons to null models, either based on site percolation, randomizationtechniques, or network growth models. In contrast to previous work, we mainlyfocus on null models that reproduce lower order characteristics of theempirical data. We find that artificial networks matching connectivityproperties of the English PN are exceedingly rare: this leads to the hypothesisthat the word repertoire might have been assembled over time by preferentiallyintroducing new words which are small modifications of old words. Our nullmodels are able to explain the "power-law-like" part of the degreedistributions and generally retrieve qualitative features of the PN such ashigh clustering, high assortativity coefficient, and small-worldcharacteristics. However, the detailed comparison to expectations from nullmodels also points out significant differences, suggesting the presence ofadditional constraints in word assembly. Key constraints we identify are theavoidance of large degrees, the avoidance of triadic closure, and the avoidanceof large non-percolating clusters.
arxiv-9600-117 | A Machine Learning Approach to Predicting the Smoothed Complexity of Sorting Algorithms | http://arxiv.org/pdf/1503.06572v1.pdf | author:Bichen Shi, Michel Schellekens, Georgiana Ifrim category:cs.LG cs.AI cs.CC published:2015-03-23 summary:Smoothed analysis is a framework for analyzing the complexity of analgorithm, acting as a bridge between average and worst-case behaviour. Forexample, Quicksort and the Simplex algorithm are widely used in practicalapplications, despite their heavy worst-case complexity. Smoothed complexityaims to better characterize such algorithms. Existing theoretical bounds forthe smoothed complexity of sorting algorithms are still quite weak.Furthermore, empirically computing the smoothed complexity via its originaldefinition is computationally infeasible, even for modest input sizes. In thispaper, we focus on accurately predicting the smoothed complexity of sortingalgorithms, using machine learning techniques. We propose two regression modelsthat take into account various properties of sorting algorithms and some of theknown theoretical results in smoothed analysis to improve prediction quality.We show experimental results for predicting the smoothed complexity ofQuicksort, Mergesort, and optimized Bubblesort for large input sizes, thereforefilling the gap between known theoretical and empirical results.
arxiv-9600-118 | A Comparative Analysis of Tensor Decomposition Models Using Hyper Spectral Image | http://arxiv.org/pdf/1503.06561v1.pdf | author:Ankit Gupta, Ashish Oberoi category:cs.NA cs.CV published:2015-03-23 summary:Hyper spectral imaging is a remote sensing technology, providing variety ofapplications such as material identification, space object identification,planetary exploitation etc. It deals with capturing continuum of images of theearth surface from different angles. Due to the multidimensional nature of theimage, multi-way arrays are one of the possible solutions for analyzing hyperspectral data. This multi-way array is called tensor. Our approach deals withimplementing three decomposition models LMLRA, BTD and CPD to the sample datafor choosing the best decomposition of the data set. The results have provedthat Block Term Decomposition (BTD) is the best tensor model for decomposingthe hyper spectral image in to resultant factor matrices.
arxiv-9600-119 | Deep metric learning using Triplet network | http://arxiv.org/pdf/1412.6622v3.pdf | author:Elad Hoffer, Nir Ailon category:cs.LG cs.CV stat.ML published:2014-12-20 summary:Deep learning has proven itself as a successful set of models for learninguseful semantic representations of data. These, however, are mostly implicitlylearned as part of a classification task. In this paper we propose the tripletnetwork model, which aims to learn useful representations by distancecomparisons. A similar model was defined by Wang et al. (2014), tailor made forlearning a ranking for image information retrieval. Here we demonstrate usingvarious datasets that our model learns a better representation than that of itsimmediate competitor, the Siamese network. We also discuss future possibleusage as a framework for unsupervised learning.
arxiv-9600-120 | Optimum Reject Options for Prototype-based Classification | http://arxiv.org/pdf/1503.06549v1.pdf | author:Lydia Fischer, Barbara Hammer, Heiko Wersing category:cs.LG published:2015-03-23 summary:We analyse optimum reject strategies for prototype-based classifiers andreal-valued rejection measures, using the distance of a data point to theclosest prototype or probabilistic counterparts. We compare reject schemes withglobal thresholds, and local thresholds for the Voronoi cells of theclassifier. For the latter, we develop a polynomial-time algorithm to computeoptimum thresholds based on a dynamic programming scheme, and we propose anintuitive linear time, memory efficient approximation thereof with competitiveaccuracy. Evaluating the performance in various benchmarks, we conclude thatlocal reject options are beneficial in particular for simple prototype-basedclassifiers, while the improvement is less pronounced for advanced models. Forthe latter, an accuracy-reject curve which is comparable to support vectormachine classifiers with state of the art reject options can be reached.
arxiv-9600-121 | Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation | http://arxiv.org/pdf/1503.03562v3.pdf | author:Zhiyong Cheng, Daniel Soudry, Zexi Mao, Zhenzhong Lan category:cs.NE cs.CV cs.LG published:2015-03-12 summary:Compared to Multilayer Neural Networks with real weights, Binary MultilayerNeural Networks (BMNNs) can be implemented more efficiently on dedicatedhardware. BMNNs have been demonstrated to be effective on binary classificationtasks with Expectation BackPropagation (EBP) algorithm on high dimensional textdatasets. In this paper, we investigate the capability of BMNNs using the EBPalgorithm on multiclass image classification tasks. The performances of binaryneural networks with multiple hidden layers and different numbers of hiddenunits are examined on MNIST. We also explore the effectiveness of image spatialfilters and the dropout technique in BMNNs. Experimental results on MNISTdataset show that EBP can obtain 2.12% test error with binary weights and 1.66%test error with real weights, which is comparable to the results of standardBackPropagation algorithm on fully connected MNNs.
arxiv-9600-122 | Machine Learning Methods for Attack Detection in the Smart Grid | http://arxiv.org/pdf/1503.06468v1.pdf | author:Mete Ozay, Inaki Esnaola, Fatos T. Yarman Vural, Sanjeev R. Kulkarni, H. Vincent Poor category:cs.LG cs.CR cs.SY published:2015-03-22 summary:Attack detection problems in the smart grid are posed as statistical learningproblems for different attack scenarios in which the measurements are observedin batch or online settings. In this approach, machine learning algorithms areused to classify measurements as being either secure or attacked. An attackdetection framework is provided to exploit any available prior knowledge aboutthe system and surmount constraints arising from the sparse structure of theproblem in the proposed approach. Well-known batch and online learningalgorithms (supervised and semi-supervised) are employed with decision andfeature level fusion to model the attack detection problem. The relationshipsbetween statistical and geometric properties of attack vectors employed in theattack scenarios and learning algorithms are analyzed to detect unobservableattacks using statistical learning methods. The proposed algorithms areexamined on various IEEE test systems. Experimental analyses show that machinelearning algorithms can detect attacks with performances higher than the attackdetection algorithms which employ state vector estimation methods in theproposed attack detection framework.
arxiv-9600-123 | Lifting Object Detection Datasets into 3D | http://arxiv.org/pdf/1503.06465v1.pdf | author:Joao Carreira, Sara Vicente, Lourdes Agapito, Jorge Batista category:cs.CV published:2015-03-22 summary:While data has certainly taken the center stage in computer vision in recentyears, it can still be difficult to obtain in certain scenarios. In particular,acquiring ground truth 3D shapes of objects pictured in 2D images remains achallenging feat and this has hampered progress in recognition-based objectreconstruction from a single image. Here we propose to bypass previoussolutions such as 3D scanning or manual design, that scale poorly, and insteadpopulate object category detection datasets semi-automatically with dense,per-object 3D reconstructions, bootstrapped from:(i) class labels, (ii) groundtruth figure-ground segmentations and (iii) a small set of keypointannotations. Our proposed algorithm first estimates camera viewpoint usingrigid structure-from-motion and then reconstructs object shapes by optimizingover visual hull proposals guided by loose within-class shape similarityassumptions. The visual hull sampling process attempts to intersect an object'sprojection cone with the cones of minimal subsets of other similar objectsamong those pictured from certain vantage points. We show that our method isable to produce convincing per-object 3D reconstructions and to accuratelyestimate cameras viewpoints on one of the most challenging existingobject-category detection datasets, PASCAL VOC. We hope that our results willre-stimulate interest on joint object recognition and 3D reconstruction from asingle image.
arxiv-9600-124 | Unsupervised model compression for multilayer bootstrap networks | http://arxiv.org/pdf/1503.06452v1.pdf | author:Xiao-Lei Zhang category:cs.LG cs.NE stat.ML published:2015-03-22 summary:Recently, multilayer bootstrap network (MBN) has demonstrated promisingperformance in unsupervised dimensionality reduction. It can learn compactrepresentations in standard data sets, i.e. MNIST and RCV1. However, as abootstrap method, the prediction complexity of MBN is high. In this paper, wepropose an unsupervised model compression framework for this general problem ofunsupervised bootstrap methods. The framework compresses a large unsupervisedbootstrap model into a small model by taking the bootstrap model and itsapplication together as a black box and learning a mapping function from theinput of the bootstrap model to the output of the application by a supervisedlearner. To specialize the framework, we propose a new technique, namedcompressive MBN. It takes MBN as the unsupervised bootstrap model and deepneural network (DNN) as the supervised learner. Our initial result on MNISTshowed that compressive MBN not only maintains the high prediction accuracy ofMBN but also is over thousands of times faster than MBN at the predictionstage. Our result suggests that the new technique integrates the effectivenessof MBN on unsupervised learning and the effectiveness and efficiency of DNN onsupervised learning together for the effectiveness and efficiency ofcompressive MBN on unsupervised learning.
arxiv-9600-125 | Retrofitting Word Vectors to Semantic Lexicons | http://arxiv.org/pdf/1411.4166v4.pdf | author:Manaal Faruqui, Jesse Dodge, Sujay K. Jauhar, Chris Dyer, Eduard Hovy, Noah A. Smith category:cs.CL published:2014-11-15 summary:Vector space word representations are learned from distributional informationof words in large corpora. Although such statistics are semanticallyinformative, they disregard the valuable information that is contained insemantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. Thispaper proposes a method for refining vector space representations usingrelational information from semantic lexicons by encouraging linked words tohave similar vector representations, and it makes no assumptions about how theinput vectors were constructed. Evaluated on a battery of standard lexicalsemantic evaluation tasks in several languages, we obtain substantialimprovements starting with a variety of word vector models. Our refinementmethod outperforms prior techniques for incorporating semantic lexicons intothe word vector training algorithms.
arxiv-9600-126 | Cross-language Wikipedia Editing of Okinawa, Japan | http://arxiv.org/pdf/1501.00657v2.pdf | author:Scott A. Hale category:cs.CY cs.CL cs.SI H.5.4, H.5.3 published:2015-01-04 summary:This article analyzes users who edit Wikipedia articles about Okinawa, Japan,in English and Japanese. It finds these users are among the most active anddedicated users in their primary languages, where they make many large,high-quality edits. However, when these users edit in their non-primarylanguages, they tend to make edits of a different type that are overall smallerin size and more often restricted to the narrow set of articles that exist inboth languages. Design changes to motivate wider contributions from users intheir non-primary languages and to encourage multilingual users to transfermore information across language divides are presented.
arxiv-9600-127 | Indian Buffet process for model selection in convolved multiple-output Gaussian processes | http://arxiv.org/pdf/1503.06432v1.pdf | author:Cristian Guarnizo, Mauricio A. Álvarez category:stat.ML published:2015-03-22 summary:Multi-output Gaussian processes have received increasing attention during thelast few years as a natural mechanism to extend the powerful flexibility ofGaussian processes to the setup of multiple output variables. The key pointhere is the ability to design kernel functions that allow exploiting thecorrelations between the outputs while fulfilling the positive definitenessrequisite for the covariance function. Alternatives to construct thesecovariance functions are the linear model of coregionalization and processconvolutions. Each of these methods demand the specification of the number oflatent Gaussian process used to build the covariance function for the outputs.We propose in this paper, the use of an Indian Buffet process as a way toperform model selection over the number of latent Gaussian processes. This typeof model is particularly important in the context of latent force models, wherethe latent forces are associated to physical quantities like protein profilesor latent forces in mechanical systems. We use variational inference toestimate posterior distributions over the variables involved, and show examplesof the model performance over artificial data, a motion capture dataset, and agene expression dataset.
arxiv-9600-128 | Asymmetric Distributions from Constrained Mixtures | http://arxiv.org/pdf/1503.06429v1.pdf | author:Conrado S. Miranda, Fernando J. Von Zuben category:stat.ML cs.LG published:2015-03-22 summary:This paper introduces constrained mixtures for continuous distributions,characterized by a mixture of distributions where each distribution has a shapesimilar to the base distribution and disjoint domains. This new concept is usedto create generalized asymmetric versions of the Laplace and normaldistributions, which are shown to define exponential families, with knownconjugate priors, and to have maximum likelihood estimates for the originalparameters, with known closed-form expressions. The asymmetric and symmetricnormal distributions are compared in a linear regression example, showing thatthe asymmetric version performs at least as well as the symmetric one, and in areal world time-series problem, where a hidden Markov model is used to fit astock index, indicating that the asymmetric version provides higher likelihoodand may learn distribution models over states and transition distributions withconsiderably less entropy.
arxiv-9600-129 | Modeling browser-based distributed evolutionary computation systems | http://arxiv.org/pdf/1503.06424v1.pdf | author:Juan Julián Merelo-Guervós, Pablo García-Sánchez category:cs.DC cs.NE cs.NI published:2015-03-22 summary:From the era of big science we are back to the "do it yourself", where you donot have any money to buy clusters or subscribe to grids but still havealgorithms that crave many computing nodes and need them to measurescalability. Fortunately, this coincides with the era of big data, cloudcomputing, and browsers that include JavaScript virtual machines. Those are thereasons why this paper will focus on two different aspects of volunteer orfreeriding computing: first, the pragmatic: where to find those resources,which ones can be used, what kind of support you have to give them; and then,the theoretical: how evolutionary algorithms can be adapted to an environmentin which nodes come and go, have different computing capabilities and operatein complete asynchrony of each other. We will examine the setup needed tocreate a very simple distributed evolutionary algorithm using JavaScript andthen find a model of how users react to it by collecting data from severalexperiments featuring different classical benchmark functions.
arxiv-9600-130 | What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes | http://arxiv.org/pdf/1503.06410v1.pdf | author:David M. W. Powers category:cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML published:2015-03-22 summary:The F-measure or F-score is one of the most commonly used single numbermeasures in Information Retrieval, Natural Language Processing and MachineLearning, but it is based on a mistake, and the flawed assumptions render itunsuitable for use in most contexts! Fortunately, there are betteralternatives.
arxiv-9600-131 | Large-scale Log-determinant Computation through Stochastic Chebyshev Expansions | http://arxiv.org/pdf/1503.06394v1.pdf | author:Insu Han, Dmitry Malioutov, Jinwoo Shin category:cs.DS cs.LG cs.NA published:2015-03-22 summary:Logarithms of determinants of large positive definite matrices appearubiquitously in machine learning applications including Gaussian graphical andGaussian process models, partition functions of discrete graphical models,minimum-volume ellipsoids, metric learning and kernel learning. Log-determinantcomputation involves the Cholesky decomposition at the cost cubic in the numberof variables, i.e., the matrix dimension, which makes it prohibitive forlarge-scale applications. We propose a linear-time randomized algorithm toapproximate log-determinants for very large-scale positive definite and generalnon-singular matrices using a stochastic trace approximation, called theHutchinson method, coupled with Chebyshev polynomial expansions that both relyon efficient matrix-vector multiplications. We establish rigorous additive andmultiplicative approximation error bounds depending on the condition number ofthe input matrix. In our experiments, the proposed algorithm can provide veryhigh accuracy solutions at orders of magnitude faster time than the Choleskydecomposition and Schur completion, and enables us to compute log-determinantsof matrices involving tens of millions of variables.
arxiv-9600-132 | Costing Generated Runtime Execution Plans for Large-Scale Machine Learning Programs | http://arxiv.org/pdf/1503.06384v1.pdf | author:Matthias Boehm category:cs.DC cs.LG published:2015-03-22 summary:Declarative large-scale machine learning (ML) aims at the specification of MLalgorithms in a high-level language and automatic generation of hybrid runtimeexecution plans ranging from single node, in-memory computations to distributedcomputations on MapReduce (MR) or similar frameworks like Spark. Thecompilation of large-scale ML programs exhibits many opportunities forautomatic optimization. Advanced cost-based optimization techniquesrequire---as a fundamental precondition---an accurate cost model for evaluatingthe impact of optimization decisions. In this paper, we share insights into asimple and robust yet accurate technique for costing alternative runtimeexecution plans of ML programs. Our cost model relies on generating and costingruntime plans in order to automatically reflect all successive optimizationphases. Costing runtime plans also captures control flow structures such asloops and branches, and a variety of cost factors like IO, latency, andcomputation costs. Finally, we linearize all these cost factors into a singlemeasure of expected execution time. Within SystemML, this cost model isleveraged by several advanced optimizers like resource optimization and globaldata flow optimization. We share our lessons learned in order to providefoundations for the optimization of ML programs.
arxiv-9600-133 | Real-time Dynamic MRI Reconstruction using Stacked Denoising Autoencoder | http://arxiv.org/pdf/1503.06383v1.pdf | author:Angshul Majumdar category:cs.CV cs.NE published:2015-03-22 summary:In this work we address the problem of real-time dynamic MRI reconstruction.There are a handful of studies on this topic; these techniques are either basedon compressed sensing or employ Kalman Filtering. These techniques cannotachieve the reconstruction speed necessary for real-time reconstruction. Inthis work, we propose a new approach to MRI reconstruction. We learn anon-linear mapping from the unstructured aliased images to the correspondingclean images using a stacked denoising autoencoder (SDAE). The training forSDAE is slow, but the reconstruction is very fast - only requiring a few matrixvector multiplications. In this work, we have shown that using SDAE one canreconstruct the MRI frame faster than the data acquisition rate, therebyachieving real-time reconstruction. The quality of reconstruction is of thesame order as a previous compressed sensing based online reconstructiontechnique.
arxiv-9600-134 | Boosting Convolutional Features for Robust Object Proposals | http://arxiv.org/pdf/1503.06350v1.pdf | author:Nikolaos Karianakis, Thomas J. Fuchs, Stefano Soatto category:cs.CV cs.AI cs.LG published:2015-03-21 summary:Deep Convolutional Neural Networks (CNNs) have demonstrated excellentperformance in image classification, but still show room for improvement inobject-detection tasks with many categories, in particular for cluttered scenesand occlusion. Modern detection algorithms like Regions with CNNs (Girshick etal., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regionswhich with high probability represent objects, where in turn CNNs are deployedfor classification. Selective Search represents a family of sophisticatedalgorithms that are engineered with multiple segmentation, appearance andsaliency cues, typically coming with a significant run-time overhead.Furthermore, (Hosang et al., 2014) have shown that most methods suffer from lowreproducibility due to unstable superpixels, even for slight imageperturbations. Although CNNs are subsequently used for classification intop-performing object-detection pipelines, current proposal methods areagnostic to how these models parse objects and their rich learnedrepresentations. As a result they may propose regions which may not resemblehigh-level objects or totally miss some of them. To overcome these drawbacks wepropose a boosting approach which directly takes advantage of hierarchical CNNfeatures for detecting regions of interest fast. We demonstrate its performanceon ImageNet 2013 detection benchmark and compare it with state-of-the-artmethods.
arxiv-9600-135 | Using novelty-biased GA to sample diversity in graphs satisfying constraints | http://arxiv.org/pdf/1503.06342v1.pdf | author:Peter Overbury, Luc Berthouze category:physics.soc-ph cs.NE cs.SI math.CO G.2.2 published:2015-03-21 summary:The structure of the network underlying many complex systems, whetherartificial or natural, plays a significant role in how these systems operate.As a result, much emphasis has been placed on accurately describing networksusing network theoretic metrics. When it comes to generating networks withsimilar properties, however, the set of available techniques and propertiesthat can be controlled for remains limited. Further, whilst it is becomingclear that some of the metrics currently used to control the generation of suchnetworks are not very prescriptive so that networks could potentially exhibitvery different higher-order structure within those constraints, networkgenerating algorithms typically produce fairly contrived networks and lackmechanisms by which to systematically explore the space of network solutions.In this paper, we explore the potential of a multi-objective novelty-biased GAto provide a viable alternative to these algorithms. We believe our resultsprovide the first proof of principle that (i) it is possible to use GAs togenerate graphs satisfying set levels of key classical graph theoreticproperties and (ii) it is possible to generate diverse solutions within theseconstraints. The paper is only a preliminary step, however, and we identify keyavenues for further development.
arxiv-9600-136 | $\ell_p$ Testing and Learning of Discrete Distributions | http://arxiv.org/pdf/1412.2314v4.pdf | author:Bo Waggoner category:cs.DS cs.LG math.ST stat.TH F.2.0; G.3 published:2014-12-07 summary:The classic problems of testing uniformity of and learning a discretedistribution, given access to independent samples from it, are examined undergeneral $\ell_p$ metrics. The intuitions and results often contrast with theclassic $\ell_1$ case. For $p > 1$, we can learn and test with a number ofsamples that is independent of the support size of the distribution: With an$\ell_p$ tolerance $\epsilon$, $O(\max\{ \sqrt{1/\epsilon^q}, 1/\epsilon^2 \})$samples suffice for testing uniformity and $O(\max\{ 1/\epsilon^q,1/\epsilon^2\})$ samples suffice for learning, where $q=p/(p-1)$ is theconjugate of $p$. As this parallels the intuition that $O(\sqrt{n})$ and $O(n)$samples suffice for the $\ell_1$ case, it seems that $1/\epsilon^q$ acts as anupper bound on the "apparent" support size. For some $\ell_p$ metrics, uniformity testing becomes easier over largersupports: a 6-sided die requires fewer trials to test for fairness than a2-sided coin, and a card-shuffler requires fewer trials than the die. In fact,this inverse dependence on support size holds if and only if $p > \frac{4}{3}$.The uniformity testing algorithm simply thresholds the number of "collisions"or "coincidences" and has an optimal sample complexity up to constant factorsfor all $1 \leq p \leq 2$. Another algorithm gives order-optimal samplecomplexity for $\ell_{\infty}$ uniformity testing. Meanwhile, the most naturallearning algorithm is shown to have order-optimal sample complexity for all$\ell_p$ metrics. The author thanks Cl\'{e}ment Canonne for discussions and contributions tothis work.
arxiv-9600-137 | Wavelet based approach for tissue fractal parameter measurement: Pre cancer detection | http://arxiv.org/pdf/1503.06323v1.pdf | author:Sabyasachi Mukhopadhyay, Nandan K. Das, Soham Mandal, Sawon Pratiher, Asish Mitra, Asima Pradhan, Nirmalya Ghosh, Prasanta K. Panigrahi category:cs.CV published:2015-03-21 summary:In this paper, we have carried out the detail studies of pre-cancer bywavelet coherency and multifractal based detrended fluctuation analysis (MFDFA)on differential interference contrast (DIC) images of stromal region amongdifferent grades of pre-cancer tissues. Discrete wavelet transform (DWT)through Daubechies basis has been performed for identifying fluctuations overpolynomial trends for clear characterization and differentiation of tissues.Wavelet coherence plots are performed for identifying the level of correlationin time scale plane between normal and various grades of DIC samples. ApplyingMFDFA on refractive index variations of cervical tissues, we have observed thatthe values of Hurst exponent (correlation) decreases from healthy (normal) topre-cancer tissues. The width of singularity spectrum has a sudden degradationat grade-I in comparison of healthy (normal) tissue but later on it increasesas cancer progresses from grade-II to grade-III.
arxiv-9600-138 | Incorporating Both Distributional and Relational Semantics in Word Representations | http://arxiv.org/pdf/1412.5836v3.pdf | author:Daniel Fried, Kevin Duh category:cs.CL published:2014-12-18 summary:We investigate the hypothesis that word representations ought to incorporateboth distributional and relational semantics. To this end, we employ theAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes adistributional objective on raw text and a relational objective on WordNet.Preliminary results on knowledge base completion, analogy tests, and parsingshow that word representations trained on both objectives can give improvementsin some cases.
arxiv-9600-139 | Incorporating Both Distributional and Relational Semantics in Word Representations | http://arxiv.org/pdf/1412.4369v3.pdf | author:Daniel Fried, Kevin Duh category:cs.CL published:2014-12-14 summary:We investigate the hypothesis that word representations ought to incorporateboth distributional and relational semantics. To this end, we employ theAlternating Direction Method of Multipliers (ADMM), which flexibly optimizes adistributional objective on raw text and a relational objective on WordNet.Preliminary results on knowledge base completion, analogy tests, and parsingshow that word representations trained on both objectives can give improvementsin some cases.
arxiv-9600-140 | Skin Detection of Animation Characters | http://arxiv.org/pdf/1503.06275v1.pdf | author:Kazi Tanvir Ahmed Siddiqui, Abu Wasif category:cs.CV published:2015-03-21 summary:The increasing popularity of animes makes it vulnerable to unwanted usageslike copyright violations and pornography. That is why, we need to develop amethod to detect and recognize animation characters. Skin detection is one ofthe most important steps in this way. Though there are some methods to detecthuman skin color, but those methods do not work properly for anime characters.Anime skin varies greatly from human skin in color, texture, tone and indifferent kinds of lighting. They also vary greatly among themselves. Moreover,many other things (for example leather, shirt, hair etc.), which are not skin,can have color similar to skin. In this paper, we have proposed three methodsthat can identify an anime character skin more successfully as compared withKovac, Swift, Saleh and Osman methods, which are primarily designed for humanskin detection. Our methods are based on RGB values and their comparativerelations.
arxiv-9600-141 | Hierarchical sparse Bayesian learning: theory and application for inferring structural damage from incomplete modal data | http://arxiv.org/pdf/1503.06267v1.pdf | author:Yong Huang, James L. Beck category:stat.AP stat.ME stat.ML published:2015-03-21 summary:Structural damage due to excessive loading or environmental degradationtypically occurs in localized areas in the absence of collapse. This priorinformation about the spatial sparseness of structural damage is exploited hereby a hierarchical sparse Bayesian learning framework with the goal of reducingthe source of ill-conditioning in the stiffness loss inversion problem fordamage detection. Sparse Bayesian learning methodologies automatically pruneaway irrelevant or inactive features from a set of potential candidates, and sothey are effective probabilistic tools for producing sparse explanatorysubsets. We have previously proposed such an approach to establish theprobability of localized stiffness reductions that serve as a proxy for damageby using noisy incomplete modal data from before and after possible damage. Thecore idea centers on a specific hierarchical Bayesian model that promotesspatial sparseness in the inferred stiffness reductions in a way that isconsistent with the Bayesian Ockham razor. In this paper, we improve the theoryof our previously proposed sparse Bayesian learning approach by eliminating anapproximation and, more importantly, incorporating a constraint on stiffnessincreases. Our approach has many appealing features that are summarized at theend of the paper. We validate the approach by applying it to the Phase IIsimulated and experimental benchmark studies sponsored by the IASC-ASCE TaskGroup on Structural Health Monitoring. The results show that it can reliablydetect, locate and assess damage by inferring substructure stiffness lossesfrom the identified modal parameters. The occurrence of missed and false damagealerts is effectively suppressed.
arxiv-9600-142 | A Randomized Nonmonotone Block Proximal Gradient Method for a Class of Structured Nonlinear Programming | http://arxiv.org/pdf/1306.5918v2.pdf | author:Zhaosong Lu, Lin Xiao category:math.OC cs.LG cs.NA math.NA stat.ML published:2013-06-25 summary:We propose a randomized nonmonotone block proximal gradient (RNBPG) methodfor minimizing the sum of a smooth (possibly nonconvex) function and ablock-separable (possibly nonconvex nonsmooth) function. At each iteration,this method randomly picks a block according to any prescribed probabilitydistribution and solves typically several associated proximal subproblems thatusually have a closed-form solution, until a certain progress on objectivevalue is achieved. In contrast to the usual randomized block coordinate descentmethod [23,20], our method has a nonmonotone flavor and uses variable stepsizesthat can partially utilize the local curvature information of the smoothcomponent of objective function. We show that any accumulation point of thesolution sequence of the method is a stationary point of the problem {\italmost surely} and the method is capable of finding an approximate stationarypoint with high probability. We also establish a sublinear rate of convergencefor the method in terms of the minimal expected squared norm of certainproximal gradients over the iterations. When the problem under consideration isconvex, we show that the expected objective values generated by RNBPG convergeto the optimal value of the problem. Under some assumptions, we furtherestablish a sublinear and linear rate of convergence on the expected objectivevalues generated by a monotone version of RNBPG. Finally, we conduct somepreliminary experiments to test the performance of RNBPG on the$\ell_1$-regularized least-squares problem and a dual SVM problem in machinelearning. The computational results demonstrate that our method substantiallyoutperforms the randomized block coordinate {\it descent} method with fixed orvariable stepsizes.
arxiv-9600-143 | Fast Imbalanced Classification of Healthcare Data with Missing Values | http://arxiv.org/pdf/1503.06250v1.pdf | author:Talayeh Razzaghi, Oleg Roderick, Ilya Safro, Nick Marko category:stat.ML cs.LG published:2015-03-21 summary:In medical domain, data features often contain missing values. This cancreate serious bias in the predictive modeling. Typical standard data miningmethods often produce poor performance measures. In this paper, we propose anew method to simultaneously classify large datasets and reduce the effects ofmissing values. The proposed method is based on a multilevel framework of thecost-sensitive SVM and the expected maximization imputation method for missingvalues, which relies on iterated regression analyses. We compare classificationresults of multilevel SVM-based algorithms on public benchmark datasets withimbalanced classes and missing values as well as real data in healthapplications, and show that our multilevel SVM-based method produces fast, andmore accurate and robust classification results.
arxiv-9600-144 | Block-Wise MAP Inference for Determinantal Point Processes with Application to Change-Point Detection | http://arxiv.org/pdf/1503.06239v1.pdf | author:Jinye Zhang, Zhijian Ou category:cs.LG cs.AI stat.ME stat.ML published:2015-03-20 summary:Existing MAP inference algorithms for determinantal point processes (DPPs)need to calculate determinants or conduct eigenvalue decomposition generally atthe scale of the full kernel, which presents a great challenge for real-worldapplications. In this paper, we introduce a class of DPPs, called BwDPPs, thatare characterized by an almost block diagonal kernel matrix and thus can allowefficient block-wise MAP inference. Furthermore, BwDPPs are successfullyapplied to address the difficulty of selecting change-points in the problem ofchange-point detection (CPD), which results in a new BwDPP-based CPD method,named BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates isfirst created based on existing well-studied metrics. Then, these change-pointcandidates are treated as DPP items, and DPP-based subset selection isconducted to give the final estimate of the change-points that favours bothquality and diversity. The effectiveness of BwDppCpd is demonstrated throughextensive experiments on five real-world datasets.
arxiv-9600-145 | Explaining and Harnessing Adversarial Examples | http://arxiv.org/pdf/1412.6572v3.pdf | author:Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy category:stat.ML cs.LG published:2014-12-20 summary:Several machine learning models, including neural networks, consistentlymisclassify adversarial examples---inputs formed by applying small butintentionally worst-case perturbations to examples from the dataset, such thatthe perturbed input results in the model outputting an incorrect answer withhigh confidence. Early attempts at explaining this phenomenon focused onnonlinearity and overfitting. We argue instead that the primary cause of neuralnetworks' vulnerability to adversarial perturbation is their linear nature.This explanation is supported by new quantitative results while giving thefirst explanation of the most intriguing fact about them: their generalizationacross architectures and training sets. Moreover, this view yields a simple andfast method of generating adversarial examples. Using this approach to provideexamples for adversarial training, we reduce the test set error of a maxoutnetwork on the MNIST dataset.
arxiv-9600-146 | Networked Stochastic Multi-Armed Bandits with Combinatorial Strategies | http://arxiv.org/pdf/1503.06169v1.pdf | author:Shaojie Tang, Yaqin Zhou category:cs.LG published:2015-03-20 summary:In this paper, we investigate a largely extended version of classical MABproblem, called networked combinatorial bandit problems. In particular, weconsider the setting of a decision maker over a networked bandits as follows:each time a combinatorial strategy, e.g., a group of arms, is chosen, and thedecision maker receives a reward resulting from her strategy and also receivesa side bonus resulting from that strategy for each arm's neighbor. This ismotivated by many real applications such as on-line social networks wherefriends can provide their feedback on shared content, therefore if we promote aproduct to a user, we can also collect feedback from her friends on thatproduct. To this end, we consider two types of side bonus in this study: sideobservation and side reward. Upon the number of arms pulled at each time slot,we study two cases: single-play and combinatorial-play. Consequently, thisleaves us four scenarios to investigate in the presence of side bonus:Single-play with Side Observation, Combinatorial-play with Side Observation,Single-play with Side Reward, and Combinatorial-play with Side Reward. For eachcase, we present and analyze a series of \emph{zero regret} polices where theexpect of regret over time approaches zero as time goes to infinity. Extensivesimulations validate the effectiveness of our results.
arxiv-9600-147 | Pragmatic Neural Language Modelling in Machine Translation | http://arxiv.org/pdf/1412.7119v3.pdf | author:Paul Baltescu, Phil Blunsom category:cs.CL published:2014-12-22 summary:This paper presents an in-depth investigation on integrating neural languagemodels in translation systems. Scaling neural language models is a difficulttask, but crucial for real-world applications. This paper evaluates the impacton end-to-end MT quality of both new and existing scaling techniques. We showwhen explicitly normalising neural models is necessary and what optimisationtricks one should use in such scenarios. We also focus on scalable trainingalgorithms and investigate noise contrastive estimation and diagonal contextsas sources for further speed improvements. We explore the trade-offs betweenneural models and back-off n-gram models and find that neural models makestrong candidates for natural language applications in memory constrainedenvironments, yet still lag behind traditional models in raw translationquality. We conclude with a set of recommendations one should follow to build ascalable neural language model for MT.
arxiv-9600-148 | On measuring linguistic intelligence | http://arxiv.org/pdf/1503.06151v1.pdf | author:Maxim Litvak category:cs.CL published:2015-03-20 summary:This work addresses the problem of measuring how many languages a person"effectively" speaks given that some of the languages are close to each other.In other words, to assign a meaningful number to her language portfolio.Intuition says that someone who speaks fluently Spanish and Portuguese islinguistically less proficient compared to someone who speaks fluently Spanishand Chinese since it takes more effort for a native Spanish speaker to learnChinese than Portuguese. As the number of languages grows and their proficiencylevels vary, it gets even more complicated to assign a score to a languageportfolio. In this article we propose such a measure ("linguistic quotient" -LQ) that can account for these effects. We define the properties that such a measure should have. They are based onthe idea of coherent risk measures from the mathematical finance. Having laiddown the foundation, we propose one such a measure together with the algorithmthat works on languages classification tree as input. The algorithm together with the input is available online at lingvometer.com
arxiv-9600-149 | Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs | http://arxiv.org/pdf/1503.02424v2.pdf | author:Yarin Gal, Richard Turner category:stat.ML published:2015-03-09 summary:Standard sparse pseudo-input approximations to the Gaussian process (GP)cannot handle complex functions well. Sparse spectrum alternatives attempt toanswer this but are known to over-fit. We suggest the use of variationalinference for the sparse spectrum approximation to avoid both issues. We modelthe covariance function with a finite Fourier series approximation and treat itas a random variable. The random covariance function has a posterior, on whicha variational distribution is placed. The variational distribution transformsthe random covariance function to fit the data. We study the properties of ourapproximate inference, compare it to alternative ones, and extend it to thedistributed and stochastic domains. Our approximation captures complexfunctions better than standard approaches and avoids over-fitting.
arxiv-9600-150 | Country-scale Exploratory Analysis of Call Detail Records through the Lens of Data Grid Models | http://arxiv.org/pdf/1503.06060v1.pdf | author:Romain Guigourès, Dominique Gay, Marc Boullé, Fabrice Clérot, Fabrice Rossi category:cs.DB stat.ML published:2015-03-20 summary:Call Detail Records (CDRs) are data recorded by telecommunications companies,consisting of basic informations related to several dimensions of the callsmade through the network: the source, destination, date and time of calls. CDRsdata analysis has received much attention in the recent years since it mightreveal valuable information about human behavior. It has shown high added valuein many application domains like e.g., communities analysis or networkplanning. In this paper, we suggest a generic methodology for summarizinginformation contained in CDRs data. The method is based on a parameter-freeestimation of the joint distribution of the variables that describe the calls.We also suggest several well-founded criteria that allows one to browse thesummary at various granularities and to explore the summary by means ofinsightful visualizations. The method handles network graph data, temporalsequence data as well as user mobility data stemming from original CDRs data.We show the relevance of our methodology for various case studies on real-worldCDRs data from Ivory Coast.
arxiv-9600-151 | Deep Transform: Cocktail Party Source Separation via Probabilistic Re-Synthesis | http://arxiv.org/pdf/1503.06046v1.pdf | author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx published:2015-03-20 summary:In cocktail party listening scenarios, the human brain is able to separatecompeting speech signals. However, the signal processing implemented by thebrain to perform cocktail party listening is not well understood. Here, wetrained two separate convolutive autoencoder deep neural networks (DNN) toseparate monaural and binaural mixtures of two concurrent speech streams. Wethen used these DNNs as convolutive deep transform (CDT) devices to performprobabilistic re-synthesis. The CDTs operated directly in the time-domain. Oursimulations demonstrate that very simple neural networks are capable ofexploiting monaural and binaural information available in a cocktail partylistening scenario.
arxiv-9600-152 | Feeder Load Balancing using Neural Network | http://arxiv.org/pdf/1503.06004v1.pdf | author:A. Ukil, W. Siti, J. Jordaan category:cs.NE published:2015-03-20 summary:The distribution system problems, such as planning, loss minimization, andenergy restoration, usually involve the phase balancing or networkreconfiguration procedures. The determination of an optimal phase balance is,in general, a combinatorial optimization problem. This paper proposes optimalreconfiguration of the phase balancing using the neural network, to switch onand off the different switches, allowing the three phases supply by thetransformer to the end-users to be balanced. This paper presents theapplication examples of the proposed method using the real and simulated testdata.
arxiv-9600-153 | Local algorithms for interactive clustering | http://arxiv.org/pdf/1312.6724v3.pdf | author:Pranjal Awasthi, Maria-Florina Balcan, Konstantin Voevodski category:cs.DS cs.LG published:2013-12-24 summary:We study the design of interactive clustering algorithms for data setssatisfying natural stability assumptions. Our algorithms start with any initialclustering and only make local changes in each step; both are desirablefeatures in many applications. We show that in this constrained setting one canstill design provably efficient algorithms that produce accurate clusterings.We also show that our algorithms perform well on real-world data.
arxiv-9600-154 | Rank Subspace Learning for Compact Hash Codes | http://arxiv.org/pdf/1503.05951v1.pdf | author:Kai Li, Guojun Qi, Jun Ye, Kien A. Hua category:cs.LG cs.IR I.2.6; H.3.3 published:2015-03-19 summary:The era of Big Data has spawned unprecedented interests in developing hashingalgorithms for efficient storage and fast nearest neighbor search. Mostexisting work learn hash functions that are numeric quantizations of featurevalues in projected feature space. In this work, we propose a novel hashlearning framework that encodes feature's rank orders instead of numeric valuesin a number of optimal low-dimensional ranking subspaces. We formulate theranking subspace learning problem as the optimization of a piece-wise linearconvex-concave function and present two versions of our algorithm: one withindependent optimization of each hash bit and the other exploiting a sequentiallearning framework. Our work is a generalization of the Winner-Take-All (WTA)hash family and naturally enjoys all the numeric stability benefits of rankcorrelation measures while being optimized to achieve high precision at veryshort code length. We compare with several state-of-the-art hashing algorithmsin both supervised and unsupervised domain, showing superior performance in anumber of data sets.
arxiv-9600-155 | Exact and Stable Covariance Estimation from Quadratic Sampling via Convex Programming | http://arxiv.org/pdf/1310.0807v5.pdf | author:Yuxin Chen, Yuejie Chi, Andrea Goldsmith category:cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH published:2013-10-02 summary:Statistical inference and information processing of high-dimensional dataoften require efficient and accurate estimation of their second-orderstatistics. With rapidly changing data, limited processing power and storage atthe acquisition devices, it is desirable to extract the covariance structurefrom a single pass over the data and a small number of stored measurements. Inthis paper, we explore a quadratic (or rank-one) measurement model whichimposes minimal memory requirements and low computational complexity during thesampling process, and is shown to be optimal in preserving variouslow-dimensional covariance structures. Specifically, four popular structuralassumptions of covariance matrices, namely low rank, Toeplitz low rank,sparsity, jointly rank-one and sparse structure, are investigated, whilerecovery is achieved via convex relaxation paradigms for the respectivestructure. The proposed quadratic sampling framework has a variety of potentialapplications including streaming data processing, high-frequency wirelesscommunication, phase space tomography and phase retrieval in optics, andnon-coherent subspace detection. Our method admits universally accuratecovariance estimation in the absence of noise, as soon as the number ofmeasurements exceeds the information theoretic limits. We also demonstrate therobustness of this approach against noise and imperfect structural assumptions.Our analysis is established upon a novel notion called the mixed-normrestricted isometry property (RIP-$\ell_{2}/\ell_{1}$), as well as theconventional RIP-$\ell_{2}/\ell_{2}$ for near-isotropic and boundedmeasurements. In addition, our results improve upon the best-known phaseretrieval (including both dense and sparse signals) guarantees using PhaseLiftwith a significantly simpler approach.
arxiv-9600-156 | Reduced Basis Decomposition: a Certified and Fast Lossy Data Compression Algorithm | http://arxiv.org/pdf/1503.05947v1.pdf | author:Yanlai Chen category:math.NA cs.AI cs.CV cs.NA published:2015-03-19 summary:Dimension reduction is often needed in the area of data mining. The goal ofthese methods is to map the given high-dimensional data into a low-dimensionalspace preserving certain properties of the initial data. There are two kinds oftechniques for this purpose. The first, projective methods, builds an explicitlinear projection from the high-dimensional space to the low-dimensional one.On the other hand, the nonlinear methods utilizes nonlinear and implicitmapping between the two spaces. In both cases, the methods considered inliterature have usually relied on computationally very intensive matrixfactorizations, frequently the Singular Value Decomposition (SVD). Thecomputational burden of SVD quickly renders these dimension reduction methodsinfeasible thanks to the ever-increasing sizes of the practical datasets. In this paper, we present a new decomposition strategy, Reduced BasisDecomposition (RBD), which is inspired by the Reduced Basis Method (RBM). Given$X$ the high-dimensional data, the method approximates it by $Y \, T (\approxX)$ with $Y$ being the low-dimensional surrogate and $T$ the transformationmatrix. $Y$ is obtained through a greedy algorithm thus extremely efficient. Infact, it is significantly faster than SVD with comparable accuracy. $T$ can becomputed on the fly. Moreover, unlike many compression algorithms, it easilyfinds the mapping for an arbitrary ``out-of-sample'' vector and it comes withan ``error indicator'' certifying the accuracy of the compression. Numericalresults are shown validating these claims.
arxiv-9600-157 | On Invariance and Selectivity in Representation Learning | http://arxiv.org/pdf/1503.05938v1.pdf | author:Fabio Anselmi, Lorenzo Rosasco, Tomaso Poggio category:cs.LG published:2015-03-19 summary:We discuss data representation which can be learned automatically from data,are invariant to transformations, and at the same time selective, in the sensethat two points have the same representation only if they are one thetransformation of the other. The mathematical results here sharpen some of thekey claims of i-theory -- a recent theory of feedforward processing in sensorycortex.
arxiv-9600-158 | Syntagma Lexical Database | http://arxiv.org/pdf/1503.05907v1.pdf | author:Daniel Christen category:cs.CL published:2015-03-19 summary:This paper discusses the structure of Syntagma's Lexical Database (focused onItalian). The basic database consists in four tables. Table Forms contains wordinflections, used by the POS-tagger for the identification of input-words.Forms is related to Lemma. Table Lemma stores all kinds of grammatical featuresof words, word-level semantic data and restrictions. In the table Meaningsmeaning-related data are stored: definition, examples, domain, and semanticinformation. Table Valency contains the argument structure of each meaning,with syntactic and semantic features for each argument. The extended version ofSLD contains the links to Syntagma's Semantic Net and to the WordNet synsets ofother languages.
arxiv-9600-159 | Building Statistical Shape Spaces for 3D Human Modeling | http://arxiv.org/pdf/1503.05860v1.pdf | author:Leonid Pishchulin, Stefanie Wuhrer, Thomas Helten, Christian Theobalt, Bernt Schiele category:cs.CV published:2015-03-19 summary:Statistical models of 3D human shape and pose learned from scan databaseshave developed into valuable tools to solve a variety of vision and graphicsproblems. Unfortunately, most publicly available models are of limitedexpressiveness as they were learned on very small databases that hardly reflectthe true variety in human body shapes. In this paper, we contribute byrebuilding a widely used statistical body representation from the largestcommercially available scan database, and making the resulting model availableto the community (visit http://humanshape.mpi-inf.mpg.de). As preprocessingseveral thousand scans for learning the model is a challenge in itself, wecontribute by developing robust best practice solutions for scan alignment thatquantitatively lead to the best learned models. We make implementations ofthese preprocessing steps also publicly available. We extensively evaluate theimproved accuracy and generality of our new model, and show its improvedperformance for human body reconstruction from sparse input data.
arxiv-9600-160 | Deep Transform: Time-Domain Audio Error Correction via Probabilistic Re-Synthesis | http://arxiv.org/pdf/1503.05849v1.pdf | author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx published:2015-03-19 summary:In the process of recording, storage and transmission of time-domain audiosignals, errors may be introduced that are difficult to correct in anunsupervised way. Here, we train a convolutional deep neural network tore-synthesize input time-domain speech signals at its output layer. We then usethis abstract transformation, which we call a deep transform (DT), to performprobabilistic re-synthesis on further speech (of the same speaker) which hasbeen degraded. Using the convolutive DT, we demonstrate the recovery of speechaudio that has been subject to extreme degradation. This approach may be usefulfor correction of errors in communications devices.
arxiv-9600-161 | Neural Network-Based Active Learning in Multivariate Calibration | http://arxiv.org/pdf/1503.05831v1.pdf | author:A. Ukil, J. Bernasconi category:cs.NE cs.CE cs.LG published:2015-03-19 summary:In chemometrics, data from infrared or near-infrared (NIR) spectroscopy areoften used to identify a compound or to analyze the composition of amaterial.This involves the calibration of models that predict the concentrationofmaterial constituents from the measured NIR spectrum. An interesting aspectof multivariate calibration is to achieve a particular accuracy level with aminimum number of training samples, as this reduces the number of laboratorytests and thus the cost of model building. In these chemometric models, theinput refers to a proper representation of the spectra and the output to theconcentrations of the sample constituents. The search for a most informativenew calibration sample thus has to be performed in the output space of themodel, rather than in the input space as in conventionalmodeling problems. Inthis paper, we propose to solve the corresponding inversion problem byutilizing the disagreements of an ensemble of neural networks to represent theprediction error in the unexplored component space. The next calibration sampleis then chosen at a composition where the individual models of the ensembledisagree most. The results obtained for a realistic chemometric calibrationexample show that the proposed active learning can achieve a given calibrationaccuracy with less training samples than random sampling.
arxiv-9600-162 | Sign Language Fingerspelling Classification from Depth and Color Images using a Deep Belief Network | http://arxiv.org/pdf/1503.05830v1.pdf | author:Lucas Rioux-Maldague, Philippe Giguère category:cs.CV published:2015-03-19 summary:Automatic sign language recognition is an open problem that has received alot of attention recently, not only because of its usefulness to signers, butalso due to the numerous applications a sign classifier can have. In thisarticle, we present a new feature extraction technique for hand poserecognition using depth and intensity images captured from a Microsoft Kinectsensor. We applied our technique to American Sign Language fingerspellingclassification using a Deep Belief Network, for which our feature extractiontechnique is tailored. We evaluated our results on a multi-user data set withtwo scenarios: one with all known users and one with an unseen user. Weachieved 99% recall and precision on the first, and 77% recall and 79%precision on the second. Our method is also capable of real-time signclassification and is adaptive to any environment or lightning intensity.
arxiv-9600-163 | Differentiating the multipoint Expected Improvement for optimal batch design | http://arxiv.org/pdf/1503.05509v2.pdf | author:Sébastien Marmin, Clément Chevalier, David Ginsbourger category:stat.ML math.ST stat.TH published:2015-03-18 summary:This work deals with parallel optimization of expensive objective functionswhich are modeled as sample realizations of Gaussian processes. The study isformalized as a Bayesian optimization problem, or continuous multi-armed banditproblem, where a batch of q \textgreater{} 0 arms is pulled in parallel at eachiteration. Several algorithms have been developed for choosing batches bytrading off exploitation and exploration. As of today, the maximum ExpectedImprovement (EI) and Upper Confidence Bound (UCB) selection rules appear as themost prominent approaches for batch selection. Here, we build upon recent workon the multipoint Expected Improvement criterion, for which an analyticexpansion relying on Tallis' formula was recently established. Thecomputational burden of this selection rule being still an issue inapplication, we derive a closed-form expression for the gradient of themultipoint Expected Improvement, which aims at facilitating its maximizationusing gradient-based ascent algorithms. Substantial computational savings areshown in application. In addition, our algorithms are tested numerically andcompared to state-of-the-art UCB-based batch-sequential algorithms. Combiningstarting designs relying on UCB with gradient-based EI local optimizationfinally appears as a sound option for batch design in distributed GaussianProcess optimization.
arxiv-9600-164 | Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications | http://arxiv.org/pdf/1405.4463v2.pdf | author:Mohammad Abu Alsheikh, Shaowei Lin, Dusit Niyato, Hwee-Pink Tan category:cs.NI cs.LG published:2014-05-18 summary:Wireless sensor networks monitor dynamic environments that change rapidlyover time. This dynamic behavior is either caused by external factors orinitiated by the system designers themselves. To adapt to such conditions,sensor networks often adopt machine learning techniques to eliminate the needfor unnecessary redesign. Machine learning also inspires many practicalsolutions that maximize resource utilization and prolong the lifespan of thenetwork. In this paper, we present an extensive literature review over theperiod 2002-2013 of machine learning methods that were used to address commonissues in wireless sensor networks (WSNs). The advantages and disadvantages ofeach proposed algorithm are evaluated against the corresponding problem. Wealso provide a comparative guide to aid WSN designers in developing suitablemachine learning solutions for their specific application challenges.
arxiv-9600-165 | A General Framework for Multi-focal Image Classification and Authentication: Application to Microscope Pollen Images | http://arxiv.org/pdf/1503.05786v1.pdf | author:François Chung, Tomás Rodríguez category:cs.CV published:2015-03-19 summary:In this article, we propose a general framework for multi-focal imageclassification and authentication, the methodology being demonstrated onmicroscope pollen images. The framework is meant to be generic and based on abrute force-like approach aimed to be efficient not only on any kind, and anynumber, of pollen images (regardless of the pollen type), but also on any kindof multi-focal images. All stages of the framework's pipeline are designed tobe used in an automatic fashion. First, the optimal focus is selected using theabsolute gradient method. Then, pollen grains are extracted using acoarse-to-fine approach involving both clustering and morphological techniques(coarse stage), and a snake-based segmentation (fine stage). Finally, featuresare extracted and selected using a generalized approach, and theirclassification is tested with four classifiers: Weighted Neighbor Distance,Neural Network, Decision Tree and Random Forest. The latter method, which hasshown the best and more robust classification accuracy results (above 97\% forany number of pollen types), is finally used for the authentication stage.
arxiv-9600-166 | Learning Hypergraph-regularized Attribute Predictors | http://arxiv.org/pdf/1503.05782v1.pdf | author:Sheng Huang, Mohamed Elhoseiny, Ahmed Elgammal, Dan Yang category:cs.CV cs.LG published:2015-03-19 summary:We present a novel attribute learning framework named Hypergraph-basedAttribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict theattribute relations in the data. Then the attribute prediction problem iscasted as a regularized hypergraph cut problem in which HAP jointly learns acollection of attribute projections from the feature space to a hypergraphembedding space aligned with the attribute space. The learned projectionsdirectly act as attribute classifiers (linear and kernelized). This formulationleads to a very efficient approach. By considering our model as a multi-graphcut task, our framework can flexibly incorporate other available information,in particular class label. We apply our approach to attribute prediction,Zero-shot and $N$-shot learning tasks. The results on AWA, USAA and CUBdatabases demonstrate the value of our methods in comparison with thestate-of-the-art approaches.
arxiv-9600-167 | Automatic Pollen Grain and Exine Segmentation from Microscope Images | http://arxiv.org/pdf/1503.05767v1.pdf | author:François Chung, Tomás Rodríguez category:cs.CV published:2015-03-19 summary:In this article, we propose an automatic method for the segmentation ofpollen grains from microscope images, followed by the automatic segmentation oftheir exine. The objective of exine segmentation is to separate the pollengrain in two regions of interest: exine and inner part. A coarse-to-fineapproach ensures a smooth and accurate segmentation of both structures. As arough stage, grain segmentation is performed by a procedure involvingclustering and morphological operations, while the exine is approximated by aniterative procedure consisting in consecutive cropping steps of the pollengrain. A snake-based segmentation is performed to refine the segmentation ofboth structures. Results have shown that our segmentation method is able todeal with different pollen types, as well as with different types of exine andinner part appearance. The proposed segmentation method aims to be generic andhas been designed as one of the core steps of an automatic pollenclassification framework.
arxiv-9600-168 | Implementation of a Practical Distributed Calculation System with Browsers and JavaScript, and Application to Distributed Deep Learning | http://arxiv.org/pdf/1503.05743v1.pdf | author:Ken Miura, Tatsuya Harada category:cs.DC cs.LG cs.MS cs.NE stat.ML published:2015-03-19 summary:Deep learning can achieve outstanding results in various fields. However, itrequires so significant computational power that graphics processing units(GPUs) and/or numerous computers are often required for the practicalapplication. We have developed a new distributed calculation framework called"Sashimi" that allows any computer to be used as a distribution node only byaccessing a website. We have also developed a new JavaScript neural networkframework called "Sukiyaki" that uses general purpose GPUs with web browsers.Sukiyaki performs 30 times faster than a conventional JavaScript library fordeep convolutional neural networks (deep CNNs) learning. The combination ofSashimi and Sukiyaki, as well as new distribution algorithms, demonstrates thedistributed deep learning of deep CNNs only with web browsers on variousdevices. The libraries that comprise the proposed methods are available underMIT license at http://mil-tokyo.github.io/.
arxiv-9600-169 | An approach to improving edge detection for facial and remotely sensed images using vector order statistics | http://arxiv.org/pdf/1503.05692v1.pdf | author:B O. Sadiq, S. M. Sani, S. Garba category:cs.CV published:2015-03-19 summary:This paper presents an improved edge detection algorithm for facial andremotely sensed images using vector order statistics. The developed algorithmprocesses colored images directly without been converted to gray scale. Anumber of the existing algorithms converts the colored images into gray scalebefore detection of edges. But this process leads to inaccurate precision ofrecognized edges, thus producing false and broken edges in the output edge map.Facial and remotely sensed images consist of curved edge lines which have to bedetected continuously to prevent broken edges. In order to deal with this, acollection of pixel approach is introduced with a view to minimizing the falseand broken edges that exists in the generated output edge map of facial andremotely sensed images.
arxiv-9600-170 | Edge Detection: A Collection of Pixel based Approach for Colored Images | http://arxiv.org/pdf/1503.05689v1.pdf | author:B. O Sadiq, S. M Sani, S Garba category:cs.CV published:2015-03-19 summary:The existing traditional edge detection algorithms process a single pixel onan image at a time, thereby calculating a value which shows the edge magnitudeof the pixel and the edge orientation. Most of these existing algorithmsconvert the coloured images into gray scale before detection of edges. However,this process leads to inaccurate precision of recognized edges, thus producingfalse and broken edges in the image. This paper presents a profile modellingscheme for collection of pixels based on the step and ramp edges, with a viewto reducing the false and broken edges present in the image. The collection ofpixel scheme generated is used with the Vector Order Statistics to reduce theimprecision of recognized edges when converting from coloured to gray scaleimages. The Pratt Figure of Merit (PFOM) is used as a quantitative comparisonbetween the existing traditional edge detection algorithm and the developedalgorithm as a means of validation. The PFOM value obtained for the developedalgorithm is 0.8480, which showed an improvement over the existing traditionaledge detection algorithms.
arxiv-9600-171 | Non-parametric Bayesian Models of Response Function in Dynamic Image Sequences | http://arxiv.org/pdf/1503.05684v1.pdf | author:Ondřej Tichý, Václav Šmídl category:stat.ML published:2015-03-19 summary:Estimation of response functions is an important task in dynamic medicalimaging. This task arises for example in dynamic renal scintigraphy, whereimpulse response or retention functions are estimated, or in functionalmagnetic resonance imaging where hemodynamic response functions are required.These functions can not be observed directly and their estimation iscomplicated because the recorded images are subject to superposition ofunderlying signals. Therefore, the response functions are estimated via blindsource separation and deconvolution. Performance of this algorithm heavilydepends on the used models of the response functions. Response functions inreal image sequences are rather complicated and finding a suitable parametricform is problematic. In this paper, we study estimation of the responsefunctions using non-parametric Bayesian priors. These priors were designed tofavor desirable properties of the functions, such as sparsity or smoothness.These assumptions are used within hierarchical priors of the blind sourceseparation and deconvolution algorithm. Comparison of the resulting algorithmswith these priors is performed on synthetic dataset as well as on real datasetsfrom dynamic renal scintigraphy. It is shown that flexible non-parametricpriors improve estimation of response functions in both cases. MATLABimplementation of the resulting algorithms is freely available for download.
arxiv-9600-172 | MIST: L0 Sparse Linear Regression with Momentum | http://arxiv.org/pdf/1409.7193v2.pdf | author:Goran Marjanovic, Magnus O. Ulfarsson, Alfred O. Hero III category:stat.ML published:2014-09-25 summary:Significant attention has been given to minimizing a penalized least squarescriterion for estimating sparse solutions to large linear systems of equations.The penalty is responsible for inducing sparsity and the natural choice is theso-called $l_0$ norm. In this paper we develop a Momentumized IterativeShrinkage Thresholding (MIST) algorithm for minimizing the resulting non-convexcriterion and prove its convergence to a local minimizer. Simulations on largedata sets show superior performance of the proposed method to other methods.
arxiv-9600-173 | L0 Sparse Inverse Covariance Estimation | http://arxiv.org/pdf/1408.0850v5.pdf | author:Goran Marjanovic, Alfred O. Hero III category:stat.ML published:2014-08-05 summary:Recently, there has been focus on penalized log-likelihood covarianceestimation for sparse inverse covariance (precision) matrices. The penalty isresponsible for inducing sparsity, and a very common choice is the convex $l_1$norm. However, the best estimator performance is not always achieved with thispenalty. The most natural sparsity promoting "norm" is the non-convex $l_0$penalty but its lack of convexity has deterred its use in sparse maximumlikelihood estimation. In this paper we consider non-convex $l_0$ penalizedlog-likelihood inverse covariance estimation and present a novel cyclic descentalgorithm for its optimization. Convergence to a local minimizer is proved,which is highly non-trivial, and we demonstrate via simulations the reducedbias and superior quality of the $l_0$ penalty as compared to the $l_1$penalty.
arxiv-9600-174 | Phrase database Approach to structural and semantic disambiguation in English-Korean Machine Translation | http://arxiv.org/pdf/1503.05626v1.pdf | author:Myong-Chol Pak category:cs.CL published:2015-03-19 summary:In machine translation it is common phenomenon that machine-readabledictionaries and standard parsing rules are not enough to ensure accuracy inparsing and translating English phrases into Korean language, which is revealedin misleading translation results due to consequent structural and semanticambiguities. This paper aims to suggest a solution to structural and semanticambiguities due to the idiomaticity and non-grammaticalness of phrases commonlyused in English language by applying bilingual phrase database inEnglish-Korean Machine Translation (EKMT). This paper firstly clarifies whatthe phrase unit in EKMT is based on the definition of the English phrase,secondly clarifies what kind of language unit can be the target of the phrasedatabase for EKMT, thirdly suggests a way to build the phrase database bypresenting the format of the phrase database with examples, and finallydiscusses briefly the method to apply this bilingual phrase database to theEKMT for structural and semantic disambiguation.
arxiv-9600-175 | The Knowledge Gradient Policy Using A Sparse Additive Belief Model | http://arxiv.org/pdf/1503.05567v1.pdf | author:Yan Li, Han Liu, Warren Powell category:stat.ML cs.IT cs.SY math.IT published:2015-03-18 summary:We propose a sequential learning policy for noisy discrete globaloptimization and ranking and selection (R\&S) problems with high dimensionalsparse belief functions, where there are hundreds or even thousands offeatures, but only a small portion of these features contain explanatory power.We aim to identify the sparsity pattern and select the best alternative beforethe finite budget is exhausted. We derive a knowledge gradient policy forsparse linear models (KGSpLin) with group Lasso penalty. This policy is aunique and novel hybrid of Bayesian R\&S with frequentist learning.Particularly, our method naturally combines B-spline basis expansion andgeneralizes to the nonparametric additive model (KGSpAM) and functional ANOVAmodel. Theoretically, we provide the estimation error bounds of the posteriormean estimate and the functional estimate. Controlled experiments show that thealgorithm efficiently learns the correct set of nonzero parameters even whenthe model is imbedded with hundreds of dummy parameters. Also it outperformsthe knowledge gradient for a linear model.
arxiv-9600-176 | Text Segmentation based on Semantic Word Embeddings | http://arxiv.org/pdf/1503.05543v1.pdf | author:Alexander A Alemi, Paul Ginsparg category:cs.CL cs.IR published:2015-03-18 summary:We explore the use of semantic word embeddings in text segmentationalgorithms, including the C99 segmentation algorithm and new algorithmsinspired by the distributed word vector representation. By developing a generalframework for discussing a class of segmentation objectives, we study theeffectiveness of greedy versus exact optimization approaches and suggest a newiterative refinement technique for improving the performance of greedystrategies. We compare our results to known benchmarks, using known metrics. Wedemonstrate state-of-the-art performance for an untrained method with ourContent Vector Segmentation (CVS) on the Choi test set. Finally, we apply thesegmentation procedure to an in-the-wild dataset consisting of text extractedfrom scholarly articles in the arXiv.org database.
arxiv-9600-177 | On Using Very Large Target Vocabulary for Neural Machine Translation | http://arxiv.org/pdf/1412.2007v2.pdf | author:Sébastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio category:cs.CL published:2014-12-05 summary:Neural machine translation, a recently proposed approach to machinetranslation based purely on neural networks, has shown promising resultscompared to the existing approaches such as phrase-based statistical machinetranslation. Despite its recent success, neural machine translation has itslimitation in handling a larger vocabulary, as training complexity as well asdecoding complexity increase proportionally to the number of target words. Inthis paper, we propose a method that allows us to use a very large targetvocabulary without increasing training complexity, based on importancesampling. We show that decoding can be efficiently done even with the modelhaving a very large target vocabulary by selecting only a small subset of thewhole target vocabulary. The models trained by the proposed approach areempirically found to outperform the baseline models with a small vocabulary aswell as the LSTM-based neural machine translation models. Furthermore, when weuse the ensemble of a few models with very large target vocabularies, weachieve the state-of-the-art translation performance (measured by BLEU) on theEnglish->German translation and almost as high performance as state-of-the-artEnglish->French translation system.
arxiv-9600-178 | Interpretable Aircraft Engine Diagnostic via Expert Indicator Aggregation | http://arxiv.org/pdf/1503.05526v1.pdf | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG math.ST stat.AP stat.TH published:2015-03-18 summary:Detecting early signs of failures (anomalies) in complex systems is one ofthe main goal of preventive maintenance. It allows in particular to avoidactual failures by (re)scheduling maintenance operations in a way thatoptimizes maintenance costs. Aircraft engine health monitoring is onerepresentative example of a field in which anomaly detection is crucial.Manufacturers collect large amount of engine related data during flights whichare used, among other applications, to detect anomalies. This articleintroduces and studies a generic methodology that allows one to build automaticearly signs of anomaly detection in a way that builds upon human expertise andthat remains understandable by human operators who make the final maintenancedecision. The main idea of the method is to generate a very large number ofbinary indicators based on parametric anomaly scores designed by experts,complemented by simple aggregations of those scores. A feature selection methodis used to keep only the most discriminant indicators which are used as inputsof a Naive Bayes classifier. This give an interpretable classifier based oninterpretable anomaly detectors whose parameters have been optimized indirectlyby the selection process. The proposed methodology is evaluated on simulateddata designed to reproduce some of the anomaly types observed in real worldengines.
arxiv-9600-179 | Nonparametric Detection of Nonlinearly Mixed Pixels and Endmember Estimation in Hyperspectral Images | http://arxiv.org/pdf/1503.05521v1.pdf | author:Tales Imbiriba, José Carlos Moreira Bermudez, Cédric Richard, Jean-Yves Tourneret category:cs.CV published:2015-03-18 summary:Mixing phenomena in hyperspectral images depend on a variety of factors suchas the resolution of observation devices, the properties of materials, and howthese materials interact with incident light in the scene. Different parametricand nonparametric models have been considered to address hyperspectral unmixingproblems. The simplest one is the linear mixing model. Nevertheless, it hasbeen recognized that mixing phenomena can also be nonlinear. The correspondingnonlinear analysis techniques are necessarily more challenging and complex thanthose employed for linear unmixing. Within this context, it makes sense todetect the nonlinearly mixed pixels in an image prior to its analysis, and thenemploy the simplest possible unmixing technique to analyze each pixel. In thispaper, we propose a technique for detecting nonlinearly mixed pixels. Thedetection approach is based on the comparison of the reconstruction errorsusing both a Gaussian process regression model and a linear regression model.The two errors are combined into a detection statistics for which a probabilitydensity function can be reasonably approximated. We also propose an iterativeendmember extraction algorithm to be employed in combination with the detectionalgorithm. The proposed Detect-then-Unmix strategy, which consists ofextracting endmembers, detecting nonlinearly mixed pixels and unmixing, istested with synthetic and real images.
arxiv-9600-180 | Homotopy based algorithms for $\ell_0$-regularized least-squares | http://arxiv.org/pdf/1406.4802v2.pdf | author:Charles Soussen, Jérôme Idier, Junbo Duan, David Brie category:cs.NA cs.LG published:2014-01-31 summary:Sparse signal restoration is usually formulated as the minimization of aquadratic cost function $\y-Ax\_2^2$, where A is a dictionary and x is anunknown sparse vector. It is well-known that imposing an $\ell_0$ constraintleads to an NP-hard minimization problem. The convex relaxation approach hasreceived considerable attention, where the $\ell_0$-norm is replaced by the$\ell_1$-norm. Among the many efficient $\ell_1$ solvers, the homotopyalgorithm minimizes $\y-Ax\_2^2+\lambda\x\_1$ with respect to x for acontinuum of $\lambda$'s. It is inspired by the piecewise regularity of the$\ell_1$-regularization path, also referred to as the homotopy path. In thispaper, we address the minimization problem $\y-Ax\_2^2+\lambda\x\_0$ for acontinuum of $\lambda$'s and propose two heuristic search algorithms for$\ell_0$-homotopy. Continuation Single Best Replacement is a forward-backwardgreedy strategy extending the Single Best Replacement algorithm, previouslyproposed for $\ell_0$-minimization at a given $\lambda$. The adaptive search ofthe $\lambda$-values is inspired by $\ell_1$-homotopy. $\ell_0$ RegularizationPath Descent is a more complex algorithm exploiting the structural propertiesof the $\ell_0$-regularization path, which is piecewise constant with respectto $\lambda$. Both algorithms are empirically evaluated for difficult inverseproblems involving ill-conditioned dictionaries. Finally, we show that they canbe easily coupled with usual methods of model order selection.
arxiv-9600-181 | Shared latent subspace modelling within Gaussian-Binary Restricted Boltzmann Machines for NIST i-Vector Challenge 2014 | http://arxiv.org/pdf/1503.05471v1.pdf | author:Danila Doroshin, Alexander Yamshinin, Nikolay Lubimov, Marina Nastasenko, Mikhail Kotov, Maxim Tkachenko category:cs.LG cs.NE cs.SD stat.ML 62M45 I.2.6; I.5.1 published:2015-03-18 summary:This paper presents a novel approach to speaker subspace modelling based onGaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model isbased on the idea of shared factors as in the Probabilistic Linear DiscriminantAnalysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,herein the speaker factor is shared over all vectors of the speaker. ThenMaximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.Various new scoring techniques for speaker verification using GRBM areproposed. The results for NIST i-vector Challenge 2014 dataset are presented.
arxiv-9600-182 | Variance-Constrained Actor-Critic Algorithms for Discounted and Average Reward MDPs | http://arxiv.org/pdf/1403.6530v2.pdf | author:Prashanth L. A., Mohammad Ghavamzadeh category:cs.LG math.OC stat.ML published:2014-03-25 summary:In many sequential decision-making problems we may want to manage risk byminimizing some measure of variability in rewards in addition to maximizing astandard criterion. Variance related risk measures are among the most commonrisk-sensitive criteria in finance and operations research. However, optimizingmany such criteria is known to be a hard problem. In this paper, we considerboth discounted and average reward Markov decision processes. For eachformulation, we first define a measure of variability for a policy, which inturn gives us a set of risk-sensitive criteria to optimize. For each of thesecriteria, we derive a formula for computing its gradient. We then deviseactor-critic algorithms that operate on three timescales - a TD critic on thefastest timescale, a policy gradient (actor) on the intermediate timescale, anda dual ascent for Lagrange multipliers on the slowest timescale. In thediscounted setting, we point out the difficulty in estimating the gradient ofthe variance of the return and incorporate simultaneous perturbation approachesto alleviate this. The average setting, on the other hand, allows for an actorupdate using compatible features to estimate the gradient of the variance. Weestablish the convergence of our algorithms to locally risk-sensitive optimalpolicies. Finally, we demonstrate the usefulness of our algorithms in a trafficsignal control application.
arxiv-9600-183 | IT-map: an Effective Nonlinear Dimensionality Reduction Method for Interactive Clustering | http://arxiv.org/pdf/1501.06450v2.pdf | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG published:2015-01-26 summary:Scientists in many fields have the common and basic need of dimensionalityreduction: visualizing the underlying structure of the massive multivariatedata in a low-dimensional space. However, many dimensionality reduction methodsconfront the so-called "crowding problem" that clusters tend to overlap witheach other in the embedding. Previously, researchers expect to avoid thatproblem and seek to make clusters maximally separated in the embedding.However, the proposed in-tree (IT) based method, called IT-map, allows clustersin the embedding to be locally overlapped, while seeking to make themdistinguishable by some small yet key parts. IT-map provides a simple,effective and novel solution to cluster-preserving mapping, which makes itpossible to cluster the original data points interactively and thus should beof general meaning in science and engineering.
arxiv-9600-184 | Nonparametric Nearest Neighbor Descent Clustering based on Delaunay Triangulation | http://arxiv.org/pdf/1502.04837v2.pdf | author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG published:2015-02-17 summary:In our physically inspired in-tree (IT) based clustering algorithm and theseries after it, there is only one free parameter involved in computing thepotential value of each point. In this work, based on the DelaunayTriangulation or its dual Voronoi tessellation, we propose a nonparametricprocess to compute potential values by the local information. This computation,though nonparametric, is relatively very rough, and consequently, many localextreme points will be generated. However, unlike those gradient-based methods,our IT-based methods are generally insensitive to those local extremes. Thispositively demonstrates the superiority of these parametric (previous) andnonparametric (in this work) IT-based methods.
arxiv-9600-185 | Global 6DOF Pose Estimation from Untextured 2D City Models | http://arxiv.org/pdf/1503.02675v2.pdf | author:Clemens Arth, Christian Pirchheim, Jonathan Ventura, Vincent Lepetit category:cs.CV published:2015-03-09 summary:We propose a method for estimating the 3D pose for the camera of a mobiledevice in outdoor conditions, using only an untextured 2D model. Previousmethods compute only a relative pose using a SLAM algorithm, or require manyregistered images, which are cumbersome to acquire. By contrast, our methodreturns an accurate, absolute camera pose in an absolute referential usingsimple 2D+height maps, which are broadly available, to refine a first estimateof the pose provided by the device's sensors. We show how to first estimate thecamera absolute orientation from straight line segments, and then how toestimate the translation by aligning the 2D map with a semantic segmentation ofthe input image. We demonstrate the robustness and accuracy of our approach ona challenging dataset.
arxiv-9600-186 | Efficient Machine Learning for Big Data: A Review | http://arxiv.org/pdf/1503.05296v1.pdf | author:O. Y. Al-Jarrah, P. D. Yoo, S Muhaidat, G. K. Karagiannidis, K. Taha category:cs.LG cs.AI published:2015-03-18 summary:With the emerging technologies and all associated devices, it is predictedthat massive amount of data will be created in the next few years, in fact, asmuch as 90% of current data were created in the last couple of years,a trendthat will continue for the foreseeable future. Sustainable computing studiesthe process by which computer engineer/scientist designs computers andassociated subsystems efficiently and effectively with minimal impact on theenvironment. However, current intelligent machine-learning systems areperformance driven, the focus is on the predictive/classification accuracy,based on known properties learned from the training samples. For instance, mostmachine-learning-based nonparametric models are known to require highcomputational cost in order to find the global optima. With the learning taskin a large dataset, the number of hidden nodes within the network willtherefore increase significantly, which eventually leads to an exponential risein computational complexity. This paper thus reviews the theoretical andexperimental data-modeling literature, in large-scale data-intensive fields,relating to: (1) model efficiency, including computational requirements inlearning, and data-intensive areas structure and design, and introduces (2) newalgorithmic approaches with the least memory requirements and processing tominimize computational cost, while maintaining/improving itspredictive/classification accuracy and stability.
arxiv-9600-187 | A warped kernel improving robustness in Bayesian optimization via random embeddings | http://arxiv.org/pdf/1411.3685v3.pdf | author:Mickaël Binois, David Ginsbourger, Olivier Roustant category:math.OC stat.ML published:2014-11-13 summary:This works extends the Random Embedding Bayesian Optimization approach byintegrating a warping of the high dimensional subspace within the covariancekernel. The proposed warping, that relies on elementary geometricconsiderations, allows mitigating the drawbacks of the high extrinsicdimensionality while avoiding the algorithm to evaluate points giving redundantinformation. It also alleviates constraints on bound selection for the embeddeddomain, thus improving the robustness, as illustrated with a test case with 25variables and intrinsic dimension 6.
arxiv-9600-188 | A General Framework for Robust Testing and Confidence Regions in High-Dimensional Quantile Regression | http://arxiv.org/pdf/1412.8724v2.pdf | author:Tianqi Zhao, Mladen Kolar, Han Liu category:stat.ML published:2014-12-30 summary:We propose a robust inferential procedure for assessing uncertainties ofparameter estimation in high-dimensional linear models, where the dimension $p$can grow exponentially fast with the sample size $n$. Our method combines thede-biasing technique with the composite quantile function to construct anestimator that is asymptotically normal. Hence it can be used to constructvalid confidence intervals and conduct hypothesis tests. Our estimator isrobust and does not require the existence of first or second moment of thenoise distribution. It also preserves efficiency in the sense that the worstcase efficiency loss is less than 30\% compared to the square-loss-basedde-biased Lasso estimator. In many cases our estimator is close to or betterthan the latter, especially when the noise is heavy-tailed. Our de-biasingprocedure does not require solving the $L_1$-penalized composite quantileregression. Instead, it allows for any first-stage estimator with desiredconvergence rate and empirical sparsity. The paper also provides new prooftechniques for developing theoretical guarantees of inferential procedures withnon-smooth loss functions. To establish the main results, we exploit the localcurvature of the conditional expectation of composite quantile loss and applyempirical process theories to control the difference between empiricalquantities and their conditional expectations. Our results are establishedunder weaker assumptions compared to existing work on inference forhigh-dimensional quantile regression. Furthermore, we consider ahigh-dimensional simultaneous test for the regression parameters by applyingthe Gaussian approximation and multiplier bootstrap theories. We also studydistributed learning and exploit the divide-and-conquer estimator to reducecomputation complexity when the sample size is massive. Finally, we provideempirical results to verify the theory.
arxiv-9600-189 | Improved Calibration of Near-Infrared Spectra by Using Ensembles of Neural Network Models | http://arxiv.org/pdf/1503.05272v1.pdf | author:A. Ukil, J. Bernasconi, H. Braendle, H. Buijs, S. Bonenfant category:cs.NE published:2015-03-18 summary:IR or near-infrared (NIR) spectroscopy is a method used to identify acompound or to analyze the composition of a material. Calibration of NIRspectra refers to the use of the spectra as multivariate descriptors to predictconcentrations of the constituents. To build a calibration model,state-of-the-art software predominantly uses linear regression techniques. Fornonlinear calibration problems, neural network-based models have proved to bean interesting alternative. In this paper, we propose a novel extension of theconventional neural network-based approach, the use of an ensemble of neuralnetwork models. The individual neural networks are obtained by resampling theavailable training data with bootstrapping or cross-validation techniques. Theresults obtained for a realistic calibration example show that theensemble-based approach produces a significantly more accurate and robustcalibration model than conventional regression methods.
arxiv-9600-190 | Scalable Nuclear-norm Minimization by Subspace Pursuit Proximal Riemannian Gradient | http://arxiv.org/pdf/1503.02828v2.pdf | author:Mingkui Tan, Shijie Xiao, Junbin Gao, Dong Xu, Anton Van Den Hengel, Qinfeng Shi category:cs.LG cs.NA published:2015-03-10 summary:Nuclear-norm regularization plays a vital role in many learning tasks, suchas low-rank matrix recovery (MR), and low-rank representation (LRR). Solvingthis problem directly can be computationally expensive due to the unknown rankof variables or large-rank singular value decompositions (SVDs). To addressthis, we propose a proximal Riemannian gradient (PRG) scheme which canefficiently solve trace-norm regularized problems defined on real-algebraicvariety $\mMLr$ of real matrices of rank at most $r$. Based on PRG, we furtherpresent a simple and novel subspace pursuit (SP) paradigm for generaltrace-norm regularized problems without the explicit rank constraint $\mMLr$.The proposed paradigm is very scalable by avoiding large-rank SVDs. Empiricalstudies on several tasks, such as matrix completion and LRR based subspaceclustering, demonstrate the superiority of the proposed paradigms over existingmethods.
arxiv-9600-191 | Improved LASSO | http://arxiv.org/pdf/1503.05160v1.pdf | author:A. K. Md. Ehsanes Saleh, Enayetur Raheem category:math.ST stat.AP stat.ML stat.TH published:2015-03-17 summary:We propose an improved LASSO estimation technique based on Stein-rule. Weshrink classical LASSO estimator using preliminary test, shrinkage, andpositive-rule shrinkage principle. Simulation results have been carried out forvarious configurations of correlation coefficients ($r$), size of the parametervector ($\beta$), error variance ($\sigma^2$) and number of non-zerocoefficients ($k$) in the model parameter vector. Several real data exampleshave been used to demonstrate the practical usefulness of the proposedestimators. Our study shows that the risk ordering given by LSE $>$ LASSO $>$Stein-type LASSO $>$ Stein-type positive rule LASSO, remains the same uniformlyin the divergence parameter $\Delta^2$ as in the traditional case.
arxiv-9600-192 | ProtVec: A Continuous Distributed Representation of Biological Sequences | http://arxiv.org/pdf/1503.05140v1.pdf | author:Ehsaneddin Asgari, Mohammad R. K. Mofrad category:q-bio.QM cs.AI cs.LG q-bio.GN published:2015-03-17 summary:We propose a new approach for representing biological sequences. This method,named protein-vectors or ProtVec for short, can be utilized in bioinformaticsapplications such as family classification, protein visualization, structureprediction, disordered protein identification, and protein-protein interactionprediction. Using the Skip-gram neural networks, protein sequences arerepresented with a single dense n-dimensional vector. This method was evaluatedby classifying protein sequences obtained from Swiss-Prot belonging to 7,027protein families where an average family classification accuracy of $94\%\pm0.03\%$ was obtained, outperforming existing family classification methods. Inaddition, our model was used to predict disordered proteins from structuredproteins. Two databases of disordered sequences were used: the DisProt databaseas well as a database featuring the disordered regions of nucleoporins richwith phenylalanine-glycine repeats (FG-Nups). Using support vector machineclassifiers, FG-Nup sequences were distinguished from structured Protein DataBank (PDB) sequences with 99.81\% accuracy, and unstructured DisProt sequencesfrom structured DisProt sequences with 100.0\% accuracy. These results indicatethat by only providing sequence data for various proteins into this model,information about protein structure can be determined with high accuracy. Thisso-called embedding model needs to be trained only once and can then be used toascertain a diverse set of information regarding the proteins of interest. Inaddition, this representation can be considered as pre-training for variousapplications of deep learning in bioinformatics.
arxiv-9600-193 | Prediction Using Note Text: Synthetic Feature Creation with word2vec | http://arxiv.org/pdf/1503.05123v1.pdf | author:Manuel Amunategui, Tristan Markwell, Yelena Rozenfeld category:cs.CL published:2015-03-17 summary:word2vec affords a simple yet powerful approach of extracting quantitativevariables from unstructured textual data. Over half of healthcare data isunstructured and therefore hard to model without involved expertise in dataengineering and natural language processing. word2vec can serve as a bridge toquickly gather intelligence from such data sources. In this study, we ran 650 megabytes of unstructured, medical chart notes fromthe Providence Health & Services electronic medical record through word2vec. Weused two different approaches in creating predictive variables and tested themon the risk of readmission for patients with COPD (Chronic Obstructive LungDisease). As a comparative benchmark, we ran the same test using the LACE riskmodel (a single score based on length of stay, acuity, comorbid conditions, andemergency department visits). Using only free text and mathematical might, we found word2vec comparable toLACE in predicting the risk of readmission of COPD patients.
arxiv-9600-194 | A Unifying Framework in Vector-valued Reproducing Kernel Hilbert Spaces for Manifold Regularization and Co-Regularized Multi-view Learning | http://arxiv.org/pdf/1401.8066v2.pdf | author:Ha Quang Minh, Loris Bazzani, Vittorio Murino category:stat.ML published:2014-01-31 summary:This paper presents a general vector-valued reproducing kernel Hilbert spaces(RKHS) framework for the problem of learning an unknown functional dependencybetween a structured input space and a structured output space. Our formulationencompasses both Vector-valued Manifold Regularization and Co-regularizedMulti-view Learning, providing in particular a unifying framework linking thesetwo important learning approaches. In the case of the least square lossfunction, we provide a closed form solution, which is obtained by solving asystem of linear equations. In the case of Support Vector Machine (SVM)classification, our formulation generalizes in particular both the binaryLaplacian SVM to the multi-class, multi-view settings and the multi-classSimplex Cone SVM to the semi-supervised, multi-view settings. The solution isobtained by solving a single quadratic optimization problem, as in standardSVM, via the Sequential Minimal Optimization (SMO) approach. Empirical resultsobtained on the task of object recognition, using several challenging datasets,demonstrate the competitiveness of our algorithms compared with otherstate-of-the-art methods.
arxiv-9600-195 | Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits | http://arxiv.org/pdf/1503.05087v1.pdf | author:Gergely Neu, Gábor Bartók category:cs.LG stat.ML published:2015-03-17 summary:We propose a sample-efficient alternative for importance weighting forsituations where one only has sample access to the probability distributionthat generates the observations. Our new method, called Recurrence Weighting(RW), is described and analyzed in the context of online combinatorialoptimization under semi-bandit feedback, where a learner sequentially selectsits actions from a combinatorial decision set so as to minimize its cumulativeloss. In particular, we show that the well-known Follow-the-Perturbed-Leader(FPL) prediction method coupled with Recurrence Weighting yields the firstcomputationally efficient reduction from offline to online optimization in thissetting. We provide a thorough theoretical analysis for the resultingalgorithm, showing that its performance is on par with previous, inefficientsolutions. Our main contribution is showing that, despite the relatively largevariance induced by the RW procedure, our performance guarantees hold with highprobability rather than only in expectation. As a side result, we also improvethe best known regret bounds for FPL in online combinatorial optimization withfull feedback, closing the perceived performance gap between FPL andexponential weights in this setting.
arxiv-9600-196 | 3D Object Class Detection in the Wild | http://arxiv.org/pdf/1503.05038v1.pdf | author:Bojan Pepik, Michael Stark, Peter Gehler, Tobias Ritschel, Bernt Schiele category:cs.CV published:2015-03-17 summary:Object class detection has been a synonym for 2D bounding box localizationfor the longest time, fueled by the success of powerful statistical learningtechniques, combined with robust image representations. Only recently, therehas been a growing interest in revisiting the promise of computer vision fromthe early days: to precisely delineate the contents of a visual scene, objectby object, in 3D. In this paper, we draw from recent advances in objectdetection and 2D-3D object lifting in order to design an object class detectorthat is particularly tailored towards 3D object class detection. Our 3D objectclass detection method consists of several stages gradually enriching theobject detection output with object viewpoint, keypoints and 3D shapeestimates. Following careful design, in each stage it constantly improves theperformance and achieves state-ofthe-art performance in simultaneous 2Dbounding box and viewpoint estimation on the challenging Pascal3D+ dataset.
arxiv-9600-197 | PiMPeR: Piecewise Dense 3D Reconstruction from Multi-View and Multi-Illumination Images | http://arxiv.org/pdf/1503.04598v2.pdf | author:Reza Sabzevari, Vittori Murino, Alessio Del Bue category:cs.CV published:2015-03-16 summary:In this paper, we address the problem of dense 3D reconstruction frommultiple view images subject to strong lighting variations. In this regard, anew piecewise framework is proposed to explicitly take into account the changeof illumination across several wide-baseline images. Unlike multi-view stereoand multi-view photometric stereo methods, this pipeline deals withwide-baseline images that are uncalibrated, in terms of both camera parametersand lighting conditions. Such a scenario is meant to avoid use of any specificimaging setup and provide a tool for normal users without any expertise. To thebest of our knowledge, this paper presents the first work that deals with suchunconstrained setting. We propose a coarse-to-fine approach, in which a coarsemesh is first created using a set of geometric constraints and, then, finedetails are recovered by exploiting photometric properties of the scene.Augmenting the fine details on the coarse mesh is done via a final optimizationstep. Note that the method does not provide a generic solution for multi-viewphotometric stereo problem but it relaxes several common assumptions of thisproblem. The approach scales very well in size given its piecewise nature,dealing with large scale optimization and with severe missing data. Experimentson a benchmark dataset Robot data-set show the method performance against 3Dground truth.
arxiv-9600-198 | Ultra-Fast Shapelets for Time Series Classification | http://arxiv.org/pdf/1503.05018v1.pdf | author:Martin Wistuba, Josif Grabocka, Lars Schmidt-Thieme category:cs.LG published:2015-03-17 summary:Time series shapelets are discriminative subsequences and their similarity toa time series can be used for time series classification. Since the discoveryof time series shapelets is costly in terms of time, the applicability on longor multivariate time series is difficult. In this work we propose Ultra-FastShapelets that uses a number of random shapelets. It is shown that Ultra-FastShapelets yield the same prediction quality as current state-of-the-artshapelet-based time series classifiers that carefully select the shapelets bybeing by up to three orders of magnitudes. Since this method allows aultra-fast shapelet discovery, using shapelets for long multivariate timeseries classification becomes feasible. A method for using shapelets for multivariate time series is proposed andUltra-Fast Shapelets is proven to be successful in comparison tostate-of-the-art multivariate time series classifiers on 15 multivariate timeseries datasets from various domains. Finally, time series derivatives thathave proven to be useful for other time series classifiers are investigated forthe shapelet-based classifiers. It is shown that they have a positive impactand that they are easy to integrate with a simple preprocessing step, withoutthe need of adapting the shapelet discovery algorithm.
arxiv-9600-199 | An Outlier Detection-based Tree Selection Approach to Extreme Pruning of Random Forests | http://arxiv.org/pdf/1503.05187v1.pdf | author:Khaled Fawagreh, Mohamad Medhat Gaber, Eyad Elyan category:cs.LG published:2015-03-17 summary:Random Forest (RF) is an ensemble classification technique that was developedby Breiman over a decade ago. Compared with other ensemble techniques, it hasproved its accuracy and superiority. Many researchers, however, believe thatthere is still room for enhancing and improving its performance in terms ofpredictive accuracy. This explains why, over the past decade, there have beenmany extensions of RF where each extension employed a variety of techniques andstrategies to improve certain aspect(s) of RF. Since it has been provenempirically that ensembles tend to yield better results when there is asignificant diversity among the constituent models, the objective of this paperis twofolds. First, it investigates how an unsupervised learning technique,namely, Local Outlier Factor (LOF) can be used to identify diverse trees in theRF. Second, trees with the highest LOF scores are then used to produce anextension of RF termed LOFB-DRF that is much smaller in size than RF, and yetperforms at least as good as RF, but mostly exhibits higher performance interms of accuracy. The latter refers to a known technique called ensemblepruning. Experimental results on 10 real datasets prove the superiority of ourproposed extension over the traditional RF. Unprecedented pruning levelsreaching 99% have been achieved at the time of boosting the predictive accuracyof the ensemble. The notably high pruning level makes the technique a goodcandidate for real-time applications.
arxiv-9600-200 | On Extreme Pruning of Random Forest Ensembles for Real-time Predictive Applications | http://arxiv.org/pdf/1503.04996v1.pdf | author:Khaled Fawagreh, Mohamad Medhat Gaber, Eyad Elyan category:cs.LG published:2015-03-17 summary:Random Forest (RF) is an ensemble supervised machine learning technique thatwas developed by Breiman over a decade ago. Compared with other ensembletechniques, it has proved its accuracy and superiority. Many researchers,however, believe that there is still room for enhancing and improving itsperformance accuracy. This explains why, over the past decade, there have beenmany extensions of RF where each extension employed a variety of techniques andstrategies to improve certain aspect(s) of RF. Since it has been provenempiricallthat ensembles tend to yield better results when there is asignificant diversity among the constituent models, the objective of this paperis twofold. First, it investigates how data clustering (a well known diversitytechnique) can be applied to identify groups of similar decision trees in an RFin order to eliminate redundant trees by selecting a representative from eachgroup (cluster). Second, these likely diverse representatives are then used toproduce an extension of RF termed CLUB-DRF that is much smaller in size thanRF, and yet performs at least as good as RF, and mostly exhibits higherperformance in terms of accuracy. The latter refers to a known technique calledensemble pruning. Experimental results on 15 real datasets from the UCIrepository prove the superiority of our proposed extension over the traditionalRF. Most of our experiments achieved at least 95% or above pruning level whileretaining or outperforming the RF accuracy.
arxiv-9600-201 | Energy Sharing for Multiple Sensor Nodes with Finite Buffers | http://arxiv.org/pdf/1503.04964v1.pdf | author:Sindhu Padakandla, Prabuchandran K. J, Shalabh Bhatnagar category:cs.NI cs.LG published:2015-03-17 summary:We consider the problem of finding optimal energy sharing policies thatmaximize the network performance of a system comprising of multiple sensornodes and a single energy harvesting (EH) source. Sensor nodes periodicallysense the random field and generate data, which is stored in the correspondingdata queues. The EH source harnesses energy from ambient energy sources and thegenerated energy is stored in an energy buffer. Sensor nodes receive energy fordata transmission from the EH source. The EH source has to efficiently sharethe stored energy among the nodes in order to minimize the long-run averagedelay in data transmission. We formulate the problem of energy sharing betweenthe nodes in the framework of average cost infinite-horizon Markov decisionprocesses (MDPs). We develop efficient energy sharing algorithms, namelyQ-learning algorithm with exploration mechanisms based on the $\epsilon$-greedymethod as well as upper confidence bound (UCB). We extend these algorithms byincorporating state and action space aggregation to tackle state-action spaceexplosion in the MDP. We also develop a cross entropy based method thatincorporates policy parameterization in order to find near optimal energysharing policies. Through simulations, we show that our algorithms yield energysharing policies that outperform the heuristic greedy method.
arxiv-9600-202 | Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels | http://arxiv.org/pdf/1412.0265v2.pdf | author:Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV published:2014-11-30 summary:In this paper, we develop an approach to exploiting kernel methods withmanifold-valued data. In many computer vision problems, the data can benaturally represented as points on a Riemannian manifold. Due to thenon-Euclidean geometry of Riemannian manifolds, usual Euclidean computer visionand machine learning algorithms yield inferior results on such data. In thispaper, we define Gaussian radial basis function (RBF)-based positive definitekernels on manifolds that permit us to embed a given manifold with acorresponding metric in a high dimensional reproducing kernel Hilbert space.These kernels make it possible to utilize algorithms developed for linearspaces on nonlinear manifold-valued data. Since the Gaussian RBF defined withany given metric is not always positive definite, we present a unifiedframework for analyzing the positive definiteness of the Gaussian RBF on ageneric metric space. We then use the proposed framework to identify positivedefinite kernels on two specific manifolds commonly encountered in computervision: the Riemannian manifold of symmetric positive definite matrices and theGrassmann manifold, i.e., the Riemannian manifold of linear subspaces of aEuclidean space. We show that many popular algorithms designed for Euclideanspaces, such as support vector machines, discriminant analysis and principalcomponent analysis can be generalized to Riemannian manifolds with the help ofsuch positive definite Gaussian kernels.
arxiv-9600-203 | How the symbol grounding of living organisms can be realized in artificial agents | http://arxiv.org/pdf/1503.04941v1.pdf | author:J. H. van Hateren category:cs.AI cs.NE cs.RO published:2015-03-17 summary:A system with artificial intelligence usually relies on symbol manipulation,at least partly and implicitly. However, the interpretation of the symbols -what they represent and what they are about - is ultimately left to humans, asdesigners and users of the system. How symbols can acquire meaning for thesystem itself, independent of external interpretation, is an unsolved problem.Some grounding of symbols can be obtained by embodiment, that is, by causallyconnecting symbols (or sub-symbolic variables) to the physical environment,such as in a robot with sensors and effectors. However, a causal connection assuch does not produce representation and aboutness of the kind that symbolshave for humans. Here I present a theory that explains how humans and otherliving organisms have acquired the capability to have symbols and sub-symbolicvariables that represent, refer to, and are about something else. The theoryshows how reference can be to physical objects, but also to abstract objects,and even how it can be misguided (errors in reference) or be about non-existingobjects. I subsequently abstract the primary components of the theory fromtheir biological context, and discuss how and under what conditions the theorycould be implemented in artificial agents. A major component of the theory isthe strong nonlinearity associated with (potentially unlimited)self-reproduction. The latter is likely not acceptable in artificial systems.It remains unclear if goals other than those inherently servingself-reproduction can have aboutness and if such goals could be stabilized.
arxiv-9600-204 | Hypoelliptic Diffusion Maps I: Tangent Bundles | http://arxiv.org/pdf/1503.05459v1.pdf | author:Tingran Gao category:math.ST stat.ML stat.TH I.2.6 published:2015-03-17 summary:We introduce the concept of Hypoelliptic Diffusion Maps (HDM), a frameworkgeneralizing Diffusion Maps in the context of manifold learning anddimensionality reduction. Standard non-linear dimensionality reduction methods(e.g., LLE, ISOMAP, Laplacian Eigenmaps, Diffusion Maps) focus on miningmassive data sets using weighted affinity graphs; Orientable Diffusion Maps andVector Diffusion Maps enrich these graphs by attaching to each node also somelocal geometry. HDM likewise considers a scenario where each node possessesadditional structure, which is now itself of interest to investigate.Virtually, HDM augments the original data set with attached structures, andprovides tools for studying and organizing the augmented ensemble. The goal isto obtain information on individual structures attached to the nodes and on therelationship between structures attached to nearby nodes, so as to study theunderlying manifold from which the nodes are sampled. In this paper, we analyzeHDM on tangent bundles, revealing its intimate connection with sub-Riemanniangeometry and a family of hypoelliptic differential operators. In a later paper,we shall consider more general fibre bundles.
arxiv-9600-205 | Scalable Latent Tree Model and its Application to Health Analytics | http://arxiv.org/pdf/1406.4566v3.pdf | author:Furong Huang, Niranjan U. N., Ioakeim Perros, Robert Chen, Jimeng Sun, Anima Anandkumar category:cs.LG stat.ML published:2014-06-18 summary:We present an integrated approach to structure and parameter estimation inlatent tree graphical models, where some nodes are hidden. Our overall approachfollows a "divide-and-conquer" strategy that learns models over small groups ofvariables and iteratively merges into a global solution. The structure learninginvolves combinatorial operations such as minimum spanning tree constructionand local recursive grouping; the parameter learning is based on the method ofmoments and on tensor decompositions. Our method is guaranteed to correctlyrecover the unknown tree structure and the model parameters with low samplecomplexity for the class of linear multivariate latent tree models whichincludes discrete and Gaussian distributions, and Gaussian mixtures. Our bulkasynchronous parallel algorithm is implemented in parallel using the OpenMPframework and scales logarithmically with the number of variables and linearlywith dimensionality of each variable. Our experiments confirm a high degree ofefficiency and accuracy on large datasets of electronic health records. Theproposed algorithm also generates intuitive and clinically meaningful diseasehierarchies.
arxiv-9600-206 | Long Short-Term Memory Over Tree Structures | http://arxiv.org/pdf/1503.04881v1.pdf | author:Xiaodan Zhu, Parinaz Sobhani, Hongyu Guo category:cs.CL cs.LG cs.NE published:2015-03-16 summary:The chain-structured long short-term memory (LSTM) has showed to be effectivein a wide range of problems such as speech recognition and machine translation.In this paper, we propose to extend it to tree structures, in which a memorycell can reflect the history memories of multiple child cells or multipledescendant cells in a recursive process. We call the model S-LSTM, whichprovides a principled way of considering long-distance interaction overhierarchies, e.g., language or image parse structures. We leverage the modelsfor semantic composition to understand the meaning of text, a fundamentalproblem in natural language understanding, and show that it outperforms astate-of-the-art recursive model by replacing its composition layers with theS-LSTM memory blocks. We also show that utilizing the given structures ishelpful in achieving a performance better than that without considering thestructures.
arxiv-9600-207 | Sequential Sensing with Model Mismatch | http://arxiv.org/pdf/1501.06241v2.pdf | author:Ruiyang Song, Yao Xie, Sebastian Pokutta category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH published:2015-01-26 summary:We characterize the performance of sequential information guided sensing,Info-Greedy Sensing, when there is a mismatch between the true signal model andthe assumed model, which may be a sample estimate. In particular, we consider asetup where the signal is low-rank Gaussian and the measurements are taken inthe directions of eigenvectors of the covariance matrix in a decreasing orderof eigenvalues. We establish a set of performance bounds when a mismatchedcovariance matrix is used, in terms of the gap of signal posterior entropy, aswell as the additional amount of power required to achieve the same signalrecovery precision. Based on this, we further study how to choose aninitialization for Info-Greedy Sensing using the sample covariance matrix, orusing an efficient covariance sketching scheme.
arxiv-9600-208 | A distributed block coordinate descent method for training $l_1$ regularized linear classifiers | http://arxiv.org/pdf/1405.4544v2.pdf | author:Dhruv Mahajan, S. Sathiya Keerthi, S. Sundararajan category:cs.LG published:2014-05-18 summary:Distributed training of $l_1$ regularized classifiers has received greatattention recently. Most existing methods approach this problem by taking stepsobtained from approximating the objective by a quadratic approximation that isdecoupled at the individual variable level. These methods are designed formulticore and MPI platforms where communication costs are low. They areineffi?cient on systems such as Hadoop running on a cluster of commoditymachines where communication costs are substantial. In this paper we design adistributed algorithm for $l_1$ regularization that is much better suited forsuch systems than existing algorithms. A careful cost analysis is used tosupport these points and motivate our method. The main idea of our algorithm isto do block optimization of many variables on the actual objective functionwithin each computing node; this increases the computational cost per step thatis matched with the communication cost, and decreases the number of outeriterations, thus yielding a faster overall method. Distributed Gauss-Seidel andGauss-Southwell greedy schemes are used for choosing variables to update ineach step. We establish global convergence theory for our algorithm, includingQ-linear rate of convergence. Experiments on two benchmark problems show ourmethod to be much faster than existing methods.
arxiv-9600-209 | Visual Chunking: A List Prediction Framework for Region-Based Object Detection | http://arxiv.org/pdf/1410.7376v2.pdf | author:Nicholas Rhinehart, Jiaji Zhou, Martial Hebert, J. Andrew Bagnell category:cs.CV published:2014-10-27 summary:We consider detecting objects in an image by iteratively selecting from a setof arbitrarily shaped candidate regions. Our generic approach, which we termvisual chunking, reasons about the locations of multiple object instances in animage while expressively describing object boundaries. We design anoptimization criterion for measuring the performance of a list of suchdetections as a natural extension to a common per-instance metric. We presentan efficient algorithm with provable performance for building a high-qualitylist of detections from any candidate set of region-based proposals. We alsodevelop a simple class-specific algorithm to generate a candidate regioninstance in near-linear time in the number of low-level superpixels thatoutperforms other region generating methods. In order to make predictions onnovel images at testing time without access to ground truth, we developlearning approaches to emulate these algorithms' behaviors. We demonstrate thatour new approach outperforms sophisticated baselines on benchmark datasets.
arxiv-9600-210 | An efficient distributed learning algorithm based on effective local functional approximations | http://arxiv.org/pdf/1310.8418v4.pdf | author:Dhruv Mahajan, Nikunj Agrawal, S. Sathiya Keerthi, S. Sundararajan, Leon Bottou category:cs.LG published:2013-10-31 summary:Scalable machine learning over big data is an important problem that isreceiving a lot of attention in recent years. On popular distributedenvironments such as Hadoop running on a cluster of commodity machines,communication costs are substantial and algorithms need to be designed suitablyconsidering those costs. In this paper we give a novel approach to thedistributed training of linear classifiers (involving smooth losses and L2regularization) that is designed to reduce the total communication costs. Ateach iteration, the nodes minimize locally formed approximate objectivefunctions; then the resulting minimizers are combined to form a descentdirection to move. Our approach gives a lot of freedom in the formation of theapproximate objective function as well as in the choice of methods to solvethem. The method is shown to have $O(log(1/\epsilon))$ time convergence. Themethod can be viewed as an iterative parameter mixing method. A specialinstantiation yields a parallel stochastic gradient descent method with strongconvergence. When communication times between nodes are large, our method ismuch faster than the Terascale method (Agarwal et al., 2011), which is a stateof the art distributed solver based on the statistical query model (Chuet al.,2006) that computes function and gradient values in a distributed fashion. Wealso evaluate against other recent distributed methods and demonstrate superiorperformance of our method.
arxiv-9600-211 | All Who Wander: On the Prevalence and Characteristics of Multi-community Engagement | http://arxiv.org/pdf/1503.01180v2.pdf | author:Chenhao Tan, Lillian Lee category:cs.SI cs.CL physics.soc-ph J.4; H.2.8 published:2015-03-04 summary:Although analyzing user behavior within individual communities is an activeand rich research domain, people usually interact with multiple communitiesboth on- and off-line. How do users act in such multi-community environments?Although there are a host of intriguing aspects to this question, it hasreceived much less attention in the research community in comparison to theintra-community case. In this paper, we examine three aspects ofmulti-community engagement: the sequence of communities that users post to, thelanguage that users employ in those communities, and the feedback that usersreceive, using longitudinal posting behavior on Reddit as our main data source,and DBLP for auxiliary experiments. We also demonstrate the effectiveness offeatures drawn from these aspects in predicting users' future level ofactivity. One might expect that a user's trajectory mimics the "settling-down" processin real life: an initial exploration of sub-communities before settling downinto a few niches. However, we find that the users in our data continually postin new communities; moreover, as time goes on, they post increasingly evenlyamong a more diverse set of smaller communities. Interestingly, it seems thatusers that eventually leave the community are "destined" to do so from the verybeginning, in the sense of showing significantly different "wandering" patternsvery early on in their trajectories; this finding has potentially importantdesign implications for community maintainers. Our multi-community perspectivealso allows us to investigate the "situation vs. personality" debate fromlanguage usage across different communities.
arxiv-9600-212 | Phase and TV Based Convex Sets for Blind Deconvolution of Microscopic Images | http://arxiv.org/pdf/1503.04776v1.pdf | author:Mohammad Tofighi, Onur Yorulmaz, A. Enis Cetin category:math.OC cs.CV published:2015-03-16 summary:In this article, two closed and convex sets for blind deconvolution problemare proposed. Most blurring functions in microscopy are symmetric with respectto the origin. Therefore, they do not modify the phase of the Fourier transform(FT) of the original image. As a result blurred image and the original imagehave the same FT phase. Therefore, the set of images with a prescribed FT phasecan be used as a constraint set in blind deconvolution problems. Another convexset that can be used during the image reconstruction process is the epigraphset of Total Variation (TV) function. This set does not need a prescribed upperbound on the total variation of the image. The upper bound is automaticallyadjusted according to the current image of the restoration process. Both ofthese two closed and convex sets can be used as a part of any blinddeconvolution algorithm. Simulation examples are presented.
arxiv-9600-213 | Skilled Impostor Attacks Against Fingerprint Verification Systems And Its Remedy | http://arxiv.org/pdf/1503.04729v1.pdf | author:Carsten Gottschlich category:cs.CV cs.CR cs.CY published:2015-03-16 summary:Fingerprint verification systems are becoming ubiquitous in everyday life.This trend is propelled especially by the proliferation of mobile devices withfingerprint sensors such as smartphones and tablet computers, and fingerprintverification is increasingly applied for authenticating financial transactions.In this study we describe a novel attack vector against fingerprintverification systems which we coin skilled impostor attack. We show thatexisting protocols for performance evaluation of fingerprint verificationsystems are flawed and as a consequence of this, the system's realvulnerability is systematically underestimated. We examine a scenario in whicha fingerprint verification system is tuned to operate at false acceptance rateof 0.1% using the traditional verification protocols with random impostors(zero-effort attacks). We demonstrate that an active and intelligent attackercan achieve a chance of success in the area of 89% or more against this systemby performing skilled impostor attacks. We describe a new protocol forevaluating fingerprint verification performance in order to improve theassessment of potential and limitations of fingerprint recognition systems.This new evaluation protocol enables a more informed decision concerning theoperating threshold in practical applications and the respective trade-offbetween security (low false acceptance rates) and usability (low falserejection rates). The skilled impostor attack is a general attack concept whichis independent of specific databases or comparison algorithms. The proposedprotocol relying on skilled impostor attacks can directly be applied forevaluating the verification performance of other biometric modalities such ase.g. iris, face, ear, finger vein, gait or speaker recognition.
arxiv-9600-214 | Deep Feelings: A Massive Cross-Lingual Study on the Relation between Emotions and Virality | http://arxiv.org/pdf/1503.04723v1.pdf | author:Marco Guerini, Jacopo Staiano category:cs.SI cs.CL cs.CY published:2015-03-16 summary:This article provides a comprehensive investigation on the relations betweenvirality of news articles and the emotions they are found to evoke. Virality,in our view, is a phenomenon with many facets, i.e. under this generic termseveral different effects of persuasive communication are comprised. Byexploiting a high-coverage and bilingual corpus of documents containing metricsof their spread on social networks as well as a massive affective annotationprovided by readers, we present a thorough analysis of the interplay betweenevoked emotions and viral facets. We highlight and discuss our findings inlight of a cross-lingual approach: while we discover differences in evokedemotions and corresponding viral effects, we provide preliminary evidence of ageneralized explanatory model rooted in the deep structure of emotions: theValence-Arousal-Dominance (VAD) circumplex. We find that viral facets appear tobe consistently affected by particular VAD configurations, and theseconfigurations indicate a clear connection with distinct phenomena underlyingpersuasive communication.
arxiv-9600-215 | A Memcomputing Pascaline | http://arxiv.org/pdf/1503.04673v1.pdf | author:Y. V. Pershin, L. K. Castelano, F. Hartmann, V. Lopez-Richard, M. Di Ventra category:cs.ET cs.NE published:2015-03-16 summary:The original Pascaline was a mechanical calculator able to sum and subtractintegers. It encodes information in the angles of mechanical wheels and througha set of gears, and aided by gravity, could perform the calculations. Here, weshow that such a concept can be realized in electronics using memory elementssuch as memristive systems. By using memristive emulators we have demonstratedexperimentally the memcomputing version of the mechanical Pascaline, capable ofprocessing and storing the numerical results in the multiple levels of eachmemristive element. Our result is the first experimental demonstration ofmultidigit arithmetics with multi-level memory devices that further emphasizesthe versatility and potential of memristive systems for futuremassively-parallel high-density computing architectures.
arxiv-9600-216 | Simple, Accurate, and Robust Nonparametric Blind Super-Resolution | http://arxiv.org/pdf/1503.03187v2.pdf | author:Wen-Ze Shao, Michael Elad category:cs.CV published:2015-03-11 summary:This paper proposes a simple, accurate, and robust approach to single imagenonparametric blind Super-Resolution (SR). This task is formulated as afunctional to be minimized with respect to both an intermediate super-resolvedimage and a nonparametric blur-kernel. The proposed approach includes aconvolution consistency constraint which uses a non-blind learning-based SRresult to better guide the estimation process. Another key component is theunnatural bi-l0-l2-norm regularization imposed on the super-resolved, sharpimage and the blur-kernel, which is shown to be quite beneficial for estimatingthe blur-kernel accurately. The numerical optimization is implemented bycoupling the splitting augmented Lagrangian and the conjugate gradient (CG).Using the pre-estimated blur-kernel, we finally reconstruct the SR image by avery simple non-blind SR method that uses a natural image prior. The proposedapproach is demonstrated to achieve better performance than the recent methodby Michaeli and Irani [2] in both terms of the kernel estimation accuracy andimage SR quality.
arxiv-9600-217 | Convex Color Image Segmentation with Optimal Transport Distances | http://arxiv.org/pdf/1503.01986v2.pdf | author:Julien Rabin, Nicolas Papadakis category:cs.CV published:2015-03-06 summary:This work is about the use of regularized optimal-transport distances forconvex, histogram-based image segmentation. In the considered framework, fixedexemplar histograms define a prior on the statistical features of the tworegions in competition. In this paper, we investigate the use of varioustransport-based cost functions as discrepancy measures and rely on aprimal-dual algorithm to solve the obtained convex optimization problem.
arxiv-9600-218 | Inexact Alternating Direction Method Based on Newton descent algorithm with Application to Poisson Image Deblurring | http://arxiv.org/pdf/1412.4433v2.pdf | author:Dai-Qiang Chen category:cs.CV published:2014-12-15 summary:The recovery of images from the observations that are degraded by a linearoperator and further corrupted by Poisson noise is an important task in modernimaging applications such as astronomical and biomedical ones. Gradient-basedregularizers involve the popular total variation semi-norm have become standardtechniques for Poisson image restoration due to its edge-preserving ability.Various efficient algorithms have been developed for solving the correspondingminimization problem with non-smooth regularization terms. In this paper,motivated by the idea of the alternating direction minimization algorithm andthe Newton's method with upper convergent rate, we further propose inexactalternating direction methods utilizing the proximal Hessian matrix informationof the objective function, in a way reminiscent of Newton descent methods.Besides, we also investigate the global convergence of the proposed algorithmsunder certain conditions. Finally, we illustrate that the proposed algorithmsoutperform the current state-of-the-art algorithms through numericalexperiments on Poisson image deblurring.
arxiv-9600-219 | Simulation of Genetic Algorithm: Traffic Light Efficiency | http://arxiv.org/pdf/1503.04475v1.pdf | author:Eric Lienert category:cs.NE published:2015-03-15 summary:Traffic is a problem in many urban areas worldwide. Traffic flow is dictatedby certain devices such as traffic lights. The traffic lights signal when eachlane is able to pass through the intersection. Often, static schedulesinterfere with ideal traffic flow. The purpose of this project was to find away to make intersections controlled with traffic lights more efficient. Thisgoal was accomplished through the creation of a genetic algorithm, whichenhances an input algorithm through genetic principles to produce the fittestalgorithm. The program was comprised of two major elements: coding in Java andcoding in Simulation of Urban Mobility (SUMO), which is an environment thatsimulates real traffic. The Java code called upon the SUMO simulation via acommand prompt which ran the simulation, received the output, altered thealgorithm, and looped. The SUMO component initialized a simulation in which a 1x 1 street layout was created, each intersection with its own traffic light.Each loop enhanced the input algorithm by altering the scheduling string(dictates the light changes). After the looped simulations were executed, thedata was then analyzed. This was accomplished by creating an algorithm basedupon regular practice, timed traffic lights, and comparing the output which wascomprised of the total time it took for all vehicles to exit the system and theaverage time it took each individual vehicle to exit the system. Thesedifferent variables: the time it took the average vehicle to exit the systemand total time for all vehicles to exit the system, where then graphed togetherto provide a visual aid. The genetic algorithm did improve traffic light andtraffic flow efficiency in comparison to traditional scheduling methods.
arxiv-9600-220 | apsis - Framework for Automated Optimization of Machine Learning Hyper Parameters | http://arxiv.org/pdf/1503.02946v2.pdf | author:Frederik Diehl, Andreas Jauch category:cs.LG published:2015-03-10 summary:The apsis toolkit presented in this paper provides a flexible framework forhyperparameter optimization and includes both random search and a bayesianoptimizer. It is implemented in Python and its architecture featuresadaptability to any desired machine learning code. It can easily be used withcommon Python ML frameworks such as scikit-learn. Published under the MITLicense other researchers are heavily encouraged to check out the code,contribute or raise any suggestions. The code can be found atgithub.com/FrederikDiehl/apsis.
arxiv-9600-221 | Separable and non-separable data representation for pattern discrimination | http://arxiv.org/pdf/1503.04400v1.pdf | author:Jarosław Adam Miszczak category:quant-ph cs.CV cs.LG I.5.2; I.4.1 published:2015-03-15 summary:We provide a complete work-flow, based on the language of quantum informationtheory, suitable for processing data for the purpose of pattern recognition.The main advantage of the introduced scheme is that it can be easilyimplemented and applied to process real-world data using modest computationresources. At the same time it can be used to investigate the difference in thepattern recognition resulting from the utilization of the tensor productstructure of the space of quantum states. We illustrate this difference byproviding a simple example based on the classification of 2D data.
arxiv-9600-222 | The placement of the head that minimizes online memory: a complex systems approach | http://arxiv.org/pdf/1309.1939v2.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL nlin.AO physics.soc-ph published:2013-09-08 summary:It is well known that the length of a syntactic dependency determines itsonline memory cost. Thus, the problem of the placement of a head and itsdependents (complements or modifiers) that minimizes online memory isequivalent to the problem of the minimum linear arrangement of a star tree.However, how that length is translated into cognitive cost is not known. Thisstudy shows that the online memory cost is minimized when the head is placed atthe center, regardless of the function that transforms length into cost,provided only that this function is strictly monotonically increasing. Onlinememory defines a quasi-convex adaptive landscape with a single central minimumif the number of elements is odd and two central minima if that number is even.We discuss various aspects of the dynamics of word order of subject (S), verb(V) and object (O) from a complex systems perspective and suggest that wordorders tend to evolve by swapping adjacent constituents from an initial orearly SOV configuration that is attracted towards a central word order byonline memory minimization. We also suggest that the stability of SVO is due toat least two factors, the quasi-convex shape of the adaptive landscape in theonline memory dimension and online memory adaptations that avoid regression toSOV. Although OVS is also optimal for placing the verb at the center, its lowfrequency is explained by its long distance to the seminal SOV in thepermutation space.
arxiv-9600-223 | Reply to the commentary "Be careful when assuming the obvious", by P. Alday | http://arxiv.org/pdf/1412.7186v2.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL physics.soc-ph published:2014-12-22 summary:Here we respond to some comments by Alday concerning headedness in linguistictheory and the validity of the assumptions of a mathematical model for wordorder. For brevity, we focus only on two assumptions: the unit of measurementof dependency length and the monotonicity of the cost of a dependency as afunction of its length. We also revise the implicit psychological bias inAlday's comments. Notwithstanding, Alday is indicating the path for linguisticresearch with his unusual concerns about parsimony from multiple dimensions.
arxiv-9600-224 | RankMerging: A supervised learning-to-rank framework to predict links in large social network | http://arxiv.org/pdf/1407.2515v3.pdf | author:Lionel Tabourier, Daniel Faria Bernardes, Anne-Sophie Libert, Renaud Lambiotte category:cs.SI cs.IR cs.LG physics.soc-ph published:2014-07-09 summary:Uncovering unknown or missing links in social networks is a difficult taskbecause of their sparsity and because links may represent different types ofrelationships, characterized by different structural patterns. In this paper,we define a simple yet efficient supervised learning-to-rank framework, calledRankMerging, which aims at combining information provided by variousunsupervised rankings. We illustrate our method on three different kinds ofsocial networks and show that it substantially improves the performances ofunsupervised metrics of ranking. We also compare it to other combinationstrategies based on standard methods. Finally, we explore various aspects ofRankMerging, such as feature selection and parameter estimation and discuss itsarea of relevance: the prediction of an adjustable number of links on largenetworks.
arxiv-9600-225 | Surrogate Losses in Passive and Active Learning | http://arxiv.org/pdf/1207.3772v3.pdf | author:Steve Hanneke, Liu Yang category:math.ST cs.LG stat.ML stat.TH published:2012-07-16 summary:Active learning is a type of sequential design for supervised machinelearning, in which the learning algorithm sequentially requests the labels ofselected instances from a large pool of unlabeled data points. The objective isto produce a classifier of relatively low risk, as measured under the 0-1 loss,ideally using fewer label requests than the number of random labeled datapoints sufficient to achieve the same. This work investigates the potentialuses of surrogate loss functions in the context of active learning.Specifically, it presents an active learning algorithm based on an arbitraryclassification-calibrated surrogate loss function, along with an analysis ofthe number of label requests sufficient for the classifier returned by thealgorithm to achieve a given risk under the 0-1 loss. Interestingly, theseresults cannot be obtained by simply optimizing the surrogate risk via activelearning to an extent sufficient to provide a guarantee on the 0-1 loss, as iscommon practice in the analysis of surrogate losses for passive learning. Someof the results have additional implications for the use of surrogate losses inpassive learning.
arxiv-9600-226 | Content-Based Bird Retrieval using Shape context, Color moments and Bag of Features | http://arxiv.org/pdf/1503.07816v1.pdf | author:Bahri Abdelkhalak, Hamid Zouaki category:cs.CV cs.IR published:2015-03-14 summary:In this paper we propose a new descriptor for birds search. First, our workwas carried on the choice of a descriptor. This choice is usually driven by theapplication requirements such as robustness to noise, stability with respect tobias, the invariance to geometrical transformations or tolerance to occlusions.In this context, we introduce a descriptor which combines the shape and colordescriptors to have an effectiveness description of birds. The proposeddescriptor is an adaptation of a descriptor based on the contours defined inarticle Belongie et al. [5] combined with color moments [19]. Specifically,points of interest are extracted from each image and information's in theregion in the vicinity of these points are represented by descriptors of shapecontext concatenated with color moments. Thus, the approach bag of visual wordsis applied to the latter. The experimental results show the effectiveness ofour descriptor for the bird search by content.
arxiv-9600-227 | LiSens --- A Scalable Architecture for Video Compressive Sensing | http://arxiv.org/pdf/1503.04267v1.pdf | author:Jian Wang, Mohit Gupta, Aswin C. Sankaranarayanan category:cs.CV published:2015-03-14 summary:The measurement rate of cameras that take spatially multiplexed measurementsby using spatial light modulators (SLM) is often limited by the switching speedof the SLMs. This is especially true for single-pixel cameras where thephotodetector operates at a rate that is many orders-of-magnitude greater thanthe SLM. We study the factors that determine the measurement rate for suchspatial multiplexing cameras (SMC) and show that increasing the number ofpixels in the device improves the measurement rate, but there is an optimumnumber of pixels (typically, few thousands) beyond which the measurement ratedoes not increase. This motivates the design of LiSens, a novel imagingarchitecture, that replaces the photodetector in the single-pixel camera with a1D linear array or a line-sensor. We illustrate the optical architectureunderlying LiSens, build a prototype, and demonstrate results of a range ofindoor and outdoor scenes. LiSens delivers on the promise of SMCs: imaging at amegapixel resolution, at video rate, using an inexpensive low-resolutionsensor.
arxiv-9600-228 | A Dictionary-based Approach for Estimating Shape and Spatially-Varying Reflectance | http://arxiv.org/pdf/1503.04265v1.pdf | author:Zhuo Hui, Aswin C. Sankaranarayanan category:cs.CV published:2015-03-14 summary:We present a technique for estimating the shape and reflectance of an objectin terms of its surface normals and spatially-varying BRDF. We assume thatmultiple images of the object are obtained under fixed view-point and varyingillumination, i.e, the setting of photometric stereo. Assuming that the BRDF ateach pixel lies in the non-negative span of a known BRDF dictionary, we derivea per-pixel surface normal and BRDF estimation framework that requires neitheriterative optimization techniques nor careful initialization, both of which areendemic to most state-of-the-art techniques. We showcase the performance of ourtechnique on a wide range of simulated and real scenes where we outperformcompeting methods.
arxiv-9600-229 | Statistical Limits of Convex Relaxations | http://arxiv.org/pdf/1503.01442v2.pdf | author:Zhaoran Wang, Quanquan Gu, Han Liu category:stat.ML published:2015-03-04 summary:Many high dimensional sparse learning problems are formulated as nonconvexoptimization. A popular approach to solve these nonconvex optimization problemsis through convex relaxations such as linear and semidefinite programming. Inthis paper, we study the statistical limits of convex relaxations.Particularly, we consider two problems: Mean estimation for sparse principalsubmatrix and edge probability estimation for stochastic block model. Weexploit the sum-of-squares relaxation hierarchy to sharply characterize thelimits of a broad class of convex relaxations. Our result shows statisticaloptimality needs to be compromised for achieving computational tractabilityusing convex relaxations. Compared with existing results on computational lowerbounds for statistical problems, which consider general polynomial-timealgorithms and rely on computational hardness hypotheses on problems likeplanted clique detection, our theory focuses on a broad class of convexrelaxations and does not rely on unproven hypotheses.
arxiv-9600-230 | The YLI-MED Corpus: Characteristics, Procedures, and Plans | http://arxiv.org/pdf/1503.04250v1.pdf | author:Julia Bernd, Damian Borth, Benjamin Elizalde, Gerald Friedland, Heather Gallagher, Luke Gottlieb, Adam Janin, Sara Karabashlieva, Jocelyn Takahashi, Jennifer Won category:cs.MM cs.CL published:2015-03-13 summary:The YLI Multimedia Event Detection corpus is a public-domain index of videoswith annotations and computed features, specialized for research in multimediaevent detection (MED), i.e., automatically identifying what's happening in avideo by analyzing the audio and visual content. The videos indexed in theYLI-MED corpus are a subset of the larger YLI feature corpus, which is beingdeveloped by the International Computer Science Institute and LawrenceLivermore National Laboratory based on the Yahoo Flickr Creative Commons 100Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depictingone of ten target events, or no target event, and are annotated for additionalattributes like language spoken and whether the video has a musical score. Theannotations also include degree of annotator agreement and average annotatorconfidence scores for the event categorization of each video. Version 1.0 ofYLI-MED includes 1823 "positive" videos that depict the target events and48,138 "negative" videos, as well as 177 supplementary videos that are similarto event videos but are not positive examples. Our goal in producing YLI-MED isto be as open about our data and procedures as possible. This report describesthe procedures used to collect the corpus; gives detailed descriptivestatistics about the corpus makeup (and how video attributes affectedannotators' judgments); discusses possible biases in the corpus introduced byour procedural choices and compares it with the most similar existing dataset,TRECVID MED's HAVIC corpus; and gives an overview of our future plans forexpanding the annotation effort.
arxiv-9600-231 | Statistical-Computational Tradeoffs in Planted Problems and Submatrix Localization with a Growing Number of Clusters and Submatrices | http://arxiv.org/pdf/1402.1267v3.pdf | author:Yudong Chen, Jiaming Xu category:stat.ML math.ST stat.TH published:2014-02-06 summary:We consider two closely related problems: planted clustering and submatrixlocalization. The planted clustering problem assumes that a random graph isgenerated based on some underlying clusters of the nodes; the task is torecover these clusters given the graph. The submatrix localization problemconcerns locating hidden submatrices with elevated means inside a largereal-valued random matrix. Of particular interest is the setting where thenumber of clusters/submatrices is allowed to grow unbounded with the problemsize. These formulations cover several classical models such as planted clique,planted densest subgraph, planted partition, planted coloring, and stochasticblock model, which are widely used for studying community detection andclustering/bi-clustering. For both problems, we show that the space of the model parameters(cluster/submatrix size, cluster density, and submatrix mean) can bepartitioned into four disjoint regions corresponding to decreasing statisticaland computational complexities: (1) the \emph{impossible} regime, where allalgorithms fail; (2) the \emph{hard} regime, where the computationallyexpensive Maximum Likelihood Estimator (MLE) succeeds; (3) the \emph{easy}regime, where the polynomial-time convexified MLE succeeds; (4) the\emph{simple} regime, where a simple counting/thresholding procedure succeeds.Moreover, we show that each of these algorithms provably fails in the previousharder regimes. Our theorems establish the minimax recovery limit, which are tight up toconstants and hold with a growing number of clusters/submatrices, and provide astronger performance guarantee than previously known for polynomial-timealgorithms. Our study demonstrates the tradeoffs between statistical andcomputational considerations, and suggests that the minimax recovery limit maynot be achievable by polynomial-time algorithms.
arxiv-9600-232 | What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision | http://arxiv.org/pdf/1503.01558v3.pdf | author:Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew Rabinovich, Kevin Murphy category:cs.CL cs.CV cs.IR published:2015-03-05 summary:We present a novel method for aligning a sequence of instructions to a videoof someone carrying out a task. In particular, we focus on the cooking domain,where the instructions correspond to the recipe. Our technique relies on an HMMto align the recipe steps to the (automatically generated) speech transcript.We then refine this alignment using a state-of-the-art visual food detector,based on a deep convolutional neural network. We show that our techniqueoutperforms simpler techniques based on keyword spotting. It also enablesinteresting applications, such as automatically illustrating recipes withkeyframes, and searching within a video for events of interest.
arxiv-9600-233 | ASKIT: Approximate Skeletonization Kernel-Independent Treecode in High Dimensions | http://arxiv.org/pdf/1410.0260v3.pdf | author:William B. March, Bo Xiao, George Biros category:cs.DS cs.LG published:2014-10-01 summary:We present a fast algorithm for kernel summation problems in high-dimensions.These problems appear in computational physics, numerical approximation,non-parametric statistics, and machine learning. In our context, the sumsdepend on a kernel function that is a pair potential defined on a dataset ofpoints in a high-dimensional Euclidean space. A direct evaluation of the sumscales quadratically with the number of points. Fast kernel summation methodscan reduce this cost to linear complexity, but the constants involved do notscale well with the dimensionality of the dataset. The main algorithmic components of fast kernel summation algorithms are theseparation of the kernel sum between near and far field (which is the basis forpruning) and the efficient and accurate approximation of the far field. We introduce novel methods for pruning and approximating the far field. Ourfar field approximation requires only kernel evaluations and does not useanalytic expansions. Pruning is not done using bounding boxes but rathercombinatorially using a sparsified nearest-neighbor graph of the input. Thetime complexity of our algorithm depends linearly on the ambient dimension. Theerror in the algorithm depends on the low-rank approximability of the farfield, which in turn depends on the kernel function and on the intrinsicdimensionality of the distribution of the points. The error of the far fieldapproximation does not depend on the ambient dimension. We present the new algorithm along with experimental results that demonstrateits performance. We report results for Gaussian kernel sums for 100 millionpoints in 64 dimensions, for one million points in 1000 dimensions, and forproblems in which the Gaussian kernel has a variable bandwidth. To the best ofour knowledge, all of these experiments are impossible or prohibitivelyexpensive with existing fast kernel summation methods.
arxiv-9600-234 | Sparse Code Formation with Linear Inhibition | http://arxiv.org/pdf/1503.04115v1.pdf | author:Nam Do-Hoang Le category:cs.CV published:2015-03-13 summary:Sparse code formation in the primary visual cortex (V1) has been inspirationfor many state-of-the-art visual recognition systems. To stimulate thisbehavior, networks are trained networks under mathematical constraint ofsparsity or selectivity. In this paper, the authors exploit another approachwhich uses lateral interconnections in feature learning networks. However,instead of adding direct lateral interconnections among neurons, we introducean inhibitory layer placed right after normal encoding layer. This ideaovercomes the challenge of computational cost and complexity on lateralnetworks while preserving crucial objective of sparse code formation. Todemonstrate this idea, we use sparse autoencoder as normal encoding layer andapply inhibitory layer. Early experiments in visual recognition show relativeimprovements over traditional approach on CIFAR-10 dataset. Moreover, simpleinstallment and training process using Hebbian rule allow inhibitory layer tobe integrated into existing networks, which enables further analysis in thefuture.
arxiv-9600-235 | LSTM: A Search Space Odyssey | http://arxiv.org/pdf/1503.04069v1.pdf | author:Klaus Greff, Rupesh Kumar Srivastava, Jan Koutník, Bas R. Steunebrink, Jürgen Schmidhuber category:cs.NE cs.LG 68T10 published:2015-03-13 summary:Several variants of the Long Short-Term Memory (LSTM) architecture forrecurrent neural networks have been proposed since its inception in 1995. Inrecent years, these networks have become the state-of-the-art models for avariety of machine learning problems. This has led to a renewed interest inunderstanding the role and utility of various computational components oftypical LSTM variants. In this paper, we present the first large-scale analysisof eight LSTM variants on three representative tasks: speech recognition,handwriting recognition, and polyphonic music modeling. The hyperparameters ofall LSTM variants for each task were optimized separately using random searchand their importance was assessed using the powerful fANOVA framework. Intotal, we summarize the results of 5400 experimental runs (about 15 years ofCPU time), which makes our study the largest of its kind on LSTM networks. Ourresults show that none of the variants can improve upon the standard LSTMarchitecture significantly, and demonstrate the forget gate and the outputactivation function to be its most critical components. We further observe thatthe studied hyperparameters are virtually independent and derive guidelines fortheir efficient adjustment.
arxiv-9600-236 | Hybrid multi-layer Deep CNN/Aggregator feature for image classification | http://arxiv.org/pdf/1503.04065v1.pdf | author:Praveen Kulkarni, Joaquin Zepeda, Frederic Jurie, Patrick Perez, Louis Chevallier category:cs.CV published:2015-03-13 summary:Deep Convolutional Neural Networks (DCNN) have established a remarkableperformance benchmark in the field of image classification, displacingclassical approaches based on hand-tailored aggregations of local descriptors.Yet DCNNs impose high computational burdens both at training and at testingtime, and training them requires collecting and annotating large amounts oftraining data. Supervised adaptation methods have been proposed in theliterature that partially re-learn a transferred DCNN structure from a newtarget dataset. Yet these require expensive bounding-box annotations and arestill computationally expensive to learn. In this paper, we address theseshortcomings of DCNN adaptation schemes by proposing a hybrid approach thatcombines conventional, unsupervised aggregators such as Bag-of-Words (BoW),with the DCNN pipeline by treating the output of intermediate layers as denselyextracted local descriptors. We test a variant of our approach that uses only intermediate DCNN layers onthe standard PASCAL VOC 2007 dataset and show performance significantly higherthan the standard BoW model and comparable to Fisher vector aggregation butwith a feature that is 150 times smaller. A second variant of our approach thatincludes the fully connected DCNN layers significantly outperforms Fishervector schemes and performs comparably to DCNN approaches adapted to Pascal VOC2007, yet at only a small fraction of the training and testing cost.
arxiv-9600-237 | Characterizing driving behavior using automatic visual analysis | http://arxiv.org/pdf/1503.04036v1.pdf | author:Mrinal Haloi, Dinesh Babu Jayagopi category:cs.CV H.4.3 published:2015-03-13 summary:In this work, we present the problem of rash driving detection algorithmusing a single wide angle camera sensor, particularly useful in the Indiancontext. To our knowledge this rash driving problem has not been addressedusing Image processing techniques (existing works use other sensors such asaccelerometer). Car Image processing literature, though rich and mature, doesnot address the rash driving problem. In this work-in-progress paper, wepresent the need to address this problem, our approach and our future plans tobuild a rash driving detector.
arxiv-9600-238 | An implementation of Apertium based Assamese morphological analyzer | http://arxiv.org/pdf/1503.03989v1.pdf | author:Mirzanur Rahman, Shikhar Kumar Sarma category:cs.CL published:2015-03-13 summary:Morphological Analysis is an important branch of linguistics for any NaturalLanguage Processing Technology. Morphology studies the word structure andformation of word of a language. In current scenario of NLP research,morphological analysis techniques have become more popular day by day. Forprocessing any language, morphology of the word should be first analyzed.Assamese language contains very complex morphological structure. In our work wehave used Apertium based Finite-State-Transducers for developing morphologicalanalyzer for Assamese Language with some limited domain and we get 72.7%accuracy
arxiv-9600-239 | Interactive Restless Multi-armed Bandit Game and Swarm Intelligence Effect | http://arxiv.org/pdf/1503.03964v1.pdf | author:Shunsuke Yoshida, Masato Hisakado, Shintaro Mori category:cs.AI cs.LG stat.ML published:2015-03-13 summary:We obtain the conditions for the emergence of the swarm intelligence effectin an interactive game of restless multi-armed bandit (rMAB). A player competeswith multiple agents. Each bandit has a payoff that changes with a probability$p_{c}$ per round. The agents and player choose one of three options: (1)Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a goodbandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probabilityfor Observe in learning. The parameters $(c,p_{obs})$ are uniformlydistributed. We determine the optimal strategies for the player using completeknowledge about the rMAB. We show whether or not social or asocial learning ismore optimal in the $(p_{c},n_{I})$ space and define the swarm intelligenceeffect. We conduct a laboratory experiment (67 subjects) and observe the swarmintelligence effect only if $(p_{c},n_{I})$ are chosen so that social learningis far more optimal than asocial learning.
arxiv-9600-240 | An Adaptive Online HDP-HMM for Segmentation and Classification of Sequential Data | http://arxiv.org/pdf/1503.02761v2.pdf | author:Ava Bargi, Richard Yi Da Xu, Massimo Piccardi category:stat.ML cs.LG published:2015-03-10 summary:In the recent years, the desire and need to understand sequential data hasbeen increasing, with particular interest in sequential contexts such aspatient monitoring, understanding daily activities, video surveillance, stockmarket and the like. Along with the constant flow of data, it is critical toclassify and segment the observations on-the-fly, without being limited to arigid number of classes. In addition, the model needs to be capable of updatingits parameters to comply with possible evolutions. This interesting problem,however, is not adequately addressed in the literature since many studies focuson offline classification over a pre-defined class set. In this paper, wepropose a principled solution to this gap by introducing an adaptive onlinesystem based on Markov switching models with hierarchical Dirichlet processpriors. This infinite adaptive online approach is capable of segmenting andclassifying the sequential data over unlimited number of classes, while meetingthe memory and delay constraints of streaming contexts. The model is furtherenhanced by introducing a learning rate, responsible for balancing the extentto which the model sustains its previous learning (parameters) or adapts to thenew streaming observations. Experimental results on several variants ofstationary and evolving synthetic data and two video datasets, TUM AssistiveKitchen and collatedWeizmann, show remarkable performance in segmentation andclassification, particularly for evolutionary sequences with changingdistributions and/or containing new, unseen classes.
arxiv-9600-241 | Diagnosing Heterogeneous Dynamics for CT Scan Images of Human Brain in Wavelet and MFDFA domain | http://arxiv.org/pdf/1503.03913v1.pdf | author:Sabyasachi Mukhopadhyay, Soham Mandal, Nandan K Das, Subhadip Dey, Asish Mitra, Nirmalya Ghosh, Prasanta K Panigrahi category:cs.CV published:2015-03-12 summary:CT scan images of human brain of a particular patient in different crosssections are taken, on which wavelet transform and multi-fractal analysis areapplied. The vertical and horizontal unfolding of images are done beforeanalyzing these images. A systematic investigation of de-noised CT scan imagesof human brain in different cross-sections are carried out through waveletnormalized energy and wavelet semi-log plots, which clearly points out themismatch between results of vertical and horizontal unfolding. The mismatch ofresults confirms the heterogeneity in spatial domain. Using the multi-fractalde-trended fluctuation analysis (MFDFA), the mismatch between the values ofHurst exponent and width of singularity spectrum by vertical and horizontalunfolding confirms the same.
arxiv-9600-242 | Detecting Overlapping Communities in Networks Using Spectral Methods | http://arxiv.org/pdf/1412.3432v4.pdf | author:Yuan Zhang, Elizaveta Levina, Ji Zhu category:stat.ML published:2014-12-10 summary:Community detection is a fundamental problem in network analysis which ismade more challenging by overlaps between communities which often occur inpractice. Here we propose a general, flexible, and interpretable generativemodel for overlapping communities, which can be thought of as a generalizationof the degree-corrected stochastic block model. We develop an efficientspectral algorithm for estimating the community memberships, which deals withthe overlaps by employing the K-medians algorithm rather than the usual K-meansfor clustering in the spectral domain. We show that the algorithm isasymptotically consistent when networks are not too sparse and the overlapsbetween communities not too large. Numerical experiments on both simulatednetworks and many real social networks demonstrate that our method performsvery well compared to a number of benchmark methods for overlapping communitydetection.
arxiv-9600-243 | Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation | http://arxiv.org/pdf/1503.01655v2.pdf | author:Eneko Agirre, Ander Barrena, Aitor Soroa category:cs.CL published:2015-03-05 summary:Hyperlinks and other relations in Wikipedia are a extraordinary resourcewhich is still not fully understood. In this paper we study the different typesof links in Wikipedia, and contrast the use of the full graph with respect tojust direct links. We apply a well-known random walk algorithm on two tasks,word relatedness and named-entity disambiguation. We show that using the fullgraph is more effective than just direct links by a large margin, thatnon-reciprocal links harm performance, and that there is no benefit fromcategories and infoboxes, with coherent results on both tasks. We set newstate-of-the-art figures for systems based on Wikipedia links, comparable tosystems exploiting several information sources and/or supervised machinelearning. Our approach is open source, with instruction to reproduce results,and amenable to be integrated with complementary text-based methods.
arxiv-9600-244 | Approximating Sparse PCA from Incomplete Data | http://arxiv.org/pdf/1503.03903v1.pdf | author:Abhisek Kundu, Petros Drineas, Malik Magdon-Ismail category:cs.LG cs.IT cs.NA math.IT stat.ML published:2015-03-12 summary:We study how well one can recover sparse principal components of a datamatrix using a sketch formed from a few of its elements. We show that for awide class of optimization problems, if the sketch is close (in the spectralnorm) to the original data matrix, then one can recover a near optimal solutionto the optimization problem by using the sketch. In particular, we use thisapproach to obtain sparse principal components and show that for \math{m} datapoints in \math{n} dimensions, \math{O(\epsilon^{-2}\tilde k\max\{m,n\})}elements gives an \math{\epsilon}-additive approximation to the sparse PCAproblem (\math{\tilde k} is the stable rank of the data matrix). We demonstrateour algorithms extensively on image, text, biological and financial data. Theresults show that not only are we able to recover the sparse PCAs from theincomplete data, but by using our sparse sketch, the running time drops by afactor of five or more.
arxiv-9600-245 | Compact Nonlinear Maps and Circulant Extensions | http://arxiv.org/pdf/1503.03893v1.pdf | author:Felix X. Yu, Sanjiv Kumar, Henry Rowley, Shih-Fu Chang category:stat.ML cs.LG published:2015-03-12 summary:Kernel approximation via nonlinear random feature maps is widely used inspeeding up kernel machines. There are two main challenges for the conventionalkernel approximation methods. First, before performing kernel approximation, agood kernel has to be chosen. Picking a good kernel is a very challengingproblem in itself. Second, high-dimensional maps are often required in order toachieve good performance. This leads to high computational cost in bothgenerating the nonlinear maps, and in the subsequent learning and predictionprocess. In this work, we propose to optimize the nonlinear maps directly withrespect to the classification objective in a data-dependent fashion. Theproposed approach achieves kernel approximation and kernel learning in a jointframework. This leads to much more compact maps without hurting theperformance. As a by-product, the same framework can also be used to achievemore compact kernel maps to approximate a known kernel. We also introduceCirculant Nonlinear Maps, which uses a circulant-structured projection matrixto speed up the nonlinear maps for high-dimensional data.
arxiv-9600-246 | Qualitative inequalities for squared partial correlations of a Gaussian random vector | http://arxiv.org/pdf/1503.03879v1.pdf | author:Sanjay Chaudhuri category:math.ST stat.AP stat.ME stat.ML stat.TH published:2015-03-12 summary:We describe various sets of conditional independence relationships,sufficient for qualitatively comparing non-vanishing squared partialcorrelations of a Gaussian random vector. These sufficient conditions aresatisfied by several graphical Markov models. Rules for comparing degree ofassociation among the vertices of such Gaussian graphical models are alsodeveloped. We apply these rules to compare conditional dependencies on Gaussiantrees. In particular for trees, we show that such dependence can be completelycharacterized by the length of the paths joining the dependent vertices to eachother and to the vertices conditioned on. We also apply our results topostulate rules for model selection for polytree models. Our rules apply tomutual information of Gaussian random vectors as well.
arxiv-9600-247 | TILDE: A Temporally Invariant Learned DEtector | http://arxiv.org/pdf/1411.4568v3.pdf | author:Yannick Verdie, Kwang Moo Yi, Pascal Fua, Vincent Lepetit category:cs.CV published:2014-11-17 summary:We introduce a learning-based approach to detect repeatable keypoints underdrastic imaging changes of weather and lighting conditions to whichstate-of-the-art keypoint detectors are surprisingly sensitive. We firstidentify good keypoint candidates in multiple training images taken from thesame viewpoint. We then train a regressor to predict a score map whose maximaare those points so that they can be found by simple non-maximum suppression.As there are no standard datasets to test the influence of these kinds ofchanges, we created our own, which we will make publicly available. We willshow that our method significantly outperforms the state-of-the-art methods insuch challenging conditions, while still achieving state-of-the-art performanceon the untrained standard Oxford dataset.
arxiv-9600-248 | Spectral Clustering for Divide-and-Conquer Graph Matching | http://arxiv.org/pdf/1310.1297v5.pdf | author:Vince Lyzinski, Daniel L. Sussman, Donniell E. Fishkind, Henry Pao, Li Chen, Joshua T. Vogelstein, Youngser Park, Carey E. Priebe category:stat.ML math.OC stat.CO published:2013-10-04 summary:We present a parallelized bijective graph matching algorithm that leveragesseeds and is designed to match very large graphs. Our algorithm combinesspectral graph embedding with existing state-of-the-art seeded graph matchingprocedures. We justify our approach by proving that modestly correlated, largestochastic block model random graphs are correctly matched utilizing very fewseeds through our divide-and-conquer procedure. We also demonstrate theeffectiveness of our approach in matching very large graphs in simulated andreal data examples, showing up to a factor of 8 improvement in runtime withminimal sacrifice in accuracy.
arxiv-9600-249 | Learning to Detect Vehicles by Clustering Appearance Patterns | http://arxiv.org/pdf/1503.03771v1.pdf | author:Eshed Ohn-Bar, Mohan M. Trivedi category:cs.CV published:2015-03-12 summary:This paper studies efficient means for dealing with intra-category diversityin object detection. Strategies for occlusion and orientation handling areexplored by learning an ensemble of detection models from visual andgeometrical clusters of object instances. An AdaBoost detection scheme isemployed with pixel lookup features for fast detection. The analysis providesinsight into the design of a robust vehicle detection system, showing promisein terms of detection performance and orientation estimation accuracy.
arxiv-9600-250 | Geometry and Expressive Power of Conditional Restricted Boltzmann Machines | http://arxiv.org/pdf/1402.3346v3.pdf | author:Guido Montufar, Nihat Ay, Keyan Ghazi-Zahedi category:cs.NE cs.LG stat.ML published:2014-02-14 summary:Conditional restricted Boltzmann machines are undirected stochastic neuralnetworks with a layer of input and output units connected bipartitely to alayer of hidden units. These networks define models of conditional probabilitydistributions on the states of the output units given the states of the inputunits, parametrized by interaction weights and biases. We address therepresentational power of these models, proving results their ability torepresent conditional Markov random fields and conditional distributions withrestricted supports, the minimal size of universal approximators, the maximalmodel approximation errors, and on the dimension of the set of representableconditional distributions. We contribute new tools for investigatingconditional probability models, which allow us to improve the results that canbe derived from existing work on restricted Boltzmann machine probabilitymodels.
arxiv-9600-251 | 2D Face Recognition System Based on Selected Gabor Filters and Linear Discriminant Analysis LDA | http://arxiv.org/pdf/1503.03741v1.pdf | author:Samir F. Hafez, Mazen M. Selim, Hala H. Zayed category:cs.CV published:2015-03-12 summary:We present a new approach for face recognition system. The method is based on2D face image features using subset of non-correlated and Orthogonal GaborFilters instead of using the whole Gabor Filter Bank, then compressing theoutput feature vector using Linear Discriminant Analysis (LDA). The face imagehas been enhanced using multi stage image processing technique to normalize itand compensate for illumination variation. Experimental results show that theproposed system is effective for both dimension reduction and good recognitionperformance when compared to the complete Gabor filter bank. The system hasbeen tested using CASIA, ORL and Cropped YaleB 2D face images Databases andachieved average recognition rate of 98.9 %.
arxiv-9600-252 | Starting engagement detection towards a companion robot using multimodal features | http://arxiv.org/pdf/1503.03732v1.pdf | author:Dominique Vaufreydaz, Wafa Johal, Claudine Combe category:cs.RO cs.CV published:2015-03-12 summary:Recognition of intentions is a subconscious cognitive process vital to humancommunication. This skill enables anticipation and increases the quality ofinteractions between humans. Within the context of engagement, non-verbalsignals are used to communicate the intention of starting the interaction witha partner. In this paper, we investigated methods to detect these signals inorder to allow a robot to know when it is about to be addressed. Originality ofour approach resides in taking inspiration from social and cognitive sciencesto perform our perception task. We investigate meaningful features, i.e. humanreadable features, and elicit which of these are important for recognizingsomeone's intention of starting an interaction. Classically, spatialinformation like the human position and speed, the human-robot distance areused to detect the engagement. Our approach integrates multimodal featuresgathered using a companion robot equipped with a Kinect. The evaluation on ourcorpus collected in spontaneous conditions highlights its robustness andvalidates the use of such a technique in a real environment. Experimentalvalidation shows that multimodal features set gives better precision and recallthan using only spatial and speed features. We also demonstrate that 7 selectedfeatures are sufficient to provide a good starting engagement detection score.In our last investigation, we show that among our full 99 features set, thespace reduction is not a solved task. This result opens new researchesperspectives on multimodal engagement detection.
arxiv-9600-253 | Boosting of Image Denoising Algorithms | http://arxiv.org/pdf/1502.06220v2.pdf | author:Yaniv Romano, Michael Elad category:cs.CV cs.NA published:2015-02-22 summary:In this paper we propose a generic recursive algorithm for improving imagedenoising methods. Given the initial denoised image, we suggest repeating thefollowing "SOS" procedure: (i) (S)trengthen the signal by adding the previousdenoised image to the degraded input image, (ii) (O)perate the denoising methodon the strengthened image, and (iii) (S)ubtract the previous denoised imagefrom the restored signal-strengthened outcome. The convergence of this processis studied for the K-SVD image denoising and related algorithms. Still in thecontext of K-SVD image denoising, we introduce an interesting interpretation ofthe SOS algorithm as a technique for closing the gap between the localpatch-modeling and the global restoration task, thereby leading to improvedperformance. In a quest for the theoretical origin of the SOS algorithm, weprovide a graph-based interpretation of our method, where the SOS recursiveupdate effectively minimizes a penalty function that aims to denoise the image,while being regularized by the graph Laplacian. We demonstrate the SOS boostingalgorithm for several leading denoising methods (K-SVD, NLM, BM3D, and EPLL),showing tendency to further improve denoising performance.
arxiv-9600-254 | RPCA-KFE: Key Frame Extraction for Consumer Video based Robust Principal Component Analysis | http://arxiv.org/pdf/1405.1678v3.pdf | author:Chinh Dang, Abdolreza Moghadam, Hayder Radha category:cs.CV published:2014-05-07 summary:Key frame extraction algorithms consider the problem of selecting a subset ofthe most informative frames from a video to summarize its content.
arxiv-9600-255 | Functional Inverse Regression in an Enlarged Dimension Reduction Space | http://arxiv.org/pdf/1503.03673v1.pdf | author:Ting-Li Chen, Su-Yun Huang, Yanyuan Ma, I-Ping Tu category:math.ST stat.ML stat.TH published:2015-03-12 summary:We consider an enlarged dimension reduction space in functional inverseregression. Our operator and functional analysis based approach facilitates acompact and rigorous formulation of the functional inverse regression problem.It also enables us to expand the possible space where the dimension reductionfunctions belong. Our formulation provides a unified framework so that theclassical notions, such as covariance standardization, Mahalanobis distance,SIR and linear discriminant analysis, can be naturally and smoothly carried outin our enlarged space. This enlarged dimension reduction space also links tothe linear discriminant space of Gaussian measures on a separable Hilbertspace.
arxiv-9600-256 | Combining Language and Vision with a Multimodal Skip-gram Model | http://arxiv.org/pdf/1501.02598v3.pdf | author:Angeliki Lazaridou, Nghia The Pham, Marco Baroni category:cs.CL cs.CV cs.LG published:2015-01-12 summary:We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visualinformation into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM)build vector-based word representations by learning to predict linguisticcontexts in text corpora. However, for a restricted set of words, the modelsare also exposed to visual representations of the objects they denote(extracted from natural images), and must predict linguistic and visualfeatures jointly. The MMSKIP-GRAM models achieve good performance on a varietyof semantic benchmarks. Moreover, since they propagate visual information toall words, we use them to improve image labeling and retrieval in the zero-shotsetup, where the test concepts are never seen during model training. Finally,the MMSKIP-GRAM models discover intriguing visual properties of abstract words,paving the way to realistic implementations of embodied theories of meaning.
arxiv-9600-257 | Single image super-resolution by approximated Heaviside functions | http://arxiv.org/pdf/1503.03630v1.pdf | author:Liang-Jian Deng, Weihong Guo, Ting-Zhu Huang category:cs.CV cs.IT math.IT math.OC published:2015-03-12 summary:Image super-resolution is a process to enhance image resolution. It is widelyused in medical imaging, satellite imaging, target recognition, etc. In thispaper, we conduct continuous modeling and assume that the unknown imageintensity function is defined on a continuous domain and belongs to a spacewith a redundant basis. We propose a new iterative model for single imagesuper-resolution based on an observation: an image is consisted of smoothcomponents and non-smooth components, and we use two classes of approximatedHeaviside functions (AHFs) to represent them respectively. Due to sparsity ofthe non-smooth components, a $L_{1}$ model is employed. In addition, we applythe proposed iterative model to image patches to reduce computation andstorage. Comparisons with some existing competitive methods show theeffectiveness of the proposed method.
arxiv-9600-258 | On the Impossibility of Learning the Missing Mass | http://arxiv.org/pdf/1503.03613v1.pdf | author:Elchanan Mossel, Mesrob I. Ohannessian category:stat.ML cs.IT cs.LG math.IT math.PR math.ST stat.TH published:2015-03-12 summary:This paper shows that one cannot learn the probability of rare events withoutimposing further structural assumptions. The event of interest is that ofobtaining an outcome outside the coverage of an i.i.d. sample from a discretedistribution. The probability of this event is referred to as the "missingmass". The impossibility result can then be stated as: the missing mass is notdistribution-free PAC-learnable in relative error. The proof issemi-constructive and relies on a coupling argument using a dithered geometricdistribution. This result formalizes the folklore that in order to predict rareevents, one necessarily needs distributions with "heavy tails".
arxiv-9600-259 | Low-Level Features for Image Retrieval Based on Extraction of Directional Binary Patterns and Its Oriented Gradients Histogram | http://arxiv.org/pdf/1503.03606v1.pdf | author:Nagaraja S., Prabhakar C. J. category:cs.CV cs.IR published:2015-03-12 summary:In this paper, we present a novel approach for image retrieval based onextraction of low level features using techniques such as Directional BinaryCode, Haar Wavelet transform and Histogram of Oriented Gradients. The DBCtexture descriptor captures the spatial relationship between any pair ofneighbourhood pixels in a local region along a given direction, while LocalBinary Patterns descriptor considers the relationship between a given pixel andits surrounding neighbours. Therefore, DBC captures more spatial informationthan LBP and its variants, also it can extract more edge information than LBP.Hence, we employ DBC technique in order to extract grey level texture featurefrom each RGB channels individually and computed texture maps are furthercombined which represents colour texture features of an image. Then, wedecomposed the extracted colour texture map and original image using Haarwavelet transform. Finally, we encode the shape and local features of wavelettransformed images using Histogram of Oriented Gradients for content basedimage retrieval. The performance of proposed method is compared with existingmethods on two databases such as Wang's corel image and Caltech 256. Theevaluation results show that our approach outperforms the existing methods forimage retrieval.
arxiv-9600-260 | Efficient Learning of Linear Separators under Bounded Noise | http://arxiv.org/pdf/1503.03594v1.pdf | author:Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, Ruth Urner category:cs.LG cs.CC published:2015-03-12 summary:We study the learnability of linear separators in $\Re^d$ in the presence ofbounded (a.k.a Massart) noise. This is a realistic generalization of the randomclassification noise model, where the adversary can flip each example $x$ withprobability $\eta(x) \leq \eta$. We provide the first polynomial time algorithmthat can learn linear separators to arbitrarily small excess error in thisnoise model under the uniform distribution over the unit ball in $\Re^d$, forsome constant value of $\eta$. While widely studied in the statistical learningtheory community in the context of getting faster convergence rates,computationally efficient algorithms in this model had remained elusive. Ourwork provides the first evidence that one can indeed design algorithmsachieving arbitrarily small excess error in polynomial time under thisrealistic noise model and thus opens up a new and exciting line of research. We additionally provide lower bounds showing that popular algorithms such ashinge loss minimization and averaging cannot lead to arbitrarily small excesserror under Massart noise, even under the uniform distribution. Our workinstead, makes use of a margin based technique developed in the context ofactive learning. As a result, our algorithm is also an active learningalgorithm with label complexity that is only a logarithmic the desired excesserror $\epsilon$.
arxiv-9600-261 | LINE: Large-scale Information Network Embedding | http://arxiv.org/pdf/1503.03578v1.pdf | author:Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei category:cs.LG published:2015-03-12 summary:This paper studies the problem of embedding very large information networksinto low-dimensional vector spaces, which is useful in many tasks such asvisualization, node classification, and link prediction. Most existing graphembedding methods do not scale for real world information networks whichusually contain millions of nodes. In this paper, we propose a novel networkembedding method called the "LINE," which is suitable for arbitrary types ofinformation networks: undirected, directed, and/or weighted. The methodoptimizes a carefully designed objective function that preserves both the localand global network structures. An edge-sampling algorithm is proposed thataddresses the limitation of the classical stochastic gradient descent andimproves both the effectiveness and the efficiency of the inference. Empiricalexperiments prove the effectiveness of the LINE on a variety of real-worldinformation networks, including language networks, social networks, andcitation networks. The algorithm is very efficient, which is able to learn theembedding of a network with millions of vertices and billions of edges in a fewhours on a typical single machine. The source code of the LINE is availableonline.
arxiv-9600-262 | Classification and its applications for drug-target interaction identification | http://arxiv.org/pdf/1502.04469v4.pdf | author:Jian-Ping Mei, Chee-Keong Kwoh, Peng Yang, Xiao-Li Li category:cs.LG q-bio.MN q-bio.QM published:2015-02-16 summary:Classification is one of the most popular and widely used supervised learningtasks, which categorizes objects into predefined classes based on knownknowledge. Classification has been an important research topic in machinelearning and data mining. Different classification methods have been proposedand applied to deal with various real-world problems. Unlike unsupervisedlearning such as clustering, a classifier is typically trained with labeleddata before being used to make prediction, and usually achieves higher accuracythan unsupervised one. In this paper, we first define classification and then review severalrepresentative methods. After that, we study in details the application ofclassification to a critical problem in drug discovery, i.e., drug-targetprediction, due to the challenges in predicting possible interactions betweendrugs and targets.
arxiv-9600-263 | Background Modelling using Octree Color Quantization | http://arxiv.org/pdf/1412.1945v2.pdf | author:Aditya A. V. Sastry category:cs.CV 65D19 published:2014-12-05 summary:By assuming that the most frequently occuring color in a video or a region ofa video I propose a new algorithm for detecting foreground objects in a video.The process of detecting the foreground objects is complicated because of thefact that there may be swaying trees, objects of the background being movedaround or lighting changes in the video. To deal with such complexities manyhave come up with solutions which heavily rely on expensive floating pointoperations. In this paper I used a data structure called Octree which isimplemented only using binary operations. Traditionally octrees were used forcolor quantization but here in this paper I used it as a data structure tostore the most frequently occuring colors in a video as well. For each of thestarting few video frames, I constructed a Octree using all the colors of thatframe. Next I pruned all the trees by removing nodes below a certain height andgave the leaf nodes a color which is dependant on the topological path fromthat node to its parent. Hence any two leaf nodes in two different octrees withthe same topological path from themselves to the root will represent the samecolor. Next I merged all these individual trees into a single tree retainingonly those nodes whose topological path to itself from the root is most commonamong all the trees. The colors represented by the leaf nodes in the resultanttree will be the most frequently occuring colors in the starting video framesof the video. Hence any color of an incomming frame that is not close to any ofthe colors represented by the leaf node of the merged tree can be regarded asbelonging to a foreground object. As an Octree is constructed using only binary operations, it is very fastcompared to other leading algorithms.
arxiv-9600-264 | A model-based approach to recovering the structure of a plant from images | http://arxiv.org/pdf/1503.03191v2.pdf | author:Ben Ward, John Bastian, Anton van den Hengel, Daniel Pooley, Rajendra Bari, Bettina Berger, Mark Tester category:cs.CV published:2015-03-11 summary:We present a method for recovering the structure of a plant directly from asmall set of widely-spaced images. Structure recovery is more complex thanshape estimation, but the resulting structure estimate is more closely relatedto phenotype than is a 3D geometric model. The method we propose is applicableto a wide variety of plants, but is demonstrated on wheat. Wheat is made up ofthin elements with few identifiable features, making it difficult to analyseusing standard feature matching techniques. Our method instead analyses thestructure of plants using only their silhouettes. We employ a generate-and-testmethod, using a database of manually modelled leaves and a model for theircomposition to synthesise plausible plant structures which are evaluatedagainst the images. The method is capable of efficiently recovering accurateestimates of plant structure in a wide variety of imaging scenarios, with nomanual intervention.
arxiv-9600-265 | Describing and Understanding Neighborhood Characteristics through Online Social Media | http://arxiv.org/pdf/1503.03524v1.pdf | author:Mohamed Kafsi, Henriette Cramer, Bart Thomee, David A. Shamma category:stat.ML cs.SI published:2015-03-11 summary:Geotagged data can be used to describe regions in the world and discoverlocal themes. However, not all data produced within a region is necessarilyspecifically descriptive of that area. To surface the content that ischaracteristic for a region, we present the geographical hierarchy model (GHM),a probabilistic model based on the assumption that data observed in a region isa random mixture of content that pertains to different levels of a hierarchy.We apply the GHM to a dataset of 8 million Flickr photos in order todiscriminate between content (i.e., tags) that specifically characterizes aregion (e.g., neighborhood) and content that characterizes surrounding areas ormore general themes. Knowledge of the discriminative and non-discriminativeterms used throughout the hierarchy enables us to quantify the uniqueness of agiven region and to compare similar but distant regions. Our evaluationdemonstrates that our model improves upon traditional Naive Bayesclassification by 47% and hierarchical TF-IDF by 27%. We further highlight thedifferences and commonalities with human reasoning about what is locallycharacteristic for a neighborhood, distilled from ten interviews and a surveythat covered themes such as time, events, and prior regional knowledge
arxiv-9600-266 | Switching to Learn | http://arxiv.org/pdf/1503.03517v1.pdf | author:Shahin Shahrampour, Mohammad Amin Rahimian, Ali Jadbabaie category:cs.LG math.OC stat.ML published:2015-03-11 summary:A network of agents attempt to learn some unknown state of the world drawn bynature from a finite set. Agents observe private signals conditioned on thetrue state, and form beliefs about the unknown state accordingly. Each agentmay face an identification problem in the sense that she cannot distinguish thetruth in isolation. However, by communicating with each other, agents are ableto benefit from side observations to learn the truth collectively. Unlike manydistributed algorithms which rely on all-time communication protocols, wepropose an efficient method by switching between Bayesian and non-Bayesianregimes. In this model, agents exchange information only when their privatesignals are not informative enough; thence, by switching between the tworegimes, agents efficiently learn the truth using only a few rounds ofcommunications. The proposed algorithm preserves learnability while incurring alower communication cost. We also verify our theoretical findings by simulationexamples.
arxiv-9600-267 | Directed Information Graphs | http://arxiv.org/pdf/1204.2003v2.pdf | author:Christopher J. Quinn, Negar Kiyavash, Todd P. Coleman category:cs.IT cs.AI math.IT stat.ML published:2012-04-09 summary:We propose a graphical model for representing networks of stochasticprocesses, the minimal generative model graph. It is based on reducedfactorizations of the joint distribution over time. We show that underappropriate conditions, it is unique and consistent with another type ofgraphical model, the directed information graph, which is based on ageneralization of Granger causality. We demonstrate how directed informationquantifies Granger causality in a particular sequential prediction setting. Wealso develop efficient methods to estimate the topological structure from datathat obviate estimating the joint statistics. One algorithm assumesupper-bounds on the degrees and uses the minimal dimension statisticsnecessary. In the event that the upper-bounds are not valid, the resultinggraph is nonetheless an optimal approximation. Another algorithm usesnear-minimal dimension statistics when no bounds are known but the distributionsatisfies a certain criterion. Analogous to how structure learning algorithmsfor undirected graphical models use mutual information estimates, thesealgorithms use directed information estimates. We characterize thesample-complexity of two plug-in directed information estimators and obtainconfidence intervals. For the setting when point estimates are unreliable, wepropose an algorithm that uses confidence intervals to identify the bestapproximation that is robust to estimation error. Lastly, we demonstrate theeffectiveness of the proposed algorithms through analysis of both syntheticdata and real data from the Twitter network. In the latter case, we identifywhich news sources influence users in the network by merely analyzing tweettimes.
arxiv-9600-268 | Appearance-based indoor localization: A comparison of patch descriptor performance | http://arxiv.org/pdf/1503.03514v1.pdf | author:Jose Rivera-Rubio, Ioannis Alexiou, Anil A. Bharath category:cs.CV cs.RO 68T45, 68T40 published:2015-03-11 summary:Vision is one of the most important of the senses, and humans use itextensively during navigation. We evaluated different types of image and videoframe descriptors that could be used to determine distinctive visual landmarksfor localizing a person based on what is seen by a camera that they carry. Todo this, we created a database containing over 3 km of video-sequences withground-truth in the form of distance travelled along different corridors. Usingthis database, the accuracy of localization - both in terms of knowing whichroute a user is on - and in terms of position along a certain route, can beevaluated. For each type of descriptor, we also tested different techniques toencode visual structure and to search between journeys to estimate a user'sposition. The techniques include single-frame descriptors, those usingsequences of frames, and both colour and achromatic descriptors. We found thatsingle-frame indexing worked better within this particular dataset. This mightbe because the motion of the person holding the camera makes the video toodependent on individual steps and motions of one particular journey. Ourresults suggest that appearance-based information could be an additional sourceof navigational data indoors, augmenting that provided by, say, radio signalstrength indicators (RSSIs). Such visual information could be collected bycrowdsourcing low-resolution video feeds, allowing journeys made by differentusers to be associated with each other, and location to be inferred withoutrequiring explicit mapping. This offers a complementary approach to methodsbased on simultaneous localization and mapping (SLAM) algorithms.
arxiv-9600-269 | Diverse Landmark Sampling from Determinantal Point Processes for Scalable Manifold Learning | http://arxiv.org/pdf/1503.03506v1.pdf | author:Christian Wachinger, Polina Golland category:cs.LG cs.AI cs.CV published:2015-03-11 summary:High computational costs of manifold learning prohibit its application forlarge point sets. A common strategy to overcome this problem is to performdimensionality reduction on selected landmarks and to successively embed theentire dataset with the Nystr\"om method. The two main challenges that ariseare: (i) the landmarks selected in non-Euclidean geometries must result in alow reconstruction error, (ii) the graph constructed from sparsely sampledlandmarks must approximate the manifold well. We propose the sampling oflandmarks from determinantal distributions on non-Euclidean spaces. Sincecurrent determinantal sampling algorithms have the same complexity as those formanifold learning, we present an efficient approximation running in lineartime. Further, we recover the local geometry after the sparsification byassigning each landmark a local covariance matrix, estimated from the originalpoint set. The resulting neighborhood selection based on the Bhattacharyyadistance improves the embedding of sparsely sampled manifolds. Our experimentsshow a significant performance improvement compared to state-of-the-artlandmark selection techniques.
arxiv-9600-270 | Computational Lower Bounds for Community Detection on Random Graphs | http://arxiv.org/pdf/1406.6625v3.pdf | author:Bruce Hajek, Yihong Wu, Jiaming Xu category:math.ST cs.CC stat.ML stat.TH published:2014-06-25 summary:This paper studies the problem of detecting the presence of a small densecommunity planted in a large Erd\H{o}s-R\'enyi random graph $\mathcal{G}(N,q)$,where the edge probability within the community exceeds $q$ by a constantfactor. Assuming the hardness of the planted clique detection problem, we showthat the computational complexity of detecting the community exhibits thefollowing phase transition phenomenon: As the graph size $N$ grows and thegraph becomes sparser according to $q=N^{-\alpha}$, there exists a criticalvalue of $\alpha = \frac{2}{3}$, below which there exists a computationallyintensive procedure that can detect far smaller communities than anycomputationally efficient procedure, and above which a linear-time procedure isstatistically optimal. The results also lead to the average-case hardnessresults for recovering the dense community and approximating the densest$K$-subgraph.
arxiv-9600-271 | Properties of simple sets in digital spaces. Contractions of simple sets preserving the homotopy type of a digital space | http://arxiv.org/pdf/1503.03491v1.pdf | author:Alexander V. Evako category:cs.CV cs.DM math.AT published:2015-03-11 summary:A point of a digital space is called simple if it can be deleted from thespace without altering topology. This paper introduces the notion simple set ofpoints of a digital space. The definition is based on contractible spaces andcontractible transformations. A set of points in a digital space is calledsimple if it can be contracted to a point without changing topology of thespace. It is shown that contracting a simple set of points does not change thehomotopy type of a digital space, and the number of points in a digital spacewithout simple points can be reduces by contracting simple sets. Using theprocess of contracting, we can substantially compress a digital space whilepreserving the topology. The paper proposes a method for thinning a digitalspace which shows that this approach can contribute to computer science such asmedical imaging, computer graphics and pattern analysis.
arxiv-9600-272 | Principal Sensitivity Analysis | http://arxiv.org/pdf/1412.6785v2.pdf | author:Sotetsu Koyamada, Masanori Koyama, Ken Nakae, Shin Ishii category:stat.ML cs.LG published:2014-12-21 summary:We present a novel algorithm (Principal Sensitivity Analysis; PSA) to analyzethe knowledge of the classifier obtained from supervised machine learningtechniques. In particular, we define principal sensitivity map (PSM) as thedirection on the input space to which the trained classifier is most sensitive,and use analogously defined k-th PSM to define a basis for the input space. Wetrain neural networks with artificial data and real data, and apply thealgorithm to the obtained supervised classifiers. We then visualize the PSMs todemonstrate the PSA's ability to decompose the knowledge acquired by thetrained classifiers.
arxiv-9600-273 | Automatic Unsupervised Tensor Mining with Quality Assessment | http://arxiv.org/pdf/1503.03355v1.pdf | author:Evangelos E. Papalexakis category:stat.ML cs.LG cs.NA stat.AP published:2015-03-11 summary:A popular tool for unsupervised modelling and mining multi-aspect data istensor decomposition. In an exploratory setting, where and no labels or groundtruth are available how can we automatically decide how many components toextract? How can we assess the quality of our results, so that a domain expertcan factor this quality measure in the interpretation of our results? In thispaper, we introduce AutoTen, a novel automatic unsupervised tensor miningalgorithm with minimal user intervention, which leverages and improves uponheuristics that assess the result quality. We extensively evaluate AutoTen'sperformance on synthetic data, outperforming existing baselines on this veryhard problem. Finally, we apply AutoTen on a variety of real datasets,providing insights and discoveries. We view this work as a step towards a fullyautomated, unsupervised tensor mining tool that can be easily adopted bypractitioners in academia and industry.
arxiv-9600-274 | Representative Selection for Big Data via Sparse Graph and Geodesic Grassmann Manifold Distance | http://arxiv.org/pdf/1405.1681v2.pdf | author:Chinh Dang, Hayder Radha category:cs.CV published:2014-05-07 summary:This paper addresses the problem of identifying a very small subset of datapoints that belong to a significantly larger massive dataset (i.e., Big Data).The small number of selected data points must adequately represent andfaithfully characterize the massive Big Data. Such identification process isknown as representative selection [19]. We propose a novel representativeselection framework by generating an l1 norm sparse graph for a given Big-Datadataset. The Big Data is partitioned recursively into clusters using a spectralclustering algorithm on the generated sparse graph. We consider each cluster asone point in a Grassmann manifold, and measure the geodesic distance amongthese points. The distances are further analyzed using a min-max algorithm [1]to extract an optimal subset of clusters. Finally, by considering a sparsesubgraph of each selected cluster, we detect a representative using principalcomponent centrality [11]. We refer to the proposed representative selectionframework as a Sparse Graph and Grassmann Manifold (SGGM) based approach. Tovalidate the proposed SGGM framework, we apply it onto the problem of videosummarization where only few video frames, known as key frames, are selectedamong a much longer video sequence. A comparison of the results obtained by theproposed algorithm with the ground truth, which is agreed by multiple humanjudges, and with some state-of-the-art methods clearly indicates the viabilityof the SGGM framework.
arxiv-9600-275 | A Novel Hybrid CNN-AIS Visual Pattern Recognition Engine | http://arxiv.org/pdf/1503.03270v1.pdf | author:Vandna Bhalla, Santanu Chaudhury, Arihant Jain category:cs.CV published:2015-03-11 summary:Machine learning methods are used today for most recognition problems.Convolutional Neural Networks (CNN) have time and again proved successful formany image processing tasks primarily for their architecture. In this paper wepropose to apply CNN to small data sets like for example, personal albums orother similar environs where the size of training dataset is a limitation,within the framework of a proposed hybrid CNN-AIS model. We use ArtificialImmune System Principles to enhance small size of training data set. A layer ofClonal Selection is added to the local filtering and max pooling of CNNArchitecture. The proposed Architecture is evaluated using the standard MNISTdataset by limiting the data size and also with a small personal data samplebelonging to two different classes. Experimental results show that the proposedhybrid CNN-AIS based recognition engine works well when the size of trainingdata is limited in size
arxiv-9600-276 | Convolutional Neural Network Architectures for Matching Natural Language Sentences | http://arxiv.org/pdf/1503.03244v1.pdf | author:Baotian Hu, Zhengdong Lu, Hang Li, Qingcai Chen category:cs.CL cs.LG cs.NE published:2015-03-11 summary:Semantic matching is of central importance to many natural language tasks\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs toadequately model the internal structures of language objects and theinteraction between them. As a step toward this goal, we propose convolutionalneural network models for matching two sentences, by adapting the convolutionalstrategy in vision and speech. The proposed models not only nicely representthe hierarchical structures of sentences with their layer-by-layer compositionand pooling, but also capture the rich matching patterns at different levels.Our models are rather generic, requiring no prior knowledge on language, andcan hence be applied to matching tasks of different nature and in differentlanguages. The empirical study on a variety of matching tasks demonstrates theefficacy of the proposed model on a variety of matching tasks and itssuperiority to competitor models.
arxiv-9600-277 | Scalable Discovery of Time-Series Shapelets | http://arxiv.org/pdf/1503.03238v1.pdf | author:Josif Grabocka, Martin Wistuba, Lars Schmidt-Thieme category:cs.LG published:2015-03-11 summary:Time-series classification is an important problem for the data miningcommunity due to the wide range of application domains involving time-seriesdata. A recent paradigm, called shapelets, represents patterns that are highlypredictive for the target variable. Shapelets are discovered by measuring theprediction accuracy of a set of potential (shapelet) candidates. The candidatestypically consist of all the segments of a dataset, therefore, the discovery ofshapelets is computationally expensive. This paper proposes a novel method thatavoids measuring the prediction accuracy of similar candidates in Euclideandistance space, through an online clustering pruning technique. In addition,our algorithm incorporates a supervised shapelet selection that filters outonly those candidates that improve classification accuracy. Empirical evidenceon 45 datasets from the UCR collection demonstrate that our method is 3-4orders of magnitudes faster than the fastest existing shapelet-discoverymethod, while providing better prediction accuracy.
arxiv-9600-278 | Adaptive-Rate Sparse Signal Reconstruction With Application in Compressive Background Subtraction | http://arxiv.org/pdf/1503.03231v1.pdf | author:Joao F. C. Mota, Nikos Deligiannis, Aswin C. Sankaranarayanan, Volkan Cevher, Miguel R. D. Rodrigues category:math.OC cs.CV cs.IT math.IT stat.ML published:2015-03-11 summary:We propose and analyze an online algorithm for reconstructing a sequence ofsignals from a limited number of linear measurements. The signals are assumedsparse, with unknown support, and evolve over time according to a genericnonlinear dynamical model. Our algorithm, based on recent theoretical resultsfor $\ell_1$-$\ell_1$ minimization, is recursive and computes the number ofmeasurements to be taken at each time on-the-fly. As an example, we apply thealgorithm to compressive video background subtraction, a problem that can bestated as follows: given a set of measurements of a sequence of images with astatic background, simultaneously reconstruct each image while separating itsforeground from the background. The performance of our method is illustrated onsequences of real images: we observe that it allows a dramatic reduction in thenumber of measurements with respect to state-of-the-art compressive backgroundsubtraction schemes.
arxiv-9600-279 | A Multi-Gene Genetic Programming Application for Predicting Students Failure at School | http://arxiv.org/pdf/1503.03211v1.pdf | author:J. O. Orove, N. E. Osegi, B. O. Eke category:cs.CY cs.AI cs.NE published:2015-03-11 summary:Several efforts to predict student failure rate (SFR) at school accuratelystill remains a core problem area faced by many in the educational sector. Theprocedure for forecasting SFR are rigid and most often times require datascaling or conversion into binary form such as is the case of the logisticmodel which may lead to lose of information and effect size attenuation. Also,the high number of factors, incomplete and unbalanced dataset, and black boxingissues as in Artificial Neural Networks and Fuzzy logic systems exposes theneed for more efficient tools. Currently the application of Genetic Programming(GP) holds great promises and has produced tremendous positive results indifferent sectors. In this regard, this study developed GPSFARPS, a softwareapplication to provide a robust solution to the prediction of SFR using anevolutionary algorithm known as multi-gene genetic programming. The approach isvalidated by feeding a testing data set to the evolved GP models. Resultobtained from GPSFARPS simulations show its unique ability to evolve a suitablefailure rate expression with a fast convergence at 30 generations from amaximum specified generation of 500. The multi-gene system was also able tominimize the evolved model expression and accurately predict student failurerate using a subset of the original expression
arxiv-9600-280 | Polarization Measurement of High Dimensional Social Media Messages With Support Vector Machine Algorithm Using Mapreduce | http://arxiv.org/pdf/1410.2686v2.pdf | author:Ferhat Özgür Çatak category:cs.LG cs.CL published:2014-10-10 summary:In this article, we propose a new Support Vector Machine (SVM) trainingalgorithm based on distributed MapReduce technique. In literature, there are alots of research that shows us SVM has highest generalization property amongclassification algorithms used in machine learning area. Also, SVM classifiermodel is not affected by correlations of the features. But SVM uses quadraticoptimization techniques in its training phase. The SVM algorithm is formulatedas quadratic optimization problem. Quadratic optimization problem has $O(m^3)$time and $O(m^2)$ space complexity, where m is the training set size. Thecomputation time of SVM training is quadratic in the number of traininginstances. In this reason, SVM is not a suitable classification algorithm forlarge scale dataset classification. To solve this training problem we developeda new distributed MapReduce method developed. Accordingly, (i) SVM algorithm istrained in distributed dataset individually; (ii) then merge all supportvectors of classifier model in every trained node; and (iii) iterate these twosteps until the classifier model converges to the optimal classifier function.In the implementation phase, large scale social media dataset is presented inTFxIDF matrix. The matrix is used for sentiment analysis to get polarizationvalue. Two and three class models are created for classification method.Confusion matrices of each classification model are presented in tables. Socialmedia messages corpus consists of 108 public and 66 private universitiesmessages in Turkey. Twitter is used for source of corpus. Twitter user messagesare collected using Twitter Streaming API. Results are shown in graphics andtables.
arxiv-9600-281 | Benchmarking NLopt and state-of-art algorithms for Continuous Global Optimization via Hybrid IACO$_\mathbb{R}$ | http://arxiv.org/pdf/1503.03175v1.pdf | author:Udit Kumar, Sumit Soman, Jayadeva category:cs.NE 80M50 G.1.6 published:2015-03-11 summary:This paper presents a comparative analysis of the performance of theIncremental Ant Colony algorithm for continuous optimization($IACO_\mathbb{R}$), with different algorithms provided in the NLopt library.The key objective is to understand how the various algorithms in the NLoptlibrary perform in combination with the Multi Trajectory Local Search (Mtsls1)technique. A hybrid approach has been introduced in the local search strategyby the use of a parameter which allows for probabilistic selection betweenMtsls1 and a NLopt algorithm. In case of stagnation, the algorithm switch ismade based on the algorithm being used in the previous iteration. The paperpresents an exhaustive comparison on the performance of these approaches onSoft Computing (SOCO) and Congress on Evolutionary Computation (CEC) 2014benchmarks. For both benchmarks, we conclude that the best performing algorithmis a hybrid variant of Mtsls1 with BFGS for local search.
arxiv-9600-282 | Learning Classifiers from Synthetic Data Using a Multichannel Autoencoder | http://arxiv.org/pdf/1503.03163v1.pdf | author:Xi Zhang, Yanwei Fu, Andi Zang, Leonid Sigal, Gady Agam category:cs.CV cs.LG published:2015-03-11 summary:We propose a method for using synthetic data to help learning classifiers.Synthetic data, even is generated based on real data, normally results in ashift from the distribution of real data in feature space. To bridge the gapbetween the real and synthetic data, and jointly learn from synthetic and realdata, this paper proposes a Multichannel Autoencoder(MCAE). We show that bysuing MCAE, it is possible to learn a better feature representation forclassification. To evaluate the proposed approach, we conduct experiments ontwo types of datasets. Experimental results on two datasets validate theefficiency of our MCAE model and our methodology of generating synthetic data.
arxiv-9600-283 | A Neurodynamical System for finding a Minimal VC Dimension Classifier | http://arxiv.org/pdf/1503.03148v1.pdf | author:Jayadeva, Sumit Soman, Amit Bhaya category:cs.LG stat.ML 70G660, 68T05 published:2015-03-11 summary:The recently proposed Minimal Complexity Machine (MCM) finds a hyperplaneclassifier by minimizing an exact bound on the Vapnik-Chervonenkis (VC)dimension. The VC dimension measures the capacity of a learning machine, and asmaller VC dimension leads to improved generalization. On many benchmarkdatasets, the MCM generalizes better than SVMs and uses far fewer supportvectors than the number used by SVMs. In this paper, we describe a neuralnetwork based on a linear dynamical system, that converges to the MCM solution.The proposed MCM dynamical system is conducive to an analogue circuitimplementation on a chip or simulation using Ordinary Differential Equation(ODE) solvers. Numerical experiments on benchmark datasets from the UCIrepository show that the proposed approach is scalable and accurate, as weobtain improved accuracies and fewer number of support vectors (upto 74.3%reduction) with the MCM dynamical system.
arxiv-9600-284 | L_1-regularized Boltzmann machine learning using majorizer minimization | http://arxiv.org/pdf/1503.03132v1.pdf | author:Masayuki Ohzeki category:stat.ML cs.LG published:2015-03-11 summary:We propose an inference method to estimate sparse interactions and biasesaccording to Boltzmann machine learning. The basis of this method is $L_1$regularization, which is often used in compressed sensing, a technique forreconstructing sparse input signals from undersampled outputs. $L_1$regularization impedes the simple application of the gradient method, whichoptimizes the cost function that leads to accurate estimations, owing to thecost function's lack of smoothness. In this study, we utilize the majorizerminimization method, which is a well-known technique implemented inoptimization problems, to avoid the non-smoothness of the cost function. Byusing the majorizer minimization method, we elucidate essentially relevantbiases and interactions from given data with seemingly strongly-correlatedcomponents.
arxiv-9600-285 | Single Image Super Resolution via Manifold Approximation | http://arxiv.org/pdf/1410.3349v2.pdf | author:Chinh Dang, Hayder Radha category:cs.CV published:2014-10-13 summary:Image super-resolution remains an important research topic to overcome thelimitations of physical acquisition systems, and to support the development ofhigh resolution displays. Previous example-based super-resolution approachesmainly focus on analyzing the co-occurrence properties of low resolution andhigh-resolution patches. Recently, we proposed a novel single imagesuper-resolution approach based on linear manifold approximation of thehigh-resolution image-patch space [1]. The image super-resolution problem isthen formulated as an optimization problem of searching for the best matchedhigh resolution patch in the manifold for a given low-resolution patch. Wedeveloped a novel technique based on the l1 norm sparse graph to learn a set oflow dimensional affine spaces or tangent subspaces of the high-resolution patchmanifold. The optimization problem is then solved based on the learned set oftangent subspaces. In this paper, we build on our recent work as follows.First, we consider and analyze each tangent subspace as one point in aGrassmann manifold, which helps to compute geodesic pairwise distances amongthese tangent subspaces. Second, we develop a min-max algorithm to select anoptimal subset of tangent subspaces. This optimal subset reduces thecomputational cost while still preserving the quality of the reconstructedhigh-resolution image. Third, and to further achieve lower computationalcomplexity, we perform hierarchical clustering on the optimal subset based onGrassmann manifold distances. Finally, we analytically prove the validity ofthe proposed Grassmann-distance based clustering. A comparison of the obtainedresults with other state-of-the-art methods clearly indicates the viability ofthe new proposed framework.
arxiv-9600-286 | Technical Analysis on Financial Forecasting | http://arxiv.org/pdf/1503.03011v1.pdf | author:S. Gopal Krishna Patro, Pragyan Parimita Sahoo, Ipsita Panda, Kishore Kumar Sahu category:cs.NE published:2015-03-10 summary:Financial forecasting is an estimation of future financial outcomes for acompany, industry, country using historical internal accounting and sales data.We may predict the future outcome of BSE_SENSEX practically by some softcomputing techniques and can also optimized using PSO (Particle SwarmOptimization), EA (Evolutionary Algorithm) or DEA (Differential EvolutionaryAlgorithm) etc. PSO is a biologically inspired computational search &optimization method developed in 1995 by Dr. Eberhart and Dr. Kennedy based onthe social behaviors of fish schooling or birds flocking. PSO is a promisingmethod to train Artificial Neural Network (ANN). It is easy to implement thenGenetic Algorithm except few parameters are adjusted. PSO is a random & patternsearch technique based on populating of particle. In PSO, the particles arehaving some position and velocity in the search space. Two terms are used inPSO one is Local Best and another one is Global Best. To optimize problems thatare like Irregular, Noisy, Change over time, Static etc. PSO uses a classicoptimization method such as Gradient Decent & Quasi-Newton Methods. Theobservation and review of few related studies in the last few years, focusingon function of PSO, modification of PSO and operation that have implementedusing PSO like function optimization, ANN Training & Fuzzy Control etc.Differential Evolution is an efficient EA technique for optimization ofnumerical problems, financial problems etc. PSO technique is introduced due tothe swarming behavior of animals which is the collective behavior of similarsize that aggregates together.
arxiv-9600-287 | Post-Regularization Confidence Bands for High Dimensional Nonparametric Models with Local Sparsity | http://arxiv.org/pdf/1503.02978v1.pdf | author:Junwei Lu, Mladen Kolar, Han Liu category:stat.ML math.ST stat.TH published:2015-03-10 summary:We propose a novel high dimensional nonparametric model named ATLAS whichnaturally generlizes the sparse additive model. Given a covariate of interest$X_j$, the ATLAS model assumes the mean function can be locally approximated bya sparse additive function whose sparsity pattern may vary from the globalperspective. We propose to infer the marginal influence function $f_j^*(z) =\mathbb{E}[f(X_1,\ldots, X_d) \mid X_j = z]$ using a new kernel-sieve approachthat combines the local kernel regression with the B-spline basisapproximation. We prove the rate of convergence for estimating $f_j^*$ underthe supremum norm. We also propose two types of confidence bands for $f_j^*$and illustrate their statistical-comptuational tradeoffs. Thorough numericalresults on both synthetic data and real-world genomic data are provided todemonstrate the efficacy of the theory.
arxiv-9600-288 | Robust recovery of complex exponential signals from random Gaussian projections via low rank Hankel matrix reconstruction | http://arxiv.org/pdf/1503.02893v1.pdf | author:Jian-Feng Cai, Xiaobo Qu, Weiyu Xu, Gui-Bo Ye category:cs.IT math.IT math.NA math.OC stat.ML published:2015-03-10 summary:This paper explores robust recovery of a superposition of $R$ distinctcomplex exponential functions from a few random Gaussian projections. We assumethat the signal of interest is of $2N-1$ dimensional and $R<<2N-1$. Thisframework covers a large class of signals arising from real applications inbiology, automation, imaging science, etc. To reconstruct such a signal, ouralgorithm is to seek a low-rank Hankel matrix of the signal by minimizing itsnuclear norm subject to the consistency on the sampled data. Our theoreticalresults show that a robust recovery is possible as long as the number ofprojections exceeds $O(R\ln^2N)$. No incoherence or separation condition isrequired in our proof. Our method can be applied to spectral compressed sensingwhere the signal of interest is a superposition of $R$ complex sinusoids.Compared to existing results, our result here does not need any separationcondition on the frequencies, while achieving better or comparable bounds onthe number of measurements. Furthermore, our method provides theoreticalguidance on how many samples are required in the state-of-the-art non-uniformsampling in NMR spectroscopy. The performance of our algorithm is furtherdemonstrated by numerical experiments.
arxiv-9600-289 | Multi-dimensional sparse structured signal approximation using split Bregman iterations | http://arxiv.org/pdf/1303.5197v3.pdf | author:Yoann Isaac, Quentin Barthélemy, Jamal Atif, Cédric Gouy-Pailler, Michèle Sebag category:cs.DS stat.ML published:2013-03-21 summary:The paper focuses on the sparse approximation of signals using overcompleterepresentations, such that it preserves the (prior) structure ofmulti-dimensional signals. The underlying optimization problem is tackled usinga multi-dimensional split Bregman optimization approach. An extensive empiricalevaluation shows how the proposed approach compares to the state of the artdepending on the signal features.
arxiv-9600-290 | Single stream parallelization of generalized LSTM-like RNNs on a GPU | http://arxiv.org/pdf/1503.02852v1.pdf | author:Kyuyeon Hwang, Wonyong Sung category:cs.NE cs.LG published:2015-03-10 summary:Recurrent neural networks (RNNs) have shown outstanding performance onprocessing sequence data. However, they suffer from long training time, whichdemands parallel implementations of the training procedure. Parallelization ofthe training algorithms for RNNs are very challenging because internalrecurrent paths form dependencies between two different time frames. In thispaper, we first propose a generalized graph-based RNN structure that covers themost popular long short-term memory (LSTM) network. Then, we present aparallelization approach that automatically explores parallelisms of arbitraryRNNs by analyzing the graph structure. The experimental results show that theproposed approach shows great speed-up even with a single training stream, andfurther accelerates the training when combined with multiple parallel trainingstreams.
arxiv-9600-291 | Parallel Statistical Multi-resolution Estimation | http://arxiv.org/pdf/1503.03492v1.pdf | author:Jan Lebert, Lutz Künneke, Johannes Hagemann, Stephan C. Kramer category:cs.CV published:2015-03-10 summary:We discuss several strategies to implement Dykstra's projection algorithm onNVIDIA's compute unified device architecture (CUDA). Dykstra's algorithm is thecentral step in and the computationally most expensive part of statisticalmulti-resolution methods. It projects a given vector onto the intersection ofconvex sets. Compared with a CPU implementation our CUDA implementation is oneorder of magnitude faster. For a further speed up and to reduce memoryconsumption we have developed a new variant, which we call incomplete Dykstra'salgorithm. Implemented in CUDA it is one order of magnitude faster than theCUDA implementation of the standard Dykstra algorithm. As sample application wediscuss using the incomplete Dykstra's algorithm as preprocessor for therecently developed super-resolution optical fluctuation imaging (SOFI) method(Dertinger et al. 2009). We show that statistical multi-resolution estimationcan enhance the resolution improvement of the plain SOFI algorithm just as theFourier-reweighting of SOFI. The results are compared in terms of their powerspectrum and their Fourier ring correlation (Saxton and Baumeister 1982). TheFourier ring correlation indicates that the resolution for typical second orderSOFI images can be improved by about 30 per cent. Our results show that acareful parallelization of Dykstra's algorithm enables its use in large-scalestatistical multi-resolution analyses.
arxiv-9600-292 | Minimax Optimal Rates of Estimation in High Dimensional Additive Models: Universal Phase Transition | http://arxiv.org/pdf/1503.02817v1.pdf | author:Ming Yuan, Ding-Xuan Zhou category:math.ST cs.IT math.IT stat.ML stat.TH published:2015-03-10 summary:We establish minimax optimal rates of convergence for estimation in a highdimensional additive model assuming that it is approximately sparse. Ourresults reveal an interesting phase transition behavior universal to this classof high dimensional problems. In the {\it sparse regime} when the componentsare sufficiently smooth or the dimensionality is sufficiently large, theoptimal rates are identical to those for high dimensional linear regression,and therefore there is no additional cost to entertain a nonparametric model.Otherwise, in the so-called {\it smooth regime}, the rates coincide with theoptimal rates for estimating a univariate function, and therefore they areimmune to the "curse of dimensionality".
arxiv-9600-293 | Short Text Hashing Improved by Integrating Multi-Granularity Topics and Tags | http://arxiv.org/pdf/1503.02801v1.pdf | author:Jiaming Xu, Bo Xu, Guanhua Tian, Jun Zhao, Fangyuan Wang, Hongwei Hao category:cs.IR cs.CL published:2015-03-10 summary:Due to computational and storage efficiencies of compact binary codes,hashing has been widely used for large-scale similarity search. Unfortunately,many existing hashing methods based on observed keyword features are noteffective for short texts due to the sparseness and shortness. Recently, someresearchers try to utilize latent topics of certain granularity to preservesemantic similarity in hash codes beyond keyword matching. However, topics ofcertain granularity are not adequate to represent the intrinsic semanticinformation. In this paper, we present a novel unified approach for short textHashing using Multi-granularity Topics and Tags, dubbed HMTT. In particular, wepropose a selection method to choose the optimal multi-granularity topicsdepending on the type of dataset, and design two distinct hashing strategies toincorporate multi-granularity topics. We also propose a simple and effectivemethod to exploit tags to enhance the similarity of related texts. We carry outextensive experiments on one short text dataset as well as on one normal textdataset. The results demonstrate that our approach is effective andsignificantly outperforms baselines on several evaluation metrics.
arxiv-9600-294 | Learning Mixtures of Gaussians in High Dimensions | http://arxiv.org/pdf/1503.00424v2.pdf | author:Rong Ge, Qingqing Huang, Sham M. Kakade category:cs.LG published:2015-03-02 summary:Efficiently learning mixture of Gaussians is a fundamental problem instatistics and learning theory. Given samples coming from a random one out of kGaussian distributions in Rn, the learning problem asks to estimate the meansand the covariance matrices of these Gaussians. This learning problem arises inmany areas ranging from the natural sciences to the social sciences, and hasalso found many machine learning applications. Unfortunately, learning mixtureof Gaussians is an information theoretically hard problem: in order to learnthe parameters up to a reasonable accuracy, the number of samples required isexponential in the number of Gaussian components in the worst case. In thiswork, we show that provided we are in high enough dimensions, the class ofGaussian mixtures is learnable in its most general form under a smoothedanalysis framework, where the parameters are randomly perturbed from anadversarial starting point. In particular, given samples from a mixture ofGaussians with randomly perturbed parameters, when n > {\Omega}(k^2), we givean algorithm that learns the parameters with polynomial running time and usingpolynomial number of samples. The central algorithmic ideas consist of new waysto decompose the moment tensor of the Gaussian mixture by exploiting itsstructural properties. The symmetries of this tensor are derived from thecombinatorial structure of higher order moments of Gaussian distributions(sometimes referred to as Isserlis' theorem or Wick's theorem). We also developnew tools for bounding smallest singular values of structured random matrices,which could be useful in other smoothed analysis settings.
arxiv-9600-295 | Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMC | http://arxiv.org/pdf/1503.01596v2.pdf | author:Sungjin Ahn, Anoop Korattikara, Nathan Liu, Suju Rajan, Max Welling category:cs.LG stat.ML published:2015-03-05 summary:Despite having various attractive qualities such as high prediction accuracyand the ability to quantify uncertainty and avoid over-fitting, Bayesian MatrixFactorization has not been widely adopted because of the prohibitive cost ofinference. In this paper, we propose a scalable distributed Bayesian matrixfactorization algorithm using stochastic gradient MCMC. Our algorithm, based onDistributed Stochastic Gradient Langevin Dynamics, can not only match theprediction accuracy of standard MCMC methods like Gibbs sampling, but at thesame time is as fast and simple as stochastic gradient descent. In ourexperiments, we show that our algorithm can achieve the same level ofprediction accuracy as Gibbs sampling an order of magnitude faster. We alsoshow that our method reduces the prediction error as fast as distributedstochastic gradient descent, achieving a 4.1% improvement in RMSE for theNetflix dataset and an 1.8% for the Yahoo music dataset.
arxiv-9600-296 | Bayesian Model-Averaged Regularization for Gaussian Graphical Models | http://arxiv.org/pdf/1503.02698v1.pdf | author:Zhe Liu category:stat.ML published:2015-03-09 summary:Graphical models are an intuitive way of exploring and modeling therelationships between variables. The graphical lasso has now become as a usefultool to estimate high-dimensional Gaussian graphical models, but its practicalapplications suffer from the problem of choosing regularization parameters in adata-dependent way. In this paper, we propose and analyze a model-averagedmethod for estimating sparse inverse covariance matrices for Gaussian graphicalmodels. We consider the graphical lasso regularization path as the model spacefor Bayesian model averaging and use Markov chain Monte Carlo techniques forthe regularization path point selection. Numerical performance of our method isinvestigated using both simulated and real datasets, in comparison with somestate-of-art model selection procedures.
arxiv-9600-297 | Modeling State-Conditional Observation Distribution using Weighted Stereo Samples for Factorial Speech Processing Models | http://arxiv.org/pdf/1503.02578v1.pdf | author:Mahdi Khademian, Mohammad Mehdi Homayounpour category:cs.LG cs.AI cs.SD published:2015-03-09 summary:This paper investigates the role of factorial speech processing models innoise-robust automatic speech recognition tasks. Factorial models can embednon-stationary noise models using Markov chains as one of its source chain. Thepaper proposes a modeling scheme for modeling state-conditional observationdistribution of factorial models based on weighted stereo samples. This schemeis an extension to previous single pass retraining for ideal model compensationand here we used it to construct ideal state-conditional observationdistributions. Experiments of this paper over the set A of the Aurora 2 datasetshows that by considering noise models with multiple states, system performancecan be improved especially in low SNR conditions up to 4% absolute wordrecognition performance. In addition to its power in accurate representation ofstate-conditional observation distribution, it has an important advantage overprevious methods by providing the opportunity to independently select featurespaces for both source and corrupted features. This opens a new window forseeking better feature spaces appropriate for noise-robust tasks independentfrom clean speech feature space.
arxiv-9600-298 | Sublinear-Time Approximate MCMC Transitions for Probabilistic Programs | http://arxiv.org/pdf/1411.1690v2.pdf | author:Yutian Chen, Vikash Mansinghka, Zoubin Ghahramani category:stat.ML published:2014-11-06 summary:Probabilistic programming languages can simplify the development of machinelearning techniques, but only if inference is sufficiently scalable.Unfortunately, Bayesian parameter estimation for highly coupled models such asregressions and state-space models still scales poorly; each MCMC transitiontakes linear time in the number of observations. This paper describes asublinear-time algorithm for making Metropolis-Hastings (MH) updates to latentvariables in probabilistic programs. The approach generalizes recentlyintroduced approximate MH techniques: instead of subsampling data items assumedto be independent, it subsamples edges in a dynamically constructed graphicalmodel. It thus applies to a broader class of problems and interoperates withother general-purpose inference techniques. Empirical results, includingconfirmation of sublinear per-transition scaling, are presented for Bayesianlogistic regression, nonlinear classification via joint Dirichlet processmixtures, and parameter estimation for stochastic volatility models (with stateestimation via particle MCMC). All three applications use the sameimplementation, and each requires under 20 lines of probabilistic code.
arxiv-9600-299 | Distilling the Knowledge in a Neural Network | http://arxiv.org/pdf/1503.02531v1.pdf | author:Geoffrey Hinton, Oriol Vinyals, Jeff Dean category:stat.ML cs.LG cs.NE published:2015-03-09 summary:A very simple way to improve the performance of almost any machine learningalgorithm is to train many different models on the same data and then toaverage their predictions. Unfortunately, making predictions using a wholeensemble of models is cumbersome and may be too computationally expensive toallow deployment to a large number of users, especially if the individualmodels are large neural nets. Caruana and his collaborators have shown that itis possible to compress the knowledge in an ensemble into a single model whichis much easier to deploy and we develop this approach further using a differentcompression technique. We achieve some surprising results on MNIST and we showthat we can significantly improve the acoustic model of a heavily usedcommercial system by distilling the knowledge in an ensemble of models into asingle model. We also introduce a new type of ensemble composed of one or morefull models and many specialist models which learn to distinguish fine-grainedclasses that the full models confuse. Unlike a mixture of experts, thesespecialist models can be trained rapidly and in parallel.
arxiv-9600-300 | Brain Tumor Segmentation: A Comparative Analysis | http://arxiv.org/pdf/1503.02466v1.pdf | author:Muhammad Ali Qadar, Yan Zhaowen category:cs.CV published:2015-03-09 summary:Five different threshold segmentation based approaches have been reviewed andcompared over here to extract the tumor from set of brain images. This researchfocuses on the analysis of image segmentation methods, a comparison of fivesemi-automated methods have been undertaken for evaluating their relativeperformance in the segmentation of tumor. Consequently, results are compared onthe basis of quantitative and qualitative analysis of respective methods. Thepurpose of this study was to analytically identify the methods, most suitablefor application for a particular genre of problems. The results show that ofthe region growing segmentation performed better than rest in most cases.
