arxiv-5400-1 | Memory Capacity of Neural Networks using a Circulant Weight Matrix | http://arxiv.org/pdf/1403.3115v1.pdf | author:Vamsi Sashank Kotagiri category:cs.NE published:2014-03-12 summary:This paper presents results on the memory capacity of a generalized feedbackneural network using a circulant matrix. Children are capable of learning soonafter birth which indicates that the neural networks of the brain have priorlearnt capacity that is a consequence of the regular structures in the brain'sorganization. Motivated by this idea, we consider the capacity of circulantmatrices as weight matrices in a feedback network.
arxiv-5400-2 | Sparse Recovery with Linear and Nonlinear Observations: Dependent and Noisy Data | http://arxiv.org/pdf/1403.3109v1.pdf | author:Cem Aksoylar, Venkatesh Saligrama category:cs.IT cs.LG math.IT math.ST stat.TH published:2014-03-12 summary:We formulate sparse support recovery as a salient set identification problemand use information-theoretic analyses to characterize the recovery performanceand sample complexity. We consider a very general model where we are notrestricted to linear models or specific distributions. We state non-asymptoticbounds on recovery probability and a tight mutual information formula forsample complexity. We evaluate our bounds for applications such as sparselinear regression and explicitly characterize effects of correlation or noisyfeatures on recovery performance. We show improvements upon previous work andidentify gaps between the performance of recovery algorithms and fundamentalinformation.
arxiv-5400-3 | Evaluation of Image Segmentation and Filtering With ANN in the Papaya Leaf | http://arxiv.org/pdf/1403.3057v1.pdf | author:Maicon A. Sartin, Alexandre C. R. da Silva category:cs.NE cs.CV published:2014-03-12 summary:Precision agriculture is area with lack of cheap technology. The refinementof the production system brings large advantages to the producer and the use ofimages makes the monitoring a more cheap methodology. Macronutrients monitoringcan to determine the health and vulnerability of the plant in specific stages.In this paper is analyzed the method based on computational intelligence towork with image segmentation in the identification of symptoms of plantnutrient deficiency. Artificial neural networks are evaluated for imagesegmentation and filtering, several variations of parameters and insertionimpulsive noise were evaluated too. Satisfactory results are achieved withartificial neural for segmentation same with high noise levels.
arxiv-5400-4 | Efficient Legendre moment computation for grey level images | http://arxiv.org/pdf/1403.3022v1.pdf | author:Guanyu Yang, Huazhong Shu, Christine Toumoulin, Guo-Niu Han, Limin M. Luo category:cs.CV math.NA published:2014-03-12 summary:Legendre orthogonal moments have been widely used in the field of imageanalysis. Because their computation by a direct method is very time expensive,recent efforts have been devoted to the reduction of computational complexity.Nevertheless, the existing algorithms are mainly focused on binary images. Wepropose here a new fast method for computing the Legendre moments, which is notonly suitable for binary images but also for grey levels. We first set up therecurrence formula of one-dimensional (1D) Legendre moments by using therecursive property of Legendre polynomials. As a result, the 1D Legendremoments of order p, Lp = Lp(0), can be expressed as a linear combination ofLp-1(1) and Lp-2(0). Based on this relationship, the 1D Legendre moments Lp(0)is thus obtained from the array of L1(a) and L0(a) where a is an integer numberless than p. To further decrease the computation complexity, an algorithm, inwhich no multiplication is required, is used to compute these quantities. Themethod is then extended to the calculation of the two-dimensional Legendremoments Lpq. We show that the proposed method is more efficient than the directmethod.
arxiv-5400-5 | Image reconstruction from limited range projections using orthogonal moments | http://arxiv.org/pdf/1403.3021v1.pdf | author:Huazhong Shu, Jian Zhou, Guo-Niu Han, Limin M. Luo, Jean-Louis Coatrieux category:cs.CV math.NA published:2014-03-12 summary:A set of orthonormal polynomials is proposed for image reconstruction fromprojection data. The relationship between the projection moments and imagemoments is discussed in detail, and some interesting properties aredemonstrated. Simulation results are provided to validate the method and tocompare its performance with previous works.
arxiv-5400-6 | 3D Well-composed Polyhedral Complexes | http://arxiv.org/pdf/1403.2980v1.pdf | author:Rocio Gonzalez-Diaz, Maria-Jose Jimenez, Belen Medrano category:cs.CV published:2014-03-12 summary:A binary three-dimensional (3D) image $I$ is well-composed if the boundarysurface of its continuous analog is a 2D manifold. Since 3D images are notoften well-composed, there are several voxel-based methods ("repairing"algorithms) for turning them into well-composed ones but these methods eitherdo not guarantee the topological equivalence between the original image and itscorresponding well-composed one or involve sub-sampling the whole image. In this paper, we present a method to locally "repair" the cubical complex$Q(I)$ (embedded in $\mathbb{R}^3$) associated to $I$ to obtain a polyhedralcomplex $P(I)$ homotopy equivalent to $Q(I)$ such that the boundary of everyconnected component of $P(I)$ is a 2D manifold. The reparation is performed viaa new codification system for $P(I)$ under the form of a 3D grayscale imagethat allows an efficient access to cells and their faces.
arxiv-5400-7 | Cancer Prognosis Prediction Using Balanced Stratified Sampling | http://arxiv.org/pdf/1403.2950v1.pdf | author:J S Saleema, N Bhagawathi, S Monica, P Deepa Shenoy, K R Venugopal, L M Patnaik category:cs.LG 62D05 I.2.6; H.2.8 published:2014-03-12 summary:High accuracy in cancer prediction is important to improve the quality of thetreatment and to improve the rate of survivability of patients. As the datavolume is increasing rapidly in the healthcare research, the analyticalchallenge exists in double. The use of effective sampling technique inclassification algorithms always yields good prediction accuracy. The SEERpublic use cancer database provides various prominent class labels forprognosis prediction. The main objective of this paper is to find the effect ofsampling techniques in classifying the prognosis variable and propose an idealsampling method based on the outcome of the experimentation. In the first phaseof this work the traditional random sampling and stratified sampling techniqueshave been used. At the next level the balanced stratified sampling withvariations as per the choice of the prognosis class labels have been tested.Much of the initial time has been focused on performing the pre_processing ofthe SEER data set. The classification model for experimentation has been builtusing the breast cancer, respiratory cancer and mixed cancer data sets withthree traditional classifiers namely Decision Tree, Naive Bayes and K-NearestNeighbor. The three prognosis factors survival, stage and metastasis have beenused as class labels for experimental comparisons. The results shows a steadyincrease in the prediction accuracy of balanced stratified model as the samplesize increases, but the traditional approach fluctuates before the optimumresults.
arxiv-5400-8 | Uav Route Planning For Maximum Target Coverage | http://arxiv.org/pdf/1403.2906v1.pdf | author:Murat Karakaya category:cs.RO cs.NE published:2014-03-12 summary:Utilization of Unmanned Aerial Vehicles (UAVs) in military and civiloperations is getting popular. One of the challenges in effectively taskingthese expensive vehicles is planning the flight routes to monitor the targets.In this work, we aim to develop an algorithm which produces routing plans for alimited number of UAVs to cover maximum number of targets considering theirflight range. The proposed solution for this practical optimization problem isdesigned by modifying the Max-Min Ant System (MMAS) algorithm. To evaluate thesuccess of the proposed method, an alternative approach, based on the NearestNeighbour (NN) heuristic, has been developed as well. The results showed thesuccess of the proposed MMAS method by increasing the number of covered targetscompared to the solution based on the NN heuristic.
arxiv-5400-9 | Indoor 3D Video Monitoring Using Multiple Kinect Depth-Cameras | http://arxiv.org/pdf/1403.2895v1.pdf | author:M. Martínez-Zarzuela, M. Pedraza-Hueso, F. J. Díaz-Pernas, D. González-Ortega, M. Antón-Rodríguez category:cs.CV published:2014-03-12 summary:This article describes the design and development of a system for remoteindoor 3D monitoring using an undetermined number of Microsoft(R) Kinectsensors. In the proposed client-server system, the Kinect cameras can beconnected to different computers, addressing this way the hardware limitationof one sensor per USB controller. The reason behind this limitation is the highbandwidth needed by the sensor, which becomes also an issue for the distributedsystem TCP/IP communications. Since traffic volume is too high, 3D data has tobe compressed before it can be sent over the network. The solution consists inselfcoding the Kinect data into RGB images and then using a standard multimediacodec to compress color maps. Information from different sources is collectedinto a central client computer, where point clouds are transformed toreconstruct the scene in 3D. An algorithm is proposed to merge the skeletonsdetected locally by each Kinect conveniently, so that monitoring of people isrobust to self and inter-user occlusions. Final skeletons are labeled andtrajectories of every joint can be saved for event reconstruction or furtheranalysis.
arxiv-5400-10 | UNLocBoX A matlab convex optimization toolbox using proximal splitting methods | http://arxiv.org/pdf/1402.0779v2.pdf | author:Nathanael Perraudin, David Shuman, Gilles Puy, Pierre Vandergheynst category:cs.LG stat.ML published:2014-02-04 summary:Nowadays the trend to solve optimization problems is to use specificalgorithms rather than very general ones. The UNLocBoX provides a generalframework allowing the user to design his own algorithms. To do so, theframework try to stay as close from the mathematical problem as possible. Moreprecisely, the UNLocBoX is a Matlab toolbox designed to solve convexoptimization problem of the form $$ \min_{x \in \mathcal{C}} \sum_{n=1}^Kf_n(x), $$ using proximal splitting techniques. It is mainly composed ofsolvers, proximal operators and demonstration files allowing the user toquickly implement a problem.
arxiv-5400-11 | A survey of dimensionality reduction techniques | http://arxiv.org/pdf/1403.2877v1.pdf | author:C. O. S. Sorzano, J. Vargas, A. Pascual Montano category:stat.ML cs.LG q-bio.QM published:2014-03-12 summary:Experimental life sciences like biology or chemistry have seen in the recentdecades an explosion of the data available from experiments. Laboratoryinstruments become more and more complex and report hundreds or thousandsmeasurements for a single experiment and therefore the statistical methods facechallenging tasks when dealing with such high dimensional data. However, muchof the data is highly redundant and can be efficiently brought down to a muchsmaller number of variables without a significant loss of information. Themathematical procedures making possible this reduction are calleddimensionality reduction techniques; they have widely been developed by fieldslike Statistics or Machine Learning, and are currently a hot research topic. Inthis review we categorize the plethora of dimension reduction techniquesavailable and give the mathematical insight behind them.
arxiv-5400-12 | Shape-Based Plagiarism Detection for Flowchart Figures in Texts | http://arxiv.org/pdf/1403.2871v1.pdf | author:Senosy Arrish, Fadhil Noer Afif, Ahmadu Maidorawa, Naomie Salim category:cs.CV cs.IR published:2014-03-12 summary:Plagiarism detection is well known phenomenon in the academic arena. Copyingother people is considered as serious offence that needs to be checked. Thereare many plagiarism detection systems such as turn-it-in that has beendeveloped to provide this checks. Most, if not all, discard the figures andcharts before checking for plagiarism. Discarding the figures and chartsresults in look holes that people can take advantage. That means people canplagiarized figures and charts easily without the current plagiarism systemsdetecting it. There are very few papers which talks about flowcharts plagiarismdetection. Therefore, there is a need to develop a system that will detectplagiarism in figures and charts. This paper presents a method for detectingflow chart figure plagiarism based on shape-based image processing andmultimedia retrieval. The method managed to retrieve flowcharts with rankedsimilarity according to different matching sets.
arxiv-5400-13 | Application of Particle Swarm Optimization to Microwave Tapered Microstrip Lines | http://arxiv.org/pdf/1403.2842v1.pdf | author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE published:2014-03-12 summary:Application of metaheuristic algorithms has been of continued interest in thefield of electrical engineering because of their powerful features. In thiswork special design is done for a tapered transmission line used for matchingan arbitrary real load to a 50{\Omega} line. The problem at hand is to matchthis arbitrary load to 50 {\Omega} line using three section taperedtransmission line with impedances in decreasing order from the load. So theproblem becomes optimizing an equation with three unknowns with variousconditions. The optimized values are obtained using Particle SwarmOptimization. It can easily be shown that PSO is very strong in solving thiskind of multiobjective optimization problems.
arxiv-5400-14 | HPS: a hierarchical Persian stemming method | http://arxiv.org/pdf/1403.2837v1.pdf | author:Ayshe Rashidi, Mina Zolfy Lighvan category:cs.CL published:2014-03-12 summary:In this paper, a novel hierarchical Persian stemming approach based on thePart-Of-Speech of the word in a sentence is presented. The implemented stemmerincludes hash tables and several deterministic finite automata in its differentlevels of hierarchy for removing the prefixes and suffixes of the words. We hadtwo intentions in using hash tables in our method. The first one is that theDFA don't support some special words, so hash table can partly solve theaddressed problem. the second goal is to speed up the implemented stemmer withomitting the time that deterministic finite automata need. Because of thehierarchical organization, this method is fast and flexible enough. Ourexperiments on test sets from Hamshahri collection and security news (istna.ir)show that our method has the average accuracy of 95.37% which is even improvedin using the method on a test set with common topics.
arxiv-5400-15 | Polyceptron: A Polyhedral Learning Algorithm | http://arxiv.org/pdf/1107.1564v3.pdf | author:Naresh Manwani, P. S. Sastry category:cs.LG cs.NE published:2011-07-08 summary:In this paper we propose a new algorithm for learning polyhedral classifierswhich we call as Polyceptron. It is a Perception like algorithm which updatesthe parameters only when the current classifier misclassifies any trainingdata. We give both batch and online version of Polyceptron algorithm. Finallywe give experimental results to show the effectiveness of our approach.
arxiv-5400-16 | Markov properties for mixed graphs | http://arxiv.org/pdf/1109.5909v5.pdf | author:Kayvan Sadeghi, Steffen Lauritzen category:stat.OT math.ST stat.ML stat.TH published:2011-09-27 summary:In this paper, we unify the Markov theory of a variety of different types ofgraphs used in graphical Markov models by introducing the class of looplessmixed graphs, and show that all independence models induced by $m$-separationon such graphs are compositional graphoids. We focus in particular on thesubclass of ribbonless graphs which as special cases include undirected graphs,bidirected graphs, and directed acyclic graphs, as well as ancestral graphs andsummary graphs. We define maximality of such graphs as well as a pairwise and aglobal Markov property. We prove that the global and pairwise Markov propertiesof a maximal ribbonless graph are equivalent for any independence model that isa compositional graphoid.
arxiv-5400-17 | Counterfactual Estimation and Optimization of Click Metrics for Search Engines | http://arxiv.org/pdf/1403.1891v2.pdf | author:Lihong Li, Shunbao Chen, Jim Kleban, Ankur Gupta category:cs.LG cs.AI stat.AP stat.ML G.3; H.3.3 published:2014-03-07 summary:Optimizing an interactive system against a predefined online metric isparticularly challenging, when the metric is computed from user feedback suchas clicks and payments. The key challenge is the counterfactual nature: in thecase of Web search, any change to a component of the search engine may resultin a different search result page for the same query, but we normally cannotinfer reliably from search log how users would react to the new result page.Consequently, it appears impossible to accurately estimate online metrics thatdepend on user feedback, unless the new engine is run to serve users andcompared with a baseline in an A/B test. This approach, while valid andsuccessful, is unfortunately expensive and time-consuming. In this paper, wepropose to address this problem using causal inference techniques, under thecontextual-bandit framework. This approach effectively allows one to run(potentially infinitely) many A/B tests offline from search log, making itpossible to estimate and optimize online metrics quickly and inexpensively.Focusing on an important component in a commercial search engine, we show howthese ideas can be instantiated and applied, and obtain very promising resultsthat suggest the wide applicability of these techniques.
arxiv-5400-18 | Learning Deep Face Representation | http://arxiv.org/pdf/1403.2802v1.pdf | author:Haoqiang Fan, Zhimin Cao, Yuning Jiang, Qi Yin, Chinchilla Doudou category:cs.CV cs.LG published:2014-03-12 summary:Face representation is a crucial step of face recognition systems. An optimalface representation should be discriminative, robust, compact, and veryeasy-to-implement. While numerous hand-crafted and learning-basedrepresentations have been proposed, considerable room for improvement is stillpresent. In this paper, we present a very easy-to-implement deep learningframework for face representation. Our method bases on a new structure of deepnetwork (called Pyramid CNN). The proposed Pyramid CNN adopts agreedy-filter-and-down-sample operation, which enables the training procedureto be very fast and computation-efficient. In addition, the structure ofPyramid CNN can naturally incorporate feature sharing across multi-scale facerepresentations, increasing the discriminative ability of resultingrepresentation. Our basic network is capable of achieving high recognitionaccuracy ($85.8\%$ on LFW benchmark) with only 8 dimension representation. Whenextended to feature-sharing Pyramid CNN, our system achieves thestate-of-the-art performance ($97.3\%$) on LFW benchmark. We also introduce anew benchmark of realistic face images on social network and validate ourproposed representation has a good ability of generalization.
arxiv-5400-19 | The Bursty Dynamics of the Twitter Information Network | http://arxiv.org/pdf/1403.2732v1.pdf | author:Seth A. Myers, Jure Leskovec category:cs.SI physics.soc-ph stat.ML published:2014-03-11 summary:In online social media systems users are not only posting, consuming, andresharing content, but also creating new and destroying existing connections inthe underlying social network. While each of these two types of dynamics hasindividually been studied in the past, much less is known about the connectionbetween the two. How does user information posting and seeking behaviorinteract with the evolution of the underlying social network structure? Here, we study ways in which network structure reacts to users posting andsharing content. We examine the complete dynamics of the Twitter informationnetwork, where users post and reshare information while they also create anddestroy connections. We find that the dynamics of network structure can becharacterized by steady rates of change, interrupted by sudden bursts.Information diffusion in the form of cascades of post re-sharing often createssuch sudden bursts of new connections, which significantly change users' localnetwork structure. These bursts transform users' networks of followers tobecome structurally more cohesive as well as more homogenous in terms offollower interests. We also explore the effect of the information content onthe dynamics of the network and find evidence that the appearance of new topicsand real-world events can lead to significant changes in edge creations anddeletions. Lastly, we develop a model that quantifies the dynamics of thenetwork and the occurrence of these bursts as a function of the informationspreading through the network. The model can successfully predict whichinformation diffusion events will lead to bursts in network dynamics.
arxiv-5400-20 | Unsupervised Learning of Invariant Representations in Hierarchical Architectures | http://arxiv.org/pdf/1311.4158v5.pdf | author:Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea Tacchetti, Tomaso Poggio category:cs.CV cs.LG published:2013-11-17 summary:The present phase of Machine Learning is characterized by supervised learningalgorithms relying on large sets of labeled examples ($n \to \infty$). The nextphase is likely to focus on algorithms capable of learning from very fewlabeled examples ($n \to 1$), like humans seem able to do. We propose anapproach to this problem and describe the underlying theory, based on theunsupervised, automatic learning of a ``good'' representation for supervisedlearning, characterized by small sample complexity ($n$). We consider the caseof visual object recognition though the theory applies to other domains. Thestarting point is the conjecture, proved in specific cases, that imagerepresentations which are invariant to translations, scaling and othertransformations can considerably reduce the sample complexity of learning. Weprove that an invariant and unique (discriminative) signature can be computedfor each image patch, $I$, in terms of empirical distributions of thedot-products between $I$ and a set of templates stored during unsupervisedlearning. A module performing filtering and pooling, like the simple andcomplex cells described by Hubel and Wiesel, can compute such estimates.Hierarchical architectures consisting of this basic Hubel-Wiesel moduli inheritits properties of invariance, stability, and discriminability while capturingthe compositional organization of the visual world in terms of wholes andparts. The theory extends existing deep learning convolutional architecturesfor image and speech recognition. It also suggests that the main computationalgoal of the ventral stream of visual cortex is to provide a hierarchicalrepresentation of new objects/images which is invariant to transformations,stable, and discriminative for recognition---and that this representation maybe continuously learned in an unsupervised way during development and visualexperience.
arxiv-5400-21 | Flying Insect Classification with Inexpensive Sensors | http://arxiv.org/pdf/1403.2654v1.pdf | author:Yanping Chen, Adena Why, Gustavo Batista, Agenor Mafra-Neto, Eamonn Keogh category:cs.LG cs.CE 68T00 I.2.6 published:2014-03-11 summary:The ability to use inexpensive, noninvasive sensors to accurately classifyflying insects would have significant implications for entomological research,and allow for the development of many useful applications in vector control forboth medical and agricultural entomology. Given this, the last sixty years haveseen many research efforts on this task. To date, however, none of thisresearch has had a lasting impact. In this work, we explain this lack ofprogress. We attribute the stagnation on this problem to several factors,including the use of acoustic sensing devices, the over-reliance on the singlefeature of wingbeat frequency, and the attempts to learn complex models withrelatively little data. In contrast, we show that pseudo-acoustic opticalsensors can produce vastly superior data, that we can exploit additionalfeatures, both intrinsic and extrinsic to the insect's flight behavior, andthat a Bayesian classification approach allows us to efficiently learnclassification models that are very robust to over-fitting. We demonstrate ourfindings with large scale experiments that dwarf all previous works combined,as measured by the number of insects and the number of species considered.
arxiv-5400-22 | Transfer Learning across Networks for Collective Classification | http://arxiv.org/pdf/1403.2484v1.pdf | author:Meng Fang, Jie Yin, Xingquan Zhu category:cs.LG cs.SI published:2014-03-11 summary:This paper addresses the problem of transferring useful knowledge from asource network to predict node labels in a newly formed target network. Whileexisting transfer learning research has primarily focused on vector-based data,in which the instances are assumed to be independent and identicallydistributed, how to effectively transfer knowledge across different informationnetworks has not been well studied, mainly because networks may have theirdistinct node features and link relationships between nodes. In this paper, wepropose a new transfer learning algorithm that attempts to transfer commonlatent structure features across the source and target networks. The proposedalgorithm discovers these latent features by constructing label propagationmatrices in the source and target networks, and mapping them into a sharedlatent feature space. The latent features capture common structure patternsshared by two networks, and serve as domain-independent features to betransferred between networks. Together with domain-dependent node features, wethereafter propose an iterative classification algorithm that leverages labelcorrelations to predict node labels in the target network. Experiments onreal-world networks demonstrate that our proposed algorithm can successfullyachieve knowledge transfer between networks to help improve the accuracy ofclassifying nodes in the target network.
arxiv-5400-23 | Removing Mixture of Gaussian and Impulse Noise by Patch-Based Weighted Means | http://arxiv.org/pdf/1403.2482v1.pdf | author:Haijuan Hu, Bing Li, Quansheng Liu category:cs.CV published:2014-03-11 summary:We first establish a law of large numbers and a convergence theorem indistribution to show the rate of convergence of the non-local means filter forremoving Gaussian noise. We then introduce the notion of degree of similarityto measure the role of similarity for the non-local means filter. Based on theconvergence theorems, we propose a patch-based weighted means filter forremoving impulse noise and its mixture with Gaussian noise by combining theessential idea of the trilateral filter and that of the non-local means filter.Our experiments show that our filter is competitive compared to recentlyproposed methods.
arxiv-5400-24 | Image Fusion Techniques in Remote Sensing | http://arxiv.org/pdf/1403.5473v1.pdf | author:Reham Gharbia, Ahmad Taher Azar, Ali El Baz, Aboul Ella Hassanien category:cs.CV published:2014-03-11 summary:Remote sensing image fusion is an effective way to use a large volume of datafrom multisensor images. Most earth satellites such as SPOT, Landsat 7, IKONOSand QuickBird provide both panchromatic (Pan) images at a higher spatialresolution and multispectral (MS) images at a lower spatial resolution and manyremote sensing applications require both high spatial and high spectralresolutions, especially for GIS based applications. An effective image fusiontechnique can produce such remotely sensed images. Image fusion is thecombination of two or more different images to form a new image by using acertain algorithm to obtain more and better information about an object or astudy area than. The image fusion is performed at three different processinglevels which are pixel level, feature level and decision level according to thestage at which the fusion takes place. There are many image fusion methods thatcan be used to produce high resolution multispectral images from a highresolution pan image and low resolution multispectral images. This paperexplores the major remote sensing data fusion techniques at pixel level andreviews the concept, principals, limitations and advantages for each technique.This paper focused on traditional techniques like intensity hue-saturation-(HIS), Brovey, principal component analysis (PCA) and Wavelet.
arxiv-5400-25 | Generalised Mixability, Constant Regret, and Bayesian Updating | http://arxiv.org/pdf/1403.2433v1.pdf | author:Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson category:cs.LG stat.ML published:2014-03-10 summary:Mixability of a loss is known to characterise when constant regret bounds areachievable in games of prediction with expert advice through the use of Vovk'saggregating algorithm. We provide a new interpretation of mixability via convexanalysis that highlights the role of the Kullback-Leibler divergence in itsdefinition. This naturally generalises to what we call $\Phi$-mixability wherethe Bregman divergence $D_\Phi$ replaces the KL divergence. We prove thatlosses that are $\Phi$-mixable also enjoy constant regret bounds via ageneralised aggregating algorithm that is similar to mirror descent.
arxiv-5400-26 | Generic Deep Networks with Wavelet Scattering | http://arxiv.org/pdf/1312.5940v3.pdf | author:Edouard Oyallon, Stéphane Mallat, Laurent Sifre category:cs.CV published:2013-12-20 summary:We introduce a two-layer wavelet scattering network, for objectclassification. This scattering transform computes a spatial wavelet transformon the first layer and a new joint wavelet transform along spatial, angular andscale variables in the second layer. Numerical experiments demonstrate thatthis two layer convolution network, which involves no learning and no maxpooling, performs efficiently on complex image data sets such as CalTech, withstructural objects variability and clutter. It opens the possibility tosimplify deep neural network learning by initializing the first layers withwavelet filters.
arxiv-5400-27 | Phase Retrieval using Lipschitz Continuous Maps | http://arxiv.org/pdf/1403.2301v1.pdf | author:Radu Balan, Dongmian Zou category:math.FA cs.IT math.IT stat.ML published:2014-03-10 summary:In this note we prove that reconstruction from magnitudes of framecoefficients (the so called "phase retrieval problem") can be performed usingLipschitz continuous maps. Specifically we show that when the nonlinearanalysis map $\alpha:{\mathcal H}\rightarrow\mathbb{R}^m$ is injective, with$(\alpha(x))_k=<x,f_k>^2$, where $\{f_1,\ldots,f_m\}$ is a frame for theHilbert space ${\mathcal H}$, then there exists a left inverse map$\omega:\mathbb{R}^m\rightarrow {\mathcal H}$ that is Lipschitz continuous.Additionally we obtain the Lipschitz constant of this inverse map in terms ofthe lower Lipschitz constant of $\alpha$. Surprisingly the increase inLipschitz constant is independent of the space dimension or frame redundancy.
arxiv-5400-28 | Sublinear Models for Graphs | http://arxiv.org/pdf/1403.2295v1.pdf | author:Brijnesh J. Jain category:cs.LG cs.CV published:2014-03-10 summary:This contribution extends linear models for feature vectors to sublinearmodels for graphs and analyzes their properties. The results are (i) ageometric interpretation of sublinear classifiers, (ii) a generic learning rulebased on the principle of empirical risk minimization, (iii) a convergencetheorem for the margin perceptron in the sublinearly separable case, and (iv)the VC-dimension of sublinear functions. Empirical results on graph data showthat sublinear models on graphs have similar properties as linear models forfeature vectors.
arxiv-5400-29 | Dropout improves Recurrent Neural Networks for Handwriting Recognition | http://arxiv.org/pdf/1312.4569v2.pdf | author:Vu Pham, Théodore Bluche, Christopher Kermorvant, Jérôme Louradour category:cs.CV cs.LG cs.NE published:2013-11-05 summary:Recurrent neural networks (RNNs) with Long Short-Term memory cells currentlyhold the best known results in unconstrained handwriting recognition. We showthat their performance can be greatly improved using dropout - a recentlyproposed regularization method for deep architectures. While previous worksshowed that dropout gave superior performance in the context of convolutionalnetworks, it had never been applied to RNNs. In our approach, dropout iscarefully used in the network so that it does not affect the recurrentconnections, hence the power of RNNs in modeling sequence is preserved.Extensive experiments on a broad range of handwritten databases confirm theeffectiveness of dropout on deep architectures even when the network mainlyconsists of recurrent and shared connections.
arxiv-5400-30 | Collaborative Representation based Classification for Face Recognition | http://arxiv.org/pdf/1204.2358v2.pdf | author:Lei Zhang, Meng Yang, Xiangchu Feng, Yi Ma, David Zhang category:cs.CV published:2012-04-11 summary:By coding a query sample as a sparse linear combination of all trainingsamples and then classifying it by evaluating which class leads to the minimalcoding residual, sparse representation based classification (SRC) leads tointeresting results for robust face recognition. It is widely believed that thel1- norm sparsity constraint on coding coefficients plays a key role in thesuccess of SRC, while its use of all training samples to collaborativelyrepresent the query sample is rather ignored. In this paper we discuss how SRCworks, and show that the collaborative representation mechanism used in SRC ismuch more crucial to its success of face classification. The SRC is a specialcase of collaborative representation based classification (CRC), which hasvarious instantiations by applying different norms to the coding residual andcoding coefficient. More specifically, the l1 or l2 norm characterization ofcoding residual is related to the robustness of CRC to outlier facial pixels,while the l1 or l2 norm characterization of coding coefficient is related tothe degree of discrimination of facial features. Extensive experiments wereconducted to verify the face recognition accuracy and efficiency of CRC withdifferent instantiations.
arxiv-5400-31 | Parsing using a grammar of word association vectors | http://arxiv.org/pdf/1403.2152v1.pdf | author:Robert John Freeman category:cs.CL cs.NE published:2014-03-10 summary:This paper was was first drafted in 2001 as a formalization of the systemdescribed in U.S. patent U.S. 7,392,174. It describes a system for implementinga parser based on a kind of cross-product over vectors of contextually similarwords. It is being published now in response to nascent interest in vectorcombination models of syntax and semantics. The method used aggressivesubstitution of contextually similar words and word groups to enable productvectors to stay in the same space as their operands and make entire sentencescomparable syntactically, and potentially semantically. The vectors generatedhad sufficient representational strength to generate parse trees at leastcomparable with contemporary symbolic parsers.
arxiv-5400-32 | Constraint-based Causal Discovery from Multiple Interventions over Overlapping Variable Sets | http://arxiv.org/pdf/1403.2150v1.pdf | author:Sofia Triantafillou, Ioannis Tsamardinos category:stat.ML cs.AI published:2014-03-10 summary:Scientific practice typically involves repeatedly studying a system, eachtime trying to unravel a different perspective. In each study, the scientistmay take measurements under different experimental conditions (interventions,manipulations, perturbations) and measure different sets of quantities(variables). The result is a collection of heterogeneous data sets coming fromdifferent data distributions. In this work, we present algorithm COmbINE, whichaccepts a collection of data sets over overlapping variable sets underdifferent experimental conditions; COmbINE then outputs a summary of all causalmodels indicating the invariant and variant structural characteristics of allmodels that simultaneously fit all of the input data sets. COmbINE convertsestimated dependencies and independencies in the data into path constraints onthe data-generating causal model and encodes them as a SAT instance. Thealgorithm is sound and complete in the sample limit. To account for conflictingconstraints arising from statistical errors, we introduce a general method forsorting constraints in order of confidence, computed as a function of theircorresponding p-values. In our empirical evaluation, COmbINE outperforms interms of efficiency the only pre-existing similar algorithm; the latteradditionally admits feedback cycles, but does not admit conflicting constraintswhich hinders the applicability on real data. As a proof-of-concept, COmbINE isemployed to co-analyze 4 real, mass-cytometry data sets measuringphosphorylated protein concentrations of overlapping protein sets under 3different interventions.
arxiv-5400-33 | Generating Music from Literature | http://arxiv.org/pdf/1403.2124v1.pdf | author:Hannah Davis, Saif M. Mohammad category:cs.CL published:2014-03-10 summary:We present a system, TransProse, that automatically generates musical piecesfrom text. TransProse uses known relations between elements of music such astempo and scale, and the emotions they evoke. Further, it uses a novelmechanism to determine sequences of notes that capture the emotional activityin the text. The work has applications in information visualization, increating audio-visual e-books, and in developing music apps.
arxiv-5400-34 | Learning Factored Representations in a Deep Mixture of Experts | http://arxiv.org/pdf/1312.4314v3.pdf | author:David Eigen, Marc'Aurelio Ranzato, Ilya Sutskever category:cs.LG published:2013-12-16 summary:Mixtures of Experts combine the outputs of several "expert" networks, each ofwhich specializes in a different part of the input space. This is achieved bytraining a "gating" network that maps each input to a distribution over theexperts. Such models show promise for building larger networks that are stillcheap to compute at test time, and more parallelizable at training time. Inthis this work, we extend the Mixture of Experts to a stacked model, the DeepMixture of Experts, with multiple sets of gating and experts. Thisexponentially increases the number of effective experts by associating eachinput with a combination of experts at each layer, yet maintains a modest modelsize. On a randomly translated version of the MNIST dataset, we find that theDeep Mixture of Experts automatically learns to develop location-dependent("where") experts at the first layer, and class-specific ("what") experts atthe second layer. In addition, we see that the different combinations are inuse when the model is applied to a dataset of speech monophones. Thesedemonstrate effective use of all expert combinations.
arxiv-5400-35 | From average case complexity to improper learning complexity | http://arxiv.org/pdf/1311.2272v2.pdf | author:Amit Daniely, Nati Linial, Shai Shalev-Shwartz category:cs.LG cs.CC published:2013-11-10 summary:The basic problem in the PAC model of computational learning theory is todetermine which hypothesis classes are efficiently learnable. There ispresently a dearth of results showing hardness of learning problems. Moreover,the existing lower bounds fall short of the best known algorithms. The biggest challenge in proving complexity results is to establish hardnessof {\em improper learning} (a.k.a. representation independent learning).Thedifficulty in proving lower bounds for improper learning is that the standardreductions from $\mathbf{NP}$-hard problems do not seem to apply in thiscontext. There is essentially only one known approach to proving lower boundson improper learning. It was initiated in (Kearns and Valiant 89) and relies oncryptographic assumptions. We introduce a new technique for proving hardness of improper learning, basedon reductions from problems that are hard on average. We put forward a (fairlystrong) generalization of Feige's assumption (Feige 02) about the complexity ofrefuting random constraint satisfaction problems. Combining this assumptionwith our new technique yields far reaching implications. In particular, 1. Learning $\mathrm{DNF}$'s is hard. 2. Agnostically learning halfspaces with a constant approximation ratio ishard. 3. Learning an intersection of $\omega(1)$ halfspaces is hard.
arxiv-5400-36 | Learning Transformations for Clustering and Classification | http://arxiv.org/pdf/1309.2074v2.pdf | author:Qiang Qiu, Guillermo Sapiro category:cs.CV cs.LG stat.ML published:2013-09-09 summary:A low-rank transformation learning framework for subspace clustering andclassification is here proposed. Many high-dimensional data, such as faceimages and motion sequences, approximately lie in a union of low-dimensionalsubspaces. The corresponding subspace clustering problem has been extensivelystudied in the literature to partition such high-dimensional data into clusterscorresponding to their underlying low-dimensional subspaces. However,low-dimensional intrinsic structures are often violated for real-worldobservations, as they can be corrupted by errors or deviate from ideal models.We propose to address this by learning a linear transformation on subspacesusing matrix rank, via its convex surrogate nuclear norm, as the optimizationcriteria. The learned linear transformation restores a low-rank structure fordata from the same subspace, and, at the same time, forces a a maximallyseparated structure for data from different subspaces. In this way, we reducevariations within subspaces, and increase separation between subspaces for amore robust subspace clustering. This proposed learned robust subspaceclustering framework significantly enhances the performance of existingsubspace clustering methods. Basic theoretical results here presented help tofurther support the underlying framework. To exploit the low-rank structures ofthe transformed subspaces, we further introduce a fast subspace clusteringtechnique, which efficiently combines robust PCA with sparse modeling. Whenclass labels are present at the training stage, we show this low-ranktransformation framework also significantly enhances classificationperformance. Extensive experiments using public datasets are presented, showingthat the proposed approach significantly outperforms state-of-the-art methodsfor subspace clustering and classification.
arxiv-5400-37 | Generalized Canonical Correlation Analysis and Its Application to Blind Source Separation Based on a Dual-Linear Predictor Structure | http://arxiv.org/pdf/1403.2073v1.pdf | author:Wei Liu category:math.NA stat.ML published:2014-03-09 summary:Blind source separation (BSS) is one of the most important and establishedresearch topics in signal processing and many algorithms have been proposedbased on different statistical properties of the source signals. Forsecond-order statistics (SOS) based methods, canonical correlation analysis(CCA) has been proved to be an effective solution to the problem. In this work,the CCA approach is generalized to accommodate the case with added white noiseand it is then applied to the BSS problem for noisy mixtures. In this approach,the noise component is assumed to be spatially and temporally white, but thevariance information of noise is not required. An adaptive blind sourceextraction algorithm is derived based on this idea and a further extension isproposed by employing a dual-linear predictor structure for blind sourceextraction (BSE).
arxiv-5400-38 | Texture Defect Detection in Gradient Space | http://arxiv.org/pdf/1403.2031v1.pdf | author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV published:2014-03-09 summary:In this paper, we propose a machine vision algorithm for automaticallydetecting defects in patterned textures with the help of gradient space and itsenergy. Experiments on real fabric images with defects show that the proposedmethod can be used for automatic detection of fabric defects in textileindustries.
arxiv-5400-39 | Fast Distribution To Real Regression | http://arxiv.org/pdf/1311.2236v2.pdf | author:Junier B. Oliva, Willie Neiswanger, Barnabas Poczos, Jeff Schneider, Eric Xing category:stat.ML cs.LG math.ST stat.TH published:2013-11-10 summary:We study the problem of distribution to real-value regression, where one aimsto regress a mapping $f$ that takes in a distribution input covariate $P\in\mathcal{I}$ (for a non-parametric family of distributions $\mathcal{I}$) andoutputs a real-valued response $Y=f(P) + \epsilon$. This setting was recentlystudied, and a "Kernel-Kernel" estimator was introduced and shown to have apolynomial rate of convergence. However, evaluating a new prediction with theKernel-Kernel estimator scales as $\Omega(N)$. This causes the difficultsituation where a large amount of data may be necessary for a low estimationrisk, but the computation cost of estimation becomes infeasible when thedata-set is too large. To this end, we propose the Double-Basis estimator,which looks to alleviate this big data problem in two ways: first, theDouble-Basis estimator is shown to have a computation complexity that isindependent of the number of of instances $N$ when evaluating new predictionsafter training; secondly, the Double-Basis estimator is shown to have a fastrate of convergence for a general class of mappings $f\in\mathcal{F}$.
arxiv-5400-40 | FuSSO: Functional Shrinkage and Selection Operator | http://arxiv.org/pdf/1311.2234v2.pdf | author:Junier B. Oliva, Barnabas Poczos, Timothy Verstynen, Aarti Singh, Jeff Schneider, Fang-Cheng Yeh, Wen-Yih Tseng category:stat.ML cs.LG math.ST stat.TH published:2013-11-10 summary:We present the FuSSO, a functional analogue to the LASSO, that efficientlyfinds a sparse set of functional input covariates to regress a real-valuedresponse against. The FuSSO does so in a semi-parametric fashion, making noparametric assumptions about the nature of input functional covariates andassuming a linear form to the mapping of functional covariates to the response.We provide a statistical backing for use of the FuSSO via proof of asymptoticsparsistency under various conditions. Furthermore, we observe good results onboth synthetic and real-world data.
arxiv-5400-41 | Natural Language Feature Selection via Cooccurrence | http://arxiv.org/pdf/1403.2004v1.pdf | author:Michael Stewart category:cs.CL published:2014-03-08 summary:Specificity is important for extracting collocations, keyphrases, multi-wordand index terms [Newman et al. 2012]. It is also useful for tagging, ontologyconstruction [Ryu and Choi 2006], and automatic summarization of documents[Louis and Nenkova 2011, Chali and Hassan 2012]. Term frequency andinverse-document frequency (TF-IDF) are typically used to do this, but fail totake advantage of the semantic relationships between terms [Church and Gale1995]. The result is that general idiomatic terms are mistaken for specificterms. We demonstrate use of relational data for estimation of termspecificity. The specificity of a term can be learned from its distribution ofrelations with other terms. This technique is useful for identifying relevantwords or terms for other natural language processing tasks.
arxiv-5400-42 | Compressive Nonparametric Graphical Model Selection For Time Series | http://arxiv.org/pdf/1311.3257v2.pdf | author:Alexander Jung, Reinhard Heckel, Helmut Bölcskei, Franz Hlawatsch category:stat.ML published:2013-11-13 summary:We propose a method for inferring the conditional indepen- dence graph (CIG)of a high-dimensional discrete-time Gaus- sian vector random process fromfinite-length observations. Our approach does not rely on a parametric model(such as, e.g., an autoregressive model) for the vector random process; rather,it only assumes certain spectral smoothness proper- ties. The proposedinference scheme is compressive in that it works for sample sizes that are(much) smaller than the number of scalar process components. We provideanalytical conditions for our method to correctly identify the CIG with highprobability.
arxiv-5400-43 | Designing an FPGA Synthesizable Computer Vision Algorithm to Detect the Greening of Potatoes | http://arxiv.org/pdf/1403.1974v1.pdf | author:Jaspinder Pal Singh category:cs.CV published:2014-03-08 summary:Potato quality control has improved in the last years thanks to automationtechniques like machine vision, mainly making the classification task betweendifferent quality degrees faster, safer and less subjective. In our study weare going to design a computer vision algorithm for grading of potatoesaccording to the greening of the surface color of potato. The ratio of greenpixels to the total number of pixels of the potato surface is found. The higherthe ratio the worse is the potato. First the image is converted into serialdata and then processing is done in RGB colour space. Green part of the potatois also shown by de-serializing the output. The same algorithm is thensynthesized on FPGA and the result shows thousand times speed improvement incase of hardware synthesis.
arxiv-5400-44 | Combination of PCA with SMOTE Resampling to Boost the Prediction Rate in Lung Cancer Dataset | http://arxiv.org/pdf/1403.1949v1.pdf | author:Mehdi Naseriparsa, Mohammad Mansour Riahi Kashani category:cs.LG cs.CE published:2014-03-08 summary:Classification algorithms are unable to make reliable models on the datasetswith huge sizes. These datasets contain many irrelevant and redundant featuresthat mislead the classifiers. Furthermore, many huge datasets have imbalancedclass distribution which leads to bias over majority class in theclassification process. In this paper combination of unsuperviseddimensionality reduction methods with resampling is proposed and the resultsare tested on Lung-Cancer dataset. In the first step PCA is applied onLung-Cancer dataset to compact the dataset and eliminate irrelevant featuresand in the second step SMOTE resampling is carried out to balance the classdistribution and increase the variety of sample domain. Finally, Naive Bayesclassifier is applied on the resulting dataset and the results are compared andevaluation metrics are calculated. The experiments show the effectiveness ofthe proposed method across four evaluation metrics: Overall accuracy, FalsePositive Rate, Precision, Recall.
arxiv-5400-45 | A Hybrid Feature Selection Method to Improve Performance of a Group of Classification Algorithms | http://arxiv.org/pdf/1403.2372v1.pdf | author:Mehdi Naseriparsa, Amir-Masoud Bidgoli, Touraj Varaee category:cs.LG published:2014-03-08 summary:In this paper a hybrid feature selection method is proposed which takesadvantages of wrapper subset evaluation with a lower cost and improves theperformance of a group of classifiers. The method uses combination of sampledomain filtering and resampling to refine the sample domain and two featuresubset evaluation methods to select reliable features. This method utilizesboth feature space and sample domain in two phases. The first phase filters andresamples the sample domain and the second phase adopts a hybrid procedure byinformation gain, wrapper subset evaluation and genetic search to find theoptimal feature space. Experiments carried out on different types of datasetsfrom UCI Repository of Machine Learning databases and the results show a risein the average performance of five classifiers (Naive Bayes, Logistic,Multilayer Perceptron, Best First Decision Tree and JRIP) simultaneously andthe classification error for these classifiers decreases considerably. Theexperiments also show that this method outperforms other feature selectionmethods with a lower cost.
arxiv-5400-46 | Improving Performance of a Group of Classification Algorithms Using Resampling and Feature Selection | http://arxiv.org/pdf/1403.1946v1.pdf | author:Mehdi Naseriparsa, Amir-masoud Bidgoli, Touraj Varaee category:cs.LG published:2014-03-08 summary:In recent years the importance of finding a meaningful pattern from hugedatasets has become more challenging. Data miners try to adopt innovativemethods to face this problem by applying feature selection methods. In thispaper we propose a new hybrid method in which we use a combination ofresampling, filtering the sample domain and wrapper subset evaluation methodwith genetic search to reduce dimensions of Lung-Cancer dataset that wereceived from UCI Repository of Machine Learning databases. Finally, we applysome well- known classification algorithms (Na\"ive Bayes, Logistic, MultilayerPerceptron, Best First Decision Tree and JRIP) to the resulting dataset andcompare the results and prediction rates before and after the application ofour feature selection method on that dataset. The results show a substantialprogress in the average performance of five classification algorithmssimultaneously and the classification error for these classifiers decreasesconsiderably. The experiments also show that this method outperforms otherfeature selection methods with a lower cost.
arxiv-5400-47 | Multi-label ensemble based on variable pairwise constraint projection | http://arxiv.org/pdf/1403.1944v1.pdf | author:Ping Li, Hong Li, Min Wu category:cs.LG cs.CV stat.ML I.2.6 published:2014-03-08 summary:Multi-label classification has attracted an increasing amount of attention inrecent years. To this end, many algorithms have been developed to classifymulti-label data in an effective manner. However, they usually do not considerthe pairwise relations indicated by sample labels, which actually playimportant roles in multi-label classification. Inspired by this, we naturallyextend the traditional pairwise constraints to the multi-label scenario via aflexible thresholding scheme. Moreover, to improve the generalization abilityof the classifier, we adopt a boosting-like strategy to construct a multi-labelensemble from a group of base classifiers. To achieve these goals, this paperpresents a novel multi-label classification framework named Variable PairwiseConstraint projection for Multi-label Ensemble (VPCME). Specifically, we takeadvantage of the variable pairwise constraint projection to learn alower-dimensional data representation, which preserves the correlations betweensamples and labels. Thereafter, the base classifiers are trained in the newdata space. For the boosting-like strategy, we employ both the variablepairwise constraints and the bootstrap steps to diversify the base classifiers.Empirical studies have shown the superiority of the proposed method incomparison with other approaches.
arxiv-5400-48 | Quality-based Multimodal Classification Using Tree-Structured Sparsity | http://arxiv.org/pdf/1403.1902v1.pdf | author:Soheil Bahrampour, Asok Ray, Nasser M. Nasrabadi, Kenneth W. Jenkins category:cs.CV published:2014-03-08 summary:Recent studies have demonstrated advantages of information fusion based onsparsity models for multimodal classification. Among several sparsity models,tree-structured sparsity provides a flexible framework for extraction ofcross-correlated information from different sources and for enforcing groupsparsity at multiple granularities. However, the existing algorithm only solvesan approximated version of the cost functional and the resulting solution isnot necessarily sparse at group levels. This paper reformulates thetree-structured sparse model for multimodal classification task. An acceleratedproximal algorithm is proposed to solve the optimization problem, which is anefficient tool for feature-level fusion among either homogeneous orheterogeneous sources of information. In addition, a (fuzzy-set-theoretic)possibilistic scheme is proposed to weight the available modalities, based ontheir respective reliability, in a joint optimization problem for finding thesparsity codes. This approach provides a general framework for quality-basedfusion that offers added robustness to several sparsity-based multimodalclassification algorithms. To demonstrate their efficacy, the proposed methodsare evaluated on three different applications - multiview face recognition,multimodal face recognition, and target classification.
arxiv-5400-49 | Zero-Shot Learning for Semantic Utterance Classification | http://arxiv.org/pdf/1401.0509v3.pdf | author:Yann N. Dauphin, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck category:cs.CL cs.LG published:2013-12-20 summary:We propose a novel zero-shot learning method for semantic utteranceclassification (SUC). It learns a classifier $f: X \to Y$ for problems wherenone of the semantic categories $Y$ are present in the training set. Theframework uncovers the link between categories and utterances using a semanticspace. We show that this semantic space can be learned by deep neural networkstrained on large amounts of search engine query log data. More precisely, wepropose a novel method that can learn discriminative semantic features withoutsupervision. It uses the zero-shot learning framework to guide the learning ofthe semantic features. We demonstrate the effectiveness of the zero-shotsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).Furthermore, we achieve state-of-the-art results by combining the semanticfeatures with a supervised method.
arxiv-5400-50 | Becoming More Robust to Label Noise with Classifier Diversity | http://arxiv.org/pdf/1403.1893v1.pdf | author:Michael R. Smith, Tony Martinez category:stat.ML cs.AI cs.LG published:2014-03-07 summary:It is widely known in the machine learning community that class noise can be(and often is) detrimental to inducing a model of the data. Many currentapproaches use a single, often biased, measurement to determine if an instanceis noisy. A biased measure may work well on certain data sets, but it can alsobe less effective on a broader set of data sets. In this paper, we presentnoise identification using classifier diversity (NICD) -- a method for derivinga less biased noise measurement and integrating it into the learning process.To lessen the bias of the noise measure, NICD selects a diverse set ofclassifiers (based on their predictions of novel instances) to determine whichinstances are noisy. We examine NICD as a technique for filtering, instanceweighting, and selecting the base classifiers of a voting ensemble. We compareNICD with several other noise handling techniques that do not considerclassifier diversity on a set of 54 data sets and 5 learning algorithms. NICDsignificantly increases the classification accuracy over the other consideredapproaches and is effective across a broad set of data sets and learningalgorithms.
arxiv-5400-51 | Statistical Structure Learning, Towards a Robust Smart Grid | http://arxiv.org/pdf/1403.1863v1.pdf | author:Hanie Sedghi, Edmond Jonckheere category:cs.LG cs.SY published:2014-03-07 summary:Robust control and maintenance of the grid relies on accurate data. Both PMUsand state estimators are prone to false data injection attacks. Thus, it iscrucial to have a mechanism for fast and accurate detection of an agentmaliciously tampering with the data---for both preventing attacks that may leadto blackouts, and for routine monitoring and control tasks of current andfuture grids. We propose a decentralized false data injection detection schemebased on Markov graph of the bus phase angles. We utilize the ConditionalCovariance Test (CCT) to learn the structure of the grid. Using the DC powerflow model, we show that under normal circumstances, and because ofwalk-summability of the grid graph, the Markov graph of the voltage angles canbe determined by the power grid graph. Therefore, a discrepancy betweencalculated Markov graph and learned structure should trigger the alarm. Localgrid topology is available online from the protection system and we exploit itto check for mismatch. Should a mismatch be detected, we use correlationanomaly score to detect the set of attacked nodes. Our method can detect themost recent stealthy deception attack on the power grid that assumes knowledgeof bus-branch model of the system and is capable of deceiving the stateestimator, damaging power network observatory, control, monitoring, demandresponse and pricing schemes. Specifically, under the stealthy deceptionattack, the Markov graph of phase angles changes. In addition to detect a stateof attack, our method can detect the set of attacked nodes. To the best of ourknowledge, our remedy is the first to comprehensively detect this sophisticatedattack and it does not need additional hardware. Moreover, our detection schemeis successful no matter the size of the attacked subset. Simulation of variouspower networks confirms our claims.
arxiv-5400-52 | Near-Optimally Teaching the Crowd to Classify | http://arxiv.org/pdf/1402.2092v4.pdf | author:Adish Singla, Ilija Bogunovic, Gábor Bartók, Amin Karbasi, Andreas Krause category:cs.LG published:2014-02-10 summary:How should we present training examples to learners to teach themclassification rules? This is a natural problem when training workers forcrowdsourcing labeling tasks, and is also motivated by challenges indata-driven online education. We propose a natural stochastic model of thelearners, modeling them as randomly switching among hypotheses based onobserved feedback. We then develop STRICT, an efficient algorithm for selectingexamples to teach to workers. Our solution greedily maximizes a submodularsurrogate objective function in order to select examples to show to thelearners. We prove that our strategy is competitive with the optimal teachingpolicy. Moreover, for the special case of linear separators, we prove that anexponential reduction in error probability can be achieved. Our experiments onsimulated workers as well as three real image annotation tasks on AmazonMechanical Turk show the effectiveness of our teaching algorithm.
arxiv-5400-53 | Analysis of Agglomerative Clustering | http://arxiv.org/pdf/1012.3697v4.pdf | author:Marcel R. Ackermann, Johannes Blömer, Daniel Kuntze, Christian Sohler category:cs.DS cs.CG cs.LG published:2010-12-16 summary:The diameter $k$-clustering problem is the problem of partitioning a finitesubset of $\mathbb{R}^d$ into $k$ subsets called clusters such that the maximumdiameter of the clusters is minimized. One early clustering algorithm thatcomputes a hierarchy of approximate solutions to this problem (for all valuesof $k$) is the agglomerative clustering algorithm with the complete linkagestrategy. For decades, this algorithm has been widely used by practitioners.However, it is not well studied theoretically. In this paper, we analyze theagglomerative complete linkage clustering algorithm. Assuming that thedimension $d$ is a constant, we show that for any $k$ the solution computed bythis algorithm is an $O(\log k)$-approximation to the diameter $k$-clusteringproblem. Our analysis does not only hold for the Euclidean distance but for anymetric that is based on a norm. Furthermore, we analyze the closely related$k$-center and discrete $k$-center problem. For the corresponding agglomerativealgorithms, we deduce an approximation factor of $O(\log k)$ as well.
arxiv-5400-54 | The Power of Localization for Efficiently Learning Linear Separators with Noise | http://arxiv.org/pdf/1307.8371v7.pdf | author:Pranjal Awasthi, Maria Florina Balcan, Philip M. Long category:cs.LG cs.CC cs.DS stat.ML published:2013-07-31 summary:We introduce a new approach for designing computationally efficient learningalgorithms that are tolerant to noise. We demonstrate the effectiveness of ourapproach by designing algorithms with improved noise tolerance guarantees forlearning linear separators. We consider the malicious noise model of Valiant and the adversarial labelnoise model of Kearns, Schapire, and Sellie. For malicious noise, where theadversary can corrupt an $\eta$ of fraction both the label part and the featurepart, we provide a polynomial-time algorithm for learning linear separators in$\Re^d$ under the uniform distribution with near information-theoretic optimalnoise tolerance of $\eta = \Omega(\epsilon)$. We also get similar improvementsfor the adversarial label noise model. We obtain similar results for moregeneral classes of distributions including isotropic log-concave distributions. In addition, our algorithms achieve a label complexity whose dependence onthe error parameter $\epsilon$ is {\em exponentially better} than that of anypassive algorithm. This provides the first polynomial-time active learningalgorithm for learning linear separators in the presence of adversarial labelnoise, as well as the first analysis of active learning under the maliciousnoise model.
arxiv-5400-55 | Leveraging Long-Term Predictions and Online-Learning in Agent-based Multiple Person Tracking | http://arxiv.org/pdf/1402.2016v2.pdf | author:Wenxi Liu, Antoni B. Chan, Rynson W. H. Lau, Dinesh Manocha category:cs.CV published:2014-02-10 summary:We present a multiple-person tracking algorithm, based on combining particlefilters and RVO, an agent-based crowd model that infers collision-freevelocities so as to predict pedestrian's motion. In addition to position andvelocity, our tracking algorithm can estimate the internal goals (desireddestination or desired velocity) of the tracked pedestrian in an online manner,thus removing the need to specify this information beforehand. Furthermore, weleverage the longer-term predictions of RVO by deriving a higher-order particlefilter, which aggregates multiple predictions from different prior time steps.This yields a tracker that can recover from short-term occlusions and spuriousnoise in the appearance model. Experimental results show that our trackingalgorithm is suitable for predicting pedestrians' behaviors online withoutneeding scene priors or hand-annotated goal information, and improves trackingin real-world crowded scenes under low frame rates.
arxiv-5400-56 | Finding Eyewitness Tweets During Crises | http://arxiv.org/pdf/1403.1773v1.pdf | author:Fred Morstatter, Nichola Lubold, Heather Pon-Barry, Jürgen Pfeffer, Huan Liu category:cs.CL cs.CY published:2014-03-07 summary:Disaster response agencies have started to incorporate social media as asource of fast-breaking information to understand the needs of people affectedby the many crises that occur around the world. These agencies look for tweetsfrom within the region affected by the crisis to get the latest updates of thestatus of the affected region. However only 1% of all tweets are geotagged withexplicit location information. First responders lose valuable informationbecause they cannot assess the origin of many of the tweets they collect. Inthis work we seek to identify non-geotagged tweets that originate from withinthe crisis region. Towards this, we address three questions: (1) is there adifference between the language of tweets originating within a crisis regionand tweets originating outside the region, (2) what are the linguistic patternsthat can be used to differentiate within-region and outside-region tweets, and(3) for non-geotagged tweets, can we automatically identify those originatingwithin the crisis region in real-time?
arxiv-5400-57 | Ant Colony based Feature Selection Heuristics for Retinal Vessel Segmentation | http://arxiv.org/pdf/1403.1735v1.pdf | author:Ahmed. H. Asad, Ahmad Taher Azar, Nashwa El-Bendary, Aboul Ella Hassaanien category:cs.NE cs.CV published:2014-03-07 summary:Features selection is an essential step for successful data classification,since it reduces the data dimensionality by removing redundant features.Consequently, that minimizes the classification complexity and time in additionto maximizing its accuracy. In this article, a comparative study consideringsix features selection heuristics is conducted in order to select the bestrelevant features subset. The tested features vector consists of fourteenfeatures that are computed for each pixel in the field of view of retinalimages in the DRIVE database. The comparison is assessed in terms ofsensitivity, specificity, and accuracy measurements of the recommended featuressubset resulted by each heuristic when applied with the ant colony system.Experimental results indicated that the features subset recommended by therelief heuristic outperformed the subsets recommended by the other experiencedheuristics.
arxiv-5400-58 | Recycling Proof Patterns in Coq: Case Studies | http://arxiv.org/pdf/1301.6039v4.pdf | author:Jónathan Heras, Ekaterina Komendantskaya category:cs.AI cs.LG cs.LO published:2013-01-25 summary:Development of Interactive Theorem Provers has led to the creation of biglibraries and varied infrastructures for formal proofs. However, despite (orperhaps due to) their sophistication, the re-use of libraries by non-experts oracross domains is a challenge. In this paper, we provide detailed case studiesand evaluate the machine-learning tool ML4PG built to interactively data-minethe electronic libraries of proofs, and to provide user guidance on the basisof proof patterns found in the existing libraries.
arxiv-5400-59 | Continuous Features Discretization for Anomaly Intrusion Detectors Generation | http://arxiv.org/pdf/1403.1729v1.pdf | author:Amira Sayed A. Aziz, Ahmad Taher Azar, Aboul Ella Hassanien, Sanaa Al-Ola Hanafy category:cs.NI cs.CR cs.NE published:2014-03-07 summary:Network security is a growing issue, with the evolution of computer systemsand expansion of attacks. Biological systems have been inspiring scientists anddesigns for new adaptive solutions, such as genetic algorithms. In this paper,we present an approach that uses the genetic algorithm to generate anomaly net-work intrusion detectors. In this paper, an algorithm propose use adiscretization method for the continuous features selected for the intrusiondetection, to create some homogeneity between values, which have different datatypes. Then,the intrusion detection system is tested against the NSL-KDD dataset using different distance methods. A comparison is held amongst the results,and it is shown by the end that this proposed approach has good results, andrecommendations is given for future experiments.
arxiv-5400-60 | On the Sequence of State Configurations in the Garden of Eden | http://arxiv.org/pdf/1403.1727v1.pdf | author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE published:2014-03-07 summary:Autonomous threshold element circuit networks are used to investigate thestructure of neural networks. With these circuits, as the transition functionsare threshold functions, it is necessary to consider the existence of sequencesof state configurations that cannot be transitioned. In this study, we focus onall logical functions of four or fewer variables, and we discuss the periodicsequences and transient series that transition from all sequences of stateconfigurations. Furthermore, by using the sequences of state configurations inthe Garden of Eden, we show that it is easy to obtain functions that determinethe operation of circuit networks.
arxiv-5400-61 | The Why and How of Nonnegative Matrix Factorization | http://arxiv.org/pdf/1401.5226v2.pdf | author:Nicolas Gillis category:stat.ML cs.IR cs.LG math.OC published:2014-01-21 summary:Nonnegative matrix factorization (NMF) has become a widely used tool for theanalysis of high-dimensional data as it automatically extracts sparse andmeaningful features from a set of nonnegative data vectors. We first illustratethis property of NMF on three applications, in image processing, text miningand hyperspectral imaging --this is the why. Then we address the problem ofsolving NMF, which is NP-hard in general. We review some standard NMFalgorithms, and also present a recent subclass of NMF problems, referred to asnear-separable NMF, that can be solved efficiently (that is, in polynomialtime), even in the presence of noise --this is the how. Finally, we brieflydescribe some problems in mathematics and computer science closely related toNMF via the nonnegative rank.
arxiv-5400-62 | Compressive Hyperspectral Imaging Using Progressive Total Variation | http://arxiv.org/pdf/1403.1697v1.pdf | author:Simeon Kamdem Kuiteing, Giulio Coluccia, Alessandro Barducci, Mauro Barni, Enrico Magli category:cs.IT cs.CV math.IT published:2014-03-07 summary:Compressed Sensing (CS) is suitable for remote acquisition of hyperspectralimages for earth observation, since it could exploit the strong spatial andspectral correlations, llowing to simplify the architecture of the onboardsensors. Solutions proposed so far tend to decouple spatial and spectraldimensions to reduce the complexity of the reconstruction, not taking intoaccount that onboard sensors progressively acquire spectral rows rather thanacquiring spectral channels. For this reason, we propose a novel progressive CSarchitecture based on separate sensing of spectral rows and jointreconstruction employing Total Variation. Experimental results run on rawAVIRIS and AIRS images confirm the validity of the proposed system.
arxiv-5400-63 | Rigid-Motion Scattering for Texture Classification | http://arxiv.org/pdf/1403.1687v1.pdf | author:Laurent SIfre, Stéphane Mallat category:cs.CV published:2014-03-07 summary:A rigid-motion scattering computes adaptive invariants along translations androtations, with a deep convolutional network. Convolutions are calculated onthe rigid-motion group, with wavelets defined on the translation and rotationvariables. It preserves joint rotation and translation information, whileproviding global invariants at any desired scale. Texture classification isstudied, through the characterization of stationary processes from a singlerealization. State-of-the-art results are obtained on multiple texture databases, with important rotation and scaling variabilities.
arxiv-5400-64 | Feature Extraction of ECG Signal Using HHT Algorithm | http://arxiv.org/pdf/1403.1660v1.pdf | author:Neha Soorma, Jaikaran Singh, Mukesh Tiwari category:cs.CV published:2014-03-07 summary:This paper describe the features extraction algorithm for electrocardiogram(ECG) signal using Huang Hilbert Transform and Wavelet Transform. ECG signalfor an individual human being is different due to unique heart structure. Thepurpose of feature extraction of ECG signal would allow successful abnormalitydetection and efficient prognosis due to heart disorder. Some major importantfeatures will be extracted from ECG signals such as amplitude, duration,pre-gradient, post-gradient and so on. Therefore, we need a strong mathematicalmodel to extract such useful parameter. Here an adaptive mathematical analysismodel is Hilbert-Huang transform (HHT). This new approach, the Hilbert-Huangtransform, is implemented to analyze the non-linear and nonstationary data. Itis unique and different from the existing methods of data analysis and does notrequire an a priori functional basis. The effectiveness of the proposed schemeis verified through the simulation.
arxiv-5400-65 | Home Location Identification of Twitter Users | http://arxiv.org/pdf/1403.2345v1.pdf | author:Jalal Mahmud, Jeffrey Nichols, Clemens Drews category:cs.SI cs.CL cs.CY published:2014-03-07 summary:We present a new algorithm for inferring the home location of Twitter usersat different granularities, including city, state, time zone or geographicregion, using the content of users tweets and their tweeting behavior. Unlikeexisting approaches, our algorithm uses an ensemble of statistical andheuristic classifiers to predict locations and makes use of a geographicgazetteer dictionary to identify place-name entities. We find that ahierarchical classification approach, where time zone, state or geographicregion is predicted first and city is predicted next, can improve predictionaccuracy. We have also analyzed movement variations of Twitter users, built aclassifier to predict whether a user was travelling in a certain period of timeand use that to further improve the location detection accuracy. Experimentalevidence suggests that our algorithm works well in practice and outperforms thebest existing algorithms for predicting the home location of Twitter users.
arxiv-5400-66 | Automated Tracking and Estimation for Control of Non-rigid Cloth | http://arxiv.org/pdf/1403.1653v1.pdf | author:Marc D. Killpack category:cs.CV published:2014-03-07 summary:This report is a summary of research conducted on cloth tracking forautomated textile manufacturing during a two semester long research course atGeorgia Tech. This work was completed in 2009. Advances in current sensingtechnology such as the Microsoft Kinect would now allow me to relax certainassumptions and generally improve the tracking performance. This is because amajor part of my approach described in this paper was to track features in a 2Dimage and use these to estimate the cloth deformation. Innovations such as theKinect would improve estimation due to the automatic depth information obtainedwhen tracking 2D pixel locations. Additionally, higher resolution camera imageswould probably give better quality feature tracking. However, although I woulduse different technology now to implement this tracker, the algorithm describedand implemented in this paper is still a viable approach which is why I ampublishing this as a tech report for reference. In addition, although therelated work is a bit exhaustive, it will be useful to a reader who is new tomethods for tracking and estimation as well as modeling of cloth.
arxiv-5400-67 | Fast Training of Convolutional Networks through FFTs | http://arxiv.org/pdf/1312.5851v5.pdf | author:Michael Mathieu, Mikael Henaff, Yann LeCun category:cs.CV cs.LG cs.NE published:2013-12-20 summary:Convolutional networks are one of the most widely employed architectures incomputer vision and machine learning. In order to leverage their ability tolearn complex functions, large amounts of data are required for training.Training a large convolutional network to produce state-of-the-art results cantake weeks, even when using modern GPUs. Producing labels using a trainednetwork can also be costly when dealing with web-scale datasets. In this work,we present a simple algorithm which accelerates training and inference by asignificant factor, and can yield improvements of over an order of magnitudecompared to existing state-of-the-art implementations. This is done bycomputing convolutions as pointwise products in the Fourier domain whilereusing the same transformed feature map many times. The algorithm isimplemented on a GPU architecture and addresses a number of related challenges.
arxiv-5400-68 | Design a Persian Automated Plagiarism Detector (AMZPPD) | http://arxiv.org/pdf/1403.1618v1.pdf | author:Maryam Mahmoodi, Mohammad Mahmoodi Varnamkhasti category:cs.AI cs.CL published:2014-03-06 summary:Currently there are lots of plagiarism detection approaches. But few of themimplemented and adapted for Persian languages. In this paper, our work ondesigning and implementation of a plagiarism detection system based onpre-processing and NLP technics will be described. And the results of testingon a corpus will be presented.
arxiv-5400-69 | Retrieval of Experiments with Sequential Dirichlet Process Mixtures in Model Space | http://arxiv.org/pdf/1310.2125v2.pdf | author:Ritabrata Dutta, Sohan Seth, Samuel Kaski category:stat.ML cs.IR stat.AP published:2013-10-08 summary:We address the problem of retrieving relevant experiments given a queryexperiment, motivated by the public databases of datasets in molecular biologyand other experimental sciences, and the need of scientists to relate toearlier work on the level of actual measurement data. Since experiments areinherently noisy and databases ever accumulating, we argue that a retrievalengine should possess two particular characteristics. First, it should comparemodels learnt from the experiments rather than the raw measurements themselves:this allows incorporating experiment-specific prior knowledge to suppress noiseeffects and focus on what is important. Second, it should be updatedsequentially from newly published experiments, without explicitly storingeither the measurements or the models, which is critical for saving storagespace and protecting data privacy: this promotes life long learning. Weformulate the retrieval as a ``supermodelling'' problem, of sequentiallylearning a model of the set of posterior distributions, represented as sets ofMCMC samples, and suggest the use of Particle-Learning-based sequentialDirichlet process mixture (DPM) for this purpose. The relevance measure forretrieval is derived from the supermodel through the mixture representation. Wedemonstrate the performance of the proposed retrieval method on simulated dataand molecular biological experiments.
arxiv-5400-70 | Collaborative Filtering with Information-Rich and Information-Sparse Entities | http://arxiv.org/pdf/1403.1600v1.pdf | author:Kai Zhu, Rui Wu, Lei Ying, R. Srikant category:stat.ML cs.IT cs.LG math.IT published:2014-03-06 summary:In this paper, we consider a popular model for collaborative filtering inrecommender systems where some users of a website rate some items, such asmovies, and the goal is to recover the ratings of some or all of the unrateditems of each user. In particular, we consider both the clustering model, whereonly users (or items) are clustered, and the co-clustering model, where bothusers and items are clustered, and further, we assume that some users rate manyitems (information-rich users) and some users rate only a few items(information-sparse users). When users (or items) are clustered, our algorithmcan recover the rating matrix with $\omega(MK \log M)$ noisy entries while $MK$entries are necessary, where $K$ is the number of clusters and $M$ is thenumber of items. In the case of co-clustering, we prove that $K^2$ entries arenecessary for recovering the rating matrix, and our algorithm achieves thislower bound within a logarithmic factor when $K$ is sufficiently large. Wecompare our algorithms with a well-known algorithms called alternatingminimization (AM), and a similarity score-based algorithm known as thepopularity-among-friends (PAF) algorithm by applying all three to the MovieLensand Netflix data sets. Our co-clustering algorithm and AM have similar overallerror rates when recovering the rating matrix, both of which are lower than theerror rate under PAF. But more importantly, the error rate of our co-clusteringalgorithm is significantly lower than AM and PAF in the scenarios of interestin recommender systems: when recommending a few items to each user or whenrecommending items to users who only rated a few items (these users are themajority of the total user population). The performance difference increaseseven more when noise is added to the datasets.
arxiv-5400-71 | Discriminative Functional Connectivity Measures for Brain Decoding | http://arxiv.org/pdf/1402.5684v2.pdf | author:Orhan Firat, Mete Ozay, Ilke Oztekin, Fatos T. Yarman Vural category:cs.AI cs.CE cs.CV cs.LG published:2014-02-23 summary:We propose a statistical learning model for classifying cognitive processesbased on distributed patterns of neural activation in the brain, acquired viafunctional magnetic resonance imaging (fMRI). In the proposed learning method,local meshes are formed around each voxel. The distance between voxels in themesh is determined by using a functional neighbourhood concept. In order todefine the functional neighbourhood, the similarities between the time seriesrecorded for voxels are measured and functional connectivity matrices areconstructed. Then, the local mesh for each voxel is formed by including thefunctionally closest neighbouring voxels in the mesh. The relationship betweenthe voxels within a mesh is estimated by using a linear regression model. Theserelationship vectors, called Functional Connectivity aware Local RelationalFeatures (FC-LRF) are then used to train a statistical learning machine. Theproposed method was tested on a recognition memory experiment, including datapertaining to encoding and retrieval of words belonging to ten differentsemantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)and Support Vector Machine (SVM), are trained in order to predict the semanticcategory of the item being retrieved, based on activation patterns duringencoding. The classification performance of the Functional Mesh Learning model,which range in 62%-71% is superior to the classical multi-voxel patternanalysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.
arxiv-5400-72 | Nonlinear hyperspectral unmixing with robust nonnegative matrix factorization | http://arxiv.org/pdf/1401.5649v2.pdf | author:Cédric Févotte, Nicolas Dobigeon category:stat.ME stat.ML published:2014-01-22 summary:This paper introduces a robust mixing model to describe hyperspectral dataresulting from the mixture of several pure spectral signatures. This new modelnot only generalizes the commonly used linear mixing model, but also allows forpossible nonlinear effects to be easily handled, relying on mild assumptionsregarding these nonlinearities. The standard nonnegativity and sum-to-oneconstraints inherent to spectral unmixing are coupled with a group-sparseconstraint imposed on the nonlinearity component. This results in a new form ofrobust nonnegative matrix factorization. The data fidelity term is expressed asa beta-divergence, a continuous family of dissimilarity measures that takes thesquared Euclidean distance and the generalized Kullback-Leibler divergence asspecial cases. The penalized objective is minimized with a block-coordinatedescent that involves majorization-minimization updates. Simulation resultsobtained on synthetic and real data show that the proposed strategy competeswith state-of-the-art linear and nonlinear unmixing methods.
arxiv-5400-73 | New Perspectives on k-Support and Cluster Norms | http://arxiv.org/pdf/1403.1481v1.pdf | author:Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos category:stat.ML published:2014-03-06 summary:The $k$-support norm is a regularizer which has been successfully applied tosparse vector prediction problems. We show that it belongs to a general classof norms which can be formulated as a parameterized infimum over quadratics. Wefurther extend the $k$-support norm to matrices, and we observe that it is aspecial case of the matrix cluster norm. Using this formulation we derive anefficient algorithm to compute the proximity operator of both norms. Thisimproves upon the standard algorithm for the $k$-support norm and allows us toapply proximal gradient methods to the cluster norm. We also describe how tosolve regularization problems which employ centered versions of these norms.Finally, we apply the matrix regularizers to different matrix completion andmultitask learning datasets. Our results indicate that the spectral $k$-supportnorm and the cluster norm give state of the art performance on these problems,significantly outperforming trace norm and elastic net penalties.
arxiv-5400-74 | Real-Time Classification of Twitter Trends | http://arxiv.org/pdf/1403.1451v1.pdf | author:Arkaitz Zubiaga, Damiano Spina, Raquel Martínez, Víctor Fresno category:cs.IR cs.CL cs.SI published:2014-03-06 summary:Social media users give rise to social trends as they share about commoninterests, which can be triggered by different reasons. In this work, weexplore the types of triggers that spark trends on Twitter, introducing atypology with following four types: 'news', 'ongoing events', 'memes', and'commemoratives'. While previous research has analyzed trending topics in along term, we look at the earliest tweets that produce a trend, with the aim ofcategorizing trends early on. This would allow to provide a filtered subset oftrends to end users. We analyze and experiment with a set of straightforwardlanguage-independent features based on the social spread of trends tocategorize them into the introduced typology. Our method provides an efficientway to accurately categorize trending topics without need of external data,enabling news organizations to discover breaking news in real-time, or toquickly identify viral memes that might enrich marketing decisions, amongothers. The analysis of social features also reveals patterns associated witheach type of trend, such as tweets about ongoing events being shorter as manywere likely sent from mobile devices, or memes having more retweets originatingfrom a few trend-setters.
arxiv-5400-75 | Illumination,Expression and Occlusion Invariant Pose-Adaptive Face Recognition System for Real-Time Applications | http://arxiv.org/pdf/1403.1362v1.pdf | author:Shireesha Chintalapati, M. V. Raghunadh category:cs.CV published:2014-03-06 summary:Face recognition in real-time scenarios is mainly affected by illumination,expression and pose variations and also by occlusion. This paper presents theframework for pose adaptive component-based face recognition system. Theframework proposed deals with all the above mentioned issues. The stepsinvolved in the presented framework are (i) facial landmark localisation, (ii)facial component extraction, (iii) pre-processing of facial image (iv) facialpose estimation (v) feature extraction using Local Binary Pattern Histograms ofeach component followed by (vi) fusion of pose adaptive classification ofcomponents. By employing pose adaptive classification, the recognition processis carried out on some part of database, based on estimated pose, instead ofapplying the recognition process on the whole database. Pre-processingtechniques employed to overcome the problems due to illumination variation arealso discussed in this paper. Component-based techniques provide betterrecognition rates when face images are occluded compared to the holisticmethods. Our method is simple, feasible and provides better results whencompared to other holistic methods.
arxiv-5400-76 | Efficient Point-to-Subspace Query in $\ell^1$ with Application to Robust Object Instance Recognition | http://arxiv.org/pdf/1208.0432v3.pdf | author:Ju Sun, Yuqian Zhang, John Wright category:cs.CV cs.LG stat.ML published:2012-08-02 summary:Motivated by vision tasks such as robust face and object recognition, weconsider the following general problem: given a collection of low-dimensionallinear subspaces in a high-dimensional ambient (image) space, and a query point(image), efficiently determine the nearest subspace to the query in $\ell^1$distance. In contrast to the naive exhaustive search which entails large-scalelinear programs, we show that the computational burden can be cut downsignificantly by a simple two-stage algorithm: (1) projecting the query anddata-base subspaces into lower-dimensional space by random Cauchy matrix, andsolving small-scale distance evaluations (linear programs) in the projectionspace to locate candidate nearest; (2) with few candidates upon independentrepetition of (1), getting back to the high-dimensional space and performingexhaustive search. To preserve the identity of the nearest subspace withnontrivial probability, the projection dimension typically is low-orderpolynomial of the subspace dimension multiplied by logarithm of number of thesubspaces (Theorem 2.1). The reduced dimensionality and hence complexityrenders the proposed algorithm particularly relevant to vision application suchas robust face and object instance recognition that we investigate empirically.
arxiv-5400-77 | Collaborative Representation for Classification, Sparse or Non-sparse? | http://arxiv.org/pdf/1403.1353v1.pdf | author:Yang Wu, Vansteenberge Jarich, Masayuki Mukunoki, Michihiko Minoh category:cs.CV cs.AI cs.LG published:2014-03-06 summary:Sparse representation based classification (SRC) has been proved to be asimple, effective and robust solution to face recognition. As it gets popular,doubts on the necessity of enforcing sparsity starts coming up, and primaryexperimental results showed that simply changing the $l_1$-norm basedregularization to the computationally much more efficient $l_2$-norm basednon-sparse version would lead to a similar or even better performance. However,that's not always the case. Given a new classification task, it's still unclearwhich regularization strategy (i.e., making the coefficients sparse ornon-sparse) is a better choice without trying both for comparison. In thispaper, we present as far as we know the first study on solving this issue,based on plenty of diverse classification experiments. We propose a scoringfunction for pre-selecting the regularization strategy using only the datasetsize, the feature dimensionality and a discrimination score derived from agiven feature representation. Moreover, we show that when dictionary learningis taking into account, non-sparse representation has a more significantsuperiority to sparse representation. This work is expected to enrich ourunderstanding of sparse/non-sparse collaborative representation forclassification and motivate further research activities.
arxiv-5400-78 | Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction | http://arxiv.org/pdf/1403.1347v1.pdf | author:Jian Zhou, Olga G. Troyanskaya category:q-bio.QM cs.CE cs.LG published:2014-03-06 summary:Predicting protein secondary structure is a fundamental problem in proteinstructure prediction. Here we present a new supervised generative stochasticnetwork (GSN) based method to predict local secondary structure with deephierarchical representations. GSN is a recently proposed deep learningtechnique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generativemodel. We present the supervised extension of GSN, which learns a Markov chainto sample from a conditional distribution, and applied it to protein structureprediction. To scale the model to full-sized, high-dimensional data, likeprotein sequences with hundreds of amino acids, we introduce a convolutionalarchitecture, which allows efficient learning across multiple layers ofhierarchical representations. Our architecture uniquely focuses on predictingstructured low-level labels informed with both low and high-levelrepresentations learned by the model. In our application this corresponds tolabeling the secondary structure state of each amino-acid residue. We trainedand tested the model on separate sets of non-homologous proteins sharing lessthan 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513dataset, better than the previously reported best performance 64.9% (Wang etal., 2011) for this challenging secondary structure prediction problem.
arxiv-5400-79 | Minimax Optimal Bayesian Aggregation | http://arxiv.org/pdf/1403.1345v1.pdf | author:Yun Yang, David B. Dunson category:math.ST stat.ME stat.ML stat.TH published:2014-03-06 summary:It is generally believed that ensemble approaches, which combine multiplealgorithms or models, can outperform any single algorithm at machine learningtasks, such as prediction. In this paper, we propose Bayesian convex and linearaggregation approaches motivated by regression applications. We show that theproposed approach is minimax optimal when the true data-generating model is aconvex or linear combination of models in the list. Moreover, the method canadapt to sparsity structure in which certain models should receive zeroweights, and the method is tuning parameter free unlike competitors. Moregenerally, under an M-open view when the truth falls outside the space of allconvex/linear combinations, our theory suggests that the posterior measuretends to concentrate on the best approximation of the truth at the minimaxrate. We illustrate the method through simulation studies and severalapplications.
arxiv-5400-80 | An Extensive Repot on the Efficiency of AIS-INMACA (A Novel Integrated MACA based Clonal Classifier for Protein Coding and Promoter Region Prediction) | http://arxiv.org/pdf/1403.1336v1.pdf | author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG published:2014-03-06 summary:This paper exclusively reports the efficiency of AIS-INMACA. AIS-INMACA hascreated good impact on solving major problems in bioinformatics like proteinregion identification and promoter region prediction with less time (PokkuluriKiran Sree, 2014). This AIS-INMACA is now came with several variations(Pokkuluri Kiran Sree, 2014) towards projecting it as a tool in bioinformaticsfor solving many problems in bioinformatics. So this paper will be very muchuseful for so many researchers who are working in the domain of bioinformaticswith cellular automata.
arxiv-5400-81 | A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property | http://arxiv.org/pdf/1311.6091v3.pdf | author:Jianshu Chen, Li Deng category:cs.LG cs.NE published:2013-11-24 summary:We present an architecture of a recurrent neural network (RNN) with afully-connected deep neural network (DNN) as its feature extractor. The RNN isequipped with both causal temporal prediction and non-causal look-ahead, viaauto-regression (AR) and moving-average (MA), respectively. The focus of thispaper is a primal-dual training method that formulates the learning of the RNNas a formal optimization problem with an inequality constraint that provides asufficient condition for the stability of the network dynamics. Experimentalresults demonstrate the effectiveness of this new method, which achieves 18.86%phone recognition error on the TIMIT benchmark for the core test set. Theresult approaches the best result of 17.7%, which was obtained by using RNNwith long short-term memory (LSTM). The results also show that the proposedprimal-dual training method produces lower recognition errors than the popularRNN methods developed earlier based on the carefully tuned threshold parameterthat heuristically prevents the gradient from exploding.
arxiv-5400-82 | Integer Programming Relaxations for Integrated Clustering and Outlier Detection | http://arxiv.org/pdf/1403.1329v1.pdf | author:Lionel Ott, Linsey Pang, Fabio Ramos, David Howe, Sanjay Chawla category:cs.LG published:2014-03-06 summary:In this paper we present methods for exemplar based clustering with outlierselection based on the facility location formulation. Given a distance functionand the number of outliers to be found, the methods automatically determine thenumber of clusters and outliers. We formulate the problem as an integer programto which we present relaxations that allow for solutions that scale to largedata sets. The advantages of combining clustering and outlier selectioninclude: (i) the resulting clusters tend to be compact and semanticallycoherent (ii) the clusters are more robust against data perturbations and (iii)the outliers are contextualised by the clusters and more interpretable, i.e. itis easier to distinguish between outliers which are the result of data errorsfrom those that may be indicative of a new pattern emergent in the data. Wepresent and contrast three relaxations to the integer program formulation: (i)a linear programming formulation (LP) (ii) an extension of affinity propagationto outlier detection (APOC) and (iii) a Lagrangian duality based formulation(LD). Evaluation on synthetic as well as real data shows the quality andscalability of these different methods.
arxiv-5400-83 | Multi-view Face Analysis Based on Gabor Features | http://arxiv.org/pdf/1403.1327v1.pdf | author:Hongli Liu, Weifeng Liu, Yanjiang Wang category:cs.CV published:2014-03-06 summary:Facial analysis has attracted much attention in the technology forhuman-machine interface. Different methods of classification based on sparserepresentation and Gabor kernels have been widely applied in the fields offacial analysis. However, most of these methods treat face from a whole viewstandpoint. In terms of the importance of different facial views, in thispaper, we present multi-view face analysis based on sparse representation andGabor wavelet coefficients. To evaluate the performance, we conduct faceanalysis experiments including face recognition (FR) and face expressionrecognition (FER) on JAFFE database. Experiments are conducted from two parts:(1) Face images are divided into three facial parts which are forehead, eye andmouth. (2) Face images are divided into 8 parts by the orientation of Gaborkernels. Experimental results demonstrate that the proposed methods cansignificantly boost the performance and perform better than the other methods.
arxiv-5400-84 | Authorship detection of SMS messages using unigrams | http://arxiv.org/pdf/1403.1314v1.pdf | author:R. G. Ragel, P. Herath, U. Senanayake category:cs.CL cs.IR published:2014-03-06 summary:SMS messaging is a popular media of communication. Because of its popularityand privacy, it could be used for many illegal purposes. Additionally, sincethey are part of the day to day life, SMSes can be used as evidence for manylegal disputes. Since a cellular phone might be accessible to people close tothe owner, it is important to establish the fact that the sender of the messageis indeed the owner of the phone. For this purpose, the straight forwardsolutions seem to be the use of popular stylometric methods. However, incomparison with the data used for stylometry in the literature, SMSes haveunusual characteristics making it hard or impossible to apply these methods ina conventional way. Our target is to come up with a method of authorshipdetection of SMS messages that could still give a usable accuracy. We arguethat, considering the methods of author attribution, the best method that couldbe applied to SMS messages is an n-gram method. To prove our point, we checkedtwo different methods of distribution comparison with varying number oftraining and testing data. We specifically try to compare how well ouralgorithms work under less amount of testing data and large number of candidateauthors (which we believe to be the real world scenario) against controlledtests with less number of authors and selected SMSes with large number ofwords. To counter the lack of information in an SMS message, we propose themethod of stacking together few SMSes.
arxiv-5400-85 | AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based Assignments | http://arxiv.org/pdf/1403.1310v1.pdf | author:M. A. C. Jiffriya, M. A. C. Akmal Jahan, R. G. Ragel, S. Deegalla category:cs.IR cs.CL cs.DL published:2014-03-06 summary:Plagiarism is one of the growing issues in academia and is always a concernin Universities and other academic institutions. The situation is becoming evenworse with the availability of ample resources on the web. This paper focuseson creating an effective and fast tool for plagiarism detection for text basedelectronic assignments. Our plagiarism detection tool named AntiPlag isdeveloped using the tri-gram sequence matching technique. Three sets of textbased assignments were tested by AntiPlag and the results were compared againstan existing commercial plagiarism detection tool. AntiPlag showed betterresults in terms of false positives compared to the commercial tool due to thepre-processing steps performed in AntiPlag. In addition, to improve thedetection latency, AntiPlag applies a data clustering technique making it fourtimes faster than the commercial tool considered. AntiPlag could be used toisolate plagiarized text based assignments from non-plagiarised assignmentseasily. Therefore, we present AntiPlag, a fast and effective tool forplagiarism detection on text based electronic assignments.
arxiv-5400-86 | Local Similarities, Global Coding: An Algorithm for Feature Coding and its Applications | http://arxiv.org/pdf/1311.6079v2.pdf | author:Amirreza Shaban, Hamid R. Rabiee, Mahyar Najibi category:cs.CV cs.AI published:2013-11-24 summary:Data coding as a building block of several image processing algorithms hasbeen received great attention recently. Indeed, the importance of the localityassumption in coding approaches is studied in numerous works and severalmethods are proposed based on this concept. We probe this assumption and claimthat taking the similarity between a data point and a more global set of anchorpoints does not necessarily weaken the coding method as long as the underlyingstructure of the anchor points are taken into account. Based on this fact, wepropose to capture this underlying structure by assuming a random walker overthe anchor points. We show that our method is a fast approximate learningalgorithm based on the diffusion map kernel. The experiments on variousdatasets show that making different state-of-the-art coding algorithms aware ofthis structure boosts them in different learning tasks.
arxiv-5400-87 | On Fast Dropout and its Applicability to Recurrent Networks | http://arxiv.org/pdf/1311.0701v7.pdf | author:Justin Bayer, Christian Osendorfer, Daniela Korhammer, Nutan Chen, Sebastian Urban, Patrick van der Smagt category:stat.ML cs.LG cs.NE published:2013-11-04 summary:Recurrent Neural Networks (RNNs) are rich models for the processing ofsequential data. Recent work on advancing the state of the art has been focusedon the optimization or modelling of RNNs, mostly motivated by adressing theproblems of the vanishing and exploding gradients. The control of overfittinghas seen considerably less attention. This paper contributes to that byanalyzing fast dropout, a recent regularization method for generalized linearmodels and neural networks from a back-propagation inspired perspective. Weshow that fast dropout implements a quadratic form of an adaptive,per-parameter regularizer, which rewards large weights in the light ofunderfitting, penalizes them for overconfident predictions and vanishes atminima of an unregularized training loss. The derivatives of that regularizerare exclusively based on the training error signal. One consequence of this isthe absense of a global weight attractor, which is particularly appealing forRNNs, since the dynamics are not biased towards a certain regime. We positivelytest the hypothesis that this improves the performance of RNNs on four musicaldata sets.
arxiv-5400-88 | Electricity Market Forecasting via Low-Rank Multi-Kernel Learning | http://arxiv.org/pdf/1310.0865v2.pdf | author:Vassilis Kekatos, Yu Zhang, Georgios B. Giannakis category:stat.ML cs.LG cs.SY published:2013-10-02 summary:The smart grid vision entails advanced information technology and dataanalytics to enhance the efficiency, sustainability, and economics of the powergrid infrastructure. Aligned to this end, modern statistical learning tools areleveraged here for electricity market inference. Day-ahead price forecasting iscast as a low-rank kernel learning problem. Uniquely exploiting the marketclearing process, congestion patterns are modeled as rank-one components in thematrix of spatio-temporally varying prices. Through a novel nuclear norm-basedregularization, kernels across pricing nodes and hours can be systematicallyselected. Even though market-wide forecasting is beneficial from a learningperspective, it involves processing high-dimensional market data. The latterbecomes possible after devising a block-coordinate descent algorithm forsolving the non-convex optimization problem involved. The algorithm utilizesresults from block-sparse vector recovery and is guaranteed to converge to astationary point. Numerical tests on real data from the Midwest ISO (MISO)market corroborate the prediction accuracy, computational efficiency, and theinterpretative merits of the developed approach over existing alternatives.
arxiv-5400-89 | Latent Semantic Word Sense Disambiguation Using Global Co-occurrence Information | http://arxiv.org/pdf/1403.1194v1.pdf | author:Minoru Sasaki category:cs.CL cs.IR published:2014-03-05 summary:In this paper, I propose a novel word sense disambiguation method based onthe global co-occurrence information using NMF. When I calculate the dependencyrelation matrix, the existing method tends to produce very sparse co-occurrencematrix from a small training set. Therefore, the NMF algorithm sometimes doesnot converge to desired solutions. To obtain a large number of co-occurrencerelations, I propose to use co-occurrence frequencies of dependency relationsbetween word features in the whole training set. This enables us to solve datasparseness problem and induce more effective latent features. To evaluate theefficiency of the method of word sense disambiguation, I make some experimentsto compare with the result of the two baseline methods. The results of theexperiments show this method is effective for word sense disambiguation incomparison with the all baseline methods. Moreover, the proposed method iseffective for obtaining a stable effect by analyzing the global co-occurrenceinformation.
arxiv-5400-90 | Is getting the right answer just about choosing the right words? The role of syntactically-informed features in short answer scoring | http://arxiv.org/pdf/1403.0801v2.pdf | author:Derrick Higgins, Chris Brew, Michael Heilman, Ramon Ziai, Lei Chen, Aoife Cahill, Michael Flor, Nitin Madnani, Joel Tetreault, Daniel Blanchard, Diane Napolitano, Chong Min Lee, John Blackmore category:cs.CL published:2014-03-04 summary:Developments in the educational landscape have spurred greater interest inthe problem of automatically scoring short answer questions. A recent sharedtask on this topic revealed a fundamental divide in the modeling approachesthat have been applied to this problem, with the best-performing systems splitbetween those that employ a knowledge engineering approach and those thatalmost solely leverage lexical information (as opposed to higher-levelsyntactic information) in assigning a score to a given response. This paperaims to introduce the NLP community to the largest corpus currently availablefor short-answer scoring, provide an overview of methods used in the sharedtask using this data, and explore the extent to which moresyntactically-informed features can contribute to the short answer scoring taskin a way that avoids the question-specific manual effort of the knowledgeengineering approach.
arxiv-5400-91 | Artificial Neuron Modelling Based on Wave Shape | http://arxiv.org/pdf/1403.1073v1.pdf | author:Kieran Greer category:cs.NE published:2014-03-05 summary:This paper describes a new model for an artificial neural network processingunit or neuron. It is slightly different to a traditional feedforward networkby the fact that it favours a mechanism of trying to match the wave-like'shape' of the input with the shape of the output against specific value errorcorrections. The expectation is then that a best fit shape can be transposedinto the desired output values more easily. This allows for notions ofreinforcement through resonance and also the construction of synapses.
arxiv-5400-92 | Fast methods for denoising matrix completion formulations, with applications to robust seismic data interpolation | http://arxiv.org/pdf/1302.4886v3.pdf | author:Aleksandr Y. Aravkin, Rajiv Kumar, Hassan Mansour, Ben Recht, Felix J. Herrmann category:stat.ML cs.LG 62F35, 65K10 published:2013-02-20 summary:Recent SVD-free matrix factorization formulations have enabled rankminimization for systems with millions of rows and columns, paving the way formatrix completion in extremely large-scale applications, such as seismic datainterpolation. In this paper, we consider matrix completion formulations designed to hit atarget data-fitting error level provided by the user, and propose an algorithmcalled LR-BPDN that is able to exploit factorized formulations to solve thecorresponding optimization problem. Since practitioners typically have strongprior knowledge about target error level, this innovation makes it easy toapply the algorithm in practice, leaving only the factor rank to be determined. Within the established framework, we propose two extensions that are highlyrelevant to solving practical challenges of data interpolation. First, wepropose a weighted extension that allows known subspace information to improvethe results of matrix completion formulations. We show how this weighting canbe used in the context of frequency continuation, an essential aspect toseismic data interpolation. Second, we propose matrix completion formulationsthat are robust to large measurement errors in the available data. We illustrate the advantages of LR-BPDN on the collaborative filteringproblem using the MovieLens 1M, 10M, and Netflix 100M datasets. Then, we usethe new method, along with its robust and subspace re-weighted extensions, toobtain high-quality reconstructions for large scale seismic interpolationproblems with real data, even in the presence of data contamination.
arxiv-5400-93 | K-Tangent Spaces on Riemannian Manifolds for Improved Pedestrian Detection | http://arxiv.org/pdf/1403.1056v1.pdf | author:Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell category:cs.CV published:2014-03-05 summary:For covariance-based image descriptors, taking into account the curvature ofthe corresponding feature space has been shown to improve discriminationperformance. This is often done through representing the descriptors as pointson Riemannian manifolds, with the discrimination accomplished on a tangentspace. However, such treatment is restrictive as distances between arbitrarypoints on the tangent space do not represent true geodesic distances, and hencedo not represent the manifold structure accurately. In this paper we propose ageneral discriminative model based on the combination of several tangentspaces, in order to preserve more details of the structure. The model can beused as a weak learner in a boosting-based pedestrian detection framework.Experiments on the challenging INRIA and DaimlerChrysler datasets show that theproposed model leads to considerably higher performance than methods based onhistograms of oriented gradients as well as previous Riemannian-basedtechniques.
arxiv-5400-94 | High-Accuracy Total Variation for Compressed Video Sensing | http://arxiv.org/pdf/1309.0270v2.pdf | author:Mahdi S. Hosseini, Konstantinos N. Plataniotis category:math.OC cs.CV published:2013-09-01 summary:Numerous total variation (TV) regularizers, engaged in image restorationproblem, encode the gradients by means of simple $[-1,1]$ FIR filter. Despiteits low computational processing, this filter severely deviates signal's highfrequency components pertinent to edge/discontinuous information and causeseveral deficiency issues known as texture and geometric loss. This paperaddresses this problem by proposing an alternative model to the TVregularization problem via high order accuracy differential FIR filters topreserve rapid transitions in signal recovery. A numerical encoding scheme isdesigned to extend the TV model into multidimensional representation (tensorialdecomposition). We adopt this design to regulate the spatial and temporalredundancy in compressed video sensing problem to jointly recover frames fromunder-sampled measurements. We then seek the solution via alternating directionmethods of multipliers and find a unique solution to quadratic minimizationstep with capability of handling different boundary conditions. The resultingalgorithm uses much lower sampling rate and highly outperforms alternativestate-of-the-art methods. This is evaluated both in terms of restorationaccuracy and visual quality of the recovered frames.
arxiv-5400-95 | Exploiting the Statistics of Learning and Inference | http://arxiv.org/pdf/1402.7025v2.pdf | author:Max Welling category:cs.LG published:2014-02-26 summary:When dealing with datasets containing a billion instances or with simulationsthat require a supercomputer to execute, computational resources become part ofthe equation. We can improve the efficiency of learning and inference byexploiting their inherent statistical nature. We propose algorithms thatexploit the redundancy of data relative to a model by subsampling data-casesfor every update and reasoning about the uncertainty created in this process.In the context of learning we propose to test for the probability that astochastically estimated gradient points more than 180 degrees in the wrongdirection. In the context of MCMC sampling we use stochastic gradients toimprove the efficiency of MCMC updates, and hypothesis tests based on adaptivemini-batches to decide whether to accept or reject a proposed parameter update.Finally, we argue that in the context of likelihood free MCMC one needs tostore all the information revealed by all simulations, for instance in aGaussian process. We conclude that Bayesian methods will remain to play acrucial role in the era of big data and big simulations, but only if weovercome a number of computational challenges.
arxiv-5400-96 | Dynamic stochastic blockmodels for time-evolving social networks | http://arxiv.org/pdf/1403.0921v1.pdf | author:Kevin S. Xu, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ME G.3; G.2.2 published:2014-03-04 summary:Significant efforts have gone into the development of statistical models foranalyzing data in the form of networks, such as social networks. Most existingwork has focused on modeling static networks, which represent either a singletime snapshot or an aggregate view over time. There has been recent interest instatistical modeling of dynamic networks, which are observed at multiple pointsin time and offer a richer representation of many complex phenomena. In thispaper, we present a state-space model for dynamic networks that extends thewell-known stochastic blockmodel for static networks to the dynamic setting. Wefit the model in a near-optimal manner using an extended Kalman filter (EKF)augmented with a local search. We demonstrate that the EKF-based algorithmperforms competitively with a state-of-the-art algorithm based on Markov chainMonte Carlo sampling but is significantly less computationally demanding.
arxiv-5400-97 | One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling | http://arxiv.org/pdf/1312.3005v3.pdf | author:Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson category:cs.CL published:2013-12-11 summary:We propose a new benchmark corpus to be used for measuring progress instatistical language modeling. With almost one billion words of training data,we hope this benchmark will be useful to quickly evaluate novel languagemodeling techniques, and to compare their contribution when combined with otheradvanced techniques. We show performance of several well-known types oflanguage models, with the best results achieved with a recurrent neural networkbased language model. The baseline unpruned Kneser-Ney 5-gram model achievesperplexity 67.6; a combination of techniques leads to 35% reduction inperplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scriptsneeded to rebuild the training/held-out data, it also makes availablelog-probability values for each word in each of ten held-out data sets, foreach of the baseline n-gram models.
arxiv-5400-98 | Matroid Regression | http://arxiv.org/pdf/1403.0873v1.pdf | author:Franz J Király, Louis Theran category:math.ST cs.DM cs.LG stat.ME stat.ML stat.TH published:2014-03-04 summary:We propose an algebraic combinatorial method for solving large sparse linearsystems of equations locally - that is, a method which can compute singleevaluations of the signal without computing the whole signal. The method scalesonly in the sparsity of the system and not in its size, and allows to provideerror estimates for any solution method. At the heart of our approach is theso-called regression matroid, a combinatorial object associated to sparsitypatterns, which allows to replace inversion of the large matrix with theinversion of a kernel matrix that is constant size. We show that our methodprovides the best linear unbiased estimator (BLUE) for this setting and theminimum variance unbiased estimator (MVUE) under Gaussian noise assumptions,and furthermore we show that the size of the kernel matrix which is to beinverted can be traded off with accuracy.
arxiv-5400-99 | Decomposition of Optical Flow on the Sphere | http://arxiv.org/pdf/1312.4354v2.pdf | author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV published:2013-12-16 summary:We propose a number of variational regularisation methods for the estimationand decomposition of motion fields on the $2$-sphere. While motion estimationis based on the optical flow equation, the presented decomposition models aremotivated by recent trends in image analysis. In particular we treat $u+v$decomposition as well as hierarchical decomposition. Helmholtz decomposition ofmotion fields is obtained as a natural by-product of the chosen numericalmethod based on vector spherical harmonics. All models are tested on time-lapsemicroscopy data depicting fluorescently labelled endodermal cells of azebrafish embryo.
arxiv-5400-100 | Dynamic Move Chains -- a Forward Pruning Approach to Tree Search in Computer Chess | http://arxiv.org/pdf/1403.0778v1.pdf | author:Kieran Greer category:cs.AI cs.NE published:2014-03-04 summary:This paper proposes a new mechanism for pruning a search game-tree incomputer chess. The algorithm stores and then reuses chains or sequences ofmoves, built up from previous searches. These move sequences have a built-inforward-pruning mechanism that can radically reduce the search space. A typicalsearch process might retrieve a move from a Transposition Table, where thedecision of what move to retrieve would be based on the position itself. Thisalgorithm stores move sequences based on what previous sequences were better,or caused cutoffs. This is therefore position independent and so it could alsobe useful in games with imperfect information or uncertainty, where the wholesituation is not known at any one time. Over a small set of tests, thealgorithm was shown to clearly out-perform Transposition Tables, both in termsof search reduction and game-play results.
arxiv-5400-101 | EnsembleSVM: A Library for Ensemble Learning Using Support Vector Machines | http://arxiv.org/pdf/1403.0745v1.pdf | author:Marc Claesen, Frank De Smet, Johan Suykens, Bart De Moor category:stat.ML cs.LG published:2014-03-04 summary:EnsembleSVM is a free software package containing efficient routines toperform ensemble learning with support vector machine (SVM) base models. Itcurrently offers ensemble methods based on binary SVM models. Ourimplementation avoids duplicate storage and evaluation of support vectors whichare shared between constituent models. Experimental results show that usingensemble approaches can drastically reduce training complexity whilemaintaining high predictive accuracy. The EnsembleSVM software package isfreely available online at http://esat.kuleuven.be/stadius/ensemblesvm.
arxiv-5400-102 | On the Intersection Property of Conditional Independence and its Application to Causal Discovery | http://arxiv.org/pdf/1403.0408v2.pdf | author:Jonas Peters category:math.PR stat.ML published:2014-03-03 summary:This work investigates the intersection property of conditional independence.It states that for random variables $A,B,C$ and $X$ we have that $X$independent of $A$ given $B,C$ and $X$ independent of $B$ given $A,C$ implies$X$ independent of $(A,B)$ given $C$. Under the assumption that the jointdistribution has a continuous density, we provide necessary and sufficientconditions under which the intersection property holds. The result has directapplications to causal inference: it leads to strictly weaker conditions underwhich the graphical structure becomes identifiable from the joint distributionof an additive noise model.
arxiv-5400-103 | A Novel Method for Vectorization | http://arxiv.org/pdf/1403.0728v1.pdf | author:Tolga Birdal, Emrah Bala category:cs.CV cs.CG cs.GR published:2014-03-04 summary:Vectorization of images is a key concern uniting computer graphics andcomputer vision communities. In this paper we are presenting a novel idea forefficient, customizable vectorization of raster images, based on Catmull Romspline fitting. The algorithm maintains a good balance between photo-realismand photo abstraction, and hence is applicable to applications with artisticconcerns or applications where less information loss is crucial. The resultingalgorithm is fast, parallelizable and can satisfy general soft realtimerequirements. Moreover, the smoothness of the vectorized images aestheticallyoutperforms outputs of many polygon-based methods
arxiv-5400-104 | Random Projections on Manifolds of Symmetric Positive Definite Matrices for Image Classification | http://arxiv.org/pdf/1403.0700v1.pdf | author:Azadeh Alavi, Arnold Wiliem, Kun Zhao, Brian C. Lovell, Conrad Sanderson category:cs.CV stat.ML published:2014-03-04 summary:Recent advances suggest that encoding images through Symmetric PositiveDefinite (SPD) matrices and then interpreting such matrices as points onRiemannian manifolds can lead to increased classification performance. Takinginto account manifold geometry is typically done via (1) embedding themanifolds in tangent spaces, or (2) embedding into Reproducing Kernel HilbertSpaces (RKHS). While embedding into tangent spaces allows the use of existingEuclidean-based learning algorithms, manifold shape is only approximated whichcan cause loss of discriminatory information. The RKHS approach retains more ofthe manifold structure, but may require non-trivial effort to kerneliseEuclidean-based learning algorithms. In contrast to the above approaches, inthis paper we offer a novel solution that allows SPD matrices to be used withunmodified Euclidean-based learning algorithms, with the true manifold shapewell-preserved. Specifically, we propose to project SPD matrices using a set ofrandom projection hyperplanes over RKHS into a random projection space, whichleads to representing each matrix as a vector of projection coefficients.Experiments on face recognition, person re-identification and textureclassification show that the proposed approach outperforms several recentmethods, such as Tensor Sparse Coding, Histogram Plus Epitome, RiemannianLocality Preserving Projection and Relational Divergence Classification.
arxiv-5400-105 | Multi-Shot Person Re-Identification via Relational Stein Divergence | http://arxiv.org/pdf/1403.0699v1.pdf | author:Azadeh Alavi, Yan Yang, Mehrtash Harandi, Conrad Sanderson category:cs.CV stat.ML published:2014-03-04 summary:Person re-identification is particularly challenging due to significantappearance changes across separate camera views. In order to re-identifypeople, a representative human signature should effectively handle differencesin illumination, pose and camera parameters. While general appearance-basedmethods are modelled in Euclidean spaces, it has been argued that someapplications in image and video analysis are better modelled via non-Euclideanmanifold geometry. To this end, recent approaches represent images ascovariance matrices, and interpret such matrices as points on Riemannianmanifolds. As direct classification on such manifolds can be difficult, in thispaper we propose to represent each manifold point as a vector of similaritiesto class representers, via a recently introduced form of Bregman matrixdivergence known as the Stein divergence. This is followed by using adiscriminative mapping of similarity vectors for final classification. The useof similarity vectors is in contrast to the traditional approach of embeddingmanifolds into tangent spaces, which can suffer from representing the manifoldstructure inaccurately. Comparative evaluations on benchmark ETHZ and iLIDSdatasets for the person re-identification task show that the proposed approachobtains better performance than recent techniques such as Histogram PlusEpitome, Partial Least Squares, and Symmetry-Driven Accumulation of LocalFeatures.
arxiv-5400-106 | Network In Network | http://arxiv.org/pdf/1312.4400v3.pdf | author:Min Lin, Qiang Chen, Shuicheng Yan category:cs.NE cs.CV cs.LG published:2013-12-16 summary:We propose a novel deep network structure called "Network In Network" (NIN)to enhance model discriminability for local patches within the receptive field.The conventional convolutional layer uses linear filters followed by anonlinear activation function to scan the input. Instead, we build micro neuralnetworks with more complex structures to abstract the data within the receptivefield. We instantiate the micro neural network with a multilayer perceptron,which is a potent function approximator. The feature maps are obtained bysliding the micro networks over the input in a similar manner as CNN; they arethen fed into the next layer. Deep NIN can be implemented by stacking mutipleof the above described structure. With enhanced local modeling via the micronetwork, we are able to utilize global average pooling over feature maps in theclassification layer, which is easier to interpret and less prone tooverfitting than traditional fully connected layers. We demonstrated thestate-of-the-art classification performances with NIN on CIFAR-10 andCIFAR-100, and reasonable performances on SVHN and MNIST datasets.
arxiv-5400-107 | An Efficient Method for Face Recognition System In Various Assorted Conditions | http://arxiv.org/pdf/1403.5475v1.pdf | author:V. Karthikeyan, K. Vijayalakshmi, P. Jeyakumar category:cs.CV published:2014-03-04 summary:In the beginning stage, face verification is done using easy method ofgeometric algorithm models, but the verification route has now developed into ascientific progress of complicated geometric representation and identicalprocedure. In recent years the technologies have boosted face recognitionsystem into the healthy focus. Researchers currently undergoing strong researchon finding face recognition system for wider area information taken underhysterical elucidation dissimilarity. The proposed face recognition systemconsists of a narrative expositionindiscreet preprocessing method, a hybridFourier-based facial feature extraction and a score fusion scheme. We haveverified the face recognition in different lightening conditions (day or night)and at different locations (indoor or outdoor). Preprocessing, Image detection,Feature- extraction and Face recognition are the methods used for faceverification system. This paper focuses mainly on the issue of toughness tolighting variations. The proposed system has obtained an average of 88.1%verification rate on Two-Dimensional images under different lighteningconditions.
arxiv-5400-108 | Multi-period Trading Prediction Markets with Connections to Machine Learning | http://arxiv.org/pdf/1403.0648v1.pdf | author:Jinli Hu, Amos Storkey category:cs.GT cs.LG q-fin.TR stat.ML published:2014-03-04 summary:We present a new model for prediction markets, in which we use risk measuresto model agents and introduce a market maker to describe the trading process.This specific choice on modelling tools brings us mathematical convenience. Theanalysis shows that the whole market effectively approaches a global objective,despite that the market is designed such that each agent only cares about itsown goal. Additionally, the market dynamics provides a sensible algorithm foroptimising the global objective. An intimate connection between machinelearning and our markets is thus established, such that we could 1) analyse amarket by applying machine learning methods to the global objective, and 2)solve machine learning problems by setting up and running certain markets.
arxiv-5400-109 | Global solar irradiation prediction using a multi-gene genetic programming approach | http://arxiv.org/pdf/1403.0623v1.pdf | author:Indranil Pan, Daya Shankar Pandey, Saptarshi Das category:cs.NE cs.CE stat.AP published:2014-03-03 summary:In this paper, a nonlinear symbolic regression technique using anevolutionary algorithm known as multi-gene genetic programming (MGGP) isapplied for a data-driven modelling between the dependent and the independentvariables. The technique is applied for modelling the measured global solarirradiation and validated through numerical simulations. The proposed modellingtechnique shows improved results over the fuzzy logic and artificial neuralnetwork (ANN) based approaches as attempted by contemporary researchers. Themethod proposed here results in nonlinear analytical expressions, unlike thosewith neural networks which is essentially a black box modelling approach. Thisadditional flexibility is an advantage from the modelling perspective and helpsto discern the important variables which affect the prediction. Due to theevolutionary nature of the algorithm, it is able to get out of local minima andconverge to a global optimum unlike the back-propagation (BP) algorithm usedfor training neural networks. This results in a better percentage fit than theones obtained using neural networks by contemporary researchers. Also ahold-out cross validation is done on the obtained genetic programming (GP)results which show that the results generalize well to new data and do notover-fit the training samples. The multi-gene GP results are compared withthose, obtained using its single-gene version and also the same with fourclassical regression models in order to show the effectiveness of the adoptedapproach.
arxiv-5400-110 | The Structurally Smoothed Graphlet Kernel | http://arxiv.org/pdf/1403.0598v1.pdf | author:Pinar Yanardag, S. V. N. Vishwanathan category:cs.LG published:2014-03-03 summary:A commonly used paradigm for representing graphs is to use a vector thatcontains normalized frequencies of occurrence of certain motifs or sub-graphs.This vector representation can be used in a variety of applications, such as,for computing similarity between graphs. The graphlet kernel of Shervashidze etal. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj[28]) as motifs in the vector representation, and computes the kernel via a dotproduct between these vectors. One can easily show that this is a valid kernelbetween graphs. However, such a vector representation suffers from a fewdrawbacks. As k becomes larger we encounter the sparsity problem; most higherorder graphlets will not occur in a given graph. This leads to diagonaldominance, that is, a given graph is similar to itself but not to any othergraph in the dataset. On the other hand, since lower order graphlets tend to bemore numerous, using lower values of k does not provide enough discriminationability. We propose a smoothing technique to tackle the above problems. Ourmethod is based on a novel extension of Kneser-Ney and Pitman-Yor smoothingtechniques from natural language processing to graphs. We use the relationshipsbetween lower order and higher order graphlets in order to derive our method.Consequently, our smoothing algorithm not only respects the dependency betweensub-graphs but also tackles the diagonal dominance problem by distributing theprobability mass across graphlets. In our experiments, the smoothed graphletkernel outperforms graph kernels based on raw frequency counts.
arxiv-5400-111 | Representing, reasoning and answering questions about biological pathways - various applications | http://arxiv.org/pdf/1403.0541v1.pdf | author:Saadat Anwar category:cs.AI cs.CE cs.CL published:2014-03-03 summary:Biological organisms are composed of numerous interconnected biochemicalprocesses. Diseases occur when normal functionality of these processes isdisrupted. Thus, understanding these biochemical processes and theirinterrelationships is a primary task in biomedical research and a prerequisitefor diagnosing diseases, and drug development. Scientists studying theseprocesses have identified various pathways responsible for drug metabolism, andsignal transduction, etc. Newer techniques and speed improvements have resulted in deeper knowledgeabout these pathways, resulting in refined models that tend to be large andcomplex, making it difficult for a person to remember all aspects of it. Thus,computer models are needed to analyze them. We want to build such a system thatallows modeling of biological systems and pathways in such a way that we cananswer questions about them. Many existing models focus on structural and/or factoid questions, usingsurface-level knowledge that does not require understanding the underlyingmodel. We believe these are not the kind of questions that a biologist may asksomeone to test their understanding of the biological processes. We want oursystem to answer the kind of questions a biologist may ask. Such questionsappear in early college level text books. Thus the main goal of our thesis is to develop a system that allows us toencode knowledge about biological pathways and answer such questions about themdemonstrating understanding of the pathway. To that end, we develop a languagethat will allow posing such questions and illustrate the utility of ourframework with various applications in the biological domain. We use someexisting tools with modifications to accomplish our goal. Finally, we apply our system to real world applications by extracting pathwayknowledge from text and answering questions related to drug development.
arxiv-5400-112 | We Tweet Like We Talk and Other Interesting Observations: An Analysis of English Communication Modalities | http://arxiv.org/pdf/1403.0531v1.pdf | author:Josiah P. Zayner category:cs.CL published:2014-03-03 summary:Modalities of communication for human beings are gradually increasing innumber with the advent of new forms of technology. Many human beings canreadily transition between these different forms of communication with littleor no effort, which brings about the question: How similar are these differentcommunication modalities? To understand technology$\text{'}$s influence onEnglish communication, four different corpora were analyzed and compared:Writing from Books using the 1-grams database from the Google Books project,Twitter, IRC Chat, and transcribed Talking. Multi-word confusion matricesrevealed that Talking has the most similarity when compared to the other modesof communication, while 1-grams were the least similar form of communicationanalyzed. Based on the analysis of word usage, word usage frequencydistributions, and word class usage, among other things, Talking is also themost similar to Twitter and IRC Chat. This suggests that communicating usingTwitter and IRC Chat evolved from Talking rather than Writing. When wecommunicate online, even though we are writing, we do not Tweet or Chat how wewrite books; we Tweet and Chat how we Speak. Nonfiction and Fiction writingwere clearly differentiable from our analysis with Twitter and Chat being muchmore similar to Fiction than Nonfiction writing. These hypotheses were thentested using author and journalists Cory Doctorow. Mr. Doctorow$\text{'}$sWriting, Twitter usage, and Talking were all found to have very similarvocabulary usage patterns as the amalgamized populations, as long as thewriting was Fiction. However, Mr. Doctorow$\text{'}$s Nonfiction writing isdifferent from 1-grams and other collected Nonfiction writings. This data couldperhaps be used to create more entertaining works of Nonfiction.
arxiv-5400-113 | A Primal Dual Active Set with Continuation Algorithm for the \ell^0-Regularized Optimization Problem | http://arxiv.org/pdf/1403.0515v1.pdf | author:Yuling Jiao, Bangti Jin, Xiliang Lu category:math.OC cs.IT math.IT stat.ML published:2014-03-03 summary:We develop a primal dual active set with continuation algorithm for solvingthe \ell^0-regularized least-squares problem that frequently arises incompressed sensing. The algorithm couples the the primal dual active set methodwith a continuation strategy on the regularization parameter. At each inneriteration, it first identifies the active set from both primal and dualvariables, and then updates the primal variable by solving a (typically small)least-squares problem defined on the active set, from which the dual variablecan be updated explicitly. Under certain conditions on the sensing matrix,i.e., mutual incoherence property or restricted isometry property, and thenoise level, the finite step global convergence of the algorithm isestablished. Extensive numerical examples are presented to illustrate theefficiency and accuracy of the algorithm and the convergence analysis.
arxiv-5400-114 | Face Recognition Methods & Applications | http://arxiv.org/pdf/1403.0485v1.pdf | author:Divyarajsinh N. Parmar, Brijesh B. Mehta category:cs.CV published:2014-03-03 summary:Face recognition presents a challenging problem in the field of imageanalysis and computer vision. The security of information is becoming verysignificant and difficult. Security cameras are presently common in airports,Offices, University, ATM, Bank and in any locations with a security system.Face recognition is a biometric system used to identify or verify a person froma digital image. Face Recognition system is used in security. Face recognitionsystem should be able to automatically detect a face in an image. This involvesextracts its features and then recognize it, regardless of lighting,expression, illumination, ageing, transformations (translate, rotate and scaleimage) and pose, which is a difficult task. This paper contains three sections.The first section describes the common methods like holistic matching method,feature extraction method and hybrid methods. The second section describesapplications with examples and finally third section describes the futureresearch directions of face recognition.
arxiv-5400-115 | Support Vector Machine Model for Currency Crisis Discrimination | http://arxiv.org/pdf/1403.0481v1.pdf | author:Arindam Chaudhuri category:cs.LG stat.ML published:2014-03-03 summary:Support Vector Machine (SVM) is powerful classification technique based onthe idea of structural risk minimization. Use of kernel function enables curseof dimensionality to be addressed. However, proper kernel function for certainproblem is dependent on specific dataset and as such there is no good method onchoice of kernel function. In this paper, SVM is used to build empirical modelsof currency crisis in Argentina. An estimation technique is developed bytraining model on real life data set which provides reasonably accurate modeloutputs and helps policy makers to identify situations in which currency crisismay happen. The third and fourth order polynomial kernel is generally bestchoice to achieve high generalization of classifier performance. SVM has highlevel of maturity with algorithms that are simple, easy to implement, toleratescurse of dimensionality and good empirical performance. The satisfactoryresults show that currency crisis situation is properly emulated using onlysmall fraction of database and could be used as an evaluation tool as well asan early warning system. To the best of knowledge this is the first work on SVMapproach for currency crisis evaluation of Argentina.
arxiv-5400-116 | Matching Image Sets via Adaptive Multi Convex Hull | http://arxiv.org/pdf/1403.0320v1.pdf | author:Shaokang Chen, Arnold Wiliem, Conrad Sanderson, Brian C. Lovell category:cs.CV stat.ML published:2014-03-03 summary:Traditional nearest points methods use all the samples in an image set toconstruct a single convex or affine hull model for classification. However,strong artificial features and noisy data may be generated from combinations oftraining samples when significant intra-class variations and/or noise occur inthe image set. Existing multi-model approaches extract local models byclustering each image set individually only once, with fixed clusters used formatching with various image sets. This may not be optimal for discrimination,as undesirable environmental conditions (eg. illumination and pose variations)may result in the two closest clusters representing different characteristicsof an object (eg. frontal face being compared to non-frontal face). To addressthe above problem, we propose a novel approach to enhance nearest points basedmethods by integrating affine/convex hull classification with an adaptedmulti-model approach. We first extract multiple local convex hulls from a queryimage set via maximum margin clustering to diminish the artificial variationsand constrain the noise in local convex hulls. We then propose adaptivereference clustering (ARC) to constrain the clustering of each gallery imageset by forcing the clusters to have resemblance to the clusters in the queryimage set. By applying ARC, noisy clusters in the query set can be discarded.Experiments on Honda, MoBo and ETH-80 datasets show that the proposed methodoutperforms single model approaches and other recent techniques, such as SparseApproximated Nearest Points, Mutual Subspace Method and Manifold DiscriminantAnalysis.
arxiv-5400-117 | Cross-Scale Cost Aggregation for Stereo Matching | http://arxiv.org/pdf/1403.0316v1.pdf | author:Kang Zhang, Yuqiang Fang, Dongbo Min, Lifeng Sun, Shiqiang Yang. Shuicheng Yan, Qi Tian category:cs.CV published:2014-03-03 summary:Human beings process stereoscopic correspondence across multiple scales.However, this bio-inspiration is ignored by state-of-the-art cost aggregationmethods for dense stereo correspondence. In this paper, a generic cross-scalecost aggregation framework is proposed to allow multi-scale interaction in costaggregation. We firstly reformulate cost aggregation from a unifiedoptimization perspective and show that different cost aggregation methodsessentially differ in the choices of similarity kernels. Then, an inter-scaleregularizer is introduced into optimization and solving this new optimizationproblem leads to the proposed framework. Since the regularization term isindependent of the similarity kernel, various cost aggregation methods can beintegrated into the proposed general framework. We show that the cross-scaleframework is important as it effectively and efficiently expandsstate-of-the-art cost aggregation methods and leads to significantimprovements, when evaluated on Middlebury, KITTI and New Tsukuba datasets.
arxiv-5400-118 | Summarisation of Short-Term and Long-Term Videos using Texture and Colour | http://arxiv.org/pdf/1403.0315v1.pdf | author:Johanna Carvajal, Chris McCool, Conrad Sanderson category:cs.CV stat.AP published:2014-03-03 summary:We present a novel approach to video summarisation that makes use of aBag-of-visual-Textures (BoT) approach. Two systems are proposed, one basedsolely on the BoT approach and another which exploits both colour informationand BoT features. On 50 short-term videos from the Open Video Project we showthat our BoT and fusion systems both achieve state-of-the-art performance,obtaining an average F-measure of 0.83 and 0.86 respectively, a relativeimprovement of 9% and 13% when compared to the previous state-of-the-art. Whenapplied to a new underwater surveillance dataset containing 33 long-termvideos, the proposed system reduces the amount of footage by a factor of 27,with only minor degradation in the information content. This order of magnitudereduction in video data represents significant savings in terms of time andpotential labour cost when manually reviewing such footage.
arxiv-5400-119 | Object Tracking via Non-Euclidean Geometry: A Grassmann Approach | http://arxiv.org/pdf/1403.0309v1.pdf | author:Sareh Shirazi, Mehrtash T. Harandi, Brian C. Lovell, Conrad Sanderson category:cs.CV math.MG stat.ML published:2014-03-03 summary:A robust visual tracking system requires an object appearance model that isable to handle occlusion, pose, and illumination variations in the videostream. This can be difficult to accomplish when the model is trained usingonly a single image. In this paper, we first propose a tracking approach basedon affine subspaces (constructed from several images) which are able toaccommodate the abovementioned variations. We use affine subspaces not only torepresent the object, but also the candidate areas that the object may occupy.We furthermore propose a novel approach to measure affine subspace-to-subspacedistance via the use of non-Euclidean geometry of Grassmann manifolds. Thetracking problem is then considered as an inference task in a Markov ChainMonte Carlo framework via particle filtering. Quantitative evaluation onchallenging video sequences indicates that the proposed approach obtainsconsiderably better performance than several recent state-of-the-art methodssuch as Tracking-Learning-Detection and MILtrack.
arxiv-5400-120 | Blind and fully constrained unmixing of hyperspectral images | http://arxiv.org/pdf/1403.0289v1.pdf | author:Rita Ammanouil, André Ferrari, Cédric Richard, David Mary category:stat.AP stat.ML published:2014-03-03 summary:This paper addresses the problem of blind and fully constrained unmixing ofhyperspectral images. Unmixing is performed without the use of any dictionary,and assumes that the number of constituent materials in the scene and theirspectral signatures are unknown. The estimated abundances satisfy the desiredsum-to-one and nonnegativity constraints. Two models with increasing complexityare developed to achieve this challenging task, depending on how noiseinteracts with hyperspectral data. The first one leads to a convex optimizationproblem, and is solved with the Alternating Direction Method of Multipliers.The second one accounts for signal-dependent noise, and is addressed with aReweighted Least Squares algorithm. Experiments on synthetic and real datademonstrate the effectiveness of our approach.
arxiv-5400-121 | Multiview Hessian regularized logistic regression for action recognition | http://arxiv.org/pdf/1403.0829v1.pdf | author:W. Liu, H. Liu, D. Tao, Y. Wang, Ke Lu category:cs.CV cs.LG stat.ML published:2014-03-03 summary:With the rapid development of social media sharing, people often need tomanage the growing volume of multimedia data such as large scale videoclassification and annotation, especially to organize those videos containinghuman activities. Recently, manifold regularized semi-supervised learning(SSL), which explores the intrinsic data probability distribution and thenimproves the generalization ability with only a small number of labeled data,has emerged as a promising paradigm for semiautomatic video classification. Inaddition, human action videos often have multi-modal content and differentrepresentations. To tackle the above problems, in this paper we proposemultiview Hessian regularized logistic regression (mHLR) for human actionrecognition. Compared with existing work, the advantages of mHLR lie in threefolds: (1) mHLR combines multiple Hessian regularization, each of whichobtained from a particular representation of instance, to leverage theexploring of local geometry; (2) mHLR naturally handle multi-view instanceswith multiple representations; (3) mHLR employs a smooth loss function and thencan be effectively optimized. We carefully conduct extensive experiments on theunstructured social activity attribute (USAA) dataset and the experimentalresults demonstrate the effectiveness of the proposed multiview Hessianregularized logistic regression for human action recognition.
arxiv-5400-122 | Particle methods enable fast and simple approximation of Sobolev gradients in image segmentation | http://arxiv.org/pdf/1403.0240v1.pdf | author:Ivo F. Sbalzarini, Sophie Schneider, Janick Cardinale category:cs.CV cs.CE cs.NA q-bio.QM published:2014-03-02 summary:Bio-image analysis is challenging due to inhomogeneous intensitydistributions and high levels of noise in the images. Bayesian inferenceprovides a principled way for regularizing the problem using prior knowledge. Afundamental choice is how one measures "distances" between shapes in an image.It has been shown that the straightforward geometric L2 distance is degenerateand leads to pathological situations. This is avoided when using Sobolevgradients, rendering the segmentation problem less ill-posed. The highcomputational cost and implementation overhead of Sobolev gradients, however,have hampered practical applications. We show how particle methods as appliedto image segmentation allow for a simple and computationally efficientimplementation of Sobolev gradients. We show that the evaluation of Sobolevgradients amounts to particle-particle interactions along the contour in animage. We extend an existing particle-based segmentation algorithm to usingSobolev gradients. Using synthetic and real-world images, we benchmark theresults for both 2D and 3D images using piecewise smooth and piecewise constantregion models. The present particle approximation of Sobolev gradients is 2.8to 10 times faster than the previous reference implementation, but retains theknown favorable properties of Sobolev gradients. This speedup is achieved byusing local particle-particle interactions instead of solving a global Poissonequation at each iteration. The computational time per iteration is higher forSobolev gradients than for L2 gradients. Since Sobolev gradients preconditionthe optimization problem, however, a smaller number of overall iterations maybe necessary for the algorithm to converge, which can in some cases amortizethe higher per-iteration cost.
arxiv-5400-123 | Learning Using Privileged Information: SVM+ and Weighted SVM | http://arxiv.org/pdf/1306.3161v2.pdf | author:Maksim Lapin, Matthias Hein, Bernt Schiele category:stat.ML cs.LG published:2013-06-13 summary:Prior knowledge can be used to improve predictive performance of learningalgorithms or reduce the amount of data required for training. The same goal ispursued within the learning using privileged information paradigm which wasrecently introduced by Vapnik et al. and is aimed at utilizing additionalinformation available only at training time -- a framework implemented by SVM+.We relate the privileged information to importance weighting and show that theprior knowledge expressible with privileged features can also be encoded byweights associated with every training example. We show that a weighted SVM canalways replicate an SVM+ solution, while the converse is not true and weconstruct a counterexample highlighting the limitations of SVM+. Finally, wetouch on the problem of choosing weights for weighted SVMs when privilegedfeatures are not available.
arxiv-5400-124 | Network Traffic Decomposition for Anomaly Detection | http://arxiv.org/pdf/1403.0157v1.pdf | author:Tahereh Babaie, Sanjay Chawla, Sebastien Ardon category:cs.LG cs.NI published:2014-03-02 summary:In this paper we focus on the detection of network anomalies like Denial ofService (DoS) attacks and port scans in a unified manner. While there has beenan extensive amount of research in network anomaly detection, current state ofthe art methods are only able to detect one class of anomalies at the cost ofothers. The key tool we will use is based on the spectral decomposition of atrajectory/hankel matrix which is able to detect deviations from both betweenand within correlation present in the observed network traffic data. Detailedexperiments on synthetic and real network traces shows a significantimprovement in detection capability over competing approaches. In the processwe also address the issue of robustness of anomaly detection systems in aprincipled fashion.
arxiv-5400-125 | Sleep Analytics and Online Selective Anomaly Detection | http://arxiv.org/pdf/1403.0156v1.pdf | author:Tahereh Babaie, Sanjay Chawla, Romesh Abeysuriya category:cs.LG published:2014-03-02 summary:We introduce a new problem, the Online Selective Anomaly Detection (OSAD), tomodel a specific scenario emerging from research in sleep science. Scientistshave segmented sleep into several stages and stage two is characterized by twopatterns (or anomalies) in the EEG time series recorded on sleep subjects.These two patterns are sleep spindle (SS) and K-complex. The OSAD problem wasintroduced to design a residual system, where all anomalies (known and unknown)are detected but the system only triggers an alarm when non-SS anomaliesappear. The solution of the OSAD problem required us to combine techniques fromboth machine learning and control theory. Experiments on data from realsubjects attest to the effectiveness of our approach.
arxiv-5400-126 | Accelerated, Parallel and Proximal Coordinate Descent | http://arxiv.org/pdf/1312.5799v2.pdf | author:Olivier Fercoq, Peter Richtárik category:math.OC cs.DC cs.NA math.NA stat.ML published:2013-12-20 summary:We propose a new stochastic coordinate descent method for minimizing the sumof convex functions each of which depends on a small number of coordinatesonly. Our method (APPROX) is simultaneously Accelerated, Parallel and PROXimal;this is the first time such a method is proposed. In the special case when thenumber of processors is equal to the number of coordinates, the methodconverges at the rate $2\bar{\omega}\bar{L} R^2/(k+1)^2 $, where $k$ is theiteration counter, $\bar{\omega}$ is an average degree of separability of theloss function, $\bar{L}$ is the average of Lipschitz constants associated withthe coordinates and individual functions in the sum, and $R$ is the distance ofthe initial point from the minimizer. We show that the method can beimplemented without the need to perform full-dimensional vector operations,which is the major bottleneck of existing accelerated coordinate descentmethods. The fact that the method depends on the average degree ofseparability, and not on the maximum degree of separability, can be attributedto the use of new safe large stepsizes, leading to improved expected separableoverapproximation (ESO). These are of independent interest and can be utilizedin all existing parallel stochastic coordinate descent algorithms based on theconcept of ESO.
arxiv-5400-127 | Temporal Image Fusion | http://arxiv.org/pdf/1403.0087v1.pdf | author:Francisco J. Estrada category:cs.CV cs.GR published:2014-03-01 summary:This paper introduces temporal image fusion. The proposed technique buildsupon previous research in exposure fusion and expands it to deal with thelimited Temporal Dynamic Range of existing sensors and camera technologies. Inparticular, temporal image fusion enables the rendering of long-exposureeffects on full frame-rate video, as well as the generation of arbitrarily longexposures from a sequence of images of the same scene taken over time. Weexplore the problem of temporal under-exposure, and show how it can beaddressed by selectively enhancing dynamic structure. Finally, we show that theuse of temporal image fusion together with content-selective image filters canproduce a range of striking visual effects on a given input sequence.
arxiv-5400-128 | TBX goes TEI -- Implementing a TBX basic extension for the Text Encoding Initiative guidelines | http://arxiv.org/pdf/1403.0052v1.pdf | author:Laurent Romary category:cs.CL published:2014-03-01 summary:This paper presents an attempt to customise the TEI (Text EncodingInitiative) guidelines in order to offer the possibility to incorporate TBX(TermBase eXchange) based terminological entries within any kind of TEIdocuments. After presenting the general historical, conceptual and technicalcontexts, we describe the various design choices we had to take while creatingthis customisation, which in turn have led to make various changes in theactual TBX serialisation. Keeping in mind the objective to provide the TEIguidelines with, again, an onomasiological model, we try to identify the bestcomprise in maintaining both the isomorphism with the existing TBX Basicstandard and the characteristics of the TEI framework.
arxiv-5400-129 | A Machine Learning Model for Stock Market Prediction | http://arxiv.org/pdf/1402.7351v1.pdf | author:Osman Hegazy, Omar S. Soliman, Mustafa Abdul Salam category:cs.CE cs.NE published:2014-02-28 summary:Stock market prediction is the act of trying to determine the future value ofa company stock or other financial instrument traded on a financial exchange.
arxiv-5400-130 | Semantics, Modelling, and the Problem of Representation of Meaning -- a Brief Survey of Recent Literature | http://arxiv.org/pdf/1402.7265v1.pdf | author:Yarin Gal category:cs.CL published:2014-02-28 summary:Over the past 50 years many have debated what representation should be usedto capture the meaning of natural language utterances. Recently new needs ofsuch representations have been raised in research. Here I survey some of theinteresting representations suggested to answer for these new needs.
arxiv-5400-131 | Visual Saliency Model using SIFT and Comparison of Learning Approaches | http://arxiv.org/pdf/1402.7162v1.pdf | author:Hamdi Yalin Yalic category:cs.CV I.2.10; I.5.4 published:2014-02-28 summary:Humans' ability to detect and locate salient objects on images is remarkablyfast and successful. Performing this process by using eye tracking equipment isexpensive and cannot be easily applied, and computer modeling of this humanbehavior is still a problem to be solved. In our study, one of the largestpublic eye-tracking databases which has fixation points of 15 observers on 1003images is used. In addition to low, medium and high-level features which havebeen used in previous studies, SIFT features extracted from the images are usedto improve the classification accuracy of the models. A second contribution ofthis paper is the comparison and statistical analysis of different machinelearning methods that can be used to train our model. As a result, a bestfeature set and learning model to predict where humans look at images, isdetermined.
arxiv-5400-132 | Neural Network Approach to Railway Stand Lateral Skew Control | http://arxiv.org/pdf/1402.7136v1.pdf | author:Peter Mark Benes, Ivo Bukovsky, Matous Cejnek, Jan Kalivoda category:cs.SY cs.NE published:2014-02-28 summary:The paper presents a study of an adaptive approach to lateral skew controlfor an experimental railway stand. The preliminary experiments with the realexperimental railway stand and simulations with its 3-D mechanical model,indicates difficulties of model-based control of the device. Thus, use ofneural networks for identification and control of lateral skew shall beinvestigated. This paper focuses on real-data based modeling of the railwaystand by various neural network models, i.e; linear neural unit and quadraticneural unit architectures. Furthermore, training methods of these neuralarchitectures as such, real-time-recurrent-learning and a variation ofback-propagation-through-time are examined, accompanied by a discussion of theproduced experimental results.
arxiv-5400-133 | Exact Post Model Selection Inference for Marginal Screening | http://arxiv.org/pdf/1402.5596v2.pdf | author:Jason D Lee, Jonathan E Taylor category:stat.ME cs.LG math.ST stat.ML stat.TH published:2014-02-23 summary:We develop a framework for post model selection inference, via marginalscreening, in linear regression. At the core of this framework is a result thatcharacterizes the exact distribution of linear functions of the response $y$,conditional on the model being selected (``condition on selection" framework).This allows us to construct valid confidence intervals and hypothesis tests forregression coefficients that account for the selection procedure. In contrastto recent work in high-dimensional statistics, our results are exact(non-asymptotic) and require no eigenvalue-like assumptions on the designmatrix $X$. Furthermore, the computational cost of marginal regression,constructing confidence intervals and hypothesis testing is negligible comparedto the cost of linear regression, thus making our methods particularly suitablefor extremely large datasets. Although we focus on marginal screening toillustrate the applicability of the condition on selection framework, thisframework is much more broadly applicable. We show how to apply the proposedframework to several other selection procedures including orthogonal matchingpursuit, non-negative least squares, and marginal screening+Lasso.
arxiv-5400-134 | Signal Recovery from Pooling Representations | http://arxiv.org/pdf/1311.4025v3.pdf | author:Joan Bruna, Arthur Szlam, Yann LeCun category:stat.ML published:2013-11-16 summary:In this work we compute lower Lipschitz bounds of $\ell_p$ pooling operatorsfor $p=1, 2, \infty$ as well as $\ell_p$ pooling operators preceded byhalf-rectification layers. These give sufficient conditions for the design ofinvertible neural network layers. Numerical experiments on MNIST and imagepatches confirm that pooling layers can be inverted with phase recoveryalgorithms. Moreover, the regularity of the inverse pooling, controlled by thelower Lipschitz constant, is empirically verified with a nearest neighborregression.
arxiv-5400-135 | Scaling Graph-based Semi Supervised Learning to Large Number of Labels Using Count-Min Sketch | http://arxiv.org/pdf/1310.2959v2.pdf | author:Partha Pratim Talukdar, William Cohen category:cs.LG published:2013-10-10 summary:Graph-based Semi-supervised learning (SSL) algorithms have been successfullyused in a large number of applications. These methods classify initiallyunlabeled nodes by propagating label information over the structure of graphstarting from seed nodes. Graph-based SSL algorithms usually scale linearlywith the number of distinct labels (m), and require O(m) space on each node.Unfortunately, there exist many applications of practical significance withvery large m over large graphs, demanding better space and time complexity. Inthis paper, we propose MAD-SKETCH, a novel graph-based SSL algorithm whichcompactly stores label distribution on each node using Count-min Sketch, arandomized data structure. We present theoretical analysis showing that undermild conditions, MAD-SKETCH can reduce space complexity at each node from O(m)to O(log m), and achieve similar savings in time complexity as well. We supportour analysis through experiments on multiple real world datasets. We observethat MAD-SKETCH achieves similar performance as existing state-of-the-artgraph- based SSL algorithms, while requiring smaller memory footprint and atthe same time achieving up to 10x speedup. We find that MAD-SKETCH is able toscale to datasets with one million labels, which is beyond the scope ofexisting graph- based SSL algorithms.
arxiv-5400-136 | Bayesian Multi-Scale Optimistic Optimization | http://arxiv.org/pdf/1402.7005v1.pdf | author:Ziyu Wang, Babak Shakibi, Lin Jin, Nando de Freitas category:stat.ML cs.LG published:2014-02-27 summary:Bayesian optimization is a powerful global optimization technique forexpensive black-box functions. One of its shortcomings is that it requiresauxiliary optimization of an acquisition function at each iteration. Thisauxiliary optimization can be costly and very hard to carry out in practice.Moreover, it creates serious theoretical concerns, as most of the convergenceresults assume that the exact optimum of the acquisition function can be found.In this paper, we introduce a new technique for efficient global optimizationthat combines Gaussian process confidence bounds and treed simultaneousoptimistic optimization to eliminate the need for auxiliary optimization ofacquisition functions. The experiments with global optimization benchmarks anda novel application to automatic information extraction demonstrate that theresulting technique is more efficient than the two approaches from which itdraws inspiration. Unlike most theoretical analyses of Bayesian optimizationwith Gaussian processes, our finite-time convergence rate proofs do not requireexact optimization of an acquisition function. That is, our approach eliminatesthe unsatisfactory assumption that a difficult, potentially NP-hard, problemhas to be solved in order to obtain vanishing regret rates.
arxiv-5400-137 | Marginalizing Corrupted Features | http://arxiv.org/pdf/1402.7001v1.pdf | author:Laurens van der Maaten, Minmin Chen, Stephen Tyree, Kilian Weinberger category:cs.LG published:2014-02-27 summary:The goal of machine learning is to develop predictors that generalize well totest data. Ideally, this is achieved by training on an almost infinitely largetraining data set that captures all variations in the data distribution. Inpractical learning settings, however, we do not have infinite data and ourpredictors may overfit. Overfitting may be combatted, for example, by adding aregularizer to the training objective or by defining a prior over the modelparameters and performing Bayesian inference. In this paper, we propose athird, alternative approach to combat overfitting: we extend the training setwith infinitely many artificial training examples that are obtained bycorrupting the original training data. We show that this approach is practicaland efficient for a range of predictors and corruption models. Our approach,called marginalized corrupted features (MCF), trains robust predictors byminimizing the expected value of the loss function under the corruption model.We show empirically on a variety of data sets that MCF classifiers can betrained efficiently, may generalize substantially better to test data, and arealso more robust to feature deletion at test time.
arxiv-5400-138 | Scalable methods for nonnegative matrix factorizations of near-separable tall-and-skinny matrices | http://arxiv.org/pdf/1402.6964v1.pdf | author:Austin R. Benson, Jason D. Lee, Bartek Rajwa, David F. Gleich category:cs.LG cs.DC cs.NA stat.ML G.1.3; G.1.6 published:2014-02-27 summary:Numerous algorithms are used for nonnegative matrix factorization under theassumption that the matrix is nearly separable. In this paper, we show how tomake these algorithms efficient for data matrices that have many more rows thancolumns, so-called "tall-and-skinny matrices". One key component to theseimproved methods is an orthogonal matrix transformation that preserves theseparability of the NMF problem. Our final methods need a single pass over thedata matrix and are suitable for streaming, multi-core, and MapReducearchitectures. We demonstrate the efficacy of these algorithms onterabyte-sized synthetic matrices and real-world matrices from scientificcomputing and bioinformatics.
arxiv-5400-139 | A Parallel Memetic Algorithm to Solve the Vehicle Routing Problem with Time Windows | http://arxiv.org/pdf/1402.6942v1.pdf | author:Jakub Nalepa, Zbigniew J. Czech category:cs.DC cs.NE published:2014-02-27 summary:This paper presents a parallel memetic algorithm for solving the vehiclerouting problem with time windows (VRPTW). The VRPTW is a well-known NP-harddiscrete optimization problem with two objectives. The main objective is tominimize the number of vehicles serving customers scattered on the map, and thesecond one is to minimize the total distance traveled by the vehicles. Here,the fleet size is minimized in the first phase of the proposed method using theparallel heuristic algorithm (PHA), and the traveled distance is minimized inthe second phase by the parallel memetic algorithm (PMA). In both parallelalgorithms, the parallel components co-operate periodically in order toexchange the best solutions found so far. An extensive experimental studyperformed on the Gehring and Homberger's benchmark proves the high convergencecapabilities and robustness of both PHA and PMA. Also, we present the speedupanalysis of the PMA.
arxiv-5400-140 | Low-Cost Compressive Sensing for Color Video and Depth | http://arxiv.org/pdf/1402.6932v1.pdf | author:Xin Yuan, Patrick Llull, Xuejun Liao, Jianbo Yang, Guillermo Sapiro, David J. Brady, Lawrence Carin category:cs.CV published:2014-02-27 summary:A simple and inexpensive (low-power and low-bandwidth) modification is madeto a conventional off-the-shelf color video camera, from which we recover{multiple} color frames for each of the original measured frames, and each ofthe recovered frames can be focused at a different depth. The recovery ofmultiple frames for each measured frame is made possible via high-speed coding,manifested via translation of a single coded aperture; the inexpensivetranslation is constituted by mounting the binary code on a piezoelectricdevice. To simultaneously recover depth information, a {liquid} lens ismodulated at high speed, via a variable voltage. Consequently, during theaforementioned coding process, the liquid lens allows the camera to sweep thefocus through multiple depths. In addition to designing and implementing thecamera, fast recovery is achieved by an anytime algorithm exploiting thegroup-sparsity of wavelet/DCT coefficients.
arxiv-5400-141 | A continuous-time approach to online optimization | http://arxiv.org/pdf/1401.6956v2.pdf | author:Joon Kwon, Panayotis Mertikopoulos category:math.OC cs.LG stat.ML published:2014-01-27 summary:We consider a family of learning strategies for online optimization problemsthat evolve in continuous time and we show that they lead to no regret. From amore traditional, discrete-time viewpoint, this continuous-time approach allowsus to derive the no-regret properties of a large class of discrete-timealgorithms including as special cases the exponential weight algorithm, onlinemirror descent, smooth fictitious play and vanishingly smooth fictitious play.In so doing, we obtain a unified view of many classical regret bounds, and weshow that they can be decomposed into a term stemming from continuous-timeconsiderations and a term which measures the disparity between discrete andcontinuous time. As a result, we obtain a general class of infinite horizonlearning strategies that guarantee an $\mathcal{O}(n^{-1/2})$ regret boundwithout having to resort to a doubling trick.
arxiv-5400-142 | CriPS: Critical Dynamics in Particle Swarm Optimization | http://arxiv.org/pdf/1402.6888v1.pdf | author:Adam Erskine, J Michael Herrmann category:cs.NE published:2014-02-27 summary:Particle Swarm Optimisation (PSO) makes use of a dynamical system for solvinga search task. Instead of adding search biases in order to improve performancein certain problems, we aim to remove algorithm-induced scales by controllingthe swarm with a mechanism that is scale-free except possibly for a suppressionof scales beyond the system size. In this way a very promising performance isachieved due to the balance of large-scale exploration and local search. Theresulting algorithm shows evidence for self-organised criticality, broughtabout via the intrinsic dynamics of the swarm as it interacts with theobjective function, rather than being explicitly specified. The CriticalParticle Swarm (CriPS) can be easily combined with many existing extensionssuch as chaotic exploration, additional force terms or non-trivial topologies.
arxiv-5400-143 | It's distributions all the way down!: Second order changes in statistical distributions also occur | http://arxiv.org/pdf/1402.6880v1.pdf | author:M. T. Keane, A. Gerow category:cs.CL published:2014-02-27 summary:The textual, big-data literature misses Bentley, OBrien, & Brocks (Bentley etals) message on distributions; it largely examines the first-order effects ofhow a single, signature distribution can predict population behaviour,neglecting second-order effects involving distributional shifts, either betweensignature distributions or within a given signature distribution. Indeed,Bentley et al. themselves under-emphasise the potential richness of the latter,within-distribution effects.
arxiv-5400-144 | Outlier Detection using Improved Genetic K-means | http://arxiv.org/pdf/1402.6859v1.pdf | author:M. H. Marghny, Ahmed I. Taloba category:cs.LG cs.DB published:2014-02-27 summary:The outlier detection problem in some cases is similar to the classificationproblem. For example, the main concern of clustering-based outlier detectionalgorithms is to find clusters and outliers, which are often regarded as noisethat should be removed in order to make more reliable clustering. In thisarticle, we present an algorithm that provides outlier detection and dataclustering simultaneously. The algorithmimprovesthe estimation of centroids ofthe generative distribution during the process of clustering and outlierdiscovery. The proposed algorithm consists of two stages. The first stageconsists of improved genetic k-means algorithm (IGK) process, while the secondstage iteratively removes the vectors which are far from their clustercentroids.
arxiv-5400-145 | An Effective Evolutionary Clustering Algorithm: Hepatitis C Case Study | http://arxiv.org/pdf/1405.6173v1.pdf | author:M. H. Marghny, Rasha M. Abd El-Aziz, Ahmed I. Taloba category:cs.NE cs.CE published:2014-02-27 summary:Clustering analysis plays an important role in scientific research andcommercial application. K-means algorithm is a widely used partition method inclustering. However, it is known that the K-means algorithm may get stuck atsuboptimal solutions, depending on the choice of the initial cluster centers.In this article, we propose a technique to handle large scale data, which canselect initial clustering center purposefully using Genetic algorithms (GAs),reduce the sensitivity to isolated point, avoid dissevering big cluster, andovercome deflexion of data in some degree that caused by the disproportion indata partitioning owing to adoption of multi-sampling. We applied our method tosome public datasets these show the advantages of the proposed approach forexample Hepatitis C dataset that has been taken from the machine learningwarehouse of University of California. Our aim is to evaluate hepatitisdataset. In order to evaluate this dataset we did some preprocessing operation,the reason to preprocessing is to summarize the data in the best and suitableway for our algorithm. Missing values of the instances are adjusted using localmean method.
arxiv-5400-146 | Delay Learning Architectures for Memory and Classification | http://arxiv.org/pdf/1311.1294v2.pdf | author:Shaista Hussain, Arindam Basu, R. Wang, Tara Julia Hamilton category:cs.NE q-bio.NC published:2013-11-06 summary:We present a neuromorphic spiking neural network, the DELTRON, that canremember and store patterns by changing the delays of every connection asopposed to modifying the weights. The advantage of this architecture overtraditional weight based ones is simpler hardware implementation withoutmultipliers or digital-analog converters (DACs) as well as being suited totime-based computing. The name is derived due to similarity in the learningrule with an earlier architecture called Tempotron. The DELTRON can remembermore patterns than other delay-based networks by modifying a few delays toremember the most 'salient' or synchronous part of every spike pattern. Wepresent simulations of memory capacity and classification ability of theDELTRON for different random spatio-temporal spike patterns. The memorycapacity for noisy spike patterns and missing spikes are also shown. Finally,we present SPICE simulation results of the core circuits involved in areconfigurable mixed signal implementation of this architecture.
arxiv-5400-147 | Information Evolution in Social Networks | http://arxiv.org/pdf/1402.6792v1.pdf | author:Lada A. Adamic, Thomas M. Lento, Eytan Adar, Pauline C. Ng category:cs.SI cs.CL physics.soc-ph published:2014-02-27 summary:Social networks readily transmit information, albeit with less than perfectfidelity. We present a large-scale measurement of this imperfect informationcopying mechanism by examining the dissemination and evolution of thousands ofmemes, collectively replicated hundreds of millions of times in the onlinesocial network Facebook. The information undergoes an evolutionary process thatexhibits several regularities. A meme's mutation rate characterizes thepopulation distribution of its variants, in accordance with the Yule process.Variants further apart in the diffusion cascade have greater edit distance, aswould be expected in an iterative, imperfect replication process. Some textsequences can confer a replicative advantage; these sequences are abundant andtransfer "laterally" between different memes. Subpopulations of the socialnetwork can preferentially transmit a specific variant of a meme if the variantmatches their beliefs or culture. Understanding the mechanism driving change indiffusing information has important implications for how we interpret andharness the information that reaches us through our social networks.
arxiv-5400-148 | Synthesis of Parametric Programs using Genetic Programming and Model Checking | http://arxiv.org/pdf/1402.6785v1.pdf | author:Gal Katz, Doron Peled category:cs.SE cs.AI cs.NE published:2014-02-27 summary:Formal methods apply algorithms based on mathematical principles to enhancethe reliability of systems. It would only be natural to try to progress fromverification, model checking or testing a system against its formalspecification into constructing it automatically. Classical algorithmicsynthesis theory provides interesting algorithms but also alarming highcomplexity and undecidability results. The use of genetic programming, incombination with model checking and testing, provides a powerful heuristic tosynthesize programs. The method is not completely automatic, as it is finetuned by a user that sets up the specification and parameters. It also does notguarantee to always succeed and converge towards a solution that satisfies allthe required properties. However, we applied it successfully on quitenontrivial examples and managed to find solutions to hard programmingchallenges, as well as to improve and to correct code. We describe here severalversions of our method for synthesizing sequential and concurrent systems.
arxiv-5400-149 | A method to identify potential ambiguous Malay words through Ambiguity Attributes mapping: An exploratory Study | http://arxiv.org/pdf/1402.6764v1.pdf | author:Hazlina Haron, Abdul Azim Abd. Ghani category:cs.SE cs.CL published:2014-02-27 summary:We describe here a methodology to identify a list of ambiguous Malay wordsthat are commonly being used in Malay documentations such as RequirementSpecification. We compiled several relevant and appropriate requirement qualityattributes and sentence rules from previous literatures and adopt it to comeout with a set of ambiguity attributes that most suit Malay words. Theextracted Malay ambiguous words (potential) are then being mapped onto theconstructed ambiguity attributes to confirm their vagueness. The list is thenverified by Malay linguist experts. This paper aims to identify a list ofpotential ambiguous words in Malay as an attempt to assist writers to avoidusing the vague words while documenting Malay Requirement Specification as wellas to any other related Malay documentation. The result of this study is a listof 120 potential ambiguous Malay words that could act as guidelines in writingMalay sentences
arxiv-5400-150 | Robust Asymmetric Clustering | http://arxiv.org/pdf/1402.6744v1.pdf | author:Katherine Morris, Paul D. McNicholas, Antonio Punzo, Ryan P. Browne category:stat.ME stat.CO stat.ML published:2014-02-26 summary:Contaminated mixture models are developed for model-based clustering of datawith asymmetric clusters as well as spurious points, outliers, and/or noise.Specifically, we introduce a contaminated mixture of contaminated shiftedasymmetric Laplace distributions and a contaminated mixture of contaminatedskew-normal distributions. In each case, mixture components have a parametercontrolling the proportion of bad points (i.e., spurious points, outliers,and/or noise) and one specifying the degree of contamination. A very importantfeature of our approaches is that these parameters do not have to be specifieda priori. Expectation-conditional maximization algorithms are outlined forparameter estimation and the number of components is selected using theBayesian information criterion. The performance of our approaches isillustrated on artificial and real data.
arxiv-5400-151 | Why Are You More Engaged? Predicting Social Engagement from Word Use | http://arxiv.org/pdf/1402.6690v1.pdf | author:Jalal Mahmud, Jilin Chen, Jeffrey Nichols category:cs.SI cs.CL cs.CY published:2014-02-26 summary:We present a study to analyze how word use can predict social engagementbehaviors such as replies and retweets in Twitter. We compute psycholinguisticcategory scores from word usage, and investigate how people with differentscores exhibited different reply and retweet behaviors on Twitter. We alsofound psycholinguistic categories that show significant correlations with suchsocial engagement behaviors. In addition, we have built predictive models ofreplies and retweets from such psycholinguistic category based features. Ourexperiments using a real world dataset collected from Twitter validates thatsuch predictions can be done with reasonable accuracy.
arxiv-5400-152 | A Novel Method for the Recognition of Isolated Handwritten Arabic Characters | http://arxiv.org/pdf/1402.6650v1.pdf | author:Ahmed Sahlol, Cheng Suen category:cs.CV 68T10 published:2014-02-26 summary:There are many difficulties facing a handwritten Arabic recognition systemsuch as unlimited variation in human handwriting, similarities of distinctcharacter shapes, interconnections of neighbouring characters and theirposition in the word. The typical Optical Character Recognition (OCR) systemsare based mainly on three stages, preprocessing, features extraction andrecognition. This paper proposes new methods for handwritten Arabic characterrecognition which is based on novel preprocessing operations includingdifferent kinds of noise removal also different kind of features likestructural, Statistical and Morphological features from the main body of thecharacter and also from the secondary components. Evaluation of the accuracy ofthe selected features is made. The system was trained and tested by backpropagation neural network with CENPRMI dataset. The proposed algorithmobtained promising results as it is able to recognize 88% of our test setaccurately. In Comparable with other related works we find that our result isthe highest among other published works.
arxiv-5400-153 | Evolutionary solving of the debts' clearing problem | http://arxiv.org/pdf/1402.6556v1.pdf | author:Csaba Patcas, Attila Bartha category:cs.NE cs.AI 97R40 I.2.8; G.2.3 published:2014-02-26 summary:The debts' clearing problem is about clearing all the debts in a group of nentities (persons, companies etc.) using a minimal number of money transactionoperations. The problem is known to be NP-hard in the strong sense. As for manyintractable problems, techniques from the field of artificial intelligence areuseful in finding solutions close to optimum for large inputs. An evolutionaryalgorithm for solving the debts' clearing problem is proposed.
arxiv-5400-154 | Renewable Energy Prediction using Weather Forecasts for Optimal Scheduling in HPC Systems | http://arxiv.org/pdf/1402.6552v1.pdf | author:Ankur Sahai category:cs.LG published:2014-02-26 summary:The objective of the GreenPAD project is to use green energy (wind, solar andbiomass) for powering data-centers that are used to run HPC jobs. As a part ofthis it is important to predict the Renewable (Wind) energy for efficientscheduling (executing jobs that require higher energy when there is more greenenergy available and vice-versa). For predicting the wind energy we firstanalyze the historical data to find a statistical model that gives relationbetween wind energy and weather attributes. Then we use this model based on theweather forecast data to predict the green energy availability in the future.Using the green energy prediction obtained from the statistical model we areable to precompute job schedules for maximizing the green energy utilization inthe future. We propose a model which uses live weather data in addition tomachine learning techniques (which can predict future deviations in weatherconditions based on current deviations from the forecast) to make on-the-flychanges to the precomputed schedule (based on green energy prediction). For this we first analyze the data using histograms and simple statisticaltools such as correlation. In addition we build (correlation) regression modelfor finding the relation between wind energy availability and weatherattributes (temperature, cloud cover, air pressure, wind speed / direction,precipitation and sunshine). We also analyze different algorithms and machinelearning techniques for optimizing the job schedules for maximizing the greenenergy utilization.
arxiv-5400-155 | Modelling the Lexicon in Unsupervised Part of Speech Induction | http://arxiv.org/pdf/1402.6516v1.pdf | author:Greg Dubbin, Phil Blunsom category:cs.CL published:2014-02-26 summary:Automatically inducing the syntactic part-of-speech categories for words intext is a fundamental task in Computational Linguistics. While the performanceof unsupervised tagging models has been slowly improving, currentstate-of-the-art systems make the obviously incorrect assumption that alltokens of a given word type must share a single part-of-speech tag. Thisone-tag-per-type heuristic counters the tendency of Hidden Markov Model basedtaggers to over generate tags for a given word type. However, it is clearlyincompatible with basic syntactic theory. In this paper we extend astate-of-the-art Pitman-Yor Hidden Markov Model tagger with an explicit modelof the lexicon. In doing so we are able to incorporate a soft bias towardsinducing few tags per type. We develop a particle filter for drawing samplesfrom the posterior of our model and present empirical results that show thatour model is competitive with and faster than the state-of-the-art withoutmaking any unrealistic restrictions.
arxiv-5400-156 | Clustering Multidimensional Data with PSO based Algorithm | http://arxiv.org/pdf/1402.6428v1.pdf | author:Jayshree Ghorpade-Aher, Vishakha A. Metre category:cs.NE published:2014-02-26 summary:Data clustering is a recognized data analysis method in data mining whereasK-Means is the well known partitional clustering method, possessing pleasantfeatures. We observed that, K-Means and other partitional clustering techniquessuffer from several limitations such as initial cluster centre selection,preknowledge of number of clusters, dead unit problem, multiple clustermembership and premature convergence to local optima. Several optimizationmethods are proposed in the literature in order to solve clusteringlimitations, but Swarm Intelligence (SI) has achieved its remarkable positionin the concerned area. Particle Swarm Optimization (PSO) is the most popular SItechnique and one of the favorite areas of researchers. In this paper, wepresent a brief overview of PSO and applicability of its variants to solveclustering challenges. Also, we propose an advanced PSO algorithm named asSubtractive Clustering based Boundary Restricted Adaptive Particle SwarmOptimization (SC-BR-APSO) algorithm for clustering multidimensional data. Forcomparison purpose, we have studied and analyzed various algorithms such asK-Means, PSO, K-Means-PSO, Hybrid Subtractive + PSO, BRAPSO, and proposedalgorithm on nine different datasets. The motivation behind proposingSC-BR-APSO algorithm is to deal with multidimensional data clustering, withminimum error rate and maximum convergence rate.
arxiv-5400-157 | Deconstruction of compound objects from image sets | http://arxiv.org/pdf/1402.6416v1.pdf | author:Anton van den Hengel, John Bastian, Anthony Dick, Lachlan Fleming category:cs.CV published:2014-02-26 summary:We propose a method to recover the structure of a compound object frommultiple silhouettes. Structure is expressed as a collection of 3D primitiveschosen from a pre-defined library, each with an associated pose. This hasseveral advantages over a volume or mesh representation both for estimation andthe utility of the recovered model. The main challenge in recovering such amodel is the combinatorial number of possible arrangements of parts. We addressthis issue by exploiting the sparse nature of the problem, and show that ourmethod scales to objects constructed from large libraries of parts.
arxiv-5400-158 | Active spline model: A shape based model-interactive segmentation | http://arxiv.org/pdf/1402.6387v1.pdf | author:Jen Hong Tan, U. Rajendra Acharya category:cs.CV published:2014-02-26 summary:Rarely in literature a method of segmentation cares for the edit after thealgorithm delivers. They provide no solution when segmentation goes wrong. Wepropose to formulate point distribution model in terms ofcentripetal-parameterized Catmull-Rom spline. Such fusion brings interactivityto model-based segmentation, so that edit is better handled. When the deliveredsegment is unsatisfactory, user simply shifts points to vary the curve. We ranthe method on three disparate imaging modalities and achieved an averageoverlap of 0.879 for automated lung segmentation on chest radiographs. The editafterward improved the average overlap to 0.945, with a minimum of 0.925. Thesource code and the demo video are available at http://wp.me/p3vCKy-2S
arxiv-5400-159 | Large-margin Learning of Compact Binary Image Encodings | http://arxiv.org/pdf/1402.6383v1.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV published:2014-02-26 summary:The use of high-dimensional features has become a normal practice in manycomputer vision applications. The large dimension of these features is alimiting factor upon the number of data points which may be effectively storedand processed, however. We address this problem by developing a novel approachto learning a compact binary encoding, which exploits both pair-wise proximityand class-label information on training data set. Exploiting this extrainformation allows the development of encodings which, although compact,outperform the original high-dimensional features in terms of finalclassification or retrieval performance. The method is general, in that it isapplicable to both non-parametric and parametric learning methods. Thisgenerality means that the embedded features are suitable for a wide variety ofcomputer vision tasks, such as image classification and content-based imageretrieval. Experimental results demonstrate that the new compact descriptorachieves an accuracy comparable to, and in some cases better than, the visualdescriptor in the original space despite being significantly more compact.Moreover, any convex loss function and convex regularization penalty (e.g., $\ell_p $ norm with $ p \ge 1 $) can be incorporated into the framework, whichprovides future flexibility.
arxiv-5400-160 | LSSVM-ABC Algorithm for Stock Price prediction | http://arxiv.org/pdf/1402.6366v1.pdf | author:Osman Hegazy, Omar S. Soliman, Mustafa Abdul Salam category:cs.CE cs.NE published:2014-02-25 summary:In this paper, Artificial Bee Colony (ABC) algorithm which inspired from thebehavior of honey bees swarm is presented. ABC is a stochastic population-basedevolutionary algorithm for problem solving. ABC algorithm, which is consideredone of the most recently swarm intelligent techniques, is proposed to optimizeleast square support vector machine (LSSVM) to predict the daily stock prices.The proposed model is based on the study of stocks historical data, technicalindicators and optimizing LSSVM with ABC algorithm. ABC selects best freeparameters combination for LSSVM to avoid over-fitting and local minimaproblems and improve prediction accuracy. LSSVM optimized by Particle swarmoptimization (PSO) algorithm, LSSVM, and ANN techniques are used for comparisonwith proposed model. Proposed model tested with twenty datasets representingdifferent sectors in S&P 500 stock market. Results presented in this paper showthat the proposed model has fast convergence speed, and it also achieves betteraccuracy than compared techniques in most cases.
arxiv-5400-161 | Oracle-Based Robust Optimization via Online Learning | http://arxiv.org/pdf/1402.6361v1.pdf | author:Aharon Ben-Tal, Elad Hazan, Tomer Koren, Shie Mannor category:math.OC cs.LG published:2014-02-25 summary:Robust optimization is a common framework in optimization under uncertaintywhen the problem parameters are not known, but it is rather known that theparameters belong to some given uncertainty set. In the robust optimizationframework the problem solved is a min-max problem where a solution is judgedaccording to its performance on the worst possible realization of theparameters. In many cases, a straightforward solution of the robustoptimization problem of a certain type requires solving an optimization problemof a more complicated type, and in some cases even NP-hard. For example,solving a robust conic quadratic program, such as those arising in robust SVM,ellipsoidal uncertainty leads in general to a semidefinite program. In thispaper we develop a method for approximately solving a robust optimizationproblem using tools from online convex optimization, where in every stage astandard (non-robust) optimization program is solved. Our algorithms find anapproximate robust solution using a number of calls to an oracle that solvesthe original (non-robust) problem that is inversely proportional to the squareof the target accuracy.
arxiv-5400-162 | Unit Tests for Stochastic Optimization | http://arxiv.org/pdf/1312.6055v3.pdf | author:Tom Schaul, Ioannis Antonoglou, David Silver category:cs.LG published:2013-12-20 summary:Optimization by stochastic gradient descent is an important component of manylarge-scale machine learning algorithms. A wide variety of such optimizationalgorithms have been devised; however, it is unclear whether these algorithmsare robust and widely applicable across many different optimization landscapes.In this paper we develop a collection of unit tests for stochasticoptimization. Each unit test rapidly evaluates an optimization algorithm on asmall-scale, isolated, and well-understood difficulty, rather than inreal-world scenarios where many such issues are entangled. Passing these unittests is not sufficient, but absolutely necessary for any algorithms withclaims to generality or robustness. We give initial quantitative andqualitative results on numerous established algorithms. The testing frameworkis open-source, extensible, and easy to apply to new algorithms.
arxiv-5400-163 | The IBMAP approach for Markov networks structure learning | http://arxiv.org/pdf/1301.3720v2.pdf | author:Federico Schlüter, Facundo Bromberg, Alejandro Edera category:cs.AI cs.LG published:2013-01-16 summary:In this work we consider the problem of learning the structure of Markovnetworks from data. We present an approach for tackling this problem calledIBMAP, together with an efficient instantiation of the approach: the IBMAP-HCalgorithm, designed for avoiding important limitations of existingindependence-based algorithms. These algorithms proceed by performingstatistical independence tests on data, trusting completely the outcome of eachtest. In practice tests may be incorrect, resulting in potential cascadingerrors and the consequent reduction in the quality of the structures learned.IBMAP contemplates this uncertainty in the outcome of the tests through aprobabilistic maximum-a-posteriori approach. The approach is instantiated inthe IBMAP-HC algorithm, a structure selection strategy that performs apolynomial heuristic local search in the space of possible structures. Wepresent an extensive empirical evaluation on synthetic and real data, showingthat our algorithm outperforms significantly the current independence-basedalgorithms, in terms of data efficiency and quality of learned structures, withequivalent computational complexities. We also show the performance of IBMAP-HCin a real-world application of knowledge discovery: EDAs, which areevolutionary algorithms that use structure learning on each generation formodeling the distribution of populations. The experiments show that whenIBMAP-HC is used to learn the structure, EDAs improve the convergence to theoptimum.
arxiv-5400-164 | Improving Collaborative Filtering based Recommenders using Topic Modelling | http://arxiv.org/pdf/1402.6238v1.pdf | author:Jobin Wilson, Santanu Chaudhury, Brejesh Lall, Prateek Kapadia category:cs.IR cs.CL cs.LG published:2014-02-25 summary:Standard Collaborative Filtering (CF) algorithms make use of interactionsbetween users and items in the form of implicit or explicit ratings alone forgenerating recommendations. Similarity among users or items is calculatedpurely based on rating overlap in this case,without considering explicitproperties of users or items involved, limiting their applicability in domainswith very sparse rating spaces. In many domains such as movies, news orelectronic commerce recommenders, considerable contextual data in text formdescribing item properties is available along with the rating data, which couldbe utilized to improve recommendation quality.In this paper, we propose a novelapproach to improve standard CF based recommenders by utilizing latentDirichlet allocation (LDA) to learn latent properties of items, expressed interms of topic proportions, derived from their textual description. We inferuser's topic preferences or persona in the same latent space,based on herhistorical ratings. While computing similarity between users, we make use of acombined similarity measure involving rating overlap as well as similarity inthe latent topic space. This approach alleviates sparsity problem as it allowscalculation of similarity between users even if they have not rated any itemsin common. Our experiments on multiple public datasets indicate that theproposed hybrid approach significantly outperforms standard user Based and itemBased CF recommenders in terms of classification accuracy metrics such asprecision, recall and f-measure.
arxiv-5400-165 | Regularization of $\ell_1$ minimization for dealing with outliers and noise in Statistics and Signal Recovery | http://arxiv.org/pdf/1310.7637v2.pdf | author:Salvador Flores, Luis M. Briceno-Arias category:math.OC stat.ML published:2013-10-28 summary:We study the robustness properties of $\ell_1$ norm minimization for theclassical linear regression problem with a given design matrix andcontamination restricted to the dependent variable. We perform a fine erroranalysis of the $\ell_1$ estimator for measurements errors consisting ofoutliers coupled with noise. We introduce a new estimation technique resultingfrom a regularization of $\ell_1$ minimization by inf-convolution with the$\ell_2$ norm. Concerning robustness to large outliers, the proposed estimatorkeeps the breakdown point of the $\ell_1$ estimator, and reduces to leastsquares when there are not outliers. We present a globally convergentforward-backward algorithm for computing our estimator and some numericalexperiments confirming its theoretical properties.
arxiv-5400-166 | Asymmetric Pruning for Learning Cascade Detectors | http://arxiv.org/pdf/1303.6066v2.pdf | author:Sakrapee Paisitkriangkrai, Chunhua Shen, Anton van den Hengel category:cs.CV published:2013-03-25 summary:Cascade classifiers are one of the most important contributions to real-timeobject detection. Nonetheless, there are many challenging problems arising intraining cascade detectors. One common issue is that the node classifier istrained with a symmetric classifier. Having a low misclassification error ratedoes not guarantee an optimal node learning goal in cascade classifiers, i.e.,an extremely high detection rate with a moderate false positive rate. In thiswork, we present a new approach to train an effective node classifier in acascade detector. The algorithm is based on two key observations: 1) Redundantweak classifiers can be safely discarded; 2) The final detector should satisfythe asymmetric learning objective of the cascade architecture. To achieve this,we separate the classifier training into two steps: finding a pool ofdiscriminative weak classifiers/features and training the final classifier bypruning weak classifiers which contribute little to the asymmetric learningcriterion (asymmetric classifier construction). Our model reduction approachhelps accelerate the learning time while achieving the pre-determined learningobjective. Experimental results on both face and car data sets verify theeffectiveness of the proposed algorithm. On the FDDB face data sets, ourapproach achieves the state-of-the-art performance, which demonstrates theadvantage of our approach.
arxiv-5400-167 | Topology preserving thinning for cell complexes | http://arxiv.org/pdf/1309.1628v2.pdf | author:Paweł Dłotko, Ruben Specogna category:cs.CV published:2013-09-06 summary:A topology preserving skeleton is a synthetic representation of an objectthat retains its topology and many of its significant morphological properties.The process of obtaining the skeleton, referred to as skeletonization orthinning, is a very active research area. It plays a central role in reducingthe amount of information to be processed during image analysis andvisualization, computer-aided diagnosis or by pattern recognition algorithms. This paper introduces a novel topology preserving thinning algorithm whichremoves \textit{simple cells}---a generalization of simple points---of a givencell complex. The test for simple cells is based on \textit{acyclicity tables}automatically produced in advance with homology computations. Using acyclicitytables render the implementation of thinning algorithms straightforward.Moreover, the fact that tables are automatically filled for all possibleconfigurations allows to rigorously prove the generality of the algorithm andto obtain fool-proof implementations. The novel approach enables, for the firsttime, according to our knowledge, to thin a general unstructured simplicialcomplex. Acyclicity tables for cubical and simplicial complexes and an opensource implementation of the thinning algorithm are provided as additionalmaterial to allow their immediate use in the vast number of practicalapplications arising in medical imaging and beyond.
arxiv-5400-168 | Bayesian Sample Size Determination of Vibration Signals in Machine Learning Approach to Fault Diagnosis of Roller Bearings | http://arxiv.org/pdf/1402.6133v1.pdf | author:Siddhant Sahu, V. Sugumaran category:stat.ML cs.LG published:2014-02-25 summary:Sample size determination for a data set is an important statistical processfor analyzing the data to an optimum level of accuracy and using minimumcomputational work. The applications of this process are credible in everydomain which deals with large data sets and high computational work. This studyuses Bayesian analysis for determination of minimum sample size of vibrationsignals to be considered for fault diagnosis of a bearing using pre-definedparameters such as the inverse standard probability and the acceptable marginof error. Thus an analytical formula for sample size determination isintroduced. The fault diagnosis of the bearing is done using a machine learningapproach using an entropy-based J48 algorithm. The following method will helpresearchers involved in fault diagnosis to determine minimum sample size ofdata for analysis for a good statistical stability and precision.
arxiv-5400-169 | Inductive Logic Boosting | http://arxiv.org/pdf/1402.6077v1.pdf | author:Wang-Zhou Dai, Zhi-Hua Zhou category:cs.LG cs.AI published:2014-02-25 summary:Recent years have seen a surge of interest in Probabilistic Logic Programming(PLP) and Statistical Relational Learning (SRL) models that combine logic withprobabilities. Structure learning of these systems is an intersection area ofInductive Logic Programming (ILP) and statistical learning (SL). However, ILPcannot deal with probabilities, SL cannot model relational hypothesis. Thebiggest challenge of integrating these two machine learning frameworks is howto estimate the probability of a logic clause only from the observation ofgrounded logic atoms. Many current methods models a joint probability byrepresenting clause as graphical model and literals as vertices in it. Thismodel is still too complicate and only can be approximate by pseudo-likelihood.We propose Inductive Logic Boosting framework to transform the relationaldataset into a feature-based dataset, induces logic rules by boosting ProblogRule Trees and relaxes the independence constraint of pseudo-likelihood.Experimental evaluation on benchmark datasets demonstrates that the AUC-PR andAUC-ROC value of ILP learned rules are higher than current state-of-the-art SRLmethods.
arxiv-5400-170 | Machine Learning at Scale | http://arxiv.org/pdf/1402.6076v1.pdf | author:Sergei Izrailev, Jeremy M. Stanley category:cs.LG cs.MS stat.ML I.5.2 published:2014-02-25 summary:It takes skill to build a meaningful predictive model even with the abundanceof implementations of modern machine learning algorithms and readily availablecomputing resources. Building a model becomes challenging if hundreds ofterabytes of data need to be processed to produce the training data set. In adigital advertising technology setting, we are faced with the need to buildthousands of such models that predict user behavior and power advertisingcampaigns in a 24/7 chaotic real-time production environment. As datascientists, we also have to convince other internal departments critical toimplementation success, our management, and our customers that our machinelearning system works. In this paper, we present the details of the design andimplementation of an automated, robust machine learning platform that impactsbillions of advertising impressions monthly. This platform enables us tocontinuously optimize thousands of campaigns over hundreds of millions ofusers, on multiple continents, against varying performance objectives.
arxiv-5400-171 | Intrinsically Motivated Learning of Visual Motion Perception and Smooth Pursuit | http://arxiv.org/pdf/1402.3344v2.pdf | author:Chong Zhang, Yu Zhao, Jochen Triesch, Bertram E. Shi category:cs.CV q-bio.NC published:2014-02-14 summary:We extend the framework of efficient coding, which has been used to model thedevelopment of sensory processing in isolation, to model the development of theperception/action cycle. Our extension combines sparse coding and reinforcementlearning so that sensory processing and behavior co-develop to optimize ashared intrinsic motivational signal: the fidelity of the neural encoding ofthe sensory input under resource constraints. Applying this framework to amodel system consisting of an active eye behaving in a time varyingenvironment, we find that this generic principle leads to the simultaneousdevelopment of both smooth pursuit behavior and model neurons whose propertiesare similar to those of primary visual cortical neurons selective for differentdirections of visual motion. We suggest that this general principle may formthe basis for a unified and integrated explanation of many perception/actionloops.
arxiv-5400-172 | A DCT Approximation for Image Compression | http://arxiv.org/pdf/1402.6034v1.pdf | author:R. J. Cintra, F. M. Bayer category:cs.MM cs.CV stat.ME published:2014-02-25 summary:An orthogonal approximation for the 8-point discrete cosine transform (DCT)is introduced. The proposed transformation matrix contains only zeros and ones;multiplications and bit-shift operations are absent. Close spectral behaviorrelative to the DCT was adopted as design criterion. The proposed algorithm issuperior to the signed discrete cosine transform. It could also outperformstate-of-the-art algorithms in low and high image compression scenarios,exhibiting at the same time a comparable computational complexity.
arxiv-5400-173 | Algorithms for multi-armed bandit problems | http://arxiv.org/pdf/1402.6028v1.pdf | author:Volodymyr Kuleshov, Doina Precup category:cs.AI cs.LG published:2014-02-25 summary:Although many algorithms for the multi-armed bandit problem arewell-understood theoretically, empirical confirmation of their effectiveness isgenerally scarce. This paper presents a thorough empirical study of the mostpopular multi-armed bandit algorithms. Three important observations can be madefrom our results. Firstly, simple heuristics such as epsilon-greedy andBoltzmann exploration outperform theoretically sound algorithms on mostsettings by a significant margin. Secondly, the performance of most algorithmsvaries dramatically with the parameters of the bandit problem. Our studyidentifies for each algorithm the settings where it performs well, and thesettings where it performs poorly. Thirdly, the algorithms' performancerelative each to other is affected only by the number of bandit arms and thevariance of the rewards. This finding may guide the design of subsequentempirical evaluations. In the second part of the paper, we turn our attentionto an important area of application of bandit algorithms: clinical trials.Although the design of clinical trials has been one of the principal practicalproblems motivating research on multi-armed bandits, bandit algorithms havenever been evaluated as potential treatment allocation strategies. Using datafrom a real study, we simulate the outcome that a 2001-2002 clinical trialwould have had if bandit algorithms had been used to allocate patients totreatments. We find that an adaptive trial would have successfully treated atleast 50% more patients, while significantly reducing the number of adverseeffects and increasing patient retention. At the end of the trial, the besttreatment could have still been identified with a high level of statisticalconfidence. Our findings demonstrate that bandit algorithms are attractivealternatives to current adaptive treatment allocation strategies.
arxiv-5400-174 | Precision Enhancement of 3D Surfaces from Multiple Compressed Depth Maps | http://arxiv.org/pdf/1405.2062v1.pdf | author:Pengfei Wan, Gene Cheung, Philip A. Chou, Dinei Florencio, Cha Zhang, Oscar C. Au category:cs.CV published:2014-02-25 summary:In texture-plus-depth representation of a 3D scene, depth maps from differentcamera viewpoints are typically lossily compressed via the classical transformcoding / coefficient quantization paradigm. In this paper we propose to reducedistortion of the decoded depth maps due to quantization. The key observationis that depth maps from different viewpoints constitute multiple descriptions(MD) of the same 3D scene. Considering the MD jointly, we perform a POCS-likeiterative procedure to project a reconstructed signal from one depth map to theother and back, so that the converged depth maps have higher precision than theoriginal quantized versions.
arxiv-5400-175 | Open science in machine learning | http://arxiv.org/pdf/1402.6013v1.pdf | author:Joaquin Vanschoren, Mikio L. Braun, Cheng Soon Ong category:cs.LG cs.DL published:2014-02-24 summary:We present OpenML and mldata, open science platforms that provides easyaccess to machine learning data, software and results to encourage furtherstudy and application. They go beyond the more traditional repositories fordata sets and software packages in that they allow researchers to also easilyshare the results they obtained in experiments and to compare their solutionswith those of others.
arxiv-5400-176 | DCT-like Transform for Image and Video Compression Requires 10 Additions Only | http://arxiv.org/pdf/1402.5979v1.pdf | author:R. J. Cintra, F. M. Bayer, V. A. Coutinho, S. Kulasekera, A. Madanayake category:cs.MM cs.CV stat.ME published:2014-02-24 summary:A multiplierless pruned approximate 8-point discrete cosine transform (DCT)requiring only 10 additions is introduced. The proposed algorithm was assessedin image and video compression, showing competitive performance withstate-of-the-art methods. Digital implementation in 45 nm CMOS technology up toplace-and-route level indicates clock speed of 255 MHz at a 1.1 V supply. The8x8 block rate is 31.875 MHz.The DCT approximation was embedded into HEVCreference software; resulting video frames, at up to 327 Hz for 8-bit RGB HEVC,presented negligible image degradation.
arxiv-5400-177 | A Testbed for Cross-Dataset Analysis | http://arxiv.org/pdf/1402.5923v1.pdf | author:Tatiana Tommasi, Tinne Tuytelaars, Barbara Caputo category:cs.CV published:2014-02-24 summary:Since its beginning visual recognition research has tried to capture the hugevariability of the visual world in several image collections. The number ofavailable datasets is still progressively growing together with the amount ofsamples per object category. However, this trend does not correspond directlyto an increasing in the generalization capabilities of the developedrecognition systems. Each collection tends to have its specific characteristicsand to cover just some aspects of the visual world: these biases often narrowthe effect of the methods defined and tested separately over each image set.Our work makes a first step towards the analysis of the dataset bias problem ona large scale. We organize twelve existing databases in a unique corpus and wepresent the visual community with a useful feature repository for futureresearch.
arxiv-5400-178 | Near Optimal Bayesian Active Learning for Decision Making | http://arxiv.org/pdf/1402.5886v1.pdf | author:Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, J. Andrew Bagnell, Siddhartha Srinivasa category:cs.LG cs.AI published:2014-02-24 summary:How should we gather information to make effective decisions? We addressBayesian active learning and experimental design problems, where wesequentially select tests to reduce uncertainty about a set of hypotheses.Instead of minimizing uncertainty per se, we consider a set of overlappingdecision regions of these hypotheses. Our goal is to drive uncertainty into asingle decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying adecision region that contains all hypotheses consistent with observations. Wedevelop a novel Hyperedge Cutting (HEC) algorithm for this problem, and provethat is competitive with the intractable optimal policy. Our efficientimplementation of the algorithm relies on computing subsets of the completehomogeneous symmetric polynomials. Finally, we demonstrate its effectiveness ontwo practical applications: approximate comparison-based learning and activelocalization using a robot manipulator.
arxiv-5400-179 | A Novel Face Recognition Method using Nearest Line Projection | http://arxiv.org/pdf/1402.5859v1.pdf | author:Huanguo Zhang, Sha Lv, Wei Li, Xun Qu category:cs.CV published:2014-02-24 summary:Face recognition is a popular application of pat- tern recognition methods,and it faces challenging problems including illumination, expression, and pose.The most popular way is to learn the subspaces of the face images so that itcould be project to another discriminant space where images of differentpersons can be separated. In this paper, a nearest line projection algorithm isdeveloped to represent the face images for face recognition. Instead ofprojecting an image to its nearest image, we try to project it to its nearestline spanned by two different face images. The subspaces are learned so thateach face image to its nearest line is minimized. We evaluated the proposedalgorithm on some benchmark face image database, and also compared it to someother image projection algorithms. The experiment results showed that theproposed algorithm outperforms other ones.
arxiv-5400-180 | Learning directed acyclic graphs based on sparsest permutations | http://arxiv.org/pdf/1307.0366v3.pdf | author:Garvesh Raskutti, Caroline Uhler category:math.ST cs.LG stat.TH published:2013-07-01 summary:We consider the problem of learning a Bayesian network or directed acyclicgraph (DAG) model from observational data. A number of constraint-based,score-based and hybrid algorithms have been developed for this purpose. Forconstraint-based methods, statistical consistency guarantees typically rely onthe faithfulness assumption, which has been show to be restrictive especiallyfor graphs with cycles in the skeleton. However, there is only limited work onconsistency guarantees for score-based and hybrid algorithms and it has beenunclear whether consistency guarantees can be proven under weaker conditionsthan the faithfulness assumption. In this paper, we propose the sparsest permutation (SP) algorithm. Thisalgorithm is based on finding the causal ordering of the variables that yieldsthe sparsest DAG. We prove that this new score-based method is consistent understrictly weaker conditions than the faithfulness assumption. We alsodemonstrate through simulations on small DAGs that the SP algorithm comparesfavorably to the constraint-based PC and SGS algorithms as well as thescore-based Greedy Equivalence Search and hybrid Max-Min Hill-Climbing method.In the Gaussian setting, we prove that our algorithm boils down to finding thepermutation of the variables with sparsest Cholesky decomposition for theinverse covariance matrix. Using this connection, we show that in the oraclesetting, where the true covariance matrix is known, the SP algorithm is in factequivalent to $\ell_0$-penalized maximum likelihood estimation.
arxiv-5400-181 | Automatic Estimation of Live Coffee Leaf Infection based on Image Processing Techniques | http://arxiv.org/pdf/1402.5805v1.pdf | author:Eric Hitimana, Oubong Gwun category:cs.CV published:2014-02-24 summary:Image segmentation is the most challenging issue in computer visionapplications. And most difficulties for crops management in agriculture are thelack of appropriate methods for detecting the leaf damage for pests treatment.In this paper we proposed an automatic method for leaf damage detection andseverity estimation of coffee leaf by avoiding defoliation. After enhancing thecontrast of the original image using LUT based gamma correction, the image isprocessed to remove the background, and the output leaf is clustered usingFuzzy c-means segmentation in V channel of YUV color space to maximize all leafdamage detection, and finally, the severity of leaf is estimated in terms ofratio for leaf pixel distribution between the normal and the detected leafdamage. The results in each proposed method was compared to the currentresearches and the accuracy is obvious either in the background removal ordamage detection.
arxiv-5400-182 | Sparse phase retrieval via group-sparse optimization | http://arxiv.org/pdf/1402.5803v1.pdf | author:Fabien Lauer, Henrik Ohlsson category:cs.IT cs.LG math.IT published:2014-02-24 summary:This paper deals with sparse phase retrieval, i.e., the problem of estimatinga vector from quadratic measurements under the assumption that few componentsare nonzero. In particular, we consider the problem of finding the sparsestvector consistent with the measurements and reformulate it as a group-sparseoptimization problem with linear constraints. Then, we analyze the convexrelaxation of the latter based on the minimization of a block l1-norm and showvarious exact recovery and stability results in the real and complex cases.Invariance to circular shifts and reflections are also discussed for realvectors measured via complex matrices.
arxiv-5400-183 | No more meta-parameter tuning in unsupervised sparse feature learning | http://arxiv.org/pdf/1402.5766v1.pdf | author:Adriana Romero, Petia Radeva, Carlo Gatta category:cs.LG cs.CV published:2014-02-24 summary:We propose a meta-parameter free, off-the-shelf, simple and fast unsupervisedfeature learning algorithm, which exploits a new way of optimizing forsparsity. Experiments on STL-10 show that the method presents state-of-the-artperformance and provides discriminative features that generalize well.
arxiv-5400-184 | Bandits with concave rewards and convex knapsacks | http://arxiv.org/pdf/1402.5758v1.pdf | author:Shipra Agrawal, Nikhil R. Devanur category:cs.LG published:2014-02-24 summary:In this paper, we consider a very general model for exploration-exploitationtradeoff which allows arbitrary concave rewards and convex constraints on thedecisions across time, in addition to the customary limitation on the timehorizon. This model subsumes the classic multi-armed bandit (MAB) model, andthe Bandits with Knapsacks (BwK) model of Badanidiyuru et al.[2013]. We alsoconsider an extension of this model to allow linear contexts, similar to thelinear contextual extension of the MAB model. We demonstrate that a natural andsimple extension of the UCB family of algorithms for MAB provides a polynomialtime algorithm that has near-optimal regret guarantees for this substantiallymore general model, and matches the bounds provided by Badanidiyuru etal.[2013] for the special case of BwK, which is quite surprising. We alsoprovide computationally more efficient algorithms by establishing interestingconnections between this problem and other well studied problems/algorithmssuch as the Blackwell approachability problem, online convex optimization, andthe Frank-Wolfe technique for convex optimization. We give examples of severalconcrete applications, where this more general model of bandits allows forricher and/or more efficient formulations of the problem.
arxiv-5400-185 | Machine Learning Methods in the Computational Biology of Cancer | http://arxiv.org/pdf/1402.5728v1.pdf | author:Mathukumalli Vidyasagar category:q-bio.QM cs.LG stat.ML 62P10 published:2014-02-24 summary:The objectives of this "perspective" paper are to review some recent advancesin sparse feature selection for regression and classification, as well ascompressed sensing, and to discuss how these might be used to develop tools toadvance personalized cancer therapy. As an illustration of the possibilities, anew algorithm for sparse regression is presented, and is applied to predict thetime to tumor recurrence in ovarian cancer. A new algorithm for sparse featureselection in classification problems is presented, and its validation inendometrial cancer is briefly discussed. Some open problems are also presented.
arxiv-5400-186 | OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks | http://arxiv.org/pdf/1312.6229v4.pdf | author:Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun category:cs.CV published:2013-12-21 summary:We present an integrated framework for using Convolutional Networks forclassification, localization and detection. We show how a multiscale andsliding window approach can be efficiently implemented within a ConvNet. Wealso introduce a novel deep learning approach to localization by learning topredict object boundaries. Bounding boxes are then accumulated rather thansuppressed in order to increase detection confidence. We show that differenttasks can be learned simultaneously using a single shared network. Thisintegrated framework is the winner of the localization task of the ImageNetLarge Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained verycompetitive results for the detection and classifications tasks. Inpost-competition work, we establish a new state of the art for the detectiontask. Finally, we release a feature extractor from our best model calledOverFeat.
arxiv-5400-187 | The Cerebellum: New Computational Model that Reveals its Primary Function to Calculate Multibody Dynamics Conform to Lagrange-Euler Formulation | http://arxiv.org/pdf/1402.5708v1.pdf | author:Lavdim Kurtaj, Ilir Limani, Vjosa Shatri, Avni Skeja category:cs.NE cs.CE cs.RO q-bio.NC published:2014-02-24 summary:Cerebellum is part of the brain that occupies only 10% of the brain volume,but it contains about 80% of total number of brain neurons. New cerebellarfunction model is developed that sets cerebellar circuits in context ofmultibody dynamics model computations, as important step in controlling balanceand movement coordination, functions performed by two oldest parts of thecerebellum. Model gives new functional interpretation for granule cells-Golgicell circuit, including distinct function for upper and lower Golgi celldendritc trees, and resolves issue of sharing Granule cells between Purkinjecells. Sets new function for basket cells, and for stellate cells according toposition in molecular layer. New model enables easily and direct integration ofsensory information from vestibular system and cutaneous mechanoreceptors, forbalance, movement and interaction with environments. Model gives explanation ofPurkinje cells convergence on deep-cerebellar nuclei.
arxiv-5400-188 | Bayesian Inference for NMR Spectroscopy with Applications to Chemical Quantification | http://arxiv.org/pdf/1402.3580v2.pdf | author:Andrew Gordon Wilson, Yuting Wu, Daniel J. Holland, Sebastian Nowozin, Mick D. Mantle, Lynn F. Gladden, Andrew Blake category:stat.AP stat.ME stat.ML published:2014-02-14 summary:Nuclear magnetic resonance (NMR) spectroscopy exploits the magneticproperties of atomic nuclei to discover the structure, reaction state andchemical environment of molecules. We propose a probabilistic generative modeland inference procedures for NMR spectroscopy. Specifically, we use a weightedsum of trigonometric functions undergoing exponential decay to model freeinduction decay (FID) signals. We discuss the challenges in estimating thecomponents of this general model -- amplitudes, phase shifts, frequencies,decay rates, and noise variances -- and offer practical solutions. We comparewith conventional Fourier transform spectroscopy for estimating the relativeconcentrations of chemicals in a mixture, using synthetic and experimentallyacquired FID signals. We find the proposed model is particularly robust to lowsignal to noise ratios (SNR), and overlapping peaks in the Fourier transform ofthe FID, enabling accurate predictions (e.g., 1% sensitivity at low SNR) whichare not possible with conventional spectroscopy (5% sensitivity).
arxiv-5400-189 | Exemplar-based Linear Discriminant Analysis for Robust Object Tracking | http://arxiv.org/pdf/1402.5697v1.pdf | author:Changxin Gao, Feifei Chen, Jin-Gang Yu, Rui Huang, Nong Sang category:cs.CV published:2014-02-24 summary:Tracking-by-detection has become an attractive tracking technique, whichtreats tracking as a category detection problem. However, the task in trackingis to search for a specific object, rather than an object category as indetection. In this paper, we propose a novel tracking framework based onexemplar detector rather than category detector. The proposed tracker is anensemble of exemplar-based linear discriminant analysis (ELDA) detectors. Eachdetector is quite specific and discriminative, because it is trained by asingle object instance and massive negatives. To improve its adaptivity, weupdate both object and background models. Experimental results on severalchallenging video sequences demonstrate the effectiveness and robustness of ourtracking algorithm.
arxiv-5400-190 | Accelerating ABC methods using Gaussian processes | http://arxiv.org/pdf/1401.1436v2.pdf | author:Richard D Wilkinson category:stat.CO stat.ML published:2014-01-07 summary:Approximate Bayesian computation (ABC) methods are used to approximateposterior distributions using simulation rather than likelihood calculations.We introduce Gaussian process (GP) accelerated ABC, which we show cansignificantly reduce the number of simulations required. As computationalresource is usually the main determinant of accuracy in ABC, GP-acceleratedmethods can thus enable more accurate inference in some models. GP models ofthe unknown log-likelihood function are used to exploit continuity andsmoothness, reducing the required computation. We use a sequence of models thatincrease in accuracy, using intermediate models to rule out regions of theparameter space as implausible. The methods will not be suitable for allproblems, but when they can be used, can result in significant computationalsavings. For the Ricker model, we are able to achieve accurate approximationsto the posterior distribution using a factor of 100 fewer simulator evaluationsthan comparable Monte Carlo approaches, and for a population genetics model weare able to approximate the exact posterior for the first time.
arxiv-5400-191 | To go deep or wide in learning? | http://arxiv.org/pdf/1402.5634v1.pdf | author:Gaurav Pandey, Ambedkar Dukkipati category:cs.LG published:2014-02-23 summary:To achieve acceptable performance for AI tasks, one can either usesophisticated feature extraction methods as the first layer in a two-layeredsupervised learning model, or learn the features directly using a deep(multi-layered) model. While the first approach is very problem-specific, thesecond approach has computational overheads in learning multiple layers andfine-tuning of the model. In this paper, we propose an approach called widelearning based on arc-cosine kernels, that learns a single layer of infinitewidth. We propose exact and inexact learning strategies for wide learning andshow that wide learning with single layer outperforms single layer as well asdeep architectures of finite width for some benchmark datasets.
arxiv-5400-192 | Localization of License Plate Using Morphological Operations | http://arxiv.org/pdf/1402.5623v1.pdf | author:V. Karthikeyan, V. J. Vijayalakshmi category:cs.CV published:2014-02-23 summary:It is believed that there are currently millions of vehicles on the roadsworldwide. The over speed of vehicles,theft of vehicles, disobeying trafficrules in public, an unauthorized person entering the restricted area are keepon increasing. In order restrict against these criminal activities, we need anautomatic public security system. Each vehicle has their own VehicleIdentification Number (VIN) as their primary identifier. The VIN is actually aLicense Number which states a legal license to participate in the publictraffic. The proposed paper is to identify the vehicle with the help ofvehicles License Plate (LP).LPRS is one the most important part of theIntelligent Transportation System (ITS) to locate the LP. In this paper certainexisting algorithm drawbacks are overcome by the proposed morphologicaloperations for LPRS. Morphological operation is chosen due to its higherefficiency, noise filter capacity, accuracy, exact localization of LP andspeed.
arxiv-5400-193 | A Novel Histogram Based Robust Image Registration Technique | http://arxiv.org/pdf/1402.5619v1.pdf | author:V. Karthikeyan category:cs.CV published:2014-02-23 summary:In this paper, a method for Automatic Image Registration (AIR) throughhistogram is proposed. Automatic image registration is one of the crucial stepsin the analysis of remotely sensed data. A new acquired image must betransformed, using image registration techniques, to match the orientation andscale of previous related images. This new approach combines severalsegmentations of the pair of images to be registered. A relaxation parameter onthe histogram modes delineation is introduced. It is followed bycharacterization of the extracted objects through the objects area, axis ratio,and perimeter and fractal dimension. The matched objects are used for rotationand translation estimation. It allows for the registration of pairs of imageswith differences in rotation and translation. This method contributes tosubpixel accuracy.
arxiv-5400-194 | Path Thresholding: Asymptotically Tuning-Free High-Dimensional Sparse Regression | http://arxiv.org/pdf/1402.5584v1.pdf | author:Divyanshu Vats, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH published:2014-02-23 summary:In this paper, we address the challenging problem of selecting tuningparameters for high-dimensional sparse regression. We propose a simple andcomputationally efficient method, called path thresholding (PaTh), thattransforms any tuning parameter-dependent sparse regression algorithm into anasymptotically tuning-free sparse regression algorithm. More specifically, weprove that, as the problem size becomes large (in the number of variables andin the number of observations), PaTh performs accurate sparse regression, underappropriate conditions, without specifying a tuning parameter. Infinite-dimensional settings, we demonstrate that PaTh can alleviate thecomputational burden of model selection algorithms by significantly reducingthe search space of tuning parameters.
arxiv-5400-195 | Semi-Supervised Nonlinear Distance Metric Learning via Forests of Max-Margin Cluster Hierarchies | http://arxiv.org/pdf/1402.5565v1.pdf | author:David M. Johnson, Caiming Xiong, Jason J. Corso category:stat.ML cs.IR cs.LG I.5.3; H.2.8 published:2014-02-23 summary:Metric learning is a key problem for many data mining and machine learningapplications, and has long been dominated by Mahalanobis methods. Recentadvances in nonlinear metric learning have demonstrated the potential power ofnon-Mahalanobis distance functions, particularly tree-based functions. Wepropose a novel nonlinear metric learning method that uses an iterative,hierarchical variant of semi-supervised max-margin clustering to construct aforest of cluster hierarchies, where each individual hierarchy can beinterpreted as a weak metric over the data. By introducing randomness duringhierarchy training and combining the output of many of the resultingsemi-random weak hierarchy metrics, we can obtain a powerful and robustnonlinear metric model. This method has two primary contributions: first, it issemi-supervised, incorporating information from both constrained andunconstrained points. Second, we take a relaxed approach to constraintsatisfaction, allowing the method to satisfy different subsets of theconstraints at different levels of the hierarchy rather than attempting tosimultaneously satisfy all of them. This leads to a more robust learningalgorithm. We compare our method to a number of state-of-the-art benchmarks on$k$-nearest neighbor classification, large-scale image retrieval andsemi-supervised clustering problems, and find that our algorithm yields resultscomparable or superior to the state-of-the-art, and is significantly morerobust to noise.
arxiv-5400-196 | Swapping Variables for High-Dimensional Sparse Regression with Correlated Measurements | http://arxiv.org/pdf/1312.1706v2.pdf | author:Divyanshu Vats, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH published:2013-12-05 summary:We consider the high-dimensional sparse linear regression problem ofaccurately estimating a sparse vector using a small number of linearmeasurements that are contaminated by noise. It is well known that the standardcadre of computationally tractable sparse regression algorithms---such as theLasso, Orthogonal Matching Pursuit (OMP), and their extensions---perform poorlywhen the measurement matrix contains highly correlated columns. To address thisshortcoming, we develop a simple greedy algorithm, called SWAP, thatiteratively swaps variables until convergence. SWAP is surprisingly effectivein handling measurement matrices with high correlations. In fact, we prove thatSWAP outputs the true support, the locations of the non-zero entries in thesparse vector, under a relatively mild condition on the measurement matrix.Furthermore, we show that SWAP can be used to boost the performance of anysparse regression algorithm. We empirically demonstrate the advantages of SWAPby comparing it with several state-of-the-art sparse regression algorithms.
arxiv-5400-197 | A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An Actor-Critic Approach | http://arxiv.org/pdf/1401.3579v2.pdf | author:Keyvan Yahya category:cs.GT cs.AI cs.LG published:2013-12-20 summary:This paper aims to find an algorithmic structure that affords to predict andexplain economical choice behaviour particularly under uncertainty(randompolicies) by manipulating the prevalent Actor-Critic learning method to complywith the requirements we have been entrusted ever since the field ofneuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics thatseem relevant to our discussion, we will try to outline some of the importantworks which have so far been done to simulate choice making processes.Concerning neurological findings that suggest the existence of two specificfunctions that are executed through Basal Ganglia all the way up to sub-cortical areas, namely 'rewards' and 'beliefs', we will offer a modifiedversion of actor/critic algorithm to shed a light on the relation between thesefunctions and most importantly resolve what is referred to as a challenge foractor-critic algorithms, that is, the lack of inheritance or hierarchy whichavoids the system being evolved in continuous time tasks whence the convergencemight not be emerged.
arxiv-5400-198 | Efficient Semidefinite Spectral Clustering via Lagrange Duality | http://arxiv.org/pdf/1402.5497v1.pdf | author:Yan Yan, Chunhua Shen, Hanzi Wang category:cs.LG cs.CV published:2014-02-22 summary:We propose an efficient approach to semidefinite spectral clustering (SSC),which addresses the Frobenius normalization with the positive semidefinite(p.s.d.) constraint for spectral clustering. Compared with the originalFrobenius norm approximation based algorithm, the proposed algorithm can moreaccurately find the closest doubly stochastic approximation to the affinitymatrix by considering the p.s.d. constraint. In this paper, SSC is formulatedas a semidefinite programming (SDP) problem. In order to solve the highcomputational complexity of SDP, we present a dual algorithm based on theLagrange dual formalization. Two versions of the proposed algorithm areproffered: one with less memory usage and the other with faster convergencerate. The proposed algorithm has much lower time complexity than that of thestandard interior-point based SDP solvers. Experimental results on both UCIdata sets and real-world image data sets demonstrate that 1) compared with thestate-of-the-art spectral clustering methods, the proposed algorithm achievesbetter clustering performance; and 2) our algorithm is much more efficient andcan solve larger-scale SSC problems than those standard interior-point SDPsolvers.
arxiv-5400-199 | Class Proportion Estimation with Application to Multiclass Anomaly Rejection | http://arxiv.org/pdf/1306.5056v3.pdf | author:Tyler Sanderson, Clayton Scott category:stat.ML cs.LG published:2013-06-21 summary:This work addresses two classification problems that fall under the headingof domain adaptation, wherein the distributions of training and testingexamples differ. The first problem studied is that of class proportionestimation, which is the problem of estimating the class proportions in anunlabeled testing data set given labeled examples of each class. Compared toprevious work on this problem, our approach has the novel feature that it doesnot require labeled training data from one of the classes. This property allowsus to address the second domain adaptation problem, namely, multiclass anomalyrejection. Here, the goal is to design a classifier that has the option ofassigning a "reject" label, indicating that the instance did not arise from aclass present in the training data. We establish consistent learning strategiesfor both of these domain adaptation problems, which to our knowledge are thefirst of their kind. We also implement the class proportion estimationtechnique and demonstrate its performance on several benchmark data sets.
arxiv-5400-200 | Scaling Nonparametric Bayesian Inference via Subsample-Annealing | http://arxiv.org/pdf/1402.5473v1.pdf | author:Fritz Obermeyer, Jonathan Glidden, Eric Jonas category:stat.ML stat.CO published:2014-02-22 summary:We describe an adaptation of the simulated annealing algorithm tononparametric clustering and related probabilistic models. This new algorithmlearns nonparametric latent structure over a growing and constantly churningsubsample of training data, where the portion of data subsampled can beinterpreted as the inverse temperature beta(t) in an annealing schedule. Gibbssampling at high temperature (i.e., with a very small subsample) can morequickly explore sketches of the final latent state by (a) making longer jumpsaround latent space (as in block Gibbs) and (b) lowering energy barriers (as insimulated annealing). We prove subsample annealing speeds up mixing time N^2 ->N in a simple clustering model and exp(N) -> N in another class of models,where N is data size. Empirically subsample-annealing outperforms naive Gibbssampling in accuracy-per-wallclock time, and can scale to larger datasets anddeeper hierarchical models. We demonstrate improved inference on million-rowsubsamples of US Census data and network log data and a 307-row hospital ratingdataset, using a Pitman-Yor generalization of the Cross Categorization model.
arxiv-5400-201 | Information Aggregation in Exponential Family Markets | http://arxiv.org/pdf/1402.5458v1.pdf | author:Jacob Abernethy, Sindhu Kutty, Sébastien Lahaie, Rahul Sami category:cs.AI cs.GT stat.ML published:2014-02-22 summary:We consider the design of prediction market mechanisms known as automatedmarket makers. We show that we can design these mechanisms via the mold of\emph{exponential family distributions}, a popular and well-studied probabilitydistribution template used in statistics. We give a full development of thisrelationship and explore a range of benefits. We draw connections between theinformation aggregation of market prices and the belief aggregation of learningagents that rely on exponential family distributions. We develop a very naturalanalysis of the market behavior as well as the price equilibrium under theassumption that the traders exhibit risk aversion according to exponentialutility. We also consider similar aspects under alternative models, such aswhen traders are budget constrained.
arxiv-5400-202 | Le Cam meets LeCun: Deficiency and Generic Feature Learning | http://arxiv.org/pdf/1402.4884v2.pdf | author:Brendan van Rooyen, Robert C. Williamson category:stat.ML published:2014-02-20 summary:"Deep Learning" methods attempt to learn generic features in an unsupervisedfashion from a large unlabelled data set. These generic features should performas well as the best hand crafted features for any learning problem that makesuse of this data. We provide a definition of generic features, characterizewhen it is possible to learn them and provide methods closely related to theautoencoder and deep belief network of deep learning. In order to do so we usethe notion of deficiency and illustrate its value in studying certain generallearning problems.
arxiv-5400-203 | An Evolutionary approach for solving Shrödinger Equation | http://arxiv.org/pdf/1402.5428v1.pdf | author:Khalid jebari, Mohammed Madiafi, Abdelaziz Elmoujahid category:cs.NE published:2014-02-21 summary:The purpose of this paper is to present a method of solving the Shr\"odingerEquation (SE) by Genetic Algorithms and Grammatical Evolution. The method formsgenerations of trial solutions expressed in an analytical form. We illustratethe effectiveness of this method providing, for example, the results of itsapplication to a quantum system minimal energy, and we compare these resultswith those produced by traditional analytical methods
arxiv-5400-204 | Recovering the Optimal Solution by Dual Random Projection | http://arxiv.org/pdf/1211.3046v4.pdf | author:Lijun Zhang, Mehrdad Mahdavi, Rong Jin, Tianbao Yang, Shenghuo Zhu category:cs.LG published:2012-11-13 summary:Random projection has been widely used in data classification. It mapshigh-dimensional data into a low-dimensional subspace in order to reduce thecomputational cost in solving the related optimization problem. While previousstudies are focused on analyzing the classification performance of using randomprojection, in this work, we consider the recovery problem, i.e., how toaccurately recover the optimal solution to the original optimization problem inthe high-dimensional space based on the solution learned from the subspacespanned by random projections. We present a simple algorithm, termed DualRandom Projection, that uses the dual solution of the low-dimensionaloptimization problem to recover the optimal solution to the original problem.Our theoretical analysis shows that with a high probability, the proposedalgorithm is able to accurately recover the optimal solution to the originalproblem, provided that the data matrix is of low rank or can be wellapproximated by a low rank matrix.
arxiv-5400-205 | Important Molecular Descriptors Selection Using Self Tuned Reweighted Sampling Method for Prediction of Antituberculosis Activity | http://arxiv.org/pdf/1402.5360v1.pdf | author:Doreswamy, Chanabasayya M. Vastrad category:cs.LG stat.AP stat.ML published:2014-02-21 summary:In this paper, a new descriptor selection method for selecting an optimalcombination of important descriptors of sulfonamide derivatives data, namedself tuned reweighted sampling (STRS), is developed. descriptors are defined asthe descriptors with large absolute coefficients in a multivariate linearregression model such as partial least squares(PLS). In this study, theabsolute values of regression coefficients of PLS model are used as an indexfor evaluating the importance of each descriptor Then, based on the importancelevel of each descriptor, STRS sequentially selects N subsets of descriptorsfrom N Monte Carlo (MC) sampling runs in an iterative and competitive manner.In each sampling run, a fixed ratio (e.g. 80%) of samples is first randomlyselected to establish a regresson model. Next, based on the regressioncoefficients, a two-step procedure including rapidly decreasing function (RDF)based enforced descriptor selection and self tuned sampling (STS) basedcompetitive descriptor selection is adopted to select the importantdescriptorss. After running the loops, a number of subsets of descriptors areobtained and root mean squared error of cross validation (RMSECV) of PLS modelsestablished with subsets of descriptors is computed. The subset of descriptorswith the lowest RMSECV is considered as the optimal descriptor subset. Theperformance of the proposed algorithm is evaluated by sulfanomide derivativedataset. The results reveal an good characteristic of STRS that it can usuallylocate an optimal combination of some important descriptors which areinterpretable to the biologically of interest. Additionally, our study showsthat better prediction is obtained by STRS when compared to full descriptor setPLS modeling, Monte Carlo uninformative variable elimination (MC-UVE).
arxiv-5400-206 | Exploiting Two-Dimensional Group Sparsity in 1-Bit Compressive Sensing | http://arxiv.org/pdf/1402.5073v2.pdf | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT math.IT published:2014-02-20 summary:We propose a new approach, {\it two-dimensional fused binary compressivesensing} (2DFBCS) to recover 2D sparse piece-wise signals from 1-bitmeasurements, exploiting 2D group sparsity for 1-bit compressive sensingrecovery. The proposed method is a modified 2D version of the previous {\itbinary iterative hard thresholding} (2DBIHT) algorithm, where the objectivefunction includes a 2D one-sided $\ell_1$ (or $\ell_2$) penalty functionencouraging agreement with the observed data, an indicator function of$K$-sparsity, and a total variation (TV) or modified TV (MTV) constraint. Thesubgradient of the 2D one-sided $\ell_1$ (or $\ell_2$) penalty and theprojection onto the $K$-sparsity and TV or MTV constraint can be computedefficiently, allowing the appliaction of algorithms of the {\itforward-backward splitting} (a.k.a. {\it iterative shrinkage-thresholding})family. Experiments on the recovery of 2D sparse piece-wise smooth signals showthat the proposed approach is able to take advantage of the piece-wisesmoothness of the original signal, achieving more accurate recovery than2DBIHT. More specifically, 2DFBCS with the MTV and the $\ell_2$ penaltyperforms best amongst the algorithms tested.
arxiv-5400-207 | Joint Video and Text Parsing for Understanding Events and Answering Queries | http://arxiv.org/pdf/1308.6628v2.pdf | author:Kewei Tu, Meng Meng, Mun Wai Lee, Tae Eun Choe, Song-Chun Zhu category:cs.CV cs.CL cs.MM published:2013-08-29 summary:We propose a framework for parsing video and text jointly for understandingevents and answering user queries. Our framework produces a parse graph thatrepresents the compositional structures of spatial information (objects andscenes), temporal information (actions and events) and causal information(causalities between events and fluents) in the video and text. The knowledgerepresentation of our framework is based on a spatial-temporal-causal And-Orgraph (S/T/C-AOG), which jointly models possible hierarchical compositions ofobjects, scenes and events as well as their interactions and mutual contexts,and specifies the prior probabilistic distribution of the parse graphs. Wepresent a probabilistic generative model for joint parsing that captures therelations between the input video/text, their corresponding parse graphs andthe joint parse graph. Based on the probabilistic model, we propose a jointparsing system consisting of three modules: video parsing, text parsing andjoint inference. Video parsing and text parsing produce two parse graphs fromthe input video and text respectively. The joint inference module produces ajoint parse graph by performing matching, deduction and revision on the videoand text parse graphs. The proposed framework has the following objectives:Firstly, we aim at deep semantic parsing of video and text that goes beyond thetraditional bag-of-words approaches; Secondly, we perform parsing and reasoningacross the spatial, temporal and causal dimensions based on the joint S/T/C-AOGrepresentation; Thirdly, we show that deep joint parsing facilitates subsequentapplications such as generating narrative text descriptions and answeringqueries in the forms of who, what, when, where and why. We empiricallyevaluated our system based on comparison against ground-truth as well asaccuracy of query answering and obtained satisfactory results.
arxiv-5400-208 | Pareto-depth for Multiple-query Image Retrieval | http://arxiv.org/pdf/1402.5176v1.pdf | author:Ko-Jen Hsiao, Jeff Calder, Alfred O. Hero III category:cs.IR cs.LG stat.ML published:2014-02-21 summary:Most content-based image retrieval systems consider either one single query,or multiple queries that include the same object or represent the same semanticinformation. In this paper we consider the content-based image retrievalproblem for multiple query images corresponding to different image semantics.We propose a novel multiple-query information retrieval algorithm that combinesthe Pareto front method (PFM) with efficient manifold ranking (EMR). We showthat our proposed algorithm outperforms state of the art multiple-queryretrieval algorithms on real-world image databases. We attribute thisperformance improvement to concavity properties of the Pareto fronts, and provea theoretical result that characterizes the asymptotic concavity of the fronts.
arxiv-5400-209 | Distribution-Independent Reliable Learning | http://arxiv.org/pdf/1402.5164v1.pdf | author:Varun Kanade, Justin Thaler category:cs.LG cs.CC cs.DS published:2014-02-20 summary:We study several questions in the reliable agnostic learning framework ofKalai et al. (2009), which captures learning tasks in which one type of erroris costlier than others. A positive reliable classifier is one that makes nofalse positive errors. The goal in the positive reliable agnostic framework isto output a hypothesis with the following properties: (i) its false positiveerror rate is at most $\epsilon$, (ii) its false negative error rate is at most$\epsilon$ more than that of the best positive reliable classifier from theclass. A closely related notion is fully reliable agnostic learning, whichconsiders partial classifiers that are allowed to predict "unknown" on someinputs. The best fully reliable partial classifier is one that makes no errorsand minimizes the probability of predicting "unknown", and the goal in fullyreliable learning is to output a hypothesis that is almost as good as the bestfully reliable partial classifier from a class. For distribution-independent learning, the best known algorithms for PAClearning typically utilize polynomial threshold representations, while thestate of the art agnostic learning algorithms use point-wise polynomialapproximations. We show that one-sided polynomial approximations, anintermediate notion between polynomial threshold representations and point-wisepolynomial approximations, suffice for learning in the reliable agnosticsettings. We then show that majorities can be fully reliably learned anddisjunctions of majorities can be positive reliably learned, throughconstructions of appropriate one-sided polynomial approximations. Our fullyreliable algorithm for majorities provides the first evidence that fullyreliable learning may be strictly easier than agnostic learning. Our algorithmsalso satisfy strong attribute-efficiency properties, and provide smoothtradeoffs between sample complexity and running time.
arxiv-5400-210 | Detecting Opinions in Tweets | http://arxiv.org/pdf/1402.5123v1.pdf | author:Abdelmalek Amine, Reda Mohamed Hamou, Michel Simonet category:cs.CL cs.SI published:2014-02-20 summary:Given the incessant growth of documents describing the opinions of differentpeople circulating on the web, including Web 2.0 has made it possible to givean opinion on any product in the net. In this paper, we examine the variousopinions expressed in the tweets and classify them positive, negative orneutral by using the emoticons for the Bayesian method and adjectives andadverbs for the Turney's method
arxiv-5400-211 | Group-sparse Matrix Recovery | http://arxiv.org/pdf/1402.5077v1.pdf | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.LG cs.CV stat.ML published:2014-02-20 summary:We apply the OSCAR (octagonal selection and clustering algorithms forregression) in recovering group-sparse matrices (two-dimensional---2D---arrays)from compressive measurements. We propose a 2D version of OSCAR (2OSCAR)consisting of the $\ell_1$ norm and the pair-wise $\ell_{\infty}$ norm, whichis convex but non-differentiable. We show that the proximity operator of 2OSCARcan be computed based on that of OSCAR. The 2OSCAR problem can thus beefficiently solved by state-of-the-art proximal splitting algorithms.Experiments on group-sparse 2D array recovery show that 2OSCAR regularizationsolved by the SpaRSA algorithm is the fastest choice, while the PADMM algorithm(with debiasing) yields the most accurate results.
arxiv-5400-212 | Binary Fused Compressive Sensing: 1-Bit Compressive Sensing meets Group Sparsity | http://arxiv.org/pdf/1402.5074v1.pdf | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.CV cs.IT math.IT published:2014-02-20 summary:We propose a new method, {\it binary fused compressive sensing} (BFCS), torecover sparse piece-wise smooth signals from 1-bit compressive measurements.The proposed algorithm is a modification of the previous {\it binary iterativehard thresholding} (BIHT) algorithm, where, in addition to the sparsityconstraint, the total-variation of the recovered signal is upper constrained.As in BIHT, the data term of the objective function is an one-sided $\ell_1$(or $\ell_2$) norm. Experiments on the recovery of sparse piece-wise smoothsignals show that the proposed algorithm is able to take advantage of thepiece-wise smoothness of the original signal, achieving more accurate recoverythan BIHT.
arxiv-5400-213 | A novel sparsity and clustering regularization | http://arxiv.org/pdf/1310.4945v2.pdf | author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.LG cs.CV stat.ML published:2013-10-18 summary:We propose a novel SPARsity and Clustering (SPARC) regularizer, which is amodified version of the previous octagonal shrinkage and clustering algorithmfor regression (OSCAR), where, the proposed regularizer consists of a$K$-sparse constraint and a pair-wise $\ell_{\infty}$ norm restricted on the$K$ largest components in magnitude. The proposed regularizer is able toseparably enforce $K$-sparsity and encourage the non-zeros to be equal inmagnitude. Moreover, it can accurately group the features without shrinkingtheir magnitude. In fact, SPARC is closely related to OSCAR, so that theproximity operator of the former can be efficiently computed based on that ofthe latter, allowing using proximal splitting algorithms to solve problems withSPARC regularization. Experiments on synthetic data and with benchmark breastcancer data show that SPARC is a competitive group-sparsity inducingregularizer for regression and classification.
arxiv-5400-214 | Real-time Automatic Emotion Recognition from Body Gestures | http://arxiv.org/pdf/1402.5047v1.pdf | author:Stefano Piana, Alessandra Staglianò, Francesca Odone, Alessandro Verri, Antonio Camurri category:cs.HC cs.CV published:2014-02-20 summary:Although psychological research indicates that bodily expressions conveyimportant affective information, to date research in emotion recognitionfocused mainly on facial expression or voice analysis. In this paper we proposean approach to realtime automatic emotion recognition from body movements. Aset of postural, kinematic, and geometrical features are extracted fromsequences 3D skeletons and fed to a multi-class SVM classifier. The proposedmethod has been assessed on data acquired through two different systems: aprofessionalgrade optical motion capture system, and Microsoft Kinect. Thesystem has been assessed on a "six emotions" recognition problem, and using aleave-one-subject-out cross validation strategy, reached an overall recognitionrate of 61.3% which is very close to the recognition rate of 61.9% obtained byhuman observers. To provide further testing of the system, two games weredeveloped, where one or two users have to interact to understand and expressemotions with their body.
arxiv-5400-215 | An Algorithm for Training Polynomial Networks | http://arxiv.org/pdf/1304.7045v2.pdf | author:Roi Livni, Shai Shalev-Shwartz, Ohad Shamir category:cs.LG cs.AI stat.ML published:2013-04-26 summary:We consider deep neural networks, in which the output of each node is aquadratic function of its inputs. Similar to other deep architectures, thesenetworks can compactly represent any function on a finite training set. Themain goal of this paper is the derivation of an efficient layer-by-layeralgorithm for training such networks, which we denote as the \emph{BasisLearner}. The algorithm is a universal learner in the sense that the trainingerror is guaranteed to decrease at every iteration, and can eventually reachzero under mild conditions. We present practical implementations of thisalgorithm, as well as preliminary experimental results. We also compare ourdeep architecture to other shallow architectures for learning polynomials, inparticular kernel learning.
arxiv-5400-216 | Enhanced Secure Algorithm for Fingerprint Recognition | http://arxiv.org/pdf/1402.4936v1.pdf | author:Amira Mohammad Abdel-Mawgoud Saleh category:cs.CV published:2014-02-20 summary:Fingerprint recognition requires a minimal effort from the user, does notcapture other information than strictly necessary for the recognition process,and provides relatively good performance. A critical step in fingerprintidentification system is thinning of the input fingerprint image. Theperformance of a minutiae extraction algorithm relies heavily on the quality ofthe thinning algorithm. So, a fast fingerprint thinning algorithm is proposed.The algorithm works directly on the gray-scale image as binarization offingerprint causes many spurious minutiae and also removes many importantfeatures. The performance of the thinning algorithm is evaluated andexperimental results show that the proposed thinning algorithm is both fast andaccurate. A new minutiae-based fingerprint matching technique is proposed. Themain idea is that each fingerprint is represented by a minutiae table of justtwo columns in the database. The number of different minutiae types(terminations and bifurcations) found in each track of a certain width aroundthe core point of the fingerprint is recorded in this table. Each row in thetable represents a certain track, in the first column, the number ofterminations in each track is recorded, in the second column, the number ofbifurcations in each track is recorded. The algorithm is rotation andtranslation invariant, and needs less storage size. Experimental results showthat recognition accuracy is 98%, with Equal Error Rate (EER) of 2%. Finally,the integrity of the data transmission via communication channels must besecure all the way from the scanner to the application. After applying Gaussiannoise addition, and JPEG compression with high and moderate quality factors onthe watermarked fingerprint images, recognition accuracy decreases slightly toreach 96%.
arxiv-5400-217 | Survey on Sparse Coded Features for Content Based Face Image Retrieval | http://arxiv.org/pdf/1402.4888v1.pdf | author:D. Johnvictor, G. Selvavinayagam category:cs.IR cs.CV cs.LG stat.ML published:2014-02-20 summary:Content based image retrieval, a technique which uses visual contents ofimage to search images from large scale image databases according to users'interests. This paper provides a comprehensive survey on recent technology usedin the area of content based face image retrieval. Nowadays digital devices andphoto sharing sites are getting more popularity, large human face photos areavailable in database. Multiple types of facial features are used to representdiscriminality on large scale human facial image database. Searching and miningof facial images are challenging problems and important research issues. Sparserepresentation on features provides significant improvement in indexing relatedimages to query image.
arxiv-5400-218 | Conclusions from a NAIVE Bayes Operator Predicting the Medicare 2011 Transaction Data Set | http://arxiv.org/pdf/1403.7087v1.pdf | author:Nick Williams category:cs.LG cs.CY 62, 91 published:2014-02-20 summary:Introduction: The United States Federal Government operates one of the worldslargest medical insurance programs, Medicare, to ensure payment for clinicalservices for the elderly, illegal aliens and those without the ability to payfor their care directly. This paper evaluates the Medicare 2011 TransactionData Set which details the transfer of funds from Medicare to private andpublic clinical care facilities for specific clinical services for theoperational year 2011. Methods: Data mining was conducted to establish therelationships between reported and computed transaction values in the data setto better understand the drivers of Medicare transactions at a programmaticlevel. Results: The models averaged 88 for average model accuracy and 38 foraverage Kappa during training. Some reported classes are highly independentfrom the available data as their predictability remains stable regardless ofredaction of supporting and contradictory evidence. DRG or procedure typeappears to be unpredictable from the available financial transaction values.Conclusions: Overlay hypotheses such as charges being driven by the volumeserved or DRG being related to charges or payments is readily false in thisanalysis despite 28 million Americans being billed through Medicare in 2011 andthe program distributing over 70 billion in this transaction set alone. It maybe impossible to predict the dependencies and data structures the payer of lastresort without data from payers of first and second resort. Political concernsabout Medicare would be better served focusing on these first and second orderpayer systems as what Medicare costs is not dependent on Medicare itself.
arxiv-5400-219 | Convergence Analysis and Parallel Computing Implementation for the Multiagent Coordination Optimization Algorithm | http://arxiv.org/pdf/1306.0225v10.pdf | author:Qing Hui, Haopeng Zhang category:math.OC cs.NE math.DS published:2013-06-02 summary:In this report, a novel variation of Particle Swarm Optimization (PSO)algorithm, called Multiagent Coordination Optimization (MCO), is implemented ina parallel computing way for practical use by introducing MATLAB built-infunction "parfor" into MCO. Then we rigorously analyze the global convergenceof MCO by means of semistability theory. Besides sharing global optimalsolutions with the PSO algorithm, the MCO algorithm integrates cooperativeswarm behavior of multiple agents into the update formula by sharing velocityand position information between neighbors to improve its performance.Numerical evaluation of the parallel MCO algorithm is provided in the report byrunning the proposed algorithm on supercomputers in the High PerformanceComputing Center at Texas Tech University. In particular, the optimal value andconsuming time are compared with PSO and serial MCO by solving severalbenchmark functions in the literature, respectively. Based on the simulationresults, the performance of the parallel MCO is not only superb compared withPSO for solving many nonlinear, noncovex optimization problems, but also is ofhigh efficiency by saving the computational time.
arxiv-5400-220 | Learning the Parameters of Determinantal Point Process Kernels | http://arxiv.org/pdf/1402.4862v1.pdf | author:Raja Hafiz Affandi, Emily B. Fox, Ryan P. Adams, Ben Taskar category:stat.ML cs.LG published:2014-02-20 summary:Determinantal point processes (DPPs) are well-suited for modeling repulsionand have proven useful in many applications where diversity is desired. WhileDPPs have many appealing properties, such as efficient sampling, learning theparameters of a DPP is still considered a difficult problem due to thenon-convex nature of the likelihood function. In this paper, we propose usingBayesian methods to learn the DPP kernel parameters. These methods areapplicable in large-scale and continuous DPP settings even when the exact formof the eigendecomposition is unknown. We demonstrate the utility of our DPPlearning methods in studying the progression of diabetic neuropathy based onspatial distribution of nerve fibers, and in studying human perception ofdiversity in images.
arxiv-5400-221 | A Quasi-Newton Method for Large Scale Support Vector Machines | http://arxiv.org/pdf/1402.4861v1.pdf | author:Aryan Mokhtari, Alejandro Ribeiro category:cs.LG published:2014-02-20 summary:This paper adapts a recently developed regularized stochastic version of theBroyden, Fletcher, Goldfarb, and Shanno (BFGS) quasi-Newton method for thesolution of support vector machine classification problems. The proposed methodis shown to converge almost surely to the optimal classifier at a rate that islinear in expectation. Numerical results show that the proposed method exhibitsa convergence rate that degrades smoothly with the dimensionality of thefeature vectors.
arxiv-5400-222 | Asymptotic Accuracy of Distribution-Based Estimation for Latent Variables | http://arxiv.org/pdf/1204.2069v4.pdf | author:Keisuke Yamazaki category:stat.ML cs.LG published:2012-04-10 summary:Hierarchical statistical models are widely employed in information scienceand data engineering. The models consist of two types of variables: observablevariables that represent the given data and latent variables for theunobservable labels. An asymptotic analysis of the models plays an importantrole in evaluating the learning process; the result of the analysis is appliednot only to theoretical but also to practical situations, such as optimal modelselection and active learning. There are many studies of generalization errors,which measure the prediction accuracy of the observable variables. However, theaccuracy of estimating the latent variables has not yet been elucidated. For aquantitative evaluation of this, the present paper formulatesdistribution-based functions for the errors in the estimation of the latentvariables. The asymptotic behavior is analyzed for both the maximum likelihoodand the Bayes methods.
arxiv-5400-223 | Diffusion Least Mean Square: Simulations | http://arxiv.org/pdf/1402.4845v1.pdf | author:Jonathan Gelati, Sithan Kanna category:cs.LG cs.MA published:2014-02-19 summary:In this technical report we analyse the performance of diffusion strategiesapplied to the Least-Mean-Square adaptive filter. We configure a network ofcooperative agents running adaptive filters and discuss their behaviour whencompared with a non-cooperative agent which represents the average of thenetwork. The analysis provides conditions under which diversity in the filterparameters is beneficial in terms of convergence and stability. Simulationsdrive and support the analysis.
arxiv-5400-224 | The Sample Complexity of Subspace Learning with Partial Information | http://arxiv.org/pdf/1402.4844v1.pdf | author:Alon Gonen, Dan Rosenbaum, Yonina Eldar, Shai Shalev-Shwartz category:cs.LG stat.ML published:2014-02-19 summary:The goal of subspace learning is to find a $k$-dimensional subspace of$\mathbb{R}^d$, such that the expected squared distance between instancevectors and the subspace is as small as possible. In this paper we study thesample complexity of subspace learning in a \emph{partial information} setting,in which the learner can only observe $r \le d$ attributes from each instancevector. We derive upper and lower bounds on the sample complexity in differentscenarios. In particular, our upper bounds involve a generalization of vectorsampling techniques, which are often used in bandit problems, to matrices.
arxiv-5400-225 | Nonparametric Weight Initialization of Neural Networks via Integral Representation | http://arxiv.org/pdf/1312.6461v3.pdf | author:Sho Sonoda, Noboru Murata category:cs.LG cs.NE published:2013-12-23 summary:A new initialization method for hidden parameters in a neural network isproposed. Derived from the integral representation of the neural network, anonparametric probability distribution of hidden parameters is introduced. Inthis proposal, hidden parameters are initialized by samples drawn from thisdistribution, and output parameters are fitted by ordinary linear regression.Numerical experiments show that backpropagation with proposed initializationconverges faster than uniformly random initialization. Also it is shown thatthe proposed method achieves enough accuracy by itself without backpropagationin some cases.
arxiv-5400-226 | Near-optimal-sample estimators for spherical Gaussian mixtures | http://arxiv.org/pdf/1402.4746v1.pdf | author:Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, Ananda Theertha Suresh category:cs.LG cs.DS cs.IT math.IT stat.ML published:2014-02-19 summary:Statistical and machine-learning algorithms are frequently applied tohigh-dimensional data. In many of these applications data is scarce, and oftenmuch more costly than computation time. We provide the first sample-efficientpolynomial-time estimator for high-dimensional spherical Gaussian mixtures. For mixtures of any $k$ $d$-dimensional spherical Gaussians, we derive anintuitive spectral-estimator that uses$\mathcal{O}_k\bigl(\frac{d\log^2d}{\epsilon^4}\bigr)$ samples and runs in time$\mathcal{O}_{k,\epsilon}(d^3\log^5 d)$, both significantly lower thanpreviously known. The constant factor $\mathcal{O}_k$ is polynomial for samplecomplexity and is exponential for the time complexity, again much smaller thanwhat was previously known. We also show that$\Omega_k\bigl(\frac{d}{\epsilon^2}\bigr)$ samples are needed for anyalgorithm. Hence the sample complexity is near-optimal in the number ofdimensions. We also derive a simple estimator for one-dimensional mixtures that uses$\mathcal{O}\bigl(\frac{k \log \frac{k}{\epsilon} }{\epsilon^2} \bigr)$ samplesand runs in time$\widetilde{\mathcal{O}}\left(\bigl(\frac{k}{\epsilon}\bigr)^{3k+1}\right)$.Our other technical contributions include a faster algorithm for choosing adensity estimate from a set of distributions, that minimizes the $\ell_1$distance to an unknown underlying distribution.
arxiv-5400-227 | Understanding Deep Architectures using a Recursive Convolutional Network | http://arxiv.org/pdf/1312.1847v2.pdf | author:David Eigen, Jason Rolfe, Rob Fergus, Yann LeCun category:cs.LG published:2013-12-06 summary:A key challenge in designing convolutional network models is sizing themappropriately. Many factors are involved in these decisions, including numberof layers, feature maps, kernel sizes, etc. Complicating this further is thefact that each of these influence not only the numbers and dimensions of theactivation units, but also the total number of parameters. In this paper wefocus on assessing the independent contributions of three of these linkedvariables: The numbers of layers, feature maps, and parameters. To accomplishthis, we employ a recursive convolutional network whose weights are tiedbetween layers; this allows us to vary each of the three factors in acontrolled setting. We find that while increasing the numbers of layers andparameters each have clear benefit, the number of feature maps (and hencedimensionality of the representation) appears ancillary, and finds most of itsbenefit through the introduction of more weights. Our results (i) empiricallyconfirm the notion that adding layers alone increases computational power,within the context of convolutional layers, and (ii) suggest that precisesizing of convolutional feature map dimensions is itself of little concern;more attention should be paid to the number of parameters in these layersinstead.
arxiv-5400-228 | Exact solutions to the nonlinear dynamics of learning in deep linear neural networks | http://arxiv.org/pdf/1312.6120v3.pdf | author:Andrew M. Saxe, James L. McClelland, Surya Ganguli category:cs.NE cs.CV cs.LG q-bio.NC stat.ML published:2013-12-20 summary:Despite the widespread practical success of deep learning methods, ourtheoretical understanding of the dynamics of learning in deep neural networksremains quite sparse. We attempt to bridge the gap between the theory andpractice of deep learning by systematically analyzing learning dynamics for therestricted case of deep linear neural networks. Despite the linearity of theirinput-output map, such networks have nonlinear gradient descent dynamics onweights that change with the addition of each new hidden layer. We show thatdeep linear networks exhibit nonlinear learning phenomena similar to those seenin simulations of nonlinear networks, including long plateaus followed by rapidtransitions to lower error solutions, and faster convergence from greedyunsupervised pretraining initial conditions than from random initialconditions. We provide an analytical description of these phenomena by findingnew exact solutions to the nonlinear dynamics of deep learning. Our theoreticalanalysis also reveals the surprising finding that as the depth of a networkapproaches infinity, learning speed can nevertheless remain finite: for aspecial class of initial conditions on the weights, very deep networks incuronly a finite, depth independent, delay in learning speed relative to shallownetworks. We show that, under certain conditions on the training data,unsupervised pretraining can find this special class of initial conditions,while scaled random Gaussian initializations cannot. We further exhibit a newclass of random orthogonal initial conditions on weights that, likeunsupervised pre-training, enjoys depth independent learning times. We furthershow that these initial conditions also lead to faithful propagation ofgradients even in deep nonlinear networks, as long as they operate in a specialregime known as the edge of chaos.
arxiv-5400-229 | Efficient Inference of Gaussian Process Modulated Renewal Processes with Application to Medical Event Data | http://arxiv.org/pdf/1402.4732v1.pdf | author:Thomas A. Lasko category:stat.ML cs.LG stat.AP published:2014-02-19 summary:The episodic, irregular and asynchronous nature of medical data render themdifficult substrates for standard machine learning algorithms. We would like toabstract away this difficulty for the class of time-stamped categoricalvariables (or events) by modeling them as a renewal process and inferring aprobability density over continuous, longitudinal, nonparametric intensityfunctions modulating that process. Several methods exist for inferring such adensity over intensity functions, but either their constraints and assumptionsprevent their use with our potentially bursty event streams, or their timecomplexity renders their use intractable on our long-duration observations ofhigh-resolution events, or both. In this paper we present a new and efficientmethod for inferring a distribution over intensity functions that uses directnumeric integration and smooth interpolation over Gaussian processes. Wedemonstrate that our direct method is up to twice as accurate and two orders ofmagnitude more efficient than the best existing method (thinning). Importantly,the direct method can infer intensity functions over the full range of burstyto memoryless to regular events, which thinning and many other methods cannot.Finally, we apply the method to clinical event data and demonstrate theface-validity of the abstraction, which is now amenable to standard learningalgorithms.
arxiv-5400-230 | Intriguing properties of neural networks | http://arxiv.org/pdf/1312.6199v4.pdf | author:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus category:cs.CV cs.LG cs.NE published:2013-12-21 summary:Deep neural networks are highly expressive models that have recently achievedstate of the art performance on speech and visual recognition tasks. Whiletheir expressiveness is the reason they succeed, it also causes them to learnuninterpretable solutions that could have counter-intuitive properties. In thispaper we report two such properties. First, we find that there is no distinction between individual high levelunits and random linear combinations of high level units, according to variousmethods of unit analysis. It suggests that it is the space, rather than theindividual units, that contains of the semantic information in the high layersof neural networks. Second, we find that deep neural networks learn input-output mappings thatare fairly discontinuous to a significant extend. We can cause the network tomisclassify an image by applying a certain imperceptible perturbation, which isfound by maximizing the network's prediction error. In addition, the specificnature of these perturbations is not a random artifact of learning: the sameperturbation can cause a different network, that was trained on a differentsubset of the dataset, to misclassify the same input.
arxiv-5400-231 | Deep learning for neuroimaging: a validation study | http://arxiv.org/pdf/1312.5847v3.pdf | author:Sergey M. Plis, Devon R. Hjelm, Ruslan Salakhutdinov, Vince D. Calhoun category:cs.NE cs.LG stat.ML published:2013-12-20 summary:Deep learning methods have recently made notable advances in the tasks ofclassification and representation learning. These tasks are important for brainimaging and neuroscience discovery, making the methods attractive for portingto a neuroimager's toolbox. Success of these methods is, in part, explained bythe flexibility of deep learning models. However, this flexibility makes theprocess of porting to new areas a difficult parameter optimization problem. Inthis work we demonstrate our results (and feasible parameter ranges) inapplication of deep learning methods to structural and functional brain imagingdata. We also describe a novel constraint-based approach to visualizing highdimensional data. We use it to analyze the effect of parameter choices on datatransformations. Our results show that deep learning methods are able to learnphysiologically important representations and detect latent relations inneuroimaging data.
arxiv-5400-232 | A Powerful Genetic Algorithm for Traveling Salesman Problem | http://arxiv.org/pdf/1402.4699v1.pdf | author:Shujia Liu category:cs.NE cs.AI published:2014-02-19 summary:This paper presents a powerful genetic algorithm(GA) to solve the travelingsalesman problem (TSP). To construct a powerful GA, I use edge swapping(ES)with a local search procedure to determine good combinations of building blocksof parent solutions for generating even better offspring solutions.Experimental results on well studied TSP benchmarks demonstrate that theproposed GA is competitive in finding very high quality solutions on instanceswith up to 16,862 cities.
arxiv-5400-233 | Retrieval of Experiments by Efficient Estimation of Marginal Likelihood | http://arxiv.org/pdf/1402.4653v1.pdf | author:Sohan Seth, John Shawe-Taylor, Samuel Kaski category:stat.ML cs.IR cs.LG published:2014-02-19 summary:We study the task of retrieving relevant experiments given a queryexperiment. By experiment, we mean a collection of measurements from a set of`covariates' and the associated `outcomes'. While similar experiments can beretrieved by comparing available `annotations', this approach ignores thevaluable information available in the measurements themselves. To incorporatethis information in the retrieval task, we suggest employing a retrieval metricthat utilizes probabilistic models learned from the measurements. We argue thatsuch a metric is a sensible measure of similarity between two experiments sinceit permits inclusion of experiment-specific prior knowledge. However, accuratemodels are often not analytical, and one must resort to storing posteriorsamples which demands considerable resources. Therefore, we study strategies toselect informative posterior samples to reduce the computational load whilemaintaining the retrieval performance. We demonstrate the efficacy of ourapproach on simulated data with simple linear regression as the models, andreal world datasets.
arxiv-5400-234 | A Survey on Semi-Supervised Learning Techniques | http://arxiv.org/pdf/1402.4645v1.pdf | author:V. Jothi Prakash, Dr. L. M. Nithya category:cs.LG published:2014-02-19 summary:Semisupervised learning is a learning standard which deals with the study ofhow computers and natural systems such as human beings acquire knowledge in thepresence of both labeled and unlabeled data. Semisupervised learning basedmethods are preferred when compared to the supervised and unsupervised learningbecause of the improved performance shown by the semisupervised approaches inthe presence of large volumes of data. Labels are very hard to attain whileunlabeled data are surplus, therefore semisupervised learning is a nobleindication to shrink human labor and improve accuracy. There has been a largespectrum of ideas on semisupervised learning. In this paper we bring out someof the key approaches for semisupervised learning.
arxiv-5400-235 | Sparse Quantile Huber Regression for Efficient and Robust Estimation | http://arxiv.org/pdf/1402.4624v1.pdf | author:Aleksandr Y. Aravkin, Anju Kambadur, Aurelie C. Lozano, Ronny Luss category:stat.ML cs.DS math.OC stat.ME 62F35, 65K10 published:2014-02-19 summary:We consider new formulations and methods for sparse quantile regression inthe high-dimensional setting. Quantile regression plays an important role inmany applications, including outlier-robust exploratory analysis in geneselection. In addition, the sparsity consideration in quantile regressionenables the exploration of the entire conditional distribution of the responsevariable given the predictors and therefore yields a more comprehensive view ofthe important predictors. We propose a generalized OMP algorithm for variableselection, taking the misfit loss to be either the traditional quantile loss ora smooth version we call quantile Huber, and compare the resulting greedyapproaches with convex sparsity-regularized formulations. We apply a recentlyproposed interior point methodology to efficiently solve all convexformulations as well as convex subproblems in the generalized OMP setting, pro-vide theoretical guarantees of consistent estimation, and demonstrate theperformance of our approach using empirical studies of simulated and genomicdatasets.
arxiv-5400-236 | Image Representation Learning Using Graph Regularized Auto-Encoders | http://arxiv.org/pdf/1312.0786v2.pdf | author:Yiyi Liao, Yue Wang, Yong Liu category:cs.LG K.3.2 published:2013-12-03 summary:We consider the problem of image representation for the tasks of unsupervisedlearning and semi-supervised learning. In those learning tasks, the raw imagevectors may not provide enough representation for their intrinsic structuresdue to their highly dense feature space. To overcome this problem, the rawimage vectors should be mapped to a proper representation space which cancapture the latent structure of the original data and represent the dataexplicitly for further learning tasks such as clustering. Inspired by the recent research works on deep neural network andrepresentation learning, in this paper, we introduce the multiple-layerauto-encoder into image representation, we also apply the locally invariantideal to our image representation with auto-encoders and propose a novelmethod, called Graph regularized Auto-Encoder (GAE). GAE can provide a compactrepresentation which uncovers the hidden semantics and simultaneously respectsthe intrinsic geometric structure. Extensive experiments on image clustering show encouraging results of theproposed algorithm in comparison to the state-of-the-art algorithms onreal-word cases.
arxiv-5400-237 | Improving Deep Neural Networks with Probabilistic Maxout Units | http://arxiv.org/pdf/1312.6116v2.pdf | author:Jost Tobias Springenberg, Martin Riedmiller category:stat.ML cs.LG cs.NE published:2013-12-20 summary:We present a probabilistic variant of the recently introduced maxout unit.The success of deep neural networks utilizing maxout can partly be attributedto favorable performance under dropout, when compared to rectified linearunits. It however also depends on the fact that each maxout unit performs apooling operation over a group of linear transformations and is thus partiallyinvariant to changes in its input. Starting from this observation we ask thequestion: Can the desirable properties of maxout units be preserved whileimproving their invariance properties ? We argue that our probabilistic maxout(probout) units successfully achieve this balance. We quantitatively verifythis claim and report classification performance matching or exceeding thecurrent state of the art on three challenging image classification benchmarks(CIFAR-10, CIFAR-100 and SVHN).
arxiv-5400-238 | Student-t Processes as Alternatives to Gaussian Processes | http://arxiv.org/pdf/1402.4306v2.pdf | author:Amar Shah, Andrew Gordon Wilson, Zoubin Ghahramani category:stat.ML cs.AI cs.LG stat.ME published:2014-02-18 summary:We investigate the Student-t process as an alternative to the Gaussianprocess as a nonparametric prior over functions. We derive closed formexpressions for the marginal likelihood and predictive distribution of aStudent-t process, by integrating away an inverse Wishart process prior overthe covariance kernel of a Gaussian process model. We show surprisingequivalences between different hierarchical Gaussian process models leading toStudent-t processes, and derive a new sampling scheme for the inverse Wishartprocess, which helps elucidate these equivalences. Overall, we show that aStudent-t process can retain the attractive properties of a Gaussian process --a nonparametric representation, analytic marginal and predictive distributions,and easy model selection through covariance kernels -- but has enhancedflexibility, and predictive covariances that, unlike a Gaussian process,explicitly depend on the values of training observations. We verify empiricallythat a Student-t process is especially useful in situations where there arechanges in covariance structure, or in applications like Bayesian optimization,where accurate predictive covariances are critical for good performance. Theseadvantages come at no additional computational cost over Gaussian processes.
arxiv-5400-239 | Analysis of Multibeam SONAR Data using Dissimilarity Representations | http://arxiv.org/pdf/1402.6636v1.pdf | author:Iain Rice, Roger Benton, Les Hart, David Lowe category:cs.CE stat.ML published:2014-02-19 summary:This paper considers the problem of low-dimensional visualisation of veryhigh dimensional information sources for the purpose of situation awareness inthe maritime environment. In response to the requirement for human decisionsupport aids to reduce information overload (and specifically, data amenable tointer-point relative similarity measures) appropriate to the below-watermaritime domain, we are investigating a preliminary prototype topographicvisualisation model. The focus of the current paper is on the mathematicalproblem of exploiting a relative dissimilarity representation of signals in avisual informatics mapping model, driven by real-world sonar systems. Anindependent source model is used to analyse the sonar beams from which a simpleprobabilistic input model to represent uncertainty is mapped to a latentvisualisation space where data uncertainty can be accommodated. The use ofeuclidean and non-euclidean measures are used and the motivation for future useof non-euclidean measures is made. Concepts are illustrated using a simulated64 beam weak SNR dataset with realistic sonar targets.
arxiv-5400-240 | Unsupervised Ranking of Multi-Attribute Objects Based on Principal Curves | http://arxiv.org/pdf/1402.4542v1.pdf | author:Chun-Guo Li, Xing Mei, Bao-Gang Hu category:cs.LG cs.AI stat.ML published:2014-02-19 summary:Unsupervised ranking faces one critical challenge in evaluation applications,that is, no ground truth is available. When PageRank and its variants show agood solution in related subjects, they are applicable only for ranking fromlink-structure data. In this work, we focus on unsupervised ranking frommulti-attribute data which is also common in evaluation tasks. To overcome thechallenge, we propose five essential meta-rules for the design and assessmentof unsupervised ranking approaches: scale and translation invariance, strictmonotonicity, linear/nonlinear capacities, smoothness, and explicitness ofparameter size. These meta-rules are regarded as high level knowledge forunsupervised ranking tasks. Inspired by the works in [8] and [14], we propose aranking principal curve (RPC) model, which learns a one-dimensional manifoldfunction to perform unsupervised ranking tasks on multi-attribute observations.Furthermore, the RPC is modeled to be a cubic B\'ezier curve with controlpoints restricted in the interior of a hypercube, thereby complying with allthe five meta-rules to infer a reasonable ranking list. With control points asthe model parameters, one is able to understand the learned manifold and tointerpret the ranking list semantically. Numerical experiments of the presentedRPC model are conducted on two open datasets of different ranking applications.In comparison with the state-of-the-art approaches, the new model is able toshow more reasonable ranking lists.
arxiv-5400-241 | A Statistical Approach to Set Classification by Feature Selection with Applications to Classification of Histopathology Images | http://arxiv.org/pdf/1402.4539v1.pdf | author:Sungkyu Jung, Xingye Qiao category:stat.ME stat.ML published:2014-02-19 summary:Set classification problems arise when classification tasks are based on setsof observations as opposed to individual observations. In set classification, aclassification rule is trained with $N$ sets of observations, where each set islabeled with class information, and the prediction of a class label isperformed also with a set of observations. Data sets for set classificationappear, for example, in diagnostics of disease based on multiple cell nucleusimages from a single tissue. Relevant statistical models for set classificationare introduced, which motivate a set classification framework based oncontext-free feature extraction. By understanding a set of observations as anempirical distribution, we employ a data-driven method to choose those featureswhich contain information on location and major variation. In particular, themethod of principal component analysis is used to extract the features of majorvariation. Multidimensional scaling is used to represent features asvector-valued points on which conventional classifiers can be applied. Theproposed set classification approaches achieve better classification resultsthan competing methods in a number of simulated data examples. The benefits ofour method are demonstrated in an analysis of histopathology images of cellnuclei related to liver cancer.
arxiv-5400-242 | High Dimensional Semiparametric Scale-Invariant Principal Component Analysis | http://arxiv.org/pdf/1402.4507v1.pdf | author:Fang Han, Han Liu category:stat.ML published:2014-02-18 summary:We propose a new high dimensional semiparametric principal component analysis(PCA) method, named Copula Component Analysis (COCA). The semiparametric modelassumes that, after unspecified marginally monotone transformations, thedistributions are multivariate Gaussian. COCA improves upon PCA and sparse PCAin three aspects: (i) It is robust to modeling assumptions; (ii) It is robustto outliers and data contamination; (iii) It is scale-invariant and yields moreinterpretable results. We prove that the COCA estimators obtain fast estimationrates and are feature selection consistent when the dimension is nearlyexponentially large relative to the sample size. Careful experiments confirmthat COCA outperforms sparse PCA on both synthetic and real-world datasets.
arxiv-5400-243 | Multi-GPU Training of ConvNets | http://arxiv.org/pdf/1312.5853v4.pdf | author:Omry Yadan, Keith Adams, Yaniv Taigman, Marc'Aurelio Ranzato category:cs.LG cs.NE published:2013-12-20 summary:In this work we evaluate different approaches to parallelize computation ofconvolutional neural networks across several GPUs.
arxiv-5400-244 | Artificial Mutation inspired Hyper-heuristic for Runtime Usage of Multi-objective Algorithms | http://arxiv.org/pdf/1402.4442v1.pdf | author:Donia El Kateb, François Fouquet, Johann Bourcier, Yves Le Traon category:cs.SE cs.NE published:2014-02-18 summary:In the last years, multi-objective evolutionary algorithms (MOEA) have beenapplied to different software engineering problems where many conflictingobjectives have to be optimized simultaneously. In theory, evolutionaryalgorithms feature a nice property for runtime optimization as they can providea solution in any execution time. In practice, based on a Darwinian inspirednatural selection, these evolutionary algorithms produce many deadbornsolutions whose computation results in a computational resources wastage:natural selection is naturally slow. In this paper, we reconsider this foundinganalogy to accelerate convergence of MOEA, by looking at modern biologystudies: artificial selection has been used to achieve an anticipated specificpurpose instead of only relying on crossover and natural selection (i.e.,Muller et al [18] research on artificial mutation of fruits with X-Ray).Putting aside the analogy with natural selection , the present paper proposesan hyper-heuristic for MOEA algorithms named Sputnik 1 that uses artificialselective mutation to improve the convergence speed of MOEA. Sputnik leveragesthe past history of mutation efficiency to select the most relevant mutationsto perform. We evaluate Sputnik on a cloud-reasoning engine, which driveson-demand provisioning while considering conflicting performance and costobjectives. We have conducted experiments to highlight the significantperformance improvement of Sputnik in terms of resolution time.
arxiv-5400-245 | Principled Non-Linear Feature Selection | http://arxiv.org/pdf/1312.5869v2.pdf | author:Dimitrios Athanasakis, John Shawe-Taylor, Delmiro Fernandez-Reyes category:cs.LG published:2013-12-20 summary:Recent non-linear feature selection approaches employing greedy optimisationof Centred Kernel Target Alignment(KTA) exhibit strong results in terms ofgeneralisation accuracy and sparsity. However, they are computationallyprohibitive for large datasets. We propose randSel, a randomised featureselection algorithm, with attractive scaling properties. Our theoreticalanalysis of randSel provides strong probabilistic guarantees for correctidentification of relevant features. RandSel's characteristics make it an idealcandidate for identifying informative learned representations. We've conductedexperimentation to establish the performance of this approach, and presentencouraging results, including a 3rd position result in the recent ICML blackbox learning challenge as well as competitive results for signal peptideprediction, an important problem in bioinformatics.
arxiv-5400-246 | Direct Processing of Run Length Compressed Document Image for Segmentation and Characterization of a Specified Block | http://arxiv.org/pdf/1402.1971v2.pdf | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-02-09 summary:Extracting a block of interest referred to as segmenting a specified block inan image and studying its characteristics is of general research interest, andcould be a challenging if such a segmentation task has to be carried outdirectly in a compressed image. This is the objective of the present researchwork. The proposal is to evolve a method which would segment and extract aspecified block, and carry out its characterization without decompressing acompressed image, for two major reasons that most of the image archives containimages in compressed format and decompressing an image indents additionalcomputing time and space. Specifically in this research work, the proposal isto work on run-length compressed document images.
arxiv-5400-247 | Automatic Detection of Font Size Straight from Run Length Compressed Text Documents | http://arxiv.org/pdf/1402.4388v1.pdf | author:Mohammed Javed, P. Nagabhushan, B. B. Chaudhuri category:cs.CV published:2014-02-18 summary:Automatic detection of font size finds many applications in the area ofintelligent OCRing and document image analysis, which has been traditionallypracticed over uncompressed documents, although in real life the documentsexist in compressed form for efficient storage and transmission. It would benovel and intelligent if the task of font size detection could be carried outdirectly from the compressed data of these documents without decompressing,which would result in saving of considerable amount of processing time andspace. Therefore, in this paper we present a novel idea of learning anddetecting font size directly from run-length compressed text documents at linelevel using simple line height features, which paves the way for intelligentOCRing and document analysis directly from compressed documents. In theproposed model, the given mixed-case text documents of different font size aresegmented into compressed text lines and the features extracted such as lineheight and ascender height are used to capture the pattern of font size in theform of a regression line, using which the automatic detection of font size isdone during the recognition stage. The method is experimented with a dataset of50 compressed documents consisting of 780 text lines of single font size and375 text lines of mixed font size resulting in an overall accuracy of 99.67%.
arxiv-5400-248 | Fast X-ray CT image reconstruction using the linearized augmented Lagrangian method with ordered subsets | http://arxiv.org/pdf/1402.4381v1.pdf | author:Hung Nien, Jeffrey A. Fessler category:math.OC cs.LG stat.ML published:2014-02-18 summary:The augmented Lagrangian (AL) method that solves convex optimization problemswith linear constraints has drawn more attention recently in imagingapplications due to its decomposable structure for composite cost functions andempirical fast convergence rate under weak conditions. However, for problemssuch as X-ray computed tomography (CT) image reconstruction and large-scalesparse regression with "big data", where there is no efficient way to solve theinner least-squares problem, the AL method can be slow due to the inevitableiterative inner updates. In this paper, we focus on solving regularized(weighted) least-squares problems using a linearized variant of the AL methodthat replaces the quadratic AL penalty term in the scaled augmented Lagrangianwith its separable quadratic surrogate (SQS) function, thus leading to a muchsimpler ordered-subsets (OS) accelerable splitting-based algorithm, OS-LALM,for X-ray CT image reconstruction. To further accelerate the proposedalgorithm, we use a second-order recursive system analysis to design adeterministic downward continuation approach that avoids tedious parametertuning and provides fast convergence. Experimental results show that theproposed algorithm significantly accelerates the "convergence" of X-ray CTimage reconstruction with negligible overhead and greatly reduces the OSartifacts in the reconstructed image when using many subsets for OSacceleration.
arxiv-5400-249 | A Comparative Study of Machine Learning Methods for Verbal Autopsy Text Classification | http://arxiv.org/pdf/1402.4380v1.pdf | author:Samuel Danso, Eric Atwell, Owen Johnson category:cs.CL published:2014-02-18 summary:A Verbal Autopsy is the record of an interview about the circumstances of anuncertified death. In developing countries, if a death occurs away from healthfacilities, a field-worker interviews a relative of the deceased about thecircumstances of the death; this Verbal Autopsy can be reviewed off-site. Wereport on a comparative study of the processes involved in Text Classificationapplied to classifying Cause of Death: feature value representation; machinelearning classification algorithms; and feature reduction strategies in orderto identify the suitable approaches applicable to the classification of VerbalAutopsy text. We demonstrate that normalised term frequency and the standardTFiDF achieve comparable performance across a number of classifiers. Theresults also show Support Vector Machine is superior to other classificationalgorithms employed in this research. Finally, we demonstrate the effectivenessof employing a "locally-semi-supervised" feature reduction strategy in order toincrease performance accuracy.
arxiv-5400-250 | Learning Type-Driven Tensor-Based Meaning Representations | http://arxiv.org/pdf/1312.5985v2.pdf | author:Tamara Polajnar, Luana Fagarasan, Stephen Clark category:cs.CL cs.LG H.3.1 published:2013-12-20 summary:This paper investigates the learning of 3rd-order tensors representing thesemantics of transitive verbs. The meaning representations are part of atype-driven tensor-based semantic framework, from the newly emerging field ofcompositional distributional semantics. Standard techniques from the neuralnetworks literature are used to learn the tensors, which are tested on aselectional preference-style task with a simple 2-dimensional sentence space.Promising results are obtained against a competitive corpus-based baseline. Weargue that extending this work beyond transitive verbs, and tohigher-dimensional sentence spaces, is an interesting and challenging problemfor the machine learning community to consider.
arxiv-5400-251 | A convergence proof of the split Bregman method for regularized least-squares problems | http://arxiv.org/pdf/1402.4371v1.pdf | author:Hung Nien, Jeffrey A. Fessler category:math.OC cs.LG stat.ML published:2014-02-18 summary:The split Bregman (SB) method [T. Goldstein and S. Osher, SIAM J. ImagingSci., 2 (2009), pp. 323-43] is a fast splitting-based algorithm that solvesimage reconstruction problems with general l1, e.g., total-variation (TV) andcompressed sensing (CS), regularizations by introducing a single variable splitto decouple the data-fitting term and the regularization term, yielding simplesubproblems that are separable (or partially separable) and easy to minimize.Several convergence proofs have been proposed, and these proofs either impose a"full column rank" assumption to the split or assume exact updates in allsubproblems. However, these assumptions are impractical in many applicationssuch as the X-ray computed tomography (CT) image reconstructions, where theinner least-squares problem usually cannot be solved efficiently due to thehighly shift-variant Hessian. In this paper, we show that when the data-fittingterm is quadratic, the SB method is a convergent alternating direction methodof multipliers (ADMM), and a straightforward convergence proof with inexactupdates is given using [J. Eckstein and D. P. Bertsekas, MathematicalProgramming, 55 (1992), pp. 293-318, Theorem 8]. Furthermore, since the SBmethod is just a special case of an ADMM algorithm, it seems likely that theADMM algorithm will be faster than the SB method if the augmented Largangian(AL) penalty parameters are selected appropriately. To have a concrete example,we conduct a convergence rate analysis of the ADMM algorithm using two splitsfor image restoration problems with quadratic data-fitting term andregularization term. According to our analysis, we can show that the two-splitADMM algorithm can be faster than the SB method if the AL penalty parameter ofthe SB method is suboptimal. Numerical experiments were conducted to verify ouranalysis.
arxiv-5400-252 | Hybrid SRL with Optimization Modulo Theories | http://arxiv.org/pdf/1402.4354v1.pdf | author:Stefano Teso, Roberto Sebastiani, Andrea Passerini category:cs.LG stat.ML published:2014-02-18 summary:Generally speaking, the goal of constructive learning could be seen as, givenan example set of structured objects, to generate novel objects with similarproperties. From a statistical-relational learning (SRL) viewpoint, the taskcan be interpreted as a constraint satisfaction problem, i.e. the generatedobjects must obey a set of soft constraints, whose weights are estimated fromthe data. Traditional SRL approaches rely on (finite) First-Order Logic (FOL)as a description language, and on MAX-SAT solvers to perform inference. Alas,FOL is unsuited for con- structive problems where the objects contain a mixtureof Boolean and numerical variables. It is in fact difficult to implement, e.g.linear arithmetic constraints within the language of FOL. In this paper wepropose a novel class of hybrid SRL methods that rely on Satisfiability ModuloTheories, an alternative class of for- mal languages that allow to describe,and reason over, mixed Boolean-numerical objects and constraints. The resultingmethods, which we call Learning Mod- ulo Theories, are formulated within thestructured output SVM framework, and employ a weighted SMT solver as anoptimization oracle to perform efficient in- ference and discriminative maxmargin weight learning. We also present a few examples of constructive learningapplications enabled by our method.
arxiv-5400-253 | Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds | http://arxiv.org/pdf/1312.5559v3.pdf | author:Irina Sergienya, Hinrich Schütze category:cs.CL I.2.6; I.2.7 published:2013-12-19 summary:There are two main approaches to the distributed representation of words:low-dimensional deep learning embeddings and high-dimensional distributionalmodels, in which each dimension corresponds to a context word. In this paper,we combine these two approaches by learning embeddings based ondistributional-model vectors - as opposed to one-hot vectors as is standardlydone in deep learning. We show that the combined approach has betterperformance on a word relatedness judgment task.
arxiv-5400-254 | On the properties of $α$-unchaining single linkage hierarchical clustering | http://arxiv.org/pdf/1402.4322v1.pdf | author:A. Martínez-Pérez category:cs.LG 62H30, 68T10 published:2014-02-18 summary:In the election of a hierarchical clustering method, theoretic properties maygive some insight to determine which method is the most suitable to treat aclustering problem. Herein, we study some basic properties of two hierarchicalclustering methods: $\alpha$-unchaining single linkage or $SL(\alpha)$ and amodified version of this one, $SL^*(\alpha)$. We compare the results with theproperties satisfied by the classical linkage-based hierarchical clusteringmethods.
arxiv-5400-255 | Factorial Hidden Markov Models for Learning Representations of Natural Language | http://arxiv.org/pdf/1312.6168v3.pdf | author:Anjan Nepal, Alexander Yates category:cs.LG cs.CL published:2013-12-20 summary:Most representation learning algorithms for language and image processing arelocal, in that they identify features for a data point based on surroundingpoints. Yet in language processing, the correct meaning of a word often dependson its global context. As a step toward incorporating global context intorepresentation learning, we develop a representation learning algorithm thatincorporates joint prediction into its technique for producing features for aword. We develop efficient variational methods for learning Factorial HiddenMarkov Models from large texts, and use variational distributions to producefeatures for each word that are sensitive to the entire input sequence, notjust to a local context window. Experiments on part-of-speech tagging andchunking indicate that the features are competitive with or better thanexisting state-of-the-art representation learning methods.
arxiv-5400-256 | The Random Forest Kernel and other kernels for big data from random partitions | http://arxiv.org/pdf/1402.4293v1.pdf | author:Alex Davies, Zoubin Ghahramani category:stat.ML cs.LG published:2014-02-18 summary:We present Random Partition Kernels, a new class of kernels derived bydemonstrating a natural connection between random partitions of objects andkernels between those objects. We show how the construction can be used tocreate kernels from methods that would not normally be viewed as randompartitions, such as Random Forest. To demonstrate the potential of this method,we propose two new kernels, the Random Forest Kernel and the Fast ClusterKernel, and show that these kernels consistently outperform standard kernels onproblems involving real-world datasets. Finally, we show how the form of thesekernels lend themselves to a natural approximation that is appropriate forcertain big data problems, allowing $O(N)$ inference in methods such asGaussian Processes, Support Vector Machines and Kernel PCA.
arxiv-5400-257 | Discretization of Temporal Data: A Survey | http://arxiv.org/pdf/1402.4283v1.pdf | author:P. Chaudhari, D. P. Rana, R. G. Mehta, N. J. Mistry, M. M. Raghuwanshi category:cs.DB cs.LG published:2014-02-18 summary:In real world, the huge amount of temporal data is to be processed in manyapplication areas such as scientific, financial, network monitoring, sensordata analysis. Data mining techniques are primarily oriented to handle discretefeatures. In the case of temporal data the time plays an important role on thecharacteristics of data. To consider this effect, the data discretizationtechniques have to consider the time while processing to resolve the issue byfinding the intervals of data which are more concise and precise with respectto time. Here, this research is reviewing different data discretizationtechniques used in temporal data applications according to the inclusion orexclusion of: class label, temporal order of the data and handling of streamdata to open the research direction for temporal data discretization to improvethe performance of data mining technique.
arxiv-5400-258 | Sparse, complex-valued representations of natural sounds learned with phase and amplitude continuity priors | http://arxiv.org/pdf/1312.4695v3.pdf | author:Wiktor Mlynarski category:cs.LG cs.SD q-bio.NC published:2013-12-17 summary:Complex-valued sparse coding is a data representation which employs adictionary of two-dimensional subspaces, while imposing a sparse, factorialprior on complex amplitudes. When trained on a dataset of natural imagepatches, it learns phase invariant features which closely resemble receptivefields of complex cells in the visual cortex. Features trained on naturalsounds however, rarely reveal phase invariance and capture other aspects of thedata. This observation is a starting point of the present work. As its firstcontribution, it provides an analysis of natural sound statistics by means oflearning sparse, complex representations of short speech intervals. Secondly,it proposes priors over the basis function set, which bias them towardsphase-invariant solutions. In this way, a dictionary of complex basis functionscan be learned from the data statistics, while preserving the phase invarianceproperty. Finally, representations trained on speech sounds with and withoutpriors are compared. Prior-based basis functions reveal performance comparableto unconstrained sparse coding, while explicitely representing phase as atemporal shift. Such representations can find applications in many perceptualand machine learning tasks.
arxiv-5400-259 | Group-sparse Embeddings in Collective Matrix Factorization | http://arxiv.org/pdf/1312.5921v2.pdf | author:Arto Klami, Guillaume Bouchard, Abhishek Tripathi category:stat.ML cs.LG published:2013-12-20 summary:CMF is a technique for simultaneously learning low-rank representations basedon a collection of matrices with shared entities. A typical example is thejoint modeling of user-item, item-property, and user-feature matrices in arecommender system. The key idea in CMF is that the embeddings are sharedacross the matrices, which enables transferring information between them. Theexisting solutions, however, break down when the individual matrices havelow-rank structure not shared with others. In this work we present a novel CMFsolution that allows each of the matrices to have a separate low-rank structurethat is independent of the other matrices, as well as structures that areshared only by a subset of them. We compare MAP and variational Bayesiansolutions based on alternating optimization algorithms and show that the modelautomatically infers the nature of each factor using group-wise sparsity. Ourapproach supports in a principled way continuous, binary and count observationsand is efficient for sparse matrices involving missing data. We illustrate thesolution on a number of examples, focusing in particular on an interestinguse-case of augmented multi-view learning.
arxiv-5400-260 | Extracting Networks of Characters and Places from Written Works with CHAPLIN | http://arxiv.org/pdf/1402.4259v1.pdf | author:Roberto Marazzato, Amelia Carolina Sparavigna category:cs.CY cs.CL published:2014-02-18 summary:We are proposing a tool able to gather information on social networks fromnarrative texts. Its name is CHAPLIN, CHAracters and PLaces InteractionNetwork, implemented in VB.NET. Characters and places of the narrative worksare extracted in a list of raw words. Aided by the interface, the user selectsnames out of them. After this choice, the tool allows the user to enter someparameters, and, according to them, creates a network where the nodes are thecharacters and places, and the edges their interactions. Edges are labelled byperformances. The output is a GV file, written in the DOT graph scriptinglanguage, which is rendered by means of the free open source software Graphviz.
arxiv-5400-261 | The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures | http://arxiv.org/pdf/1311.2891v3.pdf | author:Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, James Voss category:cs.LG cs.DS stat.ML published:2013-11-12 summary:In this paper we show that very large mixtures of Gaussians are efficientlylearnable in high dimension. More precisely, we prove that a mixture with knownidentical covariance matrices whose number of components is a polynomial of anyfixed degree in the dimension n is polynomially learnable as long as a certainnon-degeneracy condition on the means is satisfied. It turns out that thiscondition is generic in the sense of smoothed complexity, as soon as thedimensionality of the space is high enough. Moreover, we prove that no suchcondition can possibly exist in low dimension and the problem of learning theparameters is generically hard. In contrast, much of the existing work onGaussian Mixtures relies on low-dimensional projections and thus hits anartificial barrier. Our main result on mixture recovery relies on a new"Poissonization"-based technique, which transforms a mixture of Gaussians to alinear map of a product distribution. The problem of learning this map can beefficiently solved using some recent results on tensor decompositions andIndependent Component Analysis (ICA), thus giving an algorithm for recoveringthe mixture. In addition, we combine our low-dimensional hardness results forGaussian mixtures with Poissonization to show how to embed difficult instancesof low-dimensional Gaussian mixtures into the ICA setting, thus establishingexponential information-theoretic lower bounds for underdetermined ICA in lowdimension. To the best of our knowledge, this is the first such result in theliterature. In addition to contributing to the problem of Gaussian mixturelearning, we believe that this work is among the first steps toward betterunderstanding the rare phenomenon of the "blessing of dimensionality" in thecomputational aspects of statistical inference.
arxiv-5400-262 | One-Shot Adaptation of Supervised Deep Convolutional Models | http://arxiv.org/pdf/1312.6204v2.pdf | author:Judy Hoffman, Eric Tzeng, Jeff Donahue, Yangqing Jia, Kate Saenko, Trevor Darrell category:cs.CV cs.LG cs.NE published:2013-12-21 summary:Dataset bias remains a significant barrier towards solving real worldcomputer vision tasks. Though deep convolutional networks have proven to be acompetitive approach for image classification, a question remains: have thesemodels have solved the dataset bias problem? In general, training orfine-tuning a state-of-the-art deep model on a new domain requires asignificant amount of data, which for many applications is simply notavailable. Transfer of models directly to new domains without adaptation hashistorically led to poor recognition performance. In this paper, we pose thefollowing question: is a single image dataset, much larger than previouslyexplored for adaptation, comprehensive enough to learn general deep models thatmay be effectively applied to new image domains? In other words, are deep CNNstrained on large amounts of labeled data as susceptible to dataset bias asprevious methods have been shown to be? We show that a generic supervised deepCNN model trained on a large dataset reduces, but does not remove, datasetbias. Furthermore, we propose several methods for adaptation with deep modelsthat are able to operate with little (one example per category) or no labeleddomain specific data. Our experiments show that adaptation of deep models onbenchmark visual domain adaptation datasets can provide a significantperformance boost.
arxiv-5400-263 | When Learners Surpass their Sources: Mathematical Modeling of Learning from an Inconsistent Source | http://arxiv.org/pdf/1402.4678v1.pdf | author:Yelena Mandelshtam, Natalia Komarova category:cs.CL published:2014-02-18 summary:We present a new algorithm to model and investigate the learning process of alearner mastering a set of grammatical rules from an inconsistent source. Thecompelling interest of human language acquisition is that the learning succeedsin virtually every case, despite the fact that the input data are formallyinadequate to explain the success of learning. Our model explains how a learnercan successfully learn from or even surpass its imperfect source withoutpossessing any additional biases or constraints about the types of patternsthat exist in the language. We use the data collected by Singleton and Newport(2004) on the performance of a 7-year boy Simon, who mastered the American SignLanguage (ASL) by learning it from his parents, both of whom were imperfectspeakers of ASL. We show that the algorithm possesses a frequency-boostingproperty, whereby the frequency of the most common form of the source isincreased by the learner. We also explain several key features of Simon's ASL.
arxiv-5400-264 | Semistochastic Quadratic Bound Methods | http://arxiv.org/pdf/1309.1369v4.pdf | author:Aleksandr Y. Aravkin, Anna Choromanska, Tony Jebara, Dimitri Kanevsky category:stat.ML cs.LG math.NA stat.CO published:2013-09-05 summary:Partition functions arise in a variety of settings, including conditionalrandom fields, logistic regression, and latent gaussian models. In this paper,we consider semistochastic quadratic bound (SQB) methods for maximum likelihoodinference based on partition function optimization. Batch methods based on thequadratic bound were recently proposed for this class of problems, andperformed favorably in comparison to state-of-the-art techniques.Semistochastic methods fall in between batch algorithms, which use all thedata, and stochastic gradient type methods, which use small random selectionsat each iteration. We build semistochastic quadratic bound-based methods, andprove both global convergence (to a stationary point) under very weakassumptions, and linear convergence rate under stronger assumptions on theobjective. To make the proposed methods faster and more stable, we considerinexact subproblem minimization and batch-size selection schemes. The efficacyof SQB methods is demonstrated via comparison with several state-of-the-arttechniques on commonly used datasets.
arxiv-5400-265 | Continuous Learning: Engineering Super Features With Feature Algebras | http://arxiv.org/pdf/1312.5398v2.pdf | author:Michael Tetelman category:cs.LG stat.ML published:2013-12-19 summary:In this paper we consider a problem of searching a space of predictive modelsfor a given training data set. We propose an iterative procedure for deriving asequence of improving models and a corresponding sequence of sets of non-linearfeatures on the original input space. After a finite number of iterations N,the non-linear features become 2^N -degree polynomials on the original space.We show that in a limit of an infinite number of iterations derived non-linearfeatures must form an associative algebra: a product of two features is equalto a linear combination of features from the same feature space for any giveninput point. Because each iteration consists of solving a series of convexproblems that contain all previous solutions, the likelihood of the models inthe sequence is increasing with each iteration while the dimension of the modelparameter space is set to a limited controlled value.
arxiv-5400-266 | Selective Sampling with Drift | http://arxiv.org/pdf/1402.4084v1.pdf | author:Edward Moroshko, Koby Crammer category:cs.LG published:2014-02-17 summary:Recently there has been much work on selective sampling, an online activelearning setting, in which algorithms work in rounds. On each round analgorithm receives an input and makes a prediction. Then, it can decide whetherto query a label, and if so to update its model, otherwise the input isdiscarded. Most of this work is focused on the stationary case, where it isassumed that there is a fixed target model, and the performance of thealgorithm is compared to a fixed model. However, in many real-worldapplications, such as spam prediction, the best target function may drift overtime, or have shifts from time to time. We develop a novel selective samplingalgorithm for the drifting setting, analyze it under no assumptions on themechanism generating the sequence of instances, and derive new mistake boundsthat depend on the amount of drift in the problem. Simulations on synthetic andreal-world datasets demonstrate the superiority of our algorithms as aselective sampling algorithm in the drifting setting.
arxiv-5400-267 | Statistical Noise Analysis in SENSE Parallel MRI | http://arxiv.org/pdf/1402.4067v1.pdf | author:Santiago Aja-Fernandez, Gonzalo Vegas-Sanchez-Ferrero, Antonio Trsitan-Vega category:cs.CV published:2014-02-17 summary:A complete first and second order statistical characterization of noise inSENSE reconstructed data is proposed. SENSE acquisitions have usually beenmodeled as Rician distributed, since the data reconstruction takes place intothe spatial domain, where Gaussian noise is assumed. However, this model justholds for the first order statistics and obviates other effects induced bycoils correlations and the reconstruction interpolation. Those effects areproperly taken into account in this study, in order to fully justify a finalSENSE noise model. As a result, some interesting features of the reconstructedimage arise: (1) There is a strong correlation between adjacent lines. (2) Theresulting distribution is non-stationary and therefore the variance of noisewill vary from point to point across the image. Closed equations for thecalculation of the variance of noise and the correlation coefficient betweenlines are proposed. The proposed model is totally compatible with g-factorformulations.
arxiv-5400-268 | The Algebraic Approach to Phase Retrieval and Explicit Inversion at the Identifiability Threshold | http://arxiv.org/pdf/1402.4053v1.pdf | author:Franz J Király, Martin Ehler category:math.FA cs.CV cs.IT math.AG math.IT stat.ML published:2014-02-17 summary:We study phase retrieval from magnitude measurements of an unknown signal asan algebraic estimation problem. Indeed, phase retrieval from rank-one and moregeneral linear measurements can be treated in an algebraic way. It is verifiedthat a certain number of generic rank-one or generic linear measurements aresufficient to enable signal reconstruction for generic signals, and slightlymore generic measurements yield reconstructability for all signals. Our resultssolve a few open problems stated in the recent literature. Furthermore, we showhow the algebraic estimation problem can be solved by a closed-form algebraicestimation technique, termed ideal regression, providing non-asymptotic successguarantees.
arxiv-5400-269 | Modeling correlations in spontaneous activity of visual cortex with centered Gaussian-binary deep Boltzmann machines | http://arxiv.org/pdf/1312.6108v3.pdf | author:Nan Wang, Dirk Jancke, Laurenz Wiskott category:cs.NE cs.LG q-bio.NC published:2013-12-20 summary:Spontaneous cortical activity -- the ongoing cortical activities in absenceof intentional sensory input -- is considered to play a vital role in manyaspects of both normal brain functions and mental dysfunctions. We present acentered Gaussian-binary Deep Boltzmann Machine (GDBM) for modeling theactivity in early cortical visual areas and relate the random sampling in GDBMsto the spontaneous cortical activity. After training the proposed model onnatural image patches, we show that the samples collected from the model'sprobability distribution encompass similar activity patterns as found in thespontaneous activity. Specifically, filters having the same orientationpreference tend to be active together during random sampling. Our workdemonstrates the centered GDBM is a meaningful model approach for basicreceptive field properties and the emergence of spontaneous activity patternsin early cortical visual areas. Besides, we show empirically that centeredGDBMs do not suffer from the difficulties during training as GDBMs do and canbe properly trained without the layer-wise pretraining.
arxiv-5400-270 | Revisiting Natural Gradient for Deep Networks | http://arxiv.org/pdf/1301.3584v7.pdf | author:Razvan Pascanu, Yoshua Bengio category:cs.LG cs.NA published:2013-01-16 summary:We evaluate natural gradient, an algorithm originally proposed in Amari(1997), for learning deep models. The contributions of this paper are asfollows. We show the connection between natural gradient and three otherrecently proposed methods for training deep models: Hessian-Free (Martens,2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux etal., 2008). We describe how one can use unlabeled data to improve thegeneralization error obtained by natural gradient and empirically evaluate therobustness of the algorithm to the ordering of the training set compared tostochastic gradient descent. Finally we extend natural gradient to incorporatesecond order information alongside the manifold information and provide abenchmark of the new algorithm using a truncated Newton approach for invertingthe metric matrix instead of using a diagonal approximation of it.
arxiv-5400-271 | Is Spiking Logic the Route to Memristor-Based Computers? | http://arxiv.org/pdf/1402.4036v1.pdf | author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.ET cs.AR cs.NE 94C-06 C.1.3; B.3.1 published:2014-02-17 summary:Memristors have been suggested as a novel route to neuromorphic computingbased on the similarity between neurons (synapses and ion pumps) andmemristors. The D.C. action of the memristor is a current spike, which we thinkwill be fruitful for building memristor computers. In this paper, we introduce4 different logical assignations to implement sequential logic in the memristorand introduce the physical rules, summation, `bounce-back', directionality and`diminishing returns', elucidated from our investigations. We then demonstratehow memristor sequential logic works by instantiating a NOT gate, an AND gateand a Full Adder with a single memristor. The Full Adder makes use of thememristor's memory to add three binary values together and outputs the value,the carry digit and even the order they were input in.
arxiv-5400-272 | Connecting Spiking Neurons to a Spiking Memristor Network Changes the Memristor Dynamics | http://arxiv.org/pdf/1402.4029v1.pdf | author:Deborah Gater, Attya Iqbal, Jeffrey Davey, Ella Gale category:cs.ET cs.NE physics.bio-ph 92C-06, 94C-06 published:2014-02-17 summary:Memristors have been suggested as neuromorphic computing elements. Spike-timedependent plasticity and the Hodgkin-Huxley model of the neuron have both beenmodelled effectively by memristor theory. The d.c. response of the memristor isa current spike. Based on these three facts we suggest that memristors arewell-placed to interface directly with neurons. In this paper we show thatconnecting a spiking memristor network to spiking neuronal cells causes achange in the memristor network dynamics by: removing the memristor spikes,which we show is due to the effects of connection to aqueous medium; causing achange in current decay rate consistent with a change in memristor state;presenting more-linear $I-t$ dynamics; and increasing the memristor spikingrate, as a consequence of interaction with the spiking neurons. Thisdemonstrates that neurons are capable of communicating directly withmemristors, without the need for computer translation.
arxiv-5400-273 | Does the D.C. Response of Memristors Allow Robotic Short-Term Memory and a Possible Route to Artificial Time Perception? | http://arxiv.org/pdf/1402.4007v1.pdf | author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.RO cs.ET cs.NE 94Cxx published:2014-02-17 summary:Time perception is essential for task switching, and in the mammalian brainappears alongside other processes. Memristors are electronic components used assynapses and as models for neurons. The d.c. response of memristors can beconsidered as a type of short-term memory. Interactions of the memristor d.c.response within networks of memristors leads to the emergence of oscillatorydynamics and intermittent spike trains, which are similar to neural dynamics.Based on this data, the structure of a memristor network control for a robot asit undergoes task switching is discussed and it is suggested that theseemergent network dynamics could improve the performance of role switching andlearning in an artificial intelligence and perhaps create artificial timeperception.
arxiv-5400-274 | Dimensionality reduction with subgaussian matrices: a unified theory | http://arxiv.org/pdf/1402.3973v1.pdf | author:Sjoerd Dirksen category:cs.IT cs.DS math.IT stat.ML published:2014-02-17 summary:We present a theory for Euclidean dimensionality reduction with subgaussianmatrices which unifies several restricted isometry property andJohnson-Lindenstrauss type results obtained earlier for specific data sets. Inparticular, we recover and, in several cases, improve results for sets ofsparse and structured sparse vectors, low-rank matrices and tensors, and smoothmanifolds. In addition, we establish a new Johnson-Lindenstrauss embedding fordata sets taking the form of an infinite union of subspaces of a Hilbert space.
arxiv-5400-275 | Weyl group orbit functions in image processing | http://arxiv.org/pdf/1404.0566v1.pdf | author:Goce Chadzitaskos, Lenka Háková, Ondřej Kajínek category:cs.CV published:2014-02-17 summary:We deal with the Fourier-like analysis of functions on discrete grids intwo-dimensional simplexes using $C-$ and $E-$ Weyl group orbit functions. Forthese cases we present the convolution theorem. We provide an example ofapplication of image processing using the $C-$ functions and the convolutionsfor spatial filtering of the treated image.
arxiv-5400-276 | Sparse Coding Approach for Multi-Frame Image Super Resolution | http://arxiv.org/pdf/1402.3926v1.pdf | author:Toshiyuki Kato, Hideitsu Hino, Noboru Murata category:cs.CV published:2014-02-17 summary:An image super-resolution method from multiple observation of low-resolutionimages is proposed. The method is based on sub-pixel accuracy block matchingfor estimating relative displacements of observed images, and sparse signalrepresentation for estimating the corresponding high-resolution image. Relativedisplacements of small patches of observed low-resolution images are accuratelyestimated by a computationally efficient block matching method. Since theestimated displacements are also regarded as a warping component of imagedegradation process, the matching results are directly utilized to generatelow-resolution dictionary for sparse image representation. The matching scoresof the block matching are used to select a subset of low-resolution patches forreconstructing a high-resolution patch, that is, an adaptive selection ofinformative low-resolution images is realized. When there is only onelow-resolution image, the proposed method works as a single-framesuper-resolution method. The proposed method is shown to perform comparable orsuperior to conventional single- and multi-frame super-resolution methodsthrough experiments using various real-world datasets.
arxiv-5400-277 | Performance Evaluation of Machine Learning Classifiers in Sentiment Mining | http://arxiv.org/pdf/1402.3891v1.pdf | author:Vinodhini G Chandrasekaran RM category:cs.LG cs.CL cs.IR published:2014-02-17 summary:In recent years, the use of machine learning classifiers is of great value insolving a variety of problems in text classification. Sentiment mining is akind of text classification in which, messages are classified according tosentiment orientation such as positive or negative. This paper extends the ideaof evaluating the performance of various classifiers to show theireffectiveness in sentiment mining of online product reviews. The productreviews are collected from Amazon reviews. To evaluate the performance ofclassifiers various evaluation methods like random sampling, linear samplingand bootstrap sampling are used. Our results shows that support vector machinewith bootstrap sampling method outperforms others classifiers and samplingmethods in terms of misclassification rate.
arxiv-5400-278 | Correlation-based construction of neighborhood and edge features | http://arxiv.org/pdf/1312.7335v2.pdf | author:Balázs Kégl category:cs.CV cs.LG stat.ML published:2013-12-20 summary:Motivated by an abstract notion of low-level edge detector filters, wepropose a simple method of unsupervised feature construction based on pairwisestatistics of features. In the first step, we construct neighborhoods offeatures by regrouping features that correlate. Then we use these subsets asfilters to produce new neighborhood features. Next, we connect neighborhoodfeatures that correlate, and construct edge features by subtracting thecorrelated neighborhood features of each other. To validate the usefulness ofthe constructed features, we ran AdaBoost.MH on four multi-class classificationproblems. Our most significant result is a test error of 0.94% on MNIST with analgorithm which is essentially free of any image-specific priors. On CIFAR-10our method is suboptimal compared to today's best deep learning techniques,nevertheless, we show that the proposed method outperforms not only boosting onthe raw pixels, but also boosting on Haar filters.
arxiv-5400-279 | Scalable Kernel Clustering: Approximate Kernel k-means | http://arxiv.org/pdf/1402.3849v1.pdf | author:Radha Chitta, Rong Jin, Timothy C. Havens, Anil K. Jain category:cs.CV cs.DS cs.LG published:2014-02-16 summary:Kernel-based clustering algorithms have the ability to capture the non-linearstructure in real world data. Among various kernel-based clustering algorithms,kernel k-means has gained popularity due to its simple iterative nature andease of implementation. However, its run-time complexity and memory footprintincrease quadratically in terms of the size of the data set, and hence, largedata sets cannot be clustered efficiently. In this paper, we propose anapproximation scheme based on randomization, called the Approximate Kernelk-means. We approximate the cluster centers using the kernel similarity betweena few sampled points and all the points in the data set. We show that theproposed method achieves better clustering performance than the traditional lowrank kernel approximation based clustering schemes. We also demonstrate thatits running time and memory requirements are significantly lower than those ofkernel k-means, with only a small reduction in the clustering quality onseveral public domain large data sets. We then employ ensemble clusteringtechniques to further enhance the performance of our algorithm.
arxiv-5400-280 | Sparse similarity-preserving hashing | http://arxiv.org/pdf/1312.5479v3.pdf | author:Jonathan Masci, Alex M. Bronstein, Michael M. Bronstein, Pablo Sprechmann, Guillermo Sapiro category:cs.CV cs.DS published:2013-12-19 summary:In recent years, a lot of attention has been devoted to efficient nearestneighbor search by means of similarity-preserving hashing. One of the plightsof existing hashing techniques is the intrinsic trade-off between performanceand computational complexity: while longer hash codes allow for lower falsepositive rates, it is very difficult to increase the embedding dimensionalitywithout incurring in very high false negatives rates or prohibitingcomputational costs. In this paper, we propose a way to overcome thislimitation by enforcing the hash codes to be sparse. Sparse high-dimensionalcodes enjoy from the low false positive rates typical of long hashes, whilekeeping the false negative rates similar to those of a shorter dense hashingscheme with equal number of degrees of freedom. We use a tailored feed-forwardneural network for the hashing function. Extensive experimental evaluationinvolving visual and multi-modal data shows the benefits of the proposedmethod.
arxiv-5400-281 | Unsupervised feature learning by augmenting single images | http://arxiv.org/pdf/1312.5242v3.pdf | author:Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox category:cs.CV cs.LG cs.NE published:2013-12-18 summary:When deep learning is applied to visual object recognition, data augmentationis often used to generate additional training data without extra labeling cost.It helps to reduce overfitting and increase the performance of the algorithm.In this paper we investigate if it is possible to use data augmentation as themain component of an unsupervised feature learning architecture. To that end wesample a set of random image patches and declare each of them to be a separatesingle-image surrogate class. We then extend these trivial one-element classesby applying a variety of transformations to the initial 'seed' patches. Finallywe train a convolutional neural network to discriminate between these surrogateclasses. The feature representation learned by the network can then be used invarious vision tasks. We find that this simple feature learning algorithm issurprisingly successful, achieving competitive classification results onseveral popular vision datasets (STL-10, CIFAR-10, Caltech-101).
arxiv-5400-282 | Multi-View Priors for Learning Detectors from Sparse Viewpoint Data | http://arxiv.org/pdf/1312.6095v2.pdf | author:Bojan Pepik, Michael Stark, Peter Gehler, Bernt Schiele category:cs.CV published:2013-12-20 summary:While the majority of today's object class models provide only 2D boundingboxes, far richer output hypotheses are desirable including viewpoint,fine-grained category, and 3D geometry estimate. However, models trained toprovide richer output require larger amounts of training data, preferably wellcovering the relevant aspects such as viewpoint and fine-grained categories. Inthis paper, we address this issue from the perspective of transfer learning,and design an object class model that explicitly leverages correlations betweenvisual features. Specifically, our model represents prior distributions overpermissible multi-view detectors in a parametric way -- the priors are learnedonce from training data of a source object class, and can later be used tofacilitate the learning of a detector for a target class. As we show in ourexperiments, this transfer is not only beneficial for detectors based onbasic-level category representations, but also enables the robust learning ofdetectors that represent classes at finer levels of granularity, where trainingdata is typically even scarcer and more unbalanced. As a result, we reportlargely improved performance in simultaneous 2D object localization andviewpoint estimation on a recent dataset of challenging street scenes.
arxiv-5400-283 | On the Resilience of an Ant-based System in Fuzzy Environments. An Empirical Study | http://arxiv.org/pdf/1401.4660v2.pdf | author:Gloria Cerasela Crisan, Camelia-M. Pintea, Petrica C. Pop category:cs.NE published:2014-01-19 summary:The current work describes an empirical study conducted in order toinvestigate the behavior of an optimization method in a fuzzy environment.MAX-MIN Ant System, an efficient implementation of a heuristic method is usedfor solving an optimization problem derived from the Traveling Salesman Problem(TSP). Several publicly-available symmetric TSP instances and their fuzzyvariants are tested in order to extract some general features. The entry datawas adapted by introducing a two-dimensional systematic degree of fuzziness,proportional with the number of nodes, the dimension of the instance and alsowith the distances between nodes, the scale of the instance. The results showthat our proposed method can handle the data uncertainty, showing goodresilience and adaptability.
arxiv-5400-284 | word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method | http://arxiv.org/pdf/1402.3722v1.pdf | author:Yoav Goldberg, Omer Levy category:cs.CL cs.LG stat.ML published:2014-02-15 summary:The word2vec software of Tomas Mikolov and colleagues(https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, andprovides state-of-the-art word embeddings. The learning models behind thesoftware are described in two research papers. We found the description of themodels in these papers to be somewhat cryptic and hard to follow. While themotivations and presentation may be obvious to the neural-networkslanguage-modeling crowd, we had to struggle quite a bit to figure out therationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in"Distributed Representations of Words and Phrases and their Compositionality"by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.
arxiv-5400-285 | Can recursive neural tensor networks learn logical reasoning? | http://arxiv.org/pdf/1312.6192v4.pdf | author:Samuel R. Bowman category:cs.CL cs.LG published:2013-12-21 summary:Recursive neural network models and their accompanying vector representationsfor words have seen success in an array of increasingly semanticallysophisticated tasks, but almost nothing is known about their ability toaccurately capture the aspects of linguistic meaning that are necessary forinterpretation or reasoning. To evaluate this, I train a recursive model on anew corpus of constructed examples of logical reasoning in short sentences,like the inference of "some animal walks" from "some dog walks" or "some catwalks," given that dogs and cats are animals. This model learns representationsthat generalize well to new types of reasoning pattern in all but a few cases,a result which is promising for the ability of learned representation models tocapture logical reasoning.
arxiv-5400-286 | One-Shot-Learning Gesture Recognition using HOG-HOF Features | http://arxiv.org/pdf/1312.4190v2.pdf | author:Jakub Konečný, Michal Hagara category:cs.CV published:2013-12-15 summary:The purpose of this paper is to describe one-shot-learning gesturerecognition systems developed on the \textit{ChaLearn Gesture Dataset}. We useRGB and depth images and combine appearance (Histograms of Oriented Gradients)and motion descriptors (Histogram of Optical Flow) for parallel temporalsegmentation and recognition. The Quadratic-Chi distance family is used tomeasure differences between histograms to capture cross-bin relationships. Wealso propose a new algorithm for trimming videos --- to remove all theunimportant frames from videos. We present two methods that use combination ofHOG-HOF descriptors together with variants of Dynamic Time Warping technique.Both methods outperform other published methods and help narrow down the gapbetween human performance and algorithms on this task. The code has been madepublicly available in the MLOSS repository.
arxiv-5400-287 | A Linguistic Model for Terminology Extraction based Conditional Random Fields | http://arxiv.org/pdf/1210.0252v2.pdf | author:Fethi Fkih, Mohamed Nazih Omri, Imen Toumia category:cs.CL cs.AI I.2.6; I.2.7 published:2012-09-30 summary:In this paper, we show the possibility of using a linear Conditional RandomFields (CRF) for terminology extraction from a specialized text corpus.
arxiv-5400-288 | A Narrative Vehicle Protection Representation for Vehicle Speed Regulator Under Driver Exhaustion -- A Study | http://arxiv.org/pdf/1402.3657v1.pdf | author:V. Karthikeyan, B. Praveen Kumar, S. Suresh Babu, R. Purusothaman, shijin Thomas category:cs.CV cs.HC published:2014-02-15 summary:Driver fatigue is one of the important factors that cause traffic accidents,and the ever-increasing number due to diminished drivers vigilance level hasbecome a problem of serious concern to society. Drivers with a diminishedvigilance level suffer from a marked decline in their abilities of perception,recognition, and vehicle control, and therefore pose serious danger to theirown life and the lives of other people. Exhaustion resulting from sleepdeprivation or sleep disorders is an important factor in the creasing number ofaccidents. In this projected work, we discuss the various methods of theexisting and the proposed method based on a real time online safety prototypethat controls the vehicle speed under driver fatigue. The purpose of such amodel is to advance a system to detect fatigue symptoms in drivers and controlthe speed of vehicle to avoid accidents. This system was tested adequately withsubjects of different technology of various researchers finally the validity ofthe proposed model for vehicle speed controller based on driver fatiguedetection is shown.
arxiv-5400-289 | Auto Spell Suggestion for High Quality Speech Synthesis in Hindi | http://arxiv.org/pdf/1402.3648v1.pdf | author:Shikha Kabra, Ritika Agarwal category:cs.CL cs.SD published:2014-02-15 summary:The goal of Text-to-Speech (TTS) synthesis in a particular language is toconvert arbitrary input text to intelligible and natural sounding speech.However, for a particular language like Hindi, which is a highly confusinglanguage (due to very close spellings), it is not an easy task to identifyerrors/mistakes in input text and an incorrect text degrade the quality ofoutput speech hence this paper is a contribution to the development of highquality speech synthesis with the involvement of Spellchecker which generatesspell suggestions for misspelled words automatically. Involvement ofspellchecker would increase the efficiency of speech synthesis by providingspell suggestions for incorrect input text. Furthermore, we have provided thecomparative study for evaluating the resultant effect on to phonetic text byadding spellchecker on to input text.
arxiv-5400-290 | Automated Fabric Defect Inspection: A Survey of Classifiers | http://arxiv.org/pdf/1405.6177v1.pdf | author:Md. Tarek Habib, Rahat Hossain Faisal, M. Rokonuzzaman, Farruk Ahmed category:cs.CV cs.LG published:2014-02-14 summary:Quality control at each stage of production in textile industry has become akey factor to retaining the existence in the highly competitive global market.Problems of manual fabric defect inspection are lack of accuracy and high timeconsumption, where early and accurate fabric defect detection is a significantphase of quality control. Computer vision based, i.e. automated fabric defectinspection systems are thought by many researchers of different countries to bevery useful to resolve these problems. There are two major challenges to beresolved to attain a successful automated fabric defect inspection system. Theyare defect detection and defect classification. In this work, we discussdifferent techniques used for automated fabric defect classification, then showa survey of classifiers used in automated fabric defect inspection systems, andfinally, compare these classifiers by using performance metrics. This work isexpected to be very useful for the researchers in the area of automated fabricdefect inspection to understand and evaluate the many potential options in thisfield.
arxiv-5400-291 | Improving Streaming Video Segmentation with Early and Mid-Level Visual Processing | http://arxiv.org/pdf/1402.3557v1.pdf | author:Subarna Tripathi, Youngbae Hwang, Serge Belongie, Truong Nguyen category:cs.CV published:2014-02-14 summary:Despite recent advances in video segmentation, many opportunities remain toimprove it using a variety of low and mid-level visual cues. We proposeimprovements to the leading streaming graph-based hierarchical videosegmentation (streamGBH) method based on early and mid level visual processing.The extensive experimental analysis of our approach validates the improvementof hierarchical supervoxel representation by incorporating motion and colorwith effective filtering. We also pose and illuminate some open questionstowards intermediate level video analysis as further extension to streamGBH. Weexploit the supervoxels as an initialization towards estimation of dominantaffine motion regions, followed by merging of such motion regions in order tohierarchically segment a video in a novel motion-segmentation framework whichaims at subsequent applications such as foreground recognition.
arxiv-5400-292 | Modeling Human Decision-making in Generalized Gaussian Multi-armed Bandits | http://arxiv.org/pdf/1307.6134v3.pdf | author:Paul Reverdy, Vaibhav Srivastava, Naomi E. Leonard category:cs.LG math.OC stat.ML published:2013-07-23 summary:We present a formal model of human decision-making in explore-exploit tasksusing the context of multi-armed bandit problems, where the decision-maker mustchoose among multiple options with uncertain rewards. We address the standardmulti-armed bandit problem, the multi-armed bandit problem with transitioncosts, and the multi-armed bandit problem on graphs. We focus on the case ofGaussian rewards in a setting where the decision-maker uses Bayesian inferenceto estimate the reward values. We model the decision-maker's prior knowledgewith the Bayesian prior on the mean reward. We develop the upper credible limit(UCL) algorithm for the standard multi-armed bandit problem and show that thisdeterministic algorithm achieves logarithmic cumulative expected regret, whichis optimal performance for uninformative priors. We show how good priors andgood assumptions on the correlation structure among arms can greatly enhancedecision-making performance, even over short time horizons. We extend to thestochastic UCL algorithm and draw several connections to human decision-makingbehavior. We present empirical data from human experiments and show that humanperformance is efficiently captured by the stochastic UCL algorithm withappropriate parameters. For the multi-armed bandit problem with transitioncosts and the multi-armed bandit problem on graphs, we generalize the UCLalgorithm to the block UCL algorithm and the graphical block UCL algorithm,respectively. We show that these algorithms also achieve logarithmic cumulativeexpected regret and require a sub-logarithmic expected number of transitionsamong arms. We further illustrate the performance of these algorithms withnumerical examples.
arxiv-5400-293 | The Law of Total Odds | http://arxiv.org/pdf/1312.0365v5.pdf | author:Dirk Tasche category:math.PR stat.AP stat.ML 60A05, 62H30 published:2013-12-02 summary:The law of total probability may be deployed in binary classificationexercises to estimate the unconditional class probabilities if the classproportions in the training set are not representative of the population classproportions. We argue that this is not a conceptually sound approach andsuggest an alternative based on the new law of total odds. We quantify the biasof the total probability estimator of the unconditional class probabilities andshow that the total odds estimator is unbiased. The sample version of the totalodds estimator is shown to coincide with a maximum-likelihood estimator knownfrom the literature. The law of total odds can also be used for transformingthe conditional class probabilities if independent estimates of theunconditional class probabilities of the population are available. Keywords: Total probability, likelihood ratio, Bayes' formula, binaryclassification, relative odds, unbiased estimator, supervised learning, datasetshift.
arxiv-5400-294 | On the number of response regions of deep feed forward networks with piece-wise linear activations | http://arxiv.org/pdf/1312.6098v5.pdf | author:Razvan Pascanu, Guido Montufar, Yoshua Bengio category:cs.LG cs.NE published:2013-12-20 summary:This paper explores the complexity of deep feedforward networks with linearpre-synaptic couplings and rectified linear activations. This is a contributionto the growing body of work contrasting the representational power of deep andshallow network architectures. In particular, we offer a framework forcomparing deep and shallow models that belong to the family of piecewise linearfunctions based on computational geometry. We look at a deep rectifiermulti-layer perceptron (MLP) with linear outputs units and compare it with asingle layer version of the model. In the asymptotic regime, when the number ofinputs stays constant, if the shallow model has $kn$ hidden units and $n_0$inputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$layer model with $n$ hidden units on each layer it is $\Omega(\left\lfloor{n}/{n_0}\right\rfloor^{k-1}n^{n_0})$. The number$\left\lfloor{n}/{n_0}\right\rfloor^{k-1}$ grows faster than $k^{n_0}$ when $n$tends to infinity or when $k$ tends to infinity and $n \geq 2n_0$.Additionally, even when $k$ is small, if we restrict $n$ to be $2n_0$, we canshow that a deep model has considerably more linear regions that a shallow one.We consider this as a first step towards understanding the complexity of thesemodels and specifically towards providing suitable mathematical tools forfuture analysis.
arxiv-5400-295 | A Clockwork RNN | http://arxiv.org/pdf/1402.3511v1.pdf | author:Jan Koutník, Klaus Greff, Faustino Gomez, Jürgen Schmidhuber category:cs.NE cs.LG published:2014-02-14 summary:Sequence prediction and classification are ubiquitous and challengingproblems in machine learning that can require identifying complex dependenciesbetween temporally distant inputs. Recurrent Neural Networks (RNNs) have theability, in theory, to cope with these temporal dependencies by virtue of theshort-term memory implemented by their recurrent (feedback) connections.However, in practice they are difficult to train successfully when thelong-term memory is required. This paper introduces a simple, yet powerfulmodification to the standard RNN architecture, the Clockwork RNN (CW-RNN), inwhich the hidden layer is partitioned into separate modules, each processinginputs at its own temporal granularity, making computations only at itsprescribed clock rate. Rather than making the standard RNN models more complex,CW-RNN reduces the number of RNN parameters, improves the performancesignificantly in the tasks tested, and speeds up the network evaluation. Thenetwork is demonstrated in preliminary experiments involving two tasks: audiosignal generation and TIMIT spoken word classification, where it outperformsboth RNN and LSTM networks.
arxiv-5400-296 | Generative Modelling for Unsupervised Score Calibration | http://arxiv.org/pdf/1311.0707v3.pdf | author:Niko Brümmer, Daniel Garcia-Romero category:stat.ML cs.LG published:2013-11-04 summary:Score calibration enables automatic speaker recognizers to makecost-effective accept / reject decisions. Traditional calibration requiressupervised data, which is an expensive resource. We propose a 2-component GMMfor unsupervised calibration and demonstrate good performance relative to asupervised baseline on NIST SRE'10 and SRE'12. A Bayesian analysis demonstratesthat the uncertainty associated with the unsupervised calibration parameterestimates is surprisingly small.
arxiv-5400-297 | Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence | http://arxiv.org/pdf/1312.6002v3.pdf | author:Mathias Berglund, Tapani Raiko category:cs.NE cs.LG stat.ML 62M45 I.2.6 published:2013-12-20 summary:Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) arepopular methods for training the weights of Restricted Boltzmann Machines.However, both methods use an approximate method for sampling from the modeldistribution. As a side effect, these approximations yield significantlydifferent biases and variances for stochastic gradient estimates of individualdata points. It is well known that CD yields a biased gradient estimate. Inthis paper we however show empirically that CD has a lower stochastic gradientestimate variance than exact sampling, while the mean of subsequent PCDestimates has a higher variance than exact sampling. The results give oneexplanation to the finding that CD can be used with smaller minibatches orhigher learning rates than PCD.
arxiv-5400-298 | Authorship Analysis based on Data Compression | http://arxiv.org/pdf/1402.3405v1.pdf | author:Daniele Cerra, Mihai Datcu, Peter Reinartz category:cs.CL cs.DL cs.IR stat.ML published:2014-02-14 summary:This paper proposes to perform authorship analysis using the Fast CompressionDistance (FCD), a similarity measure based on compression with dictionariesdirectly extracted from the written texts. The FCD computes a similaritybetween two documents through an effective binary search on the intersectionset between the two related dictionaries. In the reported experiments theproposed method is applied to documents which are heterogeneous in style,written in five different languages and coming from different historicalperiods. Results are comparable to the state of the art and outperformtraditional compression-based methods.
arxiv-5400-299 | Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget | http://arxiv.org/pdf/1304.5299v4.pdf | author:Anoop Korattikara, Yutian Chen, Max Welling category:cs.LG stat.ML published:2013-04-19 summary:Can we make Bayesian posterior MCMC sampling more efficient when faced withvery large datasets? We argue that computing the likelihood for N datapoints inthe Metropolis-Hastings (MH) test to reach a single binary decision iscomputationally inefficient. We introduce an approximate MH rule based on asequential hypothesis test that allows us to accept or reject samples with highconfidence using only a fraction of the data required for the exact MH rule.While this method introduces an asymptotic bias, we show that this bias can becontrolled and is more than offset by a decrease in variance due to our abilityto draw more samples per unit of time.
arxiv-5400-300 | Machine Learning of Phonologically Conditioned Noun Declensions For Tamil Morphological Generators | http://arxiv.org/pdf/1402.3382v1.pdf | author:K. Rajan, Dr. V. Ramalingam, Dr. M. Ganesan category:cs.CL published:2014-02-14 summary:This paper presents machine learning solutions to a practical problem ofNatural Language Generation (NLG), particularly the word formation inagglutinative languages like Tamil, in a supervised manner. The morphologicalgenerator is an important component of Natural Language Processing inArtificial Intelligence. It generates word forms given a root and affixes. Themorphophonemic changes like addition, deletion, alternation etc., occur whentwo or more morphemes or words joined together. The Sandhi rules should beexplicitly specified in the rule based morphological analyzers and generators.In machine learning framework, these rules can be learned automatically by thesystem from the training samples and subsequently be applied for new inputs. Inthis paper we proposed the machine learning models which learn themorphophonemic rules for noun declensions from the given training data. Thesemodels are trained to learn sandhi rules using various learning algorithms andthe performance of those algorithms are presented. From this we conclude thatmachine learning of morphological processing such as word form generation canbe successfully learned in a supervised manner, without explicit description ofrules. The performance of Decision trees and Bayesian machine learningalgorithms on noun declensions are discussed.
