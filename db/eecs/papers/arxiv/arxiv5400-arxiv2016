arxiv-1312-7003 | Supervised learning of a regression model based on latent process. Application to the estimation of fuel cell life time |  http://arxiv.org/abs/1312.7003  | author:Raïssa Onanena, Faicel Chamroukhi, Latifa Oukhellou, Denis Candusso, Patrice Aknin, Daniel Hissel category:stat.ML cs.LG stat.AP published:2013-12-25 summary:This paper describes a pattern recognition approach aiming to estimate fuelcell duration time from electrochemical impedance spectroscopy measurements. Itconsists in first extracting features from both real and imaginary parts of theimpedance spectrum. A parametric model is considered in the case of the realpart, whereas regression model with latent variables is used in the lattercase. Then, a linear regression model using different subsets of extractedfeatures is used fo r the estimation of fuel cell time duration. Theperformances of the proposed approach are evaluated on experimental data set toshow its feasibility. This could lead to interesting perspectives forpredictive maintenance policy of fuel cell.
arxiv-1312-6956 | Joint segmentation of multivariate time series with hidden process regression for human activity recognition |  http://arxiv.org/abs/1312.6956  | author:Faicel Chamroukhi, Samer Mohammed, Dorra Trabelsi, Latifa Oukhellou, Yacine Amirat category:stat.ML cs.LG published:2013-12-25 summary:The problem of human activity recognition is central for understanding andpredicting the human behavior, in particular in a prospective of assistiveservices to humans, such as health monitoring, well being, security, etc. Thereis therefore a growing need to build accurate models which can take intoaccount the variability of the human activities over time (dynamic models)rather than static ones which can have some limitations in such a dynamiccontext. In this paper, the problem of activity recognition is analyzed throughthe segmentation of the multidimensional time series of the acceleration datameasured in the 3-d space using body-worn accelerometers. The proposed modelfor automatic temporal segmentation is a specific statistical latent processmodel which assumes that the observed acceleration sequence is governed bysequence of hidden (unobserved) activities. More specifically, the proposedapproach is based on a specific multiple regression model incorporating ahidden discrete logistic process which governs the switching from one activityto another over time. The model is learned in an unsupervised context bymaximizing the observed-data log-likelihood via a dedicatedexpectation-maximization (EM) algorithm. We applied it on a real-worldautomatic human activity recognition problem and its performance was assessedby performing comparisons with alternative approaches, including well-knownsupervised static classifiers and the standard hidden Markov model (HMM). Theobtained results are very encouraging and show that the proposed approach isquite competitive even it works in an entirely unsupervised way and does notrequires a feature extraction preprocessing step.
arxiv-1312-6962 | Subjectivity Classification using Machine Learning Techniques for Mining Feature-Opinion Pairs from Web Opinion Sources |  http://arxiv.org/abs/1312.6962  | author:Ahmad Kamal category:cs.IR cs.CL cs.LG published:2013-12-25 summary:Due to flourish of the Web 2.0, web opinion sources are rapidly emergingcontaining precious information useful for both customers and manufactures.Recently, feature based opinion mining techniques are gaining momentum in whichcustomer reviews are processed automatically for mining product features anduser opinions expressed over them. However, customer reviews may contain bothopinionated and factual sentences. Distillations of factual contents improvemining performance by preventing noisy and irrelevant extraction. In thispaper, combination of both supervised machine learning and rule-basedapproaches are proposed for mining feasible feature-opinion pairs fromsubjective review sentences. In the first phase of the proposed approach, asupervised machine learning technique is applied for classifying subjective andobjective sentences from customer reviews. In the next phase, a rule basedmethod is implemented which applies linguistic and semantic analysis of textsto mine feasible feature-opinion pairs from subjective sentences retained afterthe first phase. The effectiveness of the proposed methods is establishedthrough experimentation over customer reviews on different electronic products.
arxiv-1312-7001 | A regression model with a hidden logistic process for feature extraction from time series |  http://arxiv.org/abs/1312.7001  | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:A new approach for feature extraction from time series is proposed in thispaper. This approach consists of a specific regression model incorporating adiscrete hidden logistic process. The model parameters are estimated by themaximum likelihood method performed by a dedicated Expectation Maximization(EM) algorithm. The parameters of the hidden logistic process, in the innerloop of the EM algorithm, are estimated using a multi-class IterativeReweighted Least-Squares (IRLS) algorithm. A piecewise regression algorithm andits iterative variant have also been considered for comparisons. Anexperimental study using simulated and real data reveals good performances ofthe proposed approach.
arxiv-1312-6968 | A hidden process regression model for functional data description. Application to curve discrimination |  http://arxiv.org/abs/1312.6968  | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG stat.ML published:2013-12-25 summary:A new approach for functional data description is proposed in this paper. Itconsists of a regression model with a discrete hidden logistic process which isadapted for modeling curves with abrupt or smooth regime changes. The modelparameters are estimated in a maximum likelihood framework through a dedicatedExpectation Maximization (EM) algorithm. From the proposed generative model, acurve discrimination rule is derived using the Maximum A Posteriori rule. Theproposed model is evaluated using simulated curves and real world curvesacquired during railway switch operations, by performing comparisons with thepiecewise regression approach in terms of curve modeling and classification.
arxiv-1312-6965 | An Unsupervised Approach for Automatic Activity Recognition based on Hidden Markov Model Regression |  http://arxiv.org/abs/1312.6965  | author:Dorra Trabelsi, Samer Mohammed, Faicel Chamroukhi, Latifa Oukhellou, Yacine Amirat category:stat.ML cs.CV cs.LG published:2013-12-25 summary:Using supervised machine learning approaches to recognize human activitiesfrom on-body wearable accelerometers generally requires a large amount oflabelled data. When ground truth information is not available, too expensive,time consuming or difficult to collect, one has to rely on unsupervisedapproaches. This paper presents a new unsupervised approach for human activityrecognition from raw acceleration data measured using inertial wearablesensors. The proposed method is based upon joint segmentation ofmultidimensional time series using a Hidden Markov Model (HMM) in a multipleregression context. The model is learned in an unsupervised framework using theExpectation-Maximization (EM) algorithm where no activity labels are needed.The proposed method takes into account the sequential appearance of the data.It is therefore adapted for the temporal acceleration data to accurately detectthe activities. It allows both segmentation and classification of the humanactivities. Experimental results are provided to demonstrate the efficiency ofthe proposed approach with respect to standard supervised and unsupervisedclassification approaches
arxiv-1312-6995 | Towards Using Unlabeled Data in a Sparse-coding Framework for Human Activity Recognition |  http://arxiv.org/abs/1312.6995  | author:Sourav Bhattacharya, Petteri Nurmi, Nils Hammerla, Thomas Plötz category:cs.LG cs.AI stat.ML published:2013-12-25 summary:We propose a sparse-coding framework for activity recognition in ubiquitousand mobile computing that alleviates two fundamental problems of currentsupervised learning approaches. (i) It automatically derives a compact, sparseand meaningful feature representation of sensor data that does not rely onprior expert knowledge and generalizes extremely well across domain boundaries.(ii) It exploits unlabeled sample data for bootstrapping effective activityrecognizers, i.e., substantially reduces the amount of ground truth annotationrequired for model estimation. Such unlabeled data is trivial to obtain, e.g.,through contemporary smartphones carried by users as they go about theireveryday activities. Based on the self-taught learning paradigm we automatically derive anover-complete set of basis vectors from unlabeled data that captures inherentpatterns present within activity data. Through projecting raw sensor data ontothe feature space defined by such over-complete sets of basis vectors effectivefeature extraction is pursued. Given these learned feature representations,classification backends are then trained using small amounts of labeledtraining data. We study the new approach in detail using two datasets which differ in termsof the recognition tasks and sensor modalities. Primarily we focus ontransportation mode analysis task, a popular task in mobile-phone basedsensing. The sparse-coding framework significantly outperforms thestate-of-the-art in supervised learning approaches. Furthermore, we demonstratethe great practical potential of the new approach by successfully evaluatingits generalization capabilities across both domain and sensor modalities byconsidering the popular Opportunity dataset. Our feature learning approachoutperforms state-of-the-art approaches to analyzing activities in dailyliving.
arxiv-1312-6994 | A regression model with a hidden logistic process for signal parametrization |  http://arxiv.org/abs/1312.6994  | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG stat.ML published:2013-12-25 summary:A new approach for signal parametrization, which consists of a specificregression model incorporating a discrete hidden logistic process, is proposed.The model parameters are estimated by the maximum likelihood method performedby a dedicated Expectation Maximization (EM) algorithm. The parameters of thehidden logistic process, in the inner loop of the EM algorithm, are estimatedusing a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm. Anexperimental study using simulated and real data reveals good performances ofthe proposed approach.
arxiv-1312-6966 | Model-based functional mixture discriminant analysis with hidden process regression for curve classification |  http://arxiv.org/abs/1312.6966  | author:Faicel Chamroukhi, Hervé Glotin, Allou Samé category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:In this paper, we study the modeling and the classification of functionaldata presenting regime changes over time. We propose a new model-basedfunctional mixture discriminant analysis approach based on a specific hiddenprocess regression model that governs the regime changes over time. Ourapproach is particularly adapted to handle the problem of complex-shapedclasses of curves, where each class is potentially composed of severalsub-classes, and to deal with the regime changes within each homogeneoussub-class. The proposed model explicitly integrates the heterogeneity of eachclass of curves via a mixture model formulation, and the regime changes withineach sub-class through a hidden logistic process. Each class of complex-shapedcurves is modeled by a finite number of homogeneous clusters, each of thembeing decomposed into several regimes. The model parameters of each class arelearned by maximizing the observed-data log-likelihood by using a dedicatedexpectation-maximization (EM) algorithm. Comparisons are performed withalternative curve classification approaches, including functional lineardiscriminant analysis and functional mixture discriminant analysis withpolynomial regression mixtures and spline regression mixtures. Results obtainedon simulated data and real data show that the proposed approach outperforms thealternative approaches in terms of discrimination, and significantly improvesthe curves approximation.
arxiv-1312-6969 | Time series modeling by a regression approach based on a latent process |  http://arxiv.org/abs/1312.6969  | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:Time series are used in many domains including finance, engineering,economics and bioinformatics generally to represent the change of a measurementover time. Modeling techniques may then be used to give a syntheticrepresentation of such data. A new approach for time series modeling isproposed in this paper. It consists of a regression model incorporating adiscrete hidden logistic process allowing for activating smoothly or abruptlydifferent polynomial regression models. The model parameters are estimated bythe maximum likelihood method performed by a dedicated Expectation Maximization(EM) algorithm. The M step of the EM algorithm uses a multi-class IterativeReweighted Least-Squares (IRLS) algorithm to estimate the hidden processparameters. To evaluate the proposed approach, an experimental study onsimulated data and real world data was performed using two alternativeapproaches: a heteroskedastic piecewise regression model using a globaloptimization algorithm based on dynamic programming, and a Hidden MarkovRegression Model whose parameters are estimated by the Baum-Welch algorithm.Finally, in the context of the remote monitoring of components of the Frenchrailway infrastructure, and more particularly the switch mechanism, theproposed approach has been applied to modeling and classifying time seriesrepresenting the condition measurements acquired during switch operations.
arxiv-1312-6978 | Modèle à processus latent et algorithme EM pour la régression non linéaire |  http://arxiv.org/abs/1312.6978  | author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:math.ST cs.LG stat.ME stat.ML stat.TH published:2013-12-25 summary:A non linear regression approach which consists of a specific regressionmodel incorporating a latent process, allowing various polynomial regressionmodels to be activated preferentially and smoothly, is introduced in thispaper. The model parameters are estimated by maximum likelihood performed via adedicated expecation-maximization (EM) algorithm. An experimental study usingsimulated and real data sets reveals good performances of the proposedapproach.
arxiv-1312-6967 | Model-based clustering and segmentation of time series with changes in regime |  http://arxiv.org/abs/1312.6967  | author:Allou Samé, Faicel Chamroukhi, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH published:2013-12-25 summary:Mixture model-based clustering, usually applied to multidimensional data, hasbecome a popular approach in many data analysis problems, both for its goodstatistical properties and for the simplicity of implementation of theExpectation-Maximization (EM) algorithm. Within the context of a railwayapplication, this paper introduces a novel mixture model for dealing with timeseries that are subject to changes in regime. The proposed approach consists inmodeling each cluster by a regression model in which the polynomialcoefficients vary according to a discrete hidden process. In particular, thisapproach makes use of logistic functions to model the (smooth or abrupt)transitions between regimes. The model parameters are estimated by the maximumlikelihood method solved by an Expectation-Maximization algorithm. The proposedapproach can also be regarded as a clustering approach which operates byfinding groups of time series having common changes in regime. In addition toproviding a time series partition, it therefore provides a time seriessegmentation. The problem of selecting the optimal numbers of clusters andsegments is solved by means of the Bayesian Information Criterion (BIC). Theproposed approach is shown to be efficient using a variety of simulated timeseries and real-world time series of electrical power consumption from railswitching operations.
arxiv-1312-6724 | Local algorithms for interactive clustering |  http://arxiv.org/abs/1312.6724  | author:Pranjal Awasthi, Maria-Florina Balcan, Konstantin Voevodski category:cs.DS cs.LG published:2013-12-24 summary:We study the design of interactive clustering algorithms for data setssatisfying natural stability assumptions. Our algorithms start with any initialclustering and only make local changes in each step; both are desirablefeatures in many applications. We show that in this constrained setting one canstill design provably efficient algorithms that produce accurate clusterings.We also show that our algorithms perform well on real-world data.
arxiv-1401-3322 | A Subband-Based SVM Front-End for Robust ASR |  http://arxiv.org/abs/1401.3322  | author:Jibran Yousafzai, Zoran Cvetkovic, Peter Sollich, Matthew Ager category:cs.CL cs.LG cs.SD published:2013-12-24 summary:This work proposes a novel support vector machine (SVM) based robustautomatic speech recognition (ASR) front-end that operates on an ensemble ofthe subband components of high-dimensional acoustic waveforms. The key issuesof selecting the appropriate SVM kernels for classification in frequencysubbands and the combination of individual subband classifiers using ensemblemethods are addressed. The proposed front-end is compared with state-of-the-artASR front-ends in terms of robustness to additive noise and linear filtering.Experiments performed on the TIMIT phoneme classification task demonstrate thebenefits of the proposed subband based SVM front-end: it outperforms thestandard cepstral front-end in the presence of noise and linear filtering forsignal-to-noise ratio (SNR) below 12-dB. A combination of the proposedfront-end with a conventional front-end such as MFCC yields furtherimprovements over the individual front ends across the full range of noiselevels.
arxiv-1312-6782 | IVSS Integration of Color Feature Extraction Techniques for Intelligent Video Search Systems |  http://arxiv.org/abs/1312.6782  | author:Avinash N Bhute, B. B. Meshram category:cs.CV cs.IR cs.MM published:2013-12-24 summary:As large amount of visual Information is available on web in form of images,graphics, animations and videos, so it is important in internet era to have aneffective video search system. As there are number of video search engine(blinkx, Videosurf, Google, YouTube, etc.) which search for relevant videosbased on user keyword or term, But very less commercial video search engine areavailable which search videos based on visual image/clip/video. In this paperwe are recommending a system that will search for relevant video using colorfeature of video in response of user Query.
arxiv-1401-6118 | Comparative study of Authorship Identification Techniques for Cyber Forensics Analysis |  http://arxiv.org/abs/1401.6118  | author:Smita Nirkhi, R. V. Dharaskar category:cs.CY cs.CR cs.IR cs.LG published:2013-12-24 summary:Authorship Identification techniques are used to identify the mostappropriate author from group of potential suspects of online messages and findevidences to support the conclusion. Cybercriminals make misuse of onlinecommunication for sending blackmail or a spam email and then attempt to hidetheir true identities to void detection.Authorship Identification of onlinemessages is the contemporary research issue for identity tracing in cyberforensics. This is highly interdisciplinary area as it takes advantage ofmachine learning, information retrieval, and natural language processing. Inthis paper, a study of recent techniques and automated approaches toattributing authorship of online messages is presented. The focus of thisreview study is to summarize all existing authorship identification techniquesused in literature to identify authors of online messages. Also it discussesevaluation criteria and parameters for authorship attribution studies and listopen questions that will attract future work in this area.
arxiv-1312-6802 | Suffix Stripping Problem as an Optimization Problem |  http://arxiv.org/abs/1312.6802  | author:B. P. Pande, Pawan Tamta, H. S. Dhami category:cs.IR cs.CL published:2013-12-24 summary:Stemming or suffix stripping, an important part of the modern InformationRetrieval systems, is to find the root word (stem) out of a given cluster ofwords. Existing algorithms targeting this problem have been developed in ahaphazard manner. In this work, we model this problem as an optimizationproblem. An Integer Program is being developed to overcome the shortcomings ofthe existing approaches. The sample results of the proposed method are alsobeing compared with an established technique in the field for English language.An AMPL code for the same IP has also been given.
arxiv-1312-6807 | Iterative Nearest Neighborhood Oversampling in Semisupervised Learning from Imbalanced Data |  http://arxiv.org/abs/1312.6807  | author:Fengqi Li, Chuang Yu, Nanhai Yang, Feng Xia, Guangming Li, Fatemeh Kaveh-Yazdy category:cs.LG 68P20 H.3.3 published:2013-12-24 summary:Transductive graph-based semi-supervised learning methods usually build anundirected graph utilizing both labeled and unlabeled samples as vertices.Those methods propagate label information of labeled samples to neighborsthrough their edges in order to get the predicted labels of unlabeled samples.Most popular semi-supervised learning approaches are sensitive to initial labeldistribution happened in imbalanced labeled datasets. The class boundary willbe severely skewed by the majority classes in an imbalanced classification. Inthis paper, we proposed a simple and effective approach to alleviate theunfavorable influence of imbalance problem by iteratively selecting a fewunlabeled samples and adding them into the minority classes to form a balancedlabeled dataset for the learning methods afterwards. The experiments on UCIdatasets and MNIST handwritten digits dataset showed that the proposed approachoutperforms other existing state-of-art methods.
arxiv-1312-6820 | A Fast Greedy Algorithm for Generalized Column Subset Selection |  http://arxiv.org/abs/1312.6820  | author:Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel category:cs.DS cs.LG stat.ML published:2013-12-24 summary:This paper defines a generalized column subset selection problem which isconcerned with the selection of a few columns from a source matrix A that bestapproximate the span of a target matrix B. The paper then proposes a fastgreedy algorithm for solving this problem and draws connections to differentproblems that can be efficiently solved using the proposed algorithm.
arxiv-1312-6849 | Speech Recognition Front End Without Information Loss |  http://arxiv.org/abs/1312.6849  | author:Matthew Ager, Zoran Cvetkovic, Peter Sollich category:cs.CL cs.CV cs.LG published:2013-12-24 summary:Speech representation and modelling in high-dimensional spaces of acousticwaveforms, or a linear transformation thereof, is investigated with the aim ofimproving the robustness of automatic speech recognition to additive noise. Themotivation behind this approach is twofold: (i) the information in acousticwaveforms that is usually removed in the process of extracting low-dimensionalfeatures might aid robust recognition by virtue of structured redundancyanalogous to channel coding, (ii) linear feature domains allow for exact noiseadaptation, as opposed to representations that involve non-linear processingwhich makes noise adaptation challenging. Thus, we develop a generativeframework for phoneme modelling in high-dimensional linear feature domains, anduse it in phoneme classification and recognition tasks. Results show thatclassification and recognition in this framework perform better than analogousPLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensionaland MFCC features at the likelihood level performs uniformly better than eitherof the individual representations across all noise levels.
arxiv-1312-6885 | Deep learning for class-generic object detection |  http://arxiv.org/abs/1312.6885  | author:Brody Huval, Adam Coates, Andrew Ng category:cs.CV cs.LG cs.NE published:2013-12-24 summary:We investigate the use of deep neural networks for the novel task of classgeneric object detection. We show that neural networks originally designed forimage recognition can be trained to detect objects within images, regardless oftheir class, including objects for which no bounding box labels have beenprovided. In addition, we show that bounding box labels yield a 1% performanceincrease on the ImageNet recognition challenge.
arxiv-1312-6838 | Greedy Column Subset Selection for Large-scale Data Sets |  http://arxiv.org/abs/1312.6838  | author:Ahmed K. Farahat, Ahmed Elgohary, Ali Ghodsi, Mohamed S. Kamel category:cs.DS cs.LG published:2013-12-24 summary:In today's information systems, the availability of massive amounts of datanecessitates the development of fast and accurate algorithms to summarize thesedata and represent them in a succinct format. One crucial problem in big dataanalytics is the selection of representative instances from large andmassively-distributed data, which is formally known as the Column SubsetSelection (CSS) problem. The solution to this problem enables data analysts tounderstand the insights of the data and explore its hidden structure. Theselected instances can also be used for data preprocessing tasks such aslearning a low-dimensional embedding of the data points or computing a low-rankapproximation of the corresponding matrix. This paper presents a fast andaccurate greedy algorithm for large-scale column subset selection. Thealgorithm minimizes an objective function which measures the reconstructionerror of the data matrix based on the subset of selected columns. The paperfirst presents a centralized greedy algorithm for column subset selection whichdepends on a novel recursive formula for calculating the reconstruction errorof the data matrix. The paper then presents a MapReduce algorithm which selectsa few representative columns from a matrix whose columns are massivelydistributed across several commodity machines. The algorithm first learns aconcise representation of all columns using random projection, and it thensolves a generalized column subset selection problem at each machine in which asubset of columns are selected from the sub-matrix on that machine such thatthe reconstruction error of the concise representation is minimized. The paperdemonstrates the effectiveness and efficiency of the proposed algorithm throughan empirical evaluation on benchmark data sets.
arxiv-1312-6826 | 3D Interest Point Detection via Discriminative Learning |  http://arxiv.org/abs/1312.6826  | author:Leizer Teran, Philippos Mordohai category:cs.CV published:2013-12-24 summary:The task of detecting the interest points in 3D meshes has typically beenhandled by geometric methods. These methods, while greatly describing humanpreference, can be ill-equipped for handling the variety and subjectivity inhuman responses. Different tasks have different requirements for interest pointdetection; some tasks may necessitate high precision while other tasks mayrequire high recall. Sometimes points with high curvature may be desirable,while in other cases high curvature may be an indication of noise. Geometricmethods lack the required flexibility to adapt to such changes. As aconsequence, interest point detection seems to be well suited for machinelearning methods that can be trained to match the criteria applied on theannotated training data. In this paper, we formulate interest point detectionas a supervised binary classification problem using a random forest as ourclassifier. Among other challenges, we are faced with an imbalanced learningproblem due to the substantial difference in the priors between interest andnon-interest points. We address this by re-sampling the training set. Wevalidate the accuracy of our method and compare our results to those of fivestate of the art methods on a new, standard benchmark.
arxiv-1312-6813 | New explicit thresholding/shrinkage formulas for one class of regularization problems with overlapping group sparsity and their applications |  http://arxiv.org/abs/1312.6813  | author:Gang Liu, Ting-Zhu Huang, Xiao-Guang Lv, Jun Liu category:math.NA cs.CV published:2013-12-24 summary:The least-square regression problems or inverse problems have been widelystudied in many fields such as compressive sensing, signal processing, andimage processing. To solve this kind of ill-posed problems, a regularizationterm (i.e., regularizer) should be introduced, under the assumption that thesolutions have some specific properties, such as sparsity and group sparsity.Widely used regularizers include the $\ell_1$ norm, total variation (TV)semi-norm, and so on. Recently, a new regularization term with overlapping group sparsity has beenconsidered. Majorization minimization iteration method or variable duplicationmethods are often applied to solve them. However, there have been no directmethods for solve the relevant problems because of the difficulty ofoverlapping. In this paper, we proposed new explicit shrinkage formulas for oneclass of these relevant problems, whose regularization terms have translationinvariant overlapping groups. Moreover, we apply our results in TV deblurringand denoising with overlapping group sparsity. We use alternating directionmethod of multipliers to iterate solve it. Numerical results also verify thevalidity and effectiveness of our new explicit shrinkage formulas.
arxiv-1312-6599 | Image Processing based Systems and Techniques for the Recognition of Ancient and Modern Coins |  http://arxiv.org/abs/1312.6599  | author:Shatrughan Modi, Dr. Seema Bawa category:cs.CV cs.AI published:2013-12-23 summary:Coins are frequently used in everyday life at various places like in banks,grocery stores, supermarkets, automated weighing machines, vending machinesetc. So, there is a basic need to automate the counting and sorting of coins.For this machines need to recognize the coins very fast and accurately, asfurther transaction processing depends on this recognition. Three types ofsystems are available in the market: Mechanical method based systems,Electromagnetic method based systems and Image processing based systems. Thispaper presents an overview of available systems and techniques based on imageprocessing to recognize ancient and modern coins.
arxiv-1312-6661 | Rapid and deterministic estimation of probability densities using scale-free field theories |  http://arxiv.org/abs/1312.6661  | author:Justin B. Kinney category:cs.LG math.ST q-bio.QM stat.ML stat.TH published:2013-12-23 summary:The question of how best to estimate a continuous probability density fromfinite data is an intriguing open problem at the interface of statistics andphysics. Previous work has argued that this problem can be addressed in anatural way using methods from statistical field theory. Here I describe newresults that allow this field-theoretic approach to be rapidly anddeterministically computed in low dimensions, making it practical for use inday-to-day data analysis. Importantly, this approach does not impose aprivileged length scale for smoothness of the inferred probability density, butrather learns a natural length scale from the data due to the tradeoff betweengoodness-of-fit and an Occam factor. Open source software implementing thismethod in one and two dimensions is provided.
arxiv-1312-6461 | Nonparametric Weight Initialization of Neural Networks via Integral Representation |  http://arxiv.org/abs/1312.6461  | author:Sho Sonoda, Noboru Murata category:cs.LG cs.NE published:2013-12-23 summary:A new initialization method for hidden parameters in a neural network isproposed. Derived from the integral representation of the neural network, anonparametric probability distribution of hidden parameters is introduced. Inthis proposal, hidden parameters are initialized by samples drawn from thisdistribution, and output parameters are fitted by ordinary linear regression.Numerical experiments show that backpropagation with proposed initializationconverges faster than uniformly random initialization. Also it is shown thatthe proposed method achieves enough accuracy by itself without backpropagationin some cases.
arxiv-1312-6607 | Using Latent Binary Variables for Online Reconstruction of Large Scale Systems |  http://arxiv.org/abs/1312.6607  | author:Victorin Martin, Jean-Marc Lasgouttes, Cyril Furtlehner category:math.PR cs.LG stat.ML published:2013-12-23 summary:We propose a probabilistic graphical model realizing a minimal encoding ofreal variables dependencies based on possibly incomplete observation and anempirical cumulative distribution function per variable. The target applicationis a large scale partially observed system, like e.g. a traffic network, wherea small proportion of real valued variables are observed, and the othervariables have to be predicted. Our design objective is therefore to have goodscalability in a real-time setting. Instead of attempting to encode thedependencies of the system directly in the description space, we propose a wayto encode them in a latent space of binary variables, reflecting a roughperception of the observable (congested/non-congested for a traffic road). Themethod relies in part on message passing algorithms, i.e. belief propagation,but the core of the work concerns the definition of meaningful latent variablesassociated to the variables of interest and their pairwise dependencies.Numerical experiments demonstrate the applicability of the method in practice.
arxiv-1312-6597 | Co-Multistage of Multiple Classifiers for Imbalanced Multiclass Learning |  http://arxiv.org/abs/1312.6597  | author:Luis Marujo, Anatole Gershman, Jaime Carbonell, David Martins de Matos, João P. Neto category:cs.LG cs.IR published:2013-12-23 summary:In this work, we propose two stochastic architectural models (CMC and CMC-M)with two layers of classifiers applicable to datasets with one and multipleskewed classes. This distinction becomes important when the datasets have alarge number of classes. Therefore, we present a novel solution to imbalancedmulticlass learning with several skewed majority classes, which improvesminority classes identification. This fact is particularly important for textclassification tasks, such as event detection. Our models combined withpre-processing sampling techniques improved the classification results on sixwell-known datasets. Finally, we have also introduced a new metric SG-Mean toovercome the multiplication by zero limitation of G-Mean.
arxiv-1312-6506 | Top Down Approach to Multiple Plane Detection |  http://arxiv.org/abs/1312.6506  | author:Prateek Singhal, Aditya Deshpande, N Dinesh Reddy, K Madhava Krishna category:cs.CV published:2013-12-23 summary:Detecting multiple planes in images is a challenging problem, but one withmany applications. Recent work such as J-Linkage and Ordered Residual Kernelshave focussed on developing a domain independent approach to detect multiplestructures. These multiple structure detection methods are then used forestimating multiple homographies given feature matches between two images.Features participating in the multiple homographies detected, provide us themultiple scene planes. We show that these methods provide locally optimalresults and fail to merge detected planar patches to the true scene planes.These methods use only residues obtained on applying homography of one plane toanother as cue for merging. In this paper, we develop additional cues such aslocal consistency of planes, local normals, texture etc. to perform betterclassification and merging . We formulate the classification as an MRF problemand use TRWS message passing algorithm to solve non metric energy terms andcomplex sparse graph structure. We show results on challenging dataset commonin robotics navigation scenarios where our method shows accuracy of more than85 percent on average while being close or same as the actual number of sceneplanes.
arxiv-1312-6615 | Automated Coin Recognition System using ANN |  http://arxiv.org/abs/1312.6615  | author:Shatrughan Modi, Dr. Seema Bawa category:cs.CV cs.AI published:2013-12-23 summary:Coins are integral part of our day to day life. We use coins everywhere likegrocery store, banks, buses, trains etc. So it becomes a basic need that coinscan be sorted and counted automatically. For this it is necessary that coinscan be recognized automatically. In this paper we have developed an ANN(Artificial Neural Network) based Automated Coin Recognition System for therecognition of Indian Coins of denomination Rs. 1, 2, 5 and 10 with rotationinvariance. We have taken images from both sides of coin. So this system iscapable of recognizing coins from both sides. Features are extracted fromimages using techniques of Hough Transformation, Pattern Averaging etc. Then,the extracted features are passed as input to a trained Neural Network. 97.74%recognition rate has been achieved during the experiments i.e. only 2.26% missrecognition, which is quite encouraging.
arxiv-1312-6712 | Invariant Factorization Of Time-Series |  http://arxiv.org/abs/1312.6712  | author:Josif Grabocka, Lars Schmidt-Thieme category:cs.LG published:2013-12-23 summary:Time-series classification is an important domain of machine learning and aplethora of methods have been developed for the task. In comparison to existingapproaches, this study presents a novel method which decomposes a time-seriesdataset into latent patterns and membership weights of local segments to thosepatterns. The process is formalized as a constrained objective function and atailored stochastic coordinate descent optimization is applied. The time-seriesare projected to a new feature representation consisting of the sums of themembership weights, which captures frequencies of local patterns. Features fromvarious sliding window sizes are concatenated in order to encapsulate theinteraction of patterns from different sizes. Finally, a large-scaleexperimental comparison against 6 state of the art baselines and 43 real lifedatasets is conducted. The proposed method outperforms all the baselines withstatistically significant margins in terms of prediction accuracy.
arxiv-1312-6652 | Rounding Sum-of-Squares Relaxations |  http://arxiv.org/abs/1312.6652  | author:Boaz Barak, Jonathan Kelner, David Steurer category:cs.DS cs.LG quant-ph published:2013-12-23 summary:We present a general approach to rounding semidefinite programmingrelaxations obtained by the Sum-of-Squares method (Lasserre hierarchy). Ourapproach is based on using the connection between these relaxations and theSum-of-Squares proof system to transform a *combining algorithm* -- analgorithm that maps a distribution over solutions into a (possibly weaker)solution -- into a *rounding algorithm* that maps a solution of the relaxationto a solution of the original problem. Using this approach, we obtain algorithms that yield improved results fornatural variants of three well-known problems: 1) We give a quasipolynomial-time algorithm that approximates the maximum ofa low degree multivariate polynomial with non-negative coefficients over theEuclidean unit sphere. Beyond being of interest in its own right, this isrelated to an open question in quantum information theory, and our techniqueshave already led to improved results in this area (Brand\~{a}o and Harrow, STOC'13). 2) We give a polynomial-time algorithm that, given a d dimensional subspaceof R^n that (almost) contains the characteristic function of a set of size n/k,finds a vector $v$ in the subspace satisfying $v_4^4 > c(k/d^{1/3}) v_2^2$,where $v_p = (E_i v_i^p)^{1/p}$. Aside from being a natural relaxation, thisis also motivated by a connection to the Small Set Expansion problem shown byBarak et al. (STOC 2012) and our results yield a certain improvement for thatproblem. 3) We use this notion of L_4 vs. L_2 sparsity to obtain a polynomial-timealgorithm with substantially improved guarantees for recovering a planted$\mu$-sparse vector v in a random d-dimensional subspace of R^n. If v has mu nnonzero coordinates, we can recover it with high probability whenever $\mu <O(\min(1,n/d^2))$, improving for $d < n^{2/3}$ prior methods whichintrinsically required $\mu < O(1/\sqrt(d))$.
arxiv-1312-6609 | A comprehensive review of firefly algorithms |  http://arxiv.org/abs/1312.6609  | author:Iztok Fister, Iztok Fister Jr., Xin-She Yang, Janez Brest category:cs.NE published:2013-12-23 summary:The firefly algorithm has become an increasingly important tool of SwarmIntelligence that has been applied in almost all areas of optimization, as wellas engineering practice. Many problems from various areas have beensuccessfully solved using the firefly algorithm and its variants. In order touse the algorithm to solve diverse problems, the original firefly algorithmneeds to be modified or hybridized. This paper carries out a comprehensivereview of this living and evolving discipline of Swarm Intelligence, in orderto show that the firefly algorithm could be applied to every problem arising inpractice. On the other hand, it encourages new researchers and algorithmdevelopers to use this simple and yet very efficient algorithm for problemsolving. It often guarantees that the obtained results will meet theexpectations.
arxiv-1312-6430 | Growing Regression Forests by Classification: Applications to Object Pose Estimation |  http://arxiv.org/abs/1312.6430  | author:Kota Hara, Rama Chellappa category:cs.CV cs.LG stat.ML published:2013-12-22 summary:In this work, we propose a novel node splitting method for regression treesand incorporate it into the regression forest framework. Unlike traditionalbinary splitting, where the splitting rule is selected from a predefined set ofbinary splitting rules via trial-and-error, the proposed node splitting methodfirst finds clusters of the training data which at least locally minimize theempirical loss without considering the input space. Then splitting rules whichpreserve the found clusters as much as possible are determined by casting theproblem into a classification problem. Consequently, our new node splittingmethod enjoys more freedom in choosing the splitting rules, resulting in moreefficient tree structures. In addition to the Euclidean target space, wepresent a variant which can naturally deal with a circular target space by theproper use of circular statistics. We apply the regression forest employing ournode splitting to head pose estimation (Euclidean target space) and cardirection estimation (circular target space) and demonstrate that the proposedmethod significantly outperforms state-of-the-art methods (38.5% and 22.5%error reduction respectively).
arxiv-1312-6410 | A Survey on Eye-Gaze Tracking Techniques |  http://arxiv.org/abs/1312.6410  | author:H. R. Chennamma, Xiaohui Yuan category:cs.CV published:2013-12-22 summary:Study of eye-movement is being employed in Human Computer Interaction (HCI)research. Eye - gaze tracking is one of the most challenging problems in thearea of computer vision. The goal of this paper is to present a review oflatest research in this continued growth of remote eye-gaze tracking. Thisoverview includes the basic definitions and terminologies, recent advances inthe field and finally the need of future development in the field.
arxiv-1312-6370 | An Efficient Edge Detection Technique by Two Dimensional Rectangular Cellular Automata |  http://arxiv.org/abs/1312.6370  | author:Jahangir Mohammed, Deepak Ranjan Nayak category:cs.CV published:2013-12-22 summary:This paper proposes a new pattern of two dimensional cellular automata linearrules that are used for efficient edge detection of an image. Since cellularautomata is inherently parallel in nature, it has produced desired outputwithin a unit time interval. We have observed four linear rules among 512 totallinear rules of a rectangular cellular automata in adiabatic or reflexiveboundary condition that produces an optimal result. These four rules aredirectly applied once to the images and produced edge detected output. Wecompare our results with the existing edge detection algorithms and found thatour results shows better edge detection with an enhancement of edges.
arxiv-1312-6199 | Intriguing properties of neural networks |  http://arxiv.org/abs/1312.6199  | author:Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus category:cs.CV cs.LG cs.NE published:2013-12-21 summary:Deep neural networks are highly expressive models that have recently achievedstate of the art performance on speech and visual recognition tasks. Whiletheir expressiveness is the reason they succeed, it also causes them to learnuninterpretable solutions that could have counter-intuitive properties. In thispaper we report two such properties. First, we find that there is no distinction between individual high levelunits and random linear combinations of high level units, according to variousmethods of unit analysis. It suggests that it is the space, rather than theindividual units, that contains of the semantic information in the high layersof neural networks. Second, we find that deep neural networks learn input-output mappings thatare fairly discontinuous to a significant extend. We can cause the network tomisclassify an image by applying a certain imperceptible perturbation, which isfound by maximizing the network's prediction error. In addition, the specificnature of these perturbations is not a random artifact of learning: the sameperturbation can cause a different network, that was trained on a differentsubset of the dataset, to misclassify the same input.
arxiv-1312-6317 | Outlier robust system identification: a Bayesian kernel-based approach |  http://arxiv.org/abs/1312.6317  | author:Giulio Bottegal, Aleksandr Y. Aravkin, Hakan Hjalmarsson, Gianluigi Pillonetto category:stat.ML 47N30, 65K10 published:2013-12-21 summary:In this paper, we propose an outlier-robust regularized kernel-based methodfor linear system identification. The unknown impulse response is modeled as azero-mean Gaussian process whose covariance (kernel) is given by the recentlyproposed stable spline kernel, which encodes information on regularity andexponential stability. To build robustness to outliers, we model themeasurement noise as realizations of independent Laplacian random variables.The identification problem is cast in a Bayesian framework, and solved by a newMarkov Chain Monte Carlo (MCMC) scheme. In particular, exploiting therepresentation of the Laplacian random variables as scale mixtures ofGaussians, we design a Gibbs sampler which quickly converges to the targetdistribution. Numerical simulations show a substantial improvement in theaccuracy of the estimates over state-of-the-art kernel-based methods.
arxiv-1312-6229 | OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks |  http://arxiv.org/abs/1312.6229  | author:Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun category:cs.CV published:2013-12-21 summary:We present an integrated framework for using Convolutional Networks forclassification, localization and detection. We show how a multiscale andsliding window approach can be efficiently implemented within a ConvNet. Wealso introduce a novel deep learning approach to localization by learning topredict object boundaries. Bounding boxes are then accumulated rather thansuppressed in order to increase detection confidence. We show that differenttasks can be learned simultaneously using a single shared network. Thisintegrated framework is the winner of the localization task of the ImageNetLarge Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained verycompetitive results for the detection and classifications tasks. Inpost-competition work, we establish a new state of the art for the detectiontask. Finally, we release a feature extractor from our best model calledOverFeat.
arxiv-1312-6211 | An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks |  http://arxiv.org/abs/1312.6211  | author:Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2013-12-21 summary:Catastrophic forgetting is a problem faced by many machine learning modelsand algorithms. When trained on one task, then trained on a second task, manymachine learning models "forget" how to perform the first task. This is widelybelieved to be a serious problem for neural networks. Here, we investigate theextent to which the catastrophic forgetting problem occurs for modern neuralnetworks, comparing both established and recent gradient-based trainingalgorithms and activation functions. We also examine the effect of therelationship between the first task and the second task on catastrophicforgetting. We find that it is always best to train using the dropoutalgorithm--the dropout algorithm is consistently best at adapting to the newtask, remembering the old task, and has the best tradeoff curve between thesetwo extremes. We find that different tasks and relationships between tasksresult in very different rankings of activation function performance. Thissuggests the choice of activation function should always be cross-validated.
arxiv-1312-6282 | Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning |  http://arxiv.org/abs/1312.6282  | author:François Denis, Mattias Gybels, Amaury Habrard category:cs.LG published:2013-12-21 summary:Learning probabilistic models over strings is an important issue for manyapplications. Spectral methods propose elegant solutions to the problem ofinferring weighted automata from finite samples of variable-length stringsdrawn from an unknown target distribution. These methods rely on a singularvalue decomposition of a matrix $H_S$, called the Hankel matrix, that recordsthe frequencies of (some of) the observed strings. The accuracy of the learneddistribution depends both on the quantity of information embedded in $H_S$ andon the distance between $H_S$ and its mean $H_r$. Existing concentration boundsseem to indicate that the concentration over $H_r$ gets looser with the size of$H_r$, suggesting to make a trade-off between the quantity of used informationand the size of $H_r$. We propose new dimension-free concentration bounds forseveral variants of Hankel matrices. Experiments demonstrate that these boundsare tight and that they significantly improve existing bounds. These resultssuggest that the concentration rate of the Hankel matrix around its mean doesnot constitute an argument for limiting its size.
arxiv-1312-6273 | Parallel architectures for fuzzy triadic similarity learning |  http://arxiv.org/abs/1312.6273  | author:Sonia Alouane-Ksouri, Minyar Sassi-Hidri, Kamel Barkaoui category:cs.DC cs.LG stat.ML published:2013-12-21 summary:In a context of document co-clustering, we define a new similarity measurewhich iteratively computes similarity while combining fuzzy sets in athree-partite graph. The fuzzy triadic similarity (FT-Sim) model can deal withuncertainty offers by the fuzzy sets. Moreover, with the development of the Weband the high availability of storage spaces, more and more documents becomeaccessible. Documents can be provided from multiple sites and make similaritycomputation an expensive processing. This problem motivated us to use parallelcomputing. In this paper, we introduce parallel architectures which are able totreat large and multi-source data sets by a sequential, a merging or asplitting-based process. Then, we proceed to a local and a central (or global)computing using the basic FT-Sim measure. The idea behind these architecturesis to reduce both time and space complexities thanks to parallel computation.
arxiv-1312-6219 | Extracting Region of Interest for Palm Print Authentication |  http://arxiv.org/abs/1312.6219  | author:Kasturika B. Ray category:cs.CV published:2013-12-21 summary:Biometrics authentication is an effective method for automaticallyrecognizing individuals. The authentication consists of an enrollment phase andan identification or verification phase. In the stages of enrollment known(training) samples after the pre-processing stage are used for suitable featureextraction to generate the template database. In the verification stage, thetest sample is similarly pre processed and subjected to feature extractionmodules, and then it is matched with the training feature templates to decidewhether it is a genuine or not. This paper presents use of a region of interest(ROI) for palm print technology. First some of the existing methods for palmprint identification have been introduced. Then focus has been given onextraction of a suitable smaller region from the acquired palm print to improvethe identification method accuracy. Several existing work in the topic ofregion extraction have been examined. Subsequently, a simple and originalmethod has then proposed for locating the ROI that can be effectively used forpalm print analysis. The ROI extracted using this new technique is suitable fordifferent types of processing as it creates a rectangular or square area aroundthe center of activity represented by the lines, wrinkles and ridges of thepalm print. The effectiveness of the ROI approach has been tested byintegrating it with a texture based identification / authentication systemproposed earlier. The improvement has been shown by comparing theidentification accuracy rate before and after the ROI pre-processing.
arxiv-1312-6208 | Total variation with overlapping group sparsity for image deblurring under impulse noise |  http://arxiv.org/abs/1312.6208  | author:Gang Liu, Ting-Zhu Huang, Jun Liu, Xiao-Guang Lv category:math.NA cs.CV published:2013-12-21 summary:The total variation (TV) regularization method is an effective method forimage deblurring in preserving edges. However, the TV based solutions usuallyhave some staircase effects. In this paper, in order to alleviate the staircaseeffect, we propose a new model for restoring blurred images with impulse noise.The model consists of an $\ell_1$-fidelity term and a TV with overlapping groupsparsity (OGS) regularization term. Moreover, we impose a box constraint to theproposed model for getting more accurate solutions. An efficient and effectivealgorithm is proposed to solve the model under the framework of the alternatingdirection method of multipliers (ADMM). We use an inner loop which is nestedinside the majorization minimization (MM) iteration for the subproblem of theproposed method. Compared with other methods, numerical results illustrate thatthe proposed method, can significantly improve the restoration quality, both inavoiding staircase effects and in terms of peak signal-to-noise ratio (PSNR)and relative error (ReE).
arxiv-1312-6192 | Can recursive neural tensor networks learn logical reasoning? |  http://arxiv.org/abs/1312.6192  | author:Samuel R. Bowman category:cs.CL cs.LG published:2013-12-21 summary:Recursive neural network models and their accompanying vector representationsfor words have seen success in an array of increasingly semanticallysophisticated tasks, but almost nothing is known about their ability toaccurately capture the aspects of linguistic meaning that are necessary forinterpretation or reasoning. To evaluate this, I train a recursive model on anew corpus of constructed examples of logical reasoning in short sentences,like the inference of "some animal walks" from "some dog walks" or "some catwalks," given that dogs and cats are animals. This model learns representationsthat generalize well to new types of reasoning pattern in all but a few cases,a result which is promising for the ability of learned representation models tocapture logical reasoning.
arxiv-1312-6197 | An empirical analysis of dropout in piecewise linear networks |  http://arxiv.org/abs/1312.6197  | author:David Warde-Farley, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2013-12-21 summary:The recently introduced dropout training criterion for neural networks hasbeen the subject of much attention due to its simplicity and remarkableeffectiveness as a regularizer, as well as its interpretation as a trainingprocedure for an exponentially large ensemble of networks that shareparameters. In this work we empirically investigate several questions relatedto the efficacy of dropout, specifically as it concerns networks employing thepopular rectified linear activation function. We investigate the quality of thetest time weight-scaling inference procedure by evaluating the geometricaverage exactly in small models, as well as compare the performance of thegeometric mean to the arithmetic mean more commonly employed by ensembletechniques. We explore the effect of tied weights on the ensembleinterpretation by training ensembles of masked networks without tied weights.Finally, we investigate an alternative criterion based on a biased estimator ofthe maximum likelihood ensemble gradient.
arxiv-1312-6186 | GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training |  http://arxiv.org/abs/1312.6186  | author:Thomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, Thomas Huang category:cs.CV cs.DC cs.LG cs.NE published:2013-12-21 summary:The ability to train large-scale neural networks has resulted instate-of-the-art performance in many areas of computer vision. These resultshave largely come from computational break throughs of two forms: modelparallelism, e.g. GPU accelerated training, which has seen quick adoption incomputer vision circles, and data parallelism, e.g. A-SGD, whose large scalehas been used mostly in industry. We report early experiments with a systemthat makes use of both model parallelism and data parallelism, we call GPUA-SGD. We show using GPU A-SGD it is possible to speed up training of largeconvolutional neural networks useful for computer vision. We believe GPU A-SGDwill make it possible to train larger networks on larger training sets in areasonable amount of time.
arxiv-1312-6182 | Large-Scale Paralleled Sparse Principal Component Analysis |  http://arxiv.org/abs/1312.6182  | author:W. Liu, H. Zhang, D. Tao, Y. Wang, K. Lu category:cs.MS cs.LG cs.NA stat.ML published:2013-12-21 summary:Principal component analysis (PCA) is a statistical technique commonly usedin multivariate data analysis. However, PCA can be difficult to interpret andexplain since the principal components (PCs) are linear combinations of theoriginal variables. Sparse PCA (SPCA) aims to balance statistical fidelity andinterpretability by approximating sparse PCs whose projections capture themaximal variance of original data. In this paper we present an efficient andparalleled method of SPCA using graphics processing units (GPUs), which canprocess large blocks of data in parallel. Specifically, we construct parallelimplementations of the four optimization formulations of the generalized powermethod of SPCA (GP-SPCA), one of the most efficient and effective SPCAapproaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS)is up to eleven times faster than the corresponding CPU implementation (usingCBLAS), and up to 107 times faster than a MatLab implementation. Extensivecomparative experiments in several real-world datasets confirm that SPCA offersa practical advantage.
arxiv-1312-6180 | Manifold regularized kernel logistic regression for web image annotation |  http://arxiv.org/abs/1312.6180  | author:W. Liu, H. Liu, D. Tao, Y. Wang, K. Lu category:cs.LG cs.MM published:2013-12-21 summary:With the rapid advance of Internet technology and smart devices, users oftenneed to manage large amounts of multimedia information using smart devices,such as personal image and video accessing and browsing. These requirementsheavily rely on the success of image (video) annotation, and thus large scaleimage annotation through innovative machine learning methods has attractedintensive attention in recent years. One representative work is support vectormachine (SVM). Although it works well in binary classification, SVM has anon-smooth loss function and can not naturally cover multi-class case. In thispaper, we propose manifold regularized kernel logistic regression (KLR) for webimage annotation. Compared to SVM, KLR has the following advantages: (1) theKLR has a smooth loss function; (2) the KLR produces an explicit estimate ofthe probability instead of class label; and (3) the KLR can naturally begeneralized to the multi-class case. We carefully conduct experiments on MIRFLICKR dataset and demonstrate the effectiveness of manifold regularized kernellogistic regression for image annotation.
arxiv-1312-6190 | Adaptive Feature Ranking for Unsupervised Transfer Learning |  http://arxiv.org/abs/1312.6190  | author:Son N. Tran, Artur d'Avila Garcez category:cs.LG published:2013-12-21 summary:Transfer Learning is concerned with the application of knowledge gained fromsolving a problem to a different but related problem domain. In this paper, wepropose a method and efficient algorithm for ranking and selectingrepresentations from a Restricted Boltzmann Machine trained on a source domainto be transferred onto a target domain. Experiments carried out using theMNIST, ICDAR and TiCC image datasets show that the proposed adaptive featureranking and transfer learning method offers statistically significantimprovements on the training of RBMs. Our method is general in that theknowledge chosen by the ranking function does not depend on its relation to anyspecific target domain, and it works with unsupervised learning andknowledge-based transfer.
arxiv-1312-6205 | Relaxations for inference in restricted Boltzmann machines |  http://arxiv.org/abs/1312.6205  | author:Sida I. Wang, Roy Frostig, Percy Liang, Christopher D. Manning category:stat.ML cs.LG published:2013-12-21 summary:We propose a relaxation-based approximate inference algorithm that samplesnear-MAP configurations of a binary pairwise Markov random field. We experimenton MAP inference tasks in several restricted Boltzmann machines. We also useour underlying sampler to estimate the log-partition function of restrictedBoltzmann machines and compare against other sampling-based methods.
arxiv-1312-6214 | Volumetric Spanners: an Efficient Exploration Basis for Learning |  http://arxiv.org/abs/1312.6214  | author:Elad Hazan, Zohar Karnin, Raghu Mehka category:cs.LG cs.AI cs.DS published:2013-12-21 summary:Numerous machine learning problems require an exploration basis - a mechanismto explore the action space. We define a novel geometric notion of explorationbasis with low variance, called volumetric spanners, and give efficientalgorithms to construct such a basis. We show how efficient volumetric spanners give rise to the first efficientand optimal regret algorithm for bandit linear optimization over general convexsets. Previously such results were known only for specific convex sets, orunder special conditions such as the existence of an efficient self-concordantbarrier for the underlying set.
arxiv-1312-6204 | One-Shot Adaptation of Supervised Deep Convolutional Models |  http://arxiv.org/abs/1312.6204  | author:Judy Hoffman, Eric Tzeng, Jeff Donahue, Yangqing Jia, Kate Saenko, Trevor Darrell category:cs.CV cs.LG cs.NE published:2013-12-21 summary:Dataset bias remains a significant barrier towards solving real worldcomputer vision tasks. Though deep convolutional networks have proven to be acompetitive approach for image classification, a question remains: have thesemodels have solved the dataset bias problem? In general, training orfine-tuning a state-of-the-art deep model on a new domain requires asignificant amount of data, which for many applications is simply notavailable. Transfer of models directly to new domains without adaptation hashistorically led to poor recognition performance. In this paper, we pose thefollowing question: is a single image dataset, much larger than previouslyexplored for adaptation, comprehensive enough to learn general deep models thatmay be effectively applied to new image domains? In other words, are deep CNNstrained on large amounts of labeled data as susceptible to dataset bias asprevious methods have been shown to be? We show that a generic supervised deepCNN model trained on a large dataset reduces, but does not remove, datasetbias. Furthermore, we propose several methods for adaptation with deep modelsthat are able to operate with little (one example per category) or no labeleddomain specific data. Our experiments show that adaptation of deep models onbenchmark visual domain adaptation datasets can provide a significantperformance boost.
arxiv-1312-6203 | Spectral Networks and Locally Connected Networks on Graphs |  http://arxiv.org/abs/1312.6203  | author:Joan Bruna, Wojciech Zaremba, Arthur Szlam, Yann LeCun category:cs.LG cs.CV cs.NE published:2013-12-21 summary:Convolutional Neural Networks are extremely efficient architectures in imageand audio recognition tasks, thanks to their ability to exploit the localtranslational invariance of signal classes over their domain. In this paper weconsider possible generalizations of CNNs to signals defined on more generaldomains without the action of a translation group. In particular, we proposetwo constructions, one based upon a hierarchical clustering of the domain, andanother based on the spectrum of the graph Laplacian. We show throughexperiments that for low-dimensional graphs it is possible to learnconvolutional layers with a number of parameters independent of the input size,resulting in efficient deep architectures.
arxiv-1312-6184 | Do Deep Nets Really Need to be Deep? |  http://arxiv.org/abs/1312.6184  | author:Lei Jimmy Ba, Rich Caruana category:cs.LG cs.NE published:2013-12-21 summary:Currently, deep neural networks are the state of the art on problems such asspeech recognition and computer vision. In this extended abstract, we show thatshallow feed-forward networks can learn the complex functions previouslylearned by deep nets and achieve accuracies previously only achievable withdeep models. Moreover, in some cases the shallow neural nets can learn thesedeep functions using a total number of parameters similar to the original deepmodel. We evaluate our method on the TIMIT phoneme recognition task and areable to train shallow fully-connected nets that perform similarly to complex,well-engineered, deep convolutional architectures. Our success in trainingshallow neural nets to mimic deeper models suggests that there probably existbetter algorithms for training shallow feed-forward nets than those currentlyavailable.
arxiv-1312-6173 | Multilingual Distributed Representations without Word Alignment |  http://arxiv.org/abs/1312.6173  | author:Karl Moritz Hermann, Phil Blunsom category:cs.CL published:2013-12-20 summary:Distributed representations of meaning are a natural way to encode covariancerelationships between words and phrases in NLP. By overcoming data sparsityproblems, as well as providing information about semantic relatedness which isnot available in discrete representations, distributed representations haveproven useful in many NLP tasks. Recent work has shown how compositionalsemantic representations can successfully be applied to a number of monolingualapplications such as sentiment analysis. At the same time, there has been someinitial success in work on learning shared word-level representations acrosslanguages. We combine these two approaches by proposing a method for learningdistributed representations in a multilingual setup. Our model learns to assignsimilar embeddings to aligned sentences and dissimilar ones to sentence whichare not aligned while not requiring word alignments. We show that ourrepresentations are semantically informative and apply them to a cross-lingualdocument classification task where we outperform the previous state of the art.Further, by employing parallel corpora of multiple language pairs we find thatour model learns representations that capture semantic relationships acrosslanguages for which no parallel data was used.
arxiv-1401-0509 | Zero-Shot Learning for Semantic Utterance Classification |  http://arxiv.org/abs/1401.0509  | author:Yann N. Dauphin, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck category:cs.CL cs.LG published:2013-12-20 summary:We propose a novel zero-shot learning method for semantic utteranceclassification (SUC). It learns a classifier $f: X \to Y$ for problems wherenone of the semantic categories $Y$ are present in the training set. Theframework uncovers the link between categories and utterances using a semanticspace. We show that this semantic space can be learned by deep neural networkstrained on large amounts of search engine query log data. More precisely, wepropose a novel method that can learn discriminative semantic features withoutsupervision. It uses the zero-shot learning framework to guide the learning ofthe semantic features. We demonstrate the effectiveness of the zero-shotsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).Furthermore, we achieve state-of-the-art results by combining the semanticfeatures with a supervised method.
arxiv-1312-5940 | Generic Deep Networks with Wavelet Scattering |  http://arxiv.org/abs/1312.5940  | author:Edouard Oyallon, Stéphane Mallat, Laurent Sifre category:cs.CV published:2013-12-20 summary:We introduce a two-layer wavelet scattering network, for objectclassification. This scattering transform computes a spatial wavelet transformon the first layer and a new joint wavelet transform along spatial, angular andscale variables in the second layer. Numerical experiments demonstrate thatthis two layer convolution network, which involves no learning and no maxpooling, performs efficiently on complex image data sets such as CalTech, withstructural objects variability and clutter. It opens the possibility tosimplify deep neural network learning by initializing the first layers withwavelet filters.
arxiv-1312-6115 | Neuronal Synchrony in Complex-Valued Deep Networks |  http://arxiv.org/abs/1312.6115  | author:David P. Reichert, Thomas Serre category:stat.ML cs.LG cs.NE q-bio.NC published:2013-12-20 summary:Deep learning has recently led to great successes in tasks such as imagerecognition (e.g Krizhevsky et al., 2012). However, deep networks are stilloutmatched by the power and versatility of the brain, perhaps in part due tothe richer neuronal computations available to cortical circuits. The challengeis to identify which neuronal mechanisms are relevant, and to find suitableabstractions to model them. Here, we show how aspects of spike timing, longhypothesized to play a crucial role in cortical information processing, couldbe incorporated into deep networks to build richer, versatile representations. We introduce a neural network formulation based on complex-valued neuronalunits that is not only biologically meaningful but also amenable to a varietyof deep learning frameworks. Here, units are attributed both a firing rate anda phase, the latter indicating properties of spike timing. We show how thisformulation qualitatively captures several aspects thought to be related toneuronal synchrony, including gating of information processing and dynamicbinding of distributed object representations. Focusing on the latter, wedemonstrate the potential of the approach in several simple experiments. Thus,neuronal synchrony could be a flexible mechanism that fulfills multiplefunctional roles in deep networks.
arxiv-1312-6077 | Efficient Visual Coding: From Retina To V2 |  http://arxiv.org/abs/1312.6077  | author:Honghao Shan, Garrison Cottrell category:cs.CV q-bio.NC published:2013-12-20 summary:The human visual system has a hierarchical structure consisting of layers ofprocessing, such as the retina, V1, V2, etc. Understanding the functional rolesof these visual processing layers would help to integrate thepsychophysiological and neurophysiological models into a consistent theory ofhuman vision, and would also provide insights to computer vision research. Oneclassical theory of the early visual pathway hypothesizes that it serves tocapture the statistical structure of the visual inputs by efficiently codingthe visual information in its outputs. Until recently, most computationalmodels following this theory have focused upon explaining the receptive fieldproperties of one or two visual layers. Recent work in deep networks haseliminated this concern, however, there is till the retinal layer to consider.Here we improve on a previously-described hierarchical model Recursive ICA(RICA) [1] which starts with PCA, followed by a layer of sparse coding or ICA,followed by a component-wise nonlinearity derived from considerations of thevariable distributions expected by ICA. This process is then repeated. In thiswork, we improve on this model by using a new version of sparse PCA (sPCA),which results in biologically-plausible receptive fields for both the sPCA andICA/sparse coding. When applied to natural image patches, our model learnsvisual features exhibiting the receptive field properties of retinal ganglioncells/lateral geniculate nucleus (LGN) cells, V1 simple cells, V1 complexcells, and V2 cells. Our work provides predictions for experimentalneuroscience studies. For example, our result suggests that a previousneurophysiological study improperly discarded some of their recorded neurons;we predict that their discarded neurons capture the shape contour of objects.
arxiv-1312-6594 | Sequentially Generated Instance-Dependent Image Representations for Classification |  http://arxiv.org/abs/1312.6594  | author:Gabriel Dulac-Arnold, Ludovic Denoyer, Nicolas Thome, Matthieu Cord, Patrick Gallinari category:cs.CV cs.LG published:2013-12-20 summary:In this paper, we investigate a new framework for image classification thatadaptively generates spatial representations. Our strategy is based on asequential process that learns to explore the different regions of any image inorder to infer its category. In particular, the choice of regions is specificto each image, directed by the actual content of previously selectedregions.The capacity of the system to handle incomplete image information aswell as its adaptive region selection allow the system to perform well inbudgeted classification tasks by exploiting a dynamicly generatedrepresentation of each image. We demonstrate the system's abilities in a seriesof image-based exploration and classification tasks that highlight its learnedexploration and inference abilities.
arxiv-1312-5785 | EXMOVES: Classifier-based Features for Scalable Action Recognition |  http://arxiv.org/abs/1312.5785  | author:Du Tran, Lorenzo Torresani category:cs.CV published:2013-12-20 summary:This paper introduces EXMOVES, learned exemplar-based features for efficientrecognition of actions in videos. The entries in our descriptor are produced byevaluating a set of movement classifiers over spatial-temporal volumes of theinput sequence. Each movement classifier is a simple exemplar-SVM trained onlow-level features, i.e., an SVM learned using a single annotated positivespace-time volume and a large number of unannotated videos. Our representation offers two main advantages. First, since our mid-levelfeatures are learned from individual video exemplars, they require minimalamount of supervision. Second, we show that simple linear classification modelstrained on our global video descriptor yield action recognition accuracyapproaching the state-of-the-art but at orders of magnitude lower cost, sinceat test-time no sliding window is necessary and linear models are efficient totrain and test. This enables scalable action recognition, i.e., efficientclassification of a large number of different actions even in large videodatabases. We show the generality of our approach by building our mid-leveldescriptors from two different low-level feature representations. The accuracyand efficiency of the approach are demonstrated on several large-scale actionrecognition benchmarks.
arxiv-1312-6116 | Improving Deep Neural Networks with Probabilistic Maxout Units |  http://arxiv.org/abs/1312.6116  | author:Jost Tobias Springenberg, Martin Riedmiller category:stat.ML cs.LG cs.NE published:2013-12-20 summary:We present a probabilistic variant of the recently introduced maxout unit.The success of deep neural networks utilizing maxout can partly be attributedto favorable performance under dropout, when compared to rectified linearunits. It however also depends on the fact that each maxout unit performs apooling operation over a group of linear transformations and is thus partiallyinvariant to changes in its input. Starting from this observation we ask thequestion: Can the desirable properties of maxout units be preserved whileimproving their invariance properties ? We argue that our probabilistic maxout(probout) units successfully achieve this balance. We quantitatively verifythis claim and report classification performance matching or exceeding thecurrent state of the art on three challenging image classification benchmarks(CIFAR-10, CIFAR-100 and SVHN).
arxiv-1312-5851 | Fast Training of Convolutional Networks through FFTs |  http://arxiv.org/abs/1312.5851  | author:Michael Mathieu, Mikael Henaff, Yann LeCun category:cs.CV cs.LG cs.NE published:2013-12-20 summary:Convolutional networks are one of the most widely employed architectures incomputer vision and machine learning. In order to leverage their ability tolearn complex functions, large amounts of data are required for training.Training a large convolutional network to produce state-of-the-art results cantake weeks, even when using modern GPUs. Producing labels using a trainednetwork can also be costly when dealing with web-scale datasets. In this work,we present a simple algorithm which accelerates training and inference by asignificant factor, and can yield improvements of over an order of magnitudecompared to existing state-of-the-art implementations. This is done bycomputing convolutions as pointwise products in the Fourier domain whilereusing the same transformed feature map many times. The algorithm isimplemented on a GPU architecture and addresses a number of related challenges.
arxiv-1312-6095 | Multi-View Priors for Learning Detectors from Sparse Viewpoint Data |  http://arxiv.org/abs/1312.6095  | author:Bojan Pepik, Michael Stark, Peter Gehler, Bernt Schiele category:cs.CV published:2013-12-20 summary:While the majority of today's object class models provide only 2D boundingboxes, far richer output hypotheses are desirable including viewpoint,fine-grained category, and 3D geometry estimate. However, models trained toprovide richer output require larger amounts of training data, preferably wellcovering the relevant aspects such as viewpoint and fine-grained categories. Inthis paper, we address this issue from the perspective of transfer learning,and design an object class model that explicitly leverages correlations betweenvisual features. Specifically, our model represents prior distributions overpermissible multi-view detectors in a parametric way -- the priors are learnedonce from training data of a source object class, and can later be used tofacilitate the learning of a detector for a target class. As we show in ourexperiments, this transfer is not only beneficial for detectors based onbasic-level category representations, but also enables the robust learning ofdetectors that represent classes at finer levels of granularity, where trainingdata is typically even scarcer and more unbalanced. As a result, we reportlargely improved performance in simultaneous 2D object localization andviewpoint estimation on a recent dataset of challenging street scenes.
arxiv-1312-5847 | Deep learning for neuroimaging: a validation study |  http://arxiv.org/abs/1312.5847  | author:Sergey M. Plis, Devon R. Hjelm, Ruslan Salakhutdinov, Vince D. Calhoun category:cs.NE cs.LG stat.ML published:2013-12-20 summary:Deep learning methods have recently made notable advances in the tasks ofclassification and representation learning. These tasks are important for brainimaging and neuroscience discovery, making the methods attractive for portingto a neuroimager's toolbox. Success of these methods is, in part, explained bythe flexibility of deep learning models. However, this flexibility makes theprocess of porting to new areas a difficult parameter optimization problem. Inthis work we demonstrate our results (and feasible parameter ranges) inapplication of deep learning methods to structural and functional brain imagingdata. We also describe a novel constraint-based approach to visualizing highdimensional data. We use it to analyze the effect of parameter choices on datatransformations. Our results show that deep learning methods are able to learnphysiologically important representations and detect latent relations inneuroimaging data.
arxiv-1312-6120 | Exact solutions to the nonlinear dynamics of learning in deep linear neural networks |  http://arxiv.org/abs/1312.6120  | author:Andrew M. Saxe, James L. McClelland, Surya Ganguli category:cs.NE cs.CV cs.LG q-bio.NC stat.ML published:2013-12-20 summary:Despite the widespread practical success of deep learning methods, ourtheoretical understanding of the dynamics of learning in deep neural networksremains quite sparse. We attempt to bridge the gap between the theory andpractice of deep learning by systematically analyzing learning dynamics for therestricted case of deep linear neural networks. Despite the linearity of theirinput-output map, such networks have nonlinear gradient descent dynamics onweights that change with the addition of each new hidden layer. We show thatdeep linear networks exhibit nonlinear learning phenomena similar to those seenin simulations of nonlinear networks, including long plateaus followed by rapidtransitions to lower error solutions, and faster convergence from greedyunsupervised pretraining initial conditions than from random initialconditions. We provide an analytical description of these phenomena by findingnew exact solutions to the nonlinear dynamics of deep learning. Our theoreticalanalysis also reveals the surprising finding that as the depth of a networkapproaches infinity, learning speed can nevertheless remain finite: for aspecial class of initial conditions on the weights, very deep networks incuronly a finite, depth independent, delay in learning speed relative to shallownetworks. We show that, under certain conditions on the training data,unsupervised pretraining can find this special class of initial conditions,while scaled random Gaussian initializations cannot. We further exhibit a newclass of random orthogonal initial conditions on weights that, likeunsupervised pre-training, enjoys depth independent learning times. We furthershow that these initial conditions also lead to faithful propagation ofgradients even in deep nonlinear networks, as long as they operate in a specialregime known as the edge of chaos.
arxiv-1312-5845 | Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks |  http://arxiv.org/abs/1312.5845  | author:Takashi Shinozaki, Yasushi Naruse category:cs.NE cs.CV cs.LG stat.ML published:2013-12-20 summary:We propose a novel learning method for multilayered neural networks whichuses feedforward supervisory signal and associates classification of a newinput with that of pre-trained input. The proposed method effectively uses richinput information in the earlier layer for robust leaning and revising internalrepresentation in a multilayer neural network.
arxiv-1401-3579 | A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An Actor-Critic Approach |  http://arxiv.org/abs/1401.3579  | author:Keyvan Yahya category:cs.GT cs.AI cs.LG published:2013-12-20 summary:This paper aims to find an algorithmic structure that affords to predict andexplain economical choice behaviour particularly under uncertainty(randompolicies) by manipulating the prevalent Actor-Critic learning method to complywith the requirements we have been entrusted ever since the field ofneuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics thatseem relevant to our discussion, we will try to outline some of the importantworks which have so far been done to simulate choice making processes.Concerning neurological findings that suggest the existence of two specificfunctions that are executed through Basal Ganglia all the way up to sub-cortical areas, namely 'rewards' and 'beliefs', we will offer a modifiedversion of actor/critic algorithm to shed a light on the relation between thesefunctions and most importantly resolve what is referred to as a challenge foractor-critic algorithms, that is, the lack of inheritance or hierarchy whichavoids the system being evolved in continuous time tasks whence the convergencemight not be emerged.
arxiv-1312-5857 | A Generative Product-of-Filters Model of Audio |  http://arxiv.org/abs/1312.5857  | author:Dawen Liang, Matthew D. Hoffman, Gautham J. Mysore category:stat.ML cs.LG published:2013-12-20 summary:We propose the product-of-filters (PoF) model, a generative model thatdecomposes audio spectra as sparse linear combinations of "filters" in thelog-spectral domain. PoF makes similar assumptions to those used in the classichomomorphic filtering approach to signal processing, but replaces hand-designeddecompositions built of basic signal processing operations with a learneddecomposition based on statistical inference. This paper formulates the PoFmodel and derives a mean-field method for posterior inference and a variationalEM algorithm to estimate the model's free parameters. We demonstrate PoF'spotential for audio processing on a bandwidth expansion task, and show that PoFcan serve as an effective unsupervised feature extractor for a speakeridentification task.
arxiv-1312-6034 | Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps |  http://arxiv.org/abs/1312.6034  | author:Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2013-12-20 summary:This paper addresses the visualisation of image classification models, learntusing deep Convolutional Networks (ConvNets). We consider two visualisationtechniques, based on computing the gradient of the class score with respect tothe input image. The first one generates an image, which maximises the classscore [Erhan et al., 2009], thus visualising the notion of the class, capturedby a ConvNet. The second technique computes a class saliency map, specific to agiven image and class. We show that such maps can be employed for weaklysupervised object segmentation using classification ConvNets. Finally, weestablish the connection between the gradient-based ConvNet visualisationmethods and deconvolutional networks [Zeiler et al., 2013].
arxiv-1312-5853 | Multi-GPU Training of ConvNets |  http://arxiv.org/abs/1312.5853  | author:Omry Yadan, Keith Adams, Yaniv Taigman, Marc'Aurelio Ranzato category:cs.LG cs.NE published:2013-12-20 summary:In this work we evaluate different approaches to parallelize computation ofconvolutional neural networks across several GPUs.
arxiv-1312-6159 | Learned versus Hand-Designed Feature Representations for 3d Agglomeration |  http://arxiv.org/abs/1312.6159  | author:John A. Bogovic, Gary B. Huang, Viren Jain category:cs.CV published:2013-12-20 summary:For image recognition and labeling tasks, recent results suggest that machinelearning methods that rely on manually specified feature representations may beoutperformed by methods that automatically derive feature representations basedon the data. Yet for problems that involve analysis of 3d objects, such as meshsegmentation, shape retrieval, or neuron fragment agglomeration, there remainsa strong reliance on hand-designed feature descriptors. In this paper, weevaluate a large set of hand-designed 3d feature descriptors alongside featureslearned from the raw data using both end-to-end and unsupervised learningtechniques, in the context of agglomeration of 3d neuron fragments. Bycombining unsupervised learning techniques with a novel dynamic pooling scheme,we show how pure learning-based methods are for the first time competitive withhand-designed 3d shape descriptors. We investigate data augmentation strategiesfor dramatically increasing the size of the training set, and show howcombining both learned and hand-designed features leads to the highestaccuracy.
arxiv-1312-6062 | Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error |  http://arxiv.org/abs/1312.6062  | author:David Buchaca, Enrique Romero, Ferran Mazzanti, Jordi Delgado category:cs.LG published:2013-12-20 summary:Restricted Boltzmann Machines (RBMs) are general unsupervised learningdevices to ascertain generative models of data distributions. RBMs are oftentrained using the Contrastive Divergence learning algorithm (CD), anapproximation to the gradient of the data log-likelihood. A simplereconstruction error is often used to decide whether the approximation providedby the CD algorithm is good enough, though several authors (Schulz et al.,2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility ofthis procedure. However, not many alternatives to the reconstruction error havebeen used in the literature. In this manuscript we investigate simplealternatives to the reconstruction error in order to detect as soon as possiblethe decrease in the log-likelihood during learning.
arxiv-1312-6086 | The return of AdaBoost.MH: multi-class Hamming trees |  http://arxiv.org/abs/1312.6086  | author:Balázs Kégl category:cs.LG published:2013-12-20 summary:Within the framework of AdaBoost.MH, we propose to train vector-valueddecision trees to optimize the multi-class edge without reducing themulti-class problem to $K$ binary one-against-all classifications. The keyelement of the method is a vector-valued decision stump, factorized into aninput-independent vector of length $K$ and label-independent scalar classifier.At inner tree nodes, the label-dependent vector is discarded and the binaryclassifier can be used for partitioning the input space into two regions. Thealgorithm retains the conceptual elegance, power, and computational efficiencyof binary AdaBoost. In experiments it is on par with support vector machinesand with the best existing multi-class boosting algorithm AOSOLogitBoost, andit is significantly better than other known implementations of AdaBoost.MH.
arxiv-1312-6024 | Occupancy Detection in Vehicles Using Fisher Vector Image Representation |  http://arxiv.org/abs/1312.6024  | author:Yusuf Artan, Peter Paul category:cs.CV published:2013-12-20 summary:Due to the high volume of traffic on modern roadways, transportation agencieshave proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling(HOT) lanes to promote car pooling. However, enforcement of the rules of theselanes is currently performed by roadside enforcement officers using visualobservation. Manual roadside enforcement is known to be inefficient, costly,potentially dangerous, and ultimately ineffective. Violation rates up to50%-80% have been reported, while manual enforcement rates of less than 10% aretypical. Therefore, there is a need for automated vehicle occupancy detectionto support HOV/HOT lane enforcement. A key component of determining vehicleoccupancy is to determine whether or not the vehicle's front passenger seat isoccupied. In this paper, we examine two methods of determining vehicle frontseat occupancy using a near infrared (NIR) camera system pointed at thevehicle's front windshield. The first method examines a state-of-the-artdeformable part model (DPM) based face detection system that is robust tofacial pose. The second method examines state-of- the-art local aggregationbased image classification using bag-of-visual-words (BOW) and Fisher vectors(FV). A dataset of 3000 images was collected on a public roadway and is used toperform the comparison. From these experiments it is clear that the imageclassification approach is superior for this problem.
arxiv-1312-6114 | Auto-Encoding Variational Bayes |  http://arxiv.org/abs/1312.6114 | author:Diederik P Kingma, Max Welling category:stat.ML cs.LG published:2013-12-20 summary:How can we perform efficient inference and learning in directed probabilisticmodels, in the presence of continuous latent variables with intractableposterior distributions, and large datasets? We introduce a stochasticvariational inference and learning algorithm that scales to large datasets and,under some mild differentiability conditions, even works in the intractablecase. Our contributions is two-fold. First, we show that a reparameterizationof the variational lower bound yields a lower bound estimator that can bestraightforwardly optimized using standard stochastic gradient methods. Second,we show that for i.i.d. datasets with continuous latent variables perdatapoint, posterior inference can be made especially efficient by fitting anapproximate inference model (also called a recognition model) to theintractable posterior using the proposed lower bound estimator. Theoreticaladvantages are reflected in experimental results.
arxiv-1312-7335 | Correlation-based construction of neighborhood and edge features |  http://arxiv.org/abs/1312.7335  | author:Balázs Kégl category:cs.CV cs.LG stat.ML published:2013-12-20 summary:Motivated by an abstract notion of low-level edge detector filters, wepropose a simple method of unsupervised feature construction based on pairwisestatistics of features. In the first step, we construct neighborhoods offeatures by regrouping features that correlate. Then we use these subsets asfilters to produce new neighborhood features. Next, we connect neighborhoodfeatures that correlate, and construct edge features by subtracting thecorrelated neighborhood features of each other. To validate the usefulness ofthe constructed features, we ran AdaBoost.MH on four multi-class classificationproblems. Our most significant result is a test error of 0.94% on MNIST with analgorithm which is essentially free of any image-specific priors. On CIFAR-10our method is suboptimal compared to today's best deep learning techniques,nevertheless, we show that the proposed method outperforms not only boosting onthe raw pixels, but also boosting on Haar filters.
arxiv-1312-5946 | Simple Methods for Initializing the EM Algorithm for Gaussian Mixture Models |  http://arxiv.org/abs/1312.5946  | author:Johannes Blömer, Kathrin Bujna category:cs.LG published:2013-12-20 summary:In this paper, we consider simple and fast approaches to initialize theExpectation-Maximization algorithm (EM) for multivariate Gaussian mixturemodels. We present new initialization methods based on the well-known$K$-means++ algorithm and the Gonzalez algorithm. These methods close the gapbetween simple uniform initialization techniques and complex methods, that havebeen specifically designed for Gaussian mixture models and depend on the rightchoice of hyperparameters. In our evaluation we compare our methods with acommonly used random initialization method, an approach based on agglomerativehierarchical clustering, and a known, plain adaption of the Gonzalez algorithm.Our results indicate that algorithms based on $K$-means++ outperform the othermethods.
arxiv-1312-6042 | Learning States Representations in POMDP |  http://arxiv.org/abs/1312.6042  | author:Gabriella Contardo, Ludovic Denoyer, Thierry Artieres, Patrick Gallinari category:cs.LG published:2013-12-20 summary:We propose to deal with sequential processes where only partial observationsare available by learning a latent representation space on which policies maybe accurately learned.
arxiv-1312-5891 | The Sparse Principal Component of a Constant-rank Matrix |  http://arxiv.org/abs/1312.5891  | author:Megasthenis Asteris, Dimitris S. Papailiopoulos, George N. Karystinos category:cs.IT math.IT stat.ML published:2013-12-20 summary:The computation of the sparse principal component of a matrix is equivalentto the identification of its principal submatrix with the largest maximumeigenvalue. Finding this optimal submatrix is what renders the problem${\mathcal{NP}}$-hard. In this work, we prove that, if the matrix is positivesemidefinite and its rank is constant, then its sparse principal component ispolynomially computable. Our proof utilizes the auxiliary unit vector techniquethat has been recently developed to identify problems that are polynomiallysolvable. Moreover, we use this technique to design an algorithm which, for anysparsity value, computes the sparse principal component with complexity${\mathcal O}\left(N^{D+1}\right)$, where $N$ and $D$ are the matrix size andrank, respectively. Our algorithm is fully parallelizable and memory efficient.
arxiv-1312-6108 | Modeling correlations in spontaneous activity of visual cortex with centered Gaussian-binary deep Boltzmann machines |  http://arxiv.org/abs/1312.6108  | author:Nan Wang, Dirk Jancke, Laurenz Wiskott category:cs.NE cs.LG q-bio.NC published:2013-12-20 summary:Spontaneous cortical activity -- the ongoing cortical activities in absenceof intentional sensory input -- is considered to play a vital role in manyaspects of both normal brain functions and mental dysfunctions. We present acentered Gaussian-binary Deep Boltzmann Machine (GDBM) for modeling theactivity in early cortical visual areas and relate the random sampling in GDBMsto the spontaneous cortical activity. After training the proposed model onnatural image patches, we show that the samples collected from the model'sprobability distribution encompass similar activity patterns as found in thespontaneous activity. Specifically, filters having the same orientationpreference tend to be active together during random sampling. Our workdemonstrates the centered GDBM is a meaningful model approach for basicreceptive field properties and the emergence of spontaneous activity patternsin early cortical visual areas. Besides, we show empirically that centeredGDBMs do not suffer from the difficulties during training as GDBMs do and canbe properly trained without the layer-wise pretraining.
arxiv-1312-6082 | Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks |  http://arxiv.org/abs/1312.6082  | author:Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet category:cs.CV published:2013-12-20 summary:Recognizing arbitrary multi-character text in unconstrained naturalphotographs is a hard problem. In this paper, we address an equally hardsub-problem in this domain viz. recognizing arbitrary multi-digit numbers fromStreet View imagery. Traditional approaches to solve this problem typicallyseparate out the localization, segmentation, and recognition steps. In thispaper we propose a unified approach that integrates these three steps via theuse of a deep convolutional neural network that operates directly on the imagepixels. We employ the DistBelief implementation of deep neural networks inorder to train large, distributed neural networks on high quality images. Wefind that the performance of this approach increases with the depth of theconvolutional network, with the best performance occurring in the deepestarchitecture we trained, with eleven hidden layers. We evaluate this approachon the publicly available SVHN dataset and achieve over $96\%$ accuracy inrecognizing complete street numbers. We show that on a per-digit recognitiontask, we improve upon the state-of-the-art, achieving $97.84\%$ accuracy. Wealso evaluate this approach on an even more challenging dataset generated fromStreet View imagery containing several tens of millions of street numberannotations and achieve over $90\%$ accuracy. To further explore theapplicability of the proposed system to broader text recognition tasks, weapply it to synthetic distorted text from reCAPTCHA. reCAPTCHA is one of themost secure reverse turing tests that uses distorted text to distinguish humansfrom bots. We report a $99.8\%$ accuracy on the hardest category of reCAPTCHA.Our evaluations on both tasks indicate that at specific operating thresholds,the performance of the proposed system is comparable to, and in some casesexceeds, that of human operators.
arxiv-1312-5889 | Non-parametric Bayesian modeling of complex networks |  http://arxiv.org/abs/1312.5889  | author:Mikkel N. Schmidt, Morten Mørup category:stat.ML published:2013-12-20 summary:Modeling structure in complex networks using Bayesian non-parametrics makesit possible to specify flexible model structures and infer the adequate modelcomplexity from the observed data. This paper provides a gentle introduction tonon-parametric Bayesian modeling of complex networks: Using an infinite mixturemodel as running example we go through the steps of deriving the model as aninfinite limit of a finite parametric model, inferring the model parameters byMarkov chain Monte Carlo, and checking the model's fit and predictiveperformance. We explain how advanced non-parametric models for complex networkscan be derived and point out relevant literature.
arxiv-1312-5814 | Optimal parameter selection for unsupervised neural network using genetic algorithm |  http://arxiv.org/abs/1312.5814  | author:suneetha chittineni, Raveendra Babu Bhogapathi category:cs.NE published:2013-12-20 summary:K-means Fast Learning Artificial Neural Network (K-FLANN) is an unsupervisedneural network requires two parameters: tolerance and vigilance. BestClustering results are feasible only by finest parameters specified to theneural network. Selecting optimal values for these parameters is a majorproblem. To solve this issue, Genetic Algorithm (GA) is used to determineoptimal parameters of K-FLANN for finding groups in multidimensional data.K-FLANN is a simple topological network, in which output nodes growsdynamically during the clustering process on receiving input patterns. OriginalK-FLANN is enhanced to select winner unit out of the matched nodes so thatstable clusters are formed with in a less number of epochs. The experimentalresults show that the GA is efficient in finding optimal values of parametersfrom the large search space and is tested using artificial and synthetic datasets.
arxiv-1312-6110 | Learning Generative Models with Visual Attention |  http://arxiv.org/abs/1312.6110  | author:Yichuan Tang, Nitish Srivastava, Ruslan Salakhutdinov category:cs.CV published:2013-12-20 summary:Attention has long been proposed by psychologists as important foreffectively dealing with the enormous sensory stimulus available in theneocortex. Inspired by the visual attention models in computationalneuroscience and the need of object-centric data for generative models, wedescribe for generative learning framework using attentional mechanisms.Attentional mechanisms can propagate signals from region of interest in a sceneto an aligned canonical representation, where generative modeling takes place.By ignoring background clutter, generative models can concentrate theirresources on the object of interest. Our model is a proper graphical modelwhere the 2D Similarity transformation is a part of the top-down process. AConvNet is employed to provide good initializations during posterior inferencewhich is based on Hamiltonian Monte Carlo. Upon learning images of faces, ourmodel can robustly attend to face regions of novel test subjects. Moreimportantly, our model can learn generative models of new faces from a noveldataset of large images where the face locations are not known.
arxiv-1312-6169 | Learning Information Spread in Content Networks |  http://arxiv.org/abs/1312.6169  | author:Cédric Lagnier, Simon Bourigault, Sylvain Lamprier, Ludovic Denoyer, Patrick Gallinari category:cs.LG cs.SI physics.soc-ph published:2013-12-20 summary:We introduce a model for predicting the diffusion of content information onsocial media. When propagation is usually modeled on discrete graph structures,we introduce here a continuous diffusion model, where nodes in a diffusioncascade are projected onto a latent space with the property that theirproximity in this space reflects the temporal diffusion process. We focus onthe task of predicting contaminated users for an initial initial informationsource and provide preliminary results on differents datasets.
arxiv-1312-6171 | Learning Paired-associate Images with An Unsupervised Deep Learning Architecture |  http://arxiv.org/abs/1312.6171  | author:Ti Wang, Daniel L. Silver category:cs.NE cs.CV cs.LG published:2013-12-20 summary:This paper presents an unsupervised multi-modal learning system that learnsassociative representation from two input modalities, or channels, such thatinput on one channel will correctly generate the associated response at theother and vice versa. In this way, the system develops a kind of supervisedclassification model meant to simulate aspects of human associative memory. Thesystem uses a deep learning architecture (DLA) composed of two input/outputchannels formed from stacked Restricted Boltzmann Machines (RBM) and anassociative memory network that combines the two channels. The DLA is trainedon pairs of MNIST handwritten digit images to develop hierarchical features andassociative representations that are able to reconstruct one image given itspaired-associate. Experiments show that the multi-modal learning systemgenerates models that are as accurate as back-propagation networks but with theadvantage of a bi-directional network and unsupervised learning from eitherpaired or non-paired training examples.
arxiv-1312-5783 | Unsupervised Feature Learning by Deep Sparse Coding |  http://arxiv.org/abs/1312.5783  | author:Yunlong He, Koray Kavukcuoglu, Yun Wang, Arthur Szlam, Yanjun Qi category:cs.LG cs.CV cs.NE published:2013-12-20 summary:In this paper, we propose a new unsupervised feature learning framework,namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layerarchitecture for visual object recognition tasks. The main innovation of theframework is that it connects the sparse-encoders from different layers by asparse-to-dense module. The sparse-to-dense module is a composition of a localspatial pooling step and a low-dimensional embedding process, which takesadvantage of the spatial smoothness information in the image. As a result, thenew method is able to learn several levels of sparse representation of theimage which capture features at a variety of abstraction levels andsimultaneously preserve the spatial smoothness between the neighboring imagepatches. Combining the feature representations from multiple layers, DeepSCachieves the state-of-the-art performance on multiple object recognition tasks.
arxiv-1312-6002 | Stochastic Gradient Estimate Variance in Contrastive Divergence and Persistent Contrastive Divergence |  http://arxiv.org/abs/1312.6002  | author:Mathias Berglund, Tapani Raiko category:cs.NE cs.LG stat.ML 62M45 I.2.6 published:2013-12-20 summary:Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) arepopular methods for training the weights of Restricted Boltzmann Machines.However, both methods use an approximate method for sampling from the modeldistribution. As a side effect, these approximations yield significantlydifferent biases and variances for stochastic gradient estimates of individualdata points. It is well known that CD yields a biased gradient estimate. Inthis paper we however show empirically that CD has a lower stochastic gradientestimate variance than exact sampling, while the mean of subsequent PCDestimates has a higher variance than exact sampling. The results give oneexplanation to the finding that CD can be used with smaller minibatches orhigher learning rates than PCD.
arxiv-1312-6026 | How to Construct Deep Recurrent Neural Networks |  http://arxiv.org/abs/1312.6026  | author:Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML published:2013-12-20 summary:In this paper, we explore different ways to extend a recurrent neural network(RNN) to a \textit{deep} RNN. We start by arguing that the concept of depth inan RNN is not as clear as it is in feedforward neural networks. By carefullyanalyzing and understanding the architecture of an RNN, however, we find threepoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)hidden-to-hidden transition and (3) hidden-to-output function. Based on thisobservation, we propose two novel architectures of a deep RNN which areorthogonal to an earlier attempt of stacking multiple recurrent layers to builda deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide analternative interpretation of these deep RNNs using a novel framework based onneural operators. The proposed deep RNNs are empirically evaluated on the tasksof polyphonic music prediction and language modeling. The experimental resultsupports our claim that the proposed deep RNNs benefit from the depth andoutperform the conventional, shallow RNNs.
arxiv-1312-6055 | Unit Tests for Stochastic Optimization |  http://arxiv.org/abs/1312.6055  | author:Tom Schaul, Ioannis Antonoglou, David Silver category:cs.LG published:2013-12-20 summary:Optimization by stochastic gradient descent is an important component of manylarge-scale machine learning algorithms. A wide variety of such optimizationalgorithms have been devised; however, it is unclear whether these algorithmsare robust and widely applicable across many different optimization landscapes.In this paper we develop a collection of unit tests for stochasticoptimization. Each unit test rapidly evaluates an optimization algorithm on asmall-scale, isolated, and well-understood difficulty, rather than inreal-world scenarios where many such issues are entangled. Passing these unittests is not sufficient, but absolutely necessary for any algorithms withclaims to generality or robustness. We give initial quantitative andqualitative results on numerous established algorithms. The testing frameworkis open-source, extensible, and easy to apply to new algorithms.
arxiv-1312-6098 | On the number of response regions of deep feed forward networks with piece-wise linear activations |  http://arxiv.org/abs/1312.6098  | author:Razvan Pascanu, Guido Montufar, Yoshua Bengio category:cs.LG cs.NE published:2013-12-20 summary:This paper explores the complexity of deep feedforward networks with linearpre-synaptic couplings and rectified linear activations. This is a contributionto the growing body of work contrasting the representational power of deep andshallow network architectures. In particular, we offer a framework forcomparing deep and shallow models that belong to the family of piecewise linearfunctions based on computational geometry. We look at a deep rectifiermulti-layer perceptron (MLP) with linear outputs units and compare it with asingle layer version of the model. In the asymptotic regime, when the number ofinputs stays constant, if the shallow model has $kn$ hidden units and $n_0$inputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$layer model with $n$ hidden units on each layer it is $\Omega(\left\lfloor{n}/{n_0}\right\rfloor^{k-1}n^{n_0})$. The number$\left\lfloor{n}/{n_0}\right\rfloor^{k-1}$ grows faster than $k^{n_0}$ when $n$tends to infinity or when $k$ tends to infinity and $n \geq 2n_0$.Additionally, even when $k$ is small, if we restrict $n$ to be $2n_0$, we canshow that a deep model has considerably more linear regions that a shallow one.We consider this as a first step towards understanding the complexity of thesemodels and specifically towards providing suitable mathematical tools forfuture analysis.
arxiv-1312-5869 | Principled Non-Linear Feature Selection |  http://arxiv.org/abs/1312.5869  | author:Dimitrios Athanasakis, John Shawe-Taylor, Delmiro Fernandez-Reyes category:cs.LG published:2013-12-20 summary:Recent non-linear feature selection approaches employing greedy optimisationof Centred Kernel Target Alignment(KTA) exhibit strong results in terms ofgeneralisation accuracy and sparsity. However, they are computationallyprohibitive for large datasets. We propose randSel, a randomised featureselection algorithm, with attractive scaling properties. Our theoreticalanalysis of randSel provides strong probabilistic guarantees for correctidentification of relevant features. RandSel's characteristics make it an idealcandidate for identifying informative learned representations. We've conductedexperimentation to establish the performance of this approach, and presentencouraging results, including a 3rd position result in the recent ICML blackbox learning challenge as well as competitive results for signal peptideprediction, an important problem in bioinformatics.
arxiv-1312-5799 | Accelerated, Parallel and Proximal Coordinate Descent |  http://arxiv.org/abs/1312.5799  | author:Olivier Fercoq, Peter Richtárik category:math.OC cs.DC cs.NA math.NA stat.ML published:2013-12-20 summary:We propose a new stochastic coordinate descent method for minimizing the sumof convex functions each of which depends on a small number of coordinatesonly. Our method (APPROX) is simultaneously Accelerated, Parallel and PROXimal;this is the first time such a method is proposed. In the special case when thenumber of processors is equal to the number of coordinates, the methodconverges at the rate $2\bar{\omega}\bar{L} R^2/(k+1)^2 $, where $k$ is theiteration counter, $\bar{\omega}$ is an average degree of separability of theloss function, $\bar{L}$ is the average of Lipschitz constants associated withthe coordinates and individual functions in the sum, and $R$ is the distance ofthe initial point from the minimizer. We show that the method can beimplemented without the need to perform full-dimensional vector operations,which is the major bottleneck of existing accelerated coordinate descentmethods. The fact that the method depends on the average degree ofseparability, and not on the maximum degree of separability, can be attributedto the use of new safe large stepsizes, leading to improved expected separableoverapproximation (ESO). These are of independent interest and can be utilizedin all existing parallel stochastic coordinate descent algorithms based on theconcept of ESO.
arxiv-1312-5985 | Learning Type-Driven Tensor-Based Meaning Representations |  http://arxiv.org/abs/1312.5985  | author:Tamara Polajnar, Luana Fagarasan, Stephen Clark category:cs.CL cs.LG H.3.1 published:2013-12-20 summary:This paper investigates the learning of 3rd-order tensors representing thesemantics of transitive verbs. The meaning representations are part of atype-driven tensor-based semantic framework, from the newly emerging field ofcompositional distributional semantics. Standard techniques from the neuralnetworks literature are used to learn the tensors, which are tested on aselectional preference-style task with a simple 2-dimensional sentence space.Promising results are obtained against a competitive corpus-based baseline. Weargue that extending this work beyond transitive verbs, and tohigher-dimensional sentence spaces, is an interesting and challenging problemfor the machine learning community to consider.
arxiv-1312-6168 | Factorial Hidden Markov Models for Learning Representations of Natural Language |  http://arxiv.org/abs/1312.6168  | author:Anjan Nepal, Alexander Yates category:cs.LG cs.CL published:2013-12-20 summary:Most representation learning algorithms for language and image processing arelocal, in that they identify features for a data point based on surroundingpoints. Yet in language processing, the correct meaning of a word often dependson its global context. As a step toward incorporating global context intorepresentation learning, we develop a representation learning algorithm thatincorporates joint prediction into its technique for producing features for aword. We develop efficient variational methods for learning Factorial HiddenMarkov Models from large texts, and use variational distributions to producefeatures for each word that are sensitive to the entire input sequence, notjust to a local context window. Experiments on part-of-speech tagging andchunking indicate that the features are competitive with or better thanexisting state-of-the-art representation learning methods.
arxiv-1312-6157 | Distinction between features extracted using deep belief networks |  http://arxiv.org/abs/1312.6157  | author:Mohammad Pezeshki, Sajjad Gholami, Ahmad Nickabadi category:cs.LG cs.NE published:2013-12-20 summary:Data representation is an important pre-processing step in many machinelearning algorithms. There are a number of methods used for this task such asDeep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since someof the features extracted using automated feature extraction methods may notalways be related to a specific machine learning task, in this paper we proposetwo methods in order to make a distinction between extracted features based ontheir relevancy to the task. We applied these two methods to a Deep BeliefNetwork trained for a face recognition task.
arxiv-1312-5921 | Group-sparse Embeddings in Collective Matrix Factorization |  http://arxiv.org/abs/1312.5921  | author:Arto Klami, Guillaume Bouchard, Abhishek Tripathi category:stat.ML cs.LG published:2013-12-20 summary:CMF is a technique for simultaneously learning low-rank representations basedon a collection of matrices with shared entities. A typical example is thejoint modeling of user-item, item-property, and user-feature matrices in arecommender system. The key idea in CMF is that the embeddings are sharedacross the matrices, which enables transferring information between them. Theexisting solutions, however, break down when the individual matrices havelow-rank structure not shared with others. In this work we present a novel CMFsolution that allows each of the matrices to have a separate low-rank structurethat is independent of the other matrices, as well as structures that areshared only by a subset of them. We compare MAP and variational Bayesiansolutions based on alternating optimization algorithms and show that the modelautomatically infers the nature of each factor using group-wise sparsity. Ourapproach supports in a principled way continuous, binary and count observationsand is efficient for sparse matrices involving missing data. We illustrate thesolution on a number of examples, focusing in particular on an interestinguse-case of augmented multi-view learning.
arxiv-1312-6158 | Deep Belief Networks for Image Denoising |  http://arxiv.org/abs/1312.6158  | author:Mohammad Ali Keyvanrad, Mohammad Pezeshki, Mohammad Ali Homayounpour category:cs.LG cs.CV cs.NE published:2013-12-20 summary:Deep Belief Networks which are hierarchical generative models are effectivetools for feature representation and extraction. Furthermore, DBNs can be usedin numerous aspects of Machine Learning such as image denoising. In this paper,we propose a novel method for image denoising which relies on the DBNs' abilityin feature representation. This work is based upon learning of the noisebehavior. Generally, features which are extracted using DBNs are presented asthe values of the last layer nodes. We train a DBN a way that the networktotally distinguishes between nodes presenting noise and nodes presenting imagecontent in the last later of DBN, i.e. the nodes in the last layer of trainedDBN are divided into two distinct groups of nodes. After detecting the nodeswhich are presenting the noise, we are able to make the noise nodes inactiveand reconstruct a noiseless image. In section 4 we explore the results ofapplying this method on the MNIST dataset of handwritten digits which iscorrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% inaverage mean square error (MSE) was achieved when the proposed method was usedfor the reconstruction of the noisy images.
arxiv-1312-5813 | Unsupervised Pretraining Encourages Moderate-Sparseness |  http://arxiv.org/abs/1312.5813  | author:Jun Li, Wei Luo, Jian Yang, Xiaotong Yuan category:cs.LG cs.NE published:2013-12-20 summary:It is well known that direct training of deep neural networks will generallylead to poor results. A major progress in recent years is the invention ofvarious pretraining methods to initialize network parameters and it was shownthat such methods lead to good prediction performance. However, the reason forthe success of pretraining has not been fully understood, although it wasargued that regularization and better optimization play certain roles. Thispaper provides another explanation for the effectiveness of pretraining, wherewe show pretraining leads to a sparseness of hidden unit activation in theresulting neural networks. The main reason is that the pretraining models canbe interpreted as an adaptive sparse coding. Compared to deep neural networkwith sigmoid function, our experimental results on MNIST and Birdsong furthersupport this sparseness observation.
arxiv-1312-5412 | Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images |  http://arxiv.org/abs/1312.5412  | author:Taichi Kiwaki, Takaki Makino, Kazuyuki Aihara category:stat.ML cs.LG published:2013-12-19 summary:We pursue an early stopping technique that helps Gaussian RestrictedBoltzmann Machines (GRBMs) to gain good natural image representations in termsof overcompleteness and data fitting. GRBMs are widely considered as anunsuitable model for natural images because they gain non-overcompleterepresentations which include uniform filters that do not represent usefulimage features. We have recently found that GRBMs once gain and subsequentlylose useful filters during their training, contrary to this common perspective.We attribute this phenomenon to a tradeoff between overcompleteness of GRBMrepresentations and data fitting. To gain GRBM representations that areovercomplete and fit data well, we propose a measure for GRBM representationquality, approximated mutual information, and an early stopping technique basedon this measure. The proposed method boosts performance of classifiers trainedon GRBM representations.
arxiv-1312-5439 | Asynchronous Adaptation and Learning over Networks - Part III: Comparison Analysis |  http://arxiv.org/abs/1312.5439  | author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC published:2013-12-19 summary:In Part II [3] we carried out a detailed mean-square-error analysis of theperformance of asynchronous adaptation and learning over networks under afairly general model for asynchronous events including random topologies,random link failures, random data arrival times, and agents turning on and offrandomly. In this Part III, we compare the performance of synchronous andasynchronous networks. We also compare the performance of decentralizedadaptation against centralized stochastic-gradient (batch) solutions. Twointeresting conclusions stand out. First, the results establish that theperformance of adaptive networks is largely immune to the effect ofasynchronous events: the mean and mean-square convergence rates and theasymptotic bias values are not degraded relative to synchronous or centralizedimplementations. Only the steady-state mean-square-deviation suffers adegradation in the order of $\nu$, which represents the small step-sizeparameters used for adaptation. Second, the results show that the adaptivedistributed network matches the performance of the centralized solution. Theseconclusions highlight another critical benefit of cooperation by networkedagents: cooperation does not only enhance performance in comparison tostand-alone single-agent processing, but it also endows the network withremarkable resilience to various forms of random failure events and is able todeliver performance that is as powerful as batch solutions.
arxiv-1312-6117 | Comparison three methods of clustering: k-means, spectral clustering and hierarchical clustering |  http://arxiv.org/abs/1312.6117  | author:Kamran Kowsari category:cs.LG 68T10 H.3.3; I.5.3 published:2013-12-19 summary:Comparison of three kind of the clustering and find cost function and lossfunction and calculate them. Error rate of the clustering methods and how tocalculate the error percentage always be one on the important factor forevaluating the clustering methods, so this paper introduce one way to calculatethe error rate of clustering methods. Clustering algorithms can be divided intoseveral categories including partitioning clustering algorithms, hierarchicalalgorithms and density based algorithms. Generally speaking we should compareclustering algorithms by Scalability, Ability to work with different attribute,Clusters formed by conventional, Having minimal knowledge of the computer torecognize the input parameters, Classes for dealing with noise and extradeposition that same error rate for clustering a new data, Thus, there is noeffect on the input data, different dimensions of high levels, K-means is oneof the simplest approach to clustering that clustering is an unsupervisedproblem.
arxiv-1312-5434 | Asynchronous Adaptation and Learning over Networks --- Part I: Modeling and Stability Analysis |  http://arxiv.org/abs/1312.5434  | author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC published:2013-12-19 summary:In this work and the supporting Parts II [2] and III [3], we provide a ratherdetailed analysis of the stability and performance of asynchronous strategiesfor solving distributed optimization and adaptation problems over networks. Weexamine asynchronous networks that are subject to fairly general sources ofuncertainties, such as changing topologies, random link failures, random dataarrival times, and agents turning on and off randomly. Under this model, agentsin the network may stop updating their solutions or may stop sending orreceiving information in a random manner and without coordination with otheragents. We establish in Part I conditions on the first and second-order momentsof the relevant parameter distributions to ensure mean-square stable behavior.We derive in Part II expressions that reveal how the various parameters of theasynchronous behavior influence network performance. We compare in Part III theperformance of asynchronous networks to the performance of both centralizedsolutions and synchronous networks. One notable conclusion is that themean-square-error performance of asynchronous networks shows a degradation onlyof the order of $O(\nu)$, where $\nu$ is a small step-size parameter, while theconvergence rate remains largely unaltered. The results provide a solidjustification for the remarkable resilience of cooperative networks in the faceof random failures at multiple levels: agents, links, data arrivals, andtopology.
arxiv-1312-5438 | Asynchronous Adaptation and Learning over Networks - Part II: Performance Analysis |  http://arxiv.org/abs/1312.5438  | author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC published:2013-12-19 summary:In Part I \cite{Zhao13TSPasync1}, we introduced a fairly general model forasynchronous events over adaptive networks including random topologies, randomlink failures, random data arrival times, and agents turning on and offrandomly. We performed a stability analysis and established the notable factthat the network is still able to converge in the mean-square-error sense tothe desired solution. Once stable behavior is guaranteed, it becomes importantto evaluate how fast the iterates converge and how close they get to theoptimal solution. This is a demanding task due to the various asynchronousevents and due to the fact that agents influence each other. In this Part II,we carry out a detailed analysis of the mean-square-error performance ofasynchronous strategies for solving distributed optimization and adaptationproblems over networks. We derive analytical expressions for the mean-squareconvergence rate and the steady-state mean-square-deviation. The expressionsreveal how the various parameters of the asynchronous behavior influencenetwork performance. In the process, we establish the interesting conclusionthat even under the influence of asynchronous events, all agents in theadaptive network can still reach an $O(\nu^{1 + \gamma_o'})$ near-agreementwith some $\gamma_o' > 0$ while approaching the desired solution within$O(\nu)$ accuracy, where $\nu$ is proportional to the small step-size parameterfor adaptation.
arxiv-1312-5734 | Time-varying Learning and Content Analytics via Sparse Factor Analysis |  http://arxiv.org/abs/1312.5734  | author:Andrew S. Lan, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG math.OC stat.AP published:2013-12-19 summary:We propose SPARFA-Trace, a new machine learning-based framework fortime-varying learning and content analytics for education applications. Wedevelop a novel message passing-based, blind, approximate Kalman filter forsparse factor analysis (SPARFA), that jointly (i) traces learner conceptknowledge over time, (ii) analyzes learner concept knowledge state transitions(induced by interacting with learning resources, such as textbook sections,lecture videos, etc, or the forgetting effect), and (iii) estimates the contentorganization and intrinsic difficulty of the assessment questions. Thesequantities are estimated solely from binary-valued (correct/incorrect) gradedlearner response data and a summary of the specific actions each learnerperforms (e.g., answering a question or studying a learning resource) at eachtime instance. Experimental results on two online course datasets demonstratethat SPARFA-Trace is capable of tracing each learner's concept knowledgeevolution over time, as well as analyzing the quality and content organizationof learning resources, the question-concept associations, and the questionintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparableor better performance in predicting unobserved learner responses than existingcollaborative filtering and knowledge tracing approaches for personalizededucation.
arxiv-1312-5673 | Flower Pollination Algorithm for Global Optimization |  http://arxiv.org/abs/1312.5673  | author:Xin-She Yang category:math.OC cs.NE nlin.AO published:2013-12-19 summary:Flower pollination is an intriguing process in the natural world. Itsevolutionary characteristics can be used to design new optimization algorithms.In this paper, we propose a new algorithm, namely, flower pollinationalgorithm, inspired by the pollination process of flowers. We first use tentest functions to validate the new algorithm, and compare its performance withgenetic algorithms and particle swarm optimization. Our simulation results showthe flower algorithm is more efficient than both GA and PSO. We also use theflower algorithm to solve a nonlinear design benchmark, which shows theconvergence rate is almost exponential.
arxiv-1312-5479 | Sparse similarity-preserving hashing |  http://arxiv.org/abs/1312.5479  | author:Jonathan Masci, Alex M. Bronstein, Michael M. Bronstein, Pablo Sprechmann, Guillermo Sapiro category:cs.CV cs.DS published:2013-12-19 summary:In recent years, a lot of attention has been devoted to efficient nearestneighbor search by means of similarity-preserving hashing. One of the plightsof existing hashing techniques is the intrinsic trade-off between performanceand computational complexity: while longer hash codes allow for lower falsepositive rates, it is very difficult to increase the embedding dimensionalitywithout incurring in very high false negatives rates or prohibitingcomputational costs. In this paper, we propose a way to overcome thislimitation by enforcing the hash codes to be sparse. Sparse high-dimensionalcodes enjoy from the low false positive rates typical of long hashes, whilekeeping the false negative rates similar to those of a shorter dense hashingscheme with equal number of degrees of freedom. We use a tailored feed-forwardneural network for the hashing function. Extensive experimental evaluationinvolving visual and multi-modal data shows the benefits of the proposedmethod.
arxiv-1312-5602 | Playing Atari with Deep Reinforcement Learning |  http://arxiv.org/abs/1312.5602  | author:Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller category:cs.LG published:2013-12-19 summary:We present the first deep learning model to successfully learn controlpolicies directly from high-dimensional sensory input using reinforcementlearning. The model is a convolutional neural network, trained with a variantof Q-learning, whose input is raw pixels and whose output is a value functionestimating future rewards. We apply our method to seven Atari 2600 games fromthe Arcade Learning Environment, with no adjustment of the architecture orlearning algorithm. We find that it outperforms all previous approaches on sixof the games and surpasses a human expert on three of them.
arxiv-1312-5568 | An Adaptive Dictionary Learning Approach for Modeling Dynamical Textures |  http://arxiv.org/abs/1312.5568  | author:Xian Wei, Hao Shen, Martin Kleinsteuber category:cs.CV published:2013-12-19 summary:Video representation is an important and challenging task in the computervision community. In this paper, we assume that image frames of a moving scenecan be modeled as a Linear Dynamical System. We propose a sparse codingframework, named adaptive video dictionary learning (AVDL), to model a videoadaptively. The developed framework is able to capture the dynamics of a movingscene by exploring both sparse properties and the temporal correlations ofconsecutive video frames. The proposed method is compared with state of the artvideo processing methods on several benchmark data sequences, which exhibitappearance changes and heavy occlusions.
arxiv-1312-5604 | Learning Transformations for Classification Forests |  http://arxiv.org/abs/1312.5604  | author:Qiang Qiu, Guillermo Sapiro category:cs.CV cs.LG stat.ML published:2013-12-19 summary:This work introduces a transformation-based learner model for classificationforests. The weak learner at each split node plays a crucial role in aclassification tree. We propose to optimize the splitting objective by learninga linear transformation on subspaces using nuclear norm as the optimizationcriteria. The learned linear transformation restores a low-rank structure fordata from the same class, and, at the same time, maximizes the separationbetween different classes, thereby improving the performance of the splitfunction. Theoretical and experimental results support the proposed framework.
arxiv-1312-5766 | Structure-Aware Dynamic Scheduler for Parallel Machine Learning |  http://arxiv.org/abs/1312.5766  | author:Seunghak Lee, Jin Kyu Kim, Qirong Ho, Garth A. Gibson, Eric P. Xing category:stat.ML cs.LG published:2013-12-19 summary:Training large machine learning (ML) models with many variables or parameterscan take a long time if one employs sequential procedures even with stochasticupdates. A natural solution is to turn to distributed computing on a cluster;however, naive, unstructured parallelization of ML algorithms does not usuallylead to a proportional speedup and can even result in divergence, becausedependencies between model elements can attenuate the computational gains fromparallelization and compromise correctness of inference. Recent efforts towardthis issue have benefited from exploiting the static, a priori block structuresresiding in ML algorithms. In this paper, we take this path further byexploring the dynamic block structures and workloads therein present during MLprogram execution, which offers new opportunities for improving convergence,correctness, and load balancing in distributed ML. We propose and showcase ageneral-purpose scheduler, STRADS, for coordinating distributed updates in MLalgorithms, which harnesses the aforementioned opportunities in a systematicway. We provide theoretical guarantees for our scheduler, and demonstrate itsefficacy versus static block structures on Lasso and Matrix Factorization.
arxiv-1312-5697 | Using Web Co-occurrence Statistics for Improving Image Categorization |  http://arxiv.org/abs/1312.5697  | author:Samy Bengio, Jeff Dean, Dumitru Erhan, Eugene Ie, Quoc Le, Andrew Rabinovich, Jonathon Shlens, Yoram Singer category:cs.CV cs.LG published:2013-12-19 summary:Object recognition and localization are important tasks in computer vision.The focus of this work is the incorporation of contextual information in orderto improve object recognition and localization. For instance, it is natural toexpect not to see an elephant to appear in the middle of an ocean. We considera simple approach to encapsulate such common sense knowledge usingco-occurrence statistics from web documents. By merely counting the number oftimes nouns (such as elephants, sharks, oceans, etc.) co-occur in webdocuments, we obtain a good estimate of expected co-occurrences in visual data.We then cast the problem of combining textual co-occurrence statistics with thepredictions of image-based classifiers as an optimization problem. Theresulting optimization problem serves as a surrogate for our inferenceprocedure. Albeit the simplicity of the resulting optimization problem, it iseffective in improving both recognition and localization accuracy. Concretely,we observe significant improvements in recognition and localization rates forboth ImageNet Detection 2012 and Sun 2012 datasets.
arxiv-1312-5663 | k-Sparse Autoencoders |  http://arxiv.org/abs/1312.5663  | author:Alireza Makhzani, Brendan Frey category:cs.LG published:2013-12-19 summary:Recently, it has been observed that when representations are learnt in a waythat encourages sparsity, improved performance is obtained on classificationtasks. These methods involve combinations of activation functions, samplingsteps and different kinds of penalties. To investigate the effectiveness ofsparsity by itself, we propose the k-sparse autoencoder, which is anautoencoder with linear activation function, where in hidden layers only the khighest activities are kept. When applied to the MNIST and NORB datasets, wefind that this method achieves better classification results than denoisingautoencoders, networks trained with dropout, and RBMs. k-sparse autoencodersare simple to train and the encoding stage is very fast, making themwell-suited to large problem sizes, where conventional sparse coding algorithmscannot be applied.
arxiv-1312-5559 | Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds |  http://arxiv.org/abs/1312.5559  | author:Irina Sergienya, Hinrich Schütze category:cs.CL I.2.6; I.2.7 published:2013-12-19 summary:There are two main approaches to the distributed representation of words:low-dimensional deep learning embeddings and high-dimensional distributionalmodels, in which each dimension corresponds to a context word. In this paper,we combine these two approaches by learning embeddings based ondistributional-model vectors - as opposed to one-hot vectors as is standardlydone in deep learning. We show that the combined approach has betterperformance on a word relatedness judgment task.
arxiv-1312-5398 | Continuous Learning: Engineering Super Features With Feature Algebras |  http://arxiv.org/abs/1312.5398  | author:Michael Tetelman category:cs.LG stat.ML published:2013-12-19 summary:In this paper we consider a problem of searching a space of predictive modelsfor a given training data set. We propose an iterative procedure for deriving asequence of improving models and a corresponding sequence of sets of non-linearfeatures on the original input space. After a finite number of iterations N,the non-linear features become 2^N -degree polynomials on the original space.We show that in a limit of an infinite number of iterations derived non-linearfeatures must form an associative algebra: a product of two features is equalto a linear combination of features from the same feature space for any giveninput point. Because each iteration consists of solving a series of convexproblems that contain all previous solutions, the likelihood of the models inthe sequence is increasing with each iteration while the dimension of the modelparameter space is set to a limited controlled value.
arxiv-1312-5548 | My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013 |  http://arxiv.org/abs/1312.5548  | author:Jürgen Schmidhuber category:cs.NE published:2013-12-19 summary:Deep Learning has attracted significant attention in recent years. Here Ipresent a brief overview of my first Deep Learner of 1991, and its historiccontext, with a timeline of Deep Learning highlights.
arxiv-1312-5465 | Learning rates of $l^q$ coefficient regularization learning with Gaussian kernel |  http://arxiv.org/abs/1312.5465  | author:Shaobo Lin, Jinshan Zeng, Jian Fang, Zongben Xu category:cs.LG stat.ML 68T05 F.2.1 published:2013-12-19 summary:Regularization is a well recognized powerful strategy to improve theperformance of a learning machine and $l^q$ regularization schemes with$0<q<\infty$ are central in use. It is known that different $q$ leads todifferent properties of the deduced estimators, say, $l^2$ regularization leadsto smooth estimators while $l^1$ regularization leads to sparse estimators.Then, how does the generalization capabilities of $l^q$ regularization learningvary with $q$? In this paper, we study this problem in the framework ofstatistical learning theory and show that implementing $l^q$ coefficientregularization schemes in the sample dependent hypothesis space associated withGaussian kernel can attain the same almost optimal learning rates for all$0<q<\infty$. That is, the upper and lower bounds of learning rates for $l^q$regularization learning are asymptotically identical for all $0<q<\infty$. Ourfinding tentatively reveals that, in some modeling contexts, the choice of $q$might not have a strong impact with respect to the generalization capability.From this perspective, $q$ can be arbitrarily specified, or specified merely byother no generalization criteria like smoothness, computational complexity,sparsity, etc..
arxiv-1312-5457 | Codebook based Audio Feature Representation for Music Information Retrieval |  http://arxiv.org/abs/1312.5457  | author:Yonatan Vaizman, Brian McFee, Gert Lanckriet category:cs.IR cs.LG cs.MM published:2013-12-19 summary:Digital music has become prolific in the web in recent decades. Automatedrecommendation systems are essential for users to discover music they love andfor artists to reach appropriate audience. When manual annotations and userpreference data is lacking (e.g. for new artists) these systems must rely on\emph{content based} methods. Besides powerful machine learning tools forclassification and retrieval, a key component for successful recommendation isthe \emph{audio content representation}. Good representations should capture informative musical patterns in the audiosignal of songs. These representations should be concise, to enable efficient(low storage, easy indexing, fast search) management of huge musicrepositories, and should also be easy and fast to compute, to enable real-timeinteraction with a user supplying new songs to the system. Before designing new audio features, we explore the usage of traditionallocal features, while adding a stage of encoding with a pre-computed\emph{codebook} and a stage of pooling to get compact vectorialrepresentations. We experiment with different encoding methods, namely\emph{the LASSO}, \emph{vector quantization (VQ)} and \emph{cosine similarity(CS)}. We evaluate the representations' quality in two music informationretrieval applications: query-by-tag and query-by-example. Our results showthat concise representations can be used for successful performance in bothapplications. We recommend using top-$\tau$ VQ encoding, which consistentlyperforms well in both applications, and requires much less computation timethan the LASSO.
arxiv-1312-5578 | Multimodal Transitions for Generative Stochastic Networks |  http://arxiv.org/abs/1312.5578  | author:Sherjil Ozair, Li Yao, Yoshua Bengio category:cs.LG stat.ML published:2013-12-19 summary:Generative Stochastic Networks (GSNs) have been recently introduced as analternative to traditional probabilistic modeling: instead of parametrizing thedata distribution directly, one parametrizes a transition operator for a Markovchain whose stationary distribution is an estimator of the data generatingdistribution. The result of training is therefore a machine that generatessamples through this Markov chain. However, the previously introduced GSNconsistency theorems suggest that in order to capture a wide class ofdistributions, the transition operator in general should be multimodal,something that has not been done before this paper. We introduce for the firsttime multimodal transition distributions for GSNs, in particular using modelsin the NADE family (Neural Autoregressive Density Estimator) as outputdistributions of the transition operator. A NADE model is related to an RBM(and can thus model multimodal distributions) but its likelihood (andlikelihood gradient) can be computed easily. The parameters of the NADE areobtained as a learned function of the previous state of the learned Markovchain. Experiments clearly illustrate the advantage of such multimodaltransition distributions over unimodal GSNs.
arxiv-1312-5650 | Zero-Shot Learning by Convex Combination of Semantic Embeddings |  http://arxiv.org/abs/1312.5650  | author:Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S. Corrado, Jeffrey Dean category:cs.LG published:2013-12-19 summary:Several recent publications have proposed methods for mapping images intocontinuous semantic embedding spaces. In some cases the embedding space istrained jointly with the image transformation. In other cases the semanticembedding space is established by an independent natural language processingtask, and then the image transformation into that space is learned in a secondstage. Proponents of these image embedding systems have stressed theiradvantages over the traditional \nway{} classification framing of imageunderstanding, particularly in terms of the promise for zero-shot learning --the ability to correctly annotate images of previously unseen objectcategories. In this paper, we propose a simple method for constructing an imageembedding system from any existing \nway{} image classifier and a semantic wordembedding model, which contains the $\n$ class labels in its vocabulary. Ourmethod maps images into the semantic embedding space via convex combination ofthe class label embedding vectors, and requires no additional training. We showthat this simple and direct method confers many of the advantages associatedwith more complex image embedding schemes, and indeed outperforms state of theart methods on the ImageNet zero-shot learning task.
arxiv-1312-5770 | Consistency of Causal Inference under the Additive Noise Model |  http://arxiv.org/abs/1312.5770  | author:Samory Kpotufe, Eleni Sgouritsa, Dominik Janzing, Bernhard Schölkopf category:cs.LG stat.ML published:2013-12-19 summary:We analyze a family of methods for statistical causal inference from sampleunder the so-called Additive Noise Model. While most work on the subject hasconcentrated on establishing the soundness of the Additive Noise Model, thestatistical consistency of the resulting inference methods has received littleattention. We derive general conditions under which the given family ofinference methods consistently infers the causal direction in a nonparametricsetting.
arxiv-1401-6126 | Delegating Custom Object Detection Tasks to a Universal Classification System |  http://arxiv.org/abs/1401.6126  | author:Andrew Gleibman category:cs.CV 68T10 published:2013-12-19 summary:In this paper, a concept of multipurpose object detection system, recentlyintroduced in our previous work, is clarified. The business aspect of thismethod is transformation of a classifier into an object detector/locator via animage grid. This is a universal framework for locating objects of interestthrough classification. The framework standardizes and simplifiesimplementation of custom systems by doing only a custom analysis of theclassification results on the image grid.
arxiv-1312-5402 | Some Improvements on Deep Convolutional Neural Network Based Image Classification |  http://arxiv.org/abs/1312.5402  | author:Andrew G. Howard category:cs.CV published:2013-12-19 summary:We investigate multiple techniques to improve upon the current state of theart deep convolutional neural network based image classification pipeline. Thetechiques include adding more image transformations to training data, addingmore transformations to generate additional predictions at test time and usingcomplementary models applied to higher resolution images. This paper summarizesour entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Oursystem achieved a top 5 classification error rate of 13.55% using no externaldata which is over a 20% relative improvement on the previous year's winner.
arxiv-1312-5394 | Missing Value Imputation With Unsupervised Backpropagation |  http://arxiv.org/abs/1312.5394  | author:Michael S. Gashler, Michael R. Smith, Richard Morris, Tony Martinez category:cs.NE cs.LG stat.ML published:2013-12-19 summary:Many data mining and data analysis techniques operate on dense matrices orcomplete tables of data. Real-world data sets, however, often contain unknownvalues. Even many classification algorithms that are designed to operate withmissing values still exhibit deteriorated accuracy. One approach to handlingmissing values is to fill in (impute) the missing values. In this paper, wepresent a technique for unsupervised learning called UnsupervisedBackpropagation (UBP), which trains a multi-layer perceptron to fit to themanifold sampled by a set of observed point-vectors. We evaluate UBP with thetask of imputing missing values in datasets, and show that UBP is able topredict missing values with significantly lower sum-squared error than othercollaborative filtering and imputation techniques. We also demonstrate with 24datasets and 9 supervised learning algorithms that classification accuracy isusually higher when randomly-withheld values are imputed using UBP, rather thanwith other methods.
arxiv-1312-5542 | Word Emdeddings through Hellinger PCA |  http://arxiv.org/abs/1312.5542  | author:Rémi Lebret, Ronan Collobert category:cs.CL cs.LG published:2013-12-19 summary:Word embeddings resulting from neural language models have been shown to besuccessful for a large variety of NLP tasks. However, such architecture mightbe difficult to train and time-consuming. Instead, we propose to drasticallysimplify the word embeddings computation through a Hellinger PCA of the wordco-occurence matrix. We compare those new word embeddings with some well-knownembeddings on NER and movie review tasks and show that we can reach similar oreven better performance. Although deep learning is not really necessary forgenerating good word embeddings, we show that it can provide an easy way toadapt embeddings to specific tasks.
arxiv-1312-5386 | Detecting Parameter Symmetries in Probabilistic Models |  http://arxiv.org/abs/1312.5386  | author:Robert Nishihara, Thomas Minka, Daniel Tarlow category:stat.ML published:2013-12-19 summary:Probabilistic models often have parameters that can be translated, scaled,permuted, or otherwise transformed without changing the model. These symmetriescan lead to strong correlation and multimodality in the posterior distributionover the model's parameters, which can pose challenges both for performinginference and interpreting the results. In this work, we address the automaticdetection of common problematic model symmetries. To do so, we introduce localsymmetries, which cover many common cases and are amenable to automaticdetection. We show how to derive algorithms to detect several broad classes oflocal symmetries. Our algorithms are compatible with probabilistic programmingconstructs such as arrays, for loops, and if statements, and they scale tomodels with many variables.
arxiv-1312-5419 | Large-scale Multi-label Text Classification - Revisiting Neural Networks |  http://arxiv.org/abs/1312.5419  | author:Jinseok Nam, Jungi Kim, Eneldo Loza Mencía, Iryna Gurevych, Johannes Fürnkranz category:cs.LG published:2013-12-19 summary:Neural networks have recently been proposed for multi-label classificationbecause they are able to capture and model label dependencies in the outputlayer. In this work, we investigate limitations of BP-MLL, a neural network(NN) architecture that aims at minimizing pairwise ranking error. Instead, wepropose to use a comparably simple NN approach with recently proposed learningtechniques for large-scale multi-label text classification tasks. Inparticular, we show that BP-MLL's ranking loss minimization can be efficientlyand effectively replaced with the commonly used cross entropy error function,and demonstrate that several advances in neural network training that have beendeveloped in the realm of deep learning can be effectively employed in thissetting. Our experimental results show that simple NN models equipped withadvanced techniques such as rectified linear units, dropout, and AdaGradperform as well as or even outperform state-of-the-art approaches on sixlarge-scale textual datasets with diverse characteristics.
arxiv-1312-5192 | Nonlinear Eigenproblems in Data Analysis - Balanced Graph Cuts and the RatioDCA-Prox |  http://arxiv.org/abs/1312.5192  | author:Leonardo Jost, Simon Setzer, Matthias Hein category:stat.ML cs.LG math.OC published:2013-12-18 summary:It has been recently shown that a large class of balanced graph cuts allowsfor an exact relaxation into a nonlinear eigenproblem. We review briefly someof these results and propose a family of algorithms to compute nonlineareigenvectors which encompasses previous work as special cases. We provide adetailed analysis of the properties and the convergence behavior of thesealgorithms and then discuss their application in the area of balanced graphcuts.
arxiv-1312-5354 | Classification of Human Ventricular Arrhythmia in High Dimensional Representation Spaces |  http://arxiv.org/abs/1312.5354  | author:Yaqub Alwan, Zoran Cvetkovic, Michael Curtis category:cs.CE cs.LG published:2013-12-18 summary:We studied classification of human ECGs labelled as normal sinus rhythm,ventricular fibrillation and ventricular tachycardia by means of support vectormachines in different representation spaces, using different observationlengths. ECG waveform segments of duration 0.5-4 s, their Fourier magnitudespectra, and lower dimensional projections of Fourier magnitude spectra wereused for classification. All considered representations were of much higherdimension than in published studies. Classification accuracy improved withsegment duration up to 2 s, with 4 s providing little improvement. We foundthat it is possible to discriminate between ventricular tachycardia andventricular fibrillation by the present approach with much shorter runs of ECG(2 s, minimum 86% sensitivity per class) than previously imagined. Ensembles ofclassifiers acting on 1 s segments taken over 5 s observation windows gave bestresults, with sensitivities of detection for all classes exceeding 93%.
arxiv-1312-5242 | Unsupervised feature learning by augmenting single images |  http://arxiv.org/abs/1312.5242  | author:Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox category:cs.CV cs.LG cs.NE published:2013-12-18 summary:When deep learning is applied to visual object recognition, data augmentationis often used to generate additional training data without extra labeling cost.It helps to reduce overfitting and increase the performance of the algorithm.In this paper we investigate if it is possible to use data augmentation as themain component of an unsupervised feature learning architecture. To that end wesample a set of random image patches and declare each of them to be a separatesingle-image surrogate class. We then extend these trivial one-element classesby applying a variety of transformations to the initial 'seed' patches. Finallywe train a convolutional neural network to discriminate between these surrogateclasses. The feature representation learned by the network can then be used invarious vision tasks. We find that this simple feature learning algorithm issurprisingly successful, achieving competitive classification results onseveral popular vision datasets (STL-10, CIFAR-10, Caltech-101).
arxiv-1312-5047 | Stable Camera Motion Estimation Using Convex Programming |  http://arxiv.org/abs/1312.5047  | author:Onur Ozyesil, Amit Singer, Ronen Basri category:cs.CV published:2013-12-18 summary:We study the inverse problem of estimating n locations $t_1, ..., t_n$ (up toglobal scale, translation and negation) in $R^d$ from noisy measurements of asubset of the (unsigned) pairwise lines that connect them, that is, from noisymeasurements of $\pm (t_i - t_j)/\t_i - t_j\$ for some pairs (i,j) (where thesigns are unknown). This problem is at the core of the structure from motion(SfM) problem in computer vision, where the $t_i$'s represent camera locationsin $R^3$. The noiseless version of the problem, with exact line measurements,has been considered previously under the general title of parallel rigiditytheory, mainly in order to characterize the conditions for unique realizationof locations. For noisy pairwise line measurements, current methods tend toproduce spurious solutions that are clustered around a few locations. Thissensitivity of the location estimates is a well-known problem in SfM,especially for large, irregular collections of images. In this paper we introduce a semidefinite programming (SDP) formulation,specially tailored to overcome the clustering phenomenon. We further identifythe implications of parallel rigidity theory for the location estimationproblem to be well-posed, and prove exact (in the noiseless case) and stablelocation recovery results. We also formulate an alternating direction method tosolve the resulting semidefinite program, and provide a distributed version ofour formulation for large numbers of locations. Specifically for the cameralocation estimation problem, we formulate a pairwise line estimation methodbased on robust camera orientation and subspace estimation. Lastly, wedemonstrate the utility of our algorithm through experiments on real images.
arxiv-1312-5129 | Deep Learning Embeddings for Discontinuous Linguistic Units |  http://arxiv.org/abs/1312.5129  | author:Wenpeng Yin, Hinrich Schütze category:cs.CL published:2013-12-18 summary:Deep learning embeddings have been successfully used for many naturallanguage processing problems. Embeddings are mostly computed for word formsalthough a number of recent papers have extended this to other linguistic unitslike morphemes and phrases. In this paper, we argue that learning embeddingsfor discontinuous linguistic units should also be considered. In anexperimental evaluation on coreference resolution, we show that such embeddingsperform better than word form embeddings.
arxiv-1312-5370 | Perturbed Gibbs Samplers for Synthetic Data Release |  http://arxiv.org/abs/1312.5370  | author:Yubin Park, Joydeep Ghosh category:stat.ML stat.AP published:2013-12-18 summary:We propose a categorical data synthesizer with a quantifiable disclosurerisk. Our algorithm, named Perturbed Gibbs Sampler, can handle high-dimensionalcategorical data that are often intractable to represent as contingency tables.The algorithm extends a multiple imputation strategy for fully synthetic databy utilizing feature hashing and non-parametric distribution approximations.California Patient Discharge data are used to demonstrate statisticalproperties of the proposed synthesizing methodology. Marginal and conditionaldistributions, as well as the coefficients of regression models built on thesynthesized data are compared to those obtained from the original data.Intruder scenarios are simulated to evaluate disclosure risks of thesynthesized data from multiple angles. Limitations and extensions of theproposed algorithm are also discussed.
arxiv-1312-5021 | Efficient Online Bootstrapping for Large Scale Learning |  http://arxiv.org/abs/1312.5021  | author:Zhen Qin, Vaclav Petricek, Nikos Karampatziakis, Lihong Li, John Langford category:cs.LG published:2013-12-18 summary:Bootstrapping is a useful technique for estimating the uncertainty of apredictor, for example, confidence intervals for prediction. It is typicallyused on small to moderate sized datasets, due to its high computation cost.This work describes a highly scalable online bootstrapping strategy,implemented inside Vowpal Wabbit, that is several times faster than traditionalstrategies. Our experiments indicate that, in addition to providing a blackbox-like method for estimating uncertainty, our implementation of onlinebootstrapping may also help to train models with better prediction performancedue to model averaging.
arxiv-1312-5023 | Contextually Supervised Source Separation with Application to Energy Disaggregation |  http://arxiv.org/abs/1312.5023  | author:Matt Wytock, J. Zico Kolter category:stat.ML cs.LG math.OC published:2013-12-18 summary:We propose a new framework for single-channel source separation that liesbetween the fully supervised and unsupervised setting. Instead of supervision,we provide input features for each source signal and use convex methods toestimate the correlations between these features and the unobserved signaldecomposition. We analyze the case of $\ell_2$ loss theoretically and show thatrecovery of the signal components depends only on cross-correlation betweenfeatures for different signals, not on correlations between features for thesame signal. Contextually supervised source separation is a natural fit fordomains with large amounts of data but no explicit supervision; our motivatingapplication is energy disaggregation of hourly smart meter data (the separationof whole-home power signals into different energy uses). Here we applycontextual supervision to disaggregate the energy usage of thousands homes overfour years, a significantly larger scale than previously published efforts, anddemonstrate on synthetic data that our method outperforms the unsupervisedapproach.
arxiv-1312-5033 | Evaluation of Plane Detection with RANSAC According to Density of 3D Point Clouds |  http://arxiv.org/abs/1312.5033  | author:Tomofumi Fujiwara, Tetsushi Kamegawa, Akio Gofuku category:cs.RO cs.CV published:2013-12-18 summary:We have implemented a method that detects planar regions from 3D scan datausing Random Sample Consensus (RANSAC) algorithm to address the issue of atrade-off between the scanning speed and the point density of 3D scanning.However, the limitation of the implemented method has not been clear yet. Inthis paper, we conducted an additional experiment to evaluate the implementedmethod by changing its parameter and environments in both high and low pointdensity data. As a result, the number of detected planes in high point densitydata was different from that in low point density data with the same parametervalue.
arxiv-1312-5045 | Comparative analysis of evolutionary algorithms for image enhancement |  http://arxiv.org/abs/1312.5045  | author:Anupriya Gogna, Akash Tayal category:cs.CV cs.NE published:2013-12-18 summary:Evolutionary algorithms are metaheuristic techniques that derive inspirationfrom the natural process of evolution. They can efficiently solve (generateacceptable quality of solution in reasonable time) complex optimization(NP-Hard) problems. In this paper, automatic image enhancement is considered asan optimization problem and three evolutionary algorithms (Genetic Algorithm,Differential Evolution and Self Organizing Migration Algorithm) are employed tosearch for an optimum solution. They are used to find an optimum parameter setfor an image enhancement transfer function. The aim is to maximize a fitnesscriterion which is a measure of image contrast and the visibility of details inthe enhanced image. The enhancement results obtained using all threeevolutionary algorithms are compared amongst themselves and also with theoutput of histogram equalization method.
arxiv-1312-5066 | Functional Bipartite Ranking: a Wavelet-Based Filtering Approach |  http://arxiv.org/abs/1312.5066  | author:Stéphan Clémençon, Marine Depecker category:stat.ML published:2013-12-18 summary:It is the main goal of this article to address the bipartite ranking issuefrom the perspective of functional data analysis (FDA). Given a training set ofindependent realizations of a (possibly sampled) second-order random functionwith a (locally) smooth autocorrelation structure and to which a binary labelis randomly assigned, the objective is to learn a scoring function s withoptimal ROC curve. Based on linear/nonlinear wavelet-based approximations, itis shown how to select compact finite dimensional representations of the inputcurves adaptively, in order to build accurate ranking rules, using recentadvances in the ranking problem for multivariate data with binary feedback.Beyond theoretical considerations, the performance of the learning methods forfunctional bipartite ranking proposed in this paper are illustrated bynumerical experiments.
arxiv-1312-5355 | Generative NeuroEvolution for Deep Learning |  http://arxiv.org/abs/1312.5355  | author:Phillip Verbancsics, Josh Harguess category:cs.NE cs.CV published:2013-12-18 summary:An important goal for the machine learning (ML) community is to createapproaches that can learn solutions with human-level capability. One domainwhere humans have held a significant advantage is visual processing. Asignificant approach to addressing this gap has been machine learningapproaches that are inspired from the natural systems, such as artificialneural networks (ANNs), evolutionary computation (EC), and generative anddevelopmental systems (GDS). Research into deep learning has demonstrated thatsuch architectures can achieve performance competitive with humans on somevisual tasks; however, these systems have been primarily trained throughsupervised and unsupervised learning algorithms. Alternatively, research isshowing that evolution may have a significant role in the development of visualsystems. Thus this paper investigates the role neuro-evolution (NE) can take indeep learning. In particular, the Hypercube-based NeuroEvolution of AugmentingTopologies is a NE approach that can effectively learn large neural structuresby training an indirect encoding that compresses the ANN weight pattern as afunction of geometry. The results show that HyperNEAT struggles with performingimage classification by itself, but can be effective in training a featureextractor that other ML approaches can learn from. Thus NeuroEvolution combinedwith other ML methods provides an intriguing area of research that canreplicate the processes in nature.
arxiv-1312-5124 | Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the Score Matrix |  http://arxiv.org/abs/1312.5124  | author:Paul Fogel category:stat.AP cs.LG stat.ML published:2013-12-18 summary:Non-Negative Matrix Factorization, NMF, attempts to find a number ofarchetypal response profiles, or parts, such that any sample profile in thedataset can be approximated by a close profile among these archetypes or alinear combination of these profiles. The non-negativity constraint is imposedwhile estimating archetypal profiles, due to the non-negative nature of theobserved signal. Apart from non negativity, a volume constraint can be appliedon the Score matrix W to enhance the ability of learning parts of NMF. In thisreport, we describe a very simple algorithm, which in effect achieves volumeminimization, although indirectly.
arxiv-1312-5179 | The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited |  http://arxiv.org/abs/1312.5179  | author:Matthias Hein, Simon Setzer, Leonardo Jost, Syama Sundar Rangapuram category:stat.ML cs.LG math.OC published:2013-12-18 summary:Hypergraphs allow one to encode higher-order relationships in data and arethus a very flexible modeling tool. Current learning methods are either basedon approximations of the hypergraphs via graphs or on tensor methods which areonly applicable under special conditions. In this paper, we present a newlearning framework on hypergraphs which fully uses the hypergraph structure.The key element is a family of regularization functionals based on the totalvariation on hypergraphs.
arxiv-1312-5258 | On the Challenges of Physical Implementations of RBMs |  http://arxiv.org/abs/1312.5258  | author:Vincent Dumoulin, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2013-12-18 summary:Restricted Boltzmann machines (RBMs) are powerful machine learning models,but learning and some kinds of inference in the model require sampling-basedapproximations, which, in classical digital computers, are implemented usingexpensive MCMC. Physical computation offers the opportunity to reduce the costof sampling by building physical systems whose natural dynamics correspond todrawing samples from the desired RBM distribution. Such a system avoids theburn-in and mixing cost of a Markov chain. However, hardware implementations ofthis variety usually entail limitations such as low-precision and limited rangeof the parameters and restrictions on the size and topology of the RBM. Weconduct software simulations to determine how harmful each of theserestrictions is. Our simulations are designed to reproduce aspects of theD-Wave quantum computer, but the issues we investigate arise in most forms ofphysical computation.
arxiv-1312-5271 | Systematic and multifactor risk models revisited |  http://arxiv.org/abs/1312.5271  | author:Michel Fliess, Cédric Join category:q-fin.RM cs.CE math.LO q-fin.CP stat.ML published:2013-12-18 summary:Systematic and multifactor risk models are revisited via methods which werealready successfully developed in signal processing and in automatic control.The results, which bypass the usual criticisms on those risk modeling, areillustrated by several successful computer experiments.
arxiv-1312-5753 | SOMz: photometric redshift PDFs with self organizing maps and random atlas |  http://arxiv.org/abs/1312.5753  | author:M. Carrasco Kind, R. J. Brunner category:astro-ph.IM astro-ph.CO cs.LG stat.ML published:2013-12-18 summary:In this paper we explore the applicability of the unsupervised machinelearning technique of Self Organizing Maps (SOM) to estimate galaxy photometricredshift probability density functions (PDFs). This technique takes aspectroscopic training set, and maps the photometric attributes, but not theredshifts, to a two dimensional surface by using a process of competitivelearning where neurons compete to more closely resemble the training datamultidimensional space. The key feature of a SOM is that it retains thetopology of the input set, revealing correlations between the attributes thatare not easily identified. We test three different 2D topological mapping:rectangular, hexagonal, and spherical, by using data from the DEEP2 survey. Wealso explore different implementations and boundary conditions on the map andalso introduce the idea of a random atlas where a large number of differentmaps are created and their individual predictions are aggregated to produce amore robust photometric redshift PDF. We also introduced a new metric, the$I$-score, which efficiently incorporates different metrics, making it easierto compare different results (from different parameters or differentphotometric redshift codes). We find that by using a spherical topology mappingwe obtain a better representation of the underlying multidimensional topology,which provides more accurate results that are comparable to other,state-of-the-art machine learning algorithms. Our results illustrate thatunsupervised approaches have great potential for many astronomical problems,and in particular for the computation of photometric redshifts.
arxiv-1312-5198 | Learning Semantic Script Knowledge with Event Embeddings |  http://arxiv.org/abs/1312.5198  | author:Ashutosh Modi, Ivan Titov category:cs.LG cs.CL stat.ML I.2.6; I.2.7 published:2013-12-18 summary:Induction of common sense knowledge about prototypical sequences of eventshas recently received much attention. Instead of inducing this knowledge in theform of graphs, as in much of the previous work, in our method, distributedrepresentations of event realizations are computed based on distributedrepresentations of predicates and their arguments, and then theserepresentations are used to predict prototypical event orderings. Theparameters of the compositional process for computing the event representationsand the ranking component of the model are jointly estimated from texts. Weshow that this approach results in a substantial boost in ordering performancewith respect to previous methods.
arxiv-1312-4986 | A Comparative Evaluation of Curriculum Learning with Filtering and Boosting |  http://arxiv.org/abs/1312.4986  | author:Michael R. Smith, Tony Martinez category:cs.LG published:2013-12-17 summary:Not all instances in a data set are equally beneficial for inferring a modelof the data. Some instances (such as outliers) are detrimental to inferring amodel of the data. Several machine learning techniques treat instances in adata set differently during training such as curriculum learning, filtering,and boosting. However, an automated method for determining how beneficial aninstance is for inferring a model of the data does not exist. In this paper, wepresent an automated method that orders the instances in a data set bycomplexity based on the their likelihood of being misclassified (instancehardness). The underlying assumption of this method is that instances with ahigh likelihood of being misclassified represent more complex concepts in adata set. Ordering the instances in a data set allows a learning algorithm tofocus on the most beneficial instances and ignore the detrimental ones. Wecompare ordering the instances in a data set in curriculum learning, filteringand boosting. We find that ordering the instances significantly increasesclassification accuracy and that filtering has the largest impact onclassification accuracy. On a set of 52 data sets, ordering the instancesincreases the average accuracy from 81% to 84%.
arxiv-1401-3615 | Performance Engineering for a Medical Imaging Application on the Intel Xeon Phi Accelerator |  http://arxiv.org/abs/1401.3615  | author:Johannes Hofmann, Jan Treibig, Georg Hager, Gerhard Wellein category:cs.DC cs.CV cs.PF published:2013-12-17 summary:We examine the Xeon Phi, which is based on Intel's Many Integrated Coresarchitecture, for its suitability to run the FDK algorithm--the most commonlyused algorithm to perform the 3D image reconstruction in cone-beam computedtomography. We study the challenges of efficiently parallelizing theapplication and means to enable sensible data sharing between threads despitethe lack of a shared last level cache. Apart from parallelization, SIMDvectorization is critical for good performance on the Xeon Phi; we performvarious micro-benchmarks to investigate the platform's new set of vectorinstructions and put a special emphasis on the newly introduced vector gathercapability. We refine a previous performance model for the application andadapt it for the Xeon Phi to validate the performance of our optimizedhand-written assembly implementation, as well as the performance of severaldifferent auto-vectorization approaches.
arxiv-1312-4659 | DeepPose: Human Pose Estimation via Deep Neural Networks |  http://arxiv.org/abs/1312.4659  | author:Alexander Toshev, Christian Szegedy category:cs.CV published:2013-12-17 summary:We propose a method for human pose estimation based on Deep Neural Networks(DNNs). The pose estimation is formulated as a DNN-based regression problemtowards body joints. We present a cascade of such DNN regressors which resultsin high precision pose estimates. The approach has the advantage of reasoningabout pose in a holistic fashion and has a simple but yet powerful formulationwhich capitalizes on recent advances in Deep Learning. We present a detailedempirical analysis with state-of-art or better performance on four academicbenchmarks of diverse real-world images.
arxiv-1312-4695 | Sparse, complex-valued representations of natural sounds learned with phase and amplitude continuity priors |  http://arxiv.org/abs/1312.4695  | author:Wiktor Mlynarski category:cs.LG cs.SD q-bio.NC published:2013-12-17 summary:Complex-valued sparse coding is a data representation which employs adictionary of two-dimensional subspaces, while imposing a sparse, factorialprior on complex amplitudes. When trained on a dataset of natural imagepatches, it learns phase invariant features which closely resemble receptivefields of complex cells in the visual cortex. Features trained on naturalsounds however, rarely reveal phase invariance and capture other aspects of thedata. This observation is a starting point of the present work. As its firstcontribution, it provides an analysis of natural sound statistics by means oflearning sparse, complex representations of short speech intervals. Secondly,it proposes priors over the basis function set, which bias them towardsphase-invariant solutions. In this way, a dictionary of complex basis functionscan be learned from the data statistics, while preserving the phase invarianceproperty. Finally, representations trained on speech sounds with and withoutpriors are compared. Prior-based basis functions reveal performance comparableto unconstrained sparse coding, while explicitely representing phase as atemporal shift. Such representations can find applications in many perceptualand machine learning tasks.
arxiv-1312-4717 | The Matrix Ridge Approximation: Algorithms and Applications |  http://arxiv.org/abs/1312.4717  | author:Zhihua Zhang category:stat.ML published:2013-12-17 summary:We are concerned with an approximation problem for a symmetric positivesemidefinite matrix due to motivation from a class of nonlinear machinelearning methods. We discuss an approximation approach that we call {matrixridge approximation}. In particular, we define the matrix ridge approximationas an incomplete matrix factorization plus a ridge term. Moreover, we presentprobabilistic interpretations using a normal latent variable model and aWishart model for this approximation approach. The idea behind the latentvariable model in turn leads us to an efficient EM iterative method forhandling the matrix ridge approximation problem. Finally, we illustrate theapplications of the approximation approach in multivariate data analysis.Empirical studies in spectral clustering and Gaussian process regression showthat the matrix ridge approximation with the EM iteration is potentiallyuseful.
arxiv-1312-4617 | A Survey of Data Mining Techniques for Social Media Analysis |  http://arxiv.org/abs/1312.4617  | author:Mariam Adedoyin-Olowe, Mohamed Medhat Gaber, Frederic Stahl category:cs.SI cs.CL published:2013-12-17 summary:Social network has gained remarkable attention in the last decade. Accessingsocial network sites such as Twitter, Facebook LinkedIn and Google+ through theinternet and the web 2.0 technologies has become more affordable. People arebecoming more interested in and relying on social network for information, newsand opinion of other users on diverse subject matters. The heavy reliance onsocial network sites causes them to generate massive data characterised bythree computational issues namely; size, noise and dynamism. These issues oftenmake social network data very complex to analyse manually, resulting in thepertinent use of computational means of analysing them. Data mining provides awide range of techniques for detecting useful knowledge from massive datasetslike trends, patterns and rules [44]. Data mining techniques are used forinformation retrieval, statistical modelling and machine learning. Thesetechniques employ data pre-processing, data analysis, and data interpretationprocesses in the course of data analysis. This survey discusses different datamining techniques used in mining diverse aspects of the social network overdecades going from the historical techniques to the up-to-date models,including our novel technique named TRCM. All the techniques covered in thissurvey are listed in the Table.1 including the tools employed as well as namesof their authors.
arxiv-1312-4895 | Recursive Compressed Sensing |  http://arxiv.org/abs/1312.4895  | author:Nikolaos M. Freris, Orhan Öçal, Martin Vetterli category:stat.ML cs.IT math.IT 94 published:2013-12-17 summary:We introduce a recursive algorithm for performing compressed sensing onstreaming data. The approach consists of a) recursive encoding, where we samplethe input stream via overlapping windowing and make use of the previousmeasurement in obtaining the next one, and b) recursive decoding, where thesignal estimate from the previous window is utilized in order to achieve fasterconvergence in an iterative optimization scheme applied to decode the new one.To remove estimation bias, a two-step estimation procedure is proposedcomprising support set detection and signal amplitude estimation. Estimationaccuracy is enhanced by a non-linear voting method and averaging estimates overmultiple windows. We analyze the computational complexity and estimation error,and show that the normalized error variance asymptotically goes to zero forsublinear sparsity. Our simulation results show speed up of an order ofmagnitude over traditional CS, while obtaining significantly lowerreconstruction error under mild conditions on the signal magnitudes and thenoise level.
arxiv-1312-4752 | BW - Eye Ophthalmologic decision support system based on clinical workflow and data mining techniques-image registration algorithm |  http://arxiv.org/abs/1312.4752  | author:Ricardo Martins category:cs.CV published:2013-12-17 summary:Blueworks - Medical Expert Diagnosis is developing an application, BWEye, tobe used as an ophthalmology consultation decision support system. Theimplementation of this application involves several different tasks and one ofthem is the implementation of an ophthalmology images registration algorithm.The work reported in this document is related with the implementation of analgorithm to register images of angiography, colour retinography and redfreeretinography. The implementations described were developed in the softwareMATLAB. The implemented algorithm is based in the detection of the bifurcation points(y-features) of the vascular structures of the retina that usually are visiblein the referred type of images. There are proposed two approaches to establishan initial set of features correspondences. The first approach is based in themaximization of the mutual information of the bifurcation regions of thefeatures of images. The second approach is based in the characterization ofeach bifurcation point and in the minimization of the Euclidean distancebetween the descriptions of the features of the images in the descriptorsspace. The final set of the matching features for a pair of images is definedthrough the application of the RANSAC algorithm. Although, it was not achieved the implementation of a full functionalalgorithm, there were made several analysis that can be important to futureimprovement of the current implementation.
arxiv-1312-4746 | Co-Sparse Textural Similarity for Image Segmentation |  http://arxiv.org/abs/1312.4746  | author:Claudia Nieuwenhuis, Daniel Cremers, Simon Hawe, Martin Kleinsteuber category:cs.CV published:2013-12-17 summary:We propose an algorithm for segmenting natural images based on texture andcolor information, which leverages the co-sparse analysis model for imagesegmentation within a convex multilabel optimization framework. As a keyingredient of this method, we introduce a novel textural similarity measure,which builds upon the co-sparse representation of image patches. We propose aBayesian approach to merge textural similarity with information about color andlocation. Combined with recently developed convex multilabel optimizationmethods this leads to an efficient algorithm for both supervised andunsupervised segmentation, which is easily parallelized on graphics hardware.The approach provides competitive results in unsupervised segmentation andoutperforms state-of-the-art interactive segmentation methods on the GrazBenchmark.
arxiv-1312-4664 | Filtering with State-Observation Examples via Kernel Monte Carlo Filter |  http://arxiv.org/abs/1312.4664  | author:Motonobu Kanagawa, Yu Nishiyama, Arthur Gretton, Kenji Fukumizu category:stat.ML published:2013-12-17 summary:This paper addresses the problem of filtering with a state-space model.Standard approaches for filtering assume that a probabilistic model forobservations (i.e. the observation model) is given explicitly or at leastparametrically. We consider a setting where this assumption is not satisfied;we assume that the knowledge of the observation model is only provided byexamples of state-observation pairs. This setting is important and appears whenstate variables are defined as quantities that are very different from theobservations. We propose Kernel Monte Carlo Filter, a novel filtering methodthat is focused on this setting. Our approach is based on the framework ofkernel mean embeddings, which enables nonparametric posterior inference usingthe state-observation examples. The proposed method represents statedistributions as weighted samples, propagates these samples by sampling,estimates the state posteriors by Kernel Bayes' Rule, and resamples by KernelHerding. In particular, the sampling and resampling procedures are novel inbeing expressed using kernel mean embeddings, so we theoretically analyze theirbehaviors. We reveal the following properties, which are similar to those ofcorresponding procedures in particle methods: (1) the performance of samplingcan degrade if the effective sample size of a weighted sample is small; (2)resampling improves the sampling performance by increasing the effective samplesize. We first demonstrate these theoretical findings by synthetic experiments.Then we show the effectiveness of the proposed filter by artificial and realdata experiments, which include vision-based mobile robot localization.
arxiv-1312-4852 | Identification of Gaussian Process State-Space Models with Particle Stochastic Approximation EM |  http://arxiv.org/abs/1312.4852  | author:Roger Frigola, Fredrik Lindsten, Thomas B. Schön, Carl E. Rasmussen category:stat.ML cs.SY published:2013-12-17 summary:Gaussian process state-space models (GP-SSMs) are a very flexible family ofmodels of nonlinear dynamical systems. They comprise a Bayesian nonparametricrepresentation of the dynamics of the system and additional (hyper-)parametersgoverning the properties of this nonparametric representation. The Bayesianformalism enables systematic reasoning about the uncertainty in the systemdynamics. We present an approach to maximum likelihood identification of theparameters in GP-SSMs, while retaining the full nonparametric description ofthe dynamics. The method is based on a stochastic approximation version of theEM algorithm that employs recent developments in particle Markov chain MonteCarlo for efficient identification.
arxiv-1312-4710 | Markov Network Structure Learning via Ensemble-of-Forests Models |  http://arxiv.org/abs/1312.4710  | author:Eirini Arvaniti, Manfred Claassen category:stat.ML published:2013-12-17 summary:Real world systems typically feature a variety of different dependency typesand topologies that complicate model selection for probabilistic graphicalmodels. We introduce the ensemble-of-forests model, a generalization of theensemble-of-trees model. Our model enables structure learning of Markov randomfields (MRF) with multiple connected components and arbitrary potentials. Wepresent two approximate inference techniques for this model and demonstratetheir performance on synthetic data. Our results suggest that theensemble-of-forests approach can accurately recover sparse, possiblydisconnected MRF topologies, even in presence of non-Gaussian dependenciesand/or low sample size. We applied the ensemble-of-forests model to learn thestructure of perturbed signaling networks of immune cells and found that thesefrequently exhibit non-Gaussian dependencies with disconnected MRF topologies.In summary, we expect that the ensemble-of-forests model will enable MRFstructure learning in other high dimensional real world settings that aregoverned by non-trivial dependencies.
arxiv-1312-4719 | The Bernstein Function: A Unifying Framework of Nonconvex Penalization in Sparse Estimation |  http://arxiv.org/abs/1312.4719  | author:Zhihua Zhang category:stat.ML published:2013-12-17 summary:In this paper we study nonconvex penalization using Bernstein functions.Since the Bernstein function is concave and nonsmooth at the origin, it caninduce a class of nonconvex functions for high-dimensional sparse estimationproblems. We derive a threshold function based on the Bernstein penalty andgive its mathematical properties in sparsity modeling. We show that acoordinate descent algorithm is especially appropriate for penalized regressionproblems with the Bernstein penalty. Additionally, we prove that the Bernsteinfunction can be defined as the concave conjugate of a $\varphi$-divergence anddevelop a conjugate maximization algorithm for finding the sparse solution.Finally, we particularly exemplify a family of Bernstein nonconvex penaltiesbased on a generalized Gamma measure and conduct empirical analysis for thisfamily.
arxiv-1312-6872 | Matrix recovery using Split Bregman |  http://arxiv.org/abs/1312.6872  | author:Anupriya Gogna, Ankita Shukla, Angshul Majumdar category:cs.NA cs.LG published:2013-12-17 summary:In this paper we address the problem of recovering a matrix, with inherentlow rank structure, from its lower dimensional projections. This problem isfrequently encountered in wide range of areas including pattern recognition,wireless sensor networks, control systems, recommender systems, image/videoreconstruction etc. Both in theory and practice, the most optimal way to solvethe low rank matrix recovery problem is via nuclear norm minimization. In thispaper, we propose a Split Bregman algorithm for nuclear norm minimization. Theuse of Bregman technique improves the convergence speed of our algorithm andgives a higher success rate. Also, the accuracy of reconstruction is muchbetter even for cases where small number of linear measurements are available.Our claim is supported by empirical results obtained using our algorithm andits comparison to other existing methods for matrix recovery. The algorithmsare compared on the basis of NMSE, execution time and success rate for varyingranks and sampling ratios.
arxiv-1312-4824 | Generation, Implementation and Appraisal of an N-gram based Stemming Algorithm |  http://arxiv.org/abs/1312.4824  | author:B. P. Pande, Pawan Tamta, H. S. Dhami category:cs.IR cs.CL published:2013-12-17 summary:A language independent stemmer has always been looked for. Single N-gramtokenization technique works well, however, it often generates stems that startwith intermediate characters, rather than initial ones. We present a noveltechnique that takes the concept of N gram stemming one step ahead and compareour method with an established algorithm in the field, Porter's Stemmer.Results indicate that our N gram stemmer is not inferior to Porter's linguisticstemmer.
arxiv-1312-4894 | Deep Convolutional Ranking for Multilabel Image Annotation |  http://arxiv.org/abs/1312.4894  | author:Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander Toshev, Sergey Ioffe category:cs.CV published:2013-12-17 summary:Multilabel image annotation is one of the most important challenges incomputer vision with many real-world applications. While existing work usuallyuse conventional visual features for multilabel annotation, features based onDeep Neural Networks have shown potential to significantly boost performance.In this work, we propose to leverage the advantage of such features and analyzekey components that lead to better performances. Specifically, we show that asignificant performance gain could be obtained by combining convolutionalarchitectures with approximate top-$k$ ranking objectives, as thye naturallyfit the multilabel tagging problem. Our experiments on the NUS-WIDE datasetoutperforms the conventional visual features by about 10%, obtaining the bestreported performance in the literature.
arxiv-1312-4706 | Designing Spontaneous Speech Search Interface for Historical Archives |  http://arxiv.org/abs/1312.4706  | author:Donna Vakharia, Rachel Gibbs category:cs.HC cs.CL published:2013-12-17 summary:Spontaneous speech in the form of conversations, meetings, voice-mail,interviews, oral history, etc. is one of the most ubiquitous forms of humancommunication. Search engines providing access to such speech collections havethe potential to better inform intelligence and make relevant data over vastaudio/video archives available to users. This project presents a search userinterface design supporting search tasks over a speech collection consisting ofan historical archive with nearly 52,000 audiovisual testimonies of survivorsand witnesses of the Holocaust and other genocides. The design incorporatesfaceted search, along with other UI elements like highlighted search items,tags, snippets, etc., to promote discovery and exploratory search. Twodifferent designs have been created to support both manual and automatedtranscripts. Evaluation was performed using human subjects to measure accuracyin retrieving results, understanding user-perspective on the design elements,and ease of parsing information.
arxiv-1312-4599 | Evolution and Computational Learning Theory: A survey on Valiant's paper |  http://arxiv.org/abs/1312.4599  | author:Arka Bhattacharya category:cs.LG 68Q32 published:2013-12-17 summary:Darwin's theory of evolution is considered to be one of the greatestscientific gems in modern science. It not only gives us a description of howliving things evolve, but also shows how a population evolves through time andalso, why only the fittest individuals continue the generation forward. Thepaper basically gives a high level analysis of the works of Valiant[1]. Though,we know the mechanisms of evolution, but it seems that there does not exist anystrong quantitative and mathematical theory of the evolution of certainmechanisms. What is defined exactly as the fitness of an individual, why isthat only certain individuals in a population tend to mutate, how computationis done in finite time when we have exponentially many examples: there seems tobe a lot of questions which need to be answered. [1] basically treats Darwiniantheory as a form of computational learning theory, which calculates the netfitness of the hypotheses and thus distinguishes functions and their classeswhich could be evolvable using polynomial amount of resources. Evolution isconsidered as a function of the environment and the previous evolutionarystages that chooses the best hypothesis using learning techniques that makesmutation possible and hence, gives a quantitative idea that why only thefittest individuals tend to survive and have the power to mutate.
arxiv-1312-4967 | Estimation of Human Body Shape and Posture Under Clothing |  http://arxiv.org/abs/1312.4967  | author:Stefanie Wuhrer, Leonid Pishchulin, Alan Brunton, Chang Shu, Jochen Lang category:cs.CV cs.GR published:2013-12-17 summary:Estimating the body shape and posture of a dressed human subject in motionrepresented as a sequence of (possibly incomplete) 3D meshes is important forvirtual change rooms and security. To solve this problem, statistical shapespaces encoding human body shape and posture variations are commonly used toconstrain the search space for the shape estimate. In this work, we propose anovel method that uses a posture-invariant shape space to model body shapevariation combined with a skeleton-based deformation to model posturevariation. Our method can estimate the body shape and posture of both staticscans and motion sequences of dressed human body scans. In case of motionsequences, our method takes advantage of motion cues to solve for a single bodyshape estimate along with a sequence of posture estimates. We apply ourapproach to both static scans and motion sequences and demonstrate that usingour method, higher fitting accuracy is achieved than when using a variant ofthe popular SCAPE model as statistical model.
arxiv-1312-4605 | Parallelizing MCMC via Weierstrass Sampler |  http://arxiv.org/abs/1312.4605  | author:Xiangyu Wang, David B. Dunson category:stat.CO cs.DC stat.ML published:2013-12-17 summary:With the rapidly growing scales of statistical problems, subset basedcommunication-free parallel MCMC methods are a promising future for large scaleBayesian analysis. In this article, we propose a new Weierstrass sampler forparallel MCMC based on independent subsets. The new sampler approximates thefull data posterior samples via combining the posterior draws from independentsubset MCMC chains, and thus enjoys a higher computational efficiency. We showthat the approximation error for the Weierstrass sampler is bounded by sometuning parameters and provide suggestions for choice of the values. Simulationstudy shows the Weierstrass sampler is very competitive compared to othermethods for combining MCMC chains generated for subsets, including averagingand kernel smoothing.
arxiv-1312-4740 | Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data |  http://arxiv.org/abs/1312.4740  | author:Yalong Bai, Kuiyuan Yang, Wei Yu, Wei-Ying Ma, Tiejun Zhao category:cs.CV published:2013-12-17 summary:Image retrieval refers to finding relevant images from an image database fora query, which is considered difficult for the gap between low-levelrepresentation of images and high-level representation of queries. Recentlyfurther developed Deep Neural Network sheds light on automatically learninghigh-level image representation from raw pixels. In this paper, we proposed amulti-task DNN learned for image retrieval, which contains two parts, i.e.,query-sharing layers for image representation computation and query-specificlayers for relevance estimation. The weights of multi-task DNN are learned onclickthrough data by Ring Training. Experimental results on both simulated andreal dataset show the effectiveness of the proposed method.
arxiv-1312-4626 | Compact Random Feature Maps |  http://arxiv.org/abs/1312.4626  | author:Raffay Hamid, Ying Xiao, Alex Gittens, Dennis DeCoste category:stat.ML cs.LG published:2013-12-17 summary:Kernel approximation using randomized feature maps has recently gained a lotof interest. In this work, we identify that previous approaches for polynomialkernel approximation create maps that are rank deficient, and therefore do notutilize the capacity of the projected feature space effectively. To addressthis challenge, we propose compact random feature maps (CRAFTMaps) toapproximate polynomial kernels more concisely and accurately. We prove theerror bounds of CRAFTMaps demonstrating their superior kernel reconstructionperformance compared to the previous approximation schemes. We show howstructured random matrices can be used to efficiently generate CRAFTMaps, andpresent a single-pass algorithm using CRAFTMaps to learn non-linear multi-classclassifiers. We present experiments on multiple standard data-sets withperformance competitive with state-of-the-art results.
arxiv-1312-4637 | Constraint Reduction using Marginal Polytope Diagrams for MAP LP Relaxations |  http://arxiv.org/abs/1312.4637  | author:Zhen Zhang, Qinfeng Shi, Yanning Zhang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.AI published:2013-12-17 summary:LP relaxation-based message passing algorithms provide an effective tool forMAP inference over Probabilistic Graphical Models. However, different LPrelaxations often have different objective functions and variables of differingdimensions, which presents a barrier to effective comparison and analysis. Inaddition, the computational complexity of LP relaxation-based methods growsquickly with the number of constraints. Reducing the number of constraintswithout sacrificing the quality of the solutions is thus desirable. We propose a unified formulation under which existing MAP LP relaxations maybe compared and analysed. Furthermore, we propose a new tool called MarginalPolytope Diagrams. Some properties of Marginal Polytope Diagrams are exploitedsuch as node redundancy and edge equivalence. We show that using MarginalPolytope Diagrams allows the number of constraints to be reduced withoutloosening the LP relaxations. Then, using Marginal Polytope Diagrams andconstraint reduction, we develop three novel message passing algorithms, anddemonstrate that two of these show a significant improvement in speed overstate-of-art algorithms while delivering a competitive, and sometimes higher,quality of solution.
arxiv-1312-6150 | A Review on Automated Brain Tumor Detection and Segmentation from MRI of Brain |  http://arxiv.org/abs/1312.6150  | author:Sudipta Roy, Sanjay Nag, Indra Kanta Maitra, Samir Kumar Bandyopadhyay category:cs.CV published:2013-12-16 summary:Tumor segmentation from magnetic resonance imaging (MRI) data is an importantbut time consuming manual task performed by medical experts. Automating thisprocess is a challenging task because of the high diversity in the appearanceof tumor tissues among different patients and in many cases similarity with thenormal tissues. MRI is an advanced medical imaging technique providing richinformation about the human soft-tissue anatomy. There are different braintumor detection and segmentation methods to detect and segment a brain tumorfrom MRI images. These detection and segmentation approaches are reviewed withan importance placed on enlightening the advantages and drawbacks of thesemethods for brain tumor detection and segmentation. The use of MRI imagedetection and segmentation in different procedures are also described. Here abrief review of different segmentation for detection of brain tumor from MRI ofbrain has been discussed.
arxiv-1312-4400 | Network In Network |  http://arxiv.org/abs/1312.4400  | author:Min Lin, Qiang Chen, Shuicheng Yan category:cs.NE cs.CV cs.LG published:2013-12-16 summary:We propose a novel deep network structure called "Network In Network" (NIN)to enhance model discriminability for local patches within the receptive field.The conventional convolutional layer uses linear filters followed by anonlinear activation function to scan the input. Instead, we build micro neuralnetworks with more complex structures to abstract the data within the receptivefield. We instantiate the micro neural network with a multilayer perceptron,which is a potent function approximator. The feature maps are obtained bysliding the micro networks over the input in a similar manner as CNN; they arethen fed into the next layer. Deep NIN can be implemented by stacking mutipleof the above described structure. With enhanced local modeling via the micronetwork, we are able to utilize global average pooling over feature maps in theclassification layer, which is easier to interpret and less prone tooverfitting than traditional fully connected layers. We demonstrated thestate-of-the-art classification performances with NIN on CIFAR-10 andCIFAR-100, and reasonable performances on SVHN and MNIST datasets.
arxiv-1312-4426 | Optimization for Compressed Sensing: the Simplex Method and Kronecker Sparsification |  http://arxiv.org/abs/1312.4426  | author:Robert Vanderbei, Han Liu, Lie Wang, Kevin Lin category:stat.ML cs.LG published:2013-12-16 summary:In this paper we present two new approaches to efficiently solve large-scalecompressed sensing problems. These two ideas are independent of each other andcan therefore be used either separately or together. We consider allpossibilities. For the first approach, we note that the zero vector can be taken as theinitial basic (infeasible) solution for the linear programming problem andtherefore, if the true signal is very sparse, some variants of the simplexmethod can be expected to take only a small number of pivots to arrive at asolution. We implemented one such variant and demonstrate a dramaticimprovement in computation time on very sparse signals. The second approach requires a redesigned sensing mechanism in which thevector signal is stacked into a matrix. This allows us to exploit the Kroneckercompressed sensing (KCS) mechanism. We show that the Kronecker sensing requiresstronger conditions for perfect recovery compared to the original vectorproblem. However, the Kronecker sensing, modeled correctly, is a much sparserlinear optimization problem. Hence, algorithms that benefit from sparse problemrepresentation, such as interior-point methods, can solve the Kronecker sensingproblems much faster than the corresponding vector problem. In our numericalstudies, we demonstrate a ten-fold improvement in the computation time.
arxiv-1312-4564 | Adaptive Stochastic Alternating Direction Method of Multipliers |  http://arxiv.org/abs/1312.4564  | author:Peilin Zhao, Jinwei Yang, Tong Zhang, Ping Li category:stat.ML cs.LG I.2.6; G.1.6 published:2013-12-16 summary:The Alternating Direction Method of Multipliers (ADMM) has been studied foryears. The traditional ADMM algorithm needs to compute, at each iteration, an(empirical) expected loss function on all training examples, resulting in acomputational complexity proportional to the number of training examples. Toreduce the time complexity, stochastic ADMM algorithms were proposed to replacethe expected function with a random loss function associated with one uniformlydrawn example plus a Bregman divergence. The Bregman divergence, however, isderived from a simple second order proximal function, the half squared norm,which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms withoptimal second order proximal functions, which produce a new family of adaptivesubgradient methods. We theoretically prove that their regret bounds are asgood as the bounds which could be achieved by the best proximal function thatcan be chosen in hindsight. Encouraging empirical results on a variety ofreal-world datasets confirm the effectiveness and efficiency of the proposedalgorithms.
arxiv-1312-4353 | Abstraction in decision-makers with limited information processing capabilities |  http://arxiv.org/abs/1312.4353  | author:Tim Genewein, Daniel A. Braun category:cs.AI cs.IT math.IT stat.ML published:2013-12-16 summary:A distinctive property of human and animal intelligence is the ability toform abstractions by neglecting irrelevant information which allows to separatestructure from noise. From an information theoretic point of view abstractionsare desirable because they allow for very efficient information processing. Inartificial systems abstractions are often implemented through computationallycostly formations of groups or clusters. In this work we establish the relationbetween the free-energy framework for decision making and rate-distortiontheory and demonstrate how the application of rate-distortion fordecision-making leads to the emergence of abstractions. We argue thatabstractions are induced due to a limit in information processing capacity.
arxiv-1312-4405 | Learning Deep Representations By Distributed Random Samplings |  http://arxiv.org/abs/1312.4405  | author:Xiao-Lei Zhang category:cs.LG published:2013-12-16 summary:In this paper, we propose an extremely simple deep model for the unsupervisednonlinear dimensionality reduction -- deep distributed random samplings, whichperforms like a stack of unsupervised bootstrap aggregating. First, its networkstructure is novel: each layer of the network is a group of mutuallyindependent $k$-centers clusterings. Second, its learning method is extremelysimple: the $k$ centers of each clustering are only $k$ randomly selectedexamples from the training data; for small-scale data sets, the $k$ centers arefurther randomly reconstructed by a simple cyclic-shift operation. Experimentalresults on nonlinear dimensionality reduction show that the proposed method canlearn abstract representations on both large-scale and small-scale problems,and meanwhile is much faster than deep neural networks on large-scale problems.
arxiv-1312-4479 | Parametric Modelling of Multivariate Count Data Using Probabilistic Graphical Models |  http://arxiv.org/abs/1312.4479  | author:Pierre Fernique, Jean-Baptiste Durand, Yann Guédon category:stat.ML cs.LG stat.ME published:2013-12-16 summary:Multivariate count data are defined as the number of items of differentcategories issued from sampling within a population, which individuals aregrouped into categories. The analysis of multivariate count data is a recurrentand crucial issue in numerous modelling problems, particularly in the fields ofbiology and ecology (where the data can represent, for example, children countsassociated with multitype branching processes), sociology and econometrics. Wefocus on I) Identifying categories that appear simultaneously, or on thecontrary that are mutually exclusive. This is achieved by identifyingconditional independence relationships between the variables; II)Buildingparsimonious parametric models consistent with these relationships; III)Characterising and testing the effects of covariates on the joint distributionof the counts. To achieve these goals, we propose an approach based ongraphical probabilistic models, and more specifically partially directedacyclic graphs.
arxiv-1312-4314 | Learning Factored Representations in a Deep Mixture of Experts |  http://arxiv.org/abs/1312.4314  | author:David Eigen, Marc'Aurelio Ranzato, Ilya Sutskever category:cs.LG published:2013-12-16 summary:Mixtures of Experts combine the outputs of several "expert" networks, each ofwhich specializes in a different part of the input space. This is achieved bytraining a "gating" network that maps each input to a distribution over theexperts. Such models show promise for building larger networks that are stillcheap to compute at test time, and more parallelizable at training time. Inthis this work, we extend the Mixture of Experts to a stacked model, the DeepMixture of Experts, with multiple sets of gating and experts. Thisexponentially increases the number of effective experts by associating eachinput with a combination of experts at each layer, yet maintains a modest modelsize. On a randomly translated version of the MNIST dataset, we find that theDeep Mixture of Experts automatically learns to develop location-dependent("where") experts at the first layer, and class-specific ("what") experts atthe second layer. In addition, we see that the different combinations are inuse when the model is applied to a dataset of speech monophones. Thesedemonstrate effective use of all expert combinations.
arxiv-1312-4384 | Rectifying Self Organizing Maps for Automatic Concept Learning from Web Images |  http://arxiv.org/abs/1312.4384  | author:Eren Golge, Pinar Duygulu category:cs.CV cs.LG cs.NE published:2013-12-16 summary:We attack the problem of learning concepts automatically from noisy web imagesearch results. Going beyond low level attributes, such as colour and texture,we explore weakly-labelled datasets for the learning of higher level concepts,such as scene categories. The idea is based on discovering commoncharacteristics shared among subsets of images by posing a method that is ableto organise the data while eliminating irrelevant instances. We propose a novelclustering and outlier detection method, namely Rectifying Self Organizing Maps(RSOM). Given an image collection returned for a concept query, RSOM providesclusters pruned from outliers. Each cluster is used to train a modelrepresenting a different characteristics of the concept. The proposed methodoutperforms the state-of-the-art studies on the task of learning low-levelconcepts, and it is competitive in learning higher level concepts as well. Itis capable to work at large scale with no supervision through exploiting theavailable sources.
arxiv-1312-4354 | Decomposition of Optical Flow on the Sphere |  http://arxiv.org/abs/1312.4354  | author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV published:2013-12-16 summary:We propose a number of variational regularisation methods for the estimationand decomposition of motion fields on the $2$-sphere. While motion estimationis based on the optical flow equation, the presented decomposition models aremotivated by recent trends in image analysis. In particular we treat $u+v$decomposition as well as hierarchical decomposition. Helmholtz decomposition ofmotion fields is obtained as a natural by-product of the chosen numericalmethod based on vector spherical harmonics. All models are tested on time-lapsemicroscopy data depicting fluorescently labelled endodermal cells of azebrafish embryo.
arxiv-1312-4382 | Single-trial estimation of stimulus and spike-history effects on time-varying ensemble spiking activity of multiple neurons: a simulation study |  http://arxiv.org/abs/1312.4382  | author:Hideaki Shimazaki category:q-bio.NC stat.ML published:2013-12-16 summary:Neurons in cortical circuits exhibit coordinated spiking activity, and canproduce correlated synchronous spikes during behavior and cognition. Werecently developed a method for estimating the dynamics of correlated ensembleactivity by combining a model of simultaneous neuronal interactions (e.g., aspin-glass model) with a state-space method (Shimazaki et al. 2012 PLoS ComputBiol 8 e1002385). This method allows us to estimate stimulus-evoked dynamics ofneuronal interactions which is reproducible in repeated trials under identicalexperimental conditions. However, the method may not be suitable for detectingstimulus responses if the neuronal dynamics exhibits significant variabilityacross trials. In addition, the previous model does not include effects of pastspiking activity of the neurons on the current state of ensemble activity. Inthis study, we develop a parametric method for simultaneously estimating thestimulus and spike-history effects on the ensemble activity from single-trialdata even if the neurons exhibit dynamics that is largely unrelated to theseeffects. For this goal, we model ensemble neuronal activity as a latent processand include the stimulus and spike-history effects as exogenous inputs to thelatent process. We develop an expectation-maximization algorithm thatsimultaneously achieves estimation of the latent process, stimulus responses,and spike-history effects. The proposed method is useful to analyze aninteraction of internal cortical states and sensory evoked activity.
arxiv-1312-4346 | Teleoperation System Using Past Image Records Considering Narrow Communication Band |  http://arxiv.org/abs/1312.4346  | author:Noritaka Sato, Masataka Ito, Yoshifumi Morita, Fumitoshi Matsuno category:cs.RO cs.CV published:2013-12-16 summary:Teleoperation is necessary when the robot is applied to real missions, forexample surveillance, search and rescue. We proposed teleoperation system usingpast image records (SPIR). SPIR virtually generates the bird's-eye view imageby overlaying the CG model of the robot at the corresponding current positionon the background image which is captured from the camera mounted on the robotat a past time. The problem for SPIR is that the communication bandwidth isoften narrow in some teleoperation tasks. In this case, the candidates ofbackground image of SPIR are few and the position of the robot is oftendelayed. In this study, we propose zoom function for insufficiency ofcandidates of the background image and additional interpolation lines for thedelay of the position data of the robot. To evaluate proposed system, anoutdoor experiments are carried out. The outdoor experiment is conducted on atraining course of a driving school.
arxiv-1312-4527 | Probable convexity and its application to Correlated Topic Models |  http://arxiv.org/abs/1312.4527  | author:Khoat Than, Tu Bao Ho category:cs.LG stat.ML published:2013-12-16 summary:Non-convex optimization problems often arise from probabilistic modeling,such as estimation of posterior distributions. Non-convexity makes the problemsintractable, and poses various obstacles for us to design efficient algorithms.In this work, we attack non-convexity by first introducing the concept of\emph{probable convexity} for analyzing convexity of real functions inpractice. We then use the new concept to analyze an inference problem in the\emph{Correlated Topic Model} (CTM) and related nonconjugate models. Contraryto the existing belief of intractability, we show that this inference problemis concave under certain conditions. One consequence of our analyses is a novelalgorithm for learning CTM which is significantly more scalable and qualitativethan existing methods. Finally, we highlight that stochastic gradientalgorithms might be a practical choice to resolve efficiently non-convexproblems. This finding might find beneficial in many contexts which are beyondprobabilistic modeling.
arxiv-1312-4551 | Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs |  http://arxiv.org/abs/1312.4551  | author:Armen E. Allahverdyan, Aram Galstyan category:stat.ML cs.LG published:2013-12-16 summary:We present an asymptotic analysis of Viterbi Training (VT) and contrast itwith a more conventional Maximum Likelihood (ML) approach to parameterestimation in Hidden Markov Models. While ML estimator works by (locally)maximizing the likelihood of the observed data, VT seeks to maximize theprobability of the most likely hidden state sequence. We develop an analyticalframework based on a generating function formalism and illustrate it on anexactly solvable model of HMM with one unambiguous symbol. For this particularmodel the ML objective function is continuously degenerate. VT objective, incontrast, is shown to have only finite degeneracy. Furthermore, VT convergesfaster and results in sparser (simpler) models, thus realizing an automaticOccam's razor for HMM learning. For more general scenario VT can be worsecompared to ML but still capable of correctly recovering most of theparameters.
arxiv-1312-4461 | Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks |  http://arxiv.org/abs/1312.4461  | author:Andrew Davis, Itamar Arel category:cs.LG published:2013-12-16 summary:Scalability properties of deep neural networks raise key research questions,particularly as the problems considered become larger and more challenging.This paper expands on the idea of conditional computation introduced by Bengio,et. al., where the nodes of a deep network are augmented by a set of gatingunits that determine when a node should be calculated. By factorizing theweight matrix into a low-rank approximation, an estimation of the sign of thepre-nonlinearity activation can be efficiently obtained. For networks usingrectified-linear hidden units, this implies that the computation of a hiddenunit with an estimated negative pre-nonlinearity can be ommitted altogether, asits value will become zero when nonlinearity is applied. For sparse neuralnetworks, this can result in considerable speed gains. Experimental resultsusing the MNIST and SVHN data sets with a fully-connected deep neural networkdemonstrate the performance robustness of the proposed scheme with respect tothe error introduced by the conditional computation process.
arxiv-1312-4176 | Distributed k-means algorithm |  http://arxiv.org/abs/1312.4176  | author:Gabriele Oliva, Roberto Setola, Christoforos N. Hadjicostis category:cs.LG cs.DC published:2013-12-15 summary:In this paper we provide a fully distributed implementation of the k-meansclustering algorithm, intended for wireless sensor networks where each agent isendowed with a possibly high-dimensional observation (e.g., position, humidity,temperature, etc.) The proposed algorithm, by means of one-hop communication,partitions the agents into measure-dependent groups that have small in-groupand large out-group "distances". Since the partitions may not have a relationwith the topology of the network--members of the same clusters may not bespatially close--the algorithm is provided with a mechanism to compute theclusters'centroids even when the clusters are disconnected in severalsub-clusters.The results of the proposed distributed algorithm coincide, interms of minimization of the objective function, with the centralized k-meansalgorithm. Some numerical examples illustrate the capabilities of the proposedsolution.
arxiv-1312-4190 | One-Shot-Learning Gesture Recognition using HOG-HOF Features |  http://arxiv.org/abs/1312.4190  | author:Jakub Konečný, Michal Hagara category:cs.CV published:2013-12-15 summary:The purpose of this paper is to describe one-shot-learning gesturerecognition systems developed on the \textit{ChaLearn Gesture Dataset}. We useRGB and depth images and combine appearance (Histograms of Oriented Gradients)and motion descriptors (Histogram of Optical Flow) for parallel temporalsegmentation and recognition. The Quadratic-Chi distance family is used tomeasure differences between histograms to capture cross-bin relationships. Wealso propose a new algorithm for trimming videos --- to remove all theunimportant frames from videos. We present two methods that use combination ofHOG-HOF descriptors together with variants of Dynamic Time Warping technique.Both methods outperform other published methods and help narrow down the gapbetween human performance and algorithms on this task. The code has been madepublicly available in the MLOSS repository.
arxiv-1312-4124 | A robust Iris recognition method on adverse conditions |  http://arxiv.org/abs/1312.4124  | author:Maryam Soltanali Khalili, Hamed Sadjedi category:cs.CV published:2013-12-15 summary:As a stable biometric system, iris has recently attracted great attentionamong the researchers. However, research is still needed to provide appropriatesolutions to ensure the resistance of the system against error factors. Thepresent study has tried to apply a mask to the image so that the unexpectedfactors affecting the location of the iris can be removed. So, pupillocalization will be faster and robust. Then to locate the exact location ofthe iris, a simple stage of boundary displacement due to the Canny edgedetector has been applied. Then, with searching left and right IRIS edge point,outer radios of IRIS will be detect. Through the process of extracting the irisfeatures, it has been sought to obtain the distinctive iris texture features byusing a discrete stationary wavelets transform 2-D (DSWT2). Using DSWT2 tooland symlet 4 wavelet, distinctive features are extracted. To reduce thecomputational cost, the features obtained from the application of the wavelethave been investigated and a feature selection procedure, using similaritycriteria, has been implemented. Finally, the iris matching has been performedusing a semi-correlation criterion. The accuracy of the proposed method forlocalization on CASIA-v1, CASIA-v3 is 99.73%, 98.24% and 97.04%, respectively.The accuracy of the feature extraction proposed method for CASIA3 iris imagesdatabase is 97.82%, which confirms the efficiency of the proposed method.
arxiv-1312-4132 | An introduction to synchronous self-learning Pareto strategy |  http://arxiv.org/abs/1312.4132  | author:Ahmad Mozaffari, Alireza Fathi category:cs.NE published:2013-12-15 summary:In last decades optimization and control of complex systems that possessedvarious conflicted objectives simultaneously attracted an incremental interestof scientists. This is because of the vast applications of these systems invarious fields of real life engineering phenomena that are generally multimodal, non convex and multi criterion. Hence, many researchers utilizedversatile intelligent models such as Pareto based techniques, game theory(cooperative and non cooperative games), neuro evolutionary systems, fuzzylogic and advanced neural networks for handling these types of problems. Inthis paper a novel method called Synchronous Self Learning Pareto StrategyAlgorithm (SSLPSA) is presented which utilizes Evolutionary Computing (EC),Swarm Intelligence (SI) techniques and adaptive Classical Self Organizing Map(CSOM) simultaneously incorporating with a data shuffling behavior.Evolutionary Algorithms (EA) which attempt to simulate the phenomenon ofnatural evolution are powerful numerical optimization algorithms that reach anapproximate global maximum of a complex multi variable function over a widesearch space and swarm base technique can improved the intensity and therobustness in EA. CSOM is a neural network capable of learning and can improvethe quality of obtained optimal Pareto front. To prove the efficientperformance of proposed algorithm, authors utilized some well known benchmarktest functions. Obtained results indicate that the cited method is best suit inthe case of vector optimization.
arxiv-1312-6834 | Face Detection from still and Video Images using Unsupervised Cellular Automata with K means clustering algorithm |  http://arxiv.org/abs/1312.6834  | author:P. Kiran Sree, I. Ramesh Babu category:cs.CV published:2013-12-15 summary:Pattern recognition problem rely upon the features inherent in the pattern ofimages. Face detection and recognition is one of the challenging research areasin the field of computer vision. In this paper, we present a method to identifyskin pixels from still and video images using skin color. Face regions areidentified from this skin pixel region. Facial features such as eyes, nose andmouth are then located. Faces are recognized from color images using an RBFbased neural network. Unsupervised Cellular Automata with K means clusteringalgorithm is used to locate different facial elements. Orientation is correctedby using eyes. Parameters like inter eye distance, nose length, mouth position,Discrete Cosine Transform (DCT) coefficients etc. are computed and used for aRadial Basis Function (RBF) based neural network. This approach reliably worksfor face sequence with orientation in head, expressions etc.
arxiv-1312-4108 | A MapReduce based distributed SVM algorithm for binary classification |  http://arxiv.org/abs/1312.4108  | author:Ferhat Özgür Çatak, Mehmet Erdal Balaban category:cs.LG cs.DC published:2013-12-15 summary:Although Support Vector Machine (SVM) algorithm has a high generalizationproperty to classify for unseen examples after training phase and it has smallloss value, the algorithm is not suitable for real-life classification andregression problems. SVMs cannot solve hundreds of thousands examples intraining dataset. In previous studies on distributed machine learningalgorithms, SVM is trained over a costly and preconfigured computerenvironment. In this research, we present a MapReduce based distributedparallel SVM training algorithm for binary classification problems. This workshows how to distribute optimization problem over cloud computing systems withMapReduce technique. In the second step of this work, we used statisticallearning theory to find the predictive hypothesis that minimize our empiricalrisks from hypothesis spaces that created with reduce function of MapReduce.The results of this research are important for training of big datasets for SVMalgorithm based classification problems. We provided that iterative training ofsplit dataset with MapReduce technique; accuracy of the classifier functionwill converge to global optimal classifier function's accuracy in finiteiteration size. The algorithm performance was measured on samples from letterrecognition and pen-based recognition of handwritten digits dataset.
arxiv-1312-4149 | Autonomous Quantum Perceptron Neural Network |  http://arxiv.org/abs/1312.4149  | author:Alaa Sagheer, Mohammed Zidan category:cs.NE published:2013-12-15 summary:Recently, with the rapid development of technology, there are a lot ofapplications require to achieve low-cost learning. However the computationalpower of classical artificial neural networks, they are not capable to providelow-cost learning. In contrast, quantum neural networks may be representing agood computational alternate to classical neural network approaches, based onthe computational power of quantum bit (qubit) over the classical bit. In thispaper we present a new computational approach to the quantum perceptron neuralnetwork can achieve learning in low-cost computation. The proposed approach hasonly one neuron can construct self-adaptive activation operators capable toaccomplish the learning process in a limited number of iterations and, thereby,reduce the overall computational cost. The proposed approach is capable toconstruct its own set of activation operators to be applied widely in bothquantum and classical applications to overcome the linearity limitation ofclassical perceptron. The computational power of the proposed approach isillustrated via solving variety of problems where promising and comparableresults are given.
arxiv-1312-4209 | Feature Graph Architectures |  http://arxiv.org/abs/1312.4209  | author:Richard Davis, Sanjay Chawla, Philip Leong category:cs.LG published:2013-12-15 summary:In this article we propose feature graph architectures (FGA), which are deeplearning systems employing a structured initialisation and training methodbased on a feature graph which facilitates improved generalisation performancecompared with a standard shallow architecture. The goal is to explorealternative perspectives on the problem of deep network training. We evaluateFGA performance for deep SVMs on some experimental datasets, and show howgeneralisation and stability results may be derived for these models. Wedescribe the effect of permutations on the model accuracy, and give a criterionfor the optimal permutation in terms of feature correlations. The experimentalresults show that the algorithm produces robust and significant test setimprovements over a standard shallow SVM training method for a range ofdatasets. These gains are achieved with a moderate increase in time complexity.
arxiv-1312-4092 | Domain adaptation for sequence labeling using hidden Markov models |  http://arxiv.org/abs/1312.4092  | author:Edouard Grave, Guillaume Obozinski, Francis Bach category:cs.CL cs.LG published:2013-12-14 summary:Most natural language processing systems based on machine learning are notrobust to domain shift. For example, a state-of-the-art syntactic dependencyparser trained on Wall Street Journal sentences has an absolute drop inperformance of more than ten points when tested on textual data from the Web.An efficient solution to make these methods more robust to domain shift is tofirst learn a word representation using large amounts of unlabeled data fromboth domains, and then use this representation as features in a supervisedlearning algorithm. In this paper, we propose to use hidden Markov models tolearn word representations for part-of-speech tagging. In particular, we studythe influence of using data from the source, the target or both domains tolearn the representation and the different ways to represent words using anHMM.
arxiv-1312-4074 | Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm |  http://arxiv.org/abs/1312.4074  | author:Srinjoy Ganguly, Digbalay Bose, Amit Konar category:cs.CV published:2013-12-14 summary:Clustering is an important facet of explorative data mining and findsextensive use in several fields. In this paper, we propose an extension of theclassical Fuzzy C-Means clustering algorithm. The proposed algorithm,abbreviated as VFC, adopts a multi-dimensional membership vector for each datapoint instead of the traditional, scalar membership value defined in theoriginal algorithm. The membership vector for each point is obtained byconsidering each feature of that point separately and obtaining individualmembership values for the same. We also propose an algorithm to efficientlyallocate the initial cluster centers close to the actual centers, so as tofacilitate rapid convergence. Further, we propose a scheme to achieve crispclustering using the VFC algorithm. The proposed, novel clustering scheme hasbeen tested on two standard data sets in order to analyze its performance. Wealso examine the efficacy of the proposed scheme by analyzing its performanceon image segmentation examples and comparing it with the classical FuzzyC-means clustering algorithm.
arxiv-1312-4044 | CACO : Competitive Ant Colony Optimization, A Nature-Inspired Metaheuristic For Large-Scale Global Optimization |  http://arxiv.org/abs/1312.4044  | author:M. A. El-Dosuky category:cs.NE published:2013-12-14 summary:Large-scale problems are nonlinear problems that need metaheuristics, orglobal optimization algorithms. This paper reviews nature-inspiredmetaheuristics, then it introduces a framework named Competitive Ant ColonyOptimization inspired by the chemical communications among insects. Then a casestudy is presented to investigate the proposed framework for large-scale globaloptimization.
arxiv-1312-3990 | ECOC-Based Training of Neural Networks for Face Recognition |  http://arxiv.org/abs/1312.3990  | author:Nima Hatami, Reza Ebrahimpour, Reza Ghaderi category:cs.CV cs.LG published:2013-12-14 summary:Error Correcting Output Codes, ECOC, is an output representation methodcapable of discovering some of the errors produced in classification tasks.This paper describes the application of ECOC to the training of feed forwardneural networks, FFNN, for improving the overall accuracy of classificationsystems. Indeed, to improve the generalization of FFNN classifiers, this paperproposes an ECOC-Based training method for Neural Networks that use ECOC as theoutput representation, and adopts the traditional Back-Propagation algorithm,BP, to adjust weights of the network. Experimental results for face recognitionproblem on Yale database demonstrate the effectiveness of our method. With arejection scheme defined by a simple robustness rate, high reliability isachieved in this application.
arxiv-1312-3989 | Classifiers With a Reject Option for Early Time-Series Classification |  http://arxiv.org/abs/1312.3989  | author:Nima Hatami, Camelia Chira category:cs.CV cs.LG published:2013-12-14 summary:Early classification of time-series data in a dynamic environment is achallenging problem of great importance in signal processing. This paperproposes a classifier architecture with a reject option capable of onlinedecision making without the need to wait for the entire time series signal tobe present. The main idea is to classify an odor/gas signal with an acceptableaccuracy as early as possible. Instead of using posterior probability of aclassifier, the proposed method uses the "agreement" of an ensemble to decidewhether to accept or reject the candidate label. The introduced algorithm isapplied to the bio-chemistry problem of odor classification to build a novelElectronic-Nose called Forefront-Nose. Experimental results on wind tunneltest-bed facility confirms the robustness of the forefront-nose compared to thestandard classifiers from both earliness and recognition perspectives.
arxiv-1312-4078 | A natural-inspired optimization machine based on the annual migration of salmons in nature |  http://arxiv.org/abs/1312.4078  | author:Ahmad Mozaffari, Alireza Fathi category:cs.NE published:2013-12-14 summary:Bio inspiration is a branch of artificial simulation science that showspervasive contributions to variety of engineering fields such as automatepattern recognition, systematic fault detection and applied optimization. Inthis paper, a new metaheuristic optimizing algorithm that is the simulation ofThe Great Salmon Run(TGSR) is developed. The obtained results imply on theacceptable performance of implemented method in optimization of complex nonconvex, multi dimensional and multi-modal problems. To prove the superiority ofTGSR in both robustness and quality, it is also compared with most of the wellknown proposed optimizing techniques such as Simulated Annealing (SA), ParallelMigrating Genetic Algorithm (PMGA), Differential Evolutionary Algorithm (DEA),Particle Swarm Optimization (PSO), Bee Algorithm (BA), Artificial Bee Colony(ABC), Firefly Algorithm (FA) and Cuckoo Search (CS). The obtained resultsconfirm the acceptable performance of the proposed method in both robustnessand quality for different bench-mark optimizing problems and also prove theauthors claim.
arxiv-1312-3787 | Analysis and Understanding of Various Models for Efficient Representation and Accurate Recognition of Human Faces |  http://arxiv.org/abs/1312.3787  | author:Dharini S., Guru Prasad M., Hari haran. V., Kiran Tej J. L., Kunal Ghosh category:cs.CV published:2013-12-13 summary:In this paper we have tried to compare the various face recognition modelsagainst their classical problems. We look at the methods followed by theseapproaches and evaluate to what extent they are able to solve the problems. Allmethods proposed have some drawbacks under certain conditions. To overcomethese drawbacks we propose a multi-model approach
arxiv-1312-3790 | Sample Complexity of Dictionary Learning and other Matrix Factorizations |  http://arxiv.org/abs/1312.3790  | author:Rémi Gribonval, Rodolphe Jenatton, Francis Bach, Martin Kleinsteuber, Matthias Seibert category:stat.ML cs.IT math.IT published:2013-12-13 summary:Many modern tools in machine learning and signal processing, such as sparsedictionary learning, principal component analysis (PCA), non-negative matrixfactorization (NMF), $K$-means clustering, etc., rely on the factorization of amatrix obtained by concatenating high-dimensional vectors from a trainingcollection. While the idealized task would be to optimize the expected qualityof the factors over the underlying distribution of training vectors, it isachieved in practice by minimizing an empirical average over the consideredcollection. The focus of this paper is to provide sample complexity estimatesto uniformly control how much the empirical average deviates from the expectedcost function. Standard arguments imply that the performance of the empiricalpredictor also exhibit such guarantees. The level of genericity of the approachencompasses several possible constraints on the factors (tensor productstructure, shift-invariance, sparsity \ldots), thus providing a unifiedperspective on the sample complexity of several widely used matrixfactorization schemes. The derived generalization bounds behave proportional to$\sqrt{\log(n)/n}$ w.r.t.\ the number of samples $n$ for the considered matrixfactorization techniques.
arxiv-1312-3724 | ARIANNA: pAth Recognition for Indoor Assisted NavigatioN with Augmented perception |  http://arxiv.org/abs/1312.3724  | author:Pierluigi Gallo, Ilenia Tinnirello, Laura Giarré, Domenico Garlisi, Daniele Croce, Adriano Fagiolini category:cs.CV cs.HC published:2013-12-13 summary:ARIANNA stands for pAth Recognition for Indoor Assisted Navigation withAugmented perception. It is a flexible and low cost navigation system for vi-sually impaired people. Arianna permits to navigate colored paths painted orsticked on the floor revealing their directions through vibrational feedback oncommercial smartphones.
arxiv-1312-3903 | A Methodology for Player Modeling based on Machine Learning |  http://arxiv.org/abs/1312.3903  | author:Marlos C. Machado category:cs.AI cs.LG published:2013-12-13 summary:AI is gradually receiving more attention as a fundamental feature to increasethe immersion in digital games. Among the several AI approaches, playermodeling is becoming an important one. The main idea is to understand and modelthe player characteristics and behaviors in order to develop a better AI. Inthis work, we discuss several aspects of this new field. We proposed a taxonomyto organize the area, discussing several facets of this topic, ranging fromimplementation decisions up to what a model attempts to describe. We thenclassify, in our taxonomy, some of the most important works in this field. Wealso presented a generic approach to deal with player modeling using ML, and weinstantiated this approach to model players' preferences in the gameCivilization IV. The instantiation of this approach has several steps. We firstdiscuss a generic representation, regardless of what is being modeled, andevaluate it performing experiments with the strategy game Civilization IV.Continuing the instantiation of the proposed approach we evaluated theapplicability of using game score information to distinguish differentpreferences. We presented a characterization of virtual agents in the game,comparing their behavior with their stated preferences. Once we havecharacterized these agents, we were able to observe that different preferencesgenerate different behaviors, measured by several game indicators. We thentackled the preference modeling problem as a binary classification task, with asupervised learning approach. We compared four different methods, based ondifferent paradigms (SVM, AdaBoost, NaiveBayes and JRip), evaluating them on aset of matches played by different virtual agents. We conclude our work usingthe learned models to infer human players' preferences. Using some of theevaluated classifiers we obtained accuracies over 60% for most of the inferredpreferences.
arxiv-1312-3811 | Efficient Baseline-free Sampling in Parameter Exploring Policy Gradients: Super Symmetric PGPE |  http://arxiv.org/abs/1312.3811  | author:Frank Sehnke category:cs.LG published:2013-12-13 summary:Policy Gradient methods that explore directly in parameter space are amongthe most effective and robust direct policy search methods and have drawn a lotof attention lately. The basic method from this field, Policy Gradients withParameter-based Exploration, uses two samples that are symmetric around thecurrent hypothesis to circumvent misleading reward in \emph{asymmetrical}reward distributed problems gathered with the usual baseline approach. Theexploration parameters are still updated by a baseline approach - leaving theexploration prone to asymmetric reward distributions. In this paper we willshow how the exploration parameters can be sampled quasi symmetric despitehaving limited instead of free parameters for exploration. We give atransformation approximation to get quasi symmetric samples with respect to theexploration without changing the overall sampling distribution. Finally we willdemonstrate that sampling symmetrically also for the exploration parameters issuperior in needs of samples and robustness than the original samplingapproach.
arxiv-1312-3970 | An Extensive Evaluation of Filtering Misclassified Instances in Supervised Classification Tasks |  http://arxiv.org/abs/1312.3970  | author:Michael R. Smith, Tony Martinez category:cs.LG stat.ML published:2013-12-13 summary:Removing or filtering outliers and mislabeled instances prior to training alearning algorithm has been shown to increase classification accuracy. Apopular approach for handling outliers and mislabeled instances is to removeany instance that is misclassified by a learning algorithm. However, anexamination of which learning algorithms to use for filtering as well as theireffects on multiple learning algorithms over a large set of data sets has notbeen done. Previous work has generally been limited due to the largecomputational requirements to run such an experiment, and, thus, theexamination has generally been limited to learning algorithms that arecomputationally inexpensive and using a small number of data sets. In thispaper, we examine 9 learning algorithms as filtering algorithms as well asexamining the effects of filtering in the 9 chosen learning algorithms on a setof 54 data sets. In addition to using each learning algorithm individually as afilter, we also use the set of learning algorithms as an ensemble filter anduse an adaptive algorithm that selects a subset of the learning algorithms forfiltering for a specific task and learning algorithm. We find that for mostcases, using an ensemble of learning algorithms for filtering produces thegreatest increase in classification accuracy. We also compare filtering with amajority voting ensemble. The voting ensemble significantly outperformsfiltering unless there are high amounts of noise present in the data set.Additionally, we find that a majority voting ensemble is robust to noise asfiltering with a voting ensemble does not increase the classification accuracyof the voting ensemble.
arxiv-1312-3525 | Oracle Inequalities for Convex Loss Functions with Non-Linear Targets |  http://arxiv.org/abs/1312.3525  | author:Mehmet Caner, Anders Bredahl Kock category:math.ST stat.ML stat.TH published:2013-12-12 summary:This paper consider penalized empirical loss minimization of convex lossfunctions with unknown non-linear target functions. Using the elastic netpenalty we establish a finite sample oracle inequality which bounds the loss ofour estimator from above with high probability. If the unknown target is linearthis inequality also provides an upper bound of the estimation error of theestimated parameter vector. These are new results and they generalize theeconometrics and statistics literature. Next, we use the non-asymptotic resultsto show that the excess loss of our estimator is asymptotically of the sameorder as that of the oracle. If the target is linear we give sufficientconditions for consistency of the estimated parameter vector. Next, we brieflydiscuss how a thresholded version of our estimator can be used to performconsistent variable selection. We give two examples of loss functions coveredby our framework and show how penalized nonparametric series estimation iscontained as a special case and provide a finite sample upper bound on the meansquare error of the elastic net series estimator.
arxiv-1312-3516 | Density Estimation in Infinite Dimensional Exponential Families |  http://arxiv.org/abs/1312.3516  | author:Bharath Sriperumbudur, Kenji Fukumizu, Revant Kumar, Arthur Gretton, Aapo Hyvärinen category:math.ST stat.ME stat.ML stat.TH published:2013-12-12 summary:In this paper, we consider an infinite dimensional exponential family,$\mathcal{P}$ of probability densities, which are parametrized by functions ina reproducing kernel Hilbert space, $H$ and show it to be quite rich in thesense that a broad class of densities on $\mathbb{R}^d$ can be approximatedarbitrarily well in Kullback-Leibler (KL) divergence by elements in$\mathcal{P}$. The main goal of the paper is to estimate an unknown density,$p_0$ through an element in $\mathcal{P}$. Standard techniques like maximumlikelihood estimation (MLE) or pseudo MLE (based on the method of sieves),which are based on minimizing the KL divergence between $p_0$ and$\mathcal{P}$, do not yield practically useful estimators because of theirinability to efficiently handle the log-partition function. Instead, we proposean estimator, $\hat{p}_n$ based on minimizing the \emph{Fisher divergence},$J(p_0\Vert p)$ between $p_0$ and $p\in \mathcal{P}$, which involves solving asimple finite-dimensional linear system. When $p_0\in\mathcal{P}$, we show thatthe proposed estimator is consistent, and provide a convergence rate of$n^{-\min\left\{\frac{2}{3},\frac{2\beta+1}{2\beta+2}\right\}}$ in Fisherdivergence under the smoothness assumption that $\logp_0\in\mathcal{R}(C^\beta)$ for some $\beta\ge 0$, where $C$ is a certainHilbert-Schmidt operator on $H$ and $\mathcal{R}(C^\beta)$ denotes the image of$C^\beta$. We also investigate the misspecified case of $p_0\notin\mathcal{P}$and show that $J(p_0\Vert\hat{p}_n)\rightarrow \inf_{p\in\mathcal{P}}J(p_0\Vertp)$ as $n\rightarrow\infty$, and provide a rate for this convergence under asimilar smoothness condition as above. Through numerical simulations wedemonstrate that the proposed estimator outperforms the non-parametric kerneldensity estimator, and that the advantage with the proposed estimator grows as$d$ increases.
arxiv-1312-3613 | Augur: a Modeling Language for Data-Parallel Probabilistic Inference |  http://arxiv.org/abs/1312.3613  | author:Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam Pocock, Stephen J. Green, Guy L. Steele Jr category:stat.ML cs.AI cs.DC cs.PL published:2013-12-12 summary:It is time-consuming and error-prone to implement inference procedures foreach new probabilistic model. Probabilistic programming addresses this problemby allowing a user to specify the model and having a compiler automaticallygenerate an inference procedure for it. For this approach to be practical, itis important to generate inference code that has reasonable performance. Inthis paper, we present a probabilistic programming language and compiler forBayesian networks designed to make effective use of data-parallel architecturessuch as GPUs. Our language is fully integrated within the Scala programminglanguage and benefits from tools such as IDE support, type-checking, and codecompletion. We show that the compiler can generate data-parallel inference codescalable to thousands of GPU cores by making use of the conditionalindependence relationships in the Bayesian network.
arxiv-1312-3429 | Unsupervised learning of depth and motion |  http://arxiv.org/abs/1312.3429  | author:Kishore Konda, Roland Memisevic category:cs.CV cs.LG stat.ML published:2013-12-12 summary:We present a model for the joint estimation of disparity and motion. Themodel is based on learning about the interrelations between images frommultiple cameras, multiple frames in a video, or the combination of both. Weshow that learning depth and motion cues, as well as their combinations, fromdata is possible within a single type of architecture and a single type oflearning algorithm, by using biologically inspired "complex cell" like units,which encode correlations between the pixels across image pairs. Ourexperimental results show that the learning of depth and motion makes itpossible to achieve state-of-the-art performance in 3-D activity analysis, andto outperform existing hand-engineered 3-D motion features by a very largemargin.
arxiv-1312-3522 | Sparse Matrix-based Random Projection for Classification |  http://arxiv.org/abs/1312.3522  | author:Weizhi Lu, Weiyu Li, Kidiyo Kpalma, Joseph Ronsin category:cs.LG cs.CV stat.ML published:2013-12-12 summary:As a typical dimensionality reduction technique, random projection can besimply implemented with linear projection, while maintaining the pairwisedistances of high-dimensional data with high probability. Considering thistechnique is mainly exploited for the task of classification, this paper isdeveloped to study the construction of random matrix from the viewpoint offeature selection, rather than of traditional distance preservation. Thisyields a somewhat surprising theoretical result, that is, the sparse randommatrix with exactly one nonzero element per column, can present better featureselection performance than other more dense matrices, if the projectiondimension is sufficiently large (namely, not much smaller than the number offeature elements); otherwise, it will perform comparably to others. For randomprojection, this theoretical result implies considerable improvement on bothcomplexity and performance, which is widely confirmed with the classificationexperiments on both synthetic data and real data.
arxiv-1312-3393 | Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem |  http://arxiv.org/abs/1312.3393  | author:Masrour Zoghi, Shimon Whiteson, Remi Munos, Maarten de Rijke category:cs.LG published:2013-12-12 summary:This paper proposes a new method for the K-armed dueling bandit problem, avariation on the regular K-armed bandit problem that offers only relativefeedback about pairs of arms. Our approach extends the Upper Confidence Boundalgorithm to the relative setting by using estimates of the pairwiseprobabilities to select a promising arm and applying Upper Confidence Boundwith the winner as a benchmark. We prove a finite-time regret bound of orderO(log t). In addition, our empirical results using real data from aninformation retrieval application show that it greatly outperforms the state ofthe art.
arxiv-1312-3386 | Clustering for high-dimension, low-sample size data using distance vectors |  http://arxiv.org/abs/1312.3386  | author:Yoshikazu Terada category:stat.ML cs.LG published:2013-12-12 summary:In high-dimension, low-sample size (HDLSS) data, it is not always true thatcloseness of two objects reflects a hidden cluster structure. We point out theimportant fact that it is not the closeness, but the "values" of distance thatcontain information of the cluster structure in high-dimensional space. Basedon this fact, we propose an efficient and simple clustering approach, calleddistance vector clustering, for HDLSS data. Under the assumptions given in thework of Hall et al. (2005), we show the proposed approach provides a truecluster label under milder conditions when the dimension tends to infinity withthe sample size fixed. The effectiveness of the distance vector clusteringapproach is illustrated through a numerical experiment and real data analysis.
arxiv-1312-3388 | Online Bayesian Passive-Aggressive Learning |  http://arxiv.org/abs/1312.3388  | author:Tianlin Shi, Jun Zhu category:cs.LG published:2013-12-12 summary:Online Passive-Aggressive (PA) learning is an effective framework forperforming max-margin online learning. But the deterministic formulation andestimated single large-margin model could limit its capability in discoveringdescriptive structures underlying complex data. This pa- per presents onlineBayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PAand extends naturally to incorporate latent variables and perform nonparametricBayesian inference, thus providing great flexibility for explorative analysis.We apply BayesPA to topic modeling and derive efficient online learningalgorithms for max-margin topic models. We further develop nonparametricmethods to resolve the number of topics. Experimental results on real datasetsshow that our approaches significantly improve time efficiency whilemaintaining comparable results with the batch counterparts.
arxiv-1312-3251 | Towards The Development of a Bishnupriya Manipuri Corpus |  http://arxiv.org/abs/1312.3251  | author:Nayan Jyoti Kalita, Navanath Saharia, Smriti Kumar Sinha category:cs.CL published:2013-12-11 summary:For any deep computational processing of language we need evidences, and onesuch set of evidences is corpus. This paper describes the development of atext-based corpus for the Bishnupriya Manipuri language. A Corpus is consideredas a building block for any language processing tasks. Due to the lack ofawareness like other Indian languages, it is also studied less frequently. As aresult the language still lacks a good corpus and basic language processingtools. As per our knowledge this is the first effort to develop a corpus forBishnupriya Manipuri language.
arxiv-1312-3199 | Thickness Mapping of Eleven Retinal Layers in Normal Eyes Using Spectral Domain Optical Coherence Tomography |  http://arxiv.org/abs/1312.3199  | author:Raheleh Kafieh, Hossein Rabbani, Fedra Hajizadeh, Michael D. Abramoff, Milan Sonka category:cs.CV published:2013-12-11 summary:Purpose. This study was conducted to determine the thickness map of elevenretinal layers in normal subjects by spectral domain optical coherencetomography (SD-OCT) and evaluate their association with sex and age. Methods.Mean regional retinal thickness of 11 retinal layers were obtained by automaticthree-dimensional diffusion-map-based method in 112 normal eyes of 76 Iraniansubjects. Results. The thickness map of central foveal area in layer 1, 3, and4 displayed the minimum thickness (P<0.005 for all). Maximum thickness wasobserved in nasal to the fovea of layer 1 (P<0.001) and in a circular patternin the parafoveal retinal area of layers 2, 3 and 4 and in central foveal areaof layer 6 (P<0.001). Temporal and inferior quadrants of the total retinalthickness and most of other quadrants of layer 1 were significantly greater inthe men than in the women. Surrounding eight sectors of total retinal thicknessand a limited number of sectors in layer 1 and 4 significantly correlated withage. Conclusion. SD-OCT demonstrated the three-dimensional thicknessdistribution of retinal layers in normal eyes. Thickness of layers varied withsex and age and in different sectors. These variables should be consideredwhile evaluating macular thickness.
arxiv-1312-3291 | Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic |  http://arxiv.org/abs/1312.3291  | author:James Sharpnack, Akshay Krishnamurthy, Aarti Singh category:stat.ML 62G10 published:2013-12-11 summary:The detection of anomalous activity in graphs is a statistical problem thatarises in many applications, such as network surveillance, disease outbreakdetection, and activity monitoring in social networks. Beyond its wideapplicability, graph structured anomaly detection serves as a case study in thedifficulty of balancing computational complexity with statistical power. Inthis work, we develop from first principles the generalized likelihood ratiotest for determining if there is a well connected region of activation over thevertices in the graph in Gaussian noise. Because this test is computationallyinfeasible, we provide a relaxation, called the Lovasz extended scan statistic(LESS) that uses submodularity to approximate the intractable generalizedlikelihood ratio. We demonstrate a connection between LESS and maximuma-posteriori inference in Markov random fields, which provides us with apoly-time algorithm for LESS. Using electrical network theory, we are able tocontrol type 1 error for LESS and prove conditions under which LESS is riskconsistent. Finally, we consider specific graph models, the torus, k-nearestneighbor graphs, and epsilon-random graphs. We show that on these graphs ourresults provide near-optimal performance by matching our results to known lowerbounds.
arxiv-1312-3168 | Semantic Types, Lexical Sorts and Classifiers |  http://arxiv.org/abs/1312.3168  | author:Bruno Mery, Christian Retoré category:cs.CL published:2013-12-11 summary:We propose a cognitively and linguistically motivated set of sorts forlexical semantics in a compositional setting: the classifiers in languages thatdo have such pronouns. These sorts are needed to include lexical considerationsin a semantical analyser such as Boxer or Grail. Indeed, all proposed lexicalextensions of usual Montague semantics to model restriction of selection,felicitous and infelicitous copredication require a rich and refined typesystem whose base types are the lexical sorts, the basis of the many-sortedlogic in which semantical representations of sentences are stated. However,none of those approaches define precisely the actual base types or sorts to beused in the lexicon. In this article, we shall discuss some of the optionscommonly adopted by researchers in formal lexical semantics, and defend theview that classifiers in the languages which have such pronouns are anappealing solution, both linguistically and cognitively motivated.
arxiv-1312-3005 | One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling |  http://arxiv.org/abs/1312.3005  | author:Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, Tony Robinson category:cs.CL published:2013-12-11 summary:We propose a new benchmark corpus to be used for measuring progress instatistical language modeling. With almost one billion words of training data,we hope this benchmark will be useful to quickly evaluate novel languagemodeling techniques, and to compare their contribution when combined with otheradvanced techniques. We show performance of several well-known types oflanguage models, with the best results achieved with a recurrent neural networkbased language model. The baseline unpruned Kneser-Ney 5-gram model achievesperplexity 67.6; a combination of techniques leads to 35% reduction inperplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scriptsneeded to rebuild the training/held-out data, it also makes availablelog-probability values for each word in each of ten held-out data sets, foreach of the baseline n-gram models.
arxiv-1312-3240 | Associative embeddings for large-scale knowledge transfer with self-assessment |  http://arxiv.org/abs/1312.3240  | author:Alexander Vezhnevets, Vittorio Ferrari category:cs.CV published:2013-12-11 summary:We propose a method for knowledge transfer between semantically relatedclasses in ImageNet. By transferring knowledge from the images that havebounding-box annotations to the others, our method is capable of automaticallypopulating ImageNet with many more bounding-boxes and even pixel-levelsegmentations. The underlying assumption that objects from semantically relatedclasses look alike is formalized in our novel Associative Embedding (AE)representation. AE recovers the latent low-dimensional space of appearancevariations among image windows. The dimensions of AE space tend to correspondto aspects of window appearance (e.g. side view, close up, background). Wemodel the overlap of a window with an object using Gaussian Processes (GP)regression, which spreads annotation smoothly through AE space. Theprobabilistic nature of GP allows our method to perform self-assessment, i.e.assigning a quality estimate to its own output. It enables trading off theamount of returned annotations for their quality. A large scale experiment on219 classes and 0.5 million images demonstrates that our method outperformsstate-of-the-art methods and baselines for both object localization andsegmentation. Using self-assessment we can automatically return bounding-boxannotations for 30% of all images with high localization accuracy (i.e.~73%average overlap with ground-truth).
arxiv-1312-3062 | Fast Neighborhood Graph Search using Cartesian Concatenation |  http://arxiv.org/abs/1312.3062  | author:Jingdong Wang, Jing Wang, Gang Zeng, Rui Gan, Shipeng Li, Baining Guo category:cs.CV published:2013-12-11 summary:In this paper, we propose a new data structure for approximate nearestneighbor search. This structure augments the neighborhood graph with a bridgegraph. We propose to exploit Cartesian concatenation to produce a large set ofvectors, called bridge vectors, from several small sets of subvectors. Eachbridge vector is connected with a few reference vectors near to it, forming abridge graph. Our approach finds nearest neighbors by simultaneously traversingthe neighborhood graph and the bridge graph in the best-first strategy. Thesuccess of our approach stems from two factors: the exact nearest neighborsearch over a large number of bridge vectors can be done quickly, and thereference vectors connected to a bridge (reference) vector near the query arealso likely to be near the query. Experimental results on searching over largescale datasets (SIFT, GIST and HOG) show that our approach outperformsstate-of-the-art ANN search algorithms in terms of efficiency and accuracy. Thecombination of our approach with the IVFADC system also shows superiorperformance over the BIGANN dataset of $1$ billion SIFT features compared withthe best previously published result.
arxiv-1312-3061 | Fast Approximate $K$-Means via Cluster Closures |  http://arxiv.org/abs/1312.3061  | author:Jingdong Wang, Jing Wang, Qifa Ke, Gang Zeng, Shipeng Li category:cs.CV published:2013-12-11 summary:$K$-means, a simple and effective clustering algorithm, is one of the mostwidely used algorithms in multimedia and computer vision community. Traditional$k$-means is an iterative algorithm---in each iteration new cluster centers arecomputed and each data point is re-assigned to its nearest center. The clusterre-assignment step becomes prohibitively expensive when the number of datapoints and cluster centers are large. In this paper, we propose a novel approximate $k$-means algorithm to greatlyreduce the computational complexity in the assignment step. Our approach ismotivated by the observation that most active points changing their clusterassignments at each iteration are located on or near cluster boundaries. Theidea is to efficiently identify those active points by pre-assembling the datainto groups of neighboring points using multiple random spatial partitiontrees, and to use the neighborhood information to construct a closure for eachcluster, in such a way only a small number of cluster candidates need to beconsidered when assigning a data point to its nearest cluster. Using complexityanalysis, image data clustering, and applications to image retrieval, we showthat our approach out-performs state-of-the-art approximate $k$-meansalgorithms in terms of clustering quality and efficiency.
arxiv-1312-3035 | Heat kernel coupling for multiple graph analysis |  http://arxiv.org/abs/1312.3035  | author:Michael M. Bronstein, Klaus Glashoff category:cs.CV published:2013-12-11 summary:In this paper, we introduce heat kernel coupling (HKC) as a method ofconstructing multimodal spectral geometry on weighted graphs of different sizewithout vertex-wise bijective correspondence. We show that Laplacian averagingcan be derived as a limit case of HKC, and demonstrate its applications onseveral problems from the manifold learning and pattern recognition domain.
arxiv-1312-3258 | Implicit Sensitive Text Summarization based on Data Conveyed by Connectives |  http://arxiv.org/abs/1312.3258  | author:Henda Chorfi Ouertani category:cs.CL published:2013-12-11 summary:So far and trying to reach human capabilities, research in automaticsummarization has been based on hypothesis that are both enabling and limiting.Some of these limitations are: how to take into account and reflect (in thegenerated summary) the implicit information conveyed in the text, the authorintention, the reader intention, the context influence, the general worldknowledge. Thus, if we want machines to mimic human abilities, then they willneed access to this same large variety of knowledge. The implicit is affectingthe orientation and the argumentation of the text and consequently its summary.Most of Text Summarizers (TS) are processing as compressing the initial dataand they necessarily suffer from information loss. TS are focusing on featuresof the text only, not on what the author intended or why the reader is readingthe text. In this paper, we address this problem and we present a systemfocusing on acquiring knowledge that is implicit. We principally spotlight theimplicit information conveyed by the argumentative connectives such as: but,even, yet and their effect on the summary.
arxiv-1312-2877 | Automated Classification of L/R Hand Movement EEG Signals using Advanced Feature Extraction and Machine Learning |  http://arxiv.org/abs/1312.2877  | author:Mohammad H. Alomari, Aya Samaha, Khaled AlKamha category:cs.NE cs.CV cs.HC published:2013-12-10 summary:In this paper, we propose an automated computer platform for the purpose ofclassifying Electroencephalography (EEG) signals associated with left and righthand movements using a hybrid system that uses advanced feature extractiontechniques and machine learning algorithms. It is known that EEG represents thebrain activity by the electrical voltage fluctuations along the scalp, andBrain-Computer Interface (BCI) is a device that enables the use of the brainneural activity to communicate with others or to control machines, artificiallimbs, or robots without direct physical movements. In our research work, weaspired to find the best feature extraction method that enables thedifferentiation between left and right executed fist movements through variousclassification algorithms. The EEG dataset used in this research was createdand contributed to PhysioNet by the developers of the BCI2000 instrumentationsystem. Data was preprocessed using the EEGLAB MATLAB toolbox and artifactsremoval was done using AAR. Data was epoched on the basis of Event-Related (De)Synchronization (ERD/ERS) and movement-related cortical potentials (MRCP)features. Mu/beta rhythms were isolated for the ERD/ERS analysis and deltarhythms were isolated for the MRCP analysis. The Independent Component Analysis(ICA) spatial filter was applied on related channels for noise reduction andisolation of both artifactually and neutrally generated EEG sources. The finalfeature vector included the ERD, ERS, and MRCP features in addition to themean, power and energy of the activations of the resulting independentcomponents of the epoched feature datasets. The datasets were inputted into twomachine-learning algorithms: Neural Networks (NNs) and Support Vector Machines(SVMs). Intensive experiments were carried out and optimum classificationperformances of 89.8 and 97.1 were obtained using NN and SVM, respectively.
arxiv-1312-2988 | Protein Contact Prediction by Integrating Joint Evolutionary Coupling Analysis and Supervised Learning |  http://arxiv.org/abs/1312.2988  | author:Jianzhu Ma, Sheng Wang, Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.LG math.OC q-bio.BM stat.ML published:2013-12-10 summary:Protein contacts contain important information for protein structure andfunctional study, but contact prediction from sequence remains verychallenging. Both evolutionary coupling (EC) analysis and supervised machinelearning methods are developed to predict contacts, making use of differenttypes of information, respectively. This paper presents a group graphical lasso(GGL) method for contact prediction that integrates joint multi-family ECanalysis and supervised learning. Different from existing single-family ECanalysis that uses residue co-evolution information in only the target proteinfamily, our joint EC analysis uses residue co-evolution in both the targetfamily and its related families, which may have divergent sequences but similarfolds. To implement joint EC analysis, we model a set of related proteinfamilies using Gaussian graphical models (GGM) and then co-estimate theirprecision matrices by maximum-likelihood, subject to the constraint that theprecision matrices shall share similar residue co-evolution patterns. Tofurther improve the accuracy of the estimated precision matrices, we employ asupervised learning method to predict contact probability from a variety ofevolutionary and non-evolutionary information and then incorporate thepredicted probability as prior into our GGL framework. Experiments show thatour method can predict contacts much more accurately than existing methods, andthat our method performs better on both conserved and family-specific contacts.
arxiv-1312-2638 | Vertex nomination schemes for membership prediction |  http://arxiv.org/abs/1312.2638  | author:D. E. Fishkind, V. Lyzinski, H. Pao, L. Chen, C. E. Priebe category:stat.ML math.OC stat.AP published:2013-12-10 summary:Suppose that a graph is realized from a stochastic block model where one ofthe blocks is of interest, but many or all of the vertices' block labels areunobserved. The task is to order the vertices with unobserved block labels intoa ``nomination list'' such that, with high probability, vertices from theinteresting block are concentrated near the list's beginning. We proposeseveral vertex nomination schemes. Our basic - but principled - setting anddevelopment yields a best nomination scheme (which is a Bayes-Optimalanalogue), and also a likelihood maximization nomination scheme that ispractical to implement when there are a thousand vertices, and which isempirically near-optimal when the number of vertices is small enough to allowcomparison to the best nomination scheme. We then illustrate the robustness ofthe likelihood maximization nomination scheme to the modeling challengesinherent in real data, using examples which include a social network involvinghuman trafficking, the Enron Graph, a worm brain connectome and a politicalblog network.
arxiv-1312-2853 | Performance Analysis Of Neural Network Models For Oxazolines And Oxazoles Derivatives Descriptor Dataset |  http://arxiv.org/abs/1312.2853  | author:Doreswamy, Chanabasayya . M. Vastrad category:cs.CE cs.NE published:2013-12-10 summary:Neural networks have been used successfully to a broad range of areas such asbusiness, data mining, drug discovery and biology. In medicine, neural networkshave been applied widely in medical diagnosis, detection and evaluation of newdrugs and treatment cost estimation. In addition, neural networks have beginpractice in data mining strategies for the aim of prediction, knowledgediscovery. This paper will present the application of neural networks for theprediction and analysis of antitubercular activity of Oxazolines and Oxazolesderivatives. This study presents techniques based on the development of Singlehidden layer neural network (SHLFFNN), Gradient Descent Back propagation neuralnetwork (GDBPNN), Gradient Descent Back propagation with momentum neuralnetwork (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN)and Quantile regression neural network (QRNN) of artificial neural network(ANN) models Here, we comparatively evaluate the performance of five neuralnetwork techniques. The evaluation of the efficiency of each model by ways ofbenchmark experiments is an accepted application. Cross-validation andresampling techniques are commonly used to derive point estimates of theperformances which are compared to identify methods with good properties.Predictive accuracy was evaluated using the root mean squared error (RMSE),Coefficient determination(???), mean absolute error(MAE), mean percentageerror(MPE) and relative square error(RSE). We found that all five neuralnetwork models were able to produce feasible models. QRNN model is outperformswith all statistical tests amongst other four models.
arxiv-1312-2967 | Every LWF and AMP chain graph originates from a set of causal models |  http://arxiv.org/abs/1312.2967  | author:Jose M. Peña category:stat.ML published:2013-12-10 summary:This paper aims at justifying LWF and AMP chain graphs by showing that theydo not represent arbitrary independence models. Specifically, we show thatevery chain graph is inclusion optimal wrt the intersection of the independencemodels represented by a set of directed and acyclic graphs under conditioning.This implies that the independence model represented by the chain graph can beaccounted for by a set of causal models that are subject to selection bias,which in turn can be accounted for by a system that switches between differentregimes or configurations.
arxiv-1312-2710 | Improving circuit miniaturization and its efficiency using Rough Set Theory |  http://arxiv.org/abs/1312.2710  | author:Sarvesh SS Rawat, Dheeraj Dilip Mor, Anugrah Kumar, Sanjiban Shekar Roy, Rohit kumar category:cs.LG cs.AI published:2013-12-10 summary:High-speed, accuracy, meticulousness and quick response are notion of thevital necessities for modern digital world. An efficient electronic circuitunswervingly affects the maneuver of the whole system. Different tools arerequired to unravel different types of engineering tribulations. Improving theefficiency, accuracy and low power consumption in an electronic circuit isalways been a bottle neck problem. So the need of circuit miniaturization isalways there. It saves a lot of time and power that is wasted in switching ofgates, the wiring-crises is reduced, cross-sectional area of chip is reduced,the number of transistors that can implemented in chip is multiplied manyfolds. Therefore to trounce with this problem we have proposed an Artificialintelligence (AI) based approach that make use of Rough Set Theory for itsimplementation. Theory of rough set has been proposed by Z Pawlak in the year1982. Rough set theory is a new mathematical tool which deals with uncertaintyand vagueness. Decisions can be generated using rough set theory by reducingthe unwanted and superfluous data. We have condensed the number of gateswithout upsetting the productivity of the given circuit. This paper proposes anapproach with the help of rough set theory which basically lessens the numberof gates in the circuit, based on decision rules.
arxiv-1312-2646 | Guaranteed Model Order Estimation and Sample Complexity Bounds for LDA |  http://arxiv.org/abs/1312.2646  | author:E. D. Gutiérrez category:stat.ML published:2013-12-10 summary:The question of how to determine the number of independent latent factors(topics) in mixture models such as Latent Dirichlet Allocation (LDA) is ofgreat practical importance. In most applications, the exact number of topics isunknown, and depends on the application and the size of the data set. Bayesiannonparametric methods can avoid the problem of topic number selection, but theycan be impracticably slow for large sample sizes and are subject to localoptima. We develop a guaranteed procedure for topic number recovery that doesnot necessitate learning the model's latent parameters beforehand. Ourprocedure relies on adapting results from random matrix theory. Performance ofour topic number recovery procedure is superior to hLDA, a nonparametricmethod. We also discuss some implications of our results on the samplecomplexity and accuracy of popular spectral learning algorithms for LDA. Ourresults and procedure can be extended to spectral learning algorithms for otherexchangeable mixture models as well as Hidden Markov Models.
arxiv-1312-2789 | Performance Analysis Of Regularized Linear Regression Models For Oxazolines And Oxazoles Derivitive Descriptor Dataset |  http://arxiv.org/abs/1312.2789  | author:Doreswamy, Chanabasayya . M. Vastrad category:cs.LG published:2013-12-10 summary:Regularized regression techniques for linear regression have been created thelast few ten years to reduce the flaws of ordinary least squares regressionwith regard to prediction accuracy. In this paper, new methods for usingregularized regression in model choice are introduced, and we distinguish theconditions in which regularized regression develops our ability to discriminatemodels. We applied all the five methods that use penalty-based (regularization)shrinkage to handle Oxazolines and Oxazoles derivatives descriptor dataset withfar more predictors than observations. The lasso, ridge, elasticnet, lars andrelaxed lasso further possess the desirable property that they simultaneouslyselect relevant predictive descriptors and optimally estimate their effects.Here, we comparatively evaluate the performance of five regularized linearregression methods The assessment of the performance of each model by means ofbenchmark experiments is an established exercise. Cross-validation andresampling methods are generally used to arrive point evaluates theefficiencies which are compared to recognize methods with acceptable features.Predictive accuracy was evaluated using the root mean squared error (RMSE) andSquare of usual correlation between predictors and observed mean inhibitoryconcentration of antitubercular activity (R square). We found that all fiveregularized regression models were able to produce feasible models andefficient capturing the linearity in the data. The elastic net and lars hadsimilar accuracies as well as lasso and relaxed lasso had similar accuraciesbut outperformed ridge regression in terms of the RMSE and R square metrics.
arxiv-1312-2844 | mARC: Memory by Association and Reinforcement of Contexts |  http://arxiv.org/abs/1312.2844  | author:Norbert Rimoux, Patrice Descourt category:cs.IR cs.CL nlin.AO nlin.CD published:2013-12-10 summary:This paper introduces the memory by Association and Reinforcement of Contexts(mARC). mARC is a novel data modeling technology rooted in the secondquantization formulation of quantum mechanics. It is an all-purpose incrementaland unsupervised data storage and retrieval system which can be applied to alltypes of signal or data, structured or unstructured, textual or not. mARC canbe applied to a wide range of information clas-sification and retrievalproblems like e-Discovery or contextual navigation. It can also for-mulated inthe artificial life framework a.k.a Conway "Game Of Life" Theory. In contrastto Conway approach, the objects evolve in a massively multidimensional space.In order to start evaluating the potential of mARC we have built a mARC-basedInternet search en-gine demonstrator with contextual functionality. We comparethe behavior of the mARC demonstrator with Google search both in terms ofperformance and relevance. In the study we find that the mARC search enginedemonstrator outperforms Google search by an order of magnitude in responsetime while providing more relevant results for some classes of queries.
arxiv-1312-2936 | Active Player Modelling |  http://arxiv.org/abs/1312.2936  | author:Julian Togelius, Noor Shaker, Georgios N. Yannakakis category:cs.LG published:2013-12-10 summary:We argue for the use of active learning methods for player modelling. Inactive learning, the learning algorithm chooses where to sample the searchspace so as to optimise learning progress. We hypothesise that player modellingbased on active learning could result in vastly more efficient learning, butwill require big changes in how data is collected. Some example active playermodelling scenarios are described. A particular form of active learning is alsoequivalent to an influential formalisation of (human and machine) curiosity,and games with active learning could therefore be seen as being curious aboutthe player. We further hypothesise that this form of curiosity is symmetric,and therefore that games that explore their players based on the principles ofactive learning will turn out to select game configurations that areinteresting to the player that is being explored.
arxiv-1312-2565 | An SIR Graph Growth Model for the Epidemics of Communicable Diseases |  http://arxiv.org/abs/1312.2565  | author:Charanpal Dhanjal, Stéphan Clémençon category:stat.AP q-bio.PE stat.ML published:2013-12-09 summary:It is the main purpose of this paper to introduce a graph-valued stochasticprocess in order to model the spread of a communicable infectious disease. Themajor novelty of the SIR model we promote lies in the fact that the socialnetwork on which the epidemics is taking place is not specified in advance butevolves through time, accounting for the temporal evolution of the interactionsinvolving infective individuals. Without assuming the existence of a fixedunderlying network model, the stochastic process introduced describes, in aflexible and realistic manner, epidemic spread in non-uniformly mixing andpossibly heterogeneous populations. It is shown how to fit such a(parametrised) model by means of Approximate Bayesian Computation methods basedon graph-valued statistics. The concepts and statistical methods described inthis paper are finally applied to a real epidemic dataset, related to thespread of HIV in Cuba in presence of a contact tracing system, which permitsone to reconstruct partly the evolution of the graph of sexual partnersdiagnosed HIV positive between 1986 and 2006.
arxiv-1312-2578 | Kernel-based Distance Metric Learning in the Output Space |  http://arxiv.org/abs/1312.2578  | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2013-12-09 summary:In this paper we present two related, kernel-based Distance Metric Learning(DML) methods. Their respective models non-linearly map data from theiroriginal space to an output space, and subsequent distance measurements areperformed in the output space via a Mahalanobis metric. The dimensionality ofthe output space can be directly controlled to facilitate the learning of alow-rank metric. Both methods allow for simultaneous inference of theassociated metric and the mapping to the output space, which can be used tovisualize the data, when the output space is 2- or 3-dimensional. Experimentalresults for a collection of classification tasks illustrate the advantages ofthe proposed methods over other traditional and kernel-based DML approaches.
arxiv-1312-2482 | Automatic recognition and tagging of topologically different regimes in dynamical systems |  http://arxiv.org/abs/1312.2482  | author:Jesse Berwald, Marian Gidea, Mikael Vejdemo-Johansson category:cs.CG cs.LG math.DS nlin.CD published:2013-12-09 summary:Complex systems are commonly modeled using nonlinear dynamical systems. Thesemodels are often high-dimensional and chaotic. An important goal in studyingphysical systems through the lens of mathematical models is to determine whenthe system undergoes changes in qualitative behavior. A detailed description ofthe dynamics can be difficult or impossible to obtain for high-dimensional andchaotic systems. Therefore, a more sensible goal is to recognize and marktransitions of a system between qualitatively different regimes of behavior. Inpractice, one is interested in developing techniques for detection of suchtransitions from sparse observations, possibly contaminated by noise. In thispaper we develop a framework to accurately tag different regimes of complexsystems based on topological features. In particular, our framework works witha high degree of success in picking out a cyclically orbiting regime from astationary equilibrium regime in high-dimensional stochastic dynamical systems.
arxiv-1312-2366 | A preliminary survey on optimized multiobjective metaheuristic methods for data clustering using evolutionary approaches |  http://arxiv.org/abs/1312.2366  | author:Ramachandra Rao Kurada, Dr. K Karteeka Pavan, Dr. AV Dattareya Rao category:cs.NE published:2013-12-09 summary:The present survey provides the state-of-the-art of research, copiouslydevoted to Evolutionary Approach (EAs) for clustering exemplified with adiversity of evolutionary computations. The Survey provides a nomenclature thathighlights some aspects that are very important in the context of evolutionarydata clustering. The paper missions the clustering trade-offs branched out withwide-ranging Multi Objective Evolutionary Approaches (MOEAs) methods. Finally,this study addresses the potential challenges of MOEA design and dataclustering, along with conclusions and recommendations for novice andresearchers by positioning most promising paths of future research. MOEAs havesubstantial success across a variety of MOP applications, from pedagogicalmultifunction optimization to real-world engineering design. The survey papernoticeably organizes the developments witnessed in the past three decades forEAs based metaheuristics to solve multiobjective optimization problems (MOP)and to derive significant progression in ruling high quality elucidations in asingle run. Data clustering is an exigent task, whose intricacy is caused by alack of unique and precise definition of a cluster. The discrete optimizationproblem uses the cluster space to derive a solution for Multiobjective dataclustering. Discovery of a majority or all of the clusters (of illogicalshapes) present in the data is a long-standing goal of unsupervised predictivelearning problems or exploratory pattern analysis.
arxiv-1312-2368 | A Unified Markov Chain Approach to Analysing Randomised Search Heuristics |  http://arxiv.org/abs/1312.2368  | author:Jun He, Feidun He, Xin Yao category:math.OC cs.NE published:2013-12-09 summary:The convergence, convergence rate and expected hitting time play fundamentalroles in the analysis of randomised search heuristics. This paper presents aunified Markov chain approach to studying them. Using the approach, thesufficient and necessary conditions of convergence in distribution areestablished. Then the average convergence rate is introduced to randomisedsearch heuristics and its lower and upper bounds are derived. Finally, novelaverage drift analysis and backward drift analysis are proposed for boundingthe expected hitting time. A computational study is also conducted toinvestigate the convergence, convergence rate and expected hitting time. Thetheoretical study belongs to a prior and general study while the computationalstudy belongs to a posterior and case study.
arxiv-1312-2383 | On the Performance of Filters for Reduction of Speckle Noise in SAR Images off the Coast of the Gulf of Guinea |  http://arxiv.org/abs/1312.2383  | author:Griffith S. Klogo, Akpeko Gasonoo, Isaac K. E. Ampomah category:cs.CV published:2013-12-09 summary:Synthetic Aperture Radar (SAR) imagery to monitor oil spills are some methodsthat have been proposed for the West African sub-region. With the increase inthe number of oil exploration companies in Ghana (and her neighbors) and therise in the coastal activities in the sub-region, there is the need for propermonitoring of the environmental impact of these socio-economic activities onthe environment. Detection and near real-time information about oil spills arefundamental in reducing oil spill environmental impact. SAR images are prone tosome noise, which is predominantly speckle noise around the coastal areas. Thispaper evaluates the performance of the mean and median filters used in thepreprocessing filtering to reduce speckle noise in SAR images for most imageprocessing algorithms.
arxiv-1312-2606 | Multi-Task Classification Hypothesis Space with Improved Generalization Bounds |  http://arxiv.org/abs/1312.2606  | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2013-12-09 summary:This paper presents a RKHS, in general, of vector-valued functions intendedto be used as hypothesis space for multi-task classification. It extendssimilar hypothesis spaces that have previously considered in the literature.Assuming this space, an improved Empirical Rademacher Complexity-basedgeneralization bound is derived. The analysis is itself extended to an MKLsetting. The connection between the proposed hypothesis space and a Group-Lassotype regularizer is discussed. Finally, experimental results, with someSVM-based Multi-Task Learning problems, underline the quality of the derivedbounds and validate the paper's analysis.
arxiv-1312-2298 | On the Estimation of Pointwise Dimension |  http://arxiv.org/abs/1312.2298  | author:Shohei Hidaka, Neeraj Kashyap category:math.DS nlin.CD stat.ML published:2013-12-09 summary:Our goal in this paper is to develop an effective estimator of fractaldimension. We survey existing ideas in dimension estimation, with a focus onthe currently popular method of Grassberger and Procaccia for the estimation ofcorrelation dimension. There are two major difficulties in estimation based onthis method. The first is the insensitivity of correlation dimension itself todifferences in dimensionality over data, which we term "dimension blindness".The second comes from the reliance of the method on the inference of limitingbehavior from finite data. We propose pointwise dimension as an object for estimation in response to thedimension blindness of correlation dimension. Pointwise dimension is a localquantity, and the distribution of pointwise dimensions over the data containsthe information to which correlation dimension is blind. We use a "limit-free"description of pointwise dimension to develop a new estimator. We conclude bydiscussing potential applications of our estimator as well as some challengesit raises.
arxiv-1312-2177 | Machine Learning Techniques for Intrusion Detection |  http://arxiv.org/abs/1312.2177  | author:Mahdi Zamani, Mahnush Movahedi category:cs.CR cs.LG cs.NI C.2.0; K.6.5 published:2013-12-08 summary:An Intrusion Detection System (IDS) is a software that monitors a single or anetwork of computers for malicious activities (attacks) that are aimed atstealing or censoring information or corrupting network protocols. Mosttechniques used in today's IDS are not able to deal with the dynamic andcomplex nature of cyber attacks on computer networks. Hence, efficient adaptivemethods like various techniques of machine learning can result in higherdetection rates, lower false alarm rates and reasonable computation andcommunication costs. In this paper, we study several such schemes and comparetheir performance. We divide the schemes into methods based on classicalartificial intelligence (AI) and methods based on computational intelligence(CI). We explain how various characteristics of CI techniques can be used tobuild efficient IDS.
arxiv-1312-2171 | bartMachine: Machine Learning with Bayesian Additive Regression Trees |  http://arxiv.org/abs/1312.2171  | author:Adam Kapelner, Justin Bleich category:stat.ML cs.LG published:2013-12-08 summary:We present a new package in R implementing Bayesian additive regression trees(BART). The package introduces many new features for data analysis using BARTsuch as variable selection, interaction detection, model diagnostic plots,incorporation of missing data and the ability to save trees for futureprediction. It is significantly faster than the current R implementation,parallelized, and capable of handling both large sample sizes andhigh-dimensional data.
arxiv-1312-2244 | Time-dependent Hierarchical Dirichlet Model for Timeline Generation |  http://arxiv.org/abs/1312.2244  | author:Tao Wang category:cs.CL cs.IR published:2013-12-08 summary:Timeline Generation aims at summarizing news from different epochs andtelling readers how an event evolves. It is a new challenge that combinessalience ranking with novelty detection. For long-term public events, the maintopic usually includes various aspects across different epochs and each aspecthas its own evolving pattern. Existing approaches neglect such hierarchicaltopic structure involved in the news corpus in timeline generation. In thispaper, we develop a novel time-dependent Hierarchical Dirichlet Model (HDM) fortimeline generation. Our model can aptly detect different levels of topicinformation across corpus and such structure is further used for sentenceselection. Based on the topic mined fro HDM, sentences are selected byconsidering different aspects such as relevance, coherence and coverage. Wedevelop experimental systems to evaluate 8 long-term events that publicconcern. Performance comparison between different systems demonstrates theeffectiveness of our model in terms of ROUGE metrics.
arxiv-1312-2249 | Scalable Object Detection using Deep Neural Networks |  http://arxiv.org/abs/1312.2249  | author:Dumitru Erhan, Christian Szegedy, Alexander Toshev, Dragomir Anguelov category:cs.CV stat.ML published:2013-12-08 summary:Deep convolutional neural networks have recently achieved state-of-the-artperformance on a number of image recognition benchmarks, including the ImageNetLarge-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model onthe localization sub-task was a network that predicts a single bounding box anda confidence score for each object category in the image. Such a model capturesthe whole-image context around the objects but cannot handle multiple instancesof the same object in the image without naively replicating the number ofoutputs for each instance. In this work, we propose a saliency-inspired neuralnetwork model for detection, which predicts a set of class-agnostic boundingboxes along with a single score for each box, corresponding to its likelihoodof containing any object of interest. The model naturally handles a variablenumber of instances for each class and allows for cross-class generalization atthe highest levels of the network. We are able to obtain competitiverecognition performance on VOC2007 and ILSVRC2012, while using only the top fewpredicted locations in each image and a small number of neural networkevaluations.
arxiv-1312-2164 | Budgeted Influence Maximization for Multiple Products |  http://arxiv.org/abs/1312.2164  | author:Nan Du, Yingyu Liang, Maria Florina Balcan, Le Song category:cs.LG cs.SI stat.ML published:2013-12-08 summary:The typical algorithmic problem in viral marketing aims to identify a set ofinfluential users in a social network, who, when convinced to adopt a product,shall influence other users in the network and trigger a large cascade ofadoptions. However, the host (the owner of an online social platform) oftenfaces more constraints than a single product, endless user attentions,unlimited budget and unbounded time; in reality, multiple products need to beadvertised, each user can tolerate only a small number of recommendations,influencing user has a cost and advertisers have only limited budgets, and theadoptions need to be maximized within a short time window. Given theses myriads of user, monetary, and timing constraints, it isextremely challenging for the host to design principled and efficient viralmarket algorithms with provable guarantees. In this paper, we provide a novelsolution by formulating the problem as a submodular maximization in acontinuous-time diffusion model under an intersection of a matroid and multipleknapsack constraints. We also propose an adaptive threshold greedy algorithmwhich can be faster than the traditional greedy algorithm with lazy evaluation,and scalable to networks with million of nodes. Furthermore, our mathematicalformulation allows us to prove that the algorithm can achieve an approximationfactor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints areactive, which also improves over previous guarantees from combinatorialoptimization literature. In the case when influencing each user has uniformcost, the approximation becomes even better to a factor of $1/3$. Extensivesynthetic and real world experiments demonstrate that our budgeted influencemaximization algorithm achieves the-state-of-the-art in terms of botheffectiveness and scalability, often beating the next best by significantmargins.
arxiv-1312-2154 | Sequential Monte Carlo Inference of Mixed Membership Stochastic Blockmodels for Dynamic Social Networks |  http://arxiv.org/abs/1312.2154  | author:Tomoki Kobayashi, Koji Eguchi category:cs.SI cs.LG stat.ML published:2013-12-07 summary:Many kinds of data can be represented as a network or graph. It is crucial toinfer the latent structure underlying such a network and to predict unobservedlinks in the network. Mixed Membership Stochastic Blockmodel (MMSB) is apromising model for network data. Latent variables and unknown parameters inMMSB have been estimated through Bayesian inference with the entire network;however, it is important to estimate them online for evolving networks. In thispaper, we first develop online inference methods for MMSB through sequentialMonte Carlo methods, also known as particle filters. We then extend them fortime-evolving networks, taking into account the temporal dependency of thenetwork structure. We demonstrate through experiments that the time-dependentparticle filter outperformed several baselines in terms of predictionperformance in an online condition.
arxiv-1312-2132 | Robust Subspace System Identification via Weighted Nuclear Norm Optimization |  http://arxiv.org/abs/1312.2132  | author:Dorsa Sadigh, Henrik Ohlsson, S. Shankar Sastry, Sanjit A. Seshia category:cs.SY cs.LG stat.ML published:2013-12-07 summary:Subspace identification is a classical and very well studied problem insystem identification. The problem was recently posed as a convex optimizationproblem via the nuclear norm relaxation. Inspired by robust PCA, we extend thisframework to handle outliers. The proposed framework takes the form of a convexoptimization problem with an objective that trades off fit, rank and sparsity.As in robust PCA, it can be problematic to find a suitable regularizationparameter. We show how the space in which a suitable parameter should be soughtcan be limited to a bounded open set of the two dimensional parameter space. Inpractice, this is very useful since it restricts the parameter space that isneeded to be surveyed.
arxiv-1312-2087 | Towards Structural Natural Language Formalization: Mapping Discourse to Controlled Natural Language |  http://arxiv.org/abs/1312.2087  | author:Nicholas H. Kirk category:cs.CL published:2013-12-07 summary:The author describes a conceptual study towards mapping grounded naturallanguage discourse representation structures to instances of controlledlanguage statements. This can be achieved via a pipeline of preexisting stateof the art technologies, namely natural language syntax to semantic discoursemapping, and a reduction of the latter to controlled language discourse, givena set of previously learnt reduction rules. Concludingly a description onevaluation, potential and limitations for ontology-based reasoning ispresented.
arxiv-1312-2137 | End-to-end Phoneme Sequence Recognition using Convolutional Neural Networks |  http://arxiv.org/abs/1312.2137  | author:Dimitri Palaz, Ronan Collobert, Mathew Magimai. -Doss category:cs.LG cs.CL cs.NE published:2013-12-07 summary:Most phoneme recognition state-of-the-art systems rely on a classical neuralnetwork classifiers, fed with highly tuned features, such as MFCC or PLPfeatures. Recent advances in ``deep learning'' approaches questioned suchsystems, but while some attempts were made with simpler features such asspectrograms, state-of-the-art systems still rely on MFCCs. This might beviewed as a kind of failure from deep learning approaches, which are oftenclaimed to have the ability to train with raw signals, alleviating the need ofhand-crafted features. In this paper, we investigate a convolutional neuralnetwork approach for raw speech signals. While convolutional architectures gottremendous success in computer vision or text processing, they seem to havebeen let down in the past recent years in the speech processing field. We showthat it is possible to learn an end-to-end phoneme sequence classifier systemdirectly from raw signal, with similar performance on the TIMIT and WSJdatasets than existing systems based on MFCC, questioning the need of complexhand-crafted features on large datasets.
arxiv-1312-2050 | Consistency of spectral clustering in stochastic block models |  http://arxiv.org/abs/1312.2050  | author:Jing Lei, Alessandro Rinaldo category:math.ST stat.ML stat.TH published:2013-12-07 summary:We analyze the performance of spectral clustering for community extraction instochastic block models. We show that, under mild conditions, spectralclustering applied to the adjacency matrix of the network can consistentlyrecover hidden communities even when the order of the maximum expected degreeis as small as $\log n$, with $n$ the number of nodes. This result applies tosome popular polynomial time spectral clustering algorithms and is furtherextended to degree corrected stochastic block models using a spherical$k$-median spectral clustering method. A key component of our analysis is acombinatorial bound on the spectrum of binary random matrices, which is sharperthan the conventional matrix Bernstein inequality and may be of independentinterest.
arxiv-1312-2061 | Region and Location Based Indexing and Retrieval of MR-T2 Brain Tumor Images |  http://arxiv.org/abs/1312.2061  | author:Krishna A N, B G Prasad category:cs.CV cs.IR published:2013-12-07 summary:In this paper, region based and location based retrieval systems have beenimplemented for retrieval of MR-T2 axial 2-D brain images. This is done byextracting and characterizing the tumor portion of 2-D brain slices by use of asuitable threshold computed over the entire image. Indexing and retrieval isthen performed by computing texture features based on gray-tonespatial-dependence matrix of segmented regions. A Hash structure is used toindex all images. A combined index is adopted to point to all similar images interms of the texture features. At query time, only those images that are in thesame hash bucket as those of the queried image are compared for similarity,thus reducing the search space and time.
arxiv-1312-2139 | Optimal rates for zero-order convex optimization: the power of two function evaluations |  http://arxiv.org/abs/1312.2139  | author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright, Andre Wibisono category:math.OC cs.IT math.IT stat.ML published:2013-12-07 summary:We consider derivative-free algorithms for stochastic and non-stochasticconvex optimization problems that use only function values rather thangradients. Focusing on non-asymptotic bounds on convergence rates, we show thatif pairs of function values are available, algorithms for $d$-dimensionaloptimization that use gradient estimates based on random perturbations suffer afactor of at most $\sqrt{d}$ in convergence rate over traditional stochasticgradient methods. We establish such results for both smooth and non-smoothcases, sharpening previous analyses that suggested a worse dimensiondependence, and extend our results to the case of multiple ($m \ge 2$)evaluations. We complement our algorithmic development withinformation-theoretic lower bounds on the minimax convergence rate of suchproblems, establishing the sharpness of our achievable results up to constant(sometimes logarithmic) factors.
arxiv-1312-1970 | An Algorithmic Theory of Dependent Regularizers, Part 1: Submodular Structure |  http://arxiv.org/abs/1312.1970  | author:Hoyt Koepke, Marina Meila category:stat.ML published:2013-12-06 summary:We present an exploration of the rich theoretical connections between severalclasses of regularized models, network flows, and recent results in submodularfunction theory. This work unifies key aspects of these problems under a commontheory, leading to novel methods for working with several important models ofinterest in statistics, machine learning and computer vision. In Part 1, we review the concepts of network flows and submodular functionoptimization theory foundational to our results. We then examine theconnections between network flows and the minimum-norm algorithm fromsubmodular optimization, extending and improving several current results. Thisleads to a concise representation of the structure of a large class of pairwiseregularized models important in machine learning, statistics and computervision. In Part 2, we describe the full regularization path of a class of penalizedregression problems with dependent variables that includes the graph-guidedLASSO and total variation constrained models. This description also motivates apractical algorithm. This allows us to efficiently find the regularization pathof the discretized version of TV penalized models. Ultimately, our newalgorithms scale up to high-dimensional problems with millions of variables.
arxiv-1312-1931 | Multi-frame denoising of high speed optical coherence tomography data using inter-frame and intra-frame priors |  http://arxiv.org/abs/1312.1931  | author:Liheng Bian, Jinli Suo, Feng Chen, Qionghai Dai category:cs.CV published:2013-12-06 summary:Optical coherence tomography (OCT) is an important interferometric diagnostictechnique which provides cross-sectional views of the subsurface microstructureof biological tissues. However, the imaging quality of high-speed OCT islimited due to the large speckle noise. To address this problem, this paperproposes a multi-frame algorithmic method to denoise OCT volume.Mathematically, we build an optimization model which forces the temporallyregistered frames to be low rank, and the gradient in each frame to be sparse,under logarithmic image formation and noise variance constraints. Besides, aconvex optimization algorithm based on the augmented Lagrangian method isderived to solve the above model. The results reveal that our approachoutperforms the other methods in terms of both speckle noise suppression andcrucial detail preservation.
arxiv-1312-1743 | Dual coordinate solvers for large-scale structural SVMs |  http://arxiv.org/abs/1312.1743  | author:Deva Ramanan category:cs.LG cs.CV published:2013-12-06 summary:This manuscript describes a method for training linear SVMs (including binarySVMs, SVM regression, and structural SVMs) from large, out-of-core trainingdatasets. Current strategies for large-scale learning fall into one of twocamps; batch algorithms which solve the learning problem given a finitedatasets, and online algorithms which can process out-of-core datasets. Theformer typically requires datasets small enough to fit in memory. The latter isoften phrased as a stochastic optimization problem; such algorithms enjoystrong theoretical properties but often require manual tuned annealingschedules, and may converge slowly for problems with large output spaces (e.g.,structural SVMs). We discuss an algorithm for an "intermediate" regime in whichthe data is too large to fit in memory, but the active constraints (supportvectors) are small enough to remain in memory. In this case, one can designrather efficient learning algorithms that are as stable as batch algorithms,but capable of processing out-of-core datasets. We have developed such aMATLAB-based solver and used it to train a collection of recognition systemsfor articulated pose estimation, facial analysis, 3D object recognition, andaction classification, all with publicly-available code. This writeup describesthe solver in detail.
arxiv-1312-1858 | How Santa Fe Ants Evolve |  http://arxiv.org/abs/1312.1858  | author:Dominic Wilson, Devinder Kaur category:cs.NE published:2013-12-06 summary:The Santa Fe Ant model problem has been extensively used to investigate, testand evaluate Evolutionary Computing systems and methods over the past twodecades. There is however no literature on its program structures that aresystematically used for fitness improvement, the geometries of those structuresand their dynamics during optimization. This paper analyzes the Santa Fe AntProblem using a new phenotypic schema and landscape analysis based on executedinstruction sequences. For the first time we detail systematic structuralfeatures that give high fitness and the evolutionary dynamics of suchstructures. The new schema avoids variances due to introns. We develop aphenotypic variation method that tests the new understanding of the landscape.We also develop a modified function set that tests newly identifiedsynchronization constraints. We obtain favorable computational efforts comparedto those in the literature, on testing the new variation and function set onboth the Santa Fe Trail, and the more computationally demanding Los AltosTrail. Our findings suggest that for the Santa Fe Ant problem, a perspective ofprogram assembly from repetition of highly fit responses to trail conditionsleads to better analysis and performance.
arxiv-1312-1847 | Understanding Deep Architectures using a Recursive Convolutional Network |  http://arxiv.org/abs/1312.1847  | author:David Eigen, Jason Rolfe, Rob Fergus, Yann LeCun category:cs.LG published:2013-12-06 summary:A key challenge in designing convolutional network models is sizing themappropriately. Many factors are involved in these decisions, including numberof layers, feature maps, kernel sizes, etc. Complicating this further is thefact that each of these influence not only the numbers and dimensions of theactivation units, but also the total number of parameters. In this paper wefocus on assessing the independent contributions of three of these linkedvariables: The numbers of layers, feature maps, and parameters. To accomplishthis, we employ a recursive convolutional network whose weights are tiedbetween layers; this allows us to vary each of the three factors in acontrolled setting. We find that while increasing the numbers of layers andparameters each have clear benefit, the number of feature maps (and hencedimensionality of the representation) appears ancillary, and finds most of itsbenefit through the introduction of more weights. Our results (i) empiricallyconfirm the notion that adding layers alone increases computational power,within the context of convolutional layers, and (ii) suggest that precisesizing of convolutional feature map dimensions is itself of little concern;more attention should be paid to the number of parameters in these layersinstead.
arxiv-1312-1903 | A sequential reduction method for inference in generalized linear mixed models |  http://arxiv.org/abs/1312.1903  | author:Helen Ogden category:stat.CO stat.ME stat.ML published:2013-12-06 summary:The likelihood for the parameters of a generalized linear mixed modelinvolves an integral which may be of very high dimension. Because of thisintractability, many approximations to the likelihood have been proposed, butall can fail when the model is sparse, in that there is only a small amount ofinformation available on each random effect. The sequential reduction methoddescribed in this paper exploits the dependence structure of the posteriordistribution of the random effects to reduce substantially the cost of findingan accurate approximation to the likelihood in models with sparse structure.
arxiv-1312-1760 | Towards Normalizing the Edit Distance Using a Genetic Algorithms Based Scheme |  http://arxiv.org/abs/1312.1760  | author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI published:2013-12-06 summary:The normalized edit distance is one of the distances derived from the editdistance. It is useful in some applications because it takes into account thelengths of the two strings compared. The normalized edit distance is notdefined in terms of edit operations but rather in terms of the edit path. Inthis paper we propose a new derivative of the edit distance that also takesinto consideration the lengths of the two strings, but the new distance isrelated directly to the edit distance. The particularity of the new distance isthat it uses the genetic algorithms to set the values of the parameters ituses. We conduct experiments to test the new distance and we obtain promisingresults.
arxiv-1312-1752 | Particle Swarm Optimization of Information-Content Weighting of Symbolic Aggregate Approximation |  http://arxiv.org/abs/1312.1752  | author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI published:2013-12-06 summary:Bio-inspired optimization algorithms have been gaining more popularityrecently. One of the most important of these algorithms is particle swarmoptimization (PSO). PSO is based on the collective intelligence of a swam ofparticles. Each particle explores a part of the search space looking for theoptimal position and adjusts its position according to two factors; the firstis its own experience and the second is the collective experience of the wholeswarm. PSO has been successfully used to solve many optimization problems. Inthis work we use PSO to improve the performance of a well-known representationmethod of time series data which is the symbolic aggregate approximation (SAX).As with other time series representation methods, SAX results in loss ofinformation when applied to represent time series. In this paper we use PSO topropose a new minimum distance WMD for SAX to remedy this problem. Unlike theoriginal minimum distance, the new distance sets different weights to differentsegments of the time series according to their information content. Thisweighted minimum distance enhances the performance of SAX as we show throughexperiments using different time series datasets.
arxiv-1312-2451 | CEAI: CCM based Email Authorship Identification Model |  http://arxiv.org/abs/1312.2451  | author:Sarwat Nizamani, Nasrullah Memon category:cs.LG published:2013-12-06 summary:In this paper we present a model for email authorship identification (EAI) byemploying a Cluster-based Classification (CCM) technique. Traditionally,stylometric features have been successfully employed in various authorshipanalysis tasks; we extend the traditional feature-set to include some moreinteresting and effective features for email authorship identification (e.g.the last punctuation mark used in an email, the tendency of an author to usecapitalization at the start of an email, or the punctuation after a greeting orfarewell). We also included Info Gain feature selection based content features.It is observed that the use of such features in the authorship identificationprocess has a positive impact on the accuracy of the authorship identificationtask. We performed experiments to justify our arguments and compared theresults with other base line models. Experimental results reveal that theproposed CCM-based email authorship identification model, along with theproposed feature set, outperforms the state-of-the-art support vector machine(SVM)-based models, as well as the models proposed by Iqbal et al. [1, 2]. Theproposed model attains an accuracy rate of 94% for 10 authors, 89% for 25authors, and 81% for 50 authors, respectively on Enron dataset, while 89.5%accuracy has been achieved on authors' constructed real email dataset. Theresults on Enron dataset have been achieved on quite a large number of authorsas compared to the models proposed by Iqbal et al. [1, 2].
arxiv-1312-1666 | Semi-Stochastic Gradient Descent Methods |  http://arxiv.org/abs/1312.1666  | author:Jakub Konečný, Peter Richtárik category:stat.ML cs.LG cs.NA math.NA math.OC published:2013-12-05 summary:In this paper we study the problem of minimizing the average of a largenumber ($n$) of smooth convex loss functions. We propose a new method, S2GD(Semi-Stochastic Gradient Descent), which runs for one or several epochs ineach of which a single full gradient and a random number of stochasticgradients is computed, following a geometric law. The total work needed for themethod to output an $\varepsilon$-accurate solution in expectation, measured inthe number of passes over data, or equivalently, in units equivalent to thecomputation of a single gradient of the loss, is$O((\kappa/n)\log(1/\varepsilon))$, where $\kappa$ is the condition number.This is achieved by running the method for $O(\log(1/\varepsilon))$ epochs,with a single gradient evaluation and $O(\kappa)$ stochastic gradientevaluations in each. The SVRG method of Johnson and Zhang arises as a specialcase. If our method is limited to a single epoch only, it needs to evaluate atmost $O((\kappa/\varepsilon)\log(1/\varepsilon))$ stochastic gradients. Incontrast, SVRG requires $O(\kappa/\varepsilon^2)$ stochastic gradients. Toillustrate our theoretical results, S2GD only needs the workload equivalent toabout 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution fora problem with $n=10^9$ and $\kappa=10^3$.
arxiv-1312-1492 | A fast and robust algorithm to count topologically persistent holes in noisy clouds |  http://arxiv.org/abs/1312.1492  | author:Vitaliy Kurlin category:cs.CG cs.CV math.AT published:2013-12-05 summary:Preprocessing a 2D image often produces a noisy cloud of interest points. Westudy the problem of counting holes in unorganized clouds in the plane. Theholes in a given cloud are quantified by the topological persistence of theirboundary contours when the cloud is analyzed at all possible scales. We designthe algorithm to count holes that are most persistent in the filtration ofoffsets (neighborhoods) around given points. The input is a cloud of $n$ pointsin the plane without any user-defined parameters. The algorithm has $O(n\logn)$ time and $O(n)$ space. The output is the array (number of holes, relativepersistence in the filtration). We prove theoretical guarantees when thealgorithm finds the correct number of holes (components in the complement) ofan unknown shape approximated by a cloud.
arxiv-1312-1733 | Impact of regularization on Spectral Clustering |  http://arxiv.org/abs/1312.1733  | author:Antony Joseph, Bin Yu category:stat.ML published:2013-12-05 summary:The performance of spectral clustering can be considerably improved viaregularization, as demonstrated empirically in Amini et. al (2012). Here, weprovide an attempt at quantifying this improvement through theoreticalanalysis. Under the stochastic block model (SBM), and its extensions, previousresults on spectral clustering relied on the minimum degree of the graph beingsufficiently large for its good performance. By examining the scenario wherethe regularization parameter $\tau$ is large we show that the minimum degreeassumption can potentially be removed. As a special case, for an SBM with twoblocks, the results require the maximum degree to be large (grow faster than$\log n$) as opposed to the minimum degree. More importantly, we show the usefulness of regularization in situationswhere not all nodes belong to well-defined clusters. Our results rely on a`bias-variance'-like trade-off that arises from understanding the concentrationof the sample Laplacian and the eigen gap as a function of the regularizationparameter. As a byproduct of our bounds, we propose a data-driven technique\textit{DKest} (standing for estimated Davis-Kahan bounds) for choosing theregularization parameter. This technique is shown to work well throughsimulations and on a real data set.
arxiv-1312-1423 | ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of Sequential Data Using Sigma Grams |  http://arxiv.org/abs/1312.1423  | author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI published:2013-12-05 summary:The problem of similarity search is one of the main problems in computerscience. This problem has many applications in text-retrieval, web search,computational biology, bioinformatics and others. Similarity between two dataobjects can be depicted using a similarity measure or a distance metric. Thereare numerous distance metrics in the literature, some are used for a particulardata type, and others are more general. In this paper we present a new distancemetric for sequential data which is based on the sum of n-grams. The novelty ofour distance is that these n-grams are weighted using artificial bee colony; arecent optimization algorithm based on the collective intelligence of a swarmof bees on their search for nectar. This algorithm has been used in optimizinga large number of numerical problems. We validate the new distanceexperimentally.
arxiv-1312-1681 | An Approach: Modality Reduction and Face-Sketch Recognition |  http://arxiv.org/abs/1312.1681  | author:Sourav Pramanik, Dr. Debotosh Bhattacharjee category:cs.CV published:2013-12-05 summary:To recognize face sketch through face photo database is a challenging taskfor todays researchers. Because face photo images in training set and facesketch images in testing set have different modality. Difference between twoface photos of difference person is smaller than the difference between sameperson in a face photo and face sketched. In this paper, for reduction of themodality between face photo and face sketch we first bring face photo and facesketch images in a new dimension using 2D Discrete Haar wavelet transform withscale 3 followed by a negative approach. After that, extract features fromtransformed images using Principal Component Analysis (PCA). Thereafter, we useSVM classifier and K-NN classifier for better classification. Our proposedmethod is experimentally verified by its robustness against faces that arecaptured in a good lighting condition and in a frontal pose. The experiment hasbeen conducted with 100 male and female face images as training set and 100male and female face sketch images as testing set collected from CUHK trainingand testing cropped photos and CUHK training and testing cropped sketches.
arxiv-1312-1461 | Multi-Sensor Image Fusion Based on Moment Calculation |  http://arxiv.org/abs/1312.1461  | author:Sourav Pramanik, Debotosh Bhattacharjee category:cs.CV published:2013-12-05 summary:An image fusion method based on salient features is proposed in this paper.In this work, we have concentrated on salient features of the image for fusionin order to preserve all relevant information contained in the input images andtried to enhance the contrast in fused image and also suppressed noise to amaximum extent. In our system, first we have applied a mask on two input imagesin order to conserve the high frequency information along with some lowfrequency information and stifle noise to a maximum extent. Thereafter, foridentification of salience features from sources images, a local moment iscomputed in the neighborhood of a coefficient. Finally, a decision map isgenerated based on local moment in order to get the fused image. To verify ourproposed algorithm, we have tested it on 120 sensor image pairs collected fromManchester University UK database. The experimental results show that theproposed method can provide superior fused image in terms of severalquantitative fusion evaluation index.
arxiv-1312-1462 | Geometric Feature Based Face-Sketch Recognition |  http://arxiv.org/abs/1312.1462  | author:Sourav Pramanik, Debotosh Bhattacharjee category:cs.CV published:2013-12-05 summary:This paper presents a novel facial sketch image or face-sketch recognitionapproach based on facial feature extraction. To recognize a face-sketch, wehave concentrated on a set of geometric face features like eyes, nose,eyebrows, lips, etc and their length and width ratio because it is difficult tomatch photos and sketches because they belong to two different modalities. Inthis system, first the facial features/components from training images areextracted, then ratios of length, width, and area etc. are calculated and thoseare stored as feature vectors for individual images. After that the meanfeature vectors are computed and subtracted from each feature vector forcentering of the feature vectors. In the next phase, feature vector for theincoming probe face-sketch is also computed in similar fashion. Here, K-NNclassifier is used to recognize probe face-sketch. It is experimentallyverified that the proposed method is robust against faces are in a frontalpose, with normal lighting and neutral expression and have no occlusions. Theexperiment has been conducted with 80 male and female face images fromdifferent face databases. It has useful applications for both law enforcementand digital entertainment.
arxiv-1312-1512 | An adaptive block based integrated LDP,GLCM,and Morphological features for Face Recognition |  http://arxiv.org/abs/1312.1512  | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:This paper proposes a technique for automatic face recognition usingintegrated multiple feature sets extracted from the significant blocks of agradient image. We discuss about the use of novel morphological, localdirectional pattern (LDP) and gray-level co-occurrence matrix GLCM basedfeature extraction technique to recognize human faces. Firstly, the newmorphological features i.e., features based on number of runs of pixels in fourdirections (N,NE,E,NW) are extracted, together with the GLCM based statisticalfeatures and LDP features that are less sensitive to the noise andnon-monotonic illumination changes, are extracted from the significant blocksof the gradient image. Then these features are concatenated together. Weintegrate the above mentioned methods to take full advantage of the threeapproaches. Extraction of the significant blocks from the absolute gradientimage and hence from the original image to extract pertinent information withthe idea of dimension reduction forms the basis of the work. The efficiency ofour method is demonstrated by the experiment on 1100 images from the FRAV2Dface database, 2200 images from the FERET database, where the images vary inpose, expression, illumination and scale and 400 images from the ORL facedatabase, where the images slightly vary in pose. Our method has shown 90.3%,93% and 98.75% recognition accuracy for the FRAV2D, FERET and the ORL databaserespectively.
arxiv-1312-1683 | Face Recognition using Hough Peaks extracted from the significant blocks of the Gradient Image |  http://arxiv.org/abs/1312.1683  | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:This paper proposes a new technique for automatic face recognition usingintegrated peaks of the Hough transformed significant blocks of the binarygradient image. In this approach firstly the gradient of an image is calculatedand a threshold is set to obtain a binary gradient image, which is lesssensitive to noise and illumination changes. Secondly, significant blocks areextracted from the absolute gradient image, to extract pertinent informationwith the idea of dimension reduction. Finally the best fitted Hough peaks areextracted from the Hough transformed significant blocks for efficient facerecognition. Then these Hough peaks are concatenated together, which are usedas feature in classification process. The efficiency of the proposed method isdemonstrated by the experiment on 1100 images from the FRAV2D face database,2200 images from the FERET database, where the images vary in pose, expression,illumination and scale and 400 images from the ORL face database, where theimages slightly vary in pose. Our method has shown 93.3%, 88.5% and 99%recognition accuracy for the FRAV2D, FERET and the ORL database respectively.
arxiv-1312-1494 | Approximating persistent homology for a cloud of $n$ points in a subquadratic time |  http://arxiv.org/abs/1312.1494  | author:Vitaliy Kurlin category:cs.CG cs.CV math.AT published:2013-12-05 summary:The Vietoris-Rips filtration for an $n$-point metric space is a sequence oflarge simplicial complexes adding a topological structure to the otherwisedisconnected space. The persistent homology is a key tool in topological dataanalysis and studies topological features of data that persist over manyscales. The fastest algorithm for computing persistent homology of a filtrationhas time $O(M(u)+u^2\log^2 u)$, where $u$ is the number of updates (additionsor deletions of simplices), $M(u)=O(u^{2.376})$ is the time for multiplicationof $u\times u$ matrices. For a space of $n$ points given by their pairwisedistances, we approximate the Vietoris-Rips filtration by a zigzag filtrationconsisting of $u=o(n)$ updates, which is sublinear in $n$. The constant dependson a given error of approximation and on the doubling dimension of the metricspace. Then the persistent homology of this sublinear-size filtration can becomputed in time $o(n^2)$, which is subquadratic in $n$.
arxiv-1312-1517 | A Gabor block based Kernel Discriminative Common Vector (KDCV) approach using cosine kernels for Human Face Recognition |  http://arxiv.org/abs/1312.1517  | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:In this paper a nonlinear Gabor Wavelet Transform (GWT) discriminant featureextraction approach for enhanced face recognition is proposed. Firstly, thelow-energized blocks from Gabor wavelet transformed images are extracted.Secondly, the nonlinear discriminating features are analyzed and extracted fromthe selected low-energized blocks by the generalized Kernel DiscriminativeCommon Vector (KDCV) method. The KDCV method is extended to include cosinekernel function in the discriminating method. The KDCV with the cosine kernelsis then applied on the extracted low energized discriminating feature vectorsto obtain the real component of a complex quantity for face recognition. Inorder to derive positive kernel discriminative vectors; we apply only thosekernel discriminative eigenvectors that are associated with non-zeroeigenvalues. The feasibility of the low energized Gabor block based generalizedKDCV method with cosine kernel function models has been successfully tested forimage classification using the L1, L2 distance measures; and the cosinesimilarity measure on both frontal and pose-angled face recognition.Experimental results on the FRAV2D and the FERET database demonstrate theeffectiveness of this new approach.
arxiv-1312-1684 | High Performance Human Face Recognition using Gabor based Pseudo Hidden Markov Model |  http://arxiv.org/abs/1312.1684  | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:This paper introduces a novel methodology that combines the multi-resolutionfeature of the Gabor wavelet transformation (GWT) with the local interactionsof the facial structures expressed through the Pseudo Hidden Markov model(PHMM). Unlike the traditional zigzag scanning method for feature extraction acontinuous scanning method from top-left corner to right then top-down andright to left and so on until right-bottom of the image i.e. a spiral scanningtechnique has been proposed for better feature selection. Unlike traditionalHMMs, the proposed PHMM does not perform the state conditional independence ofthe visible observation sequence assumption. This is achieved via the conceptof local structures introduced by the PHMM used to extract facial bands andautomatically select the most informative features of a face image. Thus, thelong-range dependency problem inherent to traditional HMMs has been drasticallyreduced. Again with the use of most informative pixels rather than the wholeimage makes the proposed method reasonably faster for face recognition. Thismethod has been successfully tested on frontal face images from the ORL, FRAV2Dand FERET face databases where the images vary in pose, illumination,expression, and scale. The FERET data set contains 2200 frontal face images of200 subjects, while the FRAV2D data set consists of 1100 images of 100 subjectsand the full ORL database is considered. The results reported in thisapplication are far better than the recent and most referred systems.
arxiv-1312-1520 | A Face Recognition approach based on entropy estimate of the nonlinear DCT features in the Logarithm Domain together with Kernel Entropy Component Analysis |  http://arxiv.org/abs/1312.1520  | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:This paper exploits the feature extraction capabilities of the discretecosine transform (DCT) together with an illumination normalization approach inthe logarithm domain that increase its robustness to variations in facialgeometry and illumination. Secondly in the same domain the entropy measures areapplied on the DCT coefficients so that maximum entropy preserving pixels canbe extracted as the feature vector. Thus the informative features of a face canbe extracted in a low dimensional space. Finally, the kernel entropy componentanalysis (KECA) with an extension of arc cosine kernels is applied on theextracted DCT coefficients that contribute most to the entropy estimate toobtain only those real kernel ECA eigenvectors that are associated witheigenvalues having high positive entropy contribution. The resulting system wassuccessfully tested on real image sequences and is robust to significantpartial occlusion and illumination changes, validated with the experiments onthe FERET, AR, FRAV2D and ORL face databases. Experimental comparison isdemonstrated to prove the superiority of the proposed approach in respect torecognition accuracy. Using specificity and sensitivity we find that the bestis achieved when Renyi entropy is applied on the DCT coefficients. Extensiveexperimental comparison is demonstrated to prove the superiority of theproposed approach in respect to recognition accuracy. Moreover, the proposedapproach is very simple, computationally fast and can be implemented in anyreal-time face recognition system.
arxiv-1312-1737 | Curriculum Learning for Handwritten Text Line Recognition |  http://arxiv.org/abs/1312.1737  | author:Jérôme Louradour, Christopher Kermorvant category:cs.LG published:2013-12-05 summary:Recurrent Neural Networks (RNN) have recently achieved the best performancein off-line Handwriting Text Recognition. At the same time, learning RNN bygradient descent leads to slow convergence, and training times are particularlylong when the training database consists of full lines of text. In this paper,we propose an easy way to accelerate stochastic gradient descent in thisset-up, and in the general context of learning to recognize sequences. Theprinciple is called Curriculum Learning, or shaping. The idea is to first learnto recognize short sequences before training on all available trainingsequences. Experiments on three different handwritten text databases (Rimes,IAM, OpenHaRT) show that a simple implementation of this strategy cansignificantly speed up the training of RNN for Text Recognition, and evensignificantly improve performance in some cases.
arxiv-1312-1530 | Bandit Online Optimization Over the Permutahedron |  http://arxiv.org/abs/1312.1530  | author:Nir Ailon, Kohei Hatano, Eiji Takimoto category:cs.LG published:2013-12-05 summary:The permutahedron is the convex polytope with vertex set consisting of thevectors $(\pi(1),\dots, \pi(n))$ for all permutations (bijections) $\pi$ over$\{1,\dots, n\}$. We study a bandit game in which, at each step $t$, anadversary chooses a hidden weight weight vector $s_t$, a player chooses avertex $\pi_t$ of the permutahedron and suffers an observed loss of$\sum_{i=1}^n \pi(i) s_t(i)$. A previous algorithm CombBand of Cesa-Bianchi et al (2009) guarantees aregret of $O(n\sqrt{T \log n})$ for a time horizon of $T$. Unfortunately,CombBand requires at each step an $n$-by-$n$ matrix permanent approximation towithin improved accuracy as $T$ grows, resulting in a total running time thatis super linear in $T$, making it impractical for large time horizons. We provide an algorithm of regret $O(n^{3/2}\sqrt{T})$ with total timecomplexity $O(n^3T)$. The ideas are a combination of CombBand and a recentalgorithm by Ailon (2013) for online optimization over the permutahedron in thefull information setting. The technical core is a bound on the variance of thePlackett-Luce noisy sorting process's "pseudo loss". The bound is obtained byestablishing positive semi-definiteness of a family of 3-by-3 matricesgenerated from rational functions of exponentials of 3 parameters.
arxiv-1312-1725 | Book embeddings of Reeb graphs |  http://arxiv.org/abs/1312.1725  | author:Vitaliy Kurlin category:cs.CG cs.CV math.GT published:2013-12-05 summary:Let $X$ be a simplicial complex with a piecewise linear function$f:X\to\mathbb{R}$. The Reeb graph $Reeb(f,X)$ is the quotient of $X$, where wecollapse each connected component of $f^{-1}(t)$ to a single point. Let thenodes of $Reeb(f,X)$ be all homologically critical points where any homology ofthe corresponding component of the level set $f^{-1}(t)$ changes. Then we canlabel every arc of $Reeb(f,X)$ with the Betti numbers$(\beta_1,\beta_2,\dots,\beta_d)$ of the corresponding $d$-dimensionalcomponent of a level set. The homology labels give more information about theoriginal complex $X$ than the classical Reeb graph. We describe a canonicalembedding of a Reeb graph into a multi-page book (a star cross a line) and givea unique linear code of this book embedding.
arxiv-1312-1613 | Max-Min Distance Nonnegative Matrix Factorization |  http://arxiv.org/abs/1312.1613  | author:Jim Jing-Yan Wang category:stat.ML cs.LG cs.NA published:2013-12-05 summary:Nonnegative Matrix Factorization (NMF) has been a popular representationmethod for pattern classification problem. It tries to decompose a nonnegativematrix of data samples as the product of a nonnegative basic matrix and anonnegative coefficient matrix, and the coefficient matrix is used as the newrepresentation. However, traditional NMF methods ignore the class labels of thedata samples. In this paper, we proposed a supervised novel NMF algorithm toimprove the discriminative ability of the new representation. Using the classlabels, we separate all the data sample pairs into within-class pairs andbetween-class pairs. To improve the discriminate ability of the new NMFrepresentations, we hope that the maximum distance of the within-class pairs inthe new NMF space could be minimized, while the minimum distance of thebetween-class pairs pairs could be maximized. With this criterion, we constructan objective function and optimize it with regard to basic and coefficientmatrices and slack variables alternatively, resulting in a iterative algorithm.
arxiv-1312-1522 | Iterative Log Thresholding |  http://arxiv.org/abs/1312.1522  | author:Dmitry Malioutov, Aleksandr Aravkin category:stat.ML math.OC published:2013-12-05 summary:Sparse reconstruction approaches using the re-weighted l1-penalty have beenshown, both empirically and theoretically, to provide a significant improvementin recovering sparse signals in comparison to the l1-relaxation. However,numerical optimization of such penalties involves solving problems withl1-norms in the objective many times. Using the direct link of reweightedl1-penalties to the concave log-regularizer for sparsity, we derive a simpleprox-like algorithm for the log-regularized formulation. The proximal splittingstep of the algorithm has a closed form solution, and we call the algorithm'log-thresholding' in analogy to soft thresholding for the l1-penalty. We establish convergence results, and demonstrate that log-thresholdingprovides more accurate sparse reconstructions compared to both soft and hardthresholding. Furthermore, the approach can be directly extended tooptimization over matrices with penalty for rank (i.e. the nuclear norm penaltyand its re-weigthed version), where we suggest a singular-valuelog-thresholding approach.
arxiv-1312-1685 | Human Face Recognition using Gabor based Kernel Entropy Component Analysis |  http://arxiv.org/abs/1312.1685  | author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV published:2013-12-05 summary:In this paper, we present a novel Gabor wavelet based Kernel EntropyComponent Analysis (KECA) method by integrating the Gabor wavelettransformation (GWT) of facial images with the KECA method for enhanced facerecognition performance. Firstly, from the Gabor wavelet transformed images themost important discriminative desirable facial features characterized byspatial frequency, spatial locality and orientation selectivity to cope withthe variations due to illumination and facial expression changes were derived.After that KECA, relating to the Renyi entropy is extended to include cosinekernel function. The KECA with the cosine kernels is then applied on theextracted most important discriminating feature vectors of facial images toobtain only those real kernel ECA eigenvectors that are associated witheigenvalues having positive entropy contribution. Finally, these real KECAfeatures are used for image classification using the L1, L2 distance measures;the Mahalanobis distance measure and the cosine similarity measure. Thefeasibility of the Gabor based KECA method with the cosine kernel has beensuccessfully tested on both frontal and pose-angled face recognition, usingdatasets from the ORL, FRAV2D and the FERET database.
arxiv-1312-1706 | Swapping Variables for High-Dimensional Sparse Regression with Correlated Measurements |  http://arxiv.org/abs/1312.1706  | author:Divyanshu Vats, Richard G. Baraniuk category:math.ST cs.IT math.IT stat.ML stat.TH published:2013-12-05 summary:We consider the high-dimensional sparse linear regression problem ofaccurately estimating a sparse vector using a small number of linearmeasurements that are contaminated by noise. It is well known that the standardcadre of computationally tractable sparse regression algorithms---such as theLasso, Orthogonal Matching Pursuit (OMP), and their extensions---perform poorlywhen the measurement matrix contains highly correlated columns. To address thisshortcoming, we develop a simple greedy algorithm, called SWAP, thatiteratively swaps variables until convergence. SWAP is surprisingly effectivein handling measurement matrices with high correlations. In fact, we prove thatSWAP outputs the true support, the locations of the non-zero entries in thesparse vector, under a relatively mild condition on the measurement matrix.Furthermore, we show that SWAP can be used to boost the performance of anysparse regression algorithm. We empirically demonstrate the advantages of SWAPby comparing it with several state-of-the-art sparse regression algorithms.
arxiv-1312-1121 | Interpreting random forest classification models using a feature contribution method |  http://arxiv.org/abs/1312.1121  | author:Anna Palczewska, Jan Palczewski, Richard Marchese Robinson, Daniel Neagu category:cs.LG published:2013-12-04 summary:Model interpretation is one of the key aspects of the model evaluationprocess. The explanation of the relationship between model variables andoutputs is relatively easy for statistical models, such as linear regressions,thanks to the availability of model parameters and their statisticalsignificance. For "black box" models, such as random forest, this informationis hidden inside the model structure. This work presents an approach forcomputing feature contributions for random forest classification models. Itallows for the determination of the influence of each variable on the modelprediction for an individual instance. By analysing feature contributions for atraining dataset, the most significant variables can be determined and theirtypical contribution towards predictions made for individual classes, i.e.,class-specific feature contribution "patterns", are discovered. These patternsrepresent a standard behaviour of the model and allow for an additionalassessment of the model reliability for a new data. Interpretation of featurecontributions for two UCI benchmark datasets shows the potential of theproposed methodology. The robustness of results is demonstrated through anextensive analysis of feature contributions calculated for a large number ofgenerated random forest models.
arxiv-1312-1099 | Multiscale Dictionary Learning for Estimating Conditional Distributions |  http://arxiv.org/abs/1312.1099  | author:Francesca Petralia, Joshua Vogelstein, David B. Dunson category:stat.ML cs.LG published:2013-12-04 summary:Nonparametric estimation of the conditional distribution of a response givenhigh-dimensional features is a challenging problem. It is important to allownot only the mean but also the variance and shape of the response density tochange flexibly with features, which are massive-dimensional. We propose amultiscale dictionary learning model, which expresses the conditional responsedensity as a convex combination of dictionary densities, with the densitiesused and their weights dependent on the path through a tree decomposition ofthe feature space. A fast graph partitioning algorithm is applied to obtain thetree decomposition, with Bayesian methods then used to adaptively prune andaverage over different sub-trees in a soft probabilistic manner. The algorithmscales efficiently to approximately one million features. State of the artpredictive performance is demonstrated for toy examples and two neuroscienceapplications including up to a million features.
arxiv-1312-1031 | Analysis of Distributed Stochastic Dual Coordinate Ascent |  http://arxiv.org/abs/1312.1031  | author:Tianbao Yang, Shenghuo Zhu, Rong Jin, Yuanqing Lin category:cs.DC cs.LG published:2013-12-04 summary:In \citep{Yangnips13}, the author presented distributed stochastic dualcoordinate ascent (DisDCA) algorithms for solving large-scale regularized lossminimization. Extraordinary performances have been observed and reported forthe well-motivated updates, as referred to the practical updates, compared tothe naive updates. However, no serious analysis has been provided to understandthe updates and therefore the convergence rates. In the paper, we bridge thegap by providing a theoretical analysis of the convergence rates of thepractical DisDCA algorithm. Our analysis helped by empirical studies has shownthat it could yield an exponential speed-up in the convergence by increasingthe number of dual updates at each iteration. This result justifies thesuperior performances of the practical DisDCA as compared to the naive variant.As a byproduct, our analysis also reveals the convergence behavior of theone-communication DisDCA.
arxiv-1312-1054 | Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures of Gaussians |  http://arxiv.org/abs/1312.1054  | author:Constantinos Daskalakis, Gautam Kamath category:cs.DS cs.LG math.PR math.ST stat.TH published:2013-12-04 summary:We provide an algorithm for properly learning mixtures of twosingle-dimensional Gaussians without any separability assumptions. Given$\tilde{O}(1/\varepsilon^2)$ samples from an unknown mixture, our algorithmoutputs a mixture that is $\varepsilon$-close in total variation distance, intime $\tilde{O}(1/\varepsilon^5)$. Our sample complexity is optimal up tologarithmic factors, and significantly improves upon both Kalai et al., whosealgorithm has a prohibitive dependence on $1/\varepsilon$, and Feldman et al.,whose algorithm requires bounds on the mixture parameters and dependspseudo-polynomially in these parameters. One of our main contributions is an improved and generalized algorithm forselecting a good candidate distribution from among competing hypotheses.Namely, given a collection of $N$ hypotheses containing at least one candidatethat is $\varepsilon$-close to an unknown distribution, our algorithm outputs acandidate which is $O(\varepsilon)$-close to the distribution. The algorithmrequires ${O}(\log{N}/\varepsilon^2)$ samples from the unknown distribution and${O}(N \log N/\varepsilon^2)$ time, which improves previous such results (suchas the Scheff\'e estimator) from a quadratic dependence of the running time on$N$ to quasilinear. Given the wide use of such results for the purpose ofhypothesis selection, our improved algorithm implies immediate improvements toany such use.
arxiv-1312-1277 | Bandits and Experts in Metric Spaces |  http://arxiv.org/abs/1312.1277  | author:Robert Kleinberg, Aleksandrs Slivkins, Eli Upfal category:cs.DS cs.LG published:2013-12-04 summary:In a multi-armed bandit problem, an online algorithm chooses from a set ofstrategies in a sequence of trials so as to maximize the total payoff of thechosen strategies. While the performance of bandit algorithms with a smallfinite strategy set is quite well understood, bandit problems with largestrategy sets are still a topic of very active investigation, motivated bypractical applications such as online auctions and web advertisement. The goalof such research is to identify broad and natural classes of strategy sets andpayoff functions which enable the design of efficient solutions. In this work we study a very general setting for the multi-armed banditproblem in which the strategies form a metric space, and the payoff functionsatisfies a Lipschitz condition with respect to the metric. We refer to thisproblem as the "Lipschitz MAB problem". We present a solution for themulti-armed bandit problem in this setting. That is, for every metric space wedefine an isometry invariant which bounds from below the performance ofLipschitz MAB algorithms for this metric space, and we present an algorithmwhich comes arbitrarily close to meeting this bound. Furthermore, our techniquegives even better results for benign payoff functions. We also address thefull-feedback ("best expert") version of the problem, where after every roundthe payoffs from all arms are revealed.
arxiv-1312-1244 | Chebushev Greedy Algorithm in convex optimization |  http://arxiv.org/abs/1312.1244  | author:Vladimir Temlyakov category:stat.ML math.OC published:2013-12-04 summary:Chebyshev Greedy Algorithm is a generalization of the well known OrthogonalMatching Pursuit defined in a Hilbert space to the case of Banach spaces. Weapply this algorithm for constructing sparse approximate solutions (withrespect to a given dictionary) to convex optimization problems. Rate ofconvergence results in a style of the Lebesgue-type inequalities are proved.
arxiv-1312-1349 | Improving self-calibration |  http://arxiv.org/abs/1312.1349  | author:Torsten A. Enßlin, Henrik Junklewitz, Lars Winderling, Maksim Greiner, Marco Selig category:astro-ph.IM cs.IT math.IT stat.ML published:2013-12-04 summary:Response calibration is the process of inferring how much the measured datadepend on the signal one is interested in. It is essential for any quantitativesignal estimation on the basis of the data. Here, we investigateself-calibration methods for linear signal measurements and linear dependenceof the response on the calibration parameters. The common practice is toaugment an external calibration solution using a known reference signal with aninternal calibration on the unknown measurement signal itself. Contemporaryself-calibration schemes try to find a self-consistent solution for signal andcalibration by exploiting redundancies in the measurements. This can beunderstood in terms of maximizing the joint probability of signal andcalibration. However, the full uncertainty structure of this joint probabilityaround its maximum is thereby not taken into account by these schemes.Therefore better schemes -- in sense of minimal square error -- can be designedby accounting for asymmetries in the uncertainty of signal and calibration. Weargue that at least a systematic correction of the common self-calibrationscheme should be applied in many measurement situations in order to properlytreat uncertainties of the signal on which one calibrates. Otherwise thecalibration solutions suffer from a systematic bias, which consequentlydistorts the signal reconstruction. Furthermore, we argue that non-parametric,signal-to-noise filtered calibration should provide more accuratereconstructions than the common bin averages and provide a new, improvedself-calibration scheme. We illustrate our findings with a simplistic numericalexample.
arxiv-1312-0940 | Medical Aid for Automatic Detection of Malaria |  http://arxiv.org/abs/1312.0940  | author:Pramit Ghosh, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CY cs.CV published:2013-12-03 summary:The analysis and counting of blood cells in a microscope image can provideuseful information concerning to the health of a person. In particular,morphological analysis of red blood cells deformations can effectively detectimportant disease like malaria. Blood images, obtained by the microscope, whichis coupled with a digital camera, are analyzed by the computer for diagnosis orcan be transmitted easily to clinical centers than liquid blood samples.Automatic analysis system for the presence of Plasmodium in microscopic imageof blood can greatly help pathologists and doctors that typically inspect bloodfilms manually. Unfortunately, the analysis made by human experts is not rapidand not yet standardized due to the operators capabilities and tiredness. Thepaper shows how effectively and accurately it is possible to identify thePlasmodium in the blood film. In particular, the paper presents how to enhancethe microscopic image and filter out the unnecessary segments followed by thethreshold based segmentation and recognize the presence of Plasmodium. Theproposed system can be deployed in the remote area as a supporting aid fortelemedicine technology and only basic training is sufficient to operate it.This system achieved more than 98 percentage accuracy for the samples collectedto test this system.
arxiv-1312-0809 | Automatic White Blood Cell Measuring Aid for Medical Diagnosis |  http://arxiv.org/abs/1312.0809  | author:Pramit Ghosh, Debotosh Bhattacharjee, Mita Nasipuri, Dipak Kumar Basu category:cs.CY cs.CV published:2013-12-03 summary:Blood related invasive pathological investigations play a major role indiagnosis of diseases. But in India and other third world countries there areno enough pathological infrastructures for medical diagnosis. Moreover, most ofthe remote places of those countries have neither pathologists nor physicians.Telemedicine partially solves the lack of physicians. But the pathologicalinvestigation infrastructure can not be integrated with the telemedicinetechnology. The objective of this work is to automate the blood relatedpathological investigation process. Detection of different white blood cellshas been automated in this work. This system can be deployed in the remote areaas a supporting aid for telemedicine technology and only high school educationis sufficient to operate it. The proposed system achieved 97.33 percentaccuracy for the samples collected to test this system.
arxiv-1312-0712 | Generalized Non-orthogonal Joint Diagonalization with LU Decomposition and Successive Rotations |  http://arxiv.org/abs/1312.0712  | author:Xiao-Feng Gong, Xiu-Lin Wang, Qiu-Hua Lin category:stat.ML published:2013-12-03 summary:Non-orthogonal joint diagonalization (NJD) free of prewhitening has beenwidely studied in the context of blind source separation (BSS) and array signalprocessing, etc. However, NJD is used to retrieve the jointly diagonalizablestructure for a single set of target matrices which are mostly formulized witha single dataset, and thus is insufficient to handle multiple datasets withinter-set dependences, a scenario often encountered in joint BSS (J-BSS)applications. As such, we present a generalized NJD (GNJD) algorithm tosimultaneously perform asymmetric NJD upon multiple sets of target matriceswith mutually linked loading matrices, by using LU decomposition and successiverotations, to enable J-BSS over multiple datasets with indication/exploitationof their mutual dependences. Experiments with synthetic and real-world datasetsare provided to illustrate the performance of the proposed algorithm.
arxiv-1312-0760 | Template-Based Active Contours |  http://arxiv.org/abs/1312.0760  | author:Jayanth Krishna Mogali, Adithya Kumar Pediredla, Chandra Sekhar Seelamantula category:cs.CV published:2013-12-03 summary:We develop a generalized active contour formalism for image segmentationbased on shape templates. The shape template is subjected to a restrictedaffine transformation (RAT) in order to segment the object of interest. RATallows for translation, rotation, and scaling, which give a total of fivedegrees of freedom. The proposed active contour comprises an inner and outercontour pair, which are closed and concentric. The active contour energy is acontrast function defined based on the intensities of pixels that lie insidethe inner contour and those that lie in the annulus between the inner and outercontours. We show that the contrast energy functional is optimal under certainconditions. The optimal RAT parameters are computed by maximizing the contrastfunction using a gradient descent optimizer. We show that the calculations aremade efficient through use of Green's theorem. The proposed formalism iscapable of handling a variety of shapes because for a chosen template,optimization is carried with respect to the RAT parameters only. The proposedformalism is validated on multiple images to show robustness to Gaussian andPoisson noise, to initialization, and to partial loss of structure in theobject to be segmented.
arxiv-1312-0786 | Image Representation Learning Using Graph Regularized Auto-Encoders |  http://arxiv.org/abs/1312.0786  | author:Yiyi Liao, Yue Wang, Yong Liu category:cs.LG K.3.2 published:2013-12-03 summary:We consider the problem of image representation for the tasks of unsupervisedlearning and semi-supervised learning. In those learning tasks, the raw imagevectors may not provide enough representation for their intrinsic structuresdue to their highly dense feature space. To overcome this problem, the rawimage vectors should be mapped to a proper representation space which cancapture the latent structure of the original data and represent the dataexplicitly for further learning tasks such as clustering. Inspired by the recent research works on deep neural network andrepresentation learning, in this paper, we introduce the multiple-layerauto-encoder into image representation, we also apply the locally invariantideal to our image representation with auto-encoders and propose a novelmethod, called Graph regularized Auto-Encoder (GAE). GAE can provide a compactrepresentation which uncovers the hidden semantics and simultaneously respectsthe intrinsic geometric structure. Extensive experiments on image clustering show encouraging results of theproposed algorithm in comparison to the state-of-the-art algorithms onreal-word cases.
arxiv-1312-0976 | Multilinguals and Wikipedia Editing |  http://arxiv.org/abs/1312.0976  | author:Scott A. Hale category:cs.CY cs.CL cs.DL cs.SI physics.soc-ph H.5.4; H.5.3 published:2013-12-03 summary:This article analyzes one month of edits to Wikipedia in order to examine therole of users editing multiple language editions (referred to as multilingualusers). Such multilingual users may serve an important function in diffusinginformation across different language editions of the encyclopedia, and priorwork has suggested this could reduce the level of self-focus bias in eachedition. This study finds multilingual users are much more active than theirsingle-edition (monolingual) counterparts. They are found in all languageeditions, but smaller-sized editions with fewer users have a higher percentageof multilingual users than larger-sized editions. About a quarter ofmultilingual users always edit the same articles in multiple languages, whilejust over 40% of multilingual users edit different articles in differentlanguages. When non-English users do edit a second language edition, thatedition is most frequently English. Nonetheless, several regional andlinguistic cross-editing patterns are also present.
arxiv-1312-0788 | A compact formula for the derivative of a 3-D rotation in exponential coordinates |  http://arxiv.org/abs/1312.0788  | author:Guillermo Gallego, Anthony Yezzi category:cs.CV math.OC published:2013-12-03 summary:We present a compact formula for the derivative of a 3-D rotation matrix withrespect to its exponential coordinates. A geometric interpretation of theresulting expression is provided, as well as its agreement with otherless-compact but better-known formulas. To the best of our knowledge, thissimpler formula does not appear anywhere in the literature. We hope byproviding this more compact expression to alleviate the common pressure toreluctantly resort to alternative representations in various computationalapplications simply as a means to avoid the complexity of differential analysisin exponential coordinates.
arxiv-1312-0790 | Test Set Selection using Active Information Acquisition for Predictive Models |  http://arxiv.org/abs/1312.0790  | author:Sneha Chaudhari, Pankaj Dayama, Vinayaka Pandit, Indrajit Bhattacharya category:cs.AI cs.LG stat.ML published:2013-12-03 summary:In this paper, we consider active information acquisition when the predictionmodel is meant to be applied on a targeted subset of the population. The goalis to label a pre-specified fraction of customers in the target or test set byiteratively querying for information from the non-target or training set. Thenumber of queries is limited by an overall budget. Arising in the context oftwo rather disparate applications- banking and medical diagnosis, we pose theactive information acquisition problem as a constrained optimization problem.We propose two greedy iterative algorithms for solving the above problem. Weconduct experiments with synthetic data and compare results of our proposedalgorithms with few other baseline approaches. The experimental results showthat our proposed approaches perform better than the baseline schemes.
arxiv-1312-0925 | Understanding Alternating Minimization for Matrix Completion |  http://arxiv.org/abs/1312.0925  | author:Moritz Hardt category:cs.LG cs.DS stat.ML published:2013-12-03 summary:Alternating Minimization is a widely used and empirically successfulheuristic for matrix completion and related low-rank optimization problems.Theoretical guarantees for Alternating Minimization have been hard to come byand are still poorly understood. This is in part because the heuristic isiterative and non-convex in nature. We give a new algorithm based onAlternating Minimization that provably recovers an unknown low-rank matrix froma random subsample of its entries under a standard incoherence assumption. Ourresults reduce the sample size requirements of the Alternating Minimizationapproach by at least a quartic factor in the rank and the condition number ofthe unknown matrix. These improvements apply even if the matrix is only closeto low-rank in the Frobenius norm. Our algorithm runs in nearly linear time inthe dimension of the matrix and, in a broad range of parameters, gives thestrongest sample bounds among all subquadratic time algorithms that we areaware of. Underlying our work is a new robust convergence analysis of the well-knownPower Method for computing the dominant singular vectors of a matrix. Thisviewpoint leads to a conceptually simple understanding of AlternatingMinimization. In addition, we contribute a new technique for controlling thecoherence of intermediate solutions arising in iterative algorithms based on asmoothed analysis of the QR factorization. These techniques may be of interestbeyond their application here.
arxiv-1312-0852 | Feature Extraction of Human Lip Prints |  http://arxiv.org/abs/1312.0852  | author:Samir Kumar Bandyopadhyay, S Arunkumar, Saptarshi Bhattacharjee category:cs.CV published:2013-12-03 summary:Methods have been used for identification of human by recognizing lip prints.Human lips have a number of elevation and depressions features called lipprints and examination of lip prints is referred to as cheiloscopy. Lip printsof each human being are unique in nature like many others features of human. Inthis paper lip print is first smoothened using a Gaussian Filter. Next SobelEdge Detector and Canny Edge Detector are used to detect the vertical andhorizontal groove pattern in the lip. This method of identification will beuseful both in criminal forensics and personal identification. It is ourassumption that study of lip prints and their types are well connected to playa song in a better way that are well accepted to people who loves to hearsongs.
