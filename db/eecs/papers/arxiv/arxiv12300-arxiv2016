arxiv-12300-1 | Energy saving in smart homes based on consumer behaviour: A case study | http://arxiv.org/pdf/1509.05722v1.pdf | author:Michael Zehnder, Holger Wache, Hans-Friedrich Witschel, Danilo Zanatta, Miguel Rodriguez category:stat.ML cs.AI cs.MA cs.SY published:2015-09-18 summary:This paper presents a case study of a recommender system that can be used tosave energy in smart homes without lowering the comfort of the inhabitants. Wepresent an algorithm that uses consumer behavior data only and uses machinelearning to suggest actions for inhabitants to reduce the energy consumption oftheir homes. The system mines for frequent and periodic patterns in the eventdata provided by the Digitalstrom home automation system. These patterns areconverted into association rules, prioritized and compared with the currentbehavior of the inhabitants. If the system detects an opportunities to saveenergy without decreasing the comfort level it sends a recommendation to theresidents.
arxiv-12300-2 | Computational evolution of decision-making strategies | http://arxiv.org/pdf/1509.05646v1.pdf | author:Peter Kvam, Joseph Cesario, Jory Schossau, Heather Eisthen, Arend Hintze category:cs.NE q-bio.NC published:2015-09-18 summary:Most research on adaptive decision-making takes a strategy-first approach,proposing a method of solving a problem and then examining whether it can beimplemented in the brain and in what environments it succeeds. We present amethod for studying strategy development based on computational evolution thattakes the opposite approach, allowing strategies to develop in response to thedecision-making environment via Darwinian evolution. We apply this approach toa dynamic decision-making problem where artificial agents make decisions aboutthe source of incoming information. In doing so, we show that the complexity ofthe brains and strategies of evolved agents are a function of the environmentin which they develop. More difficult environments lead to larger brains andmore information use, resulting in strategies resembling a sequential samplingapproach. Less difficult environments drive evolution toward smaller brains andless information use, resulting in simpler heuristic-like strategies.
arxiv-12300-3 | A Model for Foraging Ants, Controlled by Spiking Neural Networks and Double Pheromones | http://arxiv.org/pdf/1507.08467v3.pdf | author:Cristian Jimenez-Romero, David Sousa-Rodrigues, Jeffrey H. Johnson, Vitorino Ramos category:cs.NE cs.AI published:2015-07-30 summary:A model of an Ant System where ants are controlled by a spiking neuralcircuit and a second order pheromone mechanism in a foraging task is presented.A neural circuit is trained for individual ants and subsequently the ants areexposed to a virtual environment where a swarm of ants performed a resourceforaging task. The model comprises an associative and unsupervised learningstrategy for the neural circuit of the ant. The neural circuit adapts to theenvironment by means of classical conditioning. The initially unknownenvironment includes different types of stimuli representing food and obstacleswhich, when they come in direct contact with the ant, elicit a reflex responsein the motor neural system of the ant: moving towards or away from the sourceof the stimulus. The ants are released on a landscape with multiple foodsources where one ant alone would have difficulty harvesting the landscape tomaximum efficiency. The introduction of a double pheromone mechanism yieldsbetter results than traditional ant colony optimization strategies. Traditionalant systems include mainly a positive reinforcement pheromone. This approachuses a second pheromone that acts as a marker for forbidden paths (negativefeedback). This blockade is not permanent and is controlled by the evaporationrate of the pheromones. The combined action of both pheromones acts as acollective stigmergic memory of the swarm, which reduces the search space ofthe problem. This paper explores how the adaptation and learning abilitiesobserved in biologically inspired cognitive architectures is synergisticallyenhanced by swarm optimization strategies. The model portraits two forms ofartificial intelligent behaviour: at the individual level the spiking neuralnetwork is the main controller and at the collective level the pheromonedistribution is a map towards the solution emerged by the colony.
arxiv-12300-4 | Linearized Kernel Dictionary Learning | http://arxiv.org/pdf/1509.05634v1.pdf | author:Alona Golts, Michael Elad category:cs.CV published:2015-09-18 summary:In this paper we present a new approach of incorporating kernels intodictionary learning. The kernel K-SVD algorithm (KKSVD), which has beenintroduced recently, shows an improvement in classification performance, withrelation to its linear counterpart K-SVD. However, this algorithm requires thestorage and handling of a very large kernel matrix, which leads to highcomputational cost, while also limiting its use to setups with small number oftraining examples. We address these problems by combining two ideas: first weapproximate the kernel matrix using a cleverly sampled subset of its columnsusing the Nystr\"{o}m method; secondly, as we wish to avoid using this matrixaltogether, we decompose it by SVD to form new "virtual samples," on which anylinear dictionary learning can be employed. Our method, termed "LinearizedKernel Dictionary Learning" (LKDL) can be seamlessly applied as apre-processing stage on top of any efficient off-the-shelf dictionary learningscheme, effectively "kernelizing" it. We demonstrate the effectiveness of ourmethod on several tasks of both supervised and unsupervised classification andshow the efficiency of the proposed scheme, its easy integration andperformance boosting properties.
arxiv-12300-5 | Sports highlights generation based on acoustic events detection: A rugby case study | http://arxiv.org/pdf/1509.06279v1.pdf | author:Anant Baijal, Jaeyoun Cho, Woojung Lee, Byeong-Seob Ko category:cs.SD cs.AI cs.LG published:2015-09-18 summary:We approach the challenging problem of generating highlights from sportsbroadcasts utilizing audio information only. A language-independent,multi-stage classification approach is employed for detection of key acousticevents which then act as a platform for summarization of highlight scenes.Objective results and human experience indicate that our system is highlyefficient.
arxiv-12300-6 | Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination | http://arxiv.org/pdf/1507.00955v3.pdf | author:Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste category:cs.CL cs.IR cs.LG stat.ME stat.ML published:2015-07-03 summary:This paper covers the two approaches for sentiment analysis: i) lexicon basedmethod; ii) machine learning method. We describe several techniques toimplement these approaches and discuss how they can be adopted for sentimentclassification of Twitter messages. We present a comparative study of differentlexicon combinations and show that enhancing sentiment lexicons with emoticons,abbreviations and social-media slang expressions increases the accuracy oflexicon-based classification for Twitter. We discuss the importance of featuregeneration and feature selection processes for machine learning sentimentclassification. To quantify the performance of the main sentiment analysismethods over Twitter we run these algorithms on a benchmark Twitter datasetfrom the SemEval-2013 competition, task 2-B. The results show that machinelearning method based on SVM and Naive Bayes classifiers outperforms thelexicon method. We present a new ensemble method that uses a lexicon basedsentiment score as input feature for the machine learning approach. Thecombined method proved to produce more precise classifications. We also showthat employing a cost-sensitive classifier for highly unbalanced datasetsyields an improvement of sentiment classification performance up to 7%.
arxiv-12300-7 | Color-Stripe Structured Light Robust to Surface Color and Discontinuity | http://arxiv.org/pdf/1509.05592v1.pdf | author:Kwang Hee Lee, Changsoo Je, Sang Wook Lee category:cs.CV cs.GR physics.optics I.2.10; I.4.8 published:2015-09-18 summary:Multiple color stripes have been employed for structured light-based rapidrange imaging to increase the number of uniquely identifiable stripes. The useof multiple color stripes poses two problems: (1) object surface color maydisturb the stripe color and (2) the number of adjacent stripes required foridentifying a stripe may not be maintained near surface discontinuities such asoccluding boundaries. In this paper, we present methods to alleviate thoseproblems. Log-gradient filters are employed to reduce the influence of objectcolors, and color stripes in two and three directions are used to increase thechance of identifying correct stripes near surface discontinuities.Experimental results demonstrate the effectiveness of our methods.
arxiv-12300-8 | ORB-SLAM: a Versatile and Accurate Monocular SLAM System | http://arxiv.org/pdf/1502.00956v2.pdf | author:Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos category:cs.RO cs.CV published:2015-02-03 summary:This paper presents ORB-SLAM, a feature-based monocular SLAM system thatoperates in real time, in small and large, indoor and outdoor environments. Thesystem is robust to severe motion clutter, allows wide baseline loop closingand relocalization, and includes full automatic initialization. Building onexcellent algorithms of recent years, we designed from scratch a novel systemthat uses the same features for all SLAM tasks: tracking, mapping,relocalization, and loop closing. A survival of the fittest strategy thatselects the points and keyframes of the reconstruction leads to excellentrobustness and generates a compact and trackable map that only grows if thescene content changes, allowing lifelong operation. We present an exhaustiveevaluation in 27 sequences from the most popular datasets. ORB-SLAM achievesunprecedented performance with respect to other state-of-the-art monocular SLAMapproaches. For the benefit of the community, we make the source code public.
arxiv-12300-9 | Efficient Clustering on Riemannian Manifolds: A Kernelised Random Projection Approach | http://arxiv.org/pdf/1509.05536v1.pdf | author:Kun Zhao, Azadeh Alavi, Arnold Wiliem, Brian C. Lovell category:cs.CV published:2015-09-18 summary:Reformulating computer vision problems over Riemannian manifolds hasdemonstrated superior performance in various computer vision applications. Thisis because visual data often forms a special structure lying on a lowerdimensional space embedded in a higher dimensional space. However, since thesemanifolds belong to non-Euclidean topological spaces, exploiting theirstructures is computationally expensive, especially when one considers theclustering analysis of massive amounts of data. To this end, we propose anefficient framework to address the clustering problem on Riemannian manifolds.This framework implements random projections for manifold points via kernelspace, which can preserve the geometric structure of the original space, but iscomputationally efficient. Here, we introduce three methods that follow ourframework. We then validate our framework on several computer visionapplications by comparing against popular clustering methods on Riemannianmanifolds. Experimental results demonstrate that our framework maintains theperformance of the clustering whilst massively reducing computationalcomplexity by over two orders of magnitude in some cases.
arxiv-12300-10 | Generative Image Modeling Using Spatial LSTMs | http://arxiv.org/pdf/1506.03478v2.pdf | author:Lucas Theis, Matthias Bethge category:stat.ML cs.CV cs.LG published:2015-06-10 summary:Modeling the distribution of natural images is challenging, partly because ofstrong statistical dependencies which can extend over hundreds of pixels.Recurrent neural networks have been successful in capturing long-rangedependencies in a number of problems but only recently have found their wayinto generative image models. We here introduce a recurrent image model basedon multi-dimensional long short-term memory units which are particularly suitedfor image modeling due to their spatial structure. Our model scales to imagesof arbitrary size and its likelihood is computationally tractable. We find thatit outperforms the state of the art in quantitative comparisons on severalimage datasets and produces promising results when used for texture synthesisand inpainting.
arxiv-12300-11 | An Experimental Survey on Correlation Filter-based Tracking | http://arxiv.org/pdf/1509.05520v1.pdf | author:Zhe Chen, Zhibin Hong, Dacheng Tao category:cs.CV published:2015-09-18 summary:Over these years, Correlation Filter-based Trackers (CFTs) have arousedincreasing interests in the field of visual object tracking, and have achievedextremely compelling results in different competitions and benchmarks. In thispaper, our goal is to review the developments of CFTs with extensiveexperimental results. 11 trackers are surveyed in our work, based on which ageneral framework is summarized. Furthermore, we investigate different trainingschemes for correlation filters, and also discuss various effectiveimprovements that have been made recently. Comprehensive experiments have beenconducted to evaluate the effectiveness and efficiency of the surveyed CFTs,and comparisons have been made with other competing trackers. The experimentalresults have shown that state-of-art performance, in terms of robustness, speedand accuracy, can be achieved by several recent CFTs, such as MUSTer and SAMF.We find that further improvements for correlation filter-based tracking can bemade on estimating scales, applying part-based tracking strategy andcooperating with long-term tracking methods.
arxiv-12300-12 | A Light Sliding-Window Part-of-Speech Tagger for the Apertium Free/Open-Source Machine Translation Platform | http://arxiv.org/pdf/1509.05517v1.pdf | author:Gang Chen, Mikel L. Forcada category:cs.CL published:2015-09-18 summary:This paper describes a free/open-source implementation of the lightsliding-window (LSW) part-of-speech tagger for the Apertium free/open-sourcemachine translation platform. Firstly, the mechanism and training process ofthe tagger are reviewed, and a new method for incorporating linguistic rules isproposed. Secondly, experiments are conducted to compare the performances ofthe tagger under different window settings, with or without Apertium-style"forbid" rules, with or without Constraint Grammar, and also with respect tothe traditional HMM tagger in Apertium.
arxiv-12300-13 | Accelerated Distance Computation with Encoding Tree for High Dimensional Data | http://arxiv.org/pdf/1509.05186v2.pdf | author:Shicong Liu, Junru Shao, Hongtao Lu category:cs.CV published:2015-09-17 summary:We propose a novel distance to calculate distance between high dimensionalvector pairs, utilizing vector quantization generated encodings. Vectorquantization based methods are successful in handling large scale highdimensional data. These methods compress vectors into short encodings, andallow efficient distance computation between an uncompressed vector andcompressed dataset without decompressing explicitly. However for largedatasets, these distance computing methods perform excessive computations. Weavoid excessive computations by storing the encodings on an EncodingTree(E-Tree), interestingly the memory consumption is also lowered. We alsopropose Encoding Forest(E-Forest) to further lower the computation cost. E-Treeand E-Forest is compatible with various existing quantization-based methods. Weshow by experiments our methods speed-up distance computing for highdimensional data drastically, and various existing algorithms can benefit fromour methods.
arxiv-12300-14 | A Ternary Non-Commutative Latent Factor Model for Scalable Three-Way Real Tensor Completion | http://arxiv.org/pdf/1410.7383v6.pdf | author:Guy Baruch category:stat.ML published:2014-10-26 summary:Motivated by large-scale Collaborative-Filtering applications, we present aNon-Commuting Latent Factor (NCLF) tensor-completion approach for modelingthree-way arrays, which is diagonal like the standard PARAFAC, but whereindifferent terms distinguish different kinds of three-way relations ofco-clusters, as determined by permutations of latent factors. The first keycomponent of the algebraic representation is the usage of two non-commutativereal trilinear operations as the building blocks of the approximation. Theseoperations are the standard three dimensional triple-product and a trilinearproduct on a two-dimensional real vector space, which is a representation ofthe real Clifford Algebra Cl(1,1) (a certain Majorana spinor). Both operationsare purely ternary in that they cannot be decomposed into two group-operationson the relevant spaces. The second key component of the method is combiningthese operations using permutation-symmetry preserving linear combinations. Weapply the model to the MovieLens and Fannie Mae datasets, and find that itoutperforms the PARAFAC model. We propose some future directions, such asunsupervised-learning.
arxiv-12300-15 | Algorithmic statistics, prediction and machine learning | http://arxiv.org/pdf/1509.05473v1.pdf | author:Alexey Milovanov category:cs.LG cs.IT math.IT published:2015-09-17 summary:Algorithmic statistics considers the following problem: given a binary string$x$ (e.g., some experimental data), find a "good" explanation of this data. Ituses algorithmic information theory to define formally what is a goodexplanation. In this paper we extend this framework in two directions. First, the explanations are not only interesting in themselves but also usedfor prediction: we want to know what kind of data we may reasonably expect insimilar situations (repeating the same experiment). We show that some kind ofhierarchy can be constructed both in terms of algorithmic statistics and usingthe notion of a priori probability, and these two approaches turn out to beequivalent. Second, a more realistic approach that goes back to machine learning theory,assumes that we have not a single data string $x$ but some set of "positiveexamples" $x_1,\ldots,x_l$ that all belong to some unknown set $A$, a propertythat we want to learn. We want this set $A$ to contain all positive examplesand to be as small and simple as possible. We show how algorithmic statisticcan be extended to cover this situation.
arxiv-12300-16 | Learning to Hash for Indexing Big Data - A Survey | http://arxiv.org/pdf/1509.05472v1.pdf | author:Jun Wang, Wei Liu, Sanjiv Kumar, Shih-Fu Chang category:cs.LG published:2015-09-17 summary:The explosive growth in big data has attracted much attention in designingefficient indexing and search methods recently. In many critical applicationssuch as large-scale search and pattern matching, finding the nearest neighborsto a query is a fundamental research problem. However, the straightforwardsolution using exhaustive comparison is infeasible due to the prohibitivecomputational complexity and memory requirement. In response, ApproximateNearest Neighbor (ANN) search based on hashing techniques has become populardue to its promising performance in both efficiency and accuracy. Priorrandomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), exploredata-independent hash functions with random projections or permutations.Although having elegant theoretic guarantees on the search quality in certainmetric spaces, performance of randomized hashing has been shown insufficient inmany real-world applications. As a remedy, new approaches incorporatingdata-driven learning methods in development of advanced hash functions haveemerged. Such learning to hash methods exploit information such as datadistributions or class labels when optimizing the hash codes or functions.Importantly, the learned hash codes are able to preserve the proximity ofneighboring data in the original feature spaces in the hash code spaces. Thegoal of this paper is to provide readers with systematic understanding ofinsights, pros and cons of the emerging techniques. We provide a comprehensivesurvey of the learning to hash framework and representative techniques ofvarious types, including unsupervised, semi-supervised, and supervised. Inaddition, we also summarize recent hashing approaches utilizing the deeplearning models. Finally, we discuss the future direction and trends ofresearch in this area.
arxiv-12300-17 | Sparse Fisher's Linear Discriminant Analysis for Partially Labeled Data | http://arxiv.org/pdf/1509.05438v1.pdf | author:Qiyi Lu, Xingye Qiao category:stat.ML stat.ME published:2015-09-17 summary:Classification is an important tool with many useful applications. Among themany classification methods, Fisher's Linear Discriminant Analysis (LDA) is atraditional model-based approach which makes use of the covariance information.However, in the high-dimensional, low-sample size setting, LDA cannot bedirectly deployed because the sample covariance is not invertible. While thereare modern methods designed to deal with high-dimensional data, they may notfully use the covariance information as LDA does. Hence in some situations, itis still desirable to use a model-based method such as LDA for classification.This article exploits the potential of LDA in more complicated data settings.In many real applications, it is costly to manually place labels onobservations; hence it is often that only a small portion of labeled data isavailable while a large number of observations are left without a label. It isa great challenge to obtain good classification performance through the labeleddata alone, especially when the dimension is greater than the size of thelabeled data. In order to overcome this issue, we propose a semi-supervisedsparse LDA classifier to take advantage of the seemingly useless unlabeleddata. They provide additional information which helps to boost theclassification performance in some situations. A direct estimation method isused to reconstruct LDA and achieve the sparsity; meanwhile we employ thedifference-convex algorithm to handle the non-convex loss function associatedwith the unlabeled data. Theoretical properties of the proposed classifier arestudied. Our simulated examples help to understand when and how the informationextracted from the unlabeled data can be useful. A real data example furtherillustrates the usefulness of the proposed method.
arxiv-12300-18 | DeXpression: Deep Convolutional Neural Network for Expression Recognition | http://arxiv.org/pdf/1509.05371v1.pdf | author:Peter Burkert, Felix Trier, Muhammad Zeshan Afzal, Andreas Dengel, Marcus Liwicki category:cs.CV cs.LG published:2015-09-17 summary:We propose a convolutional neural network (CNN) architecture for facialexpression recognition. The proposed architecture is independent of anyhand-crafted feature extraction and performs better than the earlier proposedconvolutional neural network based approaches. We visualize the automaticallyextracted features which have been learned by the network in order to provide abetter understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP)and MMI Facial Expression Databse are used for the quantitative evaluation. Onthe CKP set the current state of the art approach, using CNNs, achieves anaccuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotionrecognition is 93.33%. The proposed architecture achieves 99.6% for CKP and98.63% for MMI, therefore performing better than the state of the art usingCNNs. Automatic facial expression recognition has a broad spectrum ofapplications such as human-computer interaction and safety systems. This is dueto the fact that non-verbal cues are important forms of communication and playa pivotal role in interpersonal communication. The performance of the proposedarchitecture endorses the efficacy and reliable usage of the proposed work forreal world applications.
arxiv-12300-19 | Facial Descriptors for Human Interaction Recognition In Still Images | http://arxiv.org/pdf/1509.05366v1.pdf | author:Gokhan Tanisik, Cemil Zalluhoglu, Nazli Ikizler-Cinbis category:cs.CV published:2015-09-17 summary:This paper presents a novel approach in a rarely studied area of computervision: Human interaction recognition in still images. We explore whether thefacial regions and their spatial configurations contribute to the recognitionof interactions. In this respect, our method involves extraction of severalvisual features from the facial regions, as well as incorporation of scenecharacteristics and deep features to the recognition. Extracted multiplefeatures are utilized within a discriminative learning framework forrecognizing interactions between people. Our designed facial descriptors arebased on the observation that relative positions, size and locations of thefaces are likely to be important for characterizing human interactions. Sincethere is no available dataset in this relatively new domain, a comprehensivenew dataset which includes several images of human interactions is collected.Our experimental results show that faces and scene characteristics containimportant information to recognize interactions between people.
arxiv-12300-20 | Recurrent Spatial Transformer Networks | http://arxiv.org/pdf/1509.05329v1.pdf | author:Søren Kaae Sønderby, Casper Kaae Sønderby, Lars Maaløe, Ole Winther category:cs.CV published:2015-09-17 summary:We integrate the recently proposed spatial transformer network (SPN)[Jaderberg et. al 2015] into a recurrent neural network (RNN) to form anRNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNISTsequences. The proposed model achieves a single digit error of 1.5% compared to2.9% for a convolutional networks and 2.0% for convolutional networks with SPNlayers. The SPN outputs a zoomed, rotated and skewed version of the inputimage. We investigate different down-sampling factors (ratio of pixel in inputand output) for the SPN and show that the RNN-SPN model is able to down-samplethe input images without deteriorating performance. The down-sampling inRNN-SPN can be thought of as adaptive down-sampling that minimizes theinformation loss in the regions of interest. We attribute the superiorperformance of the RNN-SPN to the fact that it can attend to a sequence ofregions of interest.
arxiv-12300-21 | Humans Are Easily Fooled by Digital Images | http://arxiv.org/pdf/1509.05301v1.pdf | author:Victor Schetinger, Manuel M. Oliveira, Roberto da Silva, Tiago J. Carvalho category:cs.GR cs.CV cs.HC published:2015-09-17 summary:Digital images are ubiquitous in our modern lives, with uses ranging fromsocial media to news, and even scientific papers. For this reason, it iscrucial evaluate how accurate people are when performing the task of identifydoctored images. In this paper, we performed an extensive user study evaluatingsubjects capacity to detect fake images. After observing an image, users havebeen asked if it had been altered or not. If the user answered the image hasbeen altered, he had to provide evidence in the form of a click on the image.We collected 17,208 individual answers from 383 users, using 177 imagesselected from public forensic databases. Different from other previouslystudies, our method propose different ways to avoid lucky guess when evaluatingusers answers. Our results indicate that people show inaccurate skills atdifferentiating between altered and non-altered images, with an accuracy of58%, and only identifying the modified images 46.5% of the time. We also trackuser features such as age, answering time, confidence, providing deep analysisof how such variables influence on the users' performance.
arxiv-12300-22 | Decadal climate predictions using sequential learning algorithms | http://arxiv.org/pdf/1509.05285v1.pdf | author:Ehud Strobach, Golan Bel category:physics.ao-ph stat.ML 62C99 published:2015-09-17 summary:Ensembles of climate models are commonly used to improve climate predictionsand assess the uncertainties associated with them. Weighting the modelsaccording to their performances holds the promise of further improving theirpredictions. Here, we use an ensemble of decadal climate predictions todemonstrate the ability of sequential learning algorithms (SLAs) to reduce theforecast errors and reduce the uncertainties. Three different SLAs areconsidered, and their performances are compared with those of an equallyweighted ensemble, a linear regression and the climatology. Predictions of fourdifferent variables--the surface temperature, the zonal and meridional wind,and pressure--are considered. The spatial distributions of the performances arepresented, and the statistical significance of the improvements achieved by theSLAs is tested. Based on the performances of the SLAs, we propose one to behighly suitable for the improvement of decadal climate predictions.
arxiv-12300-23 | Network analysis of named entity interactions in written texts | http://arxiv.org/pdf/1509.05281v1.pdf | author:Diego R. Amancio category:cs.CL physics.soc-ph published:2015-09-17 summary:The use of methods borrowed from statistics and physics has allowed for thediscovery of unprecedent patterns of human behavior and cognition byestablishing links between models features and language structure. Whilecurrent models have been useful to identify patterns via analysis ofsyntactical and semantical networks, only a few works have probed the relevanceof investigating the structure arising from the relationship between relevantentities such as characters, locations and organizations. In this study, weintroduce a model that links entities appearing in the same context in order tocapture the complexity of entities organization through a networkedrepresentation. Computational simulations in books revealed that the proposedmodel displays interesting topological features, such as short typical shortestpath length, high values of clustering coefficient and modular organization.The effectiveness of the our model was verified in a practical patternrecognition task in real networks. When compared with the traditional wordadjacency networks, our model displayed optimized results in identifyingunknown references in texts. Because the proposed model plays a complementaryrole in characterizing unstructured documents via topological analysis of namedentities, we believe that it could be useful to improve the characterizationwritten texts when combined with other traditional approaches based onstatistical and deeper paradigms.
arxiv-12300-24 | Deep Multi-task Learning for Railway Track Inspection | http://arxiv.org/pdf/1509.05267v1.pdf | author:Xavier Gibert, Vishal M. Patel, Rama Chellappa category:cs.CV published:2015-09-17 summary:Railroad tracks need to be periodically inspected and monitored to ensuresafe transportation. Automated track inspection using computer vision andpattern recognition methods have recently shown the potential to improve safetyby allowing for more frequent inspections while reducing human errors.Achieving full automation is still very challenging due to the number ofdifferent possible failure modes as well as the broad range of image variationsthat can potentially trigger false alarms. Also, the number of defectivecomponents is very small, so not many training examples are available for themachine to learn a robust anomaly detector. In this paper, we show thatdetection performance can be improved by combining multiple detectors within amulti-task learning framework. We show that this approach results in betteraccuracy in detecting defects on railway ties and fasteners.
arxiv-12300-25 | (Blue) Taxi Destination and Trip Time Prediction from Partial Trajectories | http://arxiv.org/pdf/1509.05257v1.pdf | author:Hoang Thanh Lam, Ernesto Diaz-Aviles, Alessandra Pascale, Yiannis Gkoufas, Bei Chen category:stat.ML cs.AI cs.CY cs.LG I.2.6; I.5.2 published:2015-09-17 summary:Real-time estimation of destination and travel time for taxis is of greatimportance for existing electronic dispatch systems. We present an approachbased on trip matching and ensemble learning, in which we leverage the patternsobserved in a dataset of roughly 1.7 million taxi journeys to predict thecorresponding final destination and travel time for ongoing taxi trips, as asolution for the ECML/PKDD Discovery Challenge 2015 competition. The results ofour empirical evaluation show that our approach is effective and very robust,which led our team -- BlueTaxi -- to the 3rd and 7th position of the finalrankings for the trip time and destination prediction tasks, respectively.Given the fact that the final rankings were computed using a very small testset (with only 320 trips) we believe that our approach is one of the mostrobust solutions for the challenge based on the consistency of our good resultsacross the test sets.
arxiv-12300-26 | Extraction of evidence tables from abstracts of randomized clinical trials using a maximum entropy classifier and global constraints | http://arxiv.org/pdf/1509.05209v1.pdf | author:Antonio Trenta, Anthony Hunter, Sebastian Riedel category:cs.CL cs.AI published:2015-09-17 summary:Systematic use of the published results of randomized clinical trials isincreasingly important in evidence-based medicine. In order to collate andanalyze the results from potentially numerous trials, evidence tables are usedto represent trials concerning a set of interventions of interest. An evidencetable has columns for the patient group, for each of the interventions beingcompared, for the criterion for the comparison (e.g. proportion who survivedafter 5 years from treatment), and for each of the results. Currently, it is alabour-intensive activity to read each published paper and extract theinformation for each field in an evidence table. There have been some NLPstudies investigating how some of the features from papers can be extracted, orat least the relevant sentences identified. However, there is a lack of an NLPsystem for the systematic extraction of each item of information required foran evidence table. We address this need by a combination of a maximum entropyclassifier, and integer linear programming. We use the later to handleconstraints on what is an acceptable classification of the features to beextracted. With experimental results, we demonstrate substantial advantages inusing global constraints (such as the features describing the patient group,and the interventions, must occur before the features describing the results ofthe comparison).
arxiv-12300-27 | Improved Residual Vector Quantization for High-dimensional Approximate Nearest Neighbor Search | http://arxiv.org/pdf/1509.05195v1.pdf | author:Shicong Liu, Hongtao Lu, Junru Shao category:cs.CV published:2015-09-17 summary:Quantization methods have been introduced to perform large scale approximatenearest search tasks. Residual Vector Quantization (RVQ) is one of theeffective quantization methods. RVQ uses a multi-stage codebook learning schemeto lower the quantization error stage by stage. However, there are two majorlimitations for RVQ when applied to on high-dimensional approximate nearestneighbor search: 1. The performance gain diminishes quickly with added stages.2. Encoding a vector with RVQ is actually NP-hard. In this paper, we propose animproved residual vector quantization (IRVQ) method, our IRVQ learns codebookwith a hybrid method of subspace clustering and warm-started k-means on eachstage to prevent performance gain from dropping, and uses a multi-path encodingscheme to encode a vector with lower distortion. Experimental results on thebenchmark datasets show that our method gives substantially improves RVQ anddelivers better performance compared to the state-of-the-art.
arxiv-12300-28 | HCLAE: High Capacity Locally Aggregating Encodings for Approximate Nearest Neighbor Search | http://arxiv.org/pdf/1509.05194v1.pdf | author:Shicong Liu, Junru Shao, Hongtao Lu category:cs.CV published:2015-09-17 summary:Vector quantization-based approaches are successful to solve ApproximateNearest Neighbor (ANN) problems which are critical to many applications. Theidea is to generate effective encodings to allow fast distance approximation.We propose quantization-based methods should partition the data space finelyand exhibit locality of the dataset to allow efficient non-exhaustive search.In this paper, we introduce the concept of High Capacity Locality AggregatingEncodings (HCLAE) to this end, and propose Dictionary Annealing (DA) to learnHCLAE by a simulated annealing procedure. The quantization error is lower thanother state-of-the-art. The algorithms of DA can be easily extended to anonline learning scheme, allowing effective handle of large scale data. Further,we propose Aggregating-Tree (A-Tree), a non-exhaustive search method usingHCLAE to perform efficient ANN-Search. A-Tree achieves magnitudes of speed-upon ANN-Search tasks, compared to the state-of-the-art.
arxiv-12300-29 | Taming the ReLU with Parallel Dither in a Deep Neural Network | http://arxiv.org/pdf/1509.05173v1.pdf | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-09-17 summary:Rectified Linear Units (ReLU) seem to have displaced traditional 'smooth'nonlinearities as activation-function-du-jour in many - but not all - deepneural network (DNN) applications. However, nobody seems to know why. In thisarticle, we argue that ReLU are useful because they are ideal demodulators -this helps them perform fast abstract learning. However, this fast learningcomes at the expense of serious nonlinear distortion products - decoy features.We show that Parallel Dither acts to suppress the decoy features, preventingoverfitting and leaving the true features cleanly demodulated for rapid,reliable learning.
arxiv-12300-30 | Learning Preferences from Assortment Choices in a Heterogeneous Population | http://arxiv.org/pdf/1509.05113v1.pdf | author:Nathan Kallus, Madeleine Udell category:stat.ML cs.LG math.OC published:2015-09-17 summary:We consider the problem of learning the preferences of a heterogeneouscustomer population by observing their choices from an assortment of products,ads, or other offerings. Our observation model takes a form common inassortment planning: each arriving customer chooses from an assortment ofofferings consisting of a subset of all possibilities. One-size-fits-all choicemodeling can fit heterogeneous populations quite poorly, and misses theopportunity for assortment customization in online retail. On the other hand,time, revenue, and inventory targets rule out exploring the preferences ofevery customer or segment. In this paper we propose a mixture choice model witha natural underlying low-dimensional structure, and show how to estimate itsparameters. In our model, the preferences of each customer or segment follow aseparate parametric choice model, but the underlying structure of theseparameters over all the models has low dimension. We show that a nuclear-normregularized maximum likelihood estimator can learn the preferences of allcustomers using a number of observations much smaller than the number ofitem-customer combinations. This result shows the potential for structuralassumptions to speed up learning and improve revenues in assortment planningand customization.
arxiv-12300-31 | Fast Sequence Component Analysis for Attack Detection in Synchrophasor Networks | http://arxiv.org/pdf/1509.05086v1.pdf | author:Jordan Landford, Rich Meier, Richard Barella, Xinghui Zhao, Eduardo Cotilla-Sanchez, Robert B. Bass, Scott Wallace category:cs.LG cs.CR published:2015-09-17 summary:Modern power systems have begun integrating synchrophasor technologies intopart of daily operations. Given the amount of solutions offered and thematurity rate of application development it is not a matter of "if" but amatter of "when" in regards to these technologies becoming ubiquitous incontrol centers around the world. While the benefits are numerous, thefunctionality of operator-level applications can easily be nullified byinjection of deceptive data signals disguised as genuine measurements. Suchdeceptive action is a common precursor to nefarious, often malicious activity.A correlation coefficient characterization and machine learning methodology areproposed to detect and identify injection of spoofed data signals. The proposedmethod utilizes statistical relationships intrinsic to power system parameters,which are quantified and presented. Several spoofing schemes have beendeveloped to qualitatively and quantitatively demonstrate detectioncapabilities.
arxiv-12300-32 | Exact simultaneous recovery of locations and structure from known orientations and corrupted point correspondences | http://arxiv.org/pdf/1509.05064v1.pdf | author:Paul Hand, Choongbum Lee, Vladislav Voroninski category:cs.CV cs.IT math.CO math.IT math.OC published:2015-09-16 summary:Let $t_1,\ldots,t_{n_l} \in \mathbb{R}^d$ and $p_1,\ldots,p_{n_s} \in\mathbb{R}^d$ and consider the bipartite location recovery problem: given asubset of pairwise direction observations $\{(t_i - p_j) / \t_i -p_j\_2\}_{i,j \in [n_l] \times [n_s]}$, where a constant fraction of theseobservations are arbitrarily corrupted, find $\{t_i\}_{i \in [n_ll]}$ and$\{p_j\}_{j \in [n_s]}$ up to a global translation and scale. We study therecently introduced ShapeFit algorithm as a method for solving this bipartitelocation recovery problem. In this case, ShapeFit consists of a simple convexprogram over $d(n_l + n_s)$ real variables. We prove that this program recoversa set of $n_l+n_s$ i.i.d. Gaussian locations exactly and with high probabilityif the observations are given by a bipartite Erd\H{o}s-R\'{e}nyi graph, $d$ islarge enough, and provided that at most a constant fraction of observationsinvolving any particular location are adversarially corrupted. This recoverytheorem is based on a set of deterministic conditions that we prove aresufficient for exact recovery. Finally, we propose a modified pipeline for theStructure for Motion problem, based on this bipartite location recoveryproblem.
arxiv-12300-33 | Overcomplete Dictionary Learning with Jacobi Atom Updates | http://arxiv.org/pdf/1509.05054v1.pdf | author:Paul Irofti, Bogdan Dumitrescu category:cs.CV published:2015-09-16 summary:Dictionary learning for sparse representations is traditionally approachedwith sequential atom updates, in which an optimized atom is used immediatelyfor the optimization of the next atoms. We propose instead a Jacobi version, inwhich groups of atoms are updated independently, in parallel. Extensivenumerical evidence for sparse image representation shows that the parallelalgorithms, especially when all atoms are updated simultaneously, give betterdictionaries than their sequential counterparts.
arxiv-12300-34 | Recurrent Neural Networks for Driver Activity Anticipation via Sensory-Fusion Architecture | http://arxiv.org/pdf/1509.05016v1.pdf | author:Ashesh Jain, Avi Singh, Hema S Koppula, Shane Soh, Ashutosh Saxena category:cs.CV cs.AI cs.RO published:2015-09-16 summary:Anticipating the future actions of a human is a widely studied problem inrobotics that requires spatio-temporal reasoning. In this work we propose adeep learning approach for anticipation in sensory-rich robotics applications.We introduce a sensory-fusion architecture which jointly learns to anticipateand fuse information from multiple sensory streams. Our architecture consistsof Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM)units to capture long temporal dependencies. We train our architecture in asequence-to-sequence prediction manner, and it explicitly learns to predict thefuture given only a partial temporal context. We further introduce a novel losslayer for anticipation which prevents over-fitting and encourages earlyanticipation. We use our architecture to anticipate driving maneuvers severalseconds before they happen on a natural driving data set of 1180 miles. Thecontext for maneuver anticipation comes from multiple sensors installed on thevehicle. Our approach shows significant improvement over the state-of-the-artin maneuver anticipation by increasing the precision from 77.4% to 90.5% andrecall from 71.2% to 87.4%.
arxiv-12300-35 | Human and Sheep Facial Landmarks Localisation by Triplet Interpolated Features | http://arxiv.org/pdf/1509.04954v1.pdf | author:Heng Yang, Renqiao Zhang, Peter Robinson category:cs.CV published:2015-09-16 summary:In this paper we present a method for localisation of facial landmarks onhuman and sheep. We introduce a new feature extraction scheme calledtriplet-interpolated feature used at each iteration of the cascaded shaperegression framework. It is able to extract features from similar semanticlocation given an estimated shape, even when head pose variations are large andthe facial landmarks are very sparsely distributed. Furthermore, we study theimpact of training data imbalance on model performance and propose a trainingsample augmentation scheme that produces more initialisations for trainingsamples from the minority. More specifically, the augmentation number for atraining sample is made to be negatively correlated to the value of the fittedprobability density function at the sample's position. We evaluate the proposedscheme on both human and sheep facial landmarks localisation. On the benchmark300w human face dataset, we demonstrate the benefits of our proposed methodsand show very competitive performance when comparing to other methods. On anewly created sheep face dataset, we get very good performance despite the factthat we only have a limited number of training samples and a set of sparselandmarks are annotated.
arxiv-12300-36 | Thin Structure Estimation with Curvature Regularization | http://arxiv.org/pdf/1506.04654v2.pdf | author:Dmitrii Marin, Yuri Boykov, Yuchen Zhong category:cs.CV published:2015-06-15 summary:Many applications in vision require estimation of thin structures such asboundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike mostprevious approaches, we simultaneously detect and delineate thin structureswith sub-pixel localization and real-valued orientation estimation. This is anill-posed problem that requires regularization. We propose an objectivefunction combining detection likelihoods with a prior minimizing curvature ofthe center-lines or surfaces. Unlike simple block-coordinate descent, wedevelop a novel algorithm that is able to perform joint optimization oflocation and detection variables more effectively. Our lower bound optimizationalgorithm applies to quadratic or absolute curvature. The proposed early visionframework is sufficiently general and it can be used in many higher-levelapplications. We illustrate the advantage of our approach on a range of 2D and3D examples.
arxiv-12300-37 | Guiding Long-Short Term Memory for Image Caption Generation | http://arxiv.org/pdf/1509.04942v1.pdf | author:Xu Jia, Efstratios Gavves, Basura Fernando, Tinne Tuytelaars category:cs.CV published:2015-09-16 summary:In this work we focus on the problem of image caption generation. We proposean extension of the long short term memory (LSTM) model, which we coin gLSTMfor short. In particular, we add semantic information extracted from the imageas extra input to each unit of the LSTM block, with the aim of guiding themodel towards solutions that are more tightly coupled to the image content.Additionally, we explore different length normalization strategies for beamsearch in order to prevent from favoring short sentences. On various benchmarkdatasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are onpar with or even outperform the current state-of-the-art.
arxiv-12300-38 | Projection Bank: From High-dimensional Data to Medium-length Binary Codes | http://arxiv.org/pdf/1509.04916v1.pdf | author:Li Liu, Mengyang Yu, Ling Shao category:cs.CV published:2015-09-16 summary:Recently, very high-dimensional feature representations, e.g., Fisher Vector,have achieved excellent performance for visual recognition and retrieval.However, these lengthy representations always cause extremely heavycomputational and storage costs and even become unfeasible in some large-scaleapplications. A few existing techniques can transfer very high-dimensional datainto binary codes, but they still require the reduced code length to berelatively long to maintain acceptable accuracies. To target a better balancebetween computational efficiency and accuracies, in this paper, we propose anovel embedding method called Binary Projection Bank (BPB), which caneffectively reduce the very high-dimensional representations tomedium-dimensional binary codes without sacrificing accuracies. Instead ofusing conventional single linear or bilinear projections, the proposed methodlearns a bank of small projections via the max-margin constraint to optimallypreserve the intrinsic data similarity. We have systematically evaluated theproposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showingcompetitive retrieval and recognition accuracies compared with state-of-the-artapproaches, but with a significantly smaller memory footprint and lower codingcomplexity.
arxiv-12300-39 | Double Relief with progressive weighting function | http://arxiv.org/pdf/1509.04265v2.pdf | author:Gabriel Prat Masramon, Lluís A. Belanche Muñoz category:cs.LG cs.AI published:2015-09-12 summary:Feature weighting algorithms try to solve a problem of great importancenowadays in machine learning: The search of a relevance measure for thefeatures of a given domain. This relevance is primarily used for featureselection as feature weighting can be seen as a generalization of it, but it isalso useful to better understand a problem's domain or to guide an inductor inits learning process. Relief family of algorithms are proven to be veryeffective in this task. On previous work, a new extension was proposed that aimed for improving thealgorithm's performance and it was shown that in certain cases it improved theweights' estimation accuracy. However, it also seemed to be sensible to somecharacteristics of the data. An improvement of that previously presentedextension is presented in this work that aims to make it more robust to problemspecific characteristics. An experimental design is proposed to test itsperformance. Results of the tests prove that it indeed increase the robustnessof the previously proposed extension.
arxiv-12300-40 | Toward better feature weighting algorithms: a focus on Relief | http://arxiv.org/pdf/1509.03755v2.pdf | author:Gabriel Prat Masramon, Lluís A. Belanche Muñoz category:cs.LG published:2015-09-12 summary:Feature weighting algorithms try to solve a problem of great importancenowadays in machine learning: The search of a relevance measure for thefeatures of a given domain. This relevance is primarily used for featureselection as feature weighting can be seen as a generalization of it, but it isalso useful to better understand a problem's domain or to guide an inductor inits learning process. Relief family of algorithms are proven to be veryeffective in this task. Some other feature weighting methods are reviewed inorder to give some context and then the different existing extensions to theoriginal algorithm are explained. One of Relief's known issues is the performance degradation of its estimateswhen redundant features are present. A novel theoretical definition ofredundancy level is given in order to guide the work towards an extension ofthe algorithm that is more robust against redundancy. A new extension ispresented that aims for improving the algorithms performance. Some experimentswere driven to test this new extension against the existing ones with a set ofartificial and real datasets and denoted that in certain cases it improves theweight's estimation accuracy.
arxiv-12300-41 | Adapting Resilient Propagation for Deep Learning | http://arxiv.org/pdf/1509.04612v2.pdf | author:Alan Mosca, George D. Magoulas category:cs.NE cs.CV cs.LG stat.ML published:2015-09-15 summary:The Resilient Propagation (Rprop) algorithm has been very popular forbackpropagation training of multilayer feed-forward neural networks in variousapplications. The standard Rprop however encounters difficulties in the contextof deep neural networks as typically happens with gradient-based learningalgorithms. In this paper, we propose a modification of the Rprop that combinesstandard Rprop steps with a special drop out technique. We apply the method fortraining Deep Neural Networks as standalone components and in ensembleformulations. Results on the MNIST dataset show that the proposed modificationalleviates standard Rprop's problems demonstrating improved learning speed andaccuracy.
arxiv-12300-42 | A Drowsiness Detection Scheme Based on Fusion of Voice and Vision Cues | http://arxiv.org/pdf/1509.04887v1.pdf | author:Anirban Dasgupta, Bibek Kabi, Anjith George, SL Happy, Aurobinda Routray category:cs.CV published:2015-09-16 summary:Drowsiness level detection of an individual is very important in many safetycritical applications such as driving. There are several invasive and contactbased methods such as use of blood biochemical, brain signals etc. which canestimate the level of drowsiness very accurately. However, these methods arevery difficult to implement in practical scenarios, as they cause discomfort tothe user. This paper presents a combined voice and vision based drowsinessdetection system well suited to detect the drowsiness level of an automotivedriver. The vision and voice based detection, being non-contact methods, hasthe advantage of their feasibility of implementation. The authenticity of thesemethods have been cross-validated using brain signals.
arxiv-12300-43 | The Advantage of Cross Entropy over Entropy in Iterative Information Gathering | http://arxiv.org/pdf/1409.7552v2.pdf | author:Johannes Kulick, Robert Lieck, Marc Toussaint category:stat.ML cs.LG published:2014-09-26 summary:Gathering the most information by picking the least amount of data is acommon task in experimental design or when exploring an unknown environment inreinforcement learning and robotics. A widely used measure for quantifying theinformation contained in some distribution of interest is its entropy. Greedilyminimizing the expected entropy is therefore a standard method for choosingsamples in order to gain strong beliefs about the underlying random variables.We show that this approach is prone to temporally getting stuck in local optimacorresponding to wrongly biased beliefs. We suggest instead maximizing theexpected cross entropy between old and new belief, which aims at challengingrefutable beliefs and thereby avoids these local optima. We show that bothcriteria are closely related and that their difference can be traced back tothe asymmetry of the Kullback-Leibler divergence. In illustrative examples aswell as simulated and real-world experiments we demonstrate the advantage ofcross entropy over simple entropy for practical applications.
arxiv-12300-44 | Fast Template Matching by Subsampled Circulant Matrix | http://arxiv.org/pdf/1509.04863v1.pdf | author:Sung-Hsien Hsieh, Chun-Shien Lu, and Soo-Chang Pei category:cs.DS cs.CV published:2015-09-16 summary:Template matching is widely used for many applications in image and signalprocessing and usually is time-critical. Traditional methods usually focus onhow to reduce the search locations by coarse-to-fine strategy or full searchcombined with pruning strategy. However, the computation cost of those methodsis easily dominated by the size of signal N instead of that of template K. Thispaper proposes a probabilistic and fast matching scheme, which computationcosts requires O(N) additions and O(K \log K) multiplications, based oncross-correlation. The nuclear idea is to first downsample signal, which sizebecomes O(K), and then subsequent operations only involves downsampled signals.The probability of successful match depends on cross-correlation between signaland the template. We show the sufficient condition for successful match andprove that the probability is high for binary signals with K^2/log K >= O(N).The experiments shows this proposed scheme is fast and efficient and supportsthe theoretical results.
arxiv-12300-45 | Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness | http://arxiv.org/pdf/1404.4412v2.pdf | author:Guoxu Zhou, Andrzej Cichocki, Qibin Zhao, Shengli Xie category:cs.LG cs.CV stat.ML published:2014-04-17 summary:Nonnegative Tucker decomposition (NTD) is a powerful tool for the extractionof nonnegative parts-based and physically meaningful latent components fromhigh-dimensional tensor data while preserving the natural multilinear structureof data. However, as the data tensor often has multiple modes and islarge-scale, existing NTD algorithms suffer from a very high computationalcomplexity in terms of both storage and computation time, which has been onemajor obstacle for practical applications of NTD. To overcome thesedisadvantages, we show how low (multilinear) rank approximation (LRA) oftensors is able to significantly simplify the computation of the gradients ofthe cost function, upon which a family of efficient first-order NTD algorithmsare developed. Besides dramatically reducing the storage complexity and runningtime, the new algorithms are quite flexible and robust to noise because anywell-established LRA approaches can be applied. We also show how nonnegativityincorporating sparsity substantially improves the uniqueness property andpartially alleviates the curse of dimensionality of the Tucker decompositions.Simulation results on synthetic and real-world data justify the validity andhigh efficiency of the proposed NTD algorithms.
arxiv-12300-46 | An On-board Video Database of Human Drivers | http://arxiv.org/pdf/1509.04853v1.pdf | author:Anirban Dasgupta, Anjith George, SL Happy, Aurobinda Routray category:cs.CV published:2015-09-16 summary:Detection of fatigue due to drowsiness or loss of attention in human driversis an evolving area of research. Several algorithms have been implemented todetect the level of fatigue in human drivers by capturing videos of facialimage sequences and extracting facial features such as eye closure rates, eyegaze, head nodding, blink frequency etc. However, availability of standardvideo database to validate such algorithms is insufficient. This paperdiscusses the creation of such a database created under on-board conditionsduring the day as well as night. Passive Near Infra-red (NIR) illumination hasbeen used for illuminating the face during night driving since prolongedexposure to active Infra-Red lighting may lead to many health issues. Thedatabase contains videos of 30 subjects under actual driving conditions.Variation is ensured as the database contains different head orientations andwith different facial expressions, facial occlusions and illuminationvariation. This new database can be a very valuable resource for developmentand evaluation of algorithms for the video based detection of driver fatigue.
arxiv-12300-47 | amLite: Amharic Transliteration Using Key Map Dictionary | http://arxiv.org/pdf/1509.04811v1.pdf | author:Tadele Tedla category:cs.CL cs.IR published:2015-09-16 summary:amLite is a framework developed to map ASCII transliterated Amharic textsback to the original Amharic letter texts. The aim of such a framework is tomake existing Amharic linguistic data consistent and interoperable amongresearchers. For achieving the objective, a key map dictionary is constructedusing the possible ASCII combinations actively in use for transliteratingAmharic letters; and a mapping of the combinations to the corresponding Amharicletters is done. The mapping is then used to replace the Amharic linguistictext back to form the original Amharic letters text. The framework indicated97.7, 99.7 and 98.4 percentage accuracy on converting the three sample randomtest data. It is; however, possible to improve the accuracy of the framework byadding an exception to the implementation of the algorithm, or by preprocessingthe input text prior to conversion. This paper outlined the rationales behindthe need for developing the framework and the processes undertaken in thedevelopment.
arxiv-12300-48 | Group Membership Prediction | http://arxiv.org/pdf/1509.04783v1.pdf | author:Ziming Zhang, Yuting Chen, Venkatesh Saligrama category:cs.CV stat.ML published:2015-09-16 summary:The group membership prediction (GMP) problem involves predicting whether ornot a collection of instances share a certain semantic property. For instance,in kinship verification given a collection of images, the goal is to predictwhether or not they share a {\it familial} relationship. In this context wepropose a novel probability model and introduce latent {\em view-specific} and{\em view-shared} random variables to jointly account for the view-specificappearance and cross-view similarities among data instances. Our model positsthat data from each view is independent conditioned on the shared variables.This postulate leads to a parametric probability model that decomposes groupmembership likelihood into a tensor product of data-independent parameters anddata-dependent factors. We propose learning the data-independent parameters ina discriminative way with bilinear classifiers, and test our predictionalgorithm on challenging visual recognition tasks such as multi-camera personre-identification and kinship verification. On most benchmark datasets, ourmethod can significantly outperform the current state-of-the-art.
arxiv-12300-49 | Dirichlet Fragmentation Processes | http://arxiv.org/pdf/1509.04781v1.pdf | author:Hong Ge, Yarin Gal, Zoubin Ghahramani category:stat.ML published:2015-09-16 summary:Tree structures are ubiquitous in data across many domains, and many datasetsare naturally modelled by unobserved tree structures. In this paper, first wereview the theory of random fragmentation processes [Bertoin, 2006], and anumber of existing methods for modelling trees, including the popular nestedChinese restaurant process (nCRP). Then we define a general class ofprobability distributions over trees: the Dirichlet fragmentation process (DFP)through a novel combination of the theory of Dirichlet processes and randomfragmentation processes. This DFP presents a stick-breaking construction, andrelates to the nCRP in the same way the Dirichlet process relates to theChinese restaurant process. Furthermore, we develop a novel hierarchicalmixture model with the DFP, and empirically compare the new model to similarmodels in machine learning. Experiments show the DFP mixture model to beconvincingly better than existing state-of-the-art approaches for hierarchicalclustering and density modelling.
arxiv-12300-50 | On the evolution of word usage of classical Chinese poetry | http://arxiv.org/pdf/1509.04556v2.pdf | author:Liang Liu category:physics.soc-ph cs.CL published:2015-09-10 summary:The hierarchy of classical Chinese poetry has been broadly acknowledged by anumber of studies in Chinese literature. However, quantitative investigationsabout the evolution of classical Chinese poetry are limited. The primary goalof this study is to provide quantitative evidence of the evolutionary linkages,with emphasis on word usage, among different period genres for classicalChinese poetry. Specifically, various statistical analyses were performed tofind and compare the patterns of word usage in the poems of nine period genres,including shi jing, chu ci, Han shi , Jin shi, Tang shi, Song shi, Yuan shi,Ming shi, and Qing shi. The result of analysis indicates that each of nineperiod genres has unique patterns of word usage, with some Chinese charactersbeing preferably used by the poems of a particular period genre. The analysison the general pattern of word preference implies a decreasing trend in the useof ancient Chinese characters along the timeline of dynastic types of classicalChinese poetry. The phylogenetic analysis based on the distance matrix suggeststhat the evolution of different types of classical Chinese poetry is congruentwith their chronological order, suggesting that word frequencies contain usefulphylogenetic information and thus can be used to infer evolutionary linkagesamong various types of classical Chinese poetry. The statistical analysesconducted in this study can be applied to the data sets of general Chineseliterature. Such analyses can provide quantitative insights about the evolutionof general Chinese literature.
arxiv-12300-51 | Linear Embedding of Large-Scale Brain Networks for Twin fMRI | http://arxiv.org/pdf/1509.04771v1.pdf | author:Moo K. Chung, Victoria G. Vilalta, Paul J. Rathouz, Benjamin B. Lahey, David H. Zald category:cs.AI q-bio.NC stat.ML published:2015-09-15 summary:In many human brain network studies, we do not have sufficient number (n) ofimages relative to the number (p) of voxels due to the prohibitively expensivecost of scanning enough subjects. Thus, brain network models usually suffer thesmall-n large-p problem. Such a problem is often remedied by sparse networkmodels, which are usually solved numerically by optimizing L1-penalties.Unfortunately, due to the computational bottleneck associated with optimizingL1-penalties, it is not practical to apply such methods to learn large-scalebrain networks. In this paper, we introduce a new sparse network model based oncross-correlations that bypass the computational bottleneck. Our model canbuild the sparse brain networks at voxel level with p > 25000. Instead of usinga single sparse parameter that may not be optimal in other studies anddatasets, we propose to analyze the collection of networks at every possiblesparse parameter in a coherent mathematical framework using graph filtrations.The method is subsequently applied in determining the extend of genetic effectson functional brain networks at voxel-level for the first time using twin fMRI.
arxiv-12300-52 | Free-body Gesture Tracking and Augmented Reality Improvisation for Floor and Aerial Dance | http://arxiv.org/pdf/1509.04751v1.pdf | author:Tammuz Dubnov, Cheng-i Wang category:cs.MM cs.CV cs.HC published:2015-09-15 summary:This paper describes an updated interactive performance system for floor andAerial Dance that controls visual and sonic aspects of the presentation via adepth sensing camera (MS Kinect). In order to detect, measure and track freemovement in space, 3 degree of freedom (3-DOF) tracking in space (on the groundand in the air) is performed using IR markers with a method for multi targettracking capabilities added and described in detail. An improved gesturetracking and recognition system, called Action Graph (AG), is described in thepaper. Action Graph uses an efficient incremental construction from a singlelong sequence of movement features and automatically captures repeatedsub-segments in the movement from start to finish with no manual interactionneeded with other advanced capabilities discussed as well. By using the newmodel for the gesture we can unify an entire choreography piece by dynamicallytracking and recognizing gestures and sub-portions of the piece. This gives theperformer the freedom to improvise based on a set of recorded gestures/portionsof the choreography and have the system dynamically respond in relation to theperformer within a set of related rehearsed actions, an ability that has notbeen seen in any other system to date.
arxiv-12300-53 | Modeling sequences and temporal networks with dynamic community structures | http://arxiv.org/pdf/1509.04740v1.pdf | author:Tiago P. Peixoto, Martin Rosvall category:cs.SI physics.soc-ph stat.ML published:2015-09-15 summary:Community-detection methods that describe large-scale patterns in thedynamics on and of networks suffer from effects of limited memory and arbitrarytime binning. We develop a variable-order Markov chain model that generalizesthe stochastic block model for discrete time-series as well as temporalnetworks. The temporal model does not use time binning but takes full advantageof the time-ordering of the tokens or edges. When the edge ordering is random,we recover the traditional static block model as a special case. Based onstatistical evidence and without overfitting, we show how a Bayesianformulation of the model allows us to select the most appropriate Markov orderand number of communities.
arxiv-12300-54 | Improving Computer-aided Detection using Convolutional Neural Networks and Random View Aggregation | http://arxiv.org/pdf/1505.03046v2.pdf | author:Holger R. Roth, Le Lu, Jiamin Liu, Jianhua Yao, Ari Seff, Kevin Cherry, Lauren Kim, Ronald M. Summers category:cs.CV published:2015-05-12 summary:Automated computer-aided detection (CADe) in medical imaging has been animportant tool in clinical practice and research. State-of-the-art methodsoften show high sensitivities but at the cost of high false-positives (FP) perpatient rates. We design a two-tiered coarse-to-fine cascade framework thatfirst operates a candidate generation system at sensitivities of $\sim$100% butat high FP levels. By leveraging existing CAD systems, coordinates of regionsor volumes of interest (ROI or VOI) for lesion candidates are generated in thisstep and function as input for a second tier, which is our focus in this study.In this second stage, we generate $N$ 2D (two-dimensional) or 2.5D views viasampling through scale transformations, random translations and rotations withrespect to each ROI's centroid coordinates. These random views are used totrain deep convolutional neural network (ConvNet) classifiers. In testing, thetrained ConvNets are employed to assign class (e.g., lesion, pathology)probabilities for a new set of $N$ random views that are then averaged at eachROI to compute a final per-candidate classification probability. This secondtier behaves as a highly selective process to reject difficult false positiveswhile preserving high sensitivities. The methods are evaluated on threedifferent data sets with different numbers of patients: 59 patients forsclerotic metastases detection, 176 patients for lymph node detection, and1,186 patients for colonic polyp detection. Experimental results show theability of ConvNets to generalize well to different medical imaging CADeapplications and scale elegantly to various data sets. Our proposed methodsimprove CADe performance markedly in all cases. CADe sensitivities improvedfrom 57% to 70%, from 43% to 77% and from 58% to 75% at 3 FPs per patient forsclerotic metastases, lymph nodes and colonic polyps, respectively.
arxiv-12300-55 | Learning the Structure for Structured Sparsity | http://arxiv.org/pdf/1503.03082v2.pdf | author:Nino Shervashidze, Francis Bach category:stat.ML published:2015-03-10 summary:Structured sparsity has recently emerged in statistics, machine learning andsignal processing as a promising paradigm for learning in high-dimensionalsettings. All existing methods for learning under the assumption of structuredsparsity rely on prior knowledge on how to weight (or how to penalize)individual subsets of variables during the subset selection process, which isnot available in general. Inferring group weights from data is a key openresearch problem in structured sparsity.In this paper, we propose a Bayesianapproach to the problem of group weight learning. We model the group weights ashyperparameters of heavy-tailed priors on groups of variables and derive anapproximate inference scheme to infer these hyperparameters. We empiricallyshow that we are able to recover the model hyperparameters when the data aregenerated from the model, and we demonstrate the utility of learning weights insynthetic and real denoising problems.
arxiv-12300-56 | How Many Communities Are There? | http://arxiv.org/pdf/1412.1684v2.pdf | author:Diego Franco Saldana, Yi Yu, Yang Feng category:stat.ME stat.AP stat.CO stat.ML published:2014-12-04 summary:Stochastic blockmodels and variants thereof are among the most widely usedapproaches to community detection for social networks and relational data. Astochastic blockmodel partitions the nodes of a network into disjoint sets,called communities. The approach is inherently related to clustering withmixture models; and raises a similar model selection problem for the number ofcommunities. The Bayesian information criterion (BIC) is a popular solution,however, for stochastic blockmodels, the conditional independence assumptiongiven the communities of the endpoints among different edges is usuallyviolated in practice. In this regard, we propose composite likelihood BIC(CL-BIC) to select the number of communities, and we show it is robust againstpossible misspecifications in the underlying stochastic blockmodel assumptions.We derive the requisite methodology and illustrate the approach using bothsimulated and real data. Supplementary materials containing the relevantcomputer code are available online.
arxiv-12300-57 | Direct high-order edge-preserving regularization for tomographic image reconstruction | http://arxiv.org/pdf/1509.04706v1.pdf | author:Daniil Kazantsev, Evgueni Ovtchinnikov, William R. B. Lionheart, Philip J. Withers, Peter D. Lee category:cs.CV cs.MS cs.NA math.NA published:2015-09-15 summary:In this paper we present a new two-level iterative algorithm for tomographicimage reconstruction. The algorithm uses a regularization technique, which wecall edge-preserving Laplacian, that preserves sharp edges between objectswhile damping spurious oscillations in the areas where the reconstructed imageis smooth. Our numerical simulations demonstrate that the proposed methodoutperforms total variation (TV) regularization and it is competitive with thecombined TV-L2 penalty. Obtained reconstructed images show increasedsignal-to-noise ratio and visually appealing structural features. Computerimplementation and parameter control of the proposed technique isstraightforward, which increases the feasibility of it across many tomographicapplications. In this paper, we applied our method to the under-sampledcomputed tomography (CT) projection data and also considered a case ofreconstruction in emission tomography The MATLAB code is provided to supportobtained results.
arxiv-12300-58 | Self-Configuring and Evolving Fuzzy Image Thresholding | http://arxiv.org/pdf/1509.04664v1.pdf | author:A. Othman, H. R. Tizhoosh, F. Khalvati category:cs.CV published:2015-09-15 summary:Every segmentation algorithm has parameters that need to be adjusted in orderto achieve good results. Evolving fuzzy systems for adjustment of segmentationparameters have been proposed recently (Evolving fuzzy image segmentation --EFIS [1]. However, similar to any other algorithm, EFIS too suffers from a fewlimitations when used in practice. As a major drawback, EFIS depends ondetection of the object of interest for feature calculation, a task that ishighly application-dependent. In this paper, a new version of EFIS is proposedto overcome these limitations. The new EFIS, called self-configuring EFIS(SC-EFIS), uses available training data to auto-configure the parameters thatare fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selectionprocess that does not require the detection of a region of interest (ROI).
arxiv-12300-59 | Forecasting Method for Grouped Time Series with the Use of k-Means Algorithm | http://arxiv.org/pdf/1509.04705v1.pdf | author:N. N. Astakhova, L. A. Demidova, E. V. Nikulchev category:cs.LG published:2015-09-15 summary:The paper is focused on the forecasting method for time series groups withthe use of algorithms for cluster analysis. $K$-means algorithm is suggested tobe a basic one for clustering. The coordinates of the centers of clusters havebeen put in correspondence with summarizing time series data the centroids ofthe clusters. A description of time series, the centroids of the clusters, isimplemented with the use of forecasting models. They are based on strict binarytrees and a modified clonal selection algorithm. With the help of suchforecasting models, the possibility of forming analytic dependences is shown.It is suggested to use a common forecasting model, which is constructed fortime series the centroid of the cluster, in forecasting the private(individual) time series in the cluster. The promising application of thesuggested method for grouped time series forecasting is demonstrated.
arxiv-12300-60 | Dynamic Poisson Factorization | http://arxiv.org/pdf/1509.04640v1.pdf | author:Laurent Charlin, Rajesh Ranganath, James McInerney, David M. Blei category:cs.LG cs.IR stat.ML published:2015-09-15 summary:Models for recommender systems use latent factors to explain the preferencesand behaviors of users with respect to a set of items (e.g., movies, books,academic papers). Typically, the latent factors are assumed to be static and,given these factors, the observed preferences and behaviors of users areassumed to be generated without order. These assumptions limit the explorativeand predictive capabilities of such models, since users' interests and itempopularity may evolve over time. To address this, we propose dPF, a dynamicmatrix factorization model based on the recent Poisson factorization model forrecommendations. dPF models the time evolving latent factors with a Kalmanfilter and the actions with Poisson distributions. We derive a scalablevariational inference algorithm to infer the latent factors. Finally, wedemonstrate dPF on 10 years of user click data from arXiv.org, one of thelargest repository of scientific papers and a formidable source of informationabout the behavior of scientists. Empirically we show performance improvementover both static and, more recently proposed, dynamic recommendation models. Wealso provide a thorough exploration of the inferred posteriors over the latentvariables.
arxiv-12300-61 | Modeling and interpolation of the ambient magnetic field by Gaussian processes | http://arxiv.org/pdf/1509.04634v1.pdf | author:Arno Solin, Manon Kok, Niklas Wahlström, Thomas B. Schön, Simo Särkkä category:cs.RO stat.ML published:2015-09-15 summary:Anomalies in the ambient magnetic field can be used as features in indoorpositioning and navigation. By using Maxwell's equations, we derive and presenta Bayesian non-parametric probabilistic modeling approach for interpolation andextrapolation of the magnetic field. We model the magnetic field componentsjointly by imposing a Gaussian process (GP) prior on the latent scalarpotential of the magnetic field. By rewriting the GP model in terms of aHilbert space representation, we circumvent the computational pitfallsassociated with GP modeling and provide a computationally efficient andphysically justified modeling tool for the ambient magnetic field. The modelallows for sequential updating of the estimate and time-dependent changes inthe magnetic field. The model is shown to work well in practice in differentapplications: we demonstrate mapping of the magnetic field both with aninexpensive Raspberry Pi powered robot and on foot using a standard smartphone.
arxiv-12300-62 | The Shape of Data and Probability Measures | http://arxiv.org/pdf/1509.04632v1.pdf | author:Diego Hernán Díaz Martínez, Facundo Mémoli, Washington Mio category:stat.ML math.MG math.ST stat.TH published:2015-09-15 summary:We introduce the notion of multiscale covariance tensor fields (CTF)associated with Euclidean random variables as a gateway to the shape of theirdistributions. Multiscale CTFs quantify variation of the data about every pointin the data landscape at all spatial scales, unlike the usual covariance tensorthat only quantifies global variation about the mean. Empirical forms oflocalized covariance previously have been used in data analysis andvisualization, but we develop a framework for the systematic treatment oftheoretical questions and computational models based on localized covariance.We prove strong stability theorems with respect to the Wasserstein distancebetween probability measures, obtain consistency results, as well as estimatesfor the rate of convergence of empirical CTFs. These results ensure that CTFsare robust to sampling, noise and outliers. We provide numerous illustrationsof how CTFs let us extract shape from data and also apply CTFs to manifoldclustering, the problem of categorizing data points according to their noisymembership in a collection of possibly intersecting, smooth submanifolds ofEuclidean space. We prove that the proposed manifold clustering method isstable and carry out several experiments to validate the method.
arxiv-12300-63 | Medical Image Classification via SVM using LBP Features from Saliency-Based Folded Data | http://arxiv.org/pdf/1509.04619v1.pdf | author:Zehra Camlica, H. R. Tizhoosh, Farzad Khalvati category:cs.CV published:2015-09-15 summary:Good results on image classification and retrieval using support vectormachines (SVM) with local binary patterns (LBPs) as features have beenextensively reported in the literature where an entire image is retrieved orclassified. In contrast, in medical imaging, not all parts of the image may beequally significant or relevant to the image retrieval application at hand. Forinstance, in lung x-ray image, the lung region may contain a tumour, hencebeing highly significant whereas the surrounding area does not containsignificant information from medical diagnosis perspective. In this paper, wepropose to detect salient regions of images during training and fold the datato reduce the effect of irrelevant regions. As a result, smaller image areaswill be used for LBP features calculation and consequently classification bySVM. We use IRMA 2009 dataset with 14,410 x-ray images to verify theperformance of the proposed approach. The results demonstrate the benefits ofsaliency-based folding approach that delivers comparable classificationaccuracies with state-of-the-art but exhibits lower computational cost andstorage requirements, factors highly important for big data analytics.
arxiv-12300-64 | Network-based Isoform Quantification with RNA-Seq Data for Cancer Transcriptome Analysis | http://arxiv.org/pdf/1403.5029v3.pdf | author:Wei Zhang, Jae-Woong Chang, Lilong Lin, Kay Minn, Baolin Wu, Jeremy Chien, Jeongsik Yong, Hui Zheng, Rui Kuang category:cs.CE cs.AI cs.LG published:2014-03-20 summary:High-throughput mRNA sequencing (RNA-Seq) is widely used for transcriptquantification of gene isoforms. Since RNA-Seq data alone is often notsufficient to accurately identify the read origins from the isoforms forquantification, we propose to explore protein domain-domain interactions asprior knowledge for integrative analysis with RNA-seq data. We introduce aNetwork-based method for RNA-Seq-based Transcript Quantification (Net-RSTQ) tointegrate protein domain-domain interaction network with short read alignmentsfor transcript abundance estimation. Based on our observation that theabundances of the neighboring isoforms by domain-domain interactions in thenetwork are positively correlated, Net-RSTQ models the expression of theneighboring transcripts as Dirichlet priors on the likelihood of the observedread alignments against the transcripts in one gene. The transcript abundancesof all the genes are then jointly estimated with alternating optimization ofmultiple EM problems. In simulation Net-RSTQ effectively improved isoformtranscript quantifications when isoform co-expressions correlate with theirinteractions. qRT-PCR results on 25 multi-isoform genes in a stem cell line, anovarian cancer cell line, and a breast cancer cell line also showed thatNet-RSTQ estimated more consistent isoform proportions with RNA-Seq data. Inthe experiments on the RNA-Seq data in The Cancer Genome Atlas (TCGA), thetranscript abundances estimated by Net-RSTQ are more informative for patientsample classification of ovarian cancer, breast cancer and lung cancer. Allexperimental results collectively support that Net-RSTQ is a promising approachfor isoform quantification.
arxiv-12300-65 | Kernelized Deep Convolutional Neural Network for Describing Complex Images | http://arxiv.org/pdf/1509.04581v1.pdf | author:Zhen Liu category:cs.CV cs.AI cs.IR cs.MM published:2015-09-15 summary:With the impressive capability to capture visual content, deep convolutionalneural networks (CNN) have demon- strated promising performance in variousvision-based ap- plications, such as classification, recognition, and objec- tdetection. However, due to the intrinsic structure design of CNN, for imageswith complex content, it achieves lim- ited capability on invariance totranslation, rotation, and re-sizing changes, which is strongly emphasized inthe s- cenario of content-based image retrieval. In this paper, to address thisproblem, we proposed a new kernelized deep convolutional neural network. Wefirst discuss our motiva- tion by an experimental study to demonstrate thesensitivi- ty of the global CNN feature to the basic geometric trans-formations. Then, we propose to represent visual content with approximateinvariance to the above geometric trans- formations from a kernelizedperspective. We extract CNN features on the detected object-like patches andaggregate these patch-level CNN features to form a vectorial repre- sentationwith the Fisher vector model. The effectiveness of our proposed algorithm isdemonstrated on image search application with three benchmark datasets.
arxiv-12300-66 | Maximum Correntropy Kalman Filter | http://arxiv.org/pdf/1509.04580v1.pdf | author:Badong Chen, Xi Liu, Haiquan Zhao, José C. Príncipe category:stat.ML cs.SY published:2015-09-15 summary:Traditional Kalman filter (KF) is derived under the well-known minimum meansquare error (MMSE) criterion, which is optimal under Gaussian assumption.However, when the signals are non-Gaussian, especially when the system isdisturbed by some heavy-tailed impulsive noises, the performance of KF willdeteriorate seriously. To improve the robustness of KF against impulsivenoises, we propose in this work a new Kalman filter, called the maximumcorrentropy Kalman filter (MCKF), which adopts the robust maximum correntropycriterion (MCC) as the optimality criterion, instead of using the MMSE. Similarto the traditional KF, the state mean and covariance matrix propagationequations are used to give prior estimations of the state and covariance matrixin MCKF. A novel fixed-point algorithm is then used to update the posteriorestimations. A sufficient condition that guarantees the convergence of thefixed-point algorithm is given. Illustration examples are presented todemonstrate the effectiveness and robustness of the new algorithm.
arxiv-12300-67 | When are Kalman-filter restless bandits indexable? | http://arxiv.org/pdf/1509.04541v1.pdf | author:Christopher R. Dance, Tomi Silander category:stat.ML published:2015-09-15 summary:We study the restless bandit associated with an extremely simple scalarKalman filter model in discrete time. Under certain assumptions, we prove thatthe problem is indexable in the sense that the Whittle index is anon-decreasing function of the relevant belief state. In spite of the longhistory of this problem, this appears to be the first such proof. We useresults about Schur-convexity and mechanical words, which are particular binarystrings intimately related to palindromes.
arxiv-12300-68 | A Low Complexity VLSI Architecture for Multi-Focus Image Fusion in DCT Domain | http://arxiv.org/pdf/1602.07620v1.pdf | author:Ashutosh Mishra, Sudipta Mahapatra, Swapna Banerjee category:cs.CV published:2015-09-15 summary:Due to the confined focal length of optical sensors, focusing all objects ina scene with a single sensor is a difficult task. To handle such a situation,image fusion methods are used in multi-focus environment. Discrete CosineTransform (DCT) is a widely used image compression transform, image fusion inDCT domain is an efficient method. This paper presents a low complexityapproach for multi-focus image fusion and its VLSI implementation using DCT.The proposed method is evaluated using reference/non-reference fusion measurecriteria and the obtained results asserts it's effectiveness. The maximumsynthesized frequency on FPGA is found to be 221 MHz and consumes 42% of FPGAresources. The proposed method consumes very less power and can process 4Kresolution images at the rate of 60 frames per second which makes the hardwaresuitable for handheld portable devices such as camera module and wireless imagesensors.
arxiv-12300-69 | Sparse Multinomial Logistic Regression via Approximate Message Passing | http://arxiv.org/pdf/1509.04491v1.pdf | author:Evan Byrne, Philip Schniter category:cs.IT math.IT stat.ML published:2015-09-15 summary:For the problem of multi-class linear classification and feature selection,we propose approximate message passing approaches to sparse multinomiallogistic regression. First, we propose two algorithms based on the HybridGeneralized Approximate Message Passing (HyGAMP) framework: one finds themaximum a posteriori (MAP) linear classifier and the other finds anapproximation of the test-error-rate minimizing linear classifier. Then wedesign computationally simplified variants of these two algorithms. Next, wedetail methods to tune the hyperparameters of their assumed statistical modelsusing Stein's unbiased risk estimate (SURE) and expectation-maximization (EM),respectively. Finally, using both synthetic and real-world datasets, wedemonstrate improved error-rate and runtime performance relative tostate-of-the-art existing approaches.
arxiv-12300-70 | Splitting Compounds by Semantic Analogy | http://arxiv.org/pdf/1509.04473v1.pdf | author:Joachim Daiber, Lautaro Quiroz, Roger Wechsler, Stella Frank category:cs.CL published:2015-09-15 summary:Compounding is a highly productive word-formation process in some languagesthat is often problematic for natural language processing applications. In thispaper, we investigate whether distributional semantics in the form of wordembeddings can enable a deeper, i.e., more knowledge-rich, processing ofcompounds than the standard string-based methods. We present an unsupervisedapproach that exploits regularities in the semantic vector space (based onanalogies such as "bookshop is to shop as bookshelf is to shelf") to producecompound analyses of high quality. A subsequent compound splitting algorithmbased on these analyses is highly effective, particularly for ambiguouscompounds. German to English machine translation experiments show that thissemantic analogy-based compound splitter leads to better translations than acommonly used frequency-based method.
arxiv-12300-71 | Weakly supervised clustering: Learning fine-grained signals from coarse labels | http://arxiv.org/pdf/1310.1363v3.pdf | author:Stefan Wager, Alexander Blocker, Niall Cardin category:stat.ML cs.LG published:2013-10-04 summary:Consider a classification problem where we do not have access to labels forindividual training examples, but only have average labels over subpopulations.We give practical examples of this setup and show how such a classificationtask can usefully be analyzed as a weakly supervised clustering problem. Wepropose three approaches to solving the weakly supervised clustering problem,including a latent variables model that performs well in our experiments. Weillustrate our methods on an analysis of aggregated elections data and anindustry data set that was the original motivation for this research.
arxiv-12300-72 | Neuron detection in stack images: a persistent homology interpretation | http://arxiv.org/pdf/1509.04420v1.pdf | author:Jónathan Heras, Gadea Mata, Germán Cuesto, Julio Rubio, Miguel Morales category:cs.CV q-bio.NC published:2015-09-15 summary:Automation and reliability are the two main requirements when computers areapplied in Life Sciences. In this paper we report on an application to neuronrecognition, an important step in our long-term project of providing softwaresystems to the study of neural morphology and functionality from biomedicalimages. Our algorithms have been implemented in an ImageJ plugin calledNeuronPersistentJ, which has been validated experimentally. The soundness andreliability of our approach are based on the interpretation of our processingmethods with respect to persistent homology, a well-known tool in computationalmathematics.
arxiv-12300-73 | A Random Matrix Theoretical Approach to Early Event Detection in Smart Grid | http://arxiv.org/pdf/1502.00060v2.pdf | author:Xing He, Robert Caiming Qiu, Qian Ai, Yinshuang Cao, Jie Gu, Zhijian Jin category:stat.ME cs.LG published:2015-01-31 summary:Power systems are developing very fast nowadays, both in size and incomplexity; this situation is a challenge for Early Event Detection (EED). Thispaper proposes a data- driven unsupervised learning method to handle thischallenge. Specifically, the random matrix theories (RMTs) are introduced asthe statistical foundations for random matrix models (RMMs); based on the RMMs,linear eigenvalue statistics (LESs) are defined via the test functions as thesystem indicators. By comparing the values of the LES between the experimentaland the theoretical ones, the anomaly detection is conducted. Furthermore, wedevelop 3D power-map to visualize the LES; it provides a robust auxiliarydecision-making mechanism to the operators. In this sense, the proposed methodconducts EED with a pure statistical procedure, requiring no knowledge ofsystem topologies, unit operation/control models, etc. The LES, as a keyingredient during this procedure, is a high dimensional indictor deriveddirectly from raw data. As an unsupervised learning indicator, the LES is muchmore sensitive than the low dimensional indictors obtained from supervisedlearning. With the statistical procedure, the proposed method is universal andfast; moreover, it is robust against traditional EED challenges (such as erroraccumulations, spurious correlations, and even bad data in core area). Casestudies, with both simulated data and real ones, validate the proposed method.To manage large-scale distributed systems, data fusion is mentioned as anotherdata processing ingredient.
arxiv-12300-74 | Analyzing structural characteristics of object category representations from their semantic-part distributions | http://arxiv.org/pdf/1509.04399v1.pdf | author:Ravi Kiran Sarvadevabhatla, Venkatesh Babu R category:cs.CV published:2015-09-15 summary:Studies from neuroscience show that part-mapping computations are employed byhuman visual system in the process of object recognition. In this work, wepresent an approach for analyzing semantic-part characteristics of objectcategory representations. For our experiments, we use category-epitome, arecently proposed sketch-based spatial representation for objects. To enablepart-importance analysis, we first obtain semantic-part annotations ofhand-drawn sketches originally used to construct the corresponding epitomes. Wethen examine the extent to which the semantic-parts are present in the epitomesof a category and visualize the relative importance of parts as a word cloud.Finally, we show how such word cloud visualizations provide an intuitiveunderstanding of category-level structural trends that exist in thecategory-epitome object representations.
arxiv-12300-75 | Exponential Family Matrix Completion under Structural Constraints | http://arxiv.org/pdf/1509.04397v1.pdf | author:Suriya Gunasekar, Pradeep Ravikumar, Joydeep Ghosh category:stat.ML cs.LG published:2015-09-15 summary:We consider the matrix completion problem of recovering a structured matrixfrom noisy and partial measurements. Recent works have proposed tractableestimators with strong statistical guarantees for the case where the underlyingmatrix is low--rank, and the measurements consist of a subset, either of theexact individual entries, or of the entries perturbed by additive Gaussiannoise, which is thus implicitly suited for thin--tailed continuous data.Arguably, common applications of matrix completion require estimators for (a)heterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)for heterogeneous noise models (beyond Gaussian), which capture varieduncertainty in the measurements, and (c) heterogeneous structural constraintsbeyond low--rank, such as block--sparsity, or a superposition structure oflow--rank plus elementwise sparseness, among others. In this paper, we providea vastly unified framework for generalized matrix completion by considering amatrix completion setting wherein the matrix entries are sampled from anymember of the rich family of exponential family distributions; and imposegeneral structural constraints on the underlying matrix, as captured by ageneral regularizer $\mathcal{R}(.)$. We propose a simple convex regularized$M$--estimator for the generalized framework, and provide a unified and novelstatistical analysis for this general class of estimators. We finallycorroborate our theoretical results on simulated datasets.
arxiv-12300-76 | Dependency length minimization: Puzzles and Promises | http://arxiv.org/pdf/1509.04393v1.pdf | author:Haitao Liu, Chunshan Xu, Junying Liang category:cs.CL published:2015-09-15 summary:In the recent issue of PNAS, Futrell et al. claims that their study of 37languages gives the first large scale cross-language evidence for DependencyLength Minimization, which is an overstatement that ignores similar previousresearches. In addition,this study seems to pay no attention to factors likethe uniformity of genres,which weakens the validity of the argument that DLM isuniversal. Another problem is that this study sets the baseline random languageas projective, which fails to truly uncover the difference between naturallanguage and random language, since projectivity is an important feature ofmany natural languages. Finally, the paper contends an "apparent relationshipbetween head finality and dependency length" despite the lack of an explicitstatistical comparison, which renders this conclusion rather hasty andimproper.
arxiv-12300-77 | Precise Phase Transition of Total Variation Minimization | http://arxiv.org/pdf/1509.04376v1.pdf | author:Bingwen Zhang, Weiyu Xu, Jian-Feng Cai, Lifeng Lai category:cs.IT cs.LG math.IT math.OC stat.ML published:2015-09-15 summary:Characterizing the phase transitions of convex optimizations in recoveringstructured signals or data is of central importance in compressed sensing,machine learning and statistics. The phase transitions of many convexoptimization signal recovery methods such as $\ell_1$ minimization and nuclearnorm minimization are well understood through recent years' research. However,rigorously characterizing the phase transition of total variation (TV)minimization in recovering sparse-gradient signal is still open. In this paper,we fully characterize the phase transition curve of the TV minimization. Ourproof builds on Donoho, Johnstone and Montanari's conjectured phase transitioncurve for the TV approximate message passing algorithm (AMP), together with thelinkage between the minmax Mean Square Error of a denoising problem and thehigh-dimensional convex geometry for TV minimization.
arxiv-12300-78 | Improved Relation Extraction with Feature-Rich Compositional Embedding Models | http://arxiv.org/pdf/1505.02419v3.pdf | author:Matthew R. Gormley, Mo Yu, Mark Dredze category:cs.CL cs.AI cs.LG published:2015-05-10 summary:Compositional embedding models build a representation (or embedding) for alinguistic structure based on its component word embeddings. We propose aFeature-rich Compositional Embedding Model (FCM) for relation extraction thatis expressive, generalizes to new domains, and is easy-to-implement. The keyidea is to combine both (unlexicalized) hand-crafted features with learned wordembeddings. The model is able to directly tackle the difficulties met bytraditional compositional embeddings models, such as handling arbitrary typesof sentence annotations and utilizing global information for composition. Wetest the proposed model on two relation extraction tasks, and demonstrate thatour model outperforms both previous compositional models and traditionalfeature rich models on the ACE 2005 relation extraction task, and the SemEval2010 relation classification task. The combination of our model and alog-linear classifier with hand-crafted features gives state-of-the-artresults.
arxiv-12300-79 | Model Accuracy and Runtime Tradeoff in Distributed Deep Learning | http://arxiv.org/pdf/1509.04210v2.pdf | author:Suyog Gupta, Wei Zhang, Josh Milthorpe category:stat.ML cs.DC cs.LG cs.NE published:2015-09-14 summary:This paper presents Rudra, a parameter server based distributed computingframework tuned for training large-scale deep neural networks. Using variantsof the asynchronous stochastic gradient descent algorithm we study the impactof synchronization protocol, stale gradient updates, minibatch size, learningrates, and number of learners on runtime performance and model accuracy. Weintroduce a new learning rate modulation strategy to counter the effect ofstale gradients and propose a new synchronization protocol that can effectivelybound the staleness in gradients, improve runtime performance and achieve goodmodel accuracy. Our empirical investigation reveals a principled approach fordistributed training of neural networks: the mini-batch size per learner shouldbe reduced as more learners are added to the system to preserve the modelaccuracy. We validate this approach using commonly-used image classificationbenchmarks: CIFAR10 and ImageNet.
arxiv-12300-80 | Towards Making High Dimensional Distance Metric Learning Practical | http://arxiv.org/pdf/1509.04355v1.pdf | author:Qi Qian, Rong Jin, Lijun Zhang, Shenghuo Zhu category:cs.LG published:2015-09-15 summary:In this work, we study distance metric learning (DML) for high dimensionaldata. A typical approach for DML with high dimensional data is to perform thedimensionality reduction first before learning the distance metric. The mainshortcoming of this approach is that it may result in a suboptimal solution dueto the subspace removed by the dimensionality reduction method. In this work,we present a dual random projection frame for DML with high dimensional datathat explicitly addresses the limitation of dimensionality reduction for DML.The key idea is to first project all the data points into a low dimensionalspace by random projection, and compute the dual variables using the projectedvectors. It then reconstructs the distance metric in the original space usingthe estimated dual variables. The proposed method, on one hand, enjoys thelight computation of random projection, and on the other hand, alleviates thelimitation of most dimensionality reduction methods. We verify both empiricallyand theoretically the effectiveness of the proposed algorithm for highdimensional DML.
arxiv-12300-81 | Voted Kernel Regularization | http://arxiv.org/pdf/1509.04340v1.pdf | author:Corinna Cortes, Prasoon Goyal, Vitaly Kuznetsov, Mehryar Mohri category:cs.LG published:2015-09-14 summary:This paper presents an algorithm, Voted Kernel Regularization , that providesthe flexibility of using potentially very complex kernel functions such aspredictors based on much higher-degree polynomial kernels, while benefittingfrom strong learning guarantees. The success of our algorithm arises fromderived bounds that suggest a new regularization penalty in terms of theRademacher complexities of the corresponding families of kernel maps. In aseries of experiments we demonstrate the improved performance of our algorithmas compared to baselines. Furthermore, the algorithm enjoys several favorableproperties. The optimization problem is convex, it allows for learning withnon-PDS kernels, and the solutions are highly sparse, resulting in improvedclassification speed and memory requirements.
arxiv-12300-82 | A General Theory of Pathwise Coordinate Optimization | http://arxiv.org/pdf/1412.7477v3.pdf | author:Tuo Zhao, Han Liu, Tong Zhang category:stat.ML published:2014-12-23 summary:The pathwise coordinate optimization is one of the most importantcomputational frameworks for solving high dimensional convex and nonconvexsparse learning problems. It differs from the classical coordinate optimizationalgorithms in three salient features: warm start initialization, active setupdating, and strong rule for coordinate preselection. These three featuresgrant superior empirical performance, but also pose significant challenge totheoretical analysis. To tackle this long lasting problem, we develop a newtheory showing that these three features play pivotal roles in guaranteeing theoutstanding statistical and computational performance of the pathwisecoordinate optimization framework. In particular, we analyze the existingmethods for pathwise coordinate optimization and provide new theoreticalinsights into them. The obtained theory motivates the development of severalmodifications to improve the pathwise coordinate optimization framework, whichguarantees linear convergence to a unique sparse local optimum with optimalstatistical properties (e.g. minimax optimality and oracle properties). This isthe first result establishing the computational and statistical guarantees ofthe pathwise coordinate optimization framework in high dimensions. Thoroughnumerical experiments are provided to back up our theory.
arxiv-12300-83 | Learning without Recall by Random Walks on Directed Graphs | http://arxiv.org/pdf/1509.04332v1.pdf | author:Mohammad Amin Rahimian, Shahin Shahrampour, Ali Jadbabaie category:cs.SY math.OC stat.ML published:2015-09-14 summary:We consider a network of agents that aim to learn some unknown state of theworld using private observations and exchange of beliefs. At each time, agentsobserve private signals generated based on the true unknown state. Each agentmight not be able to distinguish the true state based only on her privateobservations. This occurs when some other states are observationally equivalentto the true state from the agent's perspective. To overcome this shortcoming,agents must communicate with each other to benefit from local observations. Wepropose a model where each agent selects one of her neighbors randomly at eachtime. Then, she refines her opinion using her private signal and the prior ofthat particular neighbor. The proposed rule can be thought of as a Bayesianagent who cannot recall the priors based on which other agents make inferences.This learning without recall approach preserves some aspects of the Bayesianinference while being computationally tractable. By establishing acorrespondence with a random walk on the network graph, we prove that under thedescribed protocol, agents learn the truth exponentially fast in the almostsure sense. The asymptotic rate is expressed as the sum of the relativeentropies between the signal structures of every agent weighted by thestationary distribution of the random walk.
arxiv-12300-84 | Analyzing Tensor Power Method Dynamics in Overcomplete Regime | http://arxiv.org/pdf/1411.1488v2.pdf | author:Anima Anandkumar, Rong Ge, Majid Janzamin category:cs.LG stat.ML published:2014-11-06 summary:We present a novel analysis of the dynamics of tensor power iterations in theovercomplete regime where the tensor CP rank is larger than the inputdimension. Finding the CP decomposition of an overcomplete tensor is NP-hard ingeneral. We consider the case where the tensor components are randomly drawn,and show that the simple power iteration recovers the components with boundederror under mild initialization conditions. We apply our analysis tounsupervised learning of latent variable models, such as multi-view mixturemodels and spherical Gaussian mixtures. Given the third order moment tensor, welearn the parameters using tensor power iterations. We prove it can correctlylearn the model parameters when the number of hidden components $k$ is muchlarger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize thepower iterations with data samples and prove its success under mild conditionson the signal-to-noise ratio of the samples. Our analysis significantly expandsthe class of latent variable models where spectral methods are applicable. Ouranalysis also deals with noise in the input tensor leading to sample complexityresult in the application to learning latent variable models.
arxiv-12300-85 | A Practioner's Guide to Evaluating Entity Resolution Results | http://arxiv.org/pdf/1509.04238v1.pdf | author:Matt Barnes category:cs.DB stat.ML published:2015-09-14 summary:Entity resolution (ER) is the task of identifying records belonging to thesame entity (e.g. individual, group) across one or multiple databases.Ironically, it has multiple names: deduplication and record linkage, amongothers. In this paper we survey metrics used to evaluate ER results in order toiteratively improve performance and guarantee sufficient quality prior todeployment. Some of these metrics are borrowed from multi-class classificationand clustering domains, though some key differences exist differentiatingentity resolution from general clustering. Menestrina et al. empirically showedrankings from these metrics often conflict with each other, thus our primarymotivation for studying them. This paper provides practitioners the basicknowledge to begin evaluating their entity resolution results.
arxiv-12300-86 | gSLICr: SLIC superpixels at over 250Hz | http://arxiv.org/pdf/1509.04232v1.pdf | author:Carl Yuheng Ren, Victor Adrian Prisacariu, Ian D Reid category:cs.CV published:2015-09-14 summary:We introduce a parallel GPU implementation of the Simple Linear IterativeClustering (SLIC) superpixel segmentation. Using a single graphic card, ourimplementation achieves speedups of up to $83\times$ from the standardsequential implementation. Our implementation is fully compatible with thestandard sequential implementation and the software is now available online andis open source.
arxiv-12300-87 | Twitter Sentiment Analysis | http://arxiv.org/pdf/1509.04219v1.pdf | author:Afroze Ibrahim Baqapuri category:cs.CL cs.IR cs.SI published:2015-09-14 summary:This project addresses the problem of sentiment analysis in twitter; that isclassifying tweets according to the sentiment expressed in them: positive,negative or neutral. Twitter is an online micro-blogging and social-networkingplatform which allows users to write short status updates of maximum length 140characters. It is a rapidly expanding service with over 200 million registeredusers - out of which 100 million are active users and half of them log ontwitter on a daily basis - generating nearly 250 million tweets per day. Due tothis large amount of usage we hope to achieve a reflection of public sentimentby analysing the sentiments expressed in the tweets. Analysing the publicsentiment is important for many applications such as firms trying to find outthe response of their products in the market, predicting political electionsand predicting socioeconomic phenomena like stock exchange. The aim of thisproject is to develop a functional classifier for accurate and automaticsentiment classification of an unknown tweet stream.
arxiv-12300-88 | Deep Learning Applied to Image and Text Matching | http://arxiv.org/pdf/1601.03478v1.pdf | author:Afroze Ibrahim Baqapuri category:cs.LG cs.CL cs.CV published:2015-09-14 summary:The ability to describe images with natural language sentences is thehallmark for image and language understanding. Such a system has wide rangingapplications such as annotating images and using natural sentences to searchfor images.In this project we focus on the task of bidirectional imageretrieval: such asystem is capable of retrieving an image based on a sentence(image search) andretrieve sentence based on an image query (image annotation).We present asystem based on a global ranking objective function which uses acombinationof convolutional neural networks (CNN) and multi layer perceptrons(MLP).It takes a pair of image and sentence and processes them in differentchannels,finally embedding it into a common multimodal vector space. Theseembeddingsencode abstract semantic information about the two inputs and can becomparedusing traditional information retrieval approaches. For each such pair,the modelreturns a score which is interpretted as a similarity metric. If thisscore is high,the image and sentence are likely to convey similar meaning, andif the score is low then they are likely not to. The visual input is modeled via deep convolutional neural network. Ontheother hand we explore three models for the textual module. The first oneisbag of words with an MLP. The second one uses n-grams (bigram, trigrams,and acombination of trigram & skip-grams) with an MLP. The third is morespecializeddeep network specific for modeling variable length sequences (SSE).We reportcomparable performance to recent work in the field, even though ouroverallmodel is simpler. We also show that the training time choice of how wecangenerate our negative samples has a significant impact on performance, and canbe used to specialize the bi-directional system in one particular task.
arxiv-12300-89 | Learning to See by Moving | http://arxiv.org/pdf/1505.01596v2.pdf | author:Pulkit Agrawal, Joao Carreira, Jitendra Malik category:cs.CV cs.NE cs.RO published:2015-05-07 summary:The dominant paradigm for feature learning in computer vision relies ontraining neural networks for the task of object recognition using millions ofhand labelled images. Is it possible to learn useful features for a diverse setof visual tasks using any other form of supervision? In biology, livingorganisms developed the ability of visual perception for the purpose of movingand acting in the world. Drawing inspiration from this observation, in thiswork we investigate if the awareness of egomotion can be used as a supervisorysignal for feature learning. As opposed to the knowledge of class labels,information about egomotion is freely available to mobile agents. We show thatgiven the same number of training images, features learnt using egomotion assupervision compare favourably to features learnt using class-label assupervision on visual tasks of scene recognition, object recognition, visualodometry and keypoint matching.
arxiv-12300-90 | Giraffe: Using Deep Reinforcement Learning to Play Chess | http://arxiv.org/pdf/1509.01549v2.pdf | author:Matthew Lai category:cs.AI cs.LG cs.NE published:2015-09-04 summary:This report presents Giraffe, a chess engine that uses self-play to discoverall its domain-specific knowledge, with minimal hand-crafted knowledge given bythe programmer. Unlike previous attempts using machine learning only to performparameter-tuning on hand-crafted evaluation functions, Giraffe's learningsystem also performs automatic feature extraction and pattern recognition. Thetrained evaluation function performs comparably to the evaluation functions ofstate-of-the-art chess engines - all of which containing thousands of lines ofcarefully hand-crafted pattern recognizers, tuned over many years by bothcomputer chess experts and human chess masters. Giraffe is the most successfulattempt thus far at using end-to-end machine learning to play chess.
arxiv-12300-91 | Color-Phase Analysis for Sinusoidal Structured Light in Rapid Range Imaging | http://arxiv.org/pdf/1509.04115v1.pdf | author:Changsoo Je, Sang Wook Lee, Rae-Hong Park category:cs.CV cs.GR physics.optics I.2.10; I.4.8 published:2015-09-14 summary:Active range sensing using structured-light is the most accurate and reliablemethod for obtaining 3D information. However, most of the work has been limitedto range sensing of static objects, and range sensing of dynamic (moving ordeforming) objects has been investigated recently only by a few researchers.Sinusoidal structured-light is one of the well-known optical methods for 3Dmeasurement. In this paper, we present a novel method for rapid high-resolutionrange imaging using color sinusoidal pattern. We consider the real-worldproblem of nonlinearity and color-band crosstalk in the color light projectorand color camera, and present methods for accurate recovery of color-phase. Forhigh-resolution ranging, we use high-frequency patterns and describe newunwrapping algorithms for reliable range recovery. The experimental resultsdemonstrate the effectiveness of our methods.
arxiv-12300-92 | Optimization of anemia treatment in hemodialysis patients via reinforcement learning | http://arxiv.org/pdf/1509.03977v1.pdf | author:Pablo Escandell-Montero, Milena Chermisi, José M. Martínez-Martínez, Juan Gómez-Sanchis, Carlo Barbieri, Emilio Soria-Olivas, Flavio Mari, Joan Vila-Francés, Andrea Stopper, Emanuele Gatti, José D. Martín-Guerrero category:stat.ML cs.AI cs.LG published:2015-09-14 summary:Objective: Anemia is a frequent comorbidity in hemodialysis patients that canbe successfully treated by administering erythropoiesis-stimulating agents(ESAs). ESAs dosing is currently based on clinical protocols that often do notaccount for the high inter- and intra-individual variability in the patient'sresponse. As a result, the hemoglobin level of some patients oscillates aroundthe target range, which is associated with multiple risks and side-effects.This work proposes a methodology based on reinforcement learning (RL) tooptimize ESA therapy. Methods: RL is a data-driven approach for solving sequential decision-makingproblems that are formulated as Markov decision processes (MDPs). Computingoptimal drug administration strategies for chronic diseases is a sequentialdecision-making problem in which the goal is to find the best sequence of drugdoses. MDPs are particularly suitable for modeling these problems due to theirability to capture the uncertainty associated with the outcome of the treatmentand the stochastic nature of the underlying process. The RL algorithm employedin the proposed methodology is fitted Q iteration, which stands out for itsability to make an efficient use of data. Results: The experiments reported here are based on a computational modelthat describes the effect of ESAs on the hemoglobin level. The performance ofthe proposed method is evaluated and compared with the well-known Q-learningalgorithm and with a standard protocol. Simulation results show that theperformance of Q-learning is substantially lower than FQI and the protocol. Conclusion: Although prospective validation is required, promising resultsdemonstrate the potential of RL to become an alternative to current protocols.
arxiv-12300-93 | Natural scene statistics mediate the perception of image complexity | http://arxiv.org/pdf/1509.03970v1.pdf | author:Nicolas Gauvrit, Fernando Soler-Toscano, Hector Zenil category:cs.AI cs.CV published:2015-09-14 summary:Humans are sensitive to complexity and regularity in patterns. The subjectiveperception of pattern complexity is correlated to algorithmic(Kolmogorov-Chaitin) complexity as defined in computer science, but also to thefrequency of naturally occurring patterns. However, the possible mediationalrole of natural frequencies in the perception of algorithmic complexity remainsunclear. Here we reanalyze Hsu et al. (2010) through a mediational analysis,and complement their results in a new experiment. We conclude that humanperception of complexity seems partly shaped by natural scenes statistics,thereby establishing a link between the perception of complexity and the effectof natural scene statistics.
arxiv-12300-94 | Learning to Divide and Conquer for Online Multi-Target Tracking | http://arxiv.org/pdf/1509.03956v1.pdf | author:Francesco Solera, Simone Calderara, Rita Cucchiara category:cs.CV published:2015-09-14 summary:Online Multiple Target Tracking (MTT) is often addressed within thetracking-by-detection paradigm. Detections are previously extractedindependently in each frame and then objects trajectories are built bymaximizing specifically designed coherence functions. Nevertheless, ambiguitiesarise in presence of occlusions or detection errors. In this paper we claimthat the ambiguities in tracking could be solved by a selective use of thefeatures, by working with more reliable features if possible and exploiting adeeper representation of the target only if necessary. To this end, we proposean online divide and conquer tracker for static camera scenes, which partitionsthe assignment problem in local subproblems and solves them by selectivelychoosing and combining the best features. The complete framework is cast as astructural learning task that unifies these phases and learns trackerparameters from examples. Experiments on two different datasets highlights asignificant improvement of tracking performances (MOTA +10%) over the state ofthe art.
arxiv-12300-95 | Parametric Maxflows for Structured Sparse Learning with Convex Relaxations of Submodular Functions | http://arxiv.org/pdf/1509.03946v1.pdf | author:Yoshinobu Kawahara, Yutaro Yamaguchi category:cs.LG cs.NA published:2015-09-14 summary:The proximal problem for structured penalties obtained via convex relaxationsof submodular functions is known to be equivalent to minimizing separableconvex functions over the corresponding submodular polyhedra. In this paper, wereveal a comprehensive class of structured penalties for which penalties thisproblem can be solved via an efficiently solvable class of parametric maxflowoptimization. We then show that the parametric maxflow algorithm proposed byGallo et al. and its variants, which runs, in the worst-case, at the cost ofonly a constant factor of a single computation of the corresponding maxflowoptimization, can be adapted to solve the proximal problems for thosepenalties. Several existing structured penalties satisfy these conditions;thus, regularized learning with these penalties is solvable quickly using theparametric maxflow algorithm. We also investigate the empirical runtimeperformance of the proposed framework.
arxiv-12300-96 | Geometry and dimensionality reduction of feature spaces in primary visual cortex | http://arxiv.org/pdf/1509.03942v1.pdf | author:Davide Barbieri category:q-bio.NC cs.CV math.GR published:2015-09-14 summary:Some geometric properties of the wavelet analysis performed by visual neuronsare discussed and compared with experimental data. In particular, severalrelationships between the cortical morphologies and the parametric dependenciesof extracted features are formalized and considered from a harmonic analysispoint of view.
arxiv-12300-97 | Learning Social Relation Traits from Face Images | http://arxiv.org/pdf/1509.03936v1.pdf | author:Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV cs.CY published:2015-09-14 summary:Social relation defines the association, e.g, warm, friendliness, anddominance, between two or more people. Motivated by psychological studies, weinvestigate if such fine-grained and high-level relation traits can becharacterised and quantified from face images in the wild. To address thischallenging problem we propose a deep model that learns a rich facerepresentation to capture gender, expression, head pose, and age-relatedattributes, and then performs pairwise-face reasoning for relation prediction.To learn from heterogeneous attribute sources, we formulate a new networkarchitecture with a bridging layer to leverage the inherent correspondencesamong these datasets. It can also cope with missing target attribute labels.Extensive experiments show that our approach is effective for fine-grainedsocial relation learning in images and videos.
arxiv-12300-98 | Markov Boundary Discovery with Ridge Regularized Linear Models | http://arxiv.org/pdf/1509.03935v1.pdf | author:Eric V. Strobl, Shyam Visweswaran category:math.ST stat.ME stat.ML stat.TH published:2015-09-14 summary:Ridge regularized linear models (RRLMs), such as ridge regression and theSVM, are a popular group of methods that are used in conjunction withcoefficient hypothesis testing to discover explanatory variables with asignificant multivariate association to a response. However, many investigatorsare reluctant to draw causal interpretations of the selected variables due tothe incomplete knowledge of the capabilities of RRLMs in causal inference.Under reasonable assumptions, we show that a modified form of RRLMs can getvery close to identifying a subset of the Markov boundary by providing aworst-case bound on the space of possible solutions. The results hold for anyconvex loss, even when the underlying functional relationship is nonlinear, andthe solution is not unique. Our approach combines ideas in Markov boundary andsufficient dimension reduction theory. Experimental results show that themodified RRLMs are competitive against state-of-the-art algorithms indiscovering part of the Markov boundary from gene expression data.
arxiv-12300-99 | A new estimate of mutual information based measure of dependence between two variables: properties and fast implementation | http://arxiv.org/pdf/1411.2883v4.pdf | author:Namita Jain, C. A. Murthy category:cs.IT cs.LG math.IT published:2014-10-28 summary:This article proposes a new method to estimate an existing mutual informationbased dependence measure using histogram density estimates. Finding a suitablebin length for histogram is an open problem. We propose a new way of computingthe bin length for histogram using a function of maximum separation betweenpoints. The chosen bin length leads to consistent density estimates forhistogram method. The values of density thus obtained are used to calculate anestimate of an existing dependence measure. The proposed estimate is named asMutual Information Based Dependence Index (MIDI). Some important properties ofMIDI have also been stated. The performance of the proposed method has beencompared to generally accepted measures like Distance Correlation (dcor),Maximal Information Coefficient (MINE) in terms of accuracy and computationalcomplexity with the help of several artificial data sets with different amountsof noise. The proposed method is able to detect many types of relationshipsbetween variables, without making any assumption about the functional form ofthe relationship. The power statistics of proposed method illustrate theireffectiveness in detecting non linear relationship. Thus, it is able to achievegenerality without a high rate of false positive cases. MIDI is found to workbetter on a real life data set than competing methods. The proposed method isfound to overcome some of the limitations which occur with dcor and MINE.Computationally, MIDI is found to be better than dcor and MINE, in terms oftime and memory, making it suitable for large data sets.
arxiv-12300-100 | A More Powerful Two-Sample Test in High Dimensions using Random Projection | http://arxiv.org/pdf/1108.2401v3.pdf | author:Miles E. Lopes, Laurent J. Jacob, Martin J. Wainwright category:math.ST stat.ME stat.ML stat.TH published:2011-08-11 summary:We consider the hypothesis testing problem of detecting a shift between themeans of two multivariate normal distributions in the high-dimensional setting,allowing for the data dimension p to exceed the sample size n. Specifically, wepropose a new test statistic for the two-sample test of means that integrates arandom projection with the classical Hotelling T^2 statistic. Working under ahigh-dimensional framework with (p,n) tending to infinity, we first derive anasymptotic power function for our test, and then provide sufficient conditionsfor it to achieve greater power than other state-of-the-art tests. Using ROCcurves generated from synthetic data, we demonstrate superior performanceagainst competing tests in the parameter regimes anticipated by our theoreticalresults. Lastly, we illustrate an advantage of our procedure's false positiverate with comparisons on high-dimensional gene expression data involving thediscrimination of different types of cancer.
arxiv-12300-101 | On Binary Classification with Single-Layer Convolutional Neural Networks | http://arxiv.org/pdf/1509.03891v1.pdf | author:Soroush Mehri category:cs.CV published:2015-09-13 summary:Convolutional neural networks are becoming standard tools for solving objectrecognition and visual tasks. However, most of the design and implementation ofthese complex models are based on trail-and-error. In this report, the mainfocus is to consider some of the important factors in designing convolutionalnetworks to perform better. Specifically, classification with wide single-layernetworks with large kernels as a general framework is considered. Particularly,we will show that pre-training using unsupervised schemes is vital, reasonableregularization is beneficial and applying of strong regularizers like dropoutcould be devastating. Pool size is also could be as important as learningprocedure itself. In addition, it has been presented that using such a simpleand relatively fast model for classifying cats and dogs, performance is closeto state-of-the-art achievable by a combination of SVM models on color andtexture features.
arxiv-12300-102 | The USFD Spoken Language Translation System for IWSLT 2014 | http://arxiv.org/pdf/1509.03870v1.pdf | author:Raymond W. M. Ng, Mortaza Doulaty, Rama Doddipatla, Wilker Aziz, Kashif Shah, Oscar Saz, Madina Hasan, Ghada AlHarbi, Lucia Specia, Thomas Hain category:cs.CL published:2015-09-13 summary:The University of Sheffield (USFD) participated in the International Workshopfor Spoken Language Translation (IWSLT) in 2014. In this paper, we willintroduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) isachieved by two multi-pass deep neural network systems with adaptation andrescoring techniques. Machine translation (MT) is achieved by a phrase-basedsystem. The USFD primary system incorporates state-of-the-art ASR and MTtechniques and gives a BLEU score of 23.45 and 14.75 on the English-to-Frenchand English-to-German speech-to-text translation task with the IWSLT 2014 data.The USFD contrastive systems explore the integration of ASR and MT by using aquality estimation system to rescore the ASR outputs, optimising towards bettertranslation. This gives a further 0.54 and 0.26 BLEU improvement respectivelyon the IWSLT 2012 and 2014 evaluation data.
arxiv-12300-103 | Vectors of Locally Aggregated Centers for Compact Video Representation | http://arxiv.org/pdf/1509.03844v1.pdf | author:Alhabib Abbas, Nikos Deligiannis, Yiannis Andreopoulos category:cs.MM cs.CV cs.IR published:2015-09-13 summary:We propose a novel vector aggregation technique for compact videorepresentation, with application in accurate similarity detection within largevideo datasets. The current state-of-the-art in visual search is formed by thevector of locally aggregated descriptors (VLAD) of Jegou et. al. VLAD generatescompact video representations based on scale-invariant feature transform (SIFT)vectors (extracted per frame) and local feature centers computed over atraining set. With the aim to increase robustness to visual distortions, wepropose a new approach that operates at a coarser level in the featurerepresentation. We create vectors of locally aggregated centers (VLAC) by firstclustering SIFT features to obtain local feature centers (LFCs) and thenencoding the latter with respect to given centers of local feature centers(CLFCs), extracted from a training set. The sum-of-differences between the LFCsand the CLFCs are aggregated to generate an extremely-compact video descriptionused for accurate video segment similarity detection. Experimentation using avideo dataset, comprising more than 1000 minutes of content from the Open VideoProject, shows that VLAC obtains substantial gains in terms of mean AveragePrecision (mAP) against VLAD and the hyper-pooling method of Douze et. al.,under the same compaction factor and the same set of distortions.
arxiv-12300-104 | Statistical Analysis of Loopy Belief Propagation in Random Fields | http://arxiv.org/pdf/1503.04585v3.pdf | author:Muneki Yasuda, Shun Kataoka, Kazuyuki Tanaka category:stat.ML cs.CV published:2015-03-16 summary:Loopy belief propagation (LBP), which is equivalent to the Betheapproximation in statistical mechanics, is a message-passing-type inferencemethod that is widely used to analyze systems based on Markov random fields(MRFs). In this paper, we propose a message-passing-type method to analyticallyevaluate the quenched average of LBP in random fields by using the replicacluster variation method. The proposed analytical method is applicable togeneral pair-wise MRFs with random fields whose distributions differ from eachother and can give the quenched averages of the Bethe free energies over randomfields, which are consistent with numerical results. The order of itscomputational cost is equivalent to that of standard LBP. In the latter part ofthis paper, we describe the application of the proposed method to Bayesianimage restoration, in which we observed that our theoretical results are ingood agreement with the numerical results for natural images.
arxiv-12300-105 | Sample Complexity Bounds on Differentially Private Learning via Communication Complexity | http://arxiv.org/pdf/1402.6278v4.pdf | author:Vitaly Feldman, David Xiao category:cs.DS cs.CC cs.LG published:2014-02-25 summary:In this work we analyze the sample complexity of classification bydifferentially private algorithms. Differential privacy is a strong andwell-studied notion of privacy introduced by Dwork et al. (2006) that ensuresthat the output of an algorithm leaks little information about the data pointprovided by any of the participating individuals. Sample complexity of privatePAC and agnostic learning was studied in a number of prior works starting with(Kasiviswanathan et al., 2008) but a number of basic questions still remainopen, most notably whether learning with privacy requires more samples thanlearning without privacy. We show that the sample complexity of learning with (pure) differentialprivacy can be arbitrarily higher than the sample complexity of learningwithout the privacy constraint or the sample complexity of learning withapproximate differential privacy. Our second contribution and the main tool isan equivalence between the sample complexity of (pure) differentially privatelearning of a concept class $C$ (or $SCDP(C)$) and the randomized one-waycommunication complexity of the evaluation problem for concepts from $C$. Usingthis equivalence we prove the following bounds: 1. $SCDP(C) = \Omega(LDim(C))$, where $LDim(C)$ is the Littlestone's (1987)dimension characterizing the number of mistakes in the online-mistake-boundlearning model. Known bounds on $LDim(C)$ then imply that $SCDP(C)$ can be muchhigher than the VC-dimension of $C$. 2. For any $t$, there exists a class $C$ such that $LDim(C)=2$ but $SCDP(C)\geq t$. 3. For any $t$, there exists a class $C$ such that the sample complexity of(pure) $\alpha$-differentially private PAC learning is $\Omega(t/\alpha)$ butthe sample complexity of the relaxed $(\alpha,\beta)$-differentially privatePAC learning is $O(\log(1/\beta)/\alpha)$. This resolves an open problem ofBeimel et al. (2013b).
arxiv-12300-106 | A Dual Fast and Slow Feature Interaction in Biologically Inspired Visual Recognition of Human Action | http://arxiv.org/pdf/1509.02587v2.pdf | author:Bardia Yousefi, C. K. Loo category:cs.CV published:2015-09-09 summary:Computational neuroscience studies that have examined human visual systemthrough functional magnetic resonance imaging (fMRI) have identified a modelwhere the mammalian brain pursues two distinct pathways (for recognition ofbiological movement tasks). In the brain, dorsal stream analyzes theinformation of motion (optical flow), which is the fast features, and ventralstream (form pathway) analyzes form information (through active basis modelbased incremental slow feature analysis ) as slow features. The proposedapproach suggests the motion perception of the human visual system composes offast and slow feature interactions that identifies biological movements. Formfeatures in the visual system biologically follows the application of activebasis model with incremental slow feature analysis for the extraction of theslowest form features of human objects movements in the ventral stream.Applying incremental slow feature analysis provides an opportunity to use theaction prototypes. To extract the slowest features episodic observation isrequired but the fast features updates the processing of motion information inevery frames. Experimental results have shown promising accuracy for theproposed model and good performance with two datasets (KTH and Weizmann).
arxiv-12300-107 | Compositional Dictionaries for Domain Adaptive Face Recognition | http://arxiv.org/pdf/1308.0271v2.pdf | author:Qiang Qiu, Rama Chellappa category:cs.CV published:2013-08-01 summary:We present a dictionary learning approach to compensate for thetransformation of faces due to changes in view point, illumination, resolution,etc. The key idea of our approach is to force domain-invariant sparse coding,i.e., design a consistent sparse representation of the same face in differentdomains. In this way, classifiers trained on the sparse codes in the sourcedomain consisting of frontal faces for example can be applied to the targetdomain (consisting of faces in different poses, illumination conditions, etc)without much loss in recognition accuracy. The approach is to first learn adomain base dictionary, and then describe each domain shift (identity, pose,illumination) using a sparse representation over the base dictionary. Thedictionary adapted to each domain is expressed as sparse linear combinations ofthe base dictionary. In the context of face recognition, with the proposedcompositional dictionary approach, a face image can be decomposed into sparserepresentations for a given subject, pose and illumination respectively. Thisapproach has three advantages: first, the extracted sparse representation for asubject is consistent across domains and enables pose and illuminationinsensitive face recognition. Second, sparse representations for pose andillumination can subsequently be used to estimate the pose and illuminationcondition of a face image. Finally, by composing sparse representations forsubject and the different domains, we can also perform pose alignment andillumination normalization. Extensive experiments using two public facedatasets are presented to demonstrate the effectiveness of our approach forface recognition.
arxiv-12300-108 | Generalized Uniformly Optimal Methods for Nonlinear Programming | http://arxiv.org/pdf/1508.07384v2.pdf | author:Saeed Ghadimi, Guanghui Lan, Hongchao Zhang category:math.OC stat.ML published:2015-08-29 summary:In this paper, we present a generic framework to extend existing uniformlyoptimal convex programming algorithms to solve more general nonlinear, possiblynonconvex, optimization problems. The basic idea is to incorporate a localsearch step (gradient descent or Quasi-Newton iteration) into these uniformlyoptimal convex programming methods, and then enforce a monotone decreasingproperty of the function values computed along the trajectory. Algorithms ofthese types will then achieve the best known complexity for nonconvex problems,and the optimal complexity for convex ones without requiring any problemparameters. As a consequence, we can have a unified treatment for a generalclass of nonlinear programming problems regardless of their convexity andsmoothness level. In particular, we show that the accelerated gradient andlevel methods, both originally designed for solving convex optimizationproblems only, can be used for solving both convex and nonconvex problemsuniformly. In a similar vein, we show that some well-studied techniques fornonlinear programming, e.g., Quasi-Newton iteration, can be embedded intooptimal convex optimization algorithms to possibly further enhance theirnumerical performance. Our theoretical and algorithmic developments arecomplemented by some promising numerical results obtained for solving a fewimportant nonconvex and nonlinear data analysis problems in the literature.
arxiv-12300-109 | Improving distant supervision using inference learning | http://arxiv.org/pdf/1509.03739v1.pdf | author:Roland Roller, Eneko Agirre, Aitor Soroa, Mark Stevenson category:cs.CL published:2015-09-12 summary:Distant supervision is a widely applied approach to automatic training ofrelation extraction systems and has the advantage that it can generate largeamounts of labelled data with minimal effort. However, this data may containerrors and consequently systems trained using distant supervision tend not toperform as well as those based on manually labelled data. This work proposes anovel method for detecting potential false negative training examples using aknowledge inference method. Results show that our approach improves theperformance of relation extraction systems trained using distantly superviseddata.
arxiv-12300-110 | Competitive and Penalized Clustering Auto-encoder | http://arxiv.org/pdf/1508.07175v2.pdf | author:Zihao Wang, Yiuming Cheung category:cs.LG published:2015-08-28 summary:Auto-encoders (AE) has been widely applied in different fields of machinelearning. However, as a deep model, there are a large amount of learnableparameters in the AE, which would cause over-fitting and slow learning speed inpractice. Many researchers have been study the intrinsic structure of AE andshowed different useful methods to regularize those parameters. In this paper,we present a novel regularization method based on a clustering algorithm whichis able to classify the parameters into different groups. With thisregularization, parameters in a given group have approximate equivalent valuesand over-fitting problem could be alleviated. Moreover, due to the competitivebehavior of clustering algorithm, this model also overcomes some intrinsicproblems of clustering algorithms like the determination of number of clusters.Experiments on handwritten digits recognition verify the effectiveness of ournovel model.
arxiv-12300-111 | Kannada named entity recognition and classification (nerc) based on multinomial naïve bayes (mnb) classifier | http://arxiv.org/pdf/1509.04385v1.pdf | author:S. Amarappa, S. V. Sathyanarayana category:cs.CL published:2015-09-12 summary:Named Entity Recognition and Classification (NERC) is a process ofidentification of proper nouns in the text and classification of those nounsinto certain predefined categories like person name, location, organization,date, and time etc. NERC in Kannada is an essential and challenging task. Theaim of this work is to develop a novel model for NERC, based on MultinomialNa\"ive Bayes (MNB) Classifier. The Methodology adopted in this paper is basedon feature extraction of training corpus, by using term frequency, inversedocument frequency and fitting them to a tf-idf-vectorizer. The paper discussesthe various issues in developing the proposed model. The details ofimplementation and performance evaluation are discussed. The experiments areconducted on a training corpus of size 95,170 tokens and test corpus of 5,000tokens. It is observed that the model works with Precision, Recall andF1-measure of 83%, 79% and 81% respectively.
arxiv-12300-112 | Poisson Matrix Recovery and Completion | http://arxiv.org/pdf/1504.05229v2.pdf | author:Yang Cao, Yao Xie category:cs.LG math.ST stat.ML stat.TH published:2015-04-20 summary:We extend the theory of low-rank matrix recovery and completion to the casewhen Poisson observations for a linear combination or a subset of the entriesof a matrix are available, which arises in various applications with countdata. We consider the usual matrix recovery formulation through maximumlikelihood with proper constraints on the matrix $M$ of size $d_1$-by-$d_2$,and establish theoretical upper and lower bounds on the recovery error. Ourbounds for matrix completion are nearly optimal up to a factor on the order of$\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by combing techniquesfor compressed sensing for sparse vectors with Poisson noise and for analyzinglow-rank matrices, as well as adapting the arguments used for one-bit matrixcompletion \cite{davenport20121} (although these two problems are different innature) and the adaptation requires new techniques exploiting properties of thePoisson likelihood function and tackling the difficulties posed by the locallysub-Gaussian characteristic of the Poisson distribution. Our results highlighta few important distinctions of the Poisson case compared to the prior workincluding having to impose a minimum signal-to-noise requirement on eachobserved entry and a gap in the upper and lower bounds. We also develop a setof efficient iterative algorithms and demonstrate their good performance onsynthetic examples and real data.
arxiv-12300-113 | Language Understanding for Text-based Games Using Deep Reinforcement Learning | http://arxiv.org/pdf/1506.08941v2.pdf | author:Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay category:cs.CL cs.AI published:2015-06-30 summary:In this paper, we consider the task of learning control policies fortext-based games. In these games, all interactions in the virtual world arethrough text and the underlying state is not observed. The resulting languagebarrier makes such environments challenging for automatic game players. Weemploy a deep reinforcement learning framework to jointly learn staterepresentations and action policies using game rewards as feedback. Thisframework enables us to map text descriptions into vector representations thatcapture the semantics of the game states. We evaluate our approach on two gameworlds, comparing against baselines using bag-of-words and bag-of-bigrams forstate representations. Our algorithm outperforms the baselines on both worldsdemonstrating the importance of learning expressive representations.
arxiv-12300-114 | Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models | http://arxiv.org/pdf/1409.8185v3.pdf | author:Theodoros Tsiligkaridis, Keith W. Forsythe category:stat.ML cs.LG stat.ME published:2014-09-29 summary:We develop a sequential low-complexity inference procedure for Dirichletprocess mixtures of Gaussians for online clustering and parameter estimationwhen the number of clusters are unknown a-priori. We present an easilycomputable, closed form parametric expression for the conditional likelihood,in which hyperparameters are recursively updated as a function of the streamingdata assuming conjugate priors. Motivated by large-sample asymptotics, wepropose a novel adaptive low-complexity design for the Dirichlet processconcentration parameter and show that the number of classes grow at most at alogarithmic rate. We further prove that in the large-sample limit, theconditional likelihood and data predictive distribution become asymptoticallyGaussian. We demonstrate through experiments on synthetic and real data setsthat our approach is superior to other online state-of-the-art methods.
arxiv-12300-115 | Learning Co-Sparse Analysis Operators with Separable Structures | http://arxiv.org/pdf/1503.02398v5.pdf | author:Matthias Seibert, Julian Wörmann, Rémi Gribonval, Martin Kleinsteuber category:cs.LG stat.ML published:2015-03-09 summary:In the co-sparse analysis model a set of filters is applied to a signal outof the signal class of interest yielding sparse filter responses. As such, itmay serve as a prior in inverse problems, or for structural analysis of signalsthat are known to belong to the signal class. The more the model is adapted tothe class, the more reliable it is for these purposes. The task of learningsuch operators for a given class is therefore a crucial problem. In manyapplications, it is also required that the filter responses are obtained in atimely manner, which can be achieved by filters with a separable structure. Notonly can operators of this sort be efficiently used for computing the filterresponses, but they also have the advantage that less training samples arerequired to obtain a reliable estimate of the operator. The first contributionof this work is to give theoretical evidence for this claim by providing anupper bound for the sample complexity of the learning process. The second is astochastic gradient descent (SGD) method designed to learn an analysis operatorwith separable structures, which includes a novel and efficient step sizeselection rule. Numerical experiments are provided that link the samplecomplexity to the convergence speed of the SGD algorithm.
arxiv-12300-116 | DeepSat - A Learning framework for Satellite Imagery | http://arxiv.org/pdf/1509.03602v1.pdf | author:Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert DiBiano, Manohar Karki, Ramakrishna Nemani category:cs.CV published:2015-09-11 summary:Satellite image classification is a challenging problem that lies at thecrossroads of remote sensing, computer vision, and machine learning. Due to thehigh variability inherent in satellite data, most of the current objectclassification approaches are not suitable for handling satellite datasets. Theprogress of satellite image analytics has also been inhibited by the lack of asingle labeled high-resolution dataset with multiple class labels. Thecontributions of this paper are twofold - (1) first, we present two newsatellite datasets called SAT-4 and SAT-6, and (2) then, we propose aclassification framework that extracts features from an input image, normalizesthem and feeds the normalized feature vectors to a Deep Belief Network forclassification. On the SAT-4 dataset, our best network produces aclassification accuracy of 97.95% and outperforms three state-of-the-art objectrecognition algorithms, namely - Deep Belief Networks, Convolutional NeuralNetworks and Stacked Denoising Autoencoders by ~11%. On SAT-6, it produces aclassification accuracy of 93.9% and outperforms the other algorithms by ~15%.Comparative studies with a Random Forest classifier show the advantage of anunsupervised learning approach over traditional supervised learning techniques.A statistical analysis based on Distribution Separability Criterion andIntrinsic Dimensionality Estimation substantiates the effectiveness of ourapproach in learning better representations for satellite imagery.
arxiv-12300-117 | Better Document-level Sentiment Analysis from RST Discourse Parsing | http://arxiv.org/pdf/1509.01599v2.pdf | author:Parminder Bhatia, Yangfeng Ji, Jacob Eisenstein category:cs.CL cs.AI published:2015-09-04 summary:Discourse structure is the hidden link between surface features anddocument-level properties, such as sentiment polarity. We show that thediscourse analyses produced by Rhetorical Structure Theory (RST) parsers canimprove document-level sentiment analysis, via composition of local informationup the discourse tree. First, we show that reweighting discourse unitsaccording to their position in a dependency representation of the rhetoricalstructure can yield substantial improvements on lexicon-based sentimentanalysis. Next, we present a recursive neural network over the RST structure,which offers significant improvements over classification-based methods.
arxiv-12300-118 | Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts | http://arxiv.org/pdf/1505.06973v2.pdf | author:Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavoué, Thomas Brox, Bjoern Andres category:cs.CV published:2015-05-26 summary:Formulations of the Image Decomposition Problem as a Multicut Problem (MP)w.r.t. a superpixel graph have received considerable attention. In contrast,instances of the MP w.r.t. a pixel grid graph have received little attention,firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph arehard to solve in practice, and, secondly, due to the lack of long-range termsin the objective function of the MP. We propose a generalization of the MP withlong-range terms (LMP). We design and implement two efficient algorithms(primal feasible heuristics) for the MP and LMP which allow us to studyinstances of both problems w.r.t. the pixel grid graphs of the images in theBSDS-500 benchmark. The decompositions we obtain do not differ significantlyfrom the state of the art, suggesting that the LMP is a competitive formulationof the Image Decomposition Problem. To demonstrate the generality of the LMP,we apply it also to the Mesh Decomposition Problem posed by the Princetonbenchmark, obtaining state-of-the-art decompositions.
arxiv-12300-119 | Inferring and evaluating semantic classes of verbs signaling modality | http://arxiv.org/pdf/1509.03488v1.pdf | author:Judith Eckle-Kohler category:cs.CL published:2015-09-11 summary:We infer semantic classes of verbs signaling modality from a purely syntacticclassification of 637 German verbs by applying findings from linguistics aboutcorrespondences between verb meaning and syntax. Our extensive evaluation ofthe semantic classification is based on a linking to three other lexicalresources at the word sense level: to the German wordnet GermaNet and to theEnglish resources VerbNet and FrameNet. This way, we are able to perform areproducible semantic characterization of the inferred German classes. We alsoperform a corpus-based evaluation revealing that the frequencies of the classesin corpora of different genres are significantly different. We will make theresulting bilingual resource of German-English semantically categorized verbclasses publicly available.
arxiv-12300-120 | CURL: Co-trained Unsupervised Representation Learning for Image Classification | http://arxiv.org/pdf/1505.08098v2.pdf | author:Simone Bianco, Gianluigi Ciocca, Claudio Cusano category:cs.LG cs.CV stat.ML I.2.6 published:2015-05-29 summary:In this paper we propose a strategy for semi-supervised image classificationthat leverages unsupervised representation learning and co-training. Thestrategy, that is called CURL from Co-trained Unsupervised RepresentationLearning, iteratively builds two classifiers on two different views of thedata. The two views correspond to different representations learned from bothlabeled and unlabeled data and differ in the fusion scheme used to combine theimage features. To assess the performance of our proposal, we conducted severalexperiments on widely used data sets for scene and object recognition. Weconsidered three scenarios (inductive, transductive and self-taught learning)that differ in the strategy followed to exploit the unlabeled data. As imagefeatures we considered a combination of GIST, PHOG, and LBP as well as featuresextracted from a Convolutional Neural Network. Moreover, two embodiments ofCURL are investigated: one using Ensemble Projection as unsupervisedrepresentation learning coupled with Logistic Regression, and one based onLapSVM. The results show that CURL clearly outperforms other supervised andsemi-supervised learning methods in the state of the art.
arxiv-12300-121 | Newton-based maximum likelihood estimation in nonlinear state space models | http://arxiv.org/pdf/1502.03655v2.pdf | author:Manon Kok, Johan Dahlin, Thomas B. Schön, Adrian Wills category:stat.CO stat.ML published:2015-02-12 summary:Maximum likelihood (ML) estimation using Newton's method in nonlinear statespace models (SSMs) is a challenging problem due to the analyticalintractability of the log-likelihood and its gradient and Hessian. We estimatethe gradient and Hessian using Fisher's identity in combination with asmoothing algorithm. We explore two approximations of the log-likelihood and ofthe solution of the smoothing problem. The first is a linearizationapproximation which is computationally cheap, but the accuracy typically variesbetween models. The second is a sampling approximation which is asymptoticallyvalid for any SSM but is more computationally costly. We demonstrate ourapproach for ML parameter estimation on simulated data from two different SSMswith encouraging results.
arxiv-12300-122 | Nested Sequential Monte Carlo Methods | http://arxiv.org/pdf/1502.02536v3.pdf | author:Christian A. Naesseth, Fredrik Lindsten, Thomas B. Schön category:stat.CO stat.ME stat.ML published:2015-02-09 summary:We propose nested sequential Monte Carlo (NSMC), a methodology to sample fromsequences of probability distributions, even where the random variables arehigh-dimensional. NSMC generalises the SMC framework by requiring onlyapproximate, properly weighted, samples from the SMC proposal distribution,while still resulting in a correct SMC algorithm. Furthermore, NSMC can initself be used to produce such properly weighted samples. Consequently, oneNSMC sampler can be used to construct an efficient high-dimensional proposaldistribution for another NSMC sampler, and this nesting of the algorithm can bedone to an arbitrary degree. This allows us to consider complex andhigh-dimensional models using SMC. We show results that motivate the efficacyof our approach on several filtering problems with dimensions in the order of100 to 1 000.
arxiv-12300-123 | OCR accuracy improvement on document images through a novel pre-processing approach | http://arxiv.org/pdf/1509.03456v1.pdf | author:Abdeslam El Harraj, Naoufal Raissouni category:cs.CV published:2015-09-11 summary:Digital camera and mobile document image acquisition are new trends arisingin the world of Optical Character Recognition and text detection. In somecases, such process integrates many distortions and produces poorly scannedtext or text-photo images and natural images, leading to an unreliable OCRdigitization. In this paper, we present a novel nonparametric and unsupervisedmethod to compensate for undesirable document image distortions aiming tooptimally improve OCR accuracy. Our approach relies on a very efficient stackof document image enhancing techniques to recover deformation of the entiredocument image. First, we propose a local brightness and contrast adjustmentmethod to effectively handle lighting variations and the irregular distributionof image illumination. Second, we use an optimized greyscale conversionalgorithm to transform our document image to greyscale level. Third, we sharpenthe useful information in the resulting greyscale image using Un-sharp Maskingmethod. Finally, an optimal global binarization approach is used to prepare thefinal document image to OCR recognition. The proposed approach cansignificantly improve text detection rate and optical character recognitionaccuracy. To demonstrate the efficiency of our approach, an exhaustiveexperimentation on a standard dataset is presented.
arxiv-12300-124 | A reliable order-statistics-based approximate nearest neighbor search algorithm | http://arxiv.org/pdf/1509.03453v1.pdf | author:Luisa Verdoliva, Davide Cozzolino, Giovanni Poggi category:cs.CV published:2015-09-11 summary:We propose a new algorithm for fast approximate nearest neighbor search basedon the properties of ordered vectors. Data vectors are classified based on theindex and sign of their largest components, thereby partitioning the space in anumber of cones centered in the origin. The query is itself classified, and thesearch starts from the selected cone and proceeds to neighboring ones. Overall,the proposed algorithm corresponds to locality sensitive hashing in the spaceof directions, with hashing based on the order of components. Thanks to thestatistical features emerging through ordering, it deals very well with thechallenging case of unstructured data, and is a valuable building block formore complex techniques dealing with structured data. Experiments on bothsimulated and real-world data prove the proposed algorithm to provide astate-of-the-art performance.
arxiv-12300-125 | Learning Sparse Feature Representations using Probabilistic Quadtrees and Deep Belief Nets | http://arxiv.org/pdf/1509.03413v1.pdf | author:Saikat Basu, Manohar Karki, Sangram Ganguly, Robert DiBiano, Supratik Mukhopadhyay, Ramakrishna Nemani category:cs.CV published:2015-09-11 summary:Learning sparse feature representations is a useful instrument for solving anunsupervised learning problem. In this paper, we present three labeledhandwritten digit datasets, collectively called n-MNIST. Then, we propose anovel framework for the classification of handwritten digits that learns sparserepresentations using probabilistic quadtrees and Deep Belief Nets. On theMNIST and n-MNIST datasets, our framework shows promising results andsignificantly outperforms traditional Deep Belief Networks.
arxiv-12300-126 | Order Selection of Autoregressive Processes using Bridge Criterion | http://arxiv.org/pdf/1508.02473v2.pdf | author:Jie Ding, Mohammad Noshad, Vahid Tarokh category:math.ST q-fin.EC stat.ML stat.TH published:2015-08-11 summary:A new criterion is introduced for determining the order of an autoregressivemodel fit to time series data. The proposed technique is shown to give aconsistent and asymptotically efficient order estimation. It has the benefitsof the two well-known model selection techniques, the Akaike informationcriterion and the Bayesian information criterion. When the true order of theautoregression is relatively large compared with the sample size, the Akaikeinformation criterion is known to be efficient, and the new criterion behavesin a similar manner. When the true order is finite and small compared with thesample size, the Bayesian information criterion is known to be consistent, andso is the new criterion. Thus the new criterion builds a bridge between the twoclassical criteria automatically. In practice, where the observed time seriesis given without any prior information about the autoregression, the proposedorder selection criterion is more flexible and robust compared with classicalapproaches. Numerical results are presented demonstrating the robustness of theproposed technique when applied to various datasets.
arxiv-12300-127 | Learning the Number of Autoregressive Mixtures in Time Series Using the Gap Statistics | http://arxiv.org/pdf/1509.03381v1.pdf | author:Jie Ding, Mohammad Noshad, Vahid Tarokh category:stat.ML published:2015-09-11 summary:Using a proper model to characterize a time series is crucial in makingaccurate predictions. In this work we use time-varying autoregressive process(TVAR) to describe non-stationary time series and model it as a mixture ofmultiple stable autoregressive (AR) processes. We introduce a new modelselection technique based on Gap statistics to learn the appropriate number ofAR filters needed to model a time series. We define a new distance measurebetween stable AR filters and draw a reference curve that is used to measurehow much adding a new AR filter improves the performance of the model, and thenchoose the number of AR filters that has the maximum gap with the referencecurve. To that end, we propose a new method in order to generate uniform randomstable AR filters in root domain. Numerical results are provided demonstratingthe performance of the proposed approach.
arxiv-12300-128 | A DEEP analysis of the META-DES framework for dynamic selection of ensemble of classifiers | http://arxiv.org/pdf/1509.00825v2.pdf | author:Rafael M. O. Cruz, Robert Sabourin, George D. C. Cavalcanti category:cs.LG stat.ML published:2015-09-02 summary:Dynamic ensemble selection (DES) techniques work by estimating the level ofcompetence of each classifier from a pool of classifiers. Only the mostcompetent ones are selected to classify a given test sample. Hence, the keyissue in DES is the criterion used to estimate the level of competence of theclassifiers in predicting the label of a given test sample. In order to performa more robust ensemble selection, we proposed the META-DES framework usingmeta-learning, where multiple criteria are encoded as meta-features and arepassed down to a meta-classifier that is trained to estimate the competencelevel of a given classifier. In this technical report, we present astep-by-step analysis of each phase of the framework during training and test.We show how each set of meta-features is extracted as well as their impact onthe estimation of the competence level of the base classifier. Moreover, ananalysis of the impact of several factors in the system performance, such asthe number of classifiers in the pool, the use of different linear baseclassifiers, as well as the size of the validation data. We show that using thedynamic selection of linear classifiers through the META-DES framework, we cansolve complex non-linear classification problems where other combinationtechniques such as AdaBoost cannot.
arxiv-12300-129 | Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems | http://arxiv.org/pdf/1509.03371v1.pdf | author:Fabian Tschopp category:cs.CV cs.AI I.2.6; I.5.1 published:2015-09-11 summary:This work presents and analyzes three convolutional neural network (CNN)models for efficient pixelwise classification of images. When usingconvolutional neural networks to classify single pixels in patches of a wholeimage, a lot of redundant computations are carried out when using slidingwindow networks. This set of new architectures solve this issue by eitherremoving redundant computations or using fully convolutional architectures thatinherently predict many pixels at once. The implementations of the three models are accessible through a new utilityon top of the Caffe library. The utility provides support for a wide range ofimage input and output formats, pre-processing parameters and methods toequalize the label histogram during training. The Caffe library has beenextended by new layers and a new backend for availability on a wider range ofhardware such as CPUs and GPUs through OpenCL. On AMD GPUs, speedups of $54\times$ (SK-Net), $437\times$ (U-Net) and$320\times$ (USK-Net) have been observed, taking the SK equivalent SW (slidingwindow) network as the baseline. The label throughput is up to one megapixelper second. The analyzed neural networks have distinctive characteristics that applyduring training or processing, and not every data set is suitable to everyarchitecture. The quality of the predictions is assessed on two neural tissuedata sets, of which one is the ISBI 2012 challenge data set. Two different lossfunctions, Malis loss and Softmax loss, were used during training. The whole pipeline, consisting of models, interface and modified Caffelibrary, is available as Open Source software under the working title ProjectGreentea.
arxiv-12300-130 | Performance Bounds for Pairwise Entity Resolution | http://arxiv.org/pdf/1509.03302v1.pdf | author:Matt Barnes, Kyle Miller, Artur Dubrawski category:stat.ML cs.CY cs.DB cs.LG published:2015-09-10 summary:One significant challenge to scaling entity resolution algorithms to massivedatasets is understanding how performance changes after moving beyond the realmof small, manually labeled reference datasets. Unlike traditional machinelearning tasks, when an entity resolution algorithm performs well on smallhold-out datasets, there is no guarantee this performance holds on largerhold-out datasets. We prove simple bounding properties between the performanceof a match function on a small validation set and the performance of a pairwiseentity resolution algorithm on arbitrarily sized datasets. Thus, our approachenables optimization of pairwise entity resolution algorithms for largedatasets, using a small set of labeled data.
arxiv-12300-131 | Density Evolution in the Degree-correlated Stochastic Block Model | http://arxiv.org/pdf/1509.03281v1.pdf | author:Elchanan Mossel, Jiaming Xu category:stat.ML cs.IT math.IT math.PR published:2015-09-10 summary:There is a recent surge of interest in identifying the sharp recoverythresholds for cluster recovery under the stochastic block model. In thispaper, we address the more refined question of how many vertices that will bemisclassified on average. We consider the binary form of the stochastic blockmodel, where $n$ vertices are partitioned into two clusters with edgeprobability $a/n$ within the first cluster, $c/n$ within the second cluster,and $b/n$ across clusters. Suppose that as $n \to \infty$, $a= b+ \mu \sqrt{ b}$, $c=b+ \nu \sqrt{ b} $ for two fixed constants $\mu, \nu$, and $b \to \infty$with $b=n^{o(1)}$. When the cluster sizes are balanced and $\mu \neq \nu$, weshow that the minimum fraction of misclassified vertices on average is given by$Q(\sqrt{v^*})$, where $Q(x)$ is the Q-function for standard normal, $v^*$ isthe unique fixed point of $v= \frac{(\mu-\nu)^2}{16} + \frac{ (\mu+\nu)^2 }{16}\mathbb{E}[ \tanh(v+ \sqrt{v} Z)],$ and $Z$ is standard normal. Moreover, theminimum misclassified fraction on average is attained by a local algorithm,namely belief propagation, in time linear in the number of edges. Our prooftechniques are based on connecting the cluster recovery problem to treereconstruction problems, and analyzing the density evolution of beliefpropagation on trees with Gaussian approximations.
arxiv-12300-132 | Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests | http://arxiv.org/pdf/1404.6473v2.pdf | author:Lucas Mentch, Giles Hooker category:stat.ML stat.AP stat.CO stat.ME published:2014-04-25 summary:This work develops formal statistical inference procedures for machinelearning ensemble methods. Ensemble methods based on bootstrapping, such asbagging and random forests, have improved the predictive accuracy of individualtrees, but fail to provide a framework in which distributional results can beeasily determined. Instead of aggregating full bootstrap samples, we considerpredicting by averaging over trees built on subsamples of the training set anddemonstrate that the resulting estimator takes the form of a U-statistic. Assuch, predictions for individual feature vectors are asymptotically normal,allowing for confidence intervals to accompany predictions. In practice, asubset of subsamples is used for computational speed; here our estimators takethe form of incomplete U-statistics and equivalent results are derived. Wefurther demonstrate that this setup provides a framework for testing thesignificance of features. Moreover, the internal estimation method we developallows us to estimate the variance parameters and perform these inferenceprocedures at no additional computational cost. Simulations and illustrationson a real dataset are provided.
arxiv-12300-133 | Rigid Multiview Varieties | http://arxiv.org/pdf/1509.03257v1.pdf | author:Michael Joswig, Joe Kileel, Bernd Sturmfels, André Wagner category:math.AG cs.CV math.AC 14M99, 68T45 published:2015-09-10 summary:The multiview variety from computer vision is generalized to images by $n$cameras of points linked by a distance constraint. The resultingfive-dimensional variety lives in a product of $2n$ projective planes. Wedetermine defining polynomial equations, and we explore generalizations of thisvariety to scenarios of interest in applications.
arxiv-12300-134 | Classifying Tweet Level Judgements of Rumours in Social Media | http://arxiv.org/pdf/1506.00468v2.pdf | author:Michal Lukasik, Trevor Cohn, Kalina Bontcheva category:cs.SI cs.CL cs.LG published:2015-06-01 summary:Social media is a rich source of rumours and corresponding communityreactions. Rumours reflect different characteristics, some shared and someindividual. We formulate the problem of classifying tweet level judgements ofrumours as a supervised learning task. Both supervised and unsupervised domainadaptation are considered, in which tweets from a rumour are classified on thebasis of other annotated rumours. We demonstrate how multi-task learning helpsachieve good results on rumours from the 2011 England riots.
arxiv-12300-135 | A deep matrix factorization method for learning attribute representations | http://arxiv.org/pdf/1509.03248v1.pdf | author:George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, Bjoern W. Schuller category:cs.CV cs.LG stat.ML published:2015-09-10 summary:Semi-Non-negative Matrix Factorization is a technique that learns alow-dimensional representation of a dataset that lends itself to a clusteringinterpretation. It is possible that the mapping between this new representationand our original data matrix contains rather complex hierarchical informationwith implicit lower-level hidden attributes, that classical one levelclustering methodologies can not interpret. In this work we propose a novelmodel, Deep Semi-NMF, that is able to learn such hidden representations thatallow themselves to an interpretation of clustering according to different,unknown attributes of a given dataset. We also present a semi-supervisedversion of the algorithm, named Deep WSF, that allows the use of (partial)prior information for each of the known attributes of a dataset, that allowsthe model to be used on datasets with mixed attribute knowledge. Finally, weshow that our models are able to learn low-dimensional representations that arebetter suited for clustering, but also classification, outperformingSemi-Non-negative Matrix Factorization, but also other state-of-the-artmethodologies variants.
arxiv-12300-136 | Gibbs Sampling Strategies for Semantic Perception of Streaming Video Data | http://arxiv.org/pdf/1509.03242v1.pdf | author:Yogesh Girdhar, Gregory Dudek category:cs.RO cs.LG published:2015-09-10 summary:Topic modeling of streaming sensor data can be used for high level perceptionof the environment by a mobile robot. In this paper we compare various Gibbssampling strategies for topic modeling of streaming spatiotemporal data, suchas video captured by a mobile robot. Compared to previous work on online topicmodeling, such as o-LDA and incremental LDA, we show that the proposedtechnique results in lower online and final perplexity, given the realtimeconstraints.
arxiv-12300-137 | Scalable Kernel Methods via Doubly Stochastic Gradients | http://arxiv.org/pdf/1407.5599v4.pdf | author:Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, Le Song category:cs.LG stat.ML published:2014-07-21 summary:The general perception is that kernel methods are not scalable, and neuralnets are the methods of choice for nonlinear learning problems. Or have wesimply not tried hard enough for kernel methods? Here we propose an approachthat scales up kernel methods using a novel concept called "doubly stochasticfunctional gradients". Our approach relies on the fact that many kernel methodscan be expressed as convex optimization problems, and we solve the problems bymaking two unbiased stochastic approximations to the functional gradient, oneusing random training points and another using random functions associated withthe kernel, and then descending using this noisy functional gradient. We showthat a function produced by this procedure after $t$ iterations converges tothe optimal function in the reproducing kernel Hilbert space in rate $O(1/t)$,and achieves a generalization performance of $O(1/\sqrt{t})$. This doublystochasticity also allows us to avoid keeping the support vectors and toimplement the algorithm in a small memory footprint, which is linear in numberof iterations and independent of data dimension. Our approach can readily scalekernel methods up to the regimes which are dominated by neural nets. We showthat our method can achieve competitive performance to neural nets in datasetssuch as 8 million handwritten digits from MNIST, 2.3 million energy materialsfrom MolecularSpace, and 1 million photos from ImageNet.
arxiv-12300-138 | Learning to Linearize Under Uncertainty | http://arxiv.org/pdf/1506.03011v2.pdf | author:Ross Goroshin, Michael Mathieu, Yann LeCun category:cs.CV published:2015-06-09 summary:Training deep feature hierarchies to solve supervised learning tasks hasachieved state of the art performance on many problems in computer vision.However, a principled way in which to train such hierarchies in theunsupervised setting has remained elusive. In this work we suggest a newarchitecture and loss for training deep feature hierarchies that linearize thetransformations observed in unlabeled natural video sequences. This is done bytraining a generative model to predict video frames. We also address theproblem of inherent uncertainty in prediction by introducing latent variablesthat are non-deterministic functions of the input into the networkarchitecture.
arxiv-12300-139 | Use it or Lose it: Selective Memory and Forgetting in a Perpetual Learning Machine | http://arxiv.org/pdf/1509.03185v1.pdf | author:Andrew J. R. Simpson category:cs.LG 68Txx published:2015-09-10 summary:In a recent article we described a new type of deep neural network - aPerpetual Learning Machine (PLM) - which is capable of learning 'on the fly'like a brain by existing in a state of Perpetual Stochastic Gradient Descent(PSGD). Here, by simulating the process of practice, we demonstrate bothselective memory and selective forgetting when we introduce statistical recallbiases during PSGD. Frequently recalled memories are remembered, whilstmemories recalled rarely are forgotten. This results in a 'use it or lose it'stimulus driven memory process that is similar to human memory.
arxiv-12300-140 | STC: A Simple to Complex Framework for Weakly-supervised Semantic Segmentation | http://arxiv.org/pdf/1509.03150v1.pdf | author:Yunchao Wei, Xiaodan Liang, Yunpeng Chen, Xiaohui Shen, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan category:cs.CV published:2015-09-10 summary:Recently, significant improvement has been made on semantic objectsegmentation due to the development of deep convolutional neural networks(DCNNs). Training such a DCNN usually relies on a large number of images withpixel-level segmentation masks, and annotating these images is very costly interms of both finance and human effort. In this paper, we propose a simple tocomplex (STC) framework in which only image-level annotations are utilized tolearn DCNNs for semantic segmentation. Specifically, we first train an initialsegmentation network called Initial-DCNN with the saliency maps of simpleimages (i.e., those with a single category of major object(s) and cleanbackground). These saliency maps can be automatically obtained by existingbottom-up salient object detection techniques, where no supervision informationis needed. Then, a better network called Enhanced-DCNN is learned withsupervision from the predicted segmentation masks of simple images based on theInitial-DCNN as well as the image-level annotations. Finally, more pixel-levelsegmentation masks of complex images (two or more categories of objects withcluttered background), which are inferred by using Enhanced-DCNN andimage-level annotations, are utilized as the supervision information to learnthe Powerful-DCNN for semantic segmentation. Our method utilizes $40$K simpleimages from Flickr.com and 10K complex images from PASCAL VOC for step-wiselyboosting the segmentation network. Extensive experimental results on PASCAL VOC2012 segmentation benchmark demonstrate that the proposed STC frameworkoutperforms the state-of-the-art algorithms for weakly-supervised semanticsegmentation by a large margin (e.g., 10.6% over MIL-ILP-seg [1]).
arxiv-12300-141 | OneMax in Black-Box Models with Several Restrictions | http://arxiv.org/pdf/1504.02644v2.pdf | author:Carola Doerr, Johannes Lengler category:cs.NE cs.DS published:2015-04-10 summary:Black-box complexity studies lower bounds for the efficiency ofgeneral-purpose black-box optimization algorithms such as evolutionaryalgorithms and other search heuristics. Different models exist, each one beingdesigned to analyze a different aspect of typical heuristics such as the memorysize or the variation operators in use. While most of the previous works focuson one particular such aspect, we consider in this work how the combination ofseveral algorithmic restrictions influence the black-box complexity. Ourtestbed are so-called OneMax functions, a classical set of test functions thatis intimately related to classic coin-weighing problems and to the board gameMastermind. We analyze in particular the combined memory-restricted ranking-basedblack-box complexity of OneMax for different memory sizes. While its isolatedmemory-restricted as well as its ranking-based black-box complexity for bitstrings of length $n$ is only of order $n/\log n$, the combined model does notallow for algorithms being faster than linear in $n$, as can be seen bystandard information-theoretic considerations. We show that this linear boundis indeed asymptotically tight. Similar results are obtained for other memory-and offspring-sizes. Our results also apply to the (Monte Carlo) complexity ofOneMax in the recently introduced elitist model, in which only the best-so-farsolution can be kept in the memory. Finally, we also provide improved lowerbounds for the complexity of OneMax in the regarded models. Our result enlivens the quest for natural evolutionary algorithms optimizingOneMax in $o(n \log n)$ iterations.
arxiv-12300-142 | Nonparametric regression using needlet kernels for spherical data | http://arxiv.org/pdf/1502.04168v2.pdf | author:Shaobo Lin category:cs.LG stat.ML 68T05, 62J02 F.2.2 published:2015-02-14 summary:Needlets have been recognized as state-of-the-art tools to tackle sphericaldata, due to their excellent localization properties in both spacial andfrequency domains. This paper considers developing kernel methods associated with the needletkernel for nonparametric regression problems whose predictor variables aredefined on a sphere. Due to the localization property in the frequency domain,we prove that the regularization parameter of the kernel ridge regressionassociated with the needlet kernel can decrease arbitrarily fast. A naturalconsequence is that the regularization term for the kernel ridge regression isnot necessary in the sense of rate optimality. Based on the excellentlocalization property in the spacial domain further, we also prove that all the$l^{q}$ $(01\leq q < \infty)$ kernel regularization estimates associated withthe needlet kernel, including the kernel lasso estimate and the kernel bridgeestimate, possess almost the same generalization capability for a large rangeof regularization parameters in the sense of rate optimality. This finding tentatively reveals that, if the needlet kernel is utilized,then the choice of $q$ might not have a strong impact in terms of thegeneralization capability in some modeling contexts. From this perspective, $q$can be arbitrarily specified, or specified merely by other no generalizationcriteria like smoothness, computational complexity, sparsity, etc..
arxiv-12300-143 | Integrate Document Ranking Information into Confidence Measure Calculation for Spoken Term Detection | http://arxiv.org/pdf/1509.01899v2.pdf | author:Quan Liu, Wu Guo, Zhen-Hua Ling category:cs.CL published:2015-09-07 summary:This paper proposes an algorithm to improve the calculation of confidencemeasure for spoken term detection (STD). Given an input query term, thealgorithm first calculates a measurement named document ranking weight for eachdocument in the speech database to reflect its relevance with the query term bysumming all the confidence measures of the hypothesized term occurrences inthis document. The confidence measure of each term occurrence is thenre-estimated through linear interpolation with the calculated document rankingweight to improve its reliability by integrating document-level information.Experiments are conducted on three standard STD tasks for Tamil, Vietnamese andEnglish respectively. The experimental results all demonstrate that theproposed algorithm achieves consistent improvements over the state-of-the-artmethod for confidence measure calculation. Furthermore, this algorithm is stilleffective even if a high accuracy speech recognizer is not available, whichmakes it applicable for the languages with limited speech resources.
arxiv-12300-144 | Weighted Classification Cascades for Optimizing Discovery Significance in the HiggsML Challenge | http://arxiv.org/pdf/1409.2655v5.pdf | author:Lester Mackey, Jordan Bryan, Man Yue Mo category:stat.ML cs.LG published:2014-09-09 summary:We introduce a minorization-maximization approach to optimizing commonmeasures of discovery significance in high energy physics. The approachalternates between solving a weighted binary classification problem andupdating class weights in a simple, closed-form manner. Moreover, an argumentbased on convex duality shows that an improvement in weighted classificationerror on any round yields a commensurate improvement in discovery significance.We complement our derivation with experimental results from the 2014 Higgsboson machine learning challenge.
arxiv-12300-145 | Identifying Emotion from Natural Walking | http://arxiv.org/pdf/1508.00413v2.pdf | author:Liqing Cui, Shun Li, Wan Zhang, Zhan Zhang, Tingshao Zhu category:cs.CV cs.HC published:2015-08-03 summary:Emotion identification from gait aims to automatically determine personsaffective state, it has attracted a great deal of interests and offered immensepotential value in action tendency, health care, psychological detection andhuman-computer(robot) interaction.In this paper, we propose a new method ofidentifying emotion from natural walking, and analyze the relevance between thetraits of walking and affective states. After obtaining the pure accelerationdata of wrist and ankle, we set a moving average filter window with differentsizes w, then extract 114 features including time-domain, frequency-domain,power and distribution features from each data slice, and run principalcomponent analysis (PCA) to reduce dimension. In experiments, we train SVM,Decision Tree, multilayerperception, Random Tree and Random Forestclassification models, and compare the classification accuracy on data of wristand ankle with respect to different w. The performance of emotionidentification on acceleration data of ankle is better than wrist.Comparingdifferent classification models' results, SVM has best accuracy of identifyinganger and happy could achieve 90:31% and 89:76% respectively, andidentification ratio of anger-happy is 87:10%.The anger-neutral-happyclassification reaches 85%-78%-78%.The results show that it is capable ofidentifying personal emotional states through the gait of walking.
arxiv-12300-146 | Making Risk Minimization Tolerant to Label Noise | http://arxiv.org/pdf/1403.3610v2.pdf | author:Aritra Ghosh, Naresh Manwani, P. S. Sastry category:cs.LG published:2014-03-14 summary:In many applications, the training data, from which one needs to learn aclassifier, is corrupted with label noise. Many standard algorithms such as SVMperform poorly in presence of label noise. In this paper we investigate therobustness of risk minimization to label noise. We prove a sufficient conditionon a loss function for the risk minimization under that loss to be tolerant touniform label noise. We show that the $0-1$ loss, sigmoid loss, ramp loss andprobit loss satisfy this condition though none of the standard convex lossfunctions satisfy it. We also prove that, by choosing a sufficiently largevalue of a parameter in the loss function, the sigmoid loss, ramp loss andprobit loss can be made tolerant to non-uniform label noise also if we canassume the classes to be separable under noise-free data distribution. Throughextensive empirical studies, we show that risk minimization under the $0-1$loss, the sigmoid loss and the ramp loss has much better robustness to labelnoise when compared to the SVM algorithm.
arxiv-12300-147 | Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees | http://arxiv.org/pdf/1509.03025v1.pdf | author:Yudong Chen, Martin J. Wainwright category:math.ST cs.LG stat.ML stat.TH published:2015-09-10 summary:Optimization problems with rank constraints arise in many applications,including matrix regression, structured PCA, matrix completion and matrixdecomposition problems. An attractive heuristic for solving such problems is tofactorize the low-rank matrix, and to run projected gradient descent on thenonconvex factorized optimization problem. The goal of this problem is toprovide a general theoretical framework for understanding when such methodswork well, and to characterize the nature of the resulting fixed point. Weprovide a simple set of conditions under which projected gradient descent, whengiven a suitable initialization, converges geometrically to a statisticallyuseful solution. Our results are applicable even when the initial solution isoutside any region of local convexity, and even when the problem is globallyconcave. Working in a non-asymptotic framework, we show that our conditions aresatisfied for a wide range of concrete models, including matrix regression,structured PCA, matrix completion with real and quantized observations, matrixdecomposition, and graph clustering problems. Simulation results show excellentagreement with the theoretical predictions.
arxiv-12300-148 | Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies | http://arxiv.org/pdf/1509.03005v1.pdf | author:David Balduzzi, Muhammad Ghifary category:cs.LG cs.AI cs.NE stat.ML published:2015-09-10 summary:This paper proposes GProp, a deep reinforcement learning algorithm forcontinuous policies with compatible function approximation. The algorithm isbased on two innovations. Firstly, we present a temporal-difference basedmethod for learning the gradient of the value-function. Secondly, we presentthe deviator-actor-critic (DAC) model, which comprises three neural networksthat estimate the value function, its gradient, and determine the actor'spolicy respectively. We evaluate GProp on two challenging tasks: a contextualbandit problem constructed from nonparametric regression datasets that isdesigned to probe the ability of reinforcement learning algorithms toaccurately estimate gradients; and the octopus arm, a challenging reinforcementlearning benchmark. GProp is competitive with fully supervised methods on thebandit task and achieves the best performance to date on the octopus arm.
arxiv-12300-149 | Knowlege Graph Embedding by Flexible Translation | http://arxiv.org/pdf/1505.05253v2.pdf | author:Jun Feng, Mantong Zhou, Yu Hao, Minlie Huang, Xiaoyan Zhu category:cs.CL published:2015-05-20 summary:Knowledge graph embedding refers to projecting entities and relations inknowledge graph into continuous vector spaces. State-of-the-art methods, suchas TransE, TransH, and TransR build embeddings by treating relation astranslation from head entity to tail entity. However, previous models can notdeal with reflexive/one-to-many/many-to-one/many-to-many relations properly, orlack of scalability and efficiency. Thus, we propose a novel method, flexibletranslation, named TransF, to address the above issues. TransF regards relationas translation between head entity vector and tail entity vector with flexiblemagnitude. To evaluate the proposed model, we conduct link prediction andtriple classification on benchmark datasets. Experimental results show that ourmethod remarkably improve the performance compared with severalstate-of-the-art baselines.
arxiv-12300-150 | Planar Ultrametric Rounding for Image Segmentation | http://arxiv.org/pdf/1507.02407v3.pdf | author:Julian Yarkony, Charless C. Fowlkes category:cs.DS cs.CG cs.CV 68T45 published:2015-07-09 summary:We study the problem of hierarchical clustering on planar graphs. Weformulate this in terms of an LP relaxation of ultrametric rounding. To solvethis LP efficiently we introduce a dual cutting plane scheme that uses minimumcost perfect matching as a subroutine in order to efficiently explore the spaceof planar partitions. We apply our algorithm to the problem of hierarchicalimage segmentation.
arxiv-12300-151 | Clustering Tree-structured Data on Manifold | http://arxiv.org/pdf/1507.05532v2.pdf | author:Na Lu, Hongyu Miao category:cs.CV cs.LG 68T10, 62H30 published:2015-07-20 summary:Tree-structured data usually contain both topological and geometricalinformation, and are necessarily considered on manifold instead of Euclideanspace for appropriate data parameterization and analysis. In this study, wepropose a novel tree-structured data parameterization, calledTopology-Attribute matrix (T-A matrix), so the data clustering task can beconducted on matrix manifold. We incorporate the structure constraints embeddedin data into the negative matrix factorization method to determine meta-treesfrom the T-A matrix, and the signature vector of each single tree can then beextracted by meta-tree decomposition. The meta-tree space turns out to be acone space, in which we explore the distance metric and implement theclustering algorithm based on the concepts like Fr\'echet mean. Finally, theT-A matrix based clustering (TAMBAC) framework is evaluated and compared usingboth simulated data and real retinal images to illustrate its efficiency andaccuracy.
arxiv-12300-152 | Optimizing Static and Adaptive Probing Schedules for Rapid Event Detection | http://arxiv.org/pdf/1509.02487v2.pdf | author:Ahmad Mahmoody, Evgenios M. Kornaropoulos, Eli Upfal category:cs.DS cs.LG published:2015-09-08 summary:We formulate and study a fundamental search and detection problem, ScheduleOptimization, motivated by a variety of real-world applications, ranging frommonitoring content changes on the web, social networks, and user activities todetecting failure on large systems with many individual machines. We consider a large system consists of many nodes, where each node has itsown rate of generating new events, or items. A monitoring application can probea small number of nodes at each step, and our goal is to compute a probingschedule that minimizes the expected number of undiscovered items at thesystem, or equivalently, minimizes the expected time to discover a new item inthe system. We study the Schedule Optimization problem both for deterministic andrandomized memoryless algorithms. We provide lower bounds on the cost of anoptimal schedule and construct close to optimal schedules with rigorousmathematical guarantees. Finally, we present an adaptive algorithm that startswith no prior information on the system and converges to the optimal memorylessalgorithms by adapting to observed data.
arxiv-12300-153 | Proposal-free Network for Instance-level Object Segmentation | http://arxiv.org/pdf/1509.02636v2.pdf | author:Xiaodan Liang, Yunchao Wei, Xiaohui Shen, Jianchao Yang, Liang Lin, Shuicheng Yan category:cs.CV published:2015-09-09 summary:Instance-level object segmentation is an important yet under-explored task.The few existing studies are almost all based on region proposal methods toextract candidate segments and then utilize object classification to producefinal results. Nonetheless, generating accurate region proposals itself isquite challenging. In this work, we propose a Proposal-Free Network (PFN ) toaddress the instance-level object segmentation problem, which outputs theinstance numbers of different categories and the pixel-level information on 1)the coordinates of the instance bounding box each pixel belongs to, and 2) theconfidences of different categories for each pixel, based on pixel-to-pixeldeep convolutional neural network. All the outputs together, by using anyoff-the-shelf clustering method for simple post-processing, can naturallygenerate the ultimate instance-level object segmentation results. The whole PFNcan be easily trained in an end-to-end way without the requirement of aproposal generation stage. Extensive evaluations on the challenging PASCAL VOC2012 semantic segmentation benchmark demonstrate that the proposed PFN solutionwell beats the state-of-the-arts for instance-level object segmentation. Inparticular, the $AP^r$ over 20 classes at 0.5 IoU reaches 58.7% by PFN,significantly higher than 43.8% and 46.3% by the state-of-the-art algorithms,SDS [9] and [16], respectively.
arxiv-12300-154 | Dictionary Learning and Sparse Coding for Third-order Super-symmetric Tensors | http://arxiv.org/pdf/1509.02970v1.pdf | author:Piotr Koniusz, Anoop Cherian category:cs.CV published:2015-09-09 summary:Super-symmetric tensors - a higher-order extension of scatter matrices - arebecoming increasingly popular in machine learning and computer vision formodelling data statistics, co-occurrences, or even as visual descriptors.However, the size of these tensors are exponential in the data dimensionality,which is a significant concern. In this paper, we study third-ordersuper-symmetric tensor descriptors in the context of dictionary learning andsparse coding. Our goal is to approximate these tensors as sparse coniccombinations of atoms from a learned dictionary, where each atom is a symmetricpositive semi-definite matrix. Apart from the significant benefits to tensorcompression that this framework provides, our experiments demonstrate that thesparse coefficients produced by the scheme lead to better aggregation ofhigh-dimensional data, and showcases superior performance on two commoncomputer vision tasks compared to the state-of-the-art.
arxiv-12300-155 | Coarse-to-Fine Sequential Monte Carlo for Probabilistic Programs | http://arxiv.org/pdf/1509.02962v1.pdf | author:Andreas Stuhlmüller, Robert X. D. Hawkins, N. Siddharth, Noah D. Goodman category:cs.AI stat.ML published:2015-09-09 summary:Many practical techniques for probabilistic inference require a sequence ofdistributions that interpolate between a tractable distribution and anintractable distribution of interest. Usually, the sequences used are simple,e.g., based on geometric averages between distributions. When models areexpressed as probabilistic programs, the models themselves are highlystructured objects that can be used to derive annealing sequences that are moresensitive to domain structure. We propose an algorithm for transformingprobabilistic programs to coarse-to-fine programs which have the same marginaldistribution as the original programs, but generate the data at increasinglevels of detail, from coarse to fine. We apply this algorithm to an Isingmodel, its depth-from-disparity variation, and a factorial hidden Markov model.We show preliminary evidence that the use of coarse-to-fine models can makeexisting generic inference algorithms more efficient.
arxiv-12300-156 | Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized Huber Loss and Quantile Regression | http://arxiv.org/pdf/1509.02957v1.pdf | author:Congrui Yi, Jian Huang category:stat.CO stat.ML published:2015-09-09 summary:We propose a semismooth Newton coordinate descent (SNCD) algorithm forelastic-net penalized robust regression with Huber loss and quantileregression. The SNCD is a novel combination of the semismooth Newton andcoordinate descent algorithms. It is designed for loss functions with onlyfirst order derivatives and is scalable to high-dimensional models. Unlike thestandard coordinate descent method, the SNCD updates the regression parametersand the corresponding subdifferentials based on the concept of Newtonderivatives. In addition, an adaptive version of the "strong rule" forscreening predictors is incorporated to gain extra efficiency. As an importantapplication of the proposed algorithm, we show that the SNCD can be used tocompute the solution paths for penalized quantile regression. We establish theconvergence properties of the algorithm. Through numerical experiments, wedemonstrate that the proposed algorithm works well for high-dimensional datawith heavy-tailed errors, and that for quantile regression SNCD is considerablyfaster than the existing method and has better optimization performance. Abreast cancer gene expression data set is used to illustrate the proposedalgorithm.
arxiv-12300-157 | Sensor Selection by Linear Programming | http://arxiv.org/pdf/1509.02954v1.pdf | author:Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama category:stat.ML cs.LG published:2015-09-09 summary:We learn sensor trees from training data to minimize sensor acquisition costsduring test time. Our system adaptively selects sensors at each stage ifnecessary to make a confident classification. We pose the problem as empiricalrisk minimization over the choice of trees and node decision rules. Wedecompose the problem, which is known to be intractable, into combinatorial(tree structures) and continuous parts (node decision rules) and propose tosolve them separately. Using training data we greedily solve for thecombinatorial tree structures and for the continuous part, which is anon-convex multilinear objective function, we derive convex surrogate lossfunctions that are piecewise linear. The resulting problem can be cast as alinear program and has the advantage of guaranteed convergence, globaloptimality, repeatability and computational efficiency. We show that ourproposed approach outperforms the state-of-art on a number of benchmarkdatasets.
arxiv-12300-158 | Sampling-based Approximations with Quantitative Performance for the Probabilistic Reach-Avoid Problem over General Markov Processes | http://arxiv.org/pdf/1409.0553v2.pdf | author:Sofie Haesaert, Robert Babuska, Alessandro Abate category:cs.SY cs.LG published:2014-09-01 summary:This article deals with stochastic processes endowed with the Markov(memoryless) property and evolving over general (uncountable) state spaces. Themodels further depend on a non-deterministic quantity in the form of a controlinput, which can be selected to affect the probabilistic dynamics. We addressthe computation of maximal reach-avoid specifications, together with thesynthesis of the corresponding optimal controllers. The reach-avoidspecification deals with assessing the likelihood that any finite-horizontrajectory of the model enters a given goal set, while avoiding a given set ofundesired states. This article newly provides an approximate computationalscheme for the reach-avoid specification based on the Fitted Value Iterationalgorithm, which hinges on random sample extractions, and gives a-prioricomputable formal probabilistic bounds on the error made by the approximationalgorithm: as such, the output of the numerical scheme is quantitativelyassessed and thus meaningful for safety-critical applications. Furthermore, weprovide tighter probabilistic error bounds that are sample-based. The overallcomputational scheme is put in relationship with alternative approximationalgorithms in the literature, and finally its performance is practicallyassessed over a benchmark case study.
arxiv-12300-159 | Sélection de variables par le GLM-Lasso pour la prédiction du risque palustre | http://arxiv.org/pdf/1509.02873v1.pdf | author:Bienvenue Kouwayè, Noël Fonton, Fabrice Rossi category:stat.ML published:2015-09-09 summary:In this study, we propose an automatic learning method for variablesselection based on Lasso in epidemiology context. One of the aim of thisapproach is to overcome the pretreatment of experts in medicine andepidemiology on collected data. These pretreatment consist in recoding somevariables and to choose some interactions based on expertise. The approachproposed uses all available explanatory variables without treatment andgenerate automatically all interactions between them. This lead to highdimension. We use Lasso, one of the robust methods of variable selection inhigh dimension. To avoid over fitting a two levels cross-validation is used.Because the target variable is account variable and the lasso estimators arebiased, variables selected by lasso are debiased by a GLM and used to predictthe distribution of the main vector of malaria which is Anopheles. Results showthat only few climatic and environmental variables are the mains factorsassociated to the malaria risk exposure.
arxiv-12300-160 | Fast Second-Order Stochastic Backpropagation for Variational Inference | http://arxiv.org/pdf/1509.02866v1.pdf | author:Kai Fan, Ziteng Wang, Jeff Beck, James Kwok, Katherine Heller category:stat.ML published:2015-09-09 summary:We propose a second-order (Hessian or Hessian-free) based optimization methodfor variational inference inspired by Gaussian backpropagation, and argue thatquasi-Newton optimization can be developed as well. This is accomplished bygeneralizing the gradient computation in stochastic backpropagation via areparametrization trick with lower complexity. As an illustrative example, weapply this approach to the problems of Bayesian logistic regression andvariational auto-encoder (VAE). Additionally, we compute bounds on theestimator variance of intractable expectations for the family of Lipschitzcontinuous function. Our method is practical, scalable and model free. Wedemonstrate our method on several real-world datasets and provide comparisonswith other stochastic gradient methods to show substantial enhancement inconvergence rates.
arxiv-12300-161 | Transfer learning approach for financial applications | http://arxiv.org/pdf/1509.02807v1.pdf | author:Cosmin Stamate, George D. Magoulas, Michael S. C. Thomas category:cs.NE published:2015-09-09 summary:Artificial neural networks learn how to solve new problems through acomputationally intense and time consuming process. One way to reduce theamount of time required is to inject preexisting knowledge into the network. Tomake use of past knowledge, we can take advantage of techniques that transferthe knowledge learned from one task, and reuse it on another (sometimesunrelated) task. In this paper we propose a novel selective breeding techniquethat extends the transfer learning with behavioural genetics approach proposedby Kohli, Magoulas and Thomas (2013), and evaluate its performance on financialdata. Numerical evidence demonstrates the credibility of the new approach. Weprovide insights on the operation of transfer learning and highlight thebenefits of using behavioural principles and selective breeding when tackling aset of diverse financial applications problems.
arxiv-12300-162 | Finite Dictionary Variants of the Diffusion KLMS Algorithm | http://arxiv.org/pdf/1509.02730v1.pdf | author:Rangeet Mitra, Vimal Bhatia category:cs.SY cs.DC cs.IT cs.LG math.IT published:2015-09-09 summary:The diffusion based distributed learning approaches have been found to be aviable solution for learning over linearly separable datasets over a network.However, approaches till date are suitable for linearly separable datasets andneed to be extended to scenarios in which we need to learn a non-linearity. Insuch scenarios, the recently proposed diffusion kernel least mean squares(KLMS) has been found to be performing better than diffusion least mean squares(LMS). The drawback of diffusion KLMS is that it requires infinite storage forobservations (also called dictionary). This paper formulates the diffusion KLMSin a fixed budget setting such that the storage requirement is curtailed whilemaintaining appreciable performance in terms of convergence. Simulations havebeen carried out to validate the two newly proposed algorithms named asquantised diffusion KLMS (QDKLMS) and fixed budget diffusion KLMS (FBDKLMS)against KLMS, which indicate that both the proposed algorithms deliver betterperformance as compared to the KLMS while reducing the dictionary size storagerequirement.
arxiv-12300-163 | Visual Understanding via Multi-Feature Shared Learning with Global Consistency | http://arxiv.org/pdf/1505.05233v2.pdf | author:Lei Zhang, David Zhang category:cs.CV cs.LG published:2015-05-20 summary:Image/video data is usually represented with multiple visual features. Fusionof multi-source information for establishing the attributes has been widelyrecognized. Multi-feature visual recognition has recently received muchattention in multimedia applications. This paper studies visual understandingvia a newly proposed l_2-norm based multi-feature shared learning framework,which can simultaneously learn a global label matrix and multiplesub-classifiers with the labeled multi-feature data. Additionally, a groupgraph manifold regularizer composed of the Laplacian and Hessian graph isproposed for better preserving the manifold structure of each feature, suchthat the label prediction power is much improved through the semi-supervisedlearning with global label consistency. For convenience, we call the proposedapproach Global-Label-Consistent Classifier (GLCC). The merits of the proposedmethod include: 1) the manifold structure information of each feature isexploited in learning, resulting in a more faithful classification owing to theglobal label consistency; 2) a group graph manifold regularizer based on theLaplacian and Hessian regularization is constructed; 3) an efficientalternative optimization method is introduced as a fast solver owing to theconvex sub-problems. Experiments on several benchmark visual datasets formultimedia understanding, such as the 17-category Oxford Flower dataset, thechallenging 101-category Caltech dataset, the YouTube & Consumer Videos datasetand the large-scale NUS-WIDE dataset, demonstrate that the proposed approachcompares favorably with the state-of-the-art algorithms. An extensiveexperiment on the deep convolutional activation features also show theeffectiveness of the proposed approach. The code is available onhttp://www.escience.cn/people/lei/index.html
arxiv-12300-164 | Shape Interaction Matrix Revisited and Robustified: Efficient Subspace Clustering with Corrupted and Incomplete Data | http://arxiv.org/pdf/1509.02649v1.pdf | author:Pan Ji, Mathieu Salzmann, Hongdong Li category:cs.CV published:2015-09-09 summary:The Shape Interaction Matrix (SIM) is one of the earliest approaches toperforming subspace clustering (i.e., separating points drawn from a union ofsubspaces). In this paper, we revisit the SIM and reveal its connections toseveral recent subspace clustering methods. Our analysis lets us derive asimple, yet effective algorithm to robustify the SIM and make it applicable torealistic scenarios where the data is corrupted by noise. We justify our methodby intuitive examples and the matrix perturbation theory. We then show how thisapproach can be extended to handle missing data, thus yielding an efficient andgeneral subspace clustering algorithm. We demonstrate the benefits of ourapproach over state-of-the-art subspace clustering methods on severalchallenging motion segmentation and face clustering problems, where the dataincludes corrupted and missing measurements.
arxiv-12300-165 | Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization | http://arxiv.org/pdf/1409.3257v2.pdf | author:Yuchen Zhang, Lin Xiao category:math.OC stat.ML published:2014-09-10 summary:We consider a generic convex optimization problem associated with regularizedempirical risk minimization of linear predictors. The problem structure allowsus to reformulate it as a convex-concave saddle point problem. We propose astochastic primal-dual coordinate (SPDC) method, which alternates betweenmaximizing over a randomly chosen dual variable and minimizing over the primalvariable. An extrapolation step on the primal variable is performed to obtainaccelerated convergence rate. We also develop a mini-batch version of the SPDCmethod which facilitates parallel computing, and an extension with weightedsampling probabilities on the dual variables, which has a better complexitythan uniform sampling on unnormalized data. Both theoretically and empirically,we show that the SPDC method has comparable or better performance than severalstate-of-the-art optimization methods.
arxiv-12300-166 | Efficient Semidefinite Branch-and-Cut for MAP-MRF Inference | http://arxiv.org/pdf/1404.5009v4.pdf | author:Peng Wang, Chunhua Shen, Anton van den Hengel, Philip Torr category:cs.CV cs.LG cs.NA published:2014-04-20 summary:We propose a Branch-and-Cut (B&C) method for solving general MAP-MRFinference problems. The core of our method is a very efficient boundingprocedure, which combines scalable semidefinite programming (SDP) and acutting-plane method for seeking violated constraints. In order to furtherspeed up the computation, several strategies have been exploited, includingmodel reduction, warm start and removal of inactive constraints. We analyze the performance of the proposed method under different settings,and demonstrate that our method either outperforms or performs on par withstate-of-the-art approaches. Especially when the connectivities are dense orwhen the relative magnitudes of the unary costs are low, we achieve the bestreported results. Experiments show that the proposed algorithm achieves betterapproximation than the state-of-the-art methods within a variety of timebudgets on challenging non-submodular MAP-MRF inference problems.
arxiv-12300-167 | Asynchronous Distributed ADMM for Large-Scale Optimization- Part II: Linear Convergence Analysis and Numerical Performance | http://arxiv.org/pdf/1509.02604v1.pdf | author:Tsung-Hui Chang, Wei-Cheng Liao, Mingyi Hong, Xiangfeng Wang category:cs.DC cs.LG cs.SY published:2015-09-09 summary:The alternating direction method of multipliers (ADMM) has been recognized asa versatile approach for solving modern large-scale machine learning and signalprocessing problems efficiently. When the data size and/or the problemdimension is large, a distributed version of ADMM can be used, which is capableof distributing the computation load and the data set to a network of computingnodes. Unfortunately, a direct synchronous implementation of such algorithmdoes not scale well with the problem size, as the algorithm speed is limited bythe slowest computing nodes. To address this issue, in a companion paper, wehave proposed an asynchronous distributed ADMM (AD-ADMM) and studied itsworst-case convergence conditions. In this paper, we further the study bycharacterizing the conditions under which the AD-ADMM achieves linearconvergence. Our conditions as well as the resulting linear rates reveal theimpact that various algorithm parameters, network delay and network size haveon the algorithm performance. To demonstrate the superior time efficiency ofthe proposed AD-ADMM, we test the AD-ADMM on a high-performance computercluster by solving a large-scale logistic regression problem.
arxiv-12300-168 | A nonlinear aggregation type classifier | http://arxiv.org/pdf/1509.01604v2.pdf | author:Alejandro Cholaquidis, Ricardo Fraiman, Juan Kalemkerian, Pamela Llop category:math.ST stat.ML stat.TH published:2015-09-04 summary:We introduce a nonlinear aggregation type classifier for functional datadefined on a separable and complete metric space. The new rule is built up froma collection of $M$ arbitrary training classifiers. If the classifiers areconsistent, then so is the aggregation rule. Moreover, asymptotically theaggregation rule behaves as well as the best of the $M$ classifiers. Theresults of a small simulation are reported both, for high dimensional andfunctional data, and a real data example is analyzed.
arxiv-12300-169 | Network driven sampling; a critical threshold for design effects | http://arxiv.org/pdf/1505.05461v4.pdf | author:Karl Rohe category:math.ST stat.ME stat.ML stat.TH published:2015-05-20 summary:Web crawling, snowball sampling, and respondent-driven sampling (RDS) arethree types of network driven sampling techniques that are popular when it isdifficult to contact individuals in the population of interest. This paperstudies network driven sampling as a Markov process on the social network thatis indexed by a tree. Each node in this tree corresponds to an observation andeach edge in the tree corresponds to a referral. Indexing with a tree, insteadof a chain, allows for the sampled units to refer multiple future units intothe sample. In survey sampling, the design effect characterizes the additional varianceinduced by a novel sampling strategy. If the design effect is DE, thenconstructing an estimator from the novel design makes the variance of theestimator DE times greater than it would be under a simple random sample. Undercertain assumptions on the referral tree, the design effect of network drivensampling has a critical threshold that is a function of the referral rate $m$and the clustering structure in the social network, represented by the secondeigenvalue of the Markov transition matrix, $\lambda_2$. If $m <1/\lambda_2^2$, then the design effect is finite (i.e. the standard estimatoris $\sqrt{n}$-consistent). However, if $m > 1/\lambda_2^2$, then the designeffect grows with $n$ (i.e. the standard estimator is no longer$\sqrt{n}$-consistent). Past the critical threshold, the estimator converges atthe slower rate of $\log_m \lambda_2$. The Markov model allows for nodes to beresampled. Under certain conditions, the rate of resampling is not affected bythe critical threshold, so long as $n = o(\sqrt{N})$, where $n$ is the samplesize and $N$ is the population size.
arxiv-12300-170 | Nonlinear functional mapping of the human brain | http://arxiv.org/pdf/1510.03765v1.pdf | author:Nicholas Allgaier, Tobias Banaschewski, Gareth Barker, Arun L. W. Bokde, Josh C. Bongard, Uli Bromberg, Christian Büchel, Anna Cattrell, Patricia J. Conrod, Christopher M. Danforth, Sylvane Desrivières, Peter S. Dodds, Herta Flor, Vincent Frouin, Jürgen Gallinat, Penny Gowland, Andreas Heinz, Bernd Ittermann, Scott Mackey, Jean-Luc Martinot, Kevin Murphy, Frauke Nees, Dimitri Papadopoulos-Orfanos, Luise Poustka, Michael N. Smolka, Henrik Walter, Robert Whelan, Gunter Schumann, Hugh Garavan, IMAGEN Consortium category:q-bio.NC cs.NE published:2015-09-08 summary:The field of neuroimaging has truly become data rich, and novel analyticalmethods capable of gleaning meaningful information from large stores of imagingdata are in high demand. Those methods that might also be applicable on thelevel of individual subjects, and thus potentially useful clinically, are ofspecial interest. In the present study, we introduce just such a method, callednonlinear functional mapping (NFM), and demonstrate its application in theanalysis of resting state fMRI from a 242-subject subset of the IMAGEN project,a European study of adolescents that includes longitudinal phenotypic,behavioral, genetic, and neuroimaging data. NFM employs a computationaltechnique inspired by biological evolution to discover and mathematicallycharacterize interactions among ROI (regions of interest), without makinglinear or univariate assumptions. We show that statistics of the resultinginteraction relationships comport with recent independent work, constituting apreliminary cross-validation. Furthermore, nonlinear terms are ubiquitous inthe models generated by NFM, suggesting that some of the interactionscharacterized here are not discoverable by standard linear methods of analysis.We discuss one such nonlinear interaction in the context of a direct comparisonwith a procedure involving pairwise correlation, designed to be an analogouslinear version of functional mapping. We find another such interaction thatsuggests a novel distinction in brain function between drinking andnon-drinking adolescents: a tighter coupling of ROI associated with emotion,reward, and interoceptive processes such as thirst, among drinkers. Finally, weoutline many improvements and extensions of the methodology to reducecomputational expense, complement other analytical tools like graph-theoreticanalysis, and allow for voxel level NFM to eliminate the necessity of ROIselection.
arxiv-12300-171 | DeepCough: A Deep Convolutional Neural Network in A Wearable Cough Detection System | http://arxiv.org/pdf/1509.02512v1.pdf | author:Justice Amoh, Kofi Odame category:cs.NE cs.LG published:2015-09-08 summary:In this paper, we present a system that employs a wearable acoustic sensorand a deep convolutional neural network for detecting coughs. We evaluate theperformance of our system on 14 healthy volunteers and compare it to that ofother cough detection systems that have been reported in the literature.Experimental results show that our system achieves a classification sensitivityof 95.1% and a specificity of 99.5%.
arxiv-12300-172 | Nucleosome positioning: resources and tools online | http://arxiv.org/pdf/1508.06916v4.pdf | author:Vladimir B. Teif category:q-bio.GN physics.bio-ph q-bio.BM stat.ML published:2015-08-27 summary:Nucleosome positioning is an important process required for proper genomepacking and its accessibility to execute the genetic program in acell-specific, timely manner. In the recent years hundreds of papers have beendevoted to the bioinformatics, physics and biology of nucleosome positioning.The purpose of this review is to cover a practical aspect of this field, namelyto provide a guide to the multitude of nucleosome positioning resourcesavailable online. These include almost 300 experimental datasets of genome-widenucleosome occupancy profiles determined in different cell types and more than40 computational tools for the analysis of experimental nucleosome positioningdata and prediction of intrinsic nucleosome formation probabilities from theDNA sequence. A manually curated, up to date list of these resources will bemaintained at http://generegulation.info.
arxiv-12300-173 | Designing A Composite Dictionary Adaptively From Joint Examples | http://arxiv.org/pdf/1503.03621v2.pdf | author:Zhangyang Wang, Yingzhen Yang, Jianchao Yang, Thomas S. Huang category:cs.CV published:2015-03-12 summary:We study the complementary behaviors of external and internal examples inimage restoration, and are motivated to formulate a composite dictionary designframework. The composite dictionary consists of the global part learned fromexternal examples, and the sample-specific part learned from internal examples.The dictionary atoms in both parts are further adaptively weighted to emphasizetheir model statistics. Experiments demonstrate that the joint utilization ofexternal and internal examples leads to substantial improvements, withsuccessful applications in image denoising and super resolution.
arxiv-12300-174 | Unsupervised Learning of Spatiotemporally Coherent Metrics | http://arxiv.org/pdf/1412.6056v6.pdf | author:Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun category:cs.CV published:2014-12-18 summary:Current state-of-the-art classification and detection algorithms rely onsupervised training. In this work we study unsupervised feature learning in thecontext of temporally coherent video data. We focus on feature learning fromunlabeled video data, using the assumption that adjacent video frames containsemantically similar information. This assumption is exploited to train aconvolutional pooling auto-encoder regularized by slowness and sparsity. Weestablish a connection between slow feature learning to metric learning andshow that the trained encoder can be used to define a more temporally andsemantically coherent metric.
arxiv-12300-175 | Edge-enhancing Filters with Negative Weights | http://arxiv.org/pdf/1509.02491v1.pdf | author:Andrew Knyazev category:cs.CV cs.IT math.CO math.IT 68U10, 05C85 published:2015-09-08 summary:In [DOI:10.1109/ICMEW.2014.6890711], a graph-based denoising is performed byprojecting the noisy image to a lower dimensional Krylov subspace of the graphLaplacian, constructed using nonnegative weights determined by distancesbetween image data corresponding to image pixels. We~extend the construction ofthe graph Laplacian to the case, where some graph weights can be negative.Removing the positivity constraint provides a more accurate inference of agraph model behind the data, and thus can improve quality of filters forgraph-based signal processing, e.g., denoising, compared to the standardconstruction, without affecting the costs.
arxiv-12300-176 | Deep Attributes from Context-Aware Regional Neural Codes | http://arxiv.org/pdf/1509.02470v1.pdf | author:Jianwei Luo, Jianguo Li, Jun Wang, Zhiguo Jiang, Yurong Chen category:cs.CV cs.LG cs.NE published:2015-09-08 summary:Recently, many researches employ middle-layer output of convolutional neuralnetwork models (CNN) as features for different visual recognition tasks.Although promising results have been achieved in some empirical studies, suchtype of representations still suffer from the well-known issue of semantic gap.This paper proposes so-called deep attribute framework to alleviate this issuefrom three aspects. First, we introduce object region proposals as intermediato represent target images, and extract features from region proposals. Second,we study aggregating features from different CNN layers for all regionproposals. The aggregation yields a holistic yet compact representation ofinput images. Results show that cross-region max-pooling of soft-max layeroutput outperform all other layers. As soft-max layer directly corresponds tosemantic concepts, this representation is named "deep attributes". Third, weobserve that only a small portion of generated regions by object proposalsalgorithm are correlated to classification target. Therefore, we introducecontext-aware region refining algorithm to pick out contextual regions andbuild context-aware classifiers. We apply the proposed deep attributes framework for various vision tasks.Extensive experiments are conducted on standard benchmarks for three visualrecognition tasks, i.e., image classification, fine-grained recognition andvisual instance retrieval. Results show that deep attribute approaches achievestate-of-the-art results, and outperforms existing peer methods with asignificant margin, even though some benchmarks have little overlap of conceptswith the pre-trained CNN models.
arxiv-12300-177 | Accelerated graph-based spectral polynomial filters | http://arxiv.org/pdf/1509.02468v1.pdf | author:Andrew Knyazev, Alexander Malyshev category:cs.CV published:2015-09-08 summary:Graph-based spectral denoising is a low-pass filtering using theeigendecomposition of the graph Laplacian matrix of a noisy signal. Polynomialfiltering avoids costly computation of the eigendecomposition by projectionsonto suitable Krylov subspaces. Polynomial filters can be based, e.g., on thebilateral and guided filters. We propose constructing accelerated polynomialfilters by running flexible Krylov subspace based linear and eigenvalue solverssuch as the Block Locally Optimal Preconditioned Conjugate Gradient (LOBPCG)method.
arxiv-12300-178 | Evolving TSP heuristics using Multi Expression Programming | http://arxiv.org/pdf/1509.02459v1.pdf | author:Mihai Oltean, D. Dumitrescu category:cs.AI cs.NE published:2015-09-08 summary:Multi Expression Programming (MEP) is an evolutionary technique that may beused for solving computationally difficult problems. MEP uses a linear solutionrepresentation. Each MEP individual is a string encoding complex expressions(computer programs). A MEP individual may encode multiple solutions of thecurrent problem. In this paper MEP is used for evolving a Traveling SalesmanProblem (TSP) heuristic for graphs satisfying triangle inequality. Evolved MEPheuristic is compared with Nearest Neighbor Heuristic (NN) and Minimum SpanningTree Heuristic (MST) on some difficult problems in TSPLIB. For most of theconsidered problems the evolved MEP heuristic outperforms NN and MST. Theobtained algorithm was tested against some problems in TSPLIB. The resultsemphasizes that evolved MEP heuristic is a powerful tool for solving difficultTSP instances.
arxiv-12300-179 | A Behavior Analysis-Based Game Bot Detection Approach Considering Various Play Styles | http://arxiv.org/pdf/1509.02458v1.pdf | author:Yeounoh Chung, Chang-yong Park, Noo-ri Kim, Hana Cho, Taebok Yoon, Hunjoo Lee, Jee-Hyong Lee category:cs.LG cs.AI published:2015-09-08 summary:An approach for game bot detection in MMORPGs is proposed based on theanalysis of game playing behavior. Since MMORPGs are large scale games, userscan play in various ways. This variety in playing behavior makes it hard todetect game bots based on play behaviors. In order to cope with this problem,the proposed approach observes game playing behaviors of users and groups themby their behavioral similarities. Then, it develops a local bot detection modelfor each player group. Since the locally optimized models can more accuratelydetect game bots within each player group, the combination of those modelsbrings about overall improvement. For a practical purpose of reducing theworkloads of the game servers in service, the game data is collected at a lowresolution in time. Behavioral features are selected and developed toaccurately detect game bots with the low resolution data, considering commonaspects of MMORPG playing. Through the experiment with the real data from agame currently in service, it is shown that the proposed local model approachyields more accurate results.
arxiv-12300-180 | A Variational Bayesian State-Space Approach to Online Passive-Aggressive Regression | http://arxiv.org/pdf/1509.02438v1.pdf | author:Arnold Salas, Stephen J. Roberts, Michael A. Osborne category:stat.ML published:2015-09-08 summary:Online Passive-Aggressive (PA) learning is a class of online margin-basedalgorithms suitable for a wide range of real-time prediction tasks, includingclassification and regression. PA algorithms are formulated in terms ofdeterministic point-estimation problems governed by a set of user-definedhyperparameters: the approach fails to capture model/prediction uncertainty andmakes their performance highly sensitive to hyperparameter configurations. Inthis paper, we introduce a novel PA learning framework for regression thatovercomes the above limitations. We contribute a Bayesian state-spaceinterpretation of PA regression, along with a novel online variationalinference scheme, that not only produces probabilistic predictions, but alsooffers the benefit of automatic hyperparameter tuning. Experiments with variousreal-world data sets show that our approach performs significantly better thana more standard, linear Gaussian state-space model.
arxiv-12300-181 | Improved Twitter Sentiment Prediction through Cluster-then-Predict Model | http://arxiv.org/pdf/1509.02437v1.pdf | author:Rishabh Soni, K. James Mathai category:cs.IR cs.CL cs.LG cs.SI published:2015-09-08 summary:Over the past decade humans have experienced exponential growth in the use ofonline resources, in particular social media and microblogging websites such asFacebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line,etc. Many companies have identified these resources as a rich mine of marketingknowledge. This knowledge provides valuable feedback which allows them tofurther develop the next generation of their product. In this paper, sentimentanalysis of a product is performed by extracting tweets about that product andclassifying the tweets showing it as positive and negative sentiment. Theauthors propose a hybrid approach which combines unsupervised learning in theform of K-means clustering to cluster the tweets and then performing supervisedlearning methods such as Decision Trees and Support Vector Machines forclassification.
arxiv-12300-182 | Central Pattern Generators for the control of robotic systems | http://arxiv.org/pdf/1509.02417v1.pdf | author:Carlos Garcia-Saura category:cs.RO cs.NE published:2015-09-08 summary:Bio-inspired control of motion is an active field of research with manyapplications in real world tasks. In the case of robotic systems that need toexhibit oscillatory behaviour (i.e. locomotion of snake-type or legged robots),Central Pattern Generators (CPGs) are among the most versatile solutions. Thesecontrollers are often based on loosely-coupled oscillators similar to thosefound in the neural circuits of many animal species, and can be more robust touncertainty (i.e. external perturbations) than traditional control approaches.This project provides an overview of the state-of-the-art in the field of CPGs,and in particular their applications within robotic systems. The project alsotackles the implementation of a CPG-based controller in a small 3D-printedhexapod.
arxiv-12300-183 | Unsupervised Domain Discovery using Latent Dirichlet Allocation for Acoustic Modelling in Speech Recognition | http://arxiv.org/pdf/1509.02412v1.pdf | author:Mortaza Doulaty, Oscar Saz, Thomas Hain category:cs.CL published:2015-09-08 summary:Speech recognition systems are often highly domain dependent, a fact widelyreported in the literature. However the concept of domain is complex and notbound to clear criteria. Hence it is often not evident if data should beconsidered to be out-of-domain. While both acoustic and language models can bedomain specific, work in this paper concentrates on acoustic modelling. Wepresent a novel method to perform unsupervised discovery of domains usingLatent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains isassumed to exist in the data, whereby each audio segment can be considered tobe a weighted mixture of domain properties. The classification of audiosegments into domains allows the creation of domain specific acoustic modelsfor automatic speech recognition. Experiments are conducted on a dataset ofdiverse speech data covering speech from radio and TV broadcasts, telephoneconversations, meetings, lectures and read speech, with a joint training set of60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation toLDA based domains was shown to yield relative Word Error Rate (WER)improvements of up to 16% relative, compared to pooled training, and up to 10%,compared with models adapted with human-labelled prior domain knowledge.
arxiv-12300-184 | Data-selective Transfer Learning for Multi-Domain Speech Recognition | http://arxiv.org/pdf/1509.02409v1.pdf | author:Mortaza Doulaty, Oscar Saz, Thomas Hain category:cs.LG cs.CL cs.SD published:2015-09-08 summary:Negative transfer in training of acoustic models for automatic speechrecognition has been reported in several contexts such as domain change orspeaker characteristics. This paper proposes a novel technique to overcomenegative transfer by efficient selection of speech data for acoustic modeltraining. Here data is chosen on relevance for a specific target. A submodularfunction based on likelihood ratios is used to determine how acousticallysimilar each training utterance is to a target test set. The approach isevaluated on a wide-domain data set, covering speech from radio and TVbroadcasts, telephone conversations, meetings, lectures and read speech.Experiments demonstrate that the proposed technique both finds relevant dataand limits negative transfer. Results on a 6--hour test set show a relativeimprovement of 4% with data selection over using all data in PLP based models,and 2% with DNN features.
arxiv-12300-185 | Weakly Supervised Learning for Salient Object Detection | http://arxiv.org/pdf/1501.07492v2.pdf | author:Huaizu Jiang category:cs.CV published:2015-01-29 summary:Recent advances in supervised salient object detection has resulted insignificant performance on benchmark datasets. Training such models, however,requires expensive pixel-wise annotations of salient objects. Moreover, manyexisting salient object detection models assume that at least one salientobject exists in the input image. Such an assumption often leads to lessappealing saliency maps on the background images, which contain no salientobject at all. To avoid the requirement of expensive pixel-wise salient regionannotations, in this paper, we study weakly supervised learning approaches forsalient object detection. Given a set of background images and salient objectimages, we propose a solution toward jointly addressing the salient objectexistence and detection tasks. We adopt the latent SVM framework and formulatethe two problems together in a single integrated objective function: saliencylabels of superpixels are modeled as hidden variables and involved in aclassification term conditioned to the salient object existence variable, whichin turn depends on both global image and regional saliency features andsaliency label assignment. Experimental results on benchmark datasets validatethe effectiveness of our proposed approach.
arxiv-12300-186 | Empirical risk minimization is consistent with the mean absolute percentage error | http://arxiv.org/pdf/1509.02357v1.pdf | author:Arnaud De Myttenaere, Bénédicte Le Grand, Fabrice Rossi category:stat.ML published:2015-09-08 summary:We study in this paper the consequences of using the Mean Absolute PercentageError (MAPE) as a measure of quality for regression models. We show thatfinding the best model under the MAPE is equivalent to doing weighted MeanAbsolute Error (MAE) regression. We also show that, under some asumptions,universal consistency of Empirical Risk Minimization remains possible using theMAPE.
arxiv-12300-187 | On the complexity of piecewise affine system identification | http://arxiv.org/pdf/1509.02348v1.pdf | author:Fabien Lauer category:stat.ML cs.CC published:2015-09-08 summary:The paper provides results regarding the computational complexity of hybridsystem identification. More precisely, we focus on the estimation of piecewiseaffine (PWA) maps from input-output data and analyze the complexity ofcomputing a global minimizer of the error. Previous work showed that a globalsolution could be obtained for continuous PWA maps with a worst-case complexityexponential in the number of data. In this paper, we show how global optimalitycan be reached for a slightly more general class of possibly discontinuous PWAmaps with a complexity only polynomial in the number of data, however with anexponential complexity with respect to the data dimension. This result isobtained via an analysis of the intrinsic classification subproblem ofassociating the data points to the different modes. In addition, we prove thatthe problem is NP-hard, and thus that the exponential complexity in thedimension is a natural expectation for any exact algorithm.
arxiv-12300-188 | Modelling time evolving interactions in networks through a non stationary extension of stochastic block models | http://arxiv.org/pdf/1509.02347v1.pdf | author:Marco Corneli, Pierre Latouche, Fabrice Rossi category:stat.ML published:2015-09-08 summary:In this paper, we focus on the stochastic block model (SBM),a probabilistictool describing interactions between nodes of a network using latent clusters.The SBM assumes that the networkhas a stationary structure, in whichconnections of time varying intensity are not taken into account. In otherwords, interactions between two groups are forced to have the same featuresduring the whole observation time. To overcome this limitation,we propose apartition of the whole time horizon, in which interactions are observed, anddevelop a non stationary extension of the SBM,allowing to simultaneouslycluster the nodes in a network along with fixed time intervals in which theinteractions take place. The number of clusters (K for nodes, D for timeintervals) as well as the class memberships are finallyobtained throughmaximizing the complete-data integrated likelihood by means of a greedy searchapproach. After showing that the model works properly with simulated data, wefocus on a real data set. We thus consider the three days ACM Hypertextconference held in Turin,June 29th - July 1st 2009. Proximity interactionsbetween attendees during the first day are modelled and aninterestingclustering of the daily hours is finally obtained, with times ofsocial gathering (e.g. coffee breaks) recovered by the approach. Applicationsto large networks are limited due to the computational complexity of the greedysearch which is dominated bythe number $K\_{max}$ and $D\_{max}$ of clustersused in the initialization. Therefore,advanced clustering tools are consideredto reduce the number of clusters expected in the data, making the greedy searchapplicable to large networks.
arxiv-12300-189 | HEp-2 Cell Classification: The Role of Gaussian Scale Space Theory as A Pre-processing Approach | http://arxiv.org/pdf/1509.02320v1.pdf | author:Xianbiao Qi, Guoying Zhao, Jie Chen, Matti Pietikäinen category:cs.CV published:2015-09-08 summary:\textit{Indirect Immunofluorescence Imaging of Human Epithelial Type 2}(HEp-2) cells is an effective way to identify the presence of Anti-NuclearAntibody (ANA). Most existing works on HEp-2 cell classification mainly focuson feature extraction, feature encoding and classifier design. Very few effortshave been devoted to study the importance of the pre-processing techniques. Inthis paper, we analyze the importance of the pre-processing, and investigatethe role of Gaussian Scale Space (GSS) theory as a pre-processing approach forthe HEp-2 cell classification task. We validate the GSS pre-processing underthe Local Binary Pattern (LBP) and the Bag-of-Words (BoW) frameworks. Under theBoW framework, the introduced pre-processing approach, using only one LocalOrientation Adaptive Descriptor (LOAD), achieved superior performance on theExecutable Thematic on Pattern Recognition Techniques for IndirectImmunofluorescence (ET-PRT-IIF) image analysis. Our system, using only onefeature, outperformed the winner of the ICPR 2014 contest that combined fourtypes of features. Meanwhile, the proposed pre-processing method is notrestricted to this work; it can be generalized to many existing works.
arxiv-12300-190 | Object Proposals for Text Extraction in the Wild | http://arxiv.org/pdf/1509.02317v1.pdf | author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV published:2015-09-08 summary:Object Proposals is a recent computer vision technique receiving increasinginterest from the research community. Its main objective is to generate arelatively small set of bounding box proposals that are most likely to containobjects of interest. The use of Object Proposals techniques in the scene textunderstanding field is innovative. Motivated by the success of powerful whileexpensive techniques to recognize words in a holistic way, Object Proposalstechniques emerge as an alternative to the traditional text detectors. In this paper we study to what extent the existing generic Object Proposalsmethods may be useful for scene text understanding. Also, we propose a newObject Proposals algorithm that is specifically designed for text and compareit with other generic methods in the state of the art. Experiments show thatour proposal is superior in its ability of producing good quality wordproposals in an efficient way. The source code of our method is made publiclyavailable.
arxiv-12300-191 | Deeply Learning the Messages in Message Passing Inference | http://arxiv.org/pdf/1506.02108v3.pdf | author:Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel category:cs.CV cs.LG stat.ML published:2015-06-06 summary:Deep structured output learning shows great promise in tasks like semanticimage segmentation. We proffer a new, efficient deep structured model learningscheme, in which we show how deep Convolutional Neural Networks (CNNs) can beused to estimate the messages in message passing inference for structuredprediction with Conditional Random Fields (CRFs). With such CNN messageestimators, we obviate the need to learn or evaluate potential functions formessage calculation. This confers significant efficiency for learning, sinceotherwise when performing structured learning for a CRF with CNN potentials itis necessary to undertake expensive inference for every stochastic gradientiteration. The network output dimension for message estimation is the same asthe number of classes, in contrast to the network output for general CNNpotential functions in CRFs, which is exponential in the order of thepotentials. Hence CNN message learning has fewer network parameters and is morescalable for cases that a large number of classes are involved. We apply ourmethod to semantic image segmentation on the PASCAL VOC 2012 dataset. Weachieve an intersection-over-union score of 73.4 on its test set, which is thebest reported result for methods using the VOC training images alone. Thisimpressive performance demonstrates the effectiveness and usefulness of our CNNmessage learning method.
arxiv-12300-192 | Sampled Weighted Min-Hashing for Large-Scale Topic Mining | http://arxiv.org/pdf/1509.01771v2.pdf | author:Gibran Fuentes-Pineda, Ivan Vladimir Meza-Ruiz category:cs.LG cs.CL cs.IR published:2015-09-06 summary:We present Sampled Weighted Min-Hashing (SWMH), a randomized approach toautomatically mine topics from large-scale corpora. SWMH generates multiplerandom partitions of the corpus vocabulary based on term co-occurrence andagglomerates highly overlapping inter-partition cells to produce the minedtopics. While other approaches define a topic as a probabilistic distributionover a vocabulary, SWMH topics are ordered subsets of such vocabulary.Interestingly, the topics mined by SWMH underlie themes from the corpus atdifferent levels of granularity. We extensively evaluate the meaningfulness ofthe mined topics both qualitatively and quantitatively on the NIPS (1.7 Kdocuments), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora.Additionally, we compare the quality of SWMH with Online LDA topics fordocument representation in classification.
arxiv-12300-193 | Supervised Collective Classification for Crowdsourcing | http://arxiv.org/pdf/1507.06682v2.pdf | author:Pin-Yu Chen, Chia-Wei Lien, Fu-Jen Chu, Pai-Shun Ting, Shin-Ming Cheng category:cs.SI cs.LG stat.ML published:2015-07-23 summary:Crowdsourcing utilizes the wisdom of crowds for collective classification viainformation (e.g., labels of an item) provided by labelers. Currentcrowdsourcing algorithms are mainly unsupervised methods that are unaware ofthe quality of crowdsourced data. In this paper, we propose a supervisedcollective classification algorithm that aims to identify reliable labelersfrom the training data (e.g., items with known labels). The reliability (i.e.,weighting factor) of each labeler is determined via a saddle point algorithm.The results on several crowdsourced data show that supervised methods canachieve better classification accuracy than unsupervised methods, and ourproposed method outperforms other algorithms.
arxiv-12300-194 | Dynamic Structure Embedded Online Multiple-Output Regression for Stream Data | http://arxiv.org/pdf/1412.5732v2.pdf | author:Changsheng Li, Fan Wei, Weishan Dong, Qingshan Liu, Xiangfeng Wang, Xin Zhang category:cs.LG published:2014-12-18 summary:Online multiple-output regression is an important machine learning techniquefor modeling, predicting, and compressing multi-dimensional correlated datastreams. In this paper, we propose a novel online multiple-output regressionmethod, called MORES, for stream data. MORES can \emph{dynamically} learn thestructure of the coefficients change in each update step to facilitate themodel's continuous refinement. We observe that limited expressive ability ofthe regression model, especially in the preliminary stage of online update,often leads to the variables in the residual errors being dependent. In lightof this point, MORES intends to \emph{dynamically} learn and leverage thestructure of the residual errors to improve the prediction accuracy. Moreover,we define three statistical variables to \emph{exactly} represent all the seensamples for \emph{incrementally} calculating prediction loss in each onlineupdate round, which can avoid loading all the training data into memory forupdating model, and also effectively prevent drastic fluctuation of the modelin the presence of noise. Furthermore, we introduce a forgetting factor to setdifferent weights on samples so as to track the data streams' evolvingcharacteristics quickly from the latest samples. Experiments on one syntheticdataset and three real-world datasets validate the effectiveness of theproposed method. In addition, the update speed of MORES is at least 2000samples processed per second on the three real-world datasets, more than 15times faster than the state-of-the-art online learning algorithm.
arxiv-12300-195 | Enhancing Automatically Discovered Multi-level Acoustic Patterns Considering Context Consistency With Applications in Spoken Term Detection | http://arxiv.org/pdf/1509.02217v1.pdf | author:Cheng-Tao Chung, Wei-Ning Hsu, Cheng-Yi Lee, Lin-Shan Lee category:cs.CL published:2015-09-07 summary:This paper presents a novel approach for enhancing the multiple sets ofacoustic patterns automatically discovered from a given corpus. In a previouswork it was proposed that different HMM configurations (number of states permodel, number of distinct models) for the acoustic patterns form atwo-dimensional space. Multiple sets of acoustic patterns automaticallydiscovered with the HMM configurations properly located on different pointsover this two-dimensional space were shown to be complementary to one another,jointly capturing the characteristics of the given corpus. By representing thegiven corpus as sequences of acoustic patterns on different HMM sets, thepattern indices in these sequences can be relabeled considering the contextconsistency across the different sequences. Good improvements were observed inpreliminary experiments of pattern spoken term detection (STD) performed onboth TIMIT and Mandarin Broadcast News with such enhanced patterns.
arxiv-12300-196 | Fuzzy Jets | http://arxiv.org/pdf/1509.02216v1.pdf | author:Lester Mackey, Benjamin Nachman, Ariel Schwartzman, Conrad Stansbury category:hep-ph stat.ML published:2015-09-07 summary:Collimated streams of particles produced in high energy physics experimentsare organized using clustering algorithms to form jets. To construct jets, theexperimental collaborations based at the Large Hadron Collider (LHC) primarilyuse agglomerative hierarchical clustering schemes known as sequentialrecombination. We propose a new class of algorithms for clustering jets thatuse infrared and collinear safe mixture models. These new algorithms, known asfuzzy jets, are clustered using maximum likelihood techniques and candynamically determine various properties of jets like their size. We show thatthe fuzzy jet size adds additional information to conventional jet taggingvariables. Furthermore, we study the impact of pileup and show that with someslight modifications to the algorithm, fuzzy jets can be stable up to highpileup interaction multiplicities.
arxiv-12300-197 | Unsupervised Spoken Term Detection with Spoken Queries by Multi-level Acoustic Patterns with Varying Model Granularity | http://arxiv.org/pdf/1509.02213v1.pdf | author:Cheng-Tao Chung, Chun-an Chan, Lin-shan Lee category:cs.CL published:2015-09-07 summary:This paper presents a new approach for unsupervised Spoken Term Detectionwith spoken queries using multiple sets of acoustic patterns automaticallydiscovered from the target corpus. The different pattern HMMconfigurations(number of states per model, number of distinct models, number ofGaussians per state)form a three-dimensional model granularity space. Differentsets of acoustic patterns automatically discovered on different points properlydistributed over this three-dimensional space are complementary to one another,thus can jointly capture the characteristics of the spoken terms. Byrepresenting the spoken content and spoken query as sequences of acousticpatterns, a series of approaches for matching the pattern index sequences whileconsidering the signal variations are developed. In this way, not only theon-line computation load can be reduced, but the signal distributions caused bydifferent speakers and acoustic conditions can be reasonably taken care of. Theresults indicate that this approach significantly outperformed the unsupervisedfeature-based DTW baseline by 16.16\% in mean average precision on the TIMITcorpus.
arxiv-12300-198 | Unsupervised Discovery of Linguistic Structure Including Two-level Acoustic Patterns Using Three Cascaded Stages of Iterative Optimization | http://arxiv.org/pdf/1509.02208v1.pdf | author:Cheng-Tao Chung, Chun-an Chan, Lin-shan Lee category:cs.CL published:2015-09-07 summary:Techniques for unsupervised discovery of acoustic patterns are gettingincreasingly attractive, because huge quantities of speech data are becomingavailable but manual annotations remain hard to acquire. In this paper, wepropose an approach for unsupervised discovery of linguistic structure for thetarget spoken language given raw speech data. This linguistic structureincludes two-level (subword-like and word-like) acoustic patterns, the lexiconof word-like patterns in terms of subword-like patterns and the N-gram languagemodel based on word-like patterns. All patterns, models, and parameters can beautomatically learned from the unlabelled speech corpus. This is achieved by aninitialization step followed by three cascaded stages for acoustic, linguistic,and lexical iterative optimization. The lexicon of word-like patterns definesallowed consecutive sequence of HMMs for subword-like patterns. In eachiteration, model training and decoding produces updated labels from which thelexicon and HMMs can be further updated. In this way, model parameters anddecoded labels are respectively optimized in each iteration, and the knowledgeabout the linguistic structure is learned gradually layer after layer. Theproposed approach was tested in preliminary experiments on a corpus of Mandarinbroadcast news, including a task of spoken term detection with performancecompared to a parallel test using models trained in a supervised way. Resultsshow that the proposed system not only yields reasonable performance on itsown, but is also complimentary to existing large vocabulary ASR systems.
arxiv-12300-199 | VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer vision at Large Scale | http://arxiv.org/pdf/1403.3724v4.pdf | author:William Gray Roncal, Michael Pekala, Verena Kaynig-Fittkau, Dean M. Kleissas, Joshua T. Vogelstein, Hanspeter Pfister, Randal Burns, R. Jacob Vogelstein, Mark A. Chevillet, Gregory D. Hager category:cs.CV cs.CE q-bio.QM published:2014-03-14 summary:An open challenge problem at the forefront of modern neuroscience is toobtain a comprehensive mapping of the neural pathways that underlie human brainfunction; an enhanced understanding of the wiring diagram of the brain promisesto lead to new breakthroughs in diagnosing and treating neurological disorders.Inferring brain structure from image data, such as that obtained via electronmicroscopy (EM), entails solving the problem of identifying biologicalstructures in large data volumes. Synapses, which are a key communicationstructure in the brain, are particularly difficult to detect due to their smallsize and limited contrast. Prior work in automated synapse detection has reliedupon time-intensive biological preparations (post-staining, isotropic slicethicknesses) in order to simplify the problem. This paper presents VESICLE, the first known approach designed for mammaliansynapse detection in anisotropic, non-post-stained data. Our methods explicitlyleverage biological context, and the results exceed existing synapse detectionmethods in terms of accuracy and scalability. We provide two differentapproaches - one a deep learning classifier (VESICLE-CNN) and one a lightweightRandom Forest approach (VESICLE-RF) to offer alternatives in theperformance-scalability space. Addressing this synapse detection challengeenables the analysis of high-throughput imaging data soon expected to reachpetabytes of data, and provide tools for more rapid estimation of brain-graphs.Finally, to facilitate community efforts, we developed tools for large-scaleobject detection, and demonstrated this framework to find $\approx$ 50,000synapses in 60,000 $\mu m ^3$ (220 GB on disk) of electron microscopy data.
arxiv-12300-200 | MegaFace: A Million Faces for Recognition at Scale | http://arxiv.org/pdf/1505.02108v2.pdf | author:D. Miller, E. Brossard, S. Seitz, I. Kemelmacher-Shlizerman category:cs.CV published:2015-05-08 summary:Recent face recognition experiments on the LFW benchmark show that facerecognition is performing stunningly well, surpassing human recognition rates.In this paper, we study face recognition at scale. Specifically, we havecollected from Flickr a \textbf{Million} faces and evaluated state of the artface recognition algorithms on this dataset. We found that the performance ofalgorithms varies--while all perform great on LFW, once evaluated at scalerecognition rates drop drastically for most algorithms. Interestingly, deeplearning based approach by \cite{schroff2015facenet} performs much better, butstill gets less robust at scale. We consider both verification andidentification problems, and evaluate how pose affects recognition at scale.Moreover, we ran an extensive human study on Mechanical Turk to evaluate humanrecognition at scale, and report results. All the photos are creative commonsphotos and is released at \small{\url{http://megaface.cs.washington.edu/}} forresearch and further experiments.
arxiv-12300-201 | Structured Prediction with Output Embeddings for Semantic Image Annotation | http://arxiv.org/pdf/1509.02130v1.pdf | author:Ariadna Quattoni, Arnau Ramisa, Pranava Swaroop Madhyastha, Edgar Simo-Serra, Francesc Moreno-Noguer category:cs.CV published:2015-09-07 summary:We address the task of annotating images with semantic tuples. Solving thisproblem requires an algorithm which is able to deal with hundreds of classesfor each argument of the tuple. In such contexts, data sparsity becomes a keychallenge, as there will be a large number of classes for which only a fewexamples are available. We propose handling this by incorporating featurerepresentations of both the inputs (images) and outputs (argument classes) intoa factorized log-linear model, and exploiting the flexibility of scoringfunctions based on bilinear forms. Experiments show that integrating featurerepresentations of the outputs in the structured prediction model leads tobetter overall predictions. We also conclude that the best outputrepresentation is specific for each type of argument.
arxiv-12300-202 | Convexity Shape Constraints for Image Segmentation | http://arxiv.org/pdf/1509.02122v1.pdf | author:Loic A. Royer, David L. Richmond, Carsten Rother, Bjoern Andres, Dagmar Kainmueller category:cs.CV published:2015-09-07 summary:Segmenting an image into multiple components is a central task in computervision. In many practical scenarios, prior knowledge about plausible componentsis available. Incorporating such prior knowledge into models and algorithms forimage segmentation is highly desirable, yet can be non-trivial. In this work,we introduce a new approach that allows, for the first time, to constrain someor all components of a segmentation to have convex shapes. Specifically, weextend the Minimum Cost Multicut Problem by a class of constraints that enforceconvexity. To solve instances of this APX-hard integer linear program tooptimality, we separate the proposed constraints in the branch-and-cut loop ofa state-of-the-art ILP solver. Results on natural and biological imagesdemonstrate the effectiveness of the approach as well as its advantage over thestate-of-the-art heuristic.
arxiv-12300-203 | Future Localization from an Egocentric Depth Image | http://arxiv.org/pdf/1509.02094v1.pdf | author:Hyun Soo Park, Yedong Niu, Jianbo Shi category:cs.CV published:2015-09-07 summary:This paper presents a method for future localization: to predict a set ofplausible trajectories of ego-motion given a depth image. We predict pathsavoiding obstacles, between objects, even paths turning around a corner intospace behind objects. As a byproduct of the predicted trajectories ofego-motion, we discover in the image the empty space occluded by foregroundobjects. We use no image based features such as semantic labeling/segmentationor object detection/recognition for this algorithm. Inspired by proxemics, werepresent the space around a person using an EgoSpace map, akin to anillustrated tourist map, that measures a likelihood of occlusion at theegocentric coordinate system. A future trajectory of ego-motion is modeled by alinear combination of compact trajectory bases allowing us to constrain thepredicted trajectory. We learn the relationship between the EgoSpace map andtrajectory from the EgoMotion dataset providing in-situ measurements of thefuture trajectory. A cost function that takes into account partial occlusiondue to foreground objects is minimized to predict a trajectory. This costfunction generates a trajectory that passes through the occluded space, whichallows us to discover the empty space behind the foreground objects. Wequantitatively evaluate our method to show predictive validity and apply tovarious real world scenes including walking, shopping, and social interactions.
arxiv-12300-204 | Matrix Factorisation with Linear Filters | http://arxiv.org/pdf/1509.02088v1.pdf | author:Ömer Deniz Akyıldız category:stat.ML published:2015-09-07 summary:This text investigates relations between two well-known family of algorithms,matrix factorisations and recursive linear filters, by describing aprobabilistic model in which approximate inference corresponds to a matrixfactorisation algorithm. Using the probabilistic model, we derive a matrixfactorisation algorithm as a recursive linear filter. More precisely, we derivea matrix-variate recursive linear filter in order to perform efficientinference in high dimensions. We also show that it is possible to interpret ouralgorithm as a nontrivial stochastic gradient algorithm. Demonstrations andcomparisons on an image restoration task are given.
arxiv-12300-205 | An Infra-Structure for Performance Estimation and Experimental Comparison of Predictive Models in R | http://arxiv.org/pdf/1412.0436v4.pdf | author:Luis Torgo category:cs.MS cs.LG cs.SE stat.CO published:2014-12-01 summary:This document describes an infra-structure provided by the R packageperformanceEstimation that allows to estimate the predictive performance ofdifferent approaches (workflows) to predictive tasks. The infra-structure isgeneric in the sense that it can be used to estimate the values of anyperformance metrics, for any workflow on different predictive tasks, namely,classification, regression and time series tasks. The package also includesseveral standard workflows that allow users to easily set up their experimentslimiting the amount of work and information they need to provide. The overallgoal of the infra-structure provided by our package is to facilitate the taskof estimating the predictive performance of different modeling approaches topredictive tasks in the R environment.
arxiv-12300-206 | A New Low-Rank Tensor Model for Video Completion | http://arxiv.org/pdf/1509.02027v1.pdf | author:Wenrui Hu, Dacheng Tao, Wensheng Zhang, Yuan Xie, Yehui Yang category:cs.CV published:2015-09-07 summary:In this paper, we propose a new low-rank tensor model based on the circulantalgebra, namely, twist tensor nuclear norm or t-TNN for short. The twist tensordenotes a 3-way tensor representation to laterally store 2D data slices inorder. On one hand, t-TNN convexly relaxes the tensor multi-rank of the twisttensor in the Fourier domain, which allows an efficient computation using FFT.On the other, t-TNN is equal to the nuclear norm of block circulantmatricization of the twist tensor in the original domain, which extends thetraditional matrix nuclear norm in a block circulant way. We test the t-TNNmodel on a video completion application that aims to fill missing values andthe experiment results validate its effectiveness, especially when dealing withvideo recorded by a non-stationary panning camera. The block circulantmatricization of the twist tensor can be transformed into a circulant blockrepresentation with nuclear norm invariance. This representation, aftertransformation, exploits the horizontal translation relationship between theframes in a video, and endows the t-TNN model with a more powerful ability toreconstruct panning videos than the existing state-of-the-art low-rank models.
arxiv-12300-207 | Distributed Machine Learning via Sufficient Factor Broadcasting | http://arxiv.org/pdf/1409.5705v2.pdf | author:Pengtao Xie, Jin Kyu Kim, Yi Zhou, Qirong Ho, Abhimanu Kumar, Yaoliang Yu, Eric Xing category:cs.LG cs.DC published:2014-09-19 summary:Matrix-parametrized models, including multiclass logistic regression andsparse coding, are used in machine learning (ML) applications ranging fromcomputer vision to computational biology. When these models are applied tolarge-scale ML problems starting at millions of samples and tens of thousandsof classes, their parameter matrix can grow at an unexpected rate, resulting inhigh parameter synchronization costs that greatly slow down distributedlearning. To address this issue, we propose a Sufficient Factor Broadcasting(SFB) computation model for efficient distributed learning of a large family ofmatrix-parameterized models, which share the following property: the parameterupdate computed on each data sample is a rank-1 matrix, i.e., the outer productof two "sufficient factors" (SFs). By broadcasting the SFs among workermachines and reconstructing the update matrices locally at each worker, SFBimproves communication efficiency --- communication costs are linear in theparameter matrix's dimensions, rather than quadratic --- without affectingcomputational correctness. We present a theoretical convergence analysis ofSFB, and empirically corroborate its efficiency on four differentmatrix-parametrized ML models.
arxiv-12300-208 | An Approach to the Analysis of the South Slavic Medieval Labels Using Image Texture | http://arxiv.org/pdf/1509.01978v1.pdf | author:Darko Brodic, Alessia Amelio, Zoran N. Milivojevic category:cs.CV cs.AI cs.CL I.4; I.2.7 published:2015-09-07 summary:The paper presents a new script classification method for the discriminationof the South Slavic medieval labels. It consists in the textural analysis ofthe script types. In the first step, each letter is coded by the equivalentscript type, which is defined by its typographical features. Obtained codedtext is subjected to the run-length statistical analysis and to the adjacentlocal binary pattern analysis in order to extract the features. The resultshows a diversity between the extracted features of the scripts, which makesthe feature classification more effective. It is the basis for theclassification process of the script identification by using an extension of astate-of-the-art approach for document clustering. The proposed method isevaluated on an example of hand-engraved in stone and hand-printed in paperlabels in old Cyrillic, angular and round Glagolitic. Experiments demonstratevery positive results, which prove the effectiveness of the proposed method.
arxiv-12300-209 | Automated Analysis of Behavioural Variability and Filial Imprinting of Chicks (G. gallus), using Autonomous Robots | http://arxiv.org/pdf/1509.01957v1.pdf | author:A. Gribovskiy, F. Mondada, J. L. Deneubourg, L. Cazenille, N. Bredeche, J. Halloy category:q-bio.QM cs.LG cs.RO physics.bio-ph published:2015-09-07 summary:Inter-individual variability has various impacts in animal social behaviour.This implies that not only collective behaviours have to be studied but alsothe behavioural variability of each member composing the groups. To understandthose effects on group behaviour, we develop a quantitative methodology basedon automated ethograms and autonomous robots to study the inter-individualvariability among social animals. We choose chicks of \textit{Gallus gallusdomesticus} as a classic social animal model system for their suitability inlaboratory and controlled experimentation. Moreover, even domesticated chickenpresent social structures implying forms or leadership and filial imprinting.We develop an imprinting methodology on autonomous robots to study individualand social behaviour of free moving animals. This allows to quantify thebehaviours of large number of animals. We develop an automated experimentalmethodology that allows to make relatively fast controlled experiments andefficient data analysis. Our analysis are based on high-throughput dataallowing a fine quantification of individual behavioural traits. We quantifythe efficiency of various state-of-the-art algorithms to automate data analysisand produce automated ethograms. We show that the use of robots allows toprovide controlled and quantified stimuli to the animals in absence of humanintervention. We quantify the individual behaviour of 205 chicks obtained fromhatching after synchronized fecundation. Our results show a high variability ofindividual behaviours and of imprinting quality and success. Three classes ofchicks are observed with various level of imprinting. Our study shows that theconcomitant use of autonomous robots and automated ethograms allows detailedand quantitative analysis of behavioural patterns of animals in controlledlaboratory experiments.
arxiv-12300-210 | Hierarchical Deep Learning Architecture For 10K Objects Classification | http://arxiv.org/pdf/1509.01951v1.pdf | author:Atul Laxman Katole, Krishna Prasad Yellapragada, Amish Kumar Bedi, Sehaj Singh Kalra, Mynepalli Siva Chaitanya category:cs.CV cs.LG cs.NE published:2015-09-07 summary:Evolution of visual object recognition architectures based on ConvolutionalNeural Networks & Convolutional Deep Belief Networks paradigms hasrevolutionized artificial Vision Science. These architectures extract & learnthe real world hierarchical visual features utilizing supervised & unsupervisedlearning approaches respectively. Both the approaches yet cannot scale uprealistically to provide recognition for a very large number of objects as highas 10K. We propose a two level hierarchical deep learning architecture inspiredby divide & conquer principle that decomposes the large scale recognitionarchitecture into root & leaf level model architectures. Each of the root &leaf level models is trained exclusively to provide superior results thanpossible by any 1-level deep learning architecture prevalent today. Theproposed architecture classifies objects in two steps. In the first step theroot level model classifies the object in a high level category. In the secondstep, the leaf level recognition model for the recognized high level categoryis selected among all the leaf models. This leaf level model is presented withthe same input object image which classifies it in a specific category. Also wepropose a blend of leaf level models trained with either supervised orunsupervised learning approaches. Unsupervised learning is suitable wheneverlabelled data is scarce for the specific leaf level models. Currently thetraining of leaf level models is in progress; where we have trained 25 out ofthe total 47 leaf level models as of now. We have trained the leaf models withthe best case top-5 error rate of 3.2% on the validation data set for theparticular leaf models. Also we demonstrate that the validation error of theleaf level models saturates towards the above mentioned accuracy as the numberof epochs are increased to more than sixty.
arxiv-12300-211 | A Fast and Accurate Unconstrained Face Detector | http://arxiv.org/pdf/1408.1656v3.pdf | author:Shengcai Liao, Anil K. Jain, Stan Z. Li category:cs.CV published:2014-08-06 summary:We propose a method to address challenges in unconstrained face detection,such as arbitrary pose variations and occlusions. First, a new image featurecalled Normalized Pixel Difference (NPD) is proposed. NPD feature is computedas the difference to sum ratio between two pixel values, inspired by the WeberFraction in experimental psychology. The new feature is scale invariant,bounded, and is able to reconstruct the original image. Second, we propose adeep quadratic tree to learn the optimal subset of NPD features and theircombinations, so that complex face manifolds can be partitioned by the learnedrules. This way, only a single soft-cascade classifier is needed to handleunconstrained face detection. Furthermore, we show that the NPD features can beefficiently obtained from a look up table, and the detection template can beeasily scaled, making the proposed face detector very fast. Experimentalresults on three public face datasets (FDDB, GENKI, and CMU-MIT) show that theproposed method achieves state-of-the-art performance in detectingunconstrained faces with arbitrary pose variations and occlusions in clutteredscenes.
arxiv-12300-212 | Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical Machine Translation | http://arxiv.org/pdf/1509.01938v1.pdf | author:Katrin Kirchhoff, Bing Zhao, Wen Wang category:cs.CL published:2015-09-07 summary:Statistical machine translation for dialectal Arabic is characterized by alack of data since data acquisition involves the transcription and translationof spoken language. In this study we develop techniques for extracting paralleldata for one particular dialect of Arabic (Iraqi Arabic) from out-of-domaincorpora in different dialects of Arabic or in Modern Standard Arabic. Wecompare two different data selection strategies (cross-entropy based andsubmodular selection) and demonstrate that a very small but highly targetedamount of found data can improve the performance of a baseline machinetranslation system. We furthermore report on preliminary experiments on usingautomatically translated speech data as additional training data.
arxiv-12300-213 | Approval Voting and Incentives in Crowdsourcing | http://arxiv.org/pdf/1502.05696v3.pdf | author:Nihar B. Shah, Dengyong Zhou, Yuval Peres category:cs.GT cs.AI cs.LG cs.MA published:2015-02-19 summary:The growing need for labeled training data has made crowdsourcing animportant part of machine learning. The quality of crowdsourced labels is,however, adversely affected by three factors: (1) the workers are not experts;(2) the incentives of the workers are not aligned with those of the requesters;and (3) the interface does not allow workers to convey their knowledgeaccurately, by forcing them to make a single choice among a set of options. Inthis paper, we address these issues by introducing approval voting to utilizethe expertise of workers who have partial knowledge of the true answer, andcoupling it with a ("strictly proper") incentive-compatible compensationmechanism. We show rigorous theoretical guarantees of optimality of ourmechanism together with a simple axiomatic characterization. We also conductpreliminary empirical studies on Amazon Mechanical Turk which validate ourapproach.
arxiv-12300-214 | A Hybrid Approach to Domain-Specific Entity Linking | http://arxiv.org/pdf/1509.01865v1.pdf | author:Alex Olieman, Jaap Kamps, Maarten Marx, Arjan Nusselder category:cs.IR cs.CL H.3.1 published:2015-09-06 summary:The current state-of-the-art Entity Linking (EL) systems are geared towardscorpora that are as heterogeneous as the Web, and therefore performsub-optimally on domain-specific corpora. A key open problem is how toconstruct effective EL systems for specific domains, as knowledge of the localcontext should in principle increase, rather than decrease, effectiveness. Inthis paper we propose the hybrid use of simple specialist linkers incombination with an existing generalist system to address this problem. Ourmain findings are the following. First, we construct a new reusable benchmarkfor EL on a corpus of domain-specific conversations. Second, we test theperformance of a range of approaches under the same conditions, and show thatspecialist linkers obtain high precision in isolation, and high recall whencombined with generalist linkers. Hence, we can effectively exploit localcontext and get the best of both worlds.
arxiv-12300-215 | Compressed Nonnegative Matrix Factorization is Fast and Accurate | http://arxiv.org/pdf/1505.04650v2.pdf | author:Mariano Tepper, Guillermo Sapiro category:cs.LG stat.ML published:2015-05-18 summary:Nonnegative matrix factorization (NMF) has an established reputation as auseful data analysis technique in numerous applications. However, its usage inpractical situations is undergoing challenges in recent years. The fundamentalfactor to this is the increasingly growing size of the datasets available andneeded in the information sciences. To address this, in this work we propose touse structured random compression, that is, random projections that exploit thedata structure, for two NMF variants: classical and separable. In separable NMF(SNMF) the left factors are a subset of the columns of the input matrix. Wepresent suitable formulations for each problem, dealing with differentrepresentative algorithms within each one. We show that the resultingcompressed techniques are faster than their uncompressed variants, vastlyreduce memory demands, and do not encompass any significant deterioration inperformance. The proposed structured random projections for SNMF allow to dealwith arbitrarily shaped large matrices, beyond the standard limit oftall-and-skinny matrices, granting access to very efficient computations inthis general setting. We accompany the algorithmic presentation withtheoretical foundations and numerous and diverse examples, showing thesuitability of the proposed approaches.
arxiv-12300-216 | Hierarchical Completely Random Measures for Mixed Membership Modelling | http://arxiv.org/pdf/1509.01817v1.pdf | author:Gaurav Pandey, Ambedkar Dukkipati category:math.ST cs.LG stat.TH published:2015-09-06 summary:The main aim of this paper is to establish the applicability of a broad classof random measures, that includes the gamma process, for mixed membershipmodelling. We use completely random measures~(CRM) and hierarchical CRM todefine a prior for Poisson processes. We derive the marginal distribution ofthe resultant point process, when the underlying CRM is marginalized out. Usingwell known properties unique to Poisson processes, we were able to derive anexact approach for instantiating a Poisson process with a hierarchical CRMprior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRMmodels based on Chinese restaurant franchise sampling scheme. As an example, wepresent the sum of generalized gamma process (SGGP), and show its applicationin topic-modelling. We show that one can determine the power-law behaviour ofthe topics and words in a Bayesian fashion, by defining a prior on theparameters of SGGP.
arxiv-12300-217 | Research: Analysis of Transport Model that Approximates Decision Taker's Preferences | http://arxiv.org/pdf/1509.01815v1.pdf | author:Valery Vilisov category:cs.LG cs.AI math.OC stat.AP published:2015-09-06 summary:Paper provides a method for solving the reverse Monge-Kantorovich transportproblem (TP). It allows to accumulate positive decision-taking experience madeby decision-taker in situations that can be presented in the form of TP. Theinitial data for the solution of the inverse TP is the information on orders,inventories and effective decisions take by decision-taker. The result ofsolving the inverse TP contains evaluations of the TPs payoff matrix elements.It can be used in new situations to select the solution corresponding to thepreferences of the decision-taker. The method allows to gain decision-takerexperience, so it can be used by others. The method allows to build the modelof decision-taker preferences in a specific application area. The model can beupdated regularly to ensure its relevance and adequacy to the decision-takersystem of preferences. This model is adaptive to the current preferences of thedecision taker.
arxiv-12300-218 | A Total Fractional-Order Variation Model for Image Restoration with Non-homogeneous Boundary Conditions and its Numerical Solution | http://arxiv.org/pdf/1509.04237v1.pdf | author:Jianping Zhang, Ke Chen category:cs.CV math.NA published:2015-09-06 summary:To overcome the weakness of a total variation based model for imagerestoration, various high order (typically second order) regularization modelshave been proposed and studied recently. In this paper we analyze and test afractional-order derivative based total $\alpha$-order variation model, whichcan outperform the currently popular high order regularization models. Thereexist several previous works using total $\alpha$-order variations for imagerestoration; however first no analysis is done yet and second all testedformulations, differing from each other, utilize the zero Dirichlet boundaryconditions which are not realistic (while non-zero boundary conditions violatedefinitions of fractional-order derivatives). This paper first reviews someresults of fractional-order derivatives and then analyzes the theoreticalproperties of the proposed total $\alpha$-order variational model rigorously.It then develops four algorithms for solving the variational problem, one basedon the variational Split-Bregman idea and three based on direct solution of thediscretise-optimization problem. Numerical experiments show that, in terms ofrestoration quality and solution efficiency, the proposed model can producehighly competitive results, for smooth images, to two established high ordermodels: the mean curvature and the total generalized variation.
arxiv-12300-219 | Robust and highly performant ring detection algorithm for 3d particle tracking using 2d microscope imaging | http://arxiv.org/pdf/1310.1371v3.pdf | author:Eldad Afik category:cs.CV cond-mat.soft published:2013-10-02 summary:Three-dimensional particle tracking is an essential tool in studying dynamicsunder the microscope, namely, fluid dynamics in microfluidic devices, bacteriataxis, cellular trafficking. The 3d position can be determined using 2d imagingalone by measuring the diffraction rings generated by an out-of-focusfluorescent particle, imaged on a single camera. Here I present a ringdetection algorithm exhibiting a high detection rate, which is robust to thechallenges arising from ring occlusion, inclusions and overlaps, and allowsresolving particles even when near to each other. It is capable of real timeanalysis thanks to its high performance and low memory footprint. The proposedalgorithm, an offspring of the circle Hough transform, addresses the need toefficiently trace the trajectories of many particles concurrently, when theirnumber in not necessarily fixed, by solving a classification problem, andovercomes the challenges of finding local maxima in the complex parameter spacewhich results from ring clusters and noise. Several algorithmic conceptsintroduced here can be advantageous in other cases, particularly when dealingwith noisy and sparse data. The implementation is based on open-source andcross-platform software packages only, making it easy to distribute and modify.It is implemented in a microfluidic experiment allowing real-timemulti-particle tracking at 70 Hz, achieving a detection rate which exceeds 94%and only 1% false-detection.
arxiv-12300-220 | Simultaneous Clustering and Model Selection for Multinomial Distribution: A Comparative Study | http://arxiv.org/pdf/1505.02324v2.pdf | author:Md. Abul Hasnat, Julien Velcin, Stéphane Bonnevay, Julien Jacques category:cs.LG stat.ME stat.ML published:2015-05-09 summary:In this paper, we study different discrete data clustering methods, which usethe Model-Based Clustering (MBC) framework with the Multinomial distribution.Our study comprises several relevant issues, such as initialization, modelestimation and model selection. Additionally, we propose a novel MBC method byefficiently combining the partitional and hierarchical clustering techniques.We conduct experiments on both synthetic and real data and evaluate the methodsusing accuracy, stability and computation time. Our study identifiesappropriate strategies to be used for discrete data analysis with the MBCmethods. Moreover, our proposed method is very competitive w.r.t. clusteringaccuracy and better w.r.t. stability and computation time.
arxiv-12300-221 | Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM) for unsupervised RGB-D image segmentation | http://arxiv.org/pdf/1509.01788v1.pdf | author:Md. Abul Hasnat, Olivier Alata, Alain Trémeau category:cs.CV published:2015-09-06 summary:Recent advances in depth imaging sensors provide easy access to thesynchronized depth with color, called RGB-D image. In this paper, we propose anunsupervised method for indoor RGB-D image segmentation and analysis. Weconsider a statistical image generation model based on the color and geometryof the scene. Our method consists of a joint color-spatial-directionalclustering method followed by a statistical planar region merging method. Weevaluate our method on the NYU depth database and compare it with existingunsupervised RGB-D segmentation methods. Results show that, it is comparablewith the state of the art methods and it needs less computation time. Moreover,it opens interesting perspectives to fuse color and geometry in an unsupervisedmanner.
arxiv-12300-222 | Theoretical and Experimental Analyses of Tensor-Based Regression and Classification | http://arxiv.org/pdf/1509.01770v1.pdf | author:Kishan Wimalawarne, Ryota Tomioka, Masashi Sugiyama category:cs.LG stat.ML published:2015-09-06 summary:We theoretically and experimentally investigate tensor-based regression andclassification. Our focus is regularization with various tensor norms,including the overlapped trace norm, the latent trace norm, and the scaledlatent trace norm. We first give dual optimization methods using thealternating direction method of multipliers, which is computationally efficientwhen the number of training samples is moderate. We then theoretically derivean excess risk bound for each tensor norm and clarify their behavior. Finally,we perform extensive experiments using simulated and real data and demonstratethe superiority of tensor-based learning methods over vector- and matrix-basedlearning methods.
arxiv-12300-223 | A commentary on "The now-or-never bottleneck: a fundamental constraint on language", by Christiansen and Chater (2015) | http://arxiv.org/pdf/1509.01722v1.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL published:2015-09-05 summary:In a recent article, Christiansen and Chater (2015) present a fundamentalconstraint on language, i.e. a now-or-never bottleneck that arises from ourfleeting memory, and explore its implications, e.g., chunk-and-pass processing,outlining a framework that promises to unify different areas of research. Herewe explore additional support for this constraint and suggest furtherconnections from quantitative linguistics and information theory.
arxiv-12300-224 | Unsupervised Cross-Domain Recognition by Identifying Compact Joint Subspaces | http://arxiv.org/pdf/1509.01719v1.pdf | author:Yuewei Lin, Jing Chen, Yu Cao, Youjie Zhou, Lingfeng Zhang, Yuan Yan Tang, Song Wang category:cs.CV published:2015-09-05 summary:This paper introduces a new method to solve the cross-domain recognitionproblem. Different from the traditional domain adaption methods which rely on aglobal domain shift for all classes between source and target domain, theproposed method is more flexible to capture individual class variations acrossdomains. By adopting a natural and widely used assumption -- "the data samplesfrom the same class should lay on a low-dimensional subspace, even if they comefrom different domains", the proposed method circumvents the limitation of theglobal domain shift, and solves the cross-domain recognition by finding thecompact joint subspaces of source and target domain. Specifically, givenlabeled samples in source domain, we construct subspaces for each of theclasses. Then we construct subspaces in the target domain, called anchorsubspaces, by collecting unlabeled samples that are close to each other andhighly likely all fall into the same class. The corresponding class label isthen assigned by minimizing a cost function which reflects the overlap andtopological structure consistency between subspaces across source and targetdomains, and within anchor subspaces, respectively.We further combine theanchor subspaces to corresponding source subspaces to construct the compactjoint subspaces. Subsequently, one-vs-rest SVM classifiers are trained in thecompact joint subspaces and applied to unlabeled data in the target domain. Weevaluate the proposed method on two widely used datasets: object recognitiondataset for computer vision tasks, and sentiment classification dataset fornatural language processing tasks. Comparison results demonstrate that theproposed method outperforms the comparison methods on both datasets.
arxiv-12300-225 | TabletGaze: Unconstrained Appearance-based Gaze Estimation in Mobile Tablets | http://arxiv.org/pdf/1508.01244v2.pdf | author:Qiong Huang, Ashok Veeraraghavan, Ashutosh Sabharwal category:cs.CV published:2015-08-05 summary:We study gaze estimation on tablets; our key design goal is uncalibrated gazeestimation using the front-facing camera during natural use of tablets, wherethe posture and method of holding the tablet is not constrained. We collectedthe first large unconstrained gaze dataset of tablet users, labeled RiceTabletGaze dataset. The dataset consists of 51 subjects, each with 4 differentpostures and 35 gaze locations. Subjects vary in race, gender and in their needfor prescription glasses, all of which might impact gaze estimation accuracy.Driven by our observations on the collected data, we present a TabletGazealgorithm for automatic gaze estimation using multi-level HoG feature andRandom Forests regressor. The TabletGaze algorithm achieves a mean error of3.17 cm. We perform extensive evaluation on the impact of various factors suchas dataset size, race, wearing glasses and user posture on the gaze estimationaccuracy and make important observations about the impact of these factors.
arxiv-12300-226 | Algorithm and Theoretical Analysis for Domain Adaptation Feature Learning with Linear Classifiers | http://arxiv.org/pdf/1509.01710v1.pdf | author:Wenhao Jiang, Feiping Nie, Fu-lai Korris Chung, Heng Huang category:cs.LG published:2015-09-05 summary:Domain adaptation problem arises in a variety of applications where thetraining set (\textit{source} domain) and testing set (\textit{target} domain)follow different distributions. The difficulty of such learning problem lies inhow to bridge the gap between the source distribution and target distribution.In this paper, we give an formal analysis of feature learning algorithms fordomain adaptation with linear classifiers. Our analysis shows that in order toachieve good adaptation performance, the second moments of source domaindistribution and target domain distribution should be similar. Based on such aresult, a new linear feature learning algorithm for domain adaptation isdesigned and proposed. Furthermore, the new algorithm is extended to havemultiple layers, resulting in becoming another linear feature learningalgorithm. The newly introduced method is effective for the domain adaptationtasks on Amazon review dataset and spam dataset from ECML/PKDD 2006 discoverychallenge.
arxiv-12300-227 | On Graphical Models via Univariate Exponential Family Distributions | http://arxiv.org/pdf/1301.4183v2.pdf | author:Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Zhandong Liu category:math.ST stat.ML stat.TH published:2013-01-17 summary:Undirected graphical models, or Markov networks, are a popular class ofstatistical models, used in a wide variety of applications. Popular instancesof this class include Gaussian graphical models and Ising models. In manysettings, however, it might not be clear which subclass of graphical models touse, particularly for non-Gaussian and non-categorical data. In this paper, weconsider a general sub-class of graphical models where the node-wiseconditional distributions arise from exponential families. This allows us toderive multivariate graphical model distributions from univariate exponentialfamily distributions, such as the Poisson, negative binomial, and exponentialdistributions. Our key contributions include a class of M-estimators to fitthese graphical model distributions; and rigorous statistical analysis showingthat these M-estimators recover the true graphical model structure exactly,with high probability. We provide examples of genomic and proteomic networkslearned via instances of our class of graphical models derived from Poisson andexponential distributions.
arxiv-12300-228 | Gravitational Clustering | http://arxiv.org/pdf/1509.01659v1.pdf | author:Armen Aghajanyan category:cs.LG published:2015-09-05 summary:The downfall of many supervised learning algorithms, such as neural networks,is the inherent need for a large amount of training data. Although there is alot of buzz about big data, there is still the problem of doing classificationfrom a small dataset. Other methods such as support vector machines, althoughcapable of dealing with few samples, are inherently binary classifiers, and arein need of learning strategies such as One vs All in the case ofmulti-classification. In the presence of a large number of classes this canbecome problematic. In this paper we present, a novel approach to supervisedlearning through the method of clustering. Unlike traditional methods such asK-Means, Gravitational Clustering does not require the initial number ofclusters, and automatically builds the clusters, individual samples can bearbitrarily weighted and it requires only few samples while staying resilientto over-fitting.
arxiv-12300-229 | Co-interest Person Detection from Multiple Wearable Camera Videos | http://arxiv.org/pdf/1509.01654v1.pdf | author:Yuewei Lin, Kareem Ezzeldeen, Youjie Zhou, Xiaochuan Fan, Hongkai Yu, Hui Qian, Song Wang category:cs.CV published:2015-09-05 summary:Wearable cameras, such as Google Glass and Go Pro, enable video datacollection over larger areas and from different views. In this paper, we tacklea new problem of locating the co-interest person (CIP), i.e., the one who drawsattention from most camera wearers, from temporally synchronized videos takenby multiple wearable cameras. Our basic idea is to exploit the motion patternsof people and use them to correlate the persons across different videos,instead of performing appearance-based matching as in traditional videoco-segmentation/localization. This way, we can identify CIP even if a group ofpeople with similar appearance are present in the view. More specifically, wedetect a set of persons on each frame as the candidates of the CIP and thenbuild a Conditional Random Field (CRF) model to select the one with consistentmotion patterns in different videos and high spacial-temporal consistency ineach video. We collect three sets of wearable-camera videos for testing theproposed algorithm. All the involved people have similar appearances in thecollected videos and the experiments demonstrate the effectiveness of theproposed algorithm.
arxiv-12300-230 | Stochastic gradient variational Bayes for gamma approximating distributions | http://arxiv.org/pdf/1509.01631v1.pdf | author:David A. Knowles category:stat.ML published:2015-09-04 summary:While stochastic variational inference is relatively well known for scalinginference in Bayesian probabilistic models, related methods also offer ways tocircumnavigate the approximation of analytically intractable expectations. Thekey challenge in either setting is controlling the variance of gradientestimates: recent work has shown that for continuous latent variables,particularly multivariate Gaussians, this can be achieved by using the gradientof the log posterior. In this paper we apply the same idea to gamma distributedlatent variables given gamma variational distributions, enablingstraightforward "black box" variational inference in models where sparsity andnon-negativity are appropriate. We demonstrate the method on a recentlyproposed gamma process model for network data, as well as a novel sparse factoranalysis. We outperform generic sampling algorithms and the approach of usingGaussian variational distributions on transformed variables.
arxiv-12300-231 | Chebyshev and Conjugate Gradient Filters for Graph Image Denoising | http://arxiv.org/pdf/1509.01624v1.pdf | author:Dong Tian, Hassan Mansour, Andrew Knyazev, Anthony Vetro category:cs.CV published:2015-09-04 summary:In 3D image/video acquisition, different views are often captured withvarying noise levels across the views. In this paper, we propose a graph-basedimage enhancement technique that uses a higher quality view to enhance adegraded view. A depth map is utilized as auxiliary information to match theperspectives of the two views. Our method performs graph-based filtering of thenoisy image by directly computing a projection of the image to be filtered ontoa lower dimensional Krylov subspace of the graph Laplacian. We discuss twograph spectral denoising methods: first using Chebyshev polynomials, and secondusing iterations of the conjugate gradient algorithm. Our framework generalizespreviously known polynomial graph filters, and we demonstrate through numericalsimulations that our proposed technique produces subjectively cleaner imageswith about 1-3 dB improvement in PSNR over existing polynomial graph filters.
arxiv-12300-232 | Semantic Video Segmentation : Exploring Inference Efficiency | http://arxiv.org/pdf/1509.02441v1.pdf | author:Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen category:cs.CV published:2015-09-04 summary:We explore the efficiency of the CRF inference beyond image level semanticsegmentation and perform joint inference in video frames. The key idea is tocombine best of two worlds: semantic co-labeling and more expressive models.Our formulation enables us to perform inference over ten thousand images withinseconds and makes the system amenable to perform video semantic segmentationmost effectively. On CamVid dataset, with TextonBoost unaries, our proposedmethod achieves up to 8% improvement in accuracy over individual semantic imagesegmentation without additional time overhead. The source code is available athttps://github.com/subtri/video_inference
arxiv-12300-233 | Efficient Sampling for k-Determinantal Point Processes | http://arxiv.org/pdf/1509.01618v1.pdf | author:Chengtao Li, Stefanie Jegelka, Suvrit Sra category:cs.LG published:2015-09-04 summary:Determinantal Point Processes (DPPs) provide probabilistic models overdiscrete sets of items that help model repulsion and diversity. Applicabilityof DPPs to large sets of data is, however, hindered by the expensive matrixoperations involved, especially when sampling. We therefore propose a newefficient approximate two-stage sampling algorithm for discrete k-DPPs. Asopposed to previous approximations, our algorithm aims at minimizing thevariational distance to the original distribution. Experiments indicate thatthe resulting sampling algorithm works well on large data and yields moreaccurate samples than previous approaches.
arxiv-12300-234 | Object Recognition from Short Videos for Robotic Perception | http://arxiv.org/pdf/1509.01602v1.pdf | author:Ivan Bogun, Anelia Angelova, Navdeep Jaitly category:cs.CV I.5.4 published:2015-09-04 summary:Deep neural networks have become the primary learning technique for objectrecognition. Videos, unlike still images, are temporally coherent which makesthe application of deep networks non-trivial. Here, we investigate how motioncan aid object recognition in short videos. Our approach is based on LongShort-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs,we implement each gate as a convolution. We show that convolutional-based LSTMmodels are capable of learning motion dependencies and are able to improve therecognition accuracy when more frames in a sequence are available. We evaluateour approach on the Washington RGBD Object dataset and on the Washington RGBDScenes dataset. Our approach outperforms deep nets applied to still images andsets a new state-of-the-art in this domain.
arxiv-12300-235 | Regret vs. Communication: Distributed Stochastic Multi-Armed Bandits and Beyond | http://arxiv.org/pdf/1504.03509v2.pdf | author:Shuang Liu, Cheng Chen, Zhihua Zhang category:cs.LG published:2015-04-14 summary:In this paper, we consider the distributed stochastic multi-armed banditproblem, where a global arm set can be accessed by multiple playersindependently. The players are allowed to exchange their history ofobservations with each other at specific points in time. We study therelationship between regret and communication. When the time horizon is known,we propose the Over-Exploration strategy, which only requires one-roundcommunication and whose regret does not scale with the number of players. Whenthe time horizon is unknown, we measure the frequency of communication througha new notion called the density of the communication set, and give an exactcharacterization of the interplay between regret and communication.Specifically, a lower bound is established and stable strategies that match thelower bound are developed. The results and analyses in this paper are specificbut can be translated into more general settings.
arxiv-12300-236 | Particle approximations of the score and observed information matrix for parameter estimation in state space models with linear computational cost | http://arxiv.org/pdf/1306.0735v3.pdf | author:Christopher Nemeth, Paul Fearnhead, Lyudmila Mihaylova category:stat.CO stat.ML published:2013-06-04 summary:Poyiadjis et al. (2011) show how particle methods can be used to estimateboth the score and the observed information matrix for state space models.These methods either suffer from a computational cost that is quadratic in thenumber of particles, or produce estimates whose variance increasesquadratically with the amount of data. This paper introduces an alternativeapproach for estimating these terms at a computational cost that is linear inthe number of particles. The method is derived using a combination of kerneldensity estimation, to avoid the particle degeneracy that causes thequadratically increasing variance, and Rao-Blackwellisation. Crucially, we showthe method is robust to the choice of bandwidth within the kernel densityestimation, as it has good asymptotic properties regardless of this choice. Ourestimates of the score and observed information matrix can be used within bothonline and batch procedures for estimating parameters for state space models.Empirical results show improved parameter estimates compared to existingmethods at a significantly reduced computational cost. Supplementary materialsincluding code are available.
arxiv-12300-237 | Conjugate Gradient Acceleration of Non-Linear Smoothing Filters | http://arxiv.org/pdf/1509.01514v1.pdf | author:Andrew Knyazev, Alexander Malyshev category:cs.CV published:2015-09-04 summary:The most efficient signal edge-preserving smoothing filters, e.g., fordenoising, are non-linear. Thus, their acceleration is challenging and is oftenperformed in practice by tuning filter parameters, such as by increasing thewidth of the local smoothing neighborhood, resulting in more aggressivesmoothing of a single sweep at the cost of increased edge blurring. We proposean alternative technology, accelerating the original filters without tuning, byrunning them through a special conjugate gradient method, not affecting theirquality. The filter non-linearity is dealt with by careful freezing andrestarting. Our initial numerical experiments on toy one-dimensional signalsdemonstrate 20x acceleration of the classical bilateral filter and 3-5xacceleration of the recently developed guided filter.
arxiv-12300-238 | NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining | http://arxiv.org/pdf/1509.03503v1.pdf | author:Marco Winkler category:cs.SI cs.CV cs.DS physics.soc-ph published:2015-09-04 summary:The detection of triadic subgraph motifs is a common methodology incomplex-networks research. The procedure usually applied in order to detectmotifs evaluates whether a certain subgraph pattern is overrepresented in anetwork as a whole. However, motifs do not necessarily appear frequently inevery region of a graph. For this reason, we recently introduced the frameworkof Node-Specific Pattern Mining (NoSPaM). This work is a manual for animplementation of NoSPaM which can be downloaded from www.mwinkler.eu.
arxiv-12300-239 | Quantization based Fast Inner Product Search | http://arxiv.org/pdf/1509.01469v1.pdf | author:Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, David Simcha category:cs.AI cs.LG stat.ML published:2015-09-04 summary:We propose a quantization based approach for fast approximate Maximum InnerProduct Search (MIPS). Each database vector is quantized in multiple subspacesvia a set of codebooks, learned directly by minimizing the inner productquantization error. Then, the inner product of a query to a database vector isapproximated as the sum of inner products with the subspace quantizers.Different from recently proposed LSH approaches to MIPS, the database vectorsand queries do not need to be augmented in a higher dimensional feature space.We also provide a theoretical analysis of the proposed approach, consisting ofthe concentration results under mild assumptions. Furthermore, if a smallsample of example queries is given at the training time, we propose a modifiedcodebook learning procedure which further improves the accuracy. Experimentalresults on a variety of datasets including those arising from deep neuralnetworks show that the proposed approach significantly outperforms the existingstate-of-the-art.
arxiv-12300-240 | On Computing the Translations Norm in the Epipolar Graph | http://arxiv.org/pdf/1503.03637v3.pdf | author:Federica Arrigoni, Beatrice Rossi, Andrea Fusiello category:cs.CV published:2015-03-12 summary:This paper deals with the problem of recovering the unknown norm of relativetranslations between cameras based on the knowledge of relative rotations andtranslation directions. We provide theoretical conditions for the solvabilityof such a problem, and we propose a two-stage method to solve it. First, acycle basis for the epipolar graph is computed, then all the scaling factorsare recovered simultaneously by solving a homogeneous linear system. Wedemonstrate the accuracy of our solution by means of synthetic and realexperiments.
arxiv-12300-241 | Coordinate Descent Methods for Symmetric Nonnegative Matrix Factorization | http://arxiv.org/pdf/1509.01404v1.pdf | author:Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong, Inderjit Dhillon category:cs.NA cs.CV cs.LG math.OC stat.ML published:2015-09-04 summary:Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrixfactorization (symNMF) is the problem of finding a nonnegative matrix $H$,usually with much fewer columns than $A$, such that $A \approx HH^T$. SymNMFcan be used for data analysis and in particular for various clustering tasks.In this paper, we propose simple and very efficient coordinate descent schemesto solve this problem, and that can handle large and sparse input matrices. Theeffectiveness of our methods is illustrated on synthetic and real-world datasets, and we show that they perform favorably compared to recentstate-of-the-art methods.
arxiv-12300-242 | Predicting SLA Violations in Real Time using Online Machine Learning | http://arxiv.org/pdf/1509.01386v1.pdf | author:Jawwad Ahmed, Andreas Johnsson, Rerngvit Yanggratoke, John Ardelius, Christofer Flinta, Rolf Stadler category:cs.NI cs.LG cs.SE stat.ML published:2015-09-04 summary:Detecting faults and SLA violations in a timely manner is critical fortelecom providers, in order to avoid loss in business, revenue and reputation.At the same time predicting SLA violations for user services in telecomenvironments is difficult, due to time-varying user demands and infrastructureload conditions. In this paper, we propose a service-agnostic online learning approach,whereby the behavior of the system is learned on the fly, in order to predictclient-side SLA violations. The approach uses device-level metrics, which arecollected in a streaming fashion on the server side. Our results show that the approach can produce highly accurate predictions(>90% classification accuracy and < 10% false alarm rate) in scenarios whereSLA violations are predicted for a video-on-demand service under changing loadpatterns. The paper also highlight the limitations of traditional offlinelearning methods, which perform significantly worse in many of the consideredscenarios.
arxiv-12300-243 | A statistical shape space model of the palate surface trained on 3D MRI scans of the vocal tract | http://arxiv.org/pdf/1602.07679v1.pdf | author:Alexander Hewer, Ingmar Steiner, Timo Bolkart, Stefanie Wuhrer, Korin Richmond category:cs.CV published:2015-09-04 summary:We describe a minimally-supervised method for computing a statistical shapespace model of the palate surface. The model is created from a corpus ofvolumetric magnetic resonance imaging (MRI) scans collected from 12 speakers.We extract a 3D mesh of the palate from each speaker, then train the modelusing principal component analysis (PCA). The palate model is then tested using3D MRI from another corpus and evaluated using a high-resolution optical scan.We find that the error is low even when only a handful of measured coordinatesare available. In both cases, our approach yields promising results. It can beapplied to extract the palate shape from MRI data, and could be useful to otheranalysis modalities, such as electromagnetic articulography (EMA) andultrasound tongue imaging (UTI).
arxiv-12300-244 | CNN Based Hashing for Image Retrieval | http://arxiv.org/pdf/1509.01354v1.pdf | author:Jinma Guo, Jianmin Li category:cs.CV cs.LG I.2.6; H.3.1 published:2015-09-04 summary:Along with data on the web increasing dramatically, hashing is becoming moreand more popular as a method of approximate nearest neighbor search. Previoussupervised hashing methods utilized similarity/dissimilarity matrix to getsemantic information. But the matrix is not easy to construct for a newdataset. Rather than to reconstruct the matrix, we proposed a straightforwardCNN-based hashing method, i.e. binarilizing the activations of a fullyconnected layer with threshold 0 and taking the binary result as hash codes.This method achieved the best performance on CIFAR-10 and was comparable withthe state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested thatthe signs of activations may carry more information than the relative values ofactivations between samples, and that the co-adaption between feature extractorand hash functions is important for hashing.
arxiv-12300-245 | Diffusion-KLMS Algorithm and its Performance Analysis for Non-Linear Distributed Networks | http://arxiv.org/pdf/1509.01352v1.pdf | author:Rangeet Mitra, Vimal Bhatia category:cs.LG cs.DC cs.IT cs.SY math.IT published:2015-09-04 summary:In a distributed network environment, the diffusion-least mean squares (LMS)algorithm gives faster convergence than the original LMS algorithm. It has alsobeen observed that, the diffusion-LMS generally outperforms other distributedLMS algorithms like spatial LMS and incremental LMS. However, both the originalLMS and diffusion-LMS are not applicable in non-linear environments where datamay not be linearly separable. A variant of LMS called kernel-LMS (KLMS) hasbeen proposed in the literature for such non-linearities. In this paper, wepropose kernelised version of diffusion-LMS for non-linear distributedenvironments. Simulations show that the proposed approach has superiorconvergence as compared to algorithms of the same genre. We also introduce atechnique to predict the transient and steady-state behaviour of the proposedalgorithm. The techniques proposed in this work (or algorithms of same genre)can be easily extended to distributed parameter estimation applications likecooperative spectrum sensing and massive multiple input multiple output (MIMO)receiver design which are potential components for 5G communication systems.
arxiv-12300-246 | Parallel and Distributed Approaches for Graph Based Semi-supervised Learning | http://arxiv.org/pdf/1509.01349v1.pdf | author:Konstantin Avrachenkov, Vivek Borkar, Krishnakant Saboo category:cs.LG published:2015-09-04 summary:Two approaches for graph based semi-supervised learning are proposed. Thefirstapproach is based on iteration of an affine map. A key element of theaffine map iteration is sparsematrix-vector multiplication, which has severalvery efficient parallel implementations. The secondapproach belongs to theclass of Markov Chain Monte Carlo (MCMC) algorithms. It is based onsampling ofnodes by performing a random walk on the graph. The latter approach isdistributedby its nature and can be easily implemented on several processors orover the network. Boththeoretical and practical evaluations are provided. It isfound that the nodes are classified intotheir class with very small error. Thesampling algorithm's ability to track new incoming nodesand to classify them isalso demonstrated.
arxiv-12300-247 | Deep Broad Learning - Big Models for Big Data | http://arxiv.org/pdf/1509.01346v1.pdf | author:Nayyar A. Zaidi, Geoffrey I. Webb, Mark J. Carman, Francois Petitjean category:cs.LG published:2015-09-04 summary:Deep learning has demonstrated the power of detailed modeling of complexhigh-order (multivariate) interactions in data. For some learning tasks thereis power in learning models that are not only Deep but also Broad. By Broad, wemean models that incorporate evidence from large numbers of features. This isof especial value in applications where many different features andcombinations of features all carry small amounts of information about theclass. The most accurate models will integrate all that information. In thispaper, we propose an algorithm for Deep Broad Learning called DBL. The proposedalgorithm has a tunable parameter $n$, that specifies the depth of the model.It provides straightforward paths towards out-of-core learning for large data.We demonstrate that DBL learns models from large quantities of data withaccuracy that is highly competitive with the state-of-the-art.
arxiv-12300-248 | Learning Temporal Alignment Uncertainty for Efficient Event Detection | http://arxiv.org/pdf/1509.01343v1.pdf | author:Iman Abbasnejad, Sridha Sridharan, Simon Denman, Clinton Fookes, Simon Lucey category:cs.CV published:2015-09-04 summary:In this paper we tackle the problem of efficient video event detection. Weargue that linear detection functions should be preferred in this regard due totheir scalability and efficiency during estimation and evaluation. A popularapproach in this regard is to represent a sequence using a bag of words (BOW)representation due to its: (i) fixed dimensionality irrespective of thesequence length, and (ii) its ability to compactly model the statistics in thesequence. A drawback to the BOW representation, however, is the intrinsicdestruction of the temporal ordering information. In this paper we propose anew representation that leverages the uncertainty in relative temporalalignments between pairs of sequences while not destroying temporal ordering.Our representation, like BOW, is of a fixed dimensionality making it easilyintegrated with a linear detection function. Extensive experiments on CK+,6DMG, and UvA-NEMO databases show significant performance improvements acrossboth isolated and continuous event detection tasks.
arxiv-12300-249 | Semantic Amodal Segmentation | http://arxiv.org/pdf/1509.01329v1.pdf | author:Yan Zhu, Yuandong Tian, Dimitris Mexatas, Piotr Dollár category:cs.CV published:2015-09-04 summary:Common visual recognition tasks such as classification, object detection, andsemantic segmentation are rapidly reaching maturity, and given the recent rateof progress, it is not unreasonable to conjecture that techniques for many ofthese problems will approach human levels of performance in the next few years.In this paper we look to the future: what is the next frontier in visualrecognition? We offer one possible answer to this question. We propose a detailed imageannotation that captures information beyond the visible pixels and requirescomplex reasoning about full scene structure. Specifically, we create an amodalsegmentation of each image: the full extent of each region is marked, not justthe visible pixels. Annotators outline and name all salient regions in theimage and specify a partial depth order. The result is a rich scene structure,including visible and occluded portions of each region, figure-ground edgeinformation, semantic labels, and object overlap. To date, we have labeled 500 images in the BSDS dataset with at least fiveannotators per image. Critically, the resulting full scene annotation issurprisingly consistent between annotators. For example, for edge detection ourannotations have substantially higher human consistency than the original BSDSedges while providing a greater challenge for existing algorithms. We arecurrently annotating ~5000 images from the MS COCO dataset.
arxiv-12300-250 | l1-norm Penalized Orthogonal Forward Regression | http://arxiv.org/pdf/1509.01323v1.pdf | author:Xia Hong, Sheng Chen, Yi Guo, Junbin Gao category:cs.LG stat.ML published:2015-09-04 summary:A l1-norm penalized orthogonal forward regression (l1-POFR) algorithm isproposed based on the concept of leaveone- out mean square error (LOOMSE).Firstly, a new l1-norm penalized cost function is defined in the constructedorthogonal space, and each orthogonal basis is associated with an individuallytunable regularization parameter. Secondly, due to orthogonal computation, theLOOMSE can be analytically computed without actually splitting the data set,and moreover a closed form of the optimal regularization parameter in terms ofminimal LOOMSE is derived. Thirdly, a lower bound for regularization parametersis proposed, which can be used for robust LOOMSE estimation by adaptivelydetecting and removing regressors to an inactive set so that the computationalcost of the algorithm is significantly reduced. Illustrative examples areincluded to demonstrate the effectiveness of this new l1-POFR approach.
arxiv-12300-251 | Linear Global Translation Estimation with Feature Tracks | http://arxiv.org/pdf/1503.01832v2.pdf | author:Zhaopeng Cui, Nianjuan Jiang, Chengzhou Tang, Ping Tan category:cs.CV published:2015-03-06 summary:This paper derives a novel linear position constraint for cameras seeing acommon scene point, which leads to a direct linear method for global cameratranslation estimation. Unlike previous solutions, this method deals withcollinear camera motion and weak image association at the same time. The finallinear formulation does not involve the coordinates of scene points, whichmakes it efficient even for large scale data. We solve the linear equationbased on $L_1$ norm, which makes our system more robust to outliers inessential matrices and feature correspondences. We experiment this method onboth sequentially captured images and unordered Internet images. Theexperiments demonstrate its strength in robustness, accuracy, and efficiency.
arxiv-12300-252 | The influence of Chunking on Dependency Crossing and Distance | http://arxiv.org/pdf/1509.01310v1.pdf | author:Qian Lu, Chunshan Xu, Haitao Liu category:cs.CL published:2015-09-03 summary:This paper hypothesizes that chunking plays important role in reducingdependency distance and dependency crossings. Computer simulations, whencompared with natural languages,show that chunking reduces mean dependencydistance (MDD) of a linear sequence of nodes (constrained by continuity orprojectivity) to that of natural languages. More interestingly, chunking alonebrings about less dependency crossings as well, though having failed to reducethem, to such rarity as found in human languages. These results suggest thatchunking may play a vital role in the minimization of dependency distance, anda somewhat contributing role in the rarity of dependency crossing. In addition,the results point to a possibility that the rarity of dependency crossings isnot a mere side-effect of minimization of dependency distance, but a linguisticphenomenon with its own motivations.
arxiv-12300-253 | Multivariate Spearman's rho for aggregating ranks using copulas | http://arxiv.org/pdf/1410.4391v2.pdf | author:Justin Bedo, Cheng Soon Ong category:stat.ML cs.LG published:2014-10-16 summary:We study the problem of rank aggregation: given a set of ranked lists, wewant to form a consensus ranking. Furthermore, we consider the case of extremelists: i.e., only the rank of the best or worst elements are known. We imputemissing ranks by the average value and generalise Spearman's \rho to extremeranks. Our main contribution is the derivation of a non-parametric estimatorfor rank aggregation based on multivariate extensions of Spearman's \rho, whichmeasures correlation between a set of ranked lists. Multivariate Spearman's\rho is defined using copulas, and we show that the geometric mean ofnormalised ranks maximises multivariate correlation. Motivated by this, wepropose a weighted geometric mean approach for learning to rank which has aclosed form least squares solution. When only the best or worst elements of aranked list are known, we impute the missing ranks by the average value,allowing us to apply Spearman's \rho. Finally, we demonstrate good performanceon the rank aggregation benchmarks MQ2007 and MQ2008.
arxiv-12300-254 | Incremental Active Opinion Learning Over a Stream of Opinionated Documents | http://arxiv.org/pdf/1509.01288v1.pdf | author:Max Zimmermann, Eirini Ntoutsi, Myra Spiliopoulou category:cs.IR cs.CL cs.LG published:2015-09-03 summary:Applications that learn from opinionated documents, like tweets or productreviews, face two challenges. First, the opinionated documents constitute anevolving stream, where both the author's attitude and the vocabulary itself maychange. Second, labels of documents are scarce and labels of words areunreliable, because the sentiment of a word depends on the (unknown) context inthe author's mind. Most of the research on mining over opinionated streamsfocuses on the first aspect of the problem, whereas for the second a continuoussupply of labels from the stream is assumed. Such an assumption though isutopian as the stream is infinite and the labeling cost is prohibitive. To thisend, we investigate the potential of active stream learning algorithms that askfor labels on demand. Our proposed ACOSTREAM 1 approach works with limitedlabels: it uses an initial seed of labeled documents, occasionally requestsadditional labels for documents from the human expert and incrementally adaptsto the underlying stream while exploiting the available labeled documents. Inits core, ACOSTREAM consists of a MNB classifier coupled with "sampling"strategies for requesting class labels for new unlabeled documents. In theexperiments, we evaluate the classifier performance over time by varying: (a)the class distribution of the opinionated stream, while assuming that the setof the words in the vocabulary is fixed but their polarities may change withthe class distribution; and (b) the number of unknown words arriving at eachmoment, while the class polarity may also change. Our results show that activelearning on a stream of opinionated documents, delivers good performance whilerequiring a small selection of labels
arxiv-12300-255 | Image Classification with Rejection using Contextual Information | http://arxiv.org/pdf/1509.01287v1.pdf | author:Filipe Condessa, José Bioucas-Dias, Carlos Castro, John Ozolek, Jelena Kovačević category:cs.CV 68T10 published:2015-09-03 summary:We introduce a new supervised algorithm for image classification withrejection using multiscale contextual information. Rejection is desired inimage-classification applications that require a robust classifier but not theclassification of the entire image. The proposed algorithm combines local andmultiscale contextual information with rejection, improving the classificationperformance. As a probabilistic model for classification, we adopt amultinomial logistic regression. The concept of rejection with contextualinformation is implemented by modeling the classification problem as an energyminimization problem over a graph representing local and multiscalesimilarities of the image. The rejection is introduced through an energy dataterm associated with the classification risk and the contextual informationthrough an energy smoothness term associated with the local and multiscalesimilarities within the image. We illustrate the proposed method on theclassification of images of H&E-stained teratoma tissues.
arxiv-12300-256 | Probabilistic Neural Network Training for Semi-Supervised Classifiers | http://arxiv.org/pdf/1509.01271v1.pdf | author:Hamidreza Farhidzadeh category:cs.LG published:2015-09-03 summary:In this paper, we propose another version of help-training approach byemploying a Probabilistic Neural Network (PNN) that improves the performance ofthe main discriminative classifier in the semi-supervised strategy. Weintroduce the PNN-training algorithm and use it for training the support vectormachine (SVM) with a few numbers of labeled data and a large number ofunlabeled data. We try to find the best labels for unlabeled data and then useSVM to enhance the classification rate. We test our method on two famousbenchmarks and show the efficiency of our method in comparison with perviousmethods.
arxiv-12300-257 | Machine Learning Methods to Analyze Arabidopsis Thaliana Plant Root Growth | http://arxiv.org/pdf/1509.01270v1.pdf | author:Hamidreza Farhidzadeh category:cs.LG published:2015-09-03 summary:One of the challenging problems in biology is to classify plants based ontheir reaction on genetic mutation. Arabidopsis Thaliana is a plant that is sointeresting, because its genetic structure has some similarities with that ofhuman beings. Biologists classify the type of this plant to mutated and notmutated (wild) types. Phenotypic analysis of these types is a time-consumingand costly effort by individuals. In this paper, we propose a modified featureextraction step by using velocity and acceleration of root growth. In thesecond step, for plant classification, we employed different Support VectorMachine (SVM) kernels and two hybrid systems of neural networks. Gated NegativeCorrelation Learning (GNCL) and Mixture of Negatively Correlated Experts (MNCE)are two ensemble methods based on complementary feature of classicalclassifiers; Mixture of Expert (ME) and Negative Correlation Learning (NCL).The hybrid systems conserve of advantages and decrease the effects ofdisadvantages of NCL and ME. Our Experimental shows that MNCE and GNCL improvethe efficiency of classical classifiers, however, some SVM kernels function hasbetter performance than classifiers based on neural network ensemble method.Moreover, kernels consume less time to obtain a classification rate.
arxiv-12300-258 | A Neural Attention Model for Abstractive Sentence Summarization | http://arxiv.org/pdf/1509.00685v2.pdf | author:Alexander M. Rush, Sumit Chopra, Jason Weston category:cs.CL cs.AI published:2015-09-02 summary:Summarization based on text extraction is inherently limited, butgeneration-style abstractive methods have proven challenging to build. In thiswork, we propose a fully data-driven approach to abstractive sentencesummarization. Our method utilizes a local attention-based model that generateseach word of the summary conditioned on the input sentence. While the model isstructurally simple, it can easily be trained end-to-end and scales to a largeamount of training data. The model shows significant performance gains on theDUC-2004 shared task compared with several strong baselines.
arxiv-12300-259 | Toward a generic representation of random variables for machine learning | http://arxiv.org/pdf/1506.00976v2.pdf | author:Gautier Marti, Philippe Very, Philippe Donnat category:cs.LG stat.ML published:2015-06-02 summary:This paper presents a pre-processing and a distance which improve theperformance of machine learning algorithms working on independent andidentically distributed stochastic processes. We introduce a novelnon-parametric approach to represent random variables which splits apartdependency and distribution without losing any information. We also propound anassociated metric leveraging this representation and its statistical estimate.Besides experiments on synthetic datasets, the benefits of our contribution isillustrated through the example of clustering financial time series, forinstance prices from the credit default swaps market. Results are available onthe website www.datagrapple.com and an IPython Notebook tutorial is availableat www.datagrapple.com/Tech for reproducible research.
arxiv-12300-260 | Community Detection in Networks with Node Features | http://arxiv.org/pdf/1509.01173v1.pdf | author:Yuan Zhang, Elizaveta Levina, Ji Zhu category:stat.ML cs.SI physics.soc-ph published:2015-09-03 summary:Many methods have been proposed for community detection in networks, but mostof them do not take into account additional information on the nodes that isoften available in practice. In this paper, we propose a new joint communitydetection criterion that uses both the network edge information and the nodefeatures to detect community structures. One advantage our method has overexisting joint detection approaches is the flexibility of learning the impactof different features which may differ across communities. Another advantage isthe flexibility of choosing the amount of influence the feature information hason communities. The method is asymptotically consistent under the block modelwith additional assumptions on the feature distributions, and performs well onsimulated and real networks.
arxiv-12300-261 | Semi-described and semi-supervised learning with Gaussian processes | http://arxiv.org/pdf/1509.01168v1.pdf | author:Andreas Damianou, Neil D. Lawrence category:stat.ML cs.AI cs.LG math.PR 60G15, 58E30 G.3; I.2.6 published:2015-09-03 summary:Propagating input uncertainty through non-linear Gaussian process (GP)mappings is intractable. This hinders the task of training GPs using uncertainand partially observed inputs. In this paper we refer to this task as"semi-described learning". We then introduce a GP framework that solves both,the semi-described and the semi-supervised learning problems (where missingvalues occur in the outputs). Auto-regressive state space simulation is alsorecognised as a special case of semi-described learning. To achieve our goal wedevelop variational methods for handling semi-described inputs in GPs, andcouple them with algorithms that allow for imputing the missing values whiletreating the uncertainty in a principled, Bayesian manner. Extensiveexperiments on simulated and real-world data study the problems of iterativeforecasting and regression/classification with missing values. The resultssuggest that the principled propagation of uncertainty stemming from ourframework can significantly improve performance in these tasks.
arxiv-12300-262 | Monolingually Derived Phrase Scores for Phrase Based SMT Using Neural Networks Vector Representations | http://arxiv.org/pdf/1506.00406v2.pdf | author:Amir Pouya Aghasadeghi, Mohadeseh Bastan, Shahram Khadivi category:cs.CL published:2015-06-01 summary:In this paper, we propose two new features for estimating phrase-basedmachine translation parameters from mainly monolingual data. Our method isbased on two recently introduced neural network vector representation modelsfor words and sentences. It is the first time that these models have been usedin an end to end phrase-based machine translation system. Scores obtained fromour method can recover more than 80% of BLEU loss caused by removing phrasetable probabilities. We also show that our features combined with the phrasetable probabilities improve the BLEU score by absolute 0.74 points.
arxiv-12300-263 | Training of CC4 Neural Network with Spread Unary Coding | http://arxiv.org/pdf/1509.01126v1.pdf | author:Pushpa Sree Potluri category:cs.NE published:2015-09-03 summary:This paper adapts the corner classification algorithm (CC4) to train theneural networks using spread unary inputs. This is an important problem asspread unary appears to be at the basis of data representation in biologicallearning. The modified CC4 algorithm is tested using the pattern classificationexperiment and the results are found to be good. Specifically, we show that thenumber of misclassified points is not particularly sensitive to the chosenradius of generalization.
arxiv-12300-264 | Vision-Based Road Detection using Contextual Blocks | http://arxiv.org/pdf/1509.01122v1.pdf | author:Caio César Teodoro Mendes, Vincent Frémont, Denis Fernando Wolf category:cs.CV published:2015-09-03 summary:Road detection is a fundamental task in autonomous navigation systems. Inthis paper, we consider the case of monocular road detection, where images aresegmented into road and non-road regions. Our starting point is the well-knownmachine learning approach, in which a classifier is trained to distinguish roadand non-road regions based on hand-labeled images. We proceed by introducingthe use of "contextual blocks" as an efficient way of providing contextualinformation to the classifier. Overall, the proposed methodology, including itsimage feature selection and classifier, was conceived with computational costin mind, leaving room for optimized implementations. Regarding experiments, weperform a sensible evaluation of each phase and feature subset that composesour system. The results show a great benefit from using contextual blocks anddemonstrate their computational efficiency. Finally, we submit our results tothe KITTI road detection benchmark achieving scores comparable with state ofthe art methods.
arxiv-12300-265 | A tree-based kernel for graphs with continuous attributes | http://arxiv.org/pdf/1509.01116v1.pdf | author:Giovanni Da San Martino, Nicolò Navarin, Alessandro Sperduti category:cs.LG published:2015-09-03 summary:The availability of graph data with node attributes that can be eitherdiscrete or real-valued is constantly increasing. While existing kernel methodsare effective techniques for dealing with graphs having discrete node labels,their adaptation to non-discrete or continuous node attributes has beenlimited, mainly for computational issues. Recently, a few kernels especiallytailored for this domain, have been proposed. In order to alleviate thecomputational problems, the size of the feature space of such kernels tend tobe smaller than the ones of the kernels for discrete node attributes. However,such choice might have a negative impact on the predictive performance. In thispaper, we propose a graph kernel for complex and continuous nodes' attributes,whose features are tree structures extracted from specific graph visits.Experimental results obtained on real-world datasets show that the(approximated version of the) proposed kernel is comparable with currentstate-of-the-art kernels in terms of classification accuracy while requiringshorter running times.
arxiv-12300-266 | A Novice Guide towards Human Motion Analysis and Understanding | http://arxiv.org/pdf/1509.01074v1.pdf | author:Ahmed Nabil Mohamed category:cs.CV published:2015-09-03 summary:Human motion analysis and understanding has been, and is still, the focus ofattention of many disciplines which is considered an obvious indicator of thewide and massive importance of the subject. The purpose of this article is toshed some light on this very important subject, so it can be a good insight fora novice computer vision researcher in this field by providing him/her with awealth of knowledge about the subject covering many directions. There are twomain contributions of this article. The first one investigates various aspectsof some disciplines (e.g., arts, philosophy, psychology, and neuroscience) thatare interested in the subject and review some of their contributions stressingon those that can be useful for computer vision researchers. Moreover, manyexamples are illustrated to indicate the benefits of integrating concepts andresults among different disciplines. The second contribution is concerned withthe subject from the computer vision aspect where we discuss the followingissues. First, we explore many demanding and promising applications to revealthe wide and massive importance of the field. Second, we list various types ofsensors that may be used for acquiring various data. Third, we review differenttaxonomies used for classifying motions. Fourth, we review various processesinvolved in motion analysis. Fifth, we exhibit how different surveys arestructured. Sixth, we examine many of the most cited and recent reviews in thefield that have been published during the past two decades to reveal variousapproaches used for implementing different stages of the problem and refer tovarious algorithms and their suitability for different situations. Moreover, weprovide a long list of public datasets and discuss briefly some examples ofthese datasets. Finally, we provide a general discussion of the subject fromthe aspect of computer vision.
arxiv-12300-267 | Training a Restricted Boltzmann Machine for Classification by Labeling Model Samples | http://arxiv.org/pdf/1509.01053v1.pdf | author:Malte Probst, Franz Rothlauf category:cs.LG published:2015-09-03 summary:We propose an alternative method for training a classification model. Usingthe MNIST set of handwritten digits and Restricted Boltzmann Machines, it ispossible to reach a classification performance competitive to semi-supervisedlearning if we first train a model in an unsupervised fashion on unlabeled dataonly, and then manually add labels to model samples instead of training datasamples with the help of a GUI. This approach can benefit from the fact thatmodel samples can be presented to the human labeler in a video-like fashion,resulting in a higher number of labeled examples. Also, after some initialtraining, hard-to-classify examples can be distinguished from easy onesautomatically, saving manual work.
arxiv-12300-268 | Extending local features with contextual information in graph kernels | http://arxiv.org/pdf/1507.02186v2.pdf | author:Nicolò Navarin, Alessandro Sperduti, Riccardo Tesselli category:cs.LG published:2015-07-08 summary:Graph kernels are usually defined in terms of simpler kernels over localsubstructures of the original graphs. Different kernels consider differenttypes of substructures. However, in some cases they have similar predictiveperformances, probably because the substructures can be interpreted asapproximations of the subgraphs they induce. In this paper, we propose toassociate to each feature a piece of information about the context in which thefeature appears in the graph. A substructure appearing in two different graphswill match only if it appears with the same context in both graphs. We proposea kernel based on this idea that considers trees as substructures, and wherethe contexts are features too. The kernel is inspired from the framework in[6], even if it is not part of it. We give an efficient algorithm for computingthe kernel and show promising results on real-world graph classificationdatasets.
arxiv-12300-269 | Generating Weather Forecast Texts with Case Based Reasoning | http://arxiv.org/pdf/1509.01023v1.pdf | author:Ibrahim Adeyanju category:cs.AI cs.CL published:2015-09-03 summary:Several techniques have been used to generate weather forecast texts. In thispaper, case based reasoning (CBR) is proposed for weather forecast textgeneration because similar weather conditions occur over time and should havesimilar forecast texts. CBR-METEO, a system for generating weather forecasttexts was developed using a generic framework (jCOLIBRI) which provides modulesfor the standard components of the CBR architecture. The advantage in a CBRapproach is that systems can be built in minimal time with far less humaneffort after initial consultation with experts. The approach depends heavily onthe goodness of the retrieval and revision components of the CBR process. Weevaluated CBRMETEO with NIST, an automated metric which has been shown tocorrelate well with human judgements for this domain. The system showscomparable performance with other NLG systems that perform the same task.
arxiv-12300-270 | Sampling-based Causal Inference in Cue Combination and its Neural Implementation | http://arxiv.org/pdf/1509.00998v1.pdf | author:Zhaofei Yu, Feng Chen, Jianwu Dong, Qionghai Dai category:cs.NE q-bio.NC published:2015-09-03 summary:Causal inference in cue combination is to decide whether the cues have asingle cause or multiple causes. Although the Bayesian causal inference modelexplains the problem of causal inference in cue combination successfully, howcausal inference in cue combination could be implemented by neural circuits, isunclear. The existing method based on calculating log posterior ratio withvariable elimination has the problem of being unrealistic and task-specific. Inthis paper, we take advantages of the special structure of the Bayesian causalinference model and propose a hierarchical inference algorithm based onimportance sampling. A simple neural circuit is designed to implement theproposed inference algorithm. Theoretical analyses and experimental resultsdemonstrate that our algorithm converges to the accurate value as the samplesize goes to infinite. Moreover, the neural circuit we design can be easilygeneralized to implement inference for other problems, such as themulti-stimuli cause inference and the same-different judgment.
arxiv-12300-271 | Sequential Design for Ranking Response Surfaces | http://arxiv.org/pdf/1509.00980v1.pdf | author:Ruimeng Hu, Mike Ludkovski category:stat.ML q-fin.CP stat.CO published:2015-09-03 summary:We propose and analyze sequential design methods for the problem of rankingseveral response surfaces. Namely, given $L \ge 2$ response surfaces over acontinuous input space $\cal X$, the aim is to efficiently find the index ofthe minimal response across the entire $\cal X$. The response surfaces are notknown and have to be noisily sampled one-at-a-time. This setting is motivatedby stochastic control applications and requires joint experimental design bothin space and response-index dimensions. To generate sequential designheuristics we investigate stepwise uncertainty reduction approaches, as well assampling based on posterior classification complexity. We also make connectionsbetween our continuous-input formulation and the discrete framework of pureregret in multi-armed bandits. To model the response surfaces we utilizekriging surrogates. Several numerical examples using both synthetic data and anepidemics control problem are provided to illustrate our approach and theefficacy of respective adaptive designs.
arxiv-12300-272 | A Reconfigurable Mixed-signal Implementation of a Neuromorphic ADC | http://arxiv.org/pdf/1509.00967v1.pdf | author:Ying Xu, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Runchun Wang, Andre van Schaik category:cs.NE published:2015-09-03 summary:We present a neuromorphic Analogue-to-Digital Converter (ADC), which usesintegrate-and-fire (I&F) neurons as the encoders of the analogue signal, withmodulated inhibitions to decohere the neuronal spikes trains. The architectureconsists of an analogue chip and a control module. The analogue chip comprisestwo scan chains and a twodimensional integrate-and-fire neuronal array.Individual neurons are accessed via the chains one by one without any encoderdecoder or arbiter. The control module is implemented on an FPGA (FieldProgrammable Gate Array), which sends scan enable signals to the scan chainsand controls the inhibition for individual neurons. Since the control module isimplemented on an FPGA, it can be easily reconfigured. Additionally, we proposea pulse width modulation methodology for the lateral inhibition, which makesuse of different pulse widths indicating different strengths of inhibition foreach individual neuron to decohere neuronal spikes. Software simulations inthis paper tested the robustness of the proposed ADC architecture to fixedrandom noise. A circuit simulation using ten neurons shows the performance andthe feasibility of the architecture.
arxiv-12300-273 | On TimeML-Compliant Temporal Expression Extraction in Turkish | http://arxiv.org/pdf/1509.00963v1.pdf | author:Dilek Küçük, Doğan Küçük category:cs.CL published:2015-09-03 summary:It is commonly acknowledged that temporal expression extractors are importantcomponents of larger natural language processing systems like informationretrieval and question answering systems. Extraction and normalization oftemporal expressions in Turkish has not been given attention so far except theextraction of some date and time expressions within the course of named entityrecognition. As TimeML is the current standard of temporal expression and eventannotation in natural language texts, in this paper, we present an analysis oftemporal expressions in Turkish based on the related TimeML classification(i.e., date, time, duration, and set expressions). We have created a lexiconfor Turkish temporal expressions and devised considerably wide-coveragepatterns using the lexical classes as the building blocks. We believe that theproposed patterns, together with convenient normalization rules, can be readilyused by prospective temporal expression extraction tools for Turkish.
arxiv-12300-274 | A compact aVLSI conductance-based silicon neuron | http://arxiv.org/pdf/1509.00962v1.pdf | author:Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE published:2015-09-03 summary:We present an analogue Very Large Scale Integration (aVLSI) implementationthat uses first-order lowpass filters to implement a conductance-based siliconneuron for high-speed neuromorphic systems. The aVLSI neuron consists of a soma(cell body) and a single synapse, which is capable of linearly summing both theexcitatory and inhibitory postsynaptic potentials (EPSP and IPSP) generated bythe spikes arriving from different sources. Rather than biasing the siliconneuron with different parameters for different spiking patterns, as istypically done, we provide digital control signals, generated by an FPGA, tothe silicon neuron to obtain different spiking behaviours. The proposed neuronis only ~26.5 um2 in the IBM 130nm process and thus can be integrated at veryhigh density. Circuit simulations show that this neuron can emulate differentspiking behaviours observed in biological neurons.
arxiv-12300-275 | Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE) | http://arxiv.org/pdf/1501.05427v4.pdf | author:Maurizio Filippone, Raphael Engler category:stat.ME stat.CO stat.ML published:2015-01-22 summary:In applications of Gaussian processes where quantification of uncertainty isof primary interest, it is necessary to accurately characterize the posteriordistribution over covariance parameters. This paper proposes an adaptation ofthe Stochastic Gradient Langevin Dynamics algorithm to draw samples from theposterior distribution over covariance parameters with negligible bias andwithout the need to compute the marginal likelihood. In Gaussian processregression, this has the enormous advantage that stochastic gradients can becomputed by solving linear systems only. A novel unbiased linear systems solverbased on parallelizable covariance matrix-vector products is developed toaccelerate the unbiased estimation of gradients. The results demonstrate thepossibility to enable scalable and exact (in a Monte Carlo sense)quantification of uncertainty in Gaussian processes without imposing anyspecial structure on the covariance or reducing the number of input vectors.
arxiv-12300-276 | Histogram of Oriented Principal Components for Cross-View Action Recognition | http://arxiv.org/pdf/1409.6813v2.pdf | author:Hossein Rahmani, Arif Mahmood, Du Huynh, Ajmal Mian category:cs.CV published:2014-09-24 summary:Existing techniques for 3D action recognition are sensitive to viewpointvariations because they extract features from depth images which are viewpointdependent. In contrast, we directly process pointclouds for cross-view actionrecognition from unknown and unseen views. We propose the Histogram of OrientedPrincipal Components (HOPC) descriptor that is robust to noise, viewpoint,scale and action speed variations. At a 3D point, HOPC is computed byprojecting the three scaled eigenvectors of the pointcloud within its localspatio-temporal support volume onto the vertices of a regular dodecahedron.HOPC is also used for the detection of Spatio-Temporal Keypoints (STK) in 3Dpointcloud sequences so that view-invariant STK descriptors (or Local HOPCdescriptors) at these key locations only are used for action recognition. Wealso propose a global descriptor computed from the normalized spatio-temporaldistribution of STKs in 4-D, which we refer to as STK-D. We have evaluated theperformance of our proposed descriptors against nine existing techniques on twocross-view and three single-view human action recognition datasets. TheExperimental results show that our techniques provide significant improvementover state-of-the-art methods.
arxiv-12300-277 | Nonparametric Independence Testing for Small Sample Sizes | http://arxiv.org/pdf/1406.1922v2.pdf | author:Aaditya Ramdas, Leila Wehbe category:stat.ML published:2014-06-07 summary:This paper deals with the problem of nonparametric independence testing, afundamental decision-theoretic problem that asks if two arbitrary (possiblymultivariate) random variables $X,Y$ are independent or not, a question thatcomes up in many fields like causality and neuroscience. While quantities likecorrelation of $X,Y$ only test for (univariate) linear independence, naturalalternatives like mutual information of $X,Y$ are hard to estimate due to aserious curse of dimensionality. A recent approach, avoiding both issues,estimates norms of an \textit{operator} in Reproducing Kernel Hilbert Spaces(RKHSs). Our main contribution is strong empirical evidence that by employing\textit{shrunk} operators when the sample size is small, one can attain animprovement in power at low false positive rates. We analyze the effects ofStein shrinkage on a popular test statistic called HSIC (Hilbert-SchmidtIndependence Criterion). Our observations provide insights into two recentlyproposed shrinkage estimators, SCOSE and FCOSE - we prove that SCOSE is(essentially) the optimal linear shrinkage method for \textit{estimating} thetrue operator; however, the non-linearly shrunk FCOSE usually achieves greaterimprovements in \textit{test power}. This work is important for more powerfulnonparametric detection of subtle nonlinear dependencies for small samples.
arxiv-12300-278 | Manipulated Object Proposal: A Discriminative Object Extraction and Feature Fusion Framework for First-Person Daily Activity Recognition | http://arxiv.org/pdf/1509.00651v2.pdf | author:Changzhi Luo, Bingbing Ni, Jun Yuan, Jianfeng Wang, Shuicheng Yan, Meng Wang category:cs.CV published:2015-09-02 summary:Detecting and recognizing objects interacting with humans lie in the centerof first-person (egocentric) daily activity recognition. However, due to noisycamera motion and frequent changes in viewpoint and scale, most of the previousegocentric action recognition methods fail to capture and model highlydiscriminative object features. In this work, we propose a novel pipeline forfirst-person daily activity recognition, aiming at more discriminative objectfeature representation and object-motion feature fusion. Our object featureextraction and representation pipeline is inspired by the recent success ofobject hypotheses and deep convolutional neural network based detectionframeworks. Our key contribution is a simple yet effective manipulated objectproposal generation scheme. This scheme leverages motion cues such as motionboundary and motion magnitude (in contrast, camera motion is usually consideredas "noise" for most previous methods) to generate a more compact anddiscriminative set of object proposals, which are more closely related to theobjects which are being manipulated. Then, we learn more discriminative objectdetectors from these manipulated object proposals based on region-basedconvolutional neural network (R-CNN). Meanwhile, we develop a network basedfeature fusion scheme which better combines object and motion features. We showin experiments that the proposed framework significantly outperforms thestate-of-the-art recognition performance on a challenging first-person dailyactivity benchmark.
arxiv-12300-279 | Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging | http://arxiv.org/pdf/1509.00816v1.pdf | author:Suren Jayasuriya, Adithya Pediredla, Sriram Sivaramakrishnan, Alyosha Molnar, Ashok Veeraraghavan category:cs.CV published:2015-09-02 summary:A variety of techniques such as light field, structured illumination, andtime-of-flight (TOF) are commonly used for depth acquisition in consumerimaging, robotics and many other applications. Unfortunately, each techniquesuffers from its individual limitations preventing robust depth sensing. Inthis paper, we explore the strengths and weaknesses of combining light fieldand time-of-flight imaging, particularly the feasibility of an on-chipimplementation as a single hybrid depth sensor. We refer to this combination asdepth field imaging. Depth fields combine light field advantages such assynthetic aperture refocusing with TOF imaging advantages such as high depthresolution and coded signal processing to resolve multipath interference. Weshow applications including synthesizing virtual apertures for TOF imaging,improved depth mapping through partial and scattering occluders, and singlefrequency TOF phase unwrapping. Utilizing space, angle, and temporal coding,depth fields can improve depth sensing in the wild and generate new insightsinto the dimensions of light's plenoptic function.
arxiv-12300-280 | Steps Toward Deep Kernel Methods from Infinite Neural Networks | http://arxiv.org/pdf/1508.05133v2.pdf | author:Tamir Hazan, Tommi Jaakkola category:cs.LG cs.NE published:2015-08-20 summary:Contemporary deep neural networks exhibit impressive results on practicalproblems. These networks generalize well although their inherent capacity mayextend significantly beyond the number of training examples. We analyze thisbehavior in the context of deep, infinite neural networks. We show that deepinfinite layers are naturally aligned with Gaussian processes and kernelmethods, and devise stochastic kernels that encode the information of thesenetworks. We show that stability results apply despite the size, offering anexplanation for their empirical success.
arxiv-12300-281 | Posterior calibration and exploratory analysis for natural language processing models | http://arxiv.org/pdf/1508.05154v2.pdf | author:Khanh Nguyen, Brendan O'Connor category:cs.CL published:2015-08-21 summary:Many models in natural language processing define probabilistic distributionsover linguistic structures. We argue that (1) the quality of a model' sposterior distribution can and should be directly evaluated, as to whetherprobabilities correspond to empirical frequencies, and (2) NLP uncertainty canbe projected not only to pipeline components, but also to exploratory dataanalysis, telling a user when to trust and not trust the NLP analysis. Wepresent a method to analyze calibration, and apply it to compare themiscalibration of several commonly used models. We also contribute acoreference sampling algorithm that can create confidence intervals for apolitical event extraction task.
arxiv-12300-282 | Finding Near-Optimal Independent Sets at Scale | http://arxiv.org/pdf/1509.00764v1.pdf | author:Sebastian Lamm, Peter Sanders, Christian Schulz, Darren Strash, Renato F. Werneck category:cs.DS cs.NE cs.SI F.2.2; G.2.2 published:2015-09-02 summary:The independent set problem is NP-hard and particularly difficult to solve inlarge sparse graphs. In this work, we develop an advanced evolutionaryalgorithm, which incorporates kernelization techniques to compute largeindependent sets in huge sparse networks. A recent exact algorithm has shownthat large networks can be solved exactly by employing a branch-and-reducetechnique that recursively kernelizes the graph and performs branching.However, one major drawback of their algorithm is that, for huge graphs,branching still can take exponential time. To avoid this problem, werecursively choose vertices that are likely to be in a large independent set(using an evolutionary approach), then further kernelize the graph. We showthat identifying and removing vertices likely to be in large independent setsopens up the reduction space---which not only speeds up the computation oflarge independent sets drastically, but also enables us to compute high-qualityindependent sets on much larger instances than previously reported in theliterature.
arxiv-12300-283 | Reliable ABC model choice via random forests | http://arxiv.org/pdf/1406.6288v3.pdf | author:Pierre Pudlo, Jean-Michel Marin, Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier, Christian P. Robert category:stat.ML q-bio.PE stat.CO stat.ME published:2014-06-24 summary:Approximate Bayesian computation (ABC) methods provide an elaborate approachto Bayesian inference on complex models, including model choice. Boththeoretical arguments and simulation experiments indicate, however, that modelposterior probabilities may be poorly evaluated by standard ABC techniques. Wepropose a novel approach based on a machine learning tool named random foreststo conduct selection among the highly complex models covered by ABC algorithms.We thus modify the way Bayesian model selection is both understood andoperated, in that we rephrase the inferential goal as a classification problem,first predicting the model that best fits the data with random forests andpostponing the approximation of the posterior probability of the predicted MAPfor a second stage also relying on random forests. Compared with earlierimplementations of ABC model choice, the ABC random forest approach offersseveral potential improvements: (i) it often has a larger discriminative poweramong the competing models, (ii) it is more robust against the number andchoice of statistics summarizing the data, (iii) the computing effort isdrastically reduced (with a gain in computation efficiency of at least fifty),and (iv) it includes an approximation of the posterior probability of theselected model. The call to random forests will undoubtedly extend the range ofsize of datasets and complexity of models that ABC can handle. We illustratethe power of this novel methodology by analyzing controlled experiments as wellas genuine population genetics datasets. The proposed methodologies areimplemented in the R package abcrf available on the CRAN.
arxiv-12300-284 | On Transitive Consistency for Linear Invertible Transformations between Euclidean Coordinate Systems | http://arxiv.org/pdf/1509.00728v1.pdf | author:Johan Thunberg, Florian Bernard, Jorge Goncalves category:math.OC cs.CV cs.MA cs.NA stat.ML published:2015-09-02 summary:Transitive consistency is an intrinsic property for collections of linearinvertible transformations between Euclidean coordinate frames. In practice,when the transformations are estimated from data, this property is lacking.This work addresses the problem of synchronizing transformations that are nottransitively consistent. Once the transformations have been synchronized, theysatisfy the transitive consistency condition - a transformation from frame $A$to frame $C$ is equal to the composite transformation of first transforming Ato B and then transforming B to C. The coordinate frames correspond to nodes ina graph and the transformations correspond to edges in the same graph. Twodirect or centralized synchronization methods are presented for different graphtopologies; the first one for quasi-strongly connected graphs, and the secondone for connected graphs. As an extension of the second method, an iterativeGauss-Newton method is presented, which is later adapted to the case of affineand Euclidean transformations. Two distributed synchronization methods are alsopresented for orthogonal matrices, which can be seen as distributed versions ofthe two direct or centralized methods; they are similar in nature to standardconsensus protocols used for distributed averaging. When the transformationsare orthogonal matrices, a bound on the optimality gap can be computed.Simulations show that the gap is almost right, even for noise large inmagnitude. This work also contributes on a theoretical level by providinglinear algebraic relationships for transitively consistent transformations. Oneof the benefits of the proposed methods is their simplicity - basic linearalgebraic methods are used, e.g., the Singular Value Decomposition (SVD). For awide range of parameter settings, the methods are numerically validated.
arxiv-12300-285 | Heavy-tailed Independent Component Analysis | http://arxiv.org/pdf/1509.00727v1.pdf | author:Joseph Anderson, Navin Goyal, Anupama Nandi, Luis Rademacher category:cs.LG math.ST stat.CO stat.ML stat.TH published:2015-09-02 summary:Independent component analysis (ICA) is the problem of efficiently recoveringa matrix $A \in \mathbb{R}^{n\times n}$ from i.i.d. observations of $X=AS$where $S \in \mathbb{R}^n$ is a random vector with mutually independentcoordinates. This problem has been intensively studied, but all existingefficient algorithms with provable guarantees require that the coordinates$S_i$ have finite fourth moments. We consider the heavy-tailed ICA problemwhere we do not make this assumption, about the second moment. This problemalso has received considerable attention in the applied literature. In thepresent work, we first give a provably efficient algorithm that works under theassumption that for constant $\gamma > 0$, each $S_i$ has finite$(1+\gamma)$-moment, thus substantially weakening the moment requirementcondition for the ICA problem to be solvable. We then give an algorithm thatworks under the assumption that matrix $A$ has orthogonal columns but requiresno moment assumptions. Our techniques draw ideas from convex geometry andexploit standard properties of the multivariate spherical Gaussian distributionin a novel way.
arxiv-12300-286 | Quasi-Newton particle Metropolis-Hastings | http://arxiv.org/pdf/1502.03656v2.pdf | author:Johan Dahlin, Fredrik Lindsten, Thomas B. Schön category:stat.CO q-fin.CP stat.ML published:2015-02-12 summary:Particle Metropolis-Hastings enables Bayesian parameter inference in generalnonlinear state space models (SSMs). However, in many implementations a randomwalk proposal is used and this can result in poor mixing if not tuned correctlyusing tedious pilot runs. Therefore, we consider a new proposal inspired byquasi-Newton algorithms that may achieve similar (or better) mixing with lesstuning. An advantage compared to other Hessian based proposals, is that it onlyrequires estimates of the gradient of the log-posterior. A possible applicationis parameter inference in the challenging class of SSMs with intractablelikelihoods. We exemplify this application and the benefits of the new proposalby modelling log-returns of future contracts on coffee by a stochasticvolatility model with $\alpha$-stable observations.
arxiv-12300-287 | Dictionary based Approach to Edge Detection | http://arxiv.org/pdf/1509.00714v1.pdf | author:Nitish Chandra, Kedar Khare category:cs.CV published:2015-09-02 summary:Edge detection is a very essential part of image processing, as quality andaccuracy of detection determines the success of further processing. We havedeveloped a new self learning technique for edge detection using dictionarycomprised of eigenfilters constructed using features of the input image. Thedictionary based method eliminates the need of pre or post processing of theimage and accounts for noise, blurriness, class of image and variation ofillumination during the detection process itself. Since, this method depends onthe characteristics of the image, the new technique can detect edges moreaccurately and capture greater detail than existing algorithms such as Sobel,Prewitt Laplacian of Gaussian, Canny method etc which use generic filters andoperators. We have demonstrated its application on various classes of imagessuch as text, face, barcodes, traffic and cell images. An application of thistechnique to cell counting in a microscopic image is also presented.
arxiv-12300-288 | Analysis of Communication Pattern with Scammers in Enron Corpus | http://arxiv.org/pdf/1509.00705v1.pdf | author:Dinesh Balaji Sashikanth category:cs.CL published:2015-09-02 summary:This paper is an exploratory analysis into fraud detection taking Enron emailcorpus as the case study. The paper posits conclusions like strict servitudeand unquestionable faith among employees as breeding grounds for sham amonghigher executives. We also try to infer on the nature of communication betweenfraudulent employees and between non- fraudulent-fraudulent employees
arxiv-12300-289 | Replication and Generalization of PRECISE | http://arxiv.org/pdf/1508.01306v2.pdf | author:Michael Minock, Nils Everling category:cs.CL cs.AI cs.DB published:2015-08-06 summary:This report describes an initial replication study of the PRECISE system anddevelops a clearer, more formal description of the approach. Based on ourevaluation, we conclude that the PRECISE results do not fully replicate.However the formalization developed here suggests a road map to further enhanceand extend the approach pioneered by PRECISE. After a long, productive discussion with Ana-Maria Popescu (one of theauthors of PRECISE) we got more clarity on the PRECISE approach and how thelexicon was authored for the GEO evaluation. Based on this we built a moredirect implementation over a repaired formalism. Although our new evaluation isnot yet complete, it is clear that the system is performing much better now. Wewill continue developing our ideas and implementation and generate a futurereport/publication that more accurately evaluates PRECISE like approaches.
arxiv-12300-290 | A Neural Algorithm of Artistic Style | http://arxiv.org/pdf/1508.06576v2.pdf | author:Leon A. Gatys, Alexander S. Ecker, Matthias Bethge category:cs.CV cs.NE q-bio.NC published:2015-08-26 summary:In fine art, especially painting, humans have mastered the skill to createunique visual experiences through composing a complex interplay between thecontent and style of an image. Thus far the algorithmic basis of this processis unknown and there exists no artificial system with similar capabilities.However, in other key areas of visual perception such as object and facerecognition near-human performance was recently demonstrated by a class ofbiologically inspired vision models called Deep Neural Networks. Here weintroduce an artificial system based on a Deep Neural Network that createsartistic images of high perceptual quality. The system uses neuralrepresentations to separate and recombine content and style of arbitraryimages, providing a neural algorithm for the creation of artistic images.Moreover, in light of the striking similarities between performance-optimisedartificial neural networks and biological vision, our work offers a pathforward to an algorithmic understanding of how humans create and perceiveartistic imagery.
arxiv-12300-291 | A hybrid COA-DEA method for solving multi-objective problems | http://arxiv.org/pdf/1509.00595v1.pdf | author:Mahdi Gorjestani, Elham Shadkam, Mehdi Parvizi, Sajedeh Aminzadegan category:math.OC cs.NE published:2015-09-02 summary:The Cuckoo optimization algorithm (COA) is developed for solvingsingle-objective problems and it cannot be used for solving multi-objectiveproblems. So the multi-objective cuckoo optimization algorithm based on dataenvelopment analysis (DEA) is developed in this paper and it can gain theefficient Pareto frontiers. This algorithm is presented by the CCR model of DEAand the output-oriented approach of it. The selection criterion is higherefficiency for next iteration of the proposed hybrid method. So the profitfunction of the COA is replaced by the efficiency value that is obtained fromDEA. This algorithm is compared with other methods using some test problems.The results shows using COA and DEA approach for solving multi-objectiveproblems increases the speed and the accuracy of the generated solutions.
arxiv-12300-292 | Exploring Online Ad Images Using a Deep Convolutional Neural Network Approach | http://arxiv.org/pdf/1509.00568v1.pdf | author:Michael Fire, Jonathan Schler category:cs.CV published:2015-09-02 summary:Online advertising is a huge, rapidly growing advertising market in today'sworld. One common form of online advertising is using image ads. A decision ismade (often in real time) every time a user sees an ad, and the advertiser iseager to determine the best ad to display. Consequently, many algorithms havebeen developed that calculate the optimal ad to show to the current user at thepresent time. Typically, these algorithms focus on variations of the ad,optimizing among different properties such as background color, image size, orset of images. However, there is a more fundamental layer. Our study looks atnew qualities of ads that can be determined before an ad is shown (rather thanonline optimization) and defines which ads are most likely to be successful. We present a set of novel algorithms that utilize deep-learning imageprocessing, machine learning, and graph theory to investigate onlineadvertising and to construct prediction models which can foresee an image ad'ssuccess. We evaluated our algorithms on a dataset with over 260,000 ad images,as well as a smaller dataset specifically related to the automotive industry,and we succeeded in constructing regression models for ad image click rateprediction. The obtained results emphasize the great potential of usingdeep-learning algorithms to effectively and efficiently analyze image ads andto create better and more innovative online ads. Moreover, the algorithmspresented in this paper can help predict ad success and can be applied toanalyze other large-scale image corpora.
arxiv-12300-293 | Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems | http://arxiv.org/pdf/1405.2875v2.pdf | author:Chien-Ju Ho, Aleksandrs Slivkins, Jennifer Wortman Vaughan category:cs.DS cs.GT cs.LG published:2014-05-12 summary:Crowdsourcing markets have emerged as a popular platform for matchingavailable workers with tasks to complete. The payment for a particular task istypically set by the task's requester, and may be adjusted based on the qualityof the completed work, for example, through the use of "bonus" payments. Inthis paper, we study the requester's problem of dynamically adjustingquality-contingent payments for tasks. We consider a multi-round version of thewell-known principal-agent model, whereby in each round a worker makes astrategic choice of the effort level which is not directly observable by therequester. In particular, our formulation significantly generalizes thebudget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each "arm"representing a potential contract. To cope with the large (and in fact,infinite) number of arms, we propose a new algorithm, AgnosticZooming, whichdiscretizes the contract space into a finite number of regions, effectivelytreating each region as a single arm. This discretization is adaptivelyrefined, so that more promising regions of the contract space are eventuallydiscretized more finely. We analyze this algorithm, showing that it achievesregret sublinear in the time horizon and substantially improves overnon-adaptive discretization (which is the only competing approach in theliterature). Our results advance the state of art on several different topics: the theoryof crowdsourcing markets, principal-agent problems, multi-armed bandits, anddynamic pricing.
arxiv-12300-294 | A fast numerical method for max-convolution and the application to efficient max-product inference in Bayesian networks | http://arxiv.org/pdf/1501.02627v2.pdf | author:Oliver Serang category:cs.NA math.NA stat.CO stat.ME stat.ML published:2015-01-12 summary:Observations depending on sums of random variables are common throughout manyfields; however, no efficient solution is currently known for performingmax-product inference on these sums of general discrete distributions(max-product inference can be used to obtain maximum a posteriori estimates).The limiting step to max-product inference is the max-convolution problem(sometimes presented in log-transformed form and denoted as "infimalconvolution", "min-convolution", or "convolution on the tropical semiring"),for which no O(k log(k)) method is currently known. Here I present a O(klog(k)) numerical method for estimating the max-convolution of two nonnegativevectors (e.g., two probability mass functions), where k is the length of thelarger vector. This numerical max-convolution method is then demonstrated byperforming fast max-product inference on a convolution tree, a data structurefor performing fast inference given information on the sum of n discrete randomvariables in O(n k log(n k) log(n) ) steps (where each random variable has anarbitrary prior distribution on k contiguous possible states). The numericalmax-convolution method can be applied to specialized classes of hidden Markovmodels to reduce the runtime of computing the Viterbi path from n k^2 to n klog(k), and has potential application to the all-pairs shortest paths problem.
arxiv-12300-295 | Enhancement and Recognition of Reverberant and Noisy Speech by Extending Its Coherence | http://arxiv.org/pdf/1509.00533v1.pdf | author:Scott Wisdom, Thomas Powers, Les Atlas, James Pitton category:cs.SD cs.CL stat.AP published:2015-09-02 summary:Most speech enhancement algorithms make use of the short-time Fouriertransform (STFT), which is a simple and flexible time-frequency decompositionthat estimates the short-time spectrum of a signal. However, the duration ofshort STFT frames are inherently limited by the nonstationarity of speechsignals. The main contribution of this paper is a demonstration of speechenhancement and automatic speech recognition in the presence of reverberationand noise by extending the length of analysis windows. We accomplish thisextension by performing enhancement in the short-time fan-chirp transform(STFChT) domain, an overcomplete time-frequency representation that is coherentwith speech signals over longer analysis window durations than the STFT. Thisextended coherence is gained by using a linear model of fundamental frequencyvariation of voiced speech signals. Our approach centers around using asingle-channel minimum mean-square error log-spectral amplitude (MMSE-LSA)estimator proposed by Habets, which scales coefficients in a time-frequencydomain to suppress noise and reverberation. In the case of multiplemicrophones, we preprocess the data with either a minimum variancedistortionless response (MVDR) beamformer, or a delay-and-sum beamformer (DSB).We evaluate our algorithm on both speech enhancement and recognition tasks forthe REVERB challenge dataset. Compared to the same processing done in the STFTdomain, our approach achieves significant improvement in terms of objectiveenhancement metrics (including PESQ---the ITU-T standard measurement for speechquality). In terms of automatic speech recognition (ASR) performance asmeasured by word error rate (WER), our experiments indicate that the STFT witha long window is more effective for ASR.
arxiv-12300-296 | Sensor-Type Classification in Buildings | http://arxiv.org/pdf/1509.00498v1.pdf | author:Dezhi Hong, Jorge Ortiz, Arka Bhattacharya, Kamin Whitehouse category:cs.LG C.3 published:2015-09-01 summary:Many sensors/meters are deployed in commercial buildings to monitor andoptimize their performance. However, because sensor metadata is inconsistentacross buildings, software-based solutions are tightly coupled to the sensormetadata conventions (i.e. schemas and naming) for each building. Running thesame software across buildings requires significant integration effort. Metadata normalization is critical for scaling the deployment process andallows us to decouple building-specific conventions from the code written forbuilding applications. It also allows us to deal with missing metadata. Oneimportant aspect of normalization is to differentiate sensors by the typeofphenomena being observed. In this paper, we propose a general, simple, yeteffective classification scheme to differentiate sensors in buildings by type.We perform ensemble learning on data collected from over 2000 sensor streams intwo buildings. Our approach is able to achieve more than 92% accuracy forclassification within buildings and more than 82% accuracy for acrossbuildings. We also introduce a method for identifying potential misclassifiedstreams. This is important because it allows us to identify opportunities toattain more input from experts -- input that could help improve classificationaccuracy when ground truth is unavailable. We show that by adjusting athreshold value we are able to identify at least 30% of the misclassifiedinstances.
arxiv-12300-297 | On TD(0) with function approximation: Concentration bounds and a centered variant with exponential convergence | http://arxiv.org/pdf/1411.3224v2.pdf | author:Nathaniel Korda, L. A. Prashanth category:cs.LG math.OC stat.ML published:2014-11-12 summary:We provide non-asymptotic bounds for the well-known temporal differencelearning algorithm TD(0) with linear function approximators. These includehigh-probability bounds as well as bounds in expectation. Our analysis suggeststhat a step-size inversely proportional to the number of iterations cannotguarantee optimal rate of convergence unless we assume (partial) knowledge ofthe stationary distribution for the Markov chain underlying the policyconsidered. We also provide bounds for the iterate averaged TD(0) variant,which gets rid of the step-size dependency while exhibiting the optimal rate ofconvergence. Furthermore, we propose a variant of TD(0) with linearapproximators that incorporates a centering sequence, and establish that itexhibits an exponential rate of convergence in expectation. We demonstrate theusefulness of our bounds on two synthetic experimental settings.
arxiv-12300-298 | A Robust Regression Approach for Background/Foreground Segmentation | http://arxiv.org/pdf/1412.5126v2.pdf | author:Shervin Minaee, Haoping Yu, Yao Wang category:cs.CV published:2014-12-16 summary:Background/foreground segmentation has a lot of applications in image andvideo processing. In this paper, a segmentation algorithm is proposed which ismainly designed for text and line extraction in screen content. The proposedmethod makes use of the fact that the background in each block is usuallysmoothly varying and can be modeled well by a linear combination of a fewsmoothly varying basis functions, while the foreground text and graphics createsharp discontinuity. The algorithm separates the background and foregroundpixels by trying to fit pixel values in the block into a smooth function usinga robust regression method. The inlier pixels that can fit well will beconsidered as background, while remaining outlier pixels will be consideredforeground. This algorithm has been extensively tested on several images fromHEVC standard test sequences for screen content coding, and is shown to havesuperior performance over other methods, such as the k-means clustering basedsegmentation algorithm in DjVu. This background/foreground segmentation can beused in different applications such as: text extraction, separate coding ofbackground and foreground for compression of screen content and mixed contentdocuments, principle line extraction from palmprint and crease detection infingerprint images.
arxiv-12300-299 | Learning to Segment Object Candidates | http://arxiv.org/pdf/1506.06204v2.pdf | author:Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar category:cs.CV published:2015-06-20 summary:Recent object detection systems rely on two critical steps: (1) a set ofobject proposals is predicted as efficiently as possible, and (2) this set ofcandidate proposals is then passed to an object classifier. Such approacheshave been shown they can be fast, while achieving the state of the art indetection performance. In this paper, we propose a new way to generate objectproposals, introducing an approach based on a discriminative convolutionalnetwork. Our model is trained jointly with two objectives: given an imagepatch, the first part of the system outputs a class-agnostic segmentation mask,while the second part of the system outputs the likelihood of the patch beingcentered on a full object. At test time, the model is efficiently applied onthe whole test image and generates a set of segmentation masks, each of thembeing assigned with a corresponding object likelihood score. We show that ourmodel yields significant improvements over state-of-the-art object proposalalgorithms. In particular, compared to previous approaches, our model obtainssubstantially higher object recall using fewer proposals. We also show that ourmodel is able to generalize to unseen categories it has not seen duringtraining. Unlike all previous approaches for generating object masks, we do notrely on edges, superpixels, or any other form of low-level segmentation.
arxiv-12300-300 | Iterative hypothesis testing for multi-object tracking in presence of features with variable reliability | http://arxiv.org/pdf/1509.00313v1.pdf | author:Amit Kumar K. C., Damien Delannay, Christophe De Vleeschouwer category:cs.CV published:2015-09-01 summary:This paper assumes prior detections of multiple targets at each time instant,and uses a graph-based approach to connect those detections across time, basedon their position and appearance estimates. In contrast to most earlier worksin the field, our framework has been designed to exploit the appearancefeatures, even when they are only sporadically available, or affected by anon-stationary noise, along the sequence of detections. This is done byimplementing an iterative hypothesis testing strategy to progressivelyaggregate the detections into short trajectories, named tracklets.Specifically, each iteration considers a node, named key-node, and investigateshow to link this key-node with other nodes in its neighborhood, under theassumption that the target appearance is defined by the key-node appearanceestimate. This is done through shortest path computation in a temporalneighborhood of the key-node. The approach is conservative in that it onlyaggregates the shortest paths that are sufficiently better compared toalternative paths. It is also multi-scale in that the size of the investigatedneighborhood is increased proportionally to the number of detections alreadyaggregated into the key-node. The multi-scale nature of the process and theprogressive relaxation of its conservativeness makes it both computationallyefficient and effective. Experimental validations are performed extensively on a toy example, a 15minutes long multi-view basketball dataset, and other monocular pedestriandatasets.
