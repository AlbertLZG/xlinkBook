arxiv-12300-1 | A Simple Algorithm for Maximum Margin Classification, Revisited | http://arxiv.org/pdf/1507.01563v1.pdf | author:Sariel Har-Peled category:cs.LG published:2015-07-06 summary:In this note, we revisit the algorithm of Har-Peled et. al. [HRZ07] forcomputing a linear maximum margin classifier. Our presentation is selfcontained, and the algorithm itself is slightly simpler than the originalalgorithm. The algorithm itself is a simple Perceptron like iterativealgorithm. For more details and background, the reader is referred to theoriginal paper.
arxiv-12300-2 | Correspondence Factor Analysis of Big Data Sets: A Case Study of 30 Million Words; and Contrasting Analytics using Apache Solr and Correspondence Analysis in R | http://arxiv.org/pdf/1507.01529v1.pdf | author:Fionn Murtagh category:cs.CL 62H25, 62.07 G.3; H.2.8 published:2015-07-06 summary:We consider a large number of text data sets. These are cooking recipes. Termdistribution and other distributional properties of the data are investigated.Our aim is to look at various analytical approaches which allow for mining ofinformation on both high and low detail scales. Metric space embedding isfundamental to our interest in the semantic properties of this data. Weconsider the projection of all data into analyses of aggregated versions of thedata. We contrast that with projection of aggregated versions of the data intoanalyses of all the data. Analogously for the term set, we look at analysis ofselected terms. We also look at inherent term associations such as betweensingular and plural. In addition to our use of Correspondence Analysis in R,for latent semantic space mapping, we also use Apache Solr. Setting up the Solrserver and carrying out querying is described. A further novelty is thatquerying is supported in Solr based on the principal factor plane mapping ofall the data. This uses a bounding box query, based on factor projections.
arxiv-12300-3 | Reflections on Sentiment/Opinion Analysis | http://arxiv.org/pdf/1507.01636v1.pdf | author:Jiwei Li, Eduard Hovy category:cs.CL published:2015-07-06 summary:In this paper, we described possible directions for deeper understanding,helping bridge the gap between psychology / cognitive science and computationalapproaches in sentiment/opinion analysis literature. We focus on the opinionholder's underlying needs and their resultant goals, which, in a utilitarianmodel of sentiment, provides the basis for explaining the reason a sentimentvalence is held. While these thoughts are still immature, scattered,unstructured, and even imaginary, we believe that these perspectives mightsuggest fruitful avenues for various kinds of future work.
arxiv-12300-4 | Emphatic Temporal-Difference Learning | http://arxiv.org/pdf/1507.01569v1.pdf | author:A. Rupam Mahmood, Huizhen Yu, Martha White, Richard S. Sutton category:cs.LG cs.AI published:2015-07-06 summary:Emphatic algorithms are temporal-difference learning algorithms that changetheir effective state distribution by selectively emphasizing andde-emphasizing their updates on different time steps. Recent works by Sutton,Mahmood and White (2015), and Yu (2015) show that by varying the emphasis in aparticular way, these algorithms become stable and convergent under off-policytraining with linear function approximation. This paper serves as a unifiedsummary of the available results from both works. In addition, we demonstratethe empirical benefits from the flexibility of emphatic algorithms, includingstate-dependent discounting, state-dependent bootstrapping, and theuser-specified allocation of function approximation resources.
arxiv-12300-5 | Combining Models of Approximation with Partial Learning | http://arxiv.org/pdf/1507.01215v2.pdf | author:Ziyuan Gao, Frank Stephan, Sandra Zilles category:cs.LG 68Q32 published:2015-07-05 summary:In Gold's framework of inductive inference, the model of partial learningrequires the learner to output exactly one correct index for the target objectand only the target object infinitely often. Since infinitely many of thelearner's hypotheses may be incorrect, it is not obvious whether a partiallearner can be modifed to "approximate" the target object. Fulk and Jain (Approximate inference and scientific method. Information andComputation 114(2):179--191, 1994) introduced a model of approximate learningof recursive functions. The present work extends their research and solves anopen problem of Fulk and Jain by showing that there is a learner whichapproximates and partially identifies every recursive function by outputting asequence of hypotheses which, in addition, are also almost all finite variantsof the target function. The subsequent study is dedicated to the question how these findingsgeneralise to the learning of r.e. languages from positive data. Here threevariants of approximate learning will be introduced and investigated withrespect to the question whether they can be combined with partial learning.Following the line of Fulk and Jain's research, further investigations provideconditions under which partial language learners can eventually output onlyfinite variants of the target language. The combinabilities of other partiallearning criteria will also be briefly studied.
arxiv-12300-6 | Semi-supervised Multi-sensor Classification via Consensus-based Multi-View Maximum Entropy Discrimination | http://arxiv.org/pdf/1507.01269v1.pdf | author:Tianpei Xie, Nasser M. Nasrabadi, Alfred O. Hero III category:cs.IT cs.AI cs.LG math.IT published:2015-07-05 summary:In this paper, we consider multi-sensor classification when there is a largenumber of unlabeled samples. The problem is formulated under the multi-viewlearning framework and a Consensus-based Multi-View Maximum EntropyDiscrimination (CMV-MED) algorithm is proposed. By iteratively maximizing thestochastic agreement between multiple classifiers on the unlabeled dataset, thealgorithm simultaneously learns multiple high accuracy classifiers. Wedemonstrate that our proposed method can yield improved performance overprevious multi-view learning approaches by comparing performance on three realmulti-sensor data sets.
arxiv-12300-7 | Autoencoding the Retrieval Relevance of Medical Images | http://arxiv.org/pdf/1507.01251v1.pdf | author:Zehra Camlica, H. R. Tizhoosh, Farzad Khalvati category:cs.CV published:2015-07-05 summary:Content-based image retrieval (CBIR) of medical images is a crucial task thatcan contribute to a more reliable diagnosis if applied to big data. Recentadvances in feature extraction and classification have enormously improved CBIRresults for digital images. However, considering the increasing accessibilityof big data in medical imaging, we are still in need of reducing both memoryrequirements and computational expenses of image retrieval systems. This workproposes to exclude the features of image blocks that exhibit a low encodingerror when learned by a $n/p/n$ autoencoder ($p\!<\!n$). We examine thehistogram of autoendcoding errors of image blocks for each image class tofacilitate the decision which image regions, or roughly what percentage of animage perhaps, shall be declared relevant for the retrieval task. This leads toreduction of feature dimensionality and speeds up the retrieval process. Tovalidate the proposed scheme, we employ local binary patterns (LBP) and supportvector machines (SVM) which are both well-established approaches in CBIRresearch community. As well, we use IRMA dataset with 14,410 x-ray images astest data. The results show that the dimensionality of annotated featurevectors can be reduced by up to 50% resulting in speedups greater than 27% atexpense of less than 1% decrease in the accuracy of retrieval when validatingthe precision and recall of the top 20 hits.
arxiv-12300-8 | Learning Deep Neural Network Policies with Continuous Memory States | http://arxiv.org/pdf/1507.01273v2.pdf | author:Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO published:2015-07-05 summary:Policy learning for partially observed control tasks requires policies thatcan remember salient information from past observations. In this paper, wepresent a method for learning policies with internal memory forhigh-dimensional, continuous systems, such as robotic manipulators. Ourapproach consists of augmenting the state and action space of the system withcontinuous-valued memory states that the policy can read from and write to.Learning general-purpose policies with this type of memory representationdirectly is difficult, because the policy must automatically figure out themost salient information to memorize at each time step. We show that, bydecomposing this policy search problem into a trajectory optimization phase anda supervised learning phase through a method called guided policy search, wecan acquire policies with effective memorization and recall strategies.Intuitively, the trajectory optimization phase chooses the values of the memorystates that will make it easier for the policy to produce the right action infuture states, while the supervised learning phase encourages the policy to usememorization actions to produce those memory states. We evaluate our method ontasks involving continuous control in manipulation and navigation settings, andshow that our method can learn complex policies that successfully complete arange of tasks that require memory.
arxiv-12300-9 | Scalable Sparse Subspace Clustering by Orthogonal Matching Pursuit | http://arxiv.org/pdf/1507.01238v3.pdf | author:Chong You, Daniel P. Robinson, Rene Vidal category:cs.CV cs.LG stat.ML published:2015-07-05 summary:Subspace clustering methods based on $\ell_1$, $\ell_2$ or nuclear normregularization have become very popular due to their simplicity, theoreticalguarantees and empirical success. However, the choice of the regularizer cangreatly impact both theory and practice. For instance, $\ell_1$ regularizationis guaranteed to give a subspace-preserving affinity (i.e., there are noconnections between points from different subspaces) under broad conditions(e.g., arbitrary subspaces and corrupted data). However, it requires solving alarge scale convex optimization problem. On the other hand, $\ell_2$ andnuclear norm regularization provide efficient closed form solutions, butrequire very strong assumptions to guarantee a subspace-preserving affinity,e.g., independent subspaces and uncorrupted data. In this paper we study asubspace clustering method based on orthogonal matching pursuit. We show thatthe method is both computationally efficient and guaranteed to give asubspace-preserving affinity under broad conditions. Experiments on syntheticdata verify our theoretical analysis, and applications in handwritten digit andface clustering show that our approach achieves the best trade off betweenaccuracy and efficiency.
arxiv-12300-10 | Experiments on Parallel Training of Deep Neural Network using Model Averaging | http://arxiv.org/pdf/1507.01239v1.pdf | author:Hang Su, Haoyu Chen category:cs.LG cs.NE published:2015-07-05 summary:In this work we apply model averaging to parallel training of deep neuralnetwork (DNN). Parallelization is done in a model averaging manner. Data ispartitioned and distributed to different nodes for local model updates, andmodel averaging across nodes is done every few minibatches. We use multipleGPUs for data parallelization, and Message Passing Interface (MPI) forcommunication between nodes, which allows us to perform model averagingfrequently without losing much time on communication. We investigate theeffectiveness of Natural Gradient Stochastic Gradient Descent (NG-SGD) andRestricted Boltzmann Machine (RBM) pretraining for parallel training inmodel-averaging framework, and explore the best setups in term of differentlearning rate schedules, averaging frequencies and minibatch sizes. It is shownthat NG-SGD and RBM pretraining benefits parameter-averaging based modeltraining. On the 300h Switchboard dataset, a 9.3 times speedup is achievedusing 16 GPUs and 17 times speedup using 32 GPUs with limited decoding accuracyloss.
arxiv-12300-11 | $M$-Statistic for Kernel Change-Point Detection | http://arxiv.org/pdf/1507.01279v3.pdf | author:Shuang Li, Yao Xie, Hanjun Dai, Le Song category:cs.LG math.ST stat.ML stat.TH published:2015-07-05 summary:Detecting the emergence of an abrupt change-point is a classic problem instatistics and machine learning. Kernel-based nonparametric statistics havebeen proposed for this task which make fewer assumptions on the distributionsthan traditional parametric approach. However, none of the existing kernelstatistics has provided a computationally efficient way to characterize theextremal behavior of the statistic. Such characterization is crucial forsetting the detection threshold, to control the significance level in theoffline case as well as the false alarm rate (captured by the average runlength) in the online case. In this paper we focus on the scenario when theamount of background data is large, and propose two related computationallyefficient kernel-based statistics for change-point detection, which we call"$M$-statistics". A novel theoretical result of the paper is thecharacterization of the tail probability of these statistics using a newtechnique based on change-of-measure. Such characterization provides usaccurate detection thresholds for both offline and online cases incomputationally efficient manner, without the need to resort to the moreexpensive simulations such as bootstrapping. Moreover, our $M$-statistic can beapplied to high-dimensional data by choosing a proper kernel. We show that ourmethods perform well in both synthetic and real world data.
arxiv-12300-12 | TV News Commercials Detection using Success based Locally Weighted Kernel Combination | http://arxiv.org/pdf/1507.01209v1.pdf | author:Raghvendra Kannao, Prithwijit Guha category:cs.CV cs.MM published:2015-07-05 summary:Commercial detection in news broadcast videos involves judicious selection ofmeaningful audio-visual feature combinations and efficient classifiers. And,this problem becomes much simpler if these combinations can be learned from thedata. To this end, we propose an Multiple Kernel Learning based method forboosting successful kernel functions while ignoring the irrelevant ones. Weadopt a intermediate fusion approach where, a SVM is trained with a weightedlinear combination of different kernel functions instead of single kernelfunction. Each kernel function is characterized by a feature set and kerneltype. We identify the feature sub-space locations of the prediction success ofa particular classifier trained only with particular kernel function. Wepropose to estimate a weighing function using support vector regression (withRBF kernel) for each kernel function which has high values (near 1.0) where theclassifier learned on kernel function succeeded and lower values (nearly 0.0)otherwise. Second contribution of this work is TV News Commercials Dataset of150 Hours of News videos. Classifier trained with our proposed scheme hasoutperformed the baseline methods on 6 of 8 benchmark dataset and our own TVcommercials dataset.
arxiv-12300-13 | Parsimonious Labeling | http://arxiv.org/pdf/1507.01208v1.pdf | author:Puneet K. Dokania, M. Pawan Kumar category:cs.CV published:2015-07-05 summary:We propose a new family of discrete energy minimization problems, which wecall parsimonious labeling. Specifically, our energy functional consists ofunary potentials and high-order clique potentials. While the unary potentialsare arbitrary, the clique potentials are proportional to the {\em diversity} ofset of the unique labels assigned to the clique. Intuitively, our energyfunctional encourages the labeling to be parsimonious, that is, use as fewlabels as possible. This in turn allows us to capture useful cues for importantcomputer vision applications such as stereo correspondence and image denoising.Furthermore, we propose an efficient graph-cuts based algorithm for theparsimonious labeling problem that provides strong theoretical guarantees onthe quality of the solution. Our algorithm consists of three steps. First, weapproximate a given diversity using a mixture of a novel hierarchical $P^n$Potts model. Second, we use a divide-and-conquer approach for each mixturecomponent, where each subproblem is solved using an effficient$\alpha$-expansion algorithm. This provides us with a small number of putativelabelings, one for each mixture component. Third, we choose the best putativelabeling in terms of the energy value. Using both sythetic and standard realdatasets, we show that our algorithm significantly outperforms other graph-cutsbased approaches.
arxiv-12300-14 | Correlated Multiarmed Bandit Problem: Bayesian Algorithms and Regret Analysis | http://arxiv.org/pdf/1507.01160v2.pdf | author:Vaibhav Srivastava, Paul Reverdy, Naomi Ehrich Leonard category:math.OC cs.LG stat.ML published:2015-07-05 summary:We consider the correlated multiarmed bandit (MAB) problem in which therewards associated with each arm are modeled by a multivariate Gaussian randomvariable, and we investigate the influence of the assumptions in the Bayesianprior on the performance of the upper credible limit (UCL) algorithm and a newcorrelated UCL algorithm. We rigorously characterize the influence of accuracy,confidence, and correlation scale in the prior on the decision-makingperformance of the algorithms. Our results show how priors and correlationstructure can be leveraged to improve performance.
arxiv-12300-15 | Dependency Recurrent Neural Language Models for Sentence Completion | http://arxiv.org/pdf/1507.01193v1.pdf | author:Piotr Mirowski, Andreas Vlachos category:cs.CL cs.AI cs.LG published:2015-07-05 summary:Recent work on language modelling has shifted focus from count-based modelsto neural models. In these works, the words in each sentence are alwaysconsidered in a left-to-right order. In this paper we show how we can improvethe performance of the recurrent neural network (RNN) language model byincorporating the syntactic dependencies of a sentence, which have the effectof bringing relevant contexts closer to the word being predicted. We evaluateour approach on the Microsoft Research Sentence Completion Challenge and showthat the dependency RNN proposed improves over the RNN by about 10 points inaccuracy. Furthermore, we achieve results comparable with the state-of-the-artmodels on this task.
arxiv-12300-16 | Remarks on kernel Bayes' rule | http://arxiv.org/pdf/1507.01059v2.pdf | author:Hisashi Johno, Kazunori Nakamoto, Tatsuhiko Saigo category:stat.ML published:2015-07-04 summary:Kernel Bayes' rule has been proposed as a nonparametric kernel-based methodto realize Bayesian inference in reproducing kernel Hilbert spaces. However, wedemonstrate both theoretically and experimentally that the prediction result bykernel Bayes' rule is in some cases unnatural. We consider that this phenomenonis in part due to the fact that the assumptions in kernel Bayes' rule do nothold in general.
arxiv-12300-17 | Inference for determinantal point processes without spectral knowledge | http://arxiv.org/pdf/1507.01154v1.pdf | author:Rémi Bardenet, Michalis K. Titsias category:stat.CO stat.ML published:2015-07-04 summary:Determinantal point processes (DPPs) are point process models that naturallyencode diversity between the points of a given realization, through a positivedefinite kernel $K$. DPPs possess desirable properties, such as exact samplingor analyticity of the moments, but learning the parameters of kernel $K$through likelihood-based inference is not straightforward. First, the kernelthat appears in the likelihood is not $K$, but another kernel $L$ related to$K$ through an often intractable spectral decomposition. This issue istypically bypassed in machine learning by directly parametrizing the kernel$L$, at the price of some interpretability of the model parameters. We followthis approach here. Second, the likelihood has an intractable normalizingconstant, which takes the form of a large determinant in the case of a DPP overa finite set of objects, and the form of a Fredholm determinant in the case ofa DPP over a continuous domain. Our main contribution is to derive bounds onthe likelihood of a DPP, both for finite and continuous domains. Unlikeprevious work, our bounds are cheap to evaluate since they do not rely onapproximating the spectrum of a large matrix or an operator. Through usualarguments, these bounds thus yield cheap variational inference and moderatelyexpensive exact Markov chain Monte Carlo inference methods for DPPs.
arxiv-12300-18 | AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes | http://arxiv.org/pdf/1507.01127v1.pdf | author:Sascha Rothe, Hinrich Schütze category:cs.CL published:2015-07-04 summary:We present \textit{AutoExtend}, a system to learn embeddings for synsets andlexemes. It is flexible in that it can take any word embeddings as input anddoes not need an additional training corpus. The synset/lexeme embeddingsobtained live in the same vector space as the word embeddings. A sparse tensorformalization guarantees efficiency and parallelizability. We use WordNet as alexical resource, but AutoExtend can be easily applied to other resources likeFreebase. AutoExtend achieves state-of-the-art performance on word similarityand word sense disambiguation tasks.
arxiv-12300-19 | Modeling the Mind: A brief review | http://arxiv.org/pdf/1507.01122v3.pdf | author:Gabriel Makdah category:cs.AI cs.NE published:2015-07-04 summary:The brain is a powerful tool used to achieve amazing feats. There have beenseveral significant advances in neuroscience and artificial brain research inthe past two decades. This article is a review of such advances, ranging fromthe concepts of connectionism, to neural network architectures andhigh-dimensional representations. There have also been advances in biologicallyinspired cognitive architectures of which we will cite a few. We will bepositioning relatively specific models in a much broader perspective, whilecomparing and contrasting their advantages and weaknesses. The projectspresented are targeted to model the brain at different levels, utilizingdifferent methodologies.
arxiv-12300-20 | Convex Factorization Machine for Regression | http://arxiv.org/pdf/1507.01073v3.pdf | author:Makoto Yamada, Wenzhao Lian, Amit Goyal, Jianhui Chen, Suleiman A Khan, Samuel Kaski, Hiroshi Mamitsuka, Yi Chang category:stat.ML cs.LG published:2015-07-04 summary:We propose the convex factorization machine (CFM), which is a convex variantof the widely used Factorization Machines (FMs). Specifically, we employ alinear+quadratic model and regularize the linear term with the$\ell_2$-regularizer and the quadratic term with the trace norm regularizer.Then, we formulate the CFM optimization as a semidefinite programming problemand propose an efficient optimization procedure with Hazan's algorithm. A keyadvantage of CFM over existing FMs is that it can find a globally optimalsolution, while FMs may get a poor locally optimal solution since the objectivefunction of FMs is non-convex. In addition, the proposed algorithm is simpleyet effective and can be implemented easily. Finally, CFM is a generalfactorization method and can also be used for other factorization problemsincluding multi-view matrix factorization problems. Through synthetic andmovielens datasets, we first show that the proposed CFM achieves resultscompetitive to FMs. Furthermore, in a toxicogenomics prediction task, we showthat CFM outperforms a state-of-the-art tensor factorization method.
arxiv-12300-21 | Describing Multimedia Content using Attention-based Encoder--Decoder Networks | http://arxiv.org/pdf/1507.01053v1.pdf | author:Kyunghyun Cho, Aaron Courville, Yoshua Bengio category:cs.NE cs.CL cs.CV cs.LG published:2015-07-04 summary:Whereas deep neural networks were first mostly used for classification tasks,they are rapidly expanding in the realm of structured output problems, wherethe observed target is composed of multiple random variables that have a richjoint distribution, given the input. We focus in this paper on the case wherethe input also has a rich structure and the input and output structures aresomehow related. We describe systems that learn to attend to different placesin the input, for each element of the output, for a variety of tasks: machinetranslation, image caption generation, video clip description and speechrecognition. All these systems are based on a shared set of building blocks:gated recurrent neural networks and convolutional neural networks, along withtrained attention mechanisms. We report on experimental results with thesesystems, showing impressively good performance and the advantage of theattention mechanism.
arxiv-12300-22 | A New Approach to Probabilistic Programming Inference | http://arxiv.org/pdf/1507.00996v2.pdf | author:Frank Wood, Jan Willem van de Meent, Vikash Mansinghka category:stat.ML cs.AI cs.PL published:2015-07-03 summary:We introduce and demonstrate a new approach to inference in expressiveprobabilistic programming languages based on particle Markov chain Monte Carlo.Our approach is simple to implement and easy to parallelize. It applies toTuring-complete probabilistic programming languages and supports accurateinference in models that make use of complex control flow, including stochasticrecursion. It also includes primitives from Bayesian nonparametric statistics.Our experiments show that this approach can be more efficient than previouslyintroduced single-site Metropolis-Hastings methods.
arxiv-12300-23 | Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination | http://arxiv.org/pdf/1507.00955v3.pdf | author:Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste category:cs.CL cs.IR cs.LG stat.ME stat.ML published:2015-07-03 summary:This paper covers the two approaches for sentiment analysis: i) lexicon basedmethod; ii) machine learning method. We describe several techniques toimplement these approaches and discuss how they can be adopted for sentimentclassification of Twitter messages. We present a comparative study of differentlexicon combinations and show that enhancing sentiment lexicons with emoticons,abbreviations and social-media slang expressions increases the accuracy oflexicon-based classification for Twitter. We discuss the importance of featuregeneration and feature selection processes for machine learning sentimentclassification. To quantify the performance of the main sentiment analysismethods over Twitter we run these algorithms on a benchmark Twitter datasetfrom the SemEval-2013 competition, task 2-B. The results show that machinelearning method based on SVM and Naive Bayes classifiers outperforms thelexicon method. We present a new ensemble method that uses a lexicon basedsentiment score as input feature for the machine learning approach. Thecombined method proved to produce more precise classifications. We also showthat employing a cost-sensitive classifier for highly unbalanced datasetsyields an improvement of sentiment classification performance up to 7%.
arxiv-12300-24 | Fine-grained Recognition Datasets for Biodiversity Analysis | http://arxiv.org/pdf/1507.00913v1.pdf | author:Erik Rodner, Marcel Simon, Gunnar Brehm, Stephanie Pietsch, J. Wolfgang Wägele, Joachim Denzler category:cs.CV published:2015-07-03 summary:In the following paper, we present and discuss challenging applications forfine-grained visual classification (FGVC): biodiversity and species analysis.We not only give details about two challenging new datasets suitable forcomputer vision research with up to 675 highly similar classes, but alsopresent first results with localized features using convolutional neuralnetworks (CNN). We conclude with a list of challenging new research directionsin the area of visual classification for biodiversity research.
arxiv-12300-25 | Optimal design of experiments in the presence of network-correlated outcomes | http://arxiv.org/pdf/1507.00803v1.pdf | author:Guillaume W. Basse, Edoardo M. Airoldi category:stat.ME cs.SI physics.soc-ph stat.ML published:2015-07-03 summary:We consider the problem of how to assign treatment in a randomizedexperiment, when the correlation among the outcomes is informed by a networkavailable pre-intervention. Working within the potential outcome causalframework, we develop a class of models that posit such a correlation structureamong the outcomes, and a strategy for allocating treatment optimally, for thegoal of minimizing the integrated mean squared error of the estimated averagetreatment effect. We provide insights into features of the optimal designs viaan analytical decomposition of the mean squared error used for optimization. Weillustrate how the proposed treatment allocation strategy improves onallocations that ignore the network structure, with extensive simulations.
arxiv-12300-26 | D-MFVI: Distributed Mean Field Variational Inference using Bregman ADMM | http://arxiv.org/pdf/1507.00824v1.pdf | author:Behnam Babagholami-Mohamadabadi, Sejong Yoon, Vladimir Pavlovic category:cs.LG stat.ML published:2015-07-03 summary:Bayesian models provide a framework for probabilistic modelling of complexdatasets. However, many of such models are computationally demanding especiallyin the presence of large datasets. On the other hand, in sensor networkapplications, statistical (Bayesian) parameter estimation usually needsdistributed algorithms, in which both data and computation are distributedacross the nodes of the network. In this paper we propose a general frameworkfor distributed Bayesian learning using Bregman Alternating Direction Method ofMultipliers (B-ADMM). We demonstrate the utility of our framework, with MeanField Variational Bayes (MFVB) as the primitive for distributed MatrixFactorization (MF) and distributed affine structure from motion (SfM).
arxiv-12300-27 | Ridge Regression, Hubness, and Zero-Shot Learning | http://arxiv.org/pdf/1507.00825v1.pdf | author:Yutaro Shigeto, Ikumi Suzuki, Kazuo Hara, Masashi Shimbo, Yuji Matsumoto category:cs.LG stat.ML published:2015-07-03 summary:This paper discusses the effect of hubness in zero-shot learning, when ridgeregression is used to find a mapping between the example space to the labelspace. Contrary to the existing approach, which attempts to find a mapping fromthe example space to the label space, we show that mapping labels into theexample space is desirable to suppress the emergence of hubs in the subsequentnearest neighbor search step. Assuming a simple data model, we prove that theproposed approach indeed reduces hubness. This was verified empirically on thetasks of bilingual lexicon extraction and image labeling: hubness was reducedwith both of these tasks and the accuracy was improved accordingly.
arxiv-12300-28 | Estimating the number of communities in networks by spectral methods | http://arxiv.org/pdf/1507.00827v1.pdf | author:Can M. Le, Elizaveta Levina category:stat.ML cs.SI math.ST stat.TH 62H30, 62G99 published:2015-07-03 summary:Community detection is a fundamental problem in network analysis with manymethods available to estimate communities. Most of these methods assume thatthe number of communities is known, which is often not the case in practice. Wepropose a simple and very fast method for estimating the number of communitiesbased on the spectral properties of certain graph operators, such as thenon-backtracking matrix and the Bethe Hessian matrix. We show that the methodperforms well under several models and a wide range of parameters, and isguaranteed to be consistent under several asymptotic regimes. We compare thenew method to several existing methods for estimating the number of communitiesand show that it is both more accurate and more computationally efficient.
arxiv-12300-29 | Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models | http://arxiv.org/pdf/1507.00814v3.pdf | author:Bradly C. Stadie, Sergey Levine, Pieter Abbeel category:cs.AI cs.LG stat.ML published:2015-07-03 summary:Achieving efficient and scalable exploration in complex domains poses a majorchallenge in reinforcement learning. While Bayesian and PAC-MDP approaches tothe exploration problem offer strong formal guarantees, they are oftenimpractical in higher dimensions due to their reliance on enumerating thestate-action space. Hence, exploration in complex domains is often performedwith simple epsilon-greedy methods. In this paper, we consider the challengingAtari games domain, which requires processing raw pixel inputs and delayedrewards. We evaluate several more sophisticated exploration strategies,including Thompson sampling and Boltzman exploration, and propose a newexploration method based on assigning exploration bonuses from a concurrentlylearned model of the system dynamics. By parameterizing our learned model witha neural network, we are able to develop a scalable and efficient approach toexploration bonuses that can be applied to tasks with complex, high-dimensionalstate spaces. In the Atari domain, our method provides the most consistentimprovement across a range of games that pose a major challenge for priormethods. In addition to raw game-scores, we also develop an AUC-100 metric forthe Atari Learning domain to evaluate the impact of exploration on thisbenchmark.
arxiv-12300-30 | LogDet Rank Minimization with Application to Subspace Clustering | http://arxiv.org/pdf/1507.00908v1.pdf | author:Zhao Kang, Chong Peng, Jie Cheng, Qiang Chen category:cs.CV cs.LG stat.ML published:2015-07-03 summary:Low-rank matrix is desired in many machine learning and computer visionproblems. Most of the recent studies use the nuclear norm as a convex surrogateof the rank operator. However, all singular values are simply added together bythe nuclear norm, and thus the rank may not be well approximated in practicalproblems. In this paper, we propose to use a log-determinant (LogDet) functionas a smooth and closer, though non-convex, approximation to rank for obtaininga low-rank representation in subspace clustering. Augmented Lagrangemultipliers strategy is applied to iteratively optimize the LogDet-basednon-convex objective function on potentially large-scale data. By making use ofthe angular information of principal directions of the resultant low-rankrepresentation, an affinity graph matrix is constructed for spectralclustering. Experimental results on motion segmentation and face clusteringdata demonstrate that the proposed method often outperforms state-of-the-artsubspace clustering algorithms.
arxiv-12300-31 | Optimal Transport for Domain Adaptation | http://arxiv.org/pdf/1507.00504v1.pdf | author:Nicolas Courty, Rémi Flamary, Devis Tuia, Alain Rakotomamonjy category:cs.LG published:2015-07-02 summary:Domain adaptation from one data space (or domain) to another is one of themost challenging tasks of modern data analytics. If the adaptation is donecorrectly, models built on a specific data space become more robust whenconfronted to data depicting the same semantic concepts (the classes), butobserved by another observation system with its own specificities. Among themany strategies proposed to adapt a domain to another, finding a commonrepresentation has shown excellent properties: by finding a commonrepresentation for both domains, a single classifier can be effective in bothand use labelled samples from the source domain to predict the unlabelledsamples of the target domain. In this paper, we propose a regularizedunsupervised optimal transportation model to perform the alignment of therepresentations in the source and target domains. We learn a transportationplan matching both PDFs, which constrains labelled samples in the source domainto remain close during transport. This way, we exploit at the same time the fewlabeled information in the source and the unlabelled distributions observed inboth domains. Experiments in toy and challenging real visual adaptationexamples show the interest of the method, that consistently outperforms stateof the art approaches.
arxiv-12300-32 | Distributed image reconstruction for very large arrays in radio astronomy | http://arxiv.org/pdf/1507.00501v1.pdf | author:André Ferrari, David Mary, Rémi Flamary, Cédric Richard category:astro-ph.IM cs.CV published:2015-07-02 summary:Current and future radio interferometric arrays such as LOFAR and SKA arecharacterized by a paradox. Their large number of receptors (up to millions)allow theoretically unprecedented high imaging resolution. In the same time,the ultra massive amounts of samples makes the data transfer and computationalloads (correlation and calibration) order of magnitudes too high to allow anycurrently existing image reconstruction algorithm to achieve, or even approach,the theoretical resolution. We investigate here decentralized and distributedimage reconstruction strategies which select, transfer and process only afraction of the total data. The loss in MSE incurred by the proposed approachis evaluated theoretically and numerically on simple test cases.
arxiv-12300-33 | Non-convex Regularizations for Feature Selection in Ranking With Sparse SVM | http://arxiv.org/pdf/1507.00500v1.pdf | author:Léa Laporte, Rémi Flamary, Stephane Canu, Sébastien Déjean, Josiane Mothe category:cs.LG published:2015-07-02 summary:Feature selection in learning to rank has recently emerged as a crucialissue. Whereas several preprocessing approaches have been proposed, only a fewworks have been focused on integrating the feature selection into the learningprocess. In this work, we propose a general framework for feature selection inlearning to rank using SVM with a sparse regularization term. We investigateboth classical convex regularizations such as $\ell\_1$ or weighted $\ell\_1$and non-convex regularization terms such as log penalty, Minimax ConcavePenalty (MCP) or $\ell\_p$ pseudo norm with $p\textless{}1$. Two algorithms areproposed, first an accelerated proximal approach for solving the convexproblems, second a reweighted $\ell\_1$ scheme to address the non-convexregularizations. We conduct intensive experiments on nine datasets from Letor3.0 and Letor 4.0 corpora. Numerical results show that the use of non-convexregularizations we propose leads to more sparsity in the resulting models whileprediction performance is preserved. The number of features is decreased by upto a factor of six compared to the $\ell\_1$ regularization. In addition, thesoftware is publicly available on the web.
arxiv-12300-34 | DC Proximal Newton for Non-Convex Optimization Problems | http://arxiv.org/pdf/1507.00438v1.pdf | author:Alain Rakotomamonjy, Remi Flamary, Gilles Gasso category:cs.LG cs.NA stat.ML published:2015-07-02 summary:We introduce a novel algorithm for solving learning problems where both theloss function and the regularizer are non-convex but belong to the class ofdifference of convex (DC) functions. Our contribution is a new general purposeproximal Newton algorithm that is able to deal with such a situation. Thealgorithm consists in obtaining a descent direction from an approximation ofthe loss function and then in performing a line search to ensure sufficientdescent. A theoretical analysis is provided showing that the iterates of theproposed algorithm {admit} as limit points stationary points of the DCobjective function. Numerical experiments show that our approach is moreefficient than current state of the art for a problem with a convex lossfunctions and non-convex regularizer. We have also illustrated the benefit ofour algorithm in high-dimensional transductive learning problem where both lossfunction and regularizers are non-convex.
arxiv-12300-35 | Distributional Smoothing with Virtual Adversarial Training | http://arxiv.org/pdf/1507.00677v8.pdf | author:Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, Shin Ishii category:stat.ML cs.LG published:2015-07-02 summary:We propose local distributional smoothness (LDS), a new notion of smoothnessfor statistical model that can be used as a regularization term to promote thesmoothness of the model distribution. We named the LDS based regularization asvirtual adversarial training (VAT). The LDS of a model at an input datapoint isdefined as the KL-divergence based robustness of the model distribution againstlocal perturbation around the datapoint. VAT resembles adversarial training,but distinguishes itself in that it determines the adversarial direction fromthe model distribution alone without using the label information, making itapplicable to semi-supervised learning. The computational cost for VAT isrelatively low. For neural network, the approximated gradient of the LDS can becomputed with no more than three pairs of forward and back propagations. Whenwe applied our technique to supervised and semi-supervised learning for theMNIST dataset, it outperformed all the training methods other than the currentstate of the art method, which is based on a highly advanced generative model.We also applied our method to SVHN and NORB, and confirmed our method'ssuperior performance over the current state of the art semi-supervised methodapplied to these datasets.
arxiv-12300-36 | Simple, Fast Semantic Parsing with a Tensor Kernel | http://arxiv.org/pdf/1507.00639v1.pdf | author:Daoud Clarke category:cs.CL published:2015-07-02 summary:We describe a simple approach to semantic parsing based on a tensor productkernel. We extract two feature vectors: one for the query and one for eachcandidate logical form. We then train a classifier using the tensor product ofthe two vectors. Using very simple features for both, our system achieves anaverage F1 score of 40.1% on the WebQuestions dataset. This is comparable tomore complex systems but is simpler to implement and runs faster.
arxiv-12300-37 | Convolutional Color Constancy | http://arxiv.org/pdf/1507.00410v2.pdf | author:Jonathan T. Barron category:cs.CV published:2015-07-02 summary:Color constancy is the problem of inferring the color of the light thatilluminated a scene, usually so that the illumination color can be removed.Because this problem is underconstrained, it is often solved by modeling thestatistical regularities of the colors of natural objects and illumination. Incontrast, in this paper we reformulate the problem of color constancy as a 2Dspatial localization task in a log-chrominance space, thereby allowing us toapply techniques from object detection and structured prediction to the colorconstancy problem. By directly learning how to discriminate between correctlywhite-balanced images and poorly white-balanced images, our model is able toimprove performance on standard benchmarks by nearly 40%.
arxiv-12300-38 | Categorical Matrix Completion | http://arxiv.org/pdf/1507.00421v1.pdf | author:Yang Cao, Yao Xie category:cs.NA cs.LG math.ST stat.ML stat.TH published:2015-07-02 summary:We consider the problem of completing a matrix with categorical-valuedentries from partial observations. This is achieved by extending theformulation and theory of one-bit matrix completion. We recover a low-rankmatrix $X$ by maximizing the likelihood ratio with a constraint on the nuclearnorm of $X$, and the observations are mapped from entries of $X$ throughmultiple link functions. We establish theoretical upper and lower bounds on therecovery error, which meet up to a constant factor $\mathcal{O}(K^{3/2})$ where$K$ is the fixed number of categories. The upper bound in our case depends onthe number of categories implicitly through a maximization of terms thatinvolve the smoothness of the link functions. In contrast to one-bit matrixcompletion, our bounds for categorical matrix completion are optimal up to afactor on the order of the square root of the number of categories, which isconsistent with an intuition that the problem becomes harder when the number ofcategories increases. By comparing the performance of our method with theconventional matrix completion method on the MovieLens dataset, we demonstratethe advantage of our method.
arxiv-12300-39 | SQL for SRL: Structure Learning Inside a Database System | http://arxiv.org/pdf/1507.00646v1.pdf | author:Oliver Schulte, Zhensong Qian category:cs.LG cs.DB H.2.8; H.2.4 published:2015-07-02 summary:The position we advocate in this paper is that relational algebra can providea unified language for both representing and computing withstatistical-relational objects, much as linear algebra does for traditionalsingle-table machine learning. Relational algebra is implemented in theStructured Query Language (SQL), which is the basis of relational databasemanagement systems. To support our position, we have developed the FACTORBASEsystem, which uses SQL as a high-level scripting language forstatistical-relational learning of a graphical model structure. The designphilosophy of FACTORBASE is to manage statistical models as first-classcitizens inside a database. Our implementation shows how our SQL constructs inFACTORBASE facilitate fast, modular, and reliable program development.Empirical evidence from six benchmark databases indicates that leveragingdatabase system capabilities achieves scalable model structure learning.
arxiv-12300-40 | The Elusive Present: Hidden Past and Future Dependency and Why We Build Models | http://arxiv.org/pdf/1507.00672v1.pdf | author:Pooneh M. Ara, Ryan G. James, James P. Crutchfield category:cs.IT math.DS math.IT nlin.CD stat.ML published:2015-07-02 summary:Modeling a temporal process as if it is Markovian assumes the present encodesall of the process's history. When this occurs, the present captures all of thedependency between past and future. We recently showed that if one randomlysamples in the space of structured processes, this is almost never the case.So, how does the Markov failure come about? That is, how do individualmeasurements fail to encode the past? And, how many are needed to capturedependencies between the past and future? Here, we investigate how muchinformation can be shared between the past and future, but not be reflected inthe present. We quantify this elusive information, give explicit calculationalmethods, and draw out the consequences. The most important of which is thatwhen the present hides past-future dependency we must move beyondsequence-based statistics and build state-based models.
arxiv-12300-41 | No-Regret Learning in Bayesian Games | http://arxiv.org/pdf/1507.00418v2.pdf | author:Jason Hartline, Vasilis Syrgkanis, Eva Tardos category:cs.GT cs.LG published:2015-07-02 summary:Recent price-of-anarchy analyses of games of complete information suggestthat coarse correlated equilibria, which characterize outcomes resulting fromno-regret learning dynamics, have near-optimal welfare. This work provides twomain technical results that lift this conclusion to games of incompleteinformation, a.k.a., Bayesian games. First, near-optimal welfare in Bayesiangames follows directly from the smoothness-based proof of near-optimal welfarein the same game when the private information is public. Second, no-regretlearning dynamics converge to Bayesian coarse correlated equilibrium in theseincomplete information games. These results are enabled by interpretation of aBayesian game as a stochastic game of complete information.
arxiv-12300-42 | Fast Convergence of Regularized Learning in Games | http://arxiv.org/pdf/1507.00407v5.pdf | author:Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, Robert E. Schapire category:cs.GT cs.AI cs.LG published:2015-07-02 summary:We show that natural classes of regularized learning algorithms with a formof recency bias achieve faster convergence rates to approximate efficiency andto coarse correlated equilibria in multiplayer normal form games. When eachplayer in a game uses an algorithm from our class, their individual regretdecays at $O(T^{-3/4})$, while the sum of utilities converges to an approximateoptimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates.We show a black-box reduction for any algorithm in the class to achieve$\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the fasterrates against algorithms in the class. Our results extend those of [Rakhlin andShridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-playerzero-sum games for specific algorithms.
arxiv-12300-43 | Self-Learning Cloud Controllers: Fuzzy Q-Learning for Knowledge Evolution | http://arxiv.org/pdf/1507.00567v1.pdf | author:Pooyan Jamshidi, Amir Sharifloo, Claus Pahl, Andreas Metzger, Giovani Estrada category:cs.SY cs.AI cs.DC cs.LG cs.SE I.2.6; D.2.11 published:2015-07-02 summary:Cloud controllers aim at responding to application demands by automaticallyscaling the compute resources at runtime to meet performance guarantees andminimize resource costs. Existing cloud controllers often resort to scalingstrategies that are codified as a set of adaptation rules. However, for a cloudprovider, applications running on top of the cloud infrastructure are more orless black-boxes, making it difficult at design time to define optimal orpre-emptive adaptation rules. Thus, the burden of taking adaptation decisionsoften is delegated to the cloud application. Yet, in most cases, applicationdevelopers in turn have limited knowledge of the cloud infrastructure. In thispaper, we propose learning adaptation rules during runtime. To this end, weintroduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KElearns and modifies fuzzy rules at runtime. The benefit is that for designingcloud controllers, we do not have to rely solely on precise design-timeknowledge, which may be difficult to acquire. FQL4KE empowers users to specifycloud controllers by simply adjusting weights representing priorities in systemgoals instead of specifying complex adaptation rules. The applicability ofFQL4KE has been experimentally assessed as part of the cloud applicationframework ElasticBench. The experimental results indicate that FQL4KEoutperforms our previously developed fuzzy controller without learningmechanisms and the native Azure auto-scaling.
arxiv-12300-44 | Identification of stable models via nonparametric prediction error methods | http://arxiv.org/pdf/1507.00507v1.pdf | author:Diego Romeres, Gianluigi Pillonetto, Alessandro Chiuso category:stat.ML published:2015-07-02 summary:A new Bayesian approach to linear system identification has been proposed ina series of recent papers. The main idea is to frame linear systemidentification as predictor estimation in an infinite dimensional space, withthe aid of regularization/Bayesian techniques. This approach guarantees theidentification of stable predictors based on the prediction error minimization.Unluckily, the stability of the predictors does not guarantee the stability ofthe impulse response of the system. In this paper we propose and comparevarious techniques to address this issue. Simulations results comparing thesetechniques will be provided.
arxiv-12300-45 | Learning the intensity of time events with change-points | http://arxiv.org/pdf/1507.00513v1.pdf | author:Mokhtar Zahdi Alaya, Stéphane Gaïffas, Agathe Guilloux category:math.ST stat.ML stat.TH published:2015-07-02 summary:We consider the problem of learning the inhomogeneous intensity of a countingprocess, under a sparse segmentation assumption. We introduce a weightedtotal-variation penalization, using data-driven weights that correctly scalethe penalization along the observation interval. We prove that this leads to asharp tuning of the convex relaxation of the segmentation prior, by statingoracle inequalities with fast rates of convergence, and consistency forchange-points detection. This provides first theoretical guarantees forsegmentation with a convex proxy beyond the standard i.i.d signal + white noisesetting. We introduce a fast algorithm to solve this convex problem. Numericalexperiments illustrate our approach on simulated and on a high-frequencygenomics dataset.
arxiv-12300-46 | Cross Modal Distillation for Supervision Transfer | http://arxiv.org/pdf/1507.00448v2.pdf | author:Saurabh Gupta, Judy Hoffman, Jitendra Malik category:cs.CV published:2015-07-02 summary:In this work we propose a technique that transfers supervision between imagesfrom different modalities. We use learned representations from a large labeledmodality as a supervisory signal for training representations for a newunlabeled paired modality. Our method enables learning of rich representationsfor unlabeled modalities and can be used as a pre-training procedure for newmodalities with limited labeled data. We show experimental results where wetransfer supervision from labeled RGB images to unlabeled depth and opticalflow images and demonstrate large improvements for both these cross modalsupervision transfers. Code, data and pre-trained models are available athttps://github.com/s-gupta/fast-rcnn/tree/distillation
arxiv-12300-47 | Classical vs. Bayesian methods for linear system identification: point estimators and confidence sets | http://arxiv.org/pdf/1507.00543v1.pdf | author:D. Romeres, G. Prando, G. Pillonetto, A. Chiuso category:stat.ML published:2015-07-02 summary:This paper compares classical parametric methods with recently developedBayesian methods for system identification. A Full Bayes solution is consideredtogether with one of the standard approximations based on the Empirical Bayesparadigm. Results regarding point estimators for the impulse response as wellas for confidence regions are reported.
arxiv-12300-48 | Online Transfer Learning in Reinforcement Learning Domains | http://arxiv.org/pdf/1507.00436v2.pdf | author:Yusen Zhan, Matthew E. Taylor category:cs.AI cs.LG I.2.11; I.2.6 published:2015-07-02 summary:This paper proposes an online transfer framework to capture the interactionamong agents and shows that current transfer learning in reinforcement learningis a special case of online transfer. Furthermore, this paper re-characterizesexisting agents-teaching-agents methods as online transfer and analyze one suchteaching method in three ways. First, the convergence of Q-learning and Sarsawith tabular representation with a finite budget is proven. Second, theconvergence of Q-learning and Sarsa with linear function approximation isestablished. Third, the we show the asymptotic performance cannot be hurtthrough teaching. Additionally, all theoretical results are empiricallyvalidated.
arxiv-12300-49 | Correlated Random Measures | http://arxiv.org/pdf/1507.00720v1.pdf | author:Rajesh Ranganath, David Blei category:stat.ML stat.ME published:2015-07-02 summary:We develop correlated random measures, random measures where the atom weightscan exhibit a flexible pattern of dependence, and use them to develop powerfulhierarchical Bayesian nonparametric models. Hierarchical Bayesian nonparametricmodels are usually built from completely random measures, a Poisson-processbased construction in which the atom weights are independent. Completely randommeasures imply strong independence assumptions in the correspondinghierarchical model, and these assumptions are often misplaced in real-worldsettings. Correlated random measures address this limitation. They modelcorrelation within the measure by using a Gaussian process in concert with thePoisson process. With correlated random measures, for example, we can develop alatent feature model for which we can infer both the properties of the latentfeatures and their dependency pattern. We develop several other examples aswell. We study a correlated random measure model of pairwise count data. Wederive an efficient variational inference algorithm and show improvedpredictive performance on large data sets of documents, web clicks, andelectronic health records.
arxiv-12300-50 | Regularized linear system identification using atomic, nuclear and kernel-based norms: the role of the stability constraint | http://arxiv.org/pdf/1507.00564v1.pdf | author:Gianluigi Pillonetto, Tianshi Chen, Alessandro Chiuso, Giuseppe De Nicolao, Lennart Ljung category:cs.SY cs.LG published:2015-07-02 summary:Inspired by ideas taken from the machine learning literature, newregularization techniques have been recently introduced in linear systemidentification. In particular, all the adopted estimators solve a regularizedleast squares problem, differing in the nature of the penalty term assigned tothe impulse response. Popular choices include atomic and nuclear norms (appliedto Hankel matrices) as well as norms induced by the so called stable splinekernels. In this paper, a comparative study of estimators based on thesedifferent types of regularizers is reported. Our findings reveal that stablespline kernels outperform approaches based on atomic and nuclear norms sincethey suitably embed information on impulse response stability and smoothness.This point is illustrated using the Bayesian interpretation of regularization.We also design a new class of regularizers defined by "integral" versions ofstable spline/TC kernels. Under quite realistic experimental conditions, thenew estimators outperform classical prediction error methods also when thelatter are equipped with an oracle for model order selection.
arxiv-12300-51 | Anomaly Detection and Removal Using Non-Stationary Gaussian Processes | http://arxiv.org/pdf/1507.00566v1.pdf | author:Steven Reece, Roman Garnett, Michael Osborne, Stephen Roberts category:stat.ML published:2015-07-02 summary:This paper proposes a novel Gaussian process approach to fault removal intime-series data. Fault removal does not delete the faulty signal data but,instead, massages the fault from the data. We assume that only one fault occursat any one time and model the signal by two separate non-parametric Gaussianprocess models for both the physical phenomenon and the fault. In order tofacilitate fault removal we introduce the Markov Region Link kernel forhandling non-stationary Gaussian processes. This kernel is piece-wisestationary but guarantees that functions generated by it and their derivatives(when required) are everywhere continuous. We apply this kernel to the removalof drift and bias errors in faulty sensor data and also to the recovery of EOGartifact corrupted EEG signals.
arxiv-12300-52 | The Optimal Sample Complexity of PAC Learning | http://arxiv.org/pdf/1507.00473v4.pdf | author:Steve Hanneke category:cs.LG stat.ML published:2015-07-02 summary:This work establishes a new upper bound on the number of samples sufficientfor PAC learning in the realizable case. The bound matches known lower boundsup to numerical constant factors. This solves a long-standing open problem onthe sample complexity of PAC learning. The technique and analysis build on arecent breakthrough by Hans Simon.
arxiv-12300-53 | Fast, Provable Algorithms for Isotonic Regression in all $\ell_{p}$-norms | http://arxiv.org/pdf/1507.00710v2.pdf | author:Rasmus Kyng, Anup Rao, Sushant Sachdeva category:cs.LG cs.DS math.ST stat.TH published:2015-07-02 summary:Given a directed acyclic graph $G,$ and a set of values $y$ on the vertices,the Isotonic Regression of $y$ is a vector $x$ that respects the partial orderdescribed by $G,$ and minimizes $x-y,$ for a specified norm. This papergives improved algorithms for computing the Isotonic Regression for allweighted $\ell_{p}$-norms with rigorous performance guarantees. Our algorithmsare quite practical, and their variants can be implemented to run fast inpractice.
arxiv-12300-54 | Bootstrapped Thompson Sampling and Deep Exploration | http://arxiv.org/pdf/1507.00300v1.pdf | author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG published:2015-07-01 summary:This technical note presents a new approach to carrying out the kind ofexploration achieved by Thompson sampling, but without explicitly maintainingor sampling from posterior distributions. The approach is based on a bootstraptechnique that uses a combination of observed and artificially generated data.The latter serves to induce a prior distribution which, as we will demonstrate,is critical to effective exploration. We explain how the approach can beapplied to multi-armed bandit and reinforcement learning problems and how itrelates to Thompson sampling. The approach is particularly well-suited forcontexts in which exploration is coupled with deep learning, since in thesesettings, maintaining or generating samples from a posterior distributionbecomes computationally infeasible.
arxiv-12300-55 | Pose Embeddings: A Deep Architecture for Learning to Match Human Poses | http://arxiv.org/pdf/1507.00302v1.pdf | author:Greg Mori, Caroline Pantofaru, Nisarg Kothari, Thomas Leung, George Toderici, Alexander Toshev, Weilong Yang category:cs.CV published:2015-07-01 summary:We present a method for learning an embedding that places images of humans insimilar poses nearby. This embedding can be used as a direct method ofcomparing images based on human pose, avoiding potential challenges ofestimating body joint positions. Pose embedding learning is formulated under atriplet-based distance criterion. A deep architecture is used to allow learningof a representation capable of making distinctions between different poses.Experiments on human pose matching and retrieval from video data demonstratethe potential of the method.
arxiv-12300-56 | Bigeometric Organization of Deep Nets | http://arxiv.org/pdf/1507.00220v1.pdf | author:Alexander Cloninger, Ronald R. Coifman, Nicholas Downing, Harlan M. Krumholz category:stat.ML cs.LG published:2015-07-01 summary:In this paper, we build an organization of high-dimensional datasets thatcannot be cleanly embedded into a low-dimensional representation due to missingentries and a subset of the features being irrelevant to modeling functions ofinterest. Our algorithm begins by defining coarse neighborhoods of the pointsand defining an expected empirical function value on these neighborhoods. Wethen generate new non-linear features with deep net representations tuned tomodel the approximate function, and re-organize the geometry of the points withrespect to the new representation. Finally, the points are locally z-scored tocreate an intrinsic geometric organization which is independent of theparameters of the deep net, a geometry designed to assure smoothness withrespect to the empirical function. We examine this approach on data from theCenter for Medicare and Medicaid Services Hospital Quality Initiative, andgenerate an intrinsic low-dimensional organization of the hospitals that issmooth with respect to an expert driven function of quality.
arxiv-12300-57 | Compressive Deconvolution in Medical Ultrasound Imaging | http://arxiv.org/pdf/1507.00136v2.pdf | author:Zhouye Chen, Adrian Basarab, Denis Kouamé category:cs.CV published:2015-07-01 summary:The interest of compressive sampling in ultrasound imaging has been recentlyextensively evaluated by several research teams. Following the differentapplication setups, it has been shown that the RF data may be reconstructedfrom a small number of measurements and/or using a reduced number of ultrasoundpulse emissions. Nevertheless, RF image spatial resolution, contrast and signalto noise ratio are affected by the limited bandwidth of the imaging transducerand the physical phenomenon related to US wave propagation. To overcome theselimitations, several deconvolution-based image processing techniques have beenproposed to enhance the ultrasound images. In this paper, we propose a novelframework, named compressive deconvolution, that reconstructs enhanced RFimages from compressed measurements. Exploiting an unified formulation of thedirect acquisition model, combining random projections and 2D convolution witha spatially invariant point spread function, the benefit of our approach is thejoint data volume reduction and image quality improvement. The proposedoptimization method, based on the Alternating Direction Method of Multipliers,is evaluated on both simulated and in vivo data.
arxiv-12300-58 | Beyond Semantic Image Segmentation : Exploring Efficient Inference in Video | http://arxiv.org/pdf/1507.01578v1.pdf | author:Subarna Tripathi, Serge Belongie, Truong Nguyen category:cs.CV published:2015-07-01 summary:We explore the efficiency of the CRF inference module beyond image levelsemantic segmentation. The key idea is to combine the best of two worlds ofsemantic co-labeling and exploiting more expressive models. Similar to[Alvarez14] our formulation enables us perform inference over ten thousandimages within seconds. On the other hand, it can handle higher-order cliquepotentials similar to [vineet2014] in terms of region-level label consistencyand context in terms of co-occurrences. We follow the mean-field updates forhigher order potentials similar to [vineet2014] and extend the spatialsmoothness and appearance kernels [DenseCRF13] to address video data inspiredby [Alvarez14]; thus making the system amenable to perform video semanticsegmentation most effectively.
arxiv-12300-59 | Natural Neural Networks | http://arxiv.org/pdf/1507.00210v1.pdf | author:Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray Kavukcuoglu category:stat.ML cs.LG cs.NE published:2015-07-01 summary:We introduce Natural Neural Networks, a novel family of algorithms that speedup convergence by adapting their internal representation during training toimprove conditioning of the Fisher matrix. In particular, we show a specificexample that employs a simple and efficient reparametrization of the neuralnetwork weights by implicitly whitening the representation obtained at eachlayer, while preserving the feed-forward computation of the network. Suchnetworks can be trained efficiently via the proposed Projected Natural GradientDescent algorithm (PRONG), which amortizes the cost of these reparametrizationsover many parameter updates and is closely related to the Mirror Descent onlinelearning algorithm. We highlight the benefits of our method on bothunsupervised and supervised learning tasks, and showcase its scalability bytraining on the large-scale ImageNet Challenge dataset.
arxiv-12300-60 | An Empirical Evaluation of True Online TD(λ) | http://arxiv.org/pdf/1507.00353v1.pdf | author:Harm van Seijen, A. Rupam Mahmood, Patrick M. Pilarski, Richard S. Sutton category:cs.AI cs.LG stat.ML published:2015-07-01 summary:The true online TD({\lambda}) algorithm has recently been proposed (vanSeijen and Sutton, 2014) as a universal replacement for the popularTD({\lambda}) algorithm, in temporal-difference learning and reinforcementlearning. True online TD({\lambda}) has better theoretical properties thanconventional TD({\lambda}), and the expectation is that it also results infaster learning. In this paper, we put this hypothesis to the test.Specifically, we compare the performance of true online TD({\lambda}) with thatof TD({\lambda}) on challenging examples, random Markov reward processes, and areal-world myoelectric prosthetic arm. We use linear function approximationwith tabular, binary, and non-binary features. We assess the algorithms alongthree dimensions: computational cost, learning speed, and ease of use. Ourresults confirm the strength of true online TD({\lambda}): 1) for sparsefeature vectors, the computational overhead with respect to TD({\lambda}) isminimal; for non-sparse features the computation time is at most twice that ofTD({\lambda}), 2) across all domains/representations the learning speed of trueonline TD({\lambda}) is often better, but never worse than that ofTD({\lambda}), and 3) true online TD({\lambda}) is easier to use, because itdoes not require choosing between trace types, and it is generally more stablewith respect to the step-size. Overall, our results suggest that true onlineTD({\lambda}) should be the first choice when looking for an efficient,general-purpose TD method.
arxiv-12300-61 | Energy-efficient neuromorphic classifiers | http://arxiv.org/pdf/1507.00235v1.pdf | author:Daniel Martí, Mattia Rigotti, Mingoo Seok, Stefano Fusi category:q-bio.NC cs.NE published:2015-07-01 summary:Neuromorphic engineering combines the architectural and computationalprinciples of systems neuroscience with semiconductor electronics, with the aimof building efficient and compact devices that mimic the synaptic and neuralmachinery of the brain. Neuromorphic engineering promises extremely low energyconsumptions, comparable to those of the nervous system. However, until now theneuromorphic approach has been restricted to relatively simple circuits andspecialized functions, rendering elusive a direct comparison of their energyconsumption to that used by conventional von Neumann digital machines solvingreal-world tasks. Here we show that a recent technology developed by IBM can beleveraged to realize neuromorphic circuits that operate as classifiers ofcomplex real-world stimuli. These circuits emulate enough neurons to competewith state-of-the-art classifiers. We also show that the energy consumption ofthe IBM chip is typically 2 or more orders of magnitude lower than that ofconventional digital machines when implementing classifiers with comparableperformance. Moreover, the spike-based dynamics display a trade-off betweenintegration time and accuracy, which naturally translates into algorithms thatcan be flexibly deployed for either fast and approximate classifications, ormore accurate classifications at the mere expense of longer running times andhigher energy costs. This work finally proves that the neuromorphic approachcan be efficiently used in real-world applications and it has significantadvantages over conventional digital devices when energy consumption isconsidered.
arxiv-12300-62 | Dimensionality on Summarization | http://arxiv.org/pdf/1507.00209v1.pdf | author:Hai Zhuge category:cs.CL cs.IR published:2015-07-01 summary:Summarization is one of the key features of human intelligence. It plays animportant role in understanding and representation. With rapid and continualexpansion of texts, pictures and videos in cyberspace, automatic summarizationbecomes more and more desirable. Text summarization has been studied for overhalf century, but it is still hard to automatically generate a satisfiedsummary. Traditional methods process texts empirically and neglect thefundamental characteristics and principles of language use and understanding.This paper summarizes previous text summarization approaches in amulti-dimensional classification space, introduces a multi-dimensionalmethodology for research and development, unveils the basic characteristics andprinciples of language use and understanding, investigates some fundamentalmechanisms of summarization, studies the dimensions and forms ofrepresentations, and proposes a multi-dimensional evaluation mechanisms.Investigation extends to the incorporation of pictures into summary and to thesummarization of videos, graphs and pictures, and then reaches a generalsummarization framework.
arxiv-12300-63 | Prior Polarity Lexical Resources for the Italian Language | http://arxiv.org/pdf/1507.00133v1.pdf | author:Valeria Borzì, Simone Faro, Arianna Pavone, Sabrina Sansone category:cs.CL published:2015-07-01 summary:In this paper we present SABRINA (Sentiment Analysis: a Broad Resource forItalian Natural language Applications) a manually annotated prior polaritylexical resource for Italian natural language applications in the field ofopinion mining and sentiment induction. The resource consists in two differentsets, an Italian dictionary of more than 277.000 words tagged with their priorpolarity value, and a set of polarity modifiers, containing more than 200words, which can be used in combination with non neutral terms of thedictionary in order to induce the sentiment of Italian compound terms. To thebest of our knowledge this is the first prior polarity manually annotatedresource which has been developed for the Italian natural language.
arxiv-12300-64 | Polarimetric Hierarchical Semantic Model and Scattering Mechanism Based PolSAR Image Classification | http://arxiv.org/pdf/1507.00110v1.pdf | author:Fang Liu, Junfei Shi, Licheng Jiao, Hongying Liu, Shuyuan Yang, Jie Wu, Hongxia Hao, Jialing Yuan category:cs.CV published:2015-07-01 summary:For polarimetric SAR (PolSAR) image classification, it is a challenge toclassify the aggregated terrain types, such as the urban area, into semantichomogenous regions due to sharp bright-dark variations in intensity. Theaggregated terrain type is formulated by the similar ground objects aggregatedtogether. In this paper, a polarimetric hierarchical semantic model (PHSM) isfirstly proposed to overcome this disadvantage based on the constructions of aprimal-level and a middle-level semantic. The primal-level semantic is apolarimetric sketch map which consists of sketch segments as the sparserepresentation of a PolSAR image. The middle-level semantic is a region mapwhich can extract semantic homogenous regions from the sketch map by exploitingthe topological structure of sketch segments. Mapping the region map to thePolSAR image, a complex PolSAR scene is partitioned into aggregated, structuraland homogenous pixel-level subspaces with the characteristics of relativelycoherent terrain types in each subspace. Then, according to the characteristicsof three subspaces above, three specific methods are adopted, and furthermorepolarimetric information is exploited to improve the segmentation result.Experimental results on PolSAR data sets with different bands and sensorsdemonstrate that the proposed method is superior to the state-of-the-artmethods in region homogeneity and edge preservation for terrain classification.
arxiv-12300-65 | Evaluation of Genotypic Diversity Measurements Exploited in Real-Coded Representation | http://arxiv.org/pdf/1507.00088v1.pdf | author:Guillaume Corriveau, Raynald Guilbault, Antoine Tahan, Robert Sabourin category:cs.NE published:2015-07-01 summary:Numerous genotypic diversity measures (GDMs) are available in the literatureto assess the convergence status of an evolutionary algorithm (EA) or describeits search behavior. In a recent study, the authors of this paper drewattention to the need for a GDM validation framework. In response, this studyproposes three requirements (monotonicity in individual varieties, twinning,and monotonicity in distance) that can clearly portray any GDMs. Thesediversity requirements are analysed by means of controlled populationarrangements. In this paper four GDMs are evaluated with the proposedvalidation framework. The results confirm that properly evaluating populationdiversity is a rather difficult task, as none of the analysed GDMs complieswith all the diversity requirements.
arxiv-12300-66 | Supervised Learning of Semantics-Preserving Hashing via Deep Neural Networks for Large-Scale Image Search | http://arxiv.org/pdf/1507.00101v1.pdf | author:Huei-Fang Yang, Kevin Lin, Chu-Song Chen category:cs.CV published:2015-07-01 summary:This paper presents a supervised deep hashing approach that constructs binaryhash codes from labeled data for large-scale image search. We assume thatsemantic labels are governed by a set of latent attributes in which eachattribute can be on or off, and classification relies on these attributes.Based on this assumption, our approach, dubbed supervised semantics-preservingdeep hashing (SSDH), constructs hash functions as a latent layer in a deepnetwork in which binary codes are learned by the optimization of an objectivefunction defined over classification error and other desirable properties ofhash codes. With this design, SSDH has a nice property that classification andretrieval are unified in a single learning model, and the learned binary codesnot only preserve the semantic similarity between images but also are efficientfor image search. Moreover, SSDH performs joint learning of imagerepresentations, hash codes, and classification in a pointwised manner and thusis naturally scalable to large-scale datasets. SSDH is simple and can be easilyrealized by a slight modification of an existing deep architecture forclassification; yet it is effective and outperforms other unsupervised andsupervised hashing approaches on several benchmarks and one large datasetcomprising more than 1 million images.
arxiv-12300-67 | A Study of Gradient Descent Schemes for General-Sum Stochastic Games | http://arxiv.org/pdf/1507.00093v1.pdf | author:H. L. Prasad, Shalabh Bhatnagar category:cs.LG cs.GT published:2015-07-01 summary:Zero-sum stochastic games are easy to solve as they can be cast as simpleMarkov decision processes. This is however not the case with general-sumstochastic games. A fairly general optimization problem formulation isavailable for general-sum stochastic games by Filar and Vrieze [2004]. However,the optimization problem there has a non-linear objective and non-linearconstraints with special structure. Since gradients of both the objective aswell as constraints of this optimization problem are well defined, gradientbased schemes seem to be a natural choice. We discuss a gradient scheme tunedfor two-player stochastic games. We show in simulations that this scheme indeedconverges to a Nash equilibrium, for a simple terrain exploration problemmodelled as a general-sum stochastic game. However, it turns out that onlyglobal minima of the optimization problem correspond to Nash equilibria of theunderlying general-sum stochastic game, while gradient schemes only guaranteeconvergence to local minima. We then provide important necessary conditions forgradient schemes to converge to Nash equilibria in general-sum stochasticgames.
arxiv-12300-68 | Fast Cross-Validation for Incremental Learning | http://arxiv.org/pdf/1507.00066v1.pdf | author:Pooria Joulani, András György, Csaba Szepesvári category:stat.ML cs.AI cs.LG published:2015-06-30 summary:Cross-validation (CV) is one of the main tools for performance estimation andparameter tuning in machine learning. The general recipe for computing CVestimate is to run a learning algorithm separately for each CV fold, acomputationally expensive process. In this paper, we propose a new approach toreduce the computational burden of CV-based performance estimation. As opposedto all previous attempts, which are specific to a particular learning model orproblem domain, we propose a general method applicable to a large class ofincremental learning algorithms, which are uniquely fitted to big dataproblems. In particular, our method applies to a wide range of supervised andunsupervised learning tasks with different performance criteria, as long as thebase learning algorithm is incremental. We show that the running time of thealgorithm scales logarithmically, rather than linearly, in the number of CVfolds. Furthermore, the algorithm has favorable properties for parallel anddistributed implementation. Experiments with state-of-the-art incrementallearning algorithms confirm the practicality of the proposed method.
arxiv-12300-69 | Unsupervised Learning from Narrated Instruction Videos | http://arxiv.org/pdf/1506.09215v3.pdf | author:Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, Simon Lacoste-Julien category:cs.CV cs.LG published:2015-06-30 summary:We address the problem of automatically learning the main steps to complete acertain task, such as changing a car tire, from a set of narrated instructionvideos. The contributions of this paper are three-fold. First, we develop a newunsupervised learning approach that takes advantage of the complementary natureof the input video and the associated narration. The method solves twoclustering problems, one in text and one in video, applied one after each otherand linked by joint constraints to obtain a single coherent sequence of stepsin both modalities. Second, we collect and annotate a new challenging datasetof real-world instruction videos from the Internet. The dataset contains about800,000 frames for five different tasks that include complex interactionsbetween people and objects, and are captured in a variety of indoor and outdoorsettings. Third, we experimentally demonstrate that the proposed method canautomatically discover, in an unsupervised manner, the main steps to achievethe task and locate the steps in the input videos.
arxiv-12300-70 | On the Equivalence of Factorized Information Criterion Regularization and the Chinese Restaurant Process Prior | http://arxiv.org/pdf/1506.09068v2.pdf | author:Shaohua Li category:stat.ML published:2015-06-30 summary:Factorized Information Criterion (FIC) is a recently developed informationcriterion, based on which a novel model selection methodology, namelyFactorized Asymptotic Bayesian (FAB) Inference, has been developed andsuccessfully applied to various hierarchical Bayesian models. The DirichletProcess (DP) prior, and one of its well known representations, the ChineseRestaurant Process (CRP), derive another line of model selection methods. FICcan be viewed as a prior distribution over the latent variable configurations.Under this view, we prove that when the parameter dimensionality $D_{c}=2$, FICis equivalent to CRP. We argue that when $D_{c}>2$, FIC avoids an inherentproblem of DP/CRP, i.e. the data likelihood will dominate the impact of theprior, and thus the model selection capability will weaken as $D_{c}$increases. However, FIC overestimates the data likelihood. As a result, FIC maybe overly biased towards models with less components. We propose a naturalgeneralization of FIC, which finds a middle ground between CRP and FIC, and mayyield more accurate model selection results than FIC.
arxiv-12300-71 | Selective Inference and Learning Mixed Graphical Models | http://arxiv.org/pdf/1507.00039v1.pdf | author:Jason D. Lee category:stat.ML cs.LG published:2015-06-30 summary:This thesis studies two problems in modern statistics. First, we studyselective inference, or inference for hypothesis that are chosen after lookingat the data. The motiving application is inference for regression coefficientsselected by the lasso. We present the Condition-on-Selection method that allowsfor valid selective inference, and study its application to the lasso, andseveral other selection algorithms. In the second part, we consider the problem of learning the structure of apairwise graphical model over continuous and discrete variables. We present anew pairwise model for graphical models with both continuous and discretevariables that is amenable to structure learning. In previous work, authorshave considered structure learning of Gaussian graphical models and structurelearning of discrete models. Our approach is a natural generalization of thesetwo lines of work to the mixed case. The penalization scheme involves a novelsymmetric use of the group-lasso norm and follows naturally from a particularparametrization of the model. We provide conditions under which our estimatoris model selection consistent in the high-dimensional regime.
arxiv-12300-72 | The quasispecies regime for the simple genetic algorithm with roulette-wheel selection | http://arxiv.org/pdf/1506.09081v2.pdf | author:Raphaël Cerf category:cs.NE math.PR published:2015-06-30 summary:We introduce a new parameter to discuss the behavior of a genetic algorithm.This parameter is the mean number of exact copies of the best fit chromosomesfrom one generation to the next. We argue that the genetic algorithm shouldoperate efficiently when this parameter is slightly larger than $1$. Weconsider the case of the simple genetic algorithm with the roulette--wheelselection mechanism. We denote by $\ell$ the length of the chromosomes, by $m$the population size, by $p_C$ the crossover probability and by $p_M$ themutation probability. We start the genetic algorithm with an initial populationwhose maximal fitness is equal to $f_0^*$ and whose mean fitness is equal to${\overline{f_0}}$. We show that, in the limit of large populations, thedynamics of the genetic algorithm depends in a critical way on the parameter$\pi \,=\,\big({f_0^*}/{\overline{f_0}}\big) (1-p_C)(1-p_M)^\ell\,.$ Ourresults suggest that the mutation and crossover probabilities should be tunedso that, at each generation, $\text{maximal fitness} \times (1-p_C)(1-p_M)^\ell > \text{mean fitness}$.
arxiv-12300-73 | Language Understanding for Text-based Games Using Deep Reinforcement Learning | http://arxiv.org/pdf/1506.08941v2.pdf | author:Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay category:cs.CL cs.AI published:2015-06-30 summary:In this paper, we consider the task of learning control policies fortext-based games. In these games, all interactions in the virtual world arethrough text and the underlying state is not observed. The resulting languagebarrier makes such environments challenging for automatic game players. Weemploy a deep reinforcement learning framework to jointly learn staterepresentations and action policies using game rewards as feedback. Thisframework enables us to map text descriptions into vector representations thatcapture the semantics of the game states. We evaluate our approach on two gameworlds, comparing against baselines using bag-of-words and bag-of-bigrams forstate representations. Our algorithm outperforms the baselines on both worldsdemonstrating the importance of learning expressive representations.
arxiv-12300-74 | Aging display's effect on interpretation of digital pathology slides | http://arxiv.org/pdf/1506.09166v1.pdf | author:Ali R. N. Avanaki, Kathryn S. Espig, Sameer Sawhney, Liron Pantanowitz, Anil V. Parwani, Albert Xthona, Tom R. L. Kimpe category:cs.CV cs.GR published:2015-06-30 summary:It is our conjecture that the variability of colors in a pathology imageeffects the interpretation of pathology cases, whether it is diagnosticaccuracy, diagnostic confidence, or workflow efficiency. In this paper, digitalpathology images are analyzed to quantify the perceived difference in colorthat occurs due to display aging, in particular a change in the maximumluminance, white point, and color gamut. The digital pathology images studiedinclude diagnostically important features, such as the conspicuity of nuclei.Three different display aging models are applied to images: aging of luminance& chrominance, aging of chrominance only, and a stabilized luminance &chrominance (i.e., no aging). These display models and images are then used tocompare conspicuity of nuclei using CIE deltaE2000, a perceptual colordifference metric. The effect of display aging using these display models andimages is further analyzed through a human reader study designed to quantifythe effects from a clinical perspective. Results from our reader study indicatesignificant impact of aged displays on workflow as well as diagnosis as follow.As compared to the originals (no-aging), slides with the effect of agingsimulated were significantly more difficult to read (p-value of 0.0005) andtook longer to score (p-value of 0.02). Moreover, luminance+chrominance agingsignificantly reduced inter-session percent agreement of diagnostic scores(p-value of 0.0418).
arxiv-12300-75 | Top-N recommendations in the presence of sparsity: An NCD-based approach | http://arxiv.org/pdf/1507.00043v2.pdf | author:Athanasios N. Nikolakopoulos, John D. Garofalakis category:cs.IR cs.AI stat.ML published:2015-06-30 summary:Making recommendations in the presence of sparsity is known to present one ofthe most challenging problems faced by collaborative filtering methods. In thiswork we tackle this problem by exploiting the innately hierarchical structureof the item space following an approach inspired by the theory ofDecomposability. We view the itemspace as a Nearly Decomposable system and wedefine blocks of closely related elements and corresponding indirect proximitycomponents. We study the theoretical properties of the decomposition and wederive sufficient conditions that guarantee full item space coverage even incold-start recommendation scenarios. A comprehensive set of experiments on theMovieLens and the Yahoo!R2Music datasets, using several widely appliedperformance metrics, support our model's theoretically predicted properties andverify that NCDREC outperforms several state-of-the-art algorithms, in terms ofrecommendation accuracy, diversity and sparseness insensitivity.
arxiv-12300-76 | Learning Single Index Models in High Dimensions | http://arxiv.org/pdf/1506.08910v1.pdf | author:Ravi Ganti, Nikhil Rao, Rebecca M. Willett, Robert Nowak category:stat.ML cs.LG stat.ME published:2015-06-30 summary:Single Index Models (SIMs) are simple yet flexible semi-parametric models forclassification and regression. Response variables are modeled as a nonlinear,monotonic function of a linear combination of features. Estimation in thiscontext requires learning both the feature weights, and the nonlinear function.While methods have been described to learn SIMs in the low dimensional regime,a method that can efficiently learn SIMs in high dimensions has not beenforthcoming. We propose three variants of a computationally and statisticallyefficient algorithm for SIM inference in high dimensions. We establish excessrisk bounds for the proposed algorithms and experimentally validate theadvantages that our SIM learning methods provide relative to Generalized LinearModel (GLM) and low dimensional SIM based learning methods.
arxiv-12300-77 | A Large-Scale Car Dataset for Fine-Grained Categorization and Verification | http://arxiv.org/pdf/1506.08959v2.pdf | author:Linjie Yang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV cs.AI published:2015-06-30 summary:Updated on 24/09/2015: This update provides preliminary experiment resultsfor fine-grained classification on the surveillance data of CompCars. Thetrain/test splits are provided in the updated dataset. See details in Section6.
arxiv-12300-78 | A complex network approach to stylometry | http://arxiv.org/pdf/1506.09107v2.pdf | author:Diego R. Amancio category:cs.CL published:2015-06-30 summary:Statistical methods have been widely employed to study the fundamentalproperties of language. In recent years, methods from complex and dynamicalsystems proved useful to create several language models. Despite the largeamount of studies devoted to represent texts with physical models, only alimited number of studies have shown how the properties of the underlyingphysical systems can be employed to improve the performance of natural languageprocessing tasks. In this paper, I address this problem by devising complexnetworks methods that are able to improve the performance of currentstatistical methods. Using a fuzzy classification strategy, I show that thetopological properties extracted from texts complement the traditional textualdescription. In several cases, the performance obtained with hybrid approachesoutperformed the results obtained when only traditional or networked methodswere used. Because the proposed model is generic, the framework devised herecould be straightforwardly used to study similar textual applications where thetopology plays a pivotal role in the description of the interacting agents.
arxiv-12300-79 | On anthropomorphic decision making in a model observer | http://arxiv.org/pdf/1506.09169v1.pdf | author:Ali R. N. Avanaki, Kathryn S. Espig, Tom R. L. Kimpe, Andrew D. A. Maidment category:cs.CV cs.HC published:2015-06-30 summary:By analyzing human readers' performance in detecting small round lesions insimulated digital breast tomosynthesis background in a location known exactlyscenario, we have developed a model observer that is a better predictor ofhuman performance with different levels of background complexity (i.e.,anatomical and quantum noise). Our analysis indicates that human observersperform a lesion detection task by combining a number of sub-decisions, each anindicator of the presence of a lesion in the image stack. This is in contrastto a channelized Hotelling observer, where the detection task is conductedholistically by thresholding a single decision variable, made from an optimallyweighted linear combination of channels. However, it seems that the sub-parperformance of human readers compared to the CHO cannot be fully explained bytheir reliance on sub-decisions, or perhaps we do not consider a sufficientnumber of sub-decisions. To bridge the gap between the performances of humanreaders and the model observer based upon sub-decisions, we use an additivenoise model, the power of which is modulated with the level of backgroundcomplexity. The proposed model observer better predicts the fast drop in humandetection performance with background complexity.
arxiv-12300-80 | Artificial Catalytic Reactions in 2D for Combinatorial Optimization | http://arxiv.org/pdf/1506.09019v1.pdf | author:Jaderick P. Pabico category:cs.ET cs.NE published:2015-06-30 summary:Presented in this paper is a derivation of a 2D catalytic reaction-basedmodel to solve combinatorial optimization problems (COPs). The simulatedcatalytic reactions, a computational metaphor, occurs in an artificial chemicalreactor that finds near-optimal solutions to COPs. The artificial environmentis governed by catalytic reactions that can alter the structure of artificialmolecular elements. Altering the molecular structure means finding newsolutions to the COP. The molecular mass of the elements was considered as ameasure of goodness of fit of the solutions. Several data structures andmatrices were used to record the directions and locations of the molecules.These provided the model the 2D topology. The Traveling Salesperson Problem(TSP) was used as a working example. The performance of the model in finding asolution for the TSP was compared to the performance of a topology-less model.Experimental results show that the 2D model performs better than thetopology-less one.
arxiv-12300-81 | Representing data by sparse combination of contextual data points for classification | http://arxiv.org/pdf/1507.00019v2.pdf | author:Jingyan Wang, Yihua Zhou, Ming Yin, Shaochang Chen, Benjamin Edwards category:cs.CV published:2015-06-30 summary:In this paper, we study the problem of using contextual da- ta points of adata point for its classification problem. We propose to represent a data pointas the sparse linear reconstruction of its context, and learn the sparsecontext to gather with a linear classifier in a su- pervised way to increaseits discriminative ability. We proposed a novel formulation for contextlearning, by modeling the learning of context reconstruction coefficients andclassifier in a unified objective. In this objective, the reconstruction erroris minimized and the coefficient spar- sity is encouraged. Moreover, the hingeloss of the classifier is minimized and the complexity of the classifier isreduced. This objective is opti- mized by an alternative strategy in aniterative algorithm. Experiments on three benchmark data set show its advantageover state-of-the-art context-based data representation and classificationmethods.
arxiv-12300-82 | Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty | http://arxiv.org/pdf/1506.08928v1.pdf | author:Changkyu Song, Sejong Yoon, Vladimir Pavlovic category:cs.LG cs.CV math.OC published:2015-06-30 summary:We propose new methods to speed up convergence of the Alternating DirectionMethod of Multipliers (ADMM), a common optimization tool in the context oflarge scale and distributed learning. The proposed method accelerates the speedof convergence by automatically deciding the constraint penalty needed forparameter consensus in each iteration. In addition, we also propose anextension of the method that adaptively determines the maximum number ofiterations to update the penalty. We show that this approach effectively leadsto an adaptive, dynamic network topology underlying the distributedoptimization. The utility of the new penalty update schemes is demonstrated onboth synthetic and real data, including a computer vision application ofdistributed structure from motion.
arxiv-12300-83 | Learning to Detect Blue-white Structures in Dermoscopy Images with Weak Supervision | http://arxiv.org/pdf/1506.09179v1.pdf | author:Ali Madooei, Mark S. Drew, Hossein Hajimirsadeghi category:cs.CV published:2015-06-30 summary:We propose a novel approach to identify one of the most significantdermoscopic criteria in the diagnosis of Cutaneous Melanoma: the Blue-whitishstructure. In this paper, we achieve this goal in a Multiple Instance Learningframework using only image-level labels of whether the feature is present ornot. As the output, we predict the image classification label and as welllocalize the feature in the image. Experiments are conducted on a challengingdataset with results outperforming state-of-the-art. This study provides animprovement on the scope of modelling for computerized image analysis of skinlesions, in particular in that it puts forward a framework for identificationof dermoscopic local features from weakly-labelled data.
arxiv-12300-84 | The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems | http://arxiv.org/pdf/1506.08909v3.pdf | author:Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau category:cs.CL cs.AI cs.LG cs.NE published:2015-06-30 summary:This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost1 million multi-turn dialogues, with a total of over 7 million utterances and100 million words. This provides a unique resource for research into buildingdialogue managers based on neural language models that can make use of largeamounts of unlabeled data. The dataset has both the multi-turn property ofconversations in the Dialog State Tracking Challenge datasets, and theunstructured nature of interactions from microblog services such as Twitter. Wealso describe two neural learning architectures suitable for analyzing thisdataset, and provide benchmark performance on the task of selecting the bestnext response.
arxiv-12300-85 | Forming A Random Field via Stochastic Cliques: From Random Graphs to Fully Connected Random Fields | http://arxiv.org/pdf/1506.09110v1.pdf | author:Mohammad Javad Shafiee, Alexander Wong, Paul Fieguth category:cs.CV published:2015-06-30 summary:Random fields have remained a topic of great interest over past decades forthe purpose of structured inference, especially for problems such as imagesegmentation. The local nodal interactions commonly used in such models oftensuffer the short-boundary bias problem, which are tackled primarily through theincorporation of long-range nodal interactions. However, the issue ofcomputational tractability becomes a significant issue when incorporating suchlong-range nodal interactions, particularly when a large number of long-rangenodal interactions (e.g., fully-connected random fields) are modeled. In this work, we introduce a generalized random field framework based aroundthe concept of stochastic cliques, which addresses the issue of computationaltractability when using fully-connected random fields by stochastically forminga sparse representation of the random field. The proposed framework allows forefficient structured inference using fully-connected random fields without anyrestrictions on the potential functions that can be utilized. Severalrealizations of the proposed framework using graph cuts are presented andevaluated, and experimental results demonstrate that the proposed framework canprovide competitive performance for the purpose of image segmentation whencompared to existing fully-connected and principled deep random fieldframeworks.
arxiv-12300-86 | Lens Factory: Automatic Lens Generation Using Off-the-shelf Components | http://arxiv.org/pdf/1506.08956v2.pdf | author:Libin Sun, Brian Guenter, Neel Joshi, Patrick Therien, James Hays category:cs.GR cs.CV published:2015-06-30 summary:Custom optics is a necessity for many imaging applications. Unfortunately,custom lens design is costly (thousands to tens of thousands of dollars), timeconsuming (10-12 weeks typical lead time), and requires specialized opticsdesign expertise. By using only inexpensive, off-the-shelf lens components theLens Factory automatic design system greatly reduces cost and time. Design,ordering of parts, delivery, and assembly can be completed in a few days, at acost in the low hundreds of dollars. Lens design constraints, such as focallength and field of view, are specified in terms familiar to the graphicscommunity so no optics expertise is necessary. Unlike conventional lens designsystems, which only use continuous optimization methods, Lens Factory adds adiscrete optimization stage. This stage searches the combinatorial space ofpossible combinations of lens elements to find novel designs, evolving simplecanonical lens designs into more complex, better designs. Intelligent pruningrules make the combinatorial search feasible. We have designed and builtseveral high performance optical systems which demonstrate the practicality ofthe system.
arxiv-12300-87 | Long-Range Motion Trajectories Extraction of Articulated Human Using Mesh Evolution | http://arxiv.org/pdf/1506.09075v3.pdf | author:Yuanyuan Wu, Xiaohai He, Byeongkeun Kang, Haiying Song, Truong Q. Nguyen category:cs.CV published:2015-06-30 summary:This letter presents a novel approach to extract reliable dense andlong-range motion trajectories of articulated human in a video sequence.Compared with existing approaches that emphasize temporal consistency of eachtracked point, we also consider the spatial structure of tracked points on thearticulated human. We treat points as a set of vertices, and build a trianglemesh to join them in image space. The problem of extracting long-range motiontrajectories is changed to the issue of consistency of mesh evolution overtime. First, self-occlusion is detected by a novel mesh-based method and anadaptive motion estimation method is proposed to initialize mesh betweensuccessive frames. Furthermore, we propose an iterative algorithm toefficiently adjust vertices of mesh for a physically plausible deformation,which can meet the local rigidity of mesh and silhouette constraints. Finally,we compare the proposed method with the state-of-the-art methods on a set ofchallenging sequences. Evaluations demonstrate that our method achievesfavorable performance in terms of both accuracy and integrity of extractedtrajectories.
arxiv-12300-88 | Discovering Characteristic Landmarks on Ancient Coins using Convolutional Networks | http://arxiv.org/pdf/1506.09174v2.pdf | author:Jongpil Kim, Vladimir Pavlovic category:cs.CV published:2015-06-30 summary:In this paper, we propose a novel method to find characteristic landmarks onancient Roman imperial coins using deep convolutional neural network models(CNNs). We formulate an optimization problem to discover class-specific regionswhile guaranteeing specific controlled loss of accuracy. Analysis onvisualization of the discovered region confirms that not only can the proposedmethod successfully find a set of characteristic regions per class, but alsothe discovered region is consistent with human expert annotations. We alsopropose a new framework to recognize the Roman coins which exploitshierarchical structure of the ancient Roman coins using the state-of-the-artclassification power of the CNNs adopted to a new task of coin classification.Experimental results show that the proposed framework is able to effectivelyrecognize the ancient Roman coins. For this research, we have collected a newRoman coin dataset where all coins are annotated and consist of observe (head)and reverse (tail) images.
arxiv-12300-89 | Scalable Discrete Sampling as a Multi-Armed Bandit Problem | http://arxiv.org/pdf/1506.09039v3.pdf | author:Yutian Chen, Zoubin Ghahramani category:stat.ML cs.LG published:2015-06-30 summary:Drawing a sample from a discrete distribution is one of the buildingcomponents for Monte Carlo methods. Like other sampling algorithms, discretesampling suffers from the high computational burden in large-scale inferenceproblems. We study the problem of sampling a discrete random variable with ahigh degree of dependency that is typical in large-scale Bayesian inference andgraphical models, and propose an efficient approximate solution with asubsampling approach. We make a novel connection between the discrete samplingand Multi-Armed Bandits problems with a finite reward population and providethree algorithms with theoretical guarantees. Empirical evaluations show therobustness and efficiency of the approximate algorithms in both synthetic andreal-world large-scale problems.
arxiv-12300-90 | Online Learning to Sample | http://arxiv.org/pdf/1506.09016v2.pdf | author:Guillaume Bouchard, Théo Trouillon, Julien Perez, Adrien Gaidon category:cs.LG cs.CV cs.NA math.OC stat.ML published:2015-06-30 summary:Stochastic Gradient Descent (SGD) is one of the most widely used techniquesfor online optimization in machine learning. In this work, we accelerate SGD byadaptively learning how to sample the most useful training examples at eachtime step. First, we show that SGD can be used to learn the best possiblesampling distribution of an importance sampling estimator. Second, we show thatthe sampling distribution of an SGD algorithm can be estimated online byincrementally minimizing the variance of the gradient. The resulting algorithm- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters tooptimize, as well as a set of parameters to sample learning examples. We showthat AWSGD yields faster convergence in three different applications: (i) imageclassification with deep features, where the sampling of images depends ontheir labels, (ii) matrix factorization, where rows and columns are not sampleduniformly, and (iii) reinforcement learning, where the optimized andexploration policies are estimated at the same time, where our approachcorresponds to an off-policy gradient algorithm.
arxiv-12300-91 | Gaussian Process for Noisy Inputs with Ordering Constraints | http://arxiv.org/pdf/1507.00052v2.pdf | author:Cuong Tran, Vladimir Pavlovic, Robert Kopp category:stat.ML published:2015-06-30 summary:We study the Gaussian Process regression model in the context of trainingdata with noise in both input and output. The presence of two sources of noisemakes the task of learning accurate predictive models extremely challenging.However, in some instances additional constraints may be available that canreduce the uncertainty in the resulting predictive models. In particular, weconsider the case of monotonically ordered latent input, which occurs in manyapplication domains that deal with temporal data. We present a novel inferenceand learning approach based on non-parametric Gaussian variationalapproximation to learn the GP model while taking into account the newconstraints. The resulting strategy allows one to gain access to posteriorestimates of both the input and the output and results in improved predictiveperformance. We compare our proposed models to state-of-the-art Noisy InputGaussian Process (NIGP) and other competing approaches on synthetic and realsea-level rise data. Experimental results suggest that the proposed approachconsistently outperforms selected methods while, at the same time, reducing thecomputational costs of learning and inference.
arxiv-12300-92 | Notes on Low-rank Matrix Factorization | http://arxiv.org/pdf/1507.00333v3.pdf | author:Yuan Lu, Jie Yang category:cs.NA cs.IR cs.LG published:2015-06-30 summary:Low-rank matrix factorization (MF) is an important technique in data science.The key idea of MF is that there exists latent structures in the data, byuncovering which we could obtain a compressed representation of the data. Byfactorizing an original matrix to low-rank matrices, MF provides a unifiedmethod for dimension reduction, clustering, and matrix completion. In thisarticle we review several important variants of MF, including: Basic MF,Non-negative MF, Orthogonal non-negative MF. As can be told from their names,non-negative MF and orthogonal non-negative MF are variants of basic MF withnon-negativity and/or orthogonality constraints. Such constraints are useful inspecific senarios. In the first part of this article, we introduce, for each ofthese models, the application scenarios, the distinctive properties, and theoptimizing method. By properly adapting MF, we can go beyond the problem ofclustering and matrix completion. In the second part of this article, we willextend MF to sparse matrix compeletion, enhance matrix compeletion usingvarious regularization methods, and make use of MF for (semi-)supervisedlearning by introducing latent space reinforcement and transformation. We willsee that MF is not only a useful model but also as a flexible framework that isapplicable for various prediction problems.
arxiv-12300-93 | Multi-Cue Structure Preserving MRF for Unconstrained Video Segmentation | http://arxiv.org/pdf/1506.09124v1.pdf | author:Saehoon Yi, Vladimir Pavlovic category:cs.CV published:2015-06-30 summary:Video segmentation is a stepping stone to understanding video context. Videosegmentation enables one to represent a video by decomposing it into coherentregions which comprise whole or parts of objects. However, the challengeoriginates from the fact that most of the video segmentation algorithms arebased on unsupervised learning due to expensive cost of pixelwise videoannotation and intra-class variability within similar unconstrained videoclasses. We propose a Markov Random Field model for unconstrained videosegmentation that relies on tight integration of multiple cues: vertices aredefined from contour based superpixels, unary potentials from temporal smoothlabel likelihood and pairwise potentials from global structure of a video.Multi-cue structure is a breakthrough to extracting coherent object regions forunconstrained videos in absence of supervision. Our experiments on VSB100dataset show that the proposed model significantly outperforms competingstate-of-the-art algorithms. Qualitative analysis illustrates that videosegmentation result of the proposed model is consistent with human perceptionof objects.
arxiv-12300-94 | Framework for Multi-task Multiple Kernel Learning and Applications in Genome Analysis | http://arxiv.org/pdf/1506.09153v1.pdf | author:Christian Widmer, Marius Kloft, Vipin T Sreedharan, Gunnar Rätsch category:stat.ML cs.CE cs.LG published:2015-06-30 summary:We present a general regularization-based framework for Multi-task learning(MTL), in which the similarity between tasks can be learned or refined using$\ell_p$-norm Multiple Kernel learning (MKL). Based on this very generalformulation (including a general loss function), we derive the correspondingdual formulation using Fenchel duality applied to Hermitian matrices. We showthat numerous established MTL methods can be derived as special cases fromboth, the primal and dual of our formulation. Furthermore, we derive a moderndual-coordinate descend optimization strategy for the hinge-loss variant of ourformulation and provide convergence bounds for our algorithm. As a specialcase, we implement in C++ a fast LibLinear-style solver for $\ell_p$-norm MKL.In the experimental section, we analyze various aspects of our algorithm suchas predictive performance and ability to reconstruct task relationships onbiologically inspired synthetic data, where we have full control over theunderlying ground truth. We also experiment on a new dataset from the domain ofcomputational biology that we collected for the purpose of this paper. Itconcerns the prediction of transcription start sites (TSS) over nine organisms,which is a crucial task in gene finding. Our solvers including all discussedspecial cases are made available as open-source software as part of the SHOGUNmachine learning toolbox (available at \url{http://shogun.ml}).
arxiv-12300-95 | Human Shape Variation - An Efficient Implementation using Skeleton | http://arxiv.org/pdf/1506.08682v1.pdf | author:Dhriti Sengupta, Merina Kundu, Jayati Ghosh Dastidar category:cs.CV published:2015-06-29 summary:It is at times important to detect human presence automatically in secureenvironments. This needs a shape recognition algorithm that is robust, fast andhas low error rates. The algorithm needs to process camera images quickly todetect any human in the range of vision, and generate alerts, especially if theobject under scrutiny is moving in certain directions. We present here asimple, efficient and fast algorithm using skeletons of the images, and simplefeatures like posture and length of the object.
arxiv-12300-96 | Portfolio optimization using local linear regression ensembles in RapidMiner | http://arxiv.org/pdf/1506.08690v1.pdf | author:Gabor Nagy, Gergo Barta, Tamas Henk category:q-fin.PM cs.LG stat.ML published:2015-06-29 summary:In this paper we implement a Local Linear Regression Ensemble Committee(LOLREC) to predict 1-day-ahead returns of 453 assets form the S&P500. Theestimates and the historical returns of the committees are used to compute theweights of the portfolio from the 453 stock. The proposed method outperformsbenchmark portfolio selection strategies that optimize the growth rate of thecapital. We investigate the effect of algorithm parameter m: the number ofselected stocks on achieved average annual yields. Results suggest thealgorithm's practical usefulness in everyday trading.
arxiv-12300-97 | Automatic Channel Network Extraction from Remotely Sensed Images by Singularity Analysis | http://arxiv.org/pdf/1506.08670v1.pdf | author:F. Isikdogan, A. C. Bovik, P. Passalacqua category:cs.CV published:2015-06-29 summary:Quantitative analysis of channel networks plays an important role in riverstudies. To provide a quantitative representation of channel networks, wepropose a new method that extracts channels from remotely sensed images andestimates their widths. Our fully automated method is based on a recentlyproposed Multiscale Singularity Index that responds strongly to curvilinearstructures but weakly to edges. The algorithm produces a channel map, using asingle image where water and non-water pixels have contrast, such as a Landsatnear-infrared band image or a water index defined on multiple bands. Theproposed method provides a robust alternative to the procedures that are usedin remote sensing of fluvial geomorphology and makes classification andanalysis of channel networks easier. The source code of the algorithm isavailable at: http://live.ece.utexas.edu/research/cne/.
arxiv-12300-98 | Detecting Table Region in PDF Documents Using Distant Supervision | http://arxiv.org/pdf/1506.08891v6.pdf | author:Miao Fan, Doo Soon Kim category:cs.CV cs.IR published:2015-06-29 summary:Superior to state-of-the-art approaches which compete in table recognitionwith 67 annotated government reports in PDF format released by {\it ICDAR 2013Table Competition}, this paper contributes a novel paradigm leveraginglarge-scale unlabeled PDF documents to open-domain table detection. Weintegrate the paradigm into our latest developed system ({\it PdfExtra}) todetect the region of tables by means of 9,466 academic articles from the entirerepository of {\it ACL Anthology}, where almost all papers are archived by PDFformat without annotation for tables. The paradigm first designs heuristics toautomatically construct weakly labeled data. It then feeds diverse evidences,such as layouts of documents and linguistic features, which are extracted by{\it Apache PDFBox} and processed by {\it Stanford NLP} toolkit, into differentcanonical classifiers. We finally use these classifiers, i.e. {\it NaiveBayes}, {\it Logistic Regression} and {\it Support Vector Machine}, tocollaboratively vote on the region of tables. Experimental results show that{\it PdfExtra} achieves a great leap forward, compared with thestate-of-the-art approach. Moreover, we discuss the factors of differentfeatures, learning models and even domains of documents that may impact theperformance. Extensive evaluations demonstrate that our paradigm is compatibleenough to leverage various features and learning models for open-domain tableregion detection within PDF files.
arxiv-12300-99 | Dropout as data augmentation | http://arxiv.org/pdf/1506.08700v4.pdf | author:Xavier Bouthillier, Kishore Konda, Pascal Vincent, Roland Memisevic category:stat.ML cs.LG published:2015-06-29 summary:Dropout is typically interpreted as bagging a large number of models sharingparameters. We show that using dropout in a network can also be interpreted asa kind of data augmentation in the input space without domain knowledge. Wepresent an approach to projecting the dropout noise within a network back intothe input space, thereby generating augmented versions of the training data,and we show that training a deterministic network on the augmented samplesyields similar results. Finally, we propose a new dropout noise scheme based onour observations and show that it improves dropout results without addingsignificant computational cost.
arxiv-12300-100 | Variational Inference for Background Subtraction in Infrared Imagery | http://arxiv.org/pdf/1506.08581v1.pdf | author:Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis category:cs.CV cs.LG published:2015-06-29 summary:We propose a Gaussian mixture model for background subtraction in infraredimagery. Following a Bayesian approach, our method automatically estimates thenumber of Gaussian components as well as their parameters, while simultaneouslyit avoids over/under fitting. The equations for estimating model parameters areanalytically derived and thus our method does not require any samplingalgorithm that is computationally and memory inefficient. The pixel densityestimate is followed by an efficient and highly accurate updating mechanism,which permits our system to be automatically adapted to dynamically changingoperation conditions. Experimental results and comparisons with other methodsshow that our method outperforms, in terms of precision and recall, while atthe same time it keeps computational cost suitable for real-time applications.
arxiv-12300-101 | Exact and approximate inference in graphical models: variable elimination and beyond | http://arxiv.org/pdf/1506.08544v1.pdf | author:Nathalie Peyrard, Simon de Givry, Alain Franc, Stéphane Robin, Régis Sabbadin, Thomas Schiex, Matthieu Vignes category:stat.ML cs.AI cs.LG published:2015-06-29 summary:Probabilistic graphical models offer a powerful framework to account for thedependence structure between variables, which can be represented as a graph.The dependence between variables may render inference tasks such as computingnormalizing constant, marginalization or optimization intractable. Theobjective of this paper is to review techniques exploiting the graph structurefor exact inference borrowed from optimization and computer science. They arenot yet standard in the statistician toolkit, and we specify under whichconditions they are efficient in practice. They are built on the principle ofvariable elimination whose complexity is dictated in an intricate way by theorder in which variables are eliminated in the graph. The so-called treewidthof the graph characterizes this algorithmic complexity: low-treewidth graphscan be processed efficiently. Algorithmic solutions derived from variableelimination and the notion of treewidth are illustrated on problems oftreewidth computation and inference in challenging benchmarks from optimizationcompetitions. We also review how efficient techniques for approximate inferencesuch as loopy belief propagation and variational approaches can be linked tovariable elimination and we illustrate them in the context ofExpectation-Maximisation procedures for parameter estimation in coupled HiddenMarkov Models.
arxiv-12300-102 | Tell and Predict: Kernel Classifier Prediction for Unseen Visual Classes from Unstructured Text Descriptions | http://arxiv.org/pdf/1506.08529v1.pdf | author:Mohamed Elhoseiny, Ahmed Elgammal, Babak Saleh category:cs.CV published:2015-06-29 summary:In this paper we propose a framework for predicting kernelized classifiers inthe visual domain for categories with no training images where the knowledgecomes from textual description about these categories. Through our optimizationframework, the proposed approach is capable of embedding the class-levelknowledge from the text domain as kernel classifiers in the visual domain. Wealso proposed a distributional semantic kernel between text descriptions whichis shown to be effective in our setting. The proposed framework is notrestricted to textual descriptions, and can also be applied to other formsknowledge representations. Our approach was applied for the challenging task ofzero-shot learning of fine-grained categories from text descriptions of thesecategories.
arxiv-12300-103 | Integrative analysis of gene expression and phenotype data | http://arxiv.org/pdf/1506.08511v1.pdf | author:Min Xu category:q-bio.QM q-bio.GN q-bio.MN stat.ML published:2015-06-29 summary:The linking genotype to phenotype is the fundamental aim of modern genetics.We focus on study of links between gene expression data and phenotype datathrough integrative analysis. We propose three approaches. 1) The inherent complexity of phenotypes makes high-throughput phenotypeprofiling a very difficult and laborious process. We propose a method ofautomated multi-dimensional profiling which uses gene expression similarity.Large-scale analysis show that our method can provide robust profiling thatreveals different phenotypic aspects of samples. This profiling technique isalso capable of interpolation and extrapolation beyond the phenotypeinformation given in training data. It can be used in many applications,including facilitating experimental design and detecting confounding factors. 2) Phenotype association analysis problems are complicated by small samplesize and high dimensionality. Consequently, phenotype-associated gene subsetsobtained from training data are very sensitive to selection of trainingsamples, and the constructed sample phenotype classifiers tend to have poorgeneralization properties. To eliminate these obstacles, we propose a novelapproach that generates sequences of increasingly discriminative gene clustercombinations. Our experiments on both simulated and real datasets show robustand accurate classification performance. 3) Many complex phenotypes, such as cancer, are the product of not only geneexpression, but also gene interaction. We propose an integrative approach tofind gene network modules that activate under different phenotype conditions.Using our method, we discovered cancer subtype-specific network modules, aswell as the ways in which these modules coordinate. In particular, we detecteda breast-cancer specific tumor suppressor network module with a hub gene,PDGFRL, which may play an important role in this module.
arxiv-12300-104 | Compressed Sensing of Multi-Channel EEG Signals: The Simultaneous Cosparsity and Low Rank Optimization | http://arxiv.org/pdf/1506.08499v1.pdf | author:Yipeng Liu, Maarten De Vos, Sabine Van Huffel category:cs.IT math.IT stat.ML published:2015-06-29 summary:Goal: This paper deals with the problems that some EEG signals have no goodsparse representation and single channel processing is not computationallyefficient in compressed sensing of multi-channel EEG signals. Methods: Anoptimization model with L0 norm and Schatten-0 norm is proposed to enforcecosparsity and low rank structures in the reconstructed multi-channel EEGsignals. Both convex relaxation and global consensus optimization withalternating direction method of multipliers are used to compute theoptimization model. Results: The performance of multi-channel EEG signalreconstruction is improved in term of both accuracy and computationalcomplexity. Conclusion: The proposed method is a better candidate than previoussparse signal recovery methods for compressed sensing of EEG signals.Significance: The proposed method enables successful compressed sensing of EEGsignals even when the signals have no good sparse representation. Usingcompressed sensing would much reduce the power consumption of wireless EEGsystem.
arxiv-12300-105 | The Multi-Strand Graph for a PTZ Tracker | http://arxiv.org/pdf/1506.08485v1.pdf | author:Shachaf Melman, Yael Moses, Gérard Medioni, Yinghao Cai category:cs.CV published:2015-06-29 summary:High-resolution images can be used to resolve matching ambiguities betweentrajectory fragments (tracklets), which is one of the main challenges inmultiple target tracking. A PTZ camera, which can pan, tilt and zoom, is apowerful and efficient tool that offers both close-up views and wide areacoverage on demand. The wide-area view makes it possible to track many targetswhile the close-up view allows individuals to be identified fromhigh-resolution images of their faces. A central component of a PTZ trackingsystem is a scheduling algorithm that determines which target to zoom in on. In this paper we study this scheduling problem from a theoreticalperspective, where the high resolution images are also used for trackletmatching. We propose a novel data structure, the Multi-Strand Tracking Graph(MSG), which represents the set of tracklets computed by a tracker and thepossible associations between them. The MSG allows efficient scheduling as wellas resolving -- directly or by elimination -- matching ambiguities betweentracklets. The main feature of the MSG is the auxiliary data saved in eachvertex, which allows efficient computation while avoiding time-consuming graphtraversal. Synthetic data simulations are used to evaluate our schedulingalgorithm and to demonstrate its superiority over a na\"ive one.
arxiv-12300-106 | Machine learning for many-body physics: efficient solution of dynamical mean-field theory | http://arxiv.org/pdf/1506.08858v1.pdf | author:Louis-François Arsenault, O. Anatole von Lilienfeld, Andrew J. Millis category:stat.ML published:2015-06-29 summary:Machine learning methods for solving the equations of dynamical mean-fieldtheory are developed. The method is demonstrated on the three dimensionalHubbard model. The key technical issues are defining a mapping of an inputfunction to an output function, and distinguishing metallic from insulatingsolutions. Both metallic and Mott insulator solutions can be predicted. Thevalidity of the machine learning scheme is assessed by comparing predictions offull correlation functions, of quasi-particle weight and particle density tovalues directly computed. The results indicate that with modest furtherdevelopment, machine learning approach may be an attractive computationalefficient option for real materials predictions for strongly correlatedsystems.
arxiv-12300-107 | Efficient and Parsimonious Agnostic Active Learning | http://arxiv.org/pdf/1506.08669v3.pdf | author:Tzu-Kuo Huang, Alekh Agarwal, Daniel J. Hsu, John Langford, Robert E. Schapire category:cs.LG stat.ML published:2015-06-29 summary:We develop a new active learning algorithm for the streaming settingsatisfying three important properties: 1) It provably works for any classifierrepresentation and classification problem including those with severe noise. 2)It is efficiently implementable with an ERM oracle. 3) It is more aggressivethan all previous approaches satisfying 1 and 2. To do this we create analgorithm based on a newly defined optimization problem and analyze it. We alsoconduct the first experimental analysis of all efficient agnostic activelearning algorithms, evaluating their strengths and weaknesses in differentsettings.
arxiv-12300-108 | Spectral Motion Synchronization in SE(3) | http://arxiv.org/pdf/1506.08765v1.pdf | author:Federica Arrigoni, Andrea Fusiello, Beatrice Rossi category:cs.CV published:2015-06-29 summary:This paper addresses the problem of motion synchronization (or averaging) anddescribes a simple, closed-form solution based on a spectral decomposition,which does not consider rotation and translation separately but works straightin SE(3), the manifold of rigid motions. Besides its theoretical interest,being the first closed form solution in SE(3), experimental results show thatit compares favourably with the state of the art both in terms of precision andspeed.
arxiv-12300-109 | Statistical Inference using the Morse-Smale Complex | http://arxiv.org/pdf/1506.08826v1.pdf | author:Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman category:math.ST stat.ME stat.ML stat.TH published:2015-06-29 summary:The Morse-Smale complex decomposes the sample space into cells where a givenfunction $f$ is increasing or decreasing. When applied to nonparametric densityestimation and regression, it provides a way to represent, visualize andcompare functions, even in high dimensions. In this paper, we study theestimation of the Morse-Smale complex and we use our results for a variety ofstatistical problems including: nonparametric two-sample testing, densityestimation, nonparametric regression and mode clustering.
arxiv-12300-110 | On Design Mining: Coevolution and Surrogate Models | http://arxiv.org/pdf/1506.08781v4.pdf | author:Richard J. Preen, Larry Bull category:cs.NE cs.AI cs.CE published:2015-06-29 summary:Design mining is the use of computational intelligence techniques toiteratively search and model the attribute space of physical objects evaluateddirectly through rapid prototyping to meet given objectives. It enables theexploitation of novel materials and processes without formal models or complexsimulation. In this paper, we focus upon the coevolutionary nature of thedesign process when it is decomposed into concurrent sub-design threads due tothe overall complexity of the task. Using an abstract, tuneable model ofcoevolution we consider strategies to sample sub-thread designs for wholesystem testing, how best to construct and use surrogate models within thecoevolutionary scenario, and the effects of access to multiple whole system(physical) testing equipment on performance. Drawing on our findings, the paperthen describes the effective design of an array of six heterogeneousvertical-axis wind turbines.
arxiv-12300-111 | S2: An Efficient Graph Based Active Learning Algorithm with Application to Nonparametric Classification | http://arxiv.org/pdf/1506.08760v1.pdf | author:Gautam Dasarathy, Robert Nowak, Xiaojin Zhu category:cs.LG stat.ML published:2015-06-29 summary:This paper investigates the problem of active learning for binary labelprediction on a graph. We introduce a simple and label-efficient algorithmcalled S2 for this task. At each step, S2 selects the vertex to be labeledbased on the structure of the graph and all previously gathered labels.Specifically, S2 queries for the label of the vertex that bisects the *shortestshortest* path between any pair of oppositely labeled vertices. We present atheoretical estimate of the number of queries S2 needs in terms of a novelparametrization of the complexity of binary functions on graphs. We alsopresent experimental results demonstrating the performance of S2 on both realand synthetic data. While other graph-based active learning algorithms haveshown promise in practice, our algorithm is the first with both goodperformance and theoretical guarantees. Finally, we demonstrate theimplications of the S2 algorithm to the theory of nonparametric activelearning. In particular, we show that S2 achieves near minimax optimal excessrisk for an important class of nonparametric classification problems.
arxiv-12300-112 | An automatic and efficient foreground object extraction scheme | http://arxiv.org/pdf/1506.08704v1.pdf | author:Subhajit Adhikari, Joydeep Kar, Jayati Ghosh Dastidar category:cs.CV published:2015-06-29 summary:This paper presents a method to differentiate the foreground objects from thebackground of a color image. Firstly a color image of any size is input forprocessing. The algorithm converts it to a grayscale image. Next we apply cannyedge detector to find the boundary of the foreground object. We concentrate tofind the maximum distance between each boundary pixel column wise and row wiseand we fill the region that is bound by the edges. Thus we are able to extractthe grayscale values of pixels that are in the bounded region and convert thegrayscale image back to original color image containing only the foregroundobject.
arxiv-12300-113 | Bayesian Nonparametric Kernel-Learning | http://arxiv.org/pdf/1506.08776v1.pdf | author:Junier Oliva, Avinava Dubey, Barnabas Poczos, Jeff Schneider, Eric P. Xing category:stat.ML published:2015-06-29 summary:Kernel methods are ubiquitous tools in machine learning. They have proven tobe effective in many domains and tasks. Yet, kernel methods often require theuser to select a predefined kernel to build an estimator with. However, thereis often little reason for the a priori selection of a kernel. Even if auniversal approximating kernel is selected, the quality of the finite sampleestimator may be greatly effected by the choice of kernel. Furthermore, whendirectly applying kernel methods, one typically needs to compute a $N \times N$Gram matrix of pairwise kernel evaluations to work with a dataset of $N$instances. The computation of this Gram matrix precludes the direct applicationof kernel methods on large datasets. In this paper we introduce Bayesiannonparmetric kernel (BaNK) learning, a generic, data-driven framework forscalable learning of kernels. We show that this framework can be used forperforming both regression and classification tasks and scale to largedatasets. Furthermore, we show that BaNK outperforms several other scalableapproaches for kernel learning on a variety of real world datasets.
arxiv-12300-114 | A simple yet efficient algorithm for multiple kernel learning under elastic-net constraints | http://arxiv.org/pdf/1506.08536v2.pdf | author:Luca Citi category:stat.ML cs.LG published:2015-06-29 summary:This report presents an algorithm for the solution of multiple kernellearning (MKL) problems with elastic-net constraints on the kernel weights.
arxiv-12300-115 | Requirement Tracing using Term Extraction | http://arxiv.org/pdf/1506.08789v1.pdf | author:Najla Al-Saati, Raghda Abdul-Jaleel category:cs.SE cs.CL cs.IR published:2015-06-29 summary:Requirements traceability is an essential step in ensuring the quality ofsoftware during the early stages of its development life cycle. Requirementstracing usually consists of document parsing, candidate link generation andevaluation and traceability analysis. This paper demonstrates the applicabilityof Statistical Term Extraction metrics to generate candidate links. It isapplied and validated using two data sets and four types of filters two foreach data set, 0.2 and 0.25 for MODIS, 0 and 0.05 for CM1. This methodgenerates requirements traceability matrices between textual requirementsartifacts (such as high-level requirements traced to low-level requirements).The proposed method includes ten word frequency metrics divided into three maingroups for calculating the frequency of terms. The results show that theproposed method gives better result when compared with the traditional TF-IDFmethod.
arxiv-12300-116 | Tracking Direction of Human Movement - An Efficient Implementation using Skeleton | http://arxiv.org/pdf/1506.08815v1.pdf | author:Merina Kundu, Dhriti Sengupta, Jayati Ghosh Dastidar category:cs.CV published:2015-06-29 summary:Sometimes a simple and fast algorithm is required to detect human presenceand movement with a low error rate in a controlled environment for securitypurposes. Here a light weight algorithm has been presented that generates alerton detection of human presence and its movement towards a certain direction.The algorithm uses fixed angle CCTV camera images taken over time and reliesupon skeleton transformation of successive images and calculation of differencein their coordinates.
arxiv-12300-117 | Neural Simpletrons - Minimalistic Directed Generative Networks for Learning with Few Labels | http://arxiv.org/pdf/1506.08448v3.pdf | author:Dennis Forster, Abdul-Saboor Sheikh, Jörg Lücke category:stat.ML cs.LG published:2015-06-28 summary:Deep learning is intensively studied using supervised and unsupervisedlearning, and by applying probabilistic, deterministic, and bio-inspiredapproaches. Comparisons of different approaches such as generative anddiscriminative neural networks is made difficult, however, because ofdifferences in the semantics of their graphical descriptions, differentlearning methods, different benchmarking objectives and different scalability.To allow for a direct functional comparison, we here study a generativemulti-layer neural network in a form and setting as similar to standarddiscriminative networks as possible. Based on normalized Poisson mixtures, wederive a minimalistic deep neural network with local activation and learningrules. The network learns in a semi-supervised setting and can be scaled usingstandard deep learning tools for parallelized implementations. Empiricalevaluations on standard benchmarks show that for weakly labeled data thederived minimalistic network improves on all standard deep learning approachesand is competitive with their recent variants. In comparison to recentbio-inspired approaches it suggests further improvements through top-downconnections. Furthermore, we find that the studied network is the bestperforming monolithic (`non-hybrid') system for few labels, and that it can beapplied in the limit of very few labels, where no other system has beenreported to operate so far.
arxiv-12300-118 | Deep-Plant: Plant Identification with convolutional neural networks | http://arxiv.org/pdf/1506.08425v1.pdf | author:Sue Han Lee, Chee Seng Chan, Paul Wilkin, Paolo Remagnino category:cs.CV cs.AI cs.NE published:2015-06-28 summary:This paper studies convolutional neural networks (CNN) to learn unsupervisedfeature representations for 44 different plant species, collected at the RoyalBotanic Gardens, Kew, England. To gain intuition on the chosen features fromthe CNN model (opposed to a 'black box' solution), a visualisation techniquebased on the deconvolutional networks (DN) is utilized. It is found thatvenations of different order have been chosen to uniquely represent each of theplant species. Experimental results using these CNN features with differentclassifiers show consistency and superiority compared to the state-of-the artsolutions which rely on hand-crafted features.
arxiv-12300-119 | Topic2Vec: Learning Distributed Representations of Topics | http://arxiv.org/pdf/1506.08422v1.pdf | author:Li-Qiang Niu, Xin-Yu Dai category:cs.CL cs.LG published:2015-06-28 summary:Latent Dirichlet Allocation (LDA) mining thematic structure of documentsplays an important role in nature language processing and machine learningareas. However, the probability distribution from LDA only describes thestatistical relationship of occurrences in the corpus and usually in practice,probability is not the best choice for feature representations. Recently,embedding methods have been proposed to represent words and documents bylearning essential concepts and representations, such as Word2Vec and Doc2Vec.The embedded representations have shown more effectiveness than LDA-stylerepresentations in many tasks. In this paper, we propose the Topic2Vec approachwhich can learn topic representations in the same semantic vector space withwords, as an alternative to probability. The experimental results show thatTopic2Vec achieves interesting and meaningful results.
arxiv-12300-120 | Unsupervised Semantic Parsing of Video Collections | http://arxiv.org/pdf/1506.08438v4.pdf | author:Ozan Sener, Amir Zamir, Silvio Savarese, Ashutosh Saxena category:cs.CV published:2015-06-28 summary:Human communication typically has an underlying structure. This is reflectedin the fact that in many user generated videos, a starting point, ending, andcertain objective steps between these two can be identified. In this paper, wepropose a method for parsing a video into such semantic steps in anunsupervised way. The proposed method is capable of providing a semantic"storyline" of the video composed of its objective steps. We accomplish thisusing both visual and language cues in a joint generative model. The proposedmethod can also provide a textual description for each of the identifiedsemantic steps and video segments. We evaluate this method on a large number ofcomplex YouTube videos and show results of unprecedented quality for thisintricate and impactful problem.
arxiv-12300-121 | Improved Deep Speaker Feature Learning for Text-Dependent Speaker Recognition | http://arxiv.org/pdf/1506.08349v1.pdf | author:Lantian Li, Yiye Lin, Zhiyong Zhang, Dong Wang category:cs.CL cs.LG cs.NE published:2015-06-28 summary:A deep learning approach has been proposed recently to derive speakeridentifies (d-vector) by a deep neural network (DNN). This approach has beenapplied to text-dependent speaker recognition tasks and shows reasonableperformance gains when combined with the conventional i-vector approach.Although promising, the existing d-vector implementation still can not competewith the i-vector baseline. This paper presents two improvements for the deeplearning approach: a phonedependent DNN structure to normalize phone variation,and a new scoring approach based on dynamic time warping (DTW). Experiments ona text-dependent speaker recognition task demonstrated that the proposedmethods can provide considerable performance improvement over the existingd-vector implementation.
arxiv-12300-122 | Simultaneously Solving Computational Problems Using an Artificial Chemical Reactor | http://arxiv.org/pdf/1506.08361v1.pdf | author:Jaderick P. Pabico category:cs.ET cs.NE published:2015-06-28 summary:This paper is centered on using chemical reaction as a computational metaphorfor simultaneously solving problems. An artificial chemical reactor that cansimultaneously solve instances of three unrelated problems was created. Thereactor is a distributed stochastic algorithm that simulates a chemicaluniverse wherein the molecular species are being represented either by a humangenomic contig panel, a Hamiltonian cycle, or an aircraft landing schedule. Thechemical universe is governed by reactions that can alter genomic sequences,re-order Hamiltonian cycles, or reschedule an aircraft landing program.Molecular masses were considered as measures of goodness of solutions, andrepresented radiation hybrid (RH) vector similarities, costs of Hamiltoniancycles, and penalty costs for landing an aircraft before and after targetlanding times. This method, tested by solving in tandem with deterministicalgorithms, has been shown to find quality solutions in finding the minima RHvector similarities of genomic data, minima costs in Hamiltonian cycles of thetraveling salesman, and minima costs for landing aircrafts before or aftertarget landing times.
arxiv-12300-123 | Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods | http://arxiv.org/pdf/1506.08473v3.pdf | author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG cs.NE stat.ML published:2015-06-28 summary:Training neural networks is a challenging non-convex optimization problem,and backpropagation or gradient descent can get stuck in spurious local optima.We propose a novel algorithm based on tensor decomposition for guaranteedtraining of two-layer neural networks. We provide risk bounds for our proposedmethod, with a polynomial sample complexity in the relevant parameters, such asinput dimension and number of neurons. While learning arbitrary targetfunctions is NP-hard, we provide transparent conditions on the function and theinput for learnability. Our training method is based on tensor decomposition,which provably converges to the global optimum, under a set of mildnon-degeneracy conditions. It consists of simple embarrassingly parallel linearand multi-linear operations, and is competitive with standard stochasticgradient descent (SGD), in terms of computational complexity. Thus, we proposea computationally efficient method with guaranteed risk bounds for trainingneural networks with one hidden layer.
arxiv-12300-124 | WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Information Extraction | http://arxiv.org/pdf/1506.08454v1.pdf | author:Vijil Chenthamarakshan, Prasad M Desphande, Raghu Krishnapuram, Ramakrishna Varadarajan, Knut Stolze category:cs.CL cs.DB cs.IR published:2015-06-28 summary:The visual layout of a webpage can provide valuable clues for certain typesof Information Extraction (IE) tasks. In traditional rule based IE frameworks,these layout cues are mapped to rules that operate on the HTML source of thewebpages. In contrast, we have developed a framework in which the rules can bespecified directly at the layout level. This has many advantages, since thehigher level of abstraction leads to simpler extraction rules that are largelyindependent of the source code of the page, and, therefore, more robust. It canalso enable specification of new types of rules that are not otherwisepossible. To the best of our knowledge, there is no general framework thatallows declarative specification of information extraction rules based onspatial layout. Our framework is complementary to traditional text based rulesframework and allows a seamless combination of spatial layout based rules withtraditional text based rules. We describe the algebra that enables such asystem and its efficient implementation using standard relational and textindexing features of a relational database. We demonstrate the simplicity andefficiency of this system for a task involving the extraction of softwaresystem requirements from software product pages.
arxiv-12300-125 | Stochastic Gradient Made Stable: A Manifold Propagation Approach for Large-Scale Optimization | http://arxiv.org/pdf/1506.08350v2.pdf | author:Yadong Mu, Wei Liu, Wei Fan category:cs.LG cs.NA 90C06 published:2015-06-28 summary:Stochastic gradient descent (SGD) holds as a classical method to build largescale machine learning models over big data. A stochastic gradient is typicallycalculated from a limited number of samples (known as mini-batch), so itpotentially incurs a high variance and causes the estimated parameters bouncearound the optimal solution. To improve the stability of stochastic gradient,recent years have witnessed the proposal of several semi-stochastic gradientdescent algorithms, which distinguish themselves from standard SGD byincorporating global information into gradient computation. In this paper wecontribute a novel stratified semi-stochastic gradient descent (S3GD) algorithmto this nascent research area, accelerating the optimization of a large familyof composite convex functions. Though theoretically converging faster, priorsemi-stochastic algorithms are found to suffer from high iteration complexity,which makes them even slower than SGD in practice on many datasets. In ourproposed S3GD, the semi-stochastic gradient is calculated based on efficientmanifold propagation, which can be numerically accomplished by sparse matrixmultiplications. This way S3GD is able to generate a highly-accurate estimateof the exact gradient from each mini-batch with largely-reduced computationalcomplexity. Theoretic analysis reveals that the proposed S3GD elegantlybalances the geometric algorithmic convergence rate against the space and timecomplexities during the optimization. The efficacy of S3GD is alsoexperimentally corroborated on several large-scale benchmark datasets.
arxiv-12300-126 | Robustness Analysis of Preconditioned Successive Projection Algorithm for General Form of Separable NMF Problem | http://arxiv.org/pdf/1506.08387v2.pdf | author:Tomohiko Mizutani category:stat.ML math.OC published:2015-06-28 summary:The successive projection algorithm (SPA) has been known to work well forseparable nonnegative matrix factorization (NMF) problems arising inapplications, such as topic extraction from documents and endmember detectionin hyperspectral images. One of the reasons is in that the algorithm is robustto noise. Gillis and Vavasis showed in [SIAM J. Optim., 25(1), pp. 677-698,2015] that a preconditioner can further enhance its noise robustness. The proofrested on the condition that the dimension $d$ and factorization rank $r$ inthe separable NMF problem coincide with each other. However, it may beunrealistic to expect that the condition holds in separable NMF problemsappearing in actual applications; in such problems, $d$ is usually greater than$r$. This paper shows, without the condition $d=r$, that the preconditioned SPAis robust to noise.
arxiv-12300-127 | Occlusion Coherence: Detecting and Localizing Occluded Faces | http://arxiv.org/pdf/1506.08347v1.pdf | author:Golnaz Ghiasi, Charless C. Fowlkes category:cs.CV published:2015-06-28 summary:The presence of occluders significantly impacts object recognition accuracy.However, occlusion is typically treated as an unstructured source of noise andexplicit models for occluders have lagged behind those for object appearanceand shape. In this paper we describe a hierarchical deformable part model forface detection and keypoint localization that explicitly models part occlusion.The proposed model structure makes it possible to augment positive trainingdata with large numbers of synthetically occluded instances. This allows us toeasily incorporate the statistics of occlusion patterns in a discriminativelytrained model. We test the model on several benchmarks for keypointlocalization and detection including challenging data sets featuringsignificant occlusion. We find that the addition of an explicit model ofocclusion yields a system that outperforms existing approaches in keypointlocalization accuracy and detection performance.
arxiv-12300-128 | Patch-Based Low-Rank Minimization for Image Denoising | http://arxiv.org/pdf/1506.08353v1.pdf | author:Haijuan Hu, Jacques Froment, Quansheng Liu category:cs.CV published:2015-06-28 summary:Patch-based sparse representation and low-rank approximation for imageprocessing attract much attention in recent years. The minimization of thematrix rank coupled with the Frobenius norm data fidelity can be solved by thehard thresholding filter with principle component analysis (PCA) or singularvalue decomposition (SVD). Based on this idea, we propose a patch-basedlow-rank minimization method for image denoising, which learns compactdictionaries from similar patches with PCA or SVD, and applies simple hardthresholding filters to shrink the representation coefficients. Compared torecent patch-based sparse representation methods, experiments demonstrate thatthe proposed method is not only rather rapid, but also effective for a varietyof natural images, especially for texture parts in images.
arxiv-12300-129 | Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization | http://arxiv.org/pdf/1506.08272v2.pdf | author:Xiangru Lian, Yijun Huang, Yuncheng Li, Ji Liu category:math.OC cs.NA stat.ML published:2015-06-27 summary:Asynchronous parallel implementations of stochastic gradient (SG) have beenbroadly used in solving deep neural network and received many successes inpractice recently. However, existing theories cannot explain their convergenceand speedup properties, mainly due to the nonconvexity of most deep learningformulations and the asynchronous parallel mechanism. To fill the gaps intheory and provide theoretical supports, this paper studies two asynchronousparallel implementations of SG: one is on the computer network and the other ison the shared memory system. We establish an ergodic convergence rate$O(1/\sqrt{K})$ for both algorithms and prove that the linear speedup isachievable if the number of workers is bounded by $\sqrt{K}$ ($K$ is the totalnumber of iterations). Our results generalize and improve existing analysis forconvex minimization.
arxiv-12300-130 | Twitter User Geolocation Using a Unified Text and Network Prediction Model | http://arxiv.org/pdf/1506.08259v3.pdf | author:Afshin Rahimi, Trevor Cohn, Timothy Baldwin category:cs.CL cs.SI published:2015-06-27 summary:We propose a label propagation approach to geolocation prediction based onModified Adsorption, with two enhancements:(1) the removal of "celebrity" nodesto increase location homophily and boost tractability, and (2) he incorporationof text-based geolocation priors for test users. Experiments over three Twitterbenchmark datasets achieve state-of-the-art results, and demonstrate theeffectiveness of the enhancements.
arxiv-12300-131 | Keypoint Encoding for Improved Feature Extraction from Compressed Video at Low Bitrates | http://arxiv.org/pdf/1506.08316v2.pdf | author:Jianshu Chao, Eckehard Steinbach category:cs.MM cs.CV cs.IR published:2015-06-27 summary:In many mobile visual analysis applications, compressed video is transmittedover a communication network and analyzed by a server. Typical processing stepsperformed at the server include keypoint detection, descriptor calculation, andfeature matching. Video compression has been shown to have an adverse effect onfeature-matching performance. The negative impact of compression can be reducedby using the keypoints extracted from the uncompressed video to calculatedescriptors from the compressed video. Based on this observation, we propose toprovide these keypoints to the server as side information and to extract onlythe descriptors from the compressed video. First, we introduce four differentframe types for keypoint encoding to address different types of changes invideo content. These frame types represent a new scene, the same scene, aslowly changing scene, or a rapidly moving scene and are determined bycomparing features between successive video frames. Then, we propose Intra,Skip and Inter modes of encoding the keypoints for different frame types. Forexample, keypoints for new scenes are encoded using the Intra mode, andkeypoints for unchanged scenes are skipped. As a result, the bitrate of theside information related to keypoint encoding is significantly reduced.Finally, we present pairwise matching and image retrieval experiments conductedto evaluate the performance of the proposed approach using the Stanford mobileaugmented reality dataset and 720p format videos. The results show that theproposed approach offers significantly improved feature matching and imageretrieval performance at a given bitrate.
arxiv-12300-132 | Occam's Gates | http://arxiv.org/pdf/1506.08251v1.pdf | author:Jonathan Raiman, Szymon Sidor category:cs.LG published:2015-06-27 summary:We present a complimentary objective for training recurrent neural networks(RNN) with gating units that helps with regularization and interpretability ofthe trained model. Attention-based RNN models have shown success in manydifficult sequence to sequence classification problems with long and short termdependencies, however these models are prone to overfitting. In this paper, wedescribe how to regularize these models through an L1 penalty on the activationof the gating units, and show that this technique reduces overfitting on avariety of tasks while also providing to us a human-interpretable visualizationof the inputs used by the network. These tasks include sentiment analysis,paraphrase recognition, and question answering.
arxiv-12300-133 | A Novel Feature Selection Approach for Analyzing High dimensional Functional MRI Data | http://arxiv.org/pdf/1506.08301v1.pdf | author:Zhiqiang Li, Yilun Wang, Yifeng Wang, Xiaona Wang, Junjie Zheng, Huafu Chen category:cs.CV cs.LG stat.ML I.5.2 published:2015-06-27 summary:Feature selection based on traditional multivariate methods is likely toobtain unstable and unreliable results in case of an extremely high dimensionalspace and very limited training samples. In order to overcome this difficulty,we introduced a novel feature selection method which combines the idea ofstability selection approach and the elastic net approach to detectdiscriminative features in a stable and robust way. This new method is appliedto functional magnetic resonance imaging (fMRI) data, whose discriminativefeatures are often correlated or redundant. Compared with the originalstability selection approach with the pure l_1 -norm regularized model servingas the baseline model, the proposed method achieves a better sensitivityempirically, because elastic net encourages a grouping effect besides sparsity.Compared with the feature selection method based on the plain Elastic Net, ourmethod achieves the finite sample control for certain error rates of falsediscoveries, transparent principle for choosing a proper amount ofregularization and the robustness of the feature selection results, due to theincorporation of the stability selection idea. A simulation study showed thatour approach are less influenced than other methods by label noise. Inaddition, the advantage in terms of better control of false discoveries andmissed discoveries of our approach was verified in a real fMRI experiment.Finally, a multi-center resting-state fMRI data about Attention-deficit/hyperactivity disorder (ADHD) suggested that the resulted classifier based onour feature selection method achieves the best and most robust predictionaccuracy.
arxiv-12300-134 | Convolutional networks and learning invariant to homogeneous multiplicative scalings | http://arxiv.org/pdf/1506.08230v4.pdf | author:Mark Tygert, Arthur Szlam, Soumith Chintala, Marc'Aurelio Ranzato, Yuandong Tian, Wojciech Zaremba category:cs.LG cs.NE published:2015-06-26 summary:The conventional classification schemes -- notably multinomial logisticregression -- used in conjunction with convolutional networks (convnets) areclassical in statistics, designed without consideration for the usual couplingwith convnets, stochastic gradient descent, and backpropagation. In thespecific application to supervised learning for convnets, a simplescale-invariant classification stage turns out to be more robust thanmultinomial logistic regression, appears to result in slightly lower errors onseveral standard test sets, has similar computational costs, and featuresprecise control over the actual rate of learning. "Scale-invariant" means thatmultiplying the input values by any nonzero scalar leaves the output unchanged.
arxiv-12300-135 | ASOC: An Adaptive Parameter-free Stochastic Optimization Techinique for Continuous Variables | http://arxiv.org/pdf/1506.08004v1.pdf | author:Jayanta Basak category:cs.NE published:2015-06-26 summary:Stochastic optimization is an important task in many optimization problemswhere the tasks are not expressible as convex optimization problems. In thecase of non-convex optimization problems, various different stochasticalgorithms like simulated annealing, evolutionary algorithms, and tabu searchare available. Most of these algorithms require user-defined parametersspecific to the problem in order to find out the optimal solution. Moreover, inmany situations, iterative fine-tunings are required for the user-definedparameters, and therefore these algorithms cannot adapt if the search space andthe optima changes over time. In this paper we propose an \underline{a}daptiveparameter-free \underline{s}tochastic \underline{o}ptimization technique for\underline{c}ontinuous random variables called ASOC.
arxiv-12300-136 | A Java Implementation of the SGA, UMDA, ECGA, and HBOA | http://arxiv.org/pdf/1506.07980v1.pdf | author:José C. Pereira, Fernando G. Lobo category:cs.NE cs.MS I.2.8 published:2015-06-26 summary:The Simple Genetic Algorithm, the Univariate Marginal Distribution Algorithm,the Extended Compact Genetic Algorithm, and the Hierarchical BayesianOptimization Algorithm are all well known Evolutionary Algorithms. In this report we present a Java implementation of these four algorithms withdetailed instructions on how to use each of them to solve a given set ofoptimization problems. Additionally, it is explained how to implement andintegrate new problems within the provided set. The source and binary files ofthe Java implementations are available for free download athttps://github.com/JoseCPereira/2015EvolutionaryAlgorithmsJava.
arxiv-12300-137 | Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest | http://arxiv.org/pdf/1506.08126v1.pdf | author:Dragomir Radev, Amanda Stent, Joel Tetreault, Aasish Pappu, Aikaterini Iliakopoulou, Agustin Chanfreau, Paloma de Juan, Jordi Vallmitjana, Alejandro Jaimes, Rahul Jha, Bob Mankoff category:cs.CL cs.AI cs.MM stat.ML published:2015-06-26 summary:The New Yorker publishes a weekly captionless cartoon. More than 5,000readers submit captions for it. The editors select three of them and ask thereaders to pick the funniest one. We describe an experiment that compares adozen automatic methods for selecting the funniest caption. We show thatnegative sentiment, human-centeredness, and lexical centrality most stronglymatch the funniest captions, followed by positive sentiment. These results areuseful for understanding humor and also in the design of more engagingconversational agents in text and multimodal (vision+text) systems. As part ofthis work, a large set of cartoons and captions is being made available to thecommunity.
arxiv-12300-138 | Java Implementation of a Parameter-less Evolutionary Portfolio | http://arxiv.org/pdf/1506.08867v1.pdf | author:José C. Pereira, Fernando G. Lobo category:cs.MS cs.NE I.2.8 published:2015-06-26 summary:The Java implementation of a portfolio of parameter-less evolutionaryalgorithms is presented. The Parameter-less Evolutionary Portfolio implements aheuristic that performs adaptive selection of parameter-less evolutionaryalgorithms in accordance with performance criteria that are measured duringrunning time. At present time, the portfolio includes three parameter-lessevolutionary algorithms: Parameter-less Univariate Marginal DistributionAlgorithm, Parameter-less Extended Compact Genetic Algorithm, andParameter-less Hierarchical Bayesian Optimization Algorithm. Initialexperiments showed that the parameter-less portfolio can solve various classesof problems without the need for any prior parameter setting technique and withan increase in computational effort that can be considered acceptable.
arxiv-12300-139 | Factorized Asymptotic Bayesian Inference for Factorial Hidden Markov Models | http://arxiv.org/pdf/1506.07959v1.pdf | author:Shaohua Li, Ryohei Fujimaki, Chunyan Miao category:stat.ML published:2015-06-26 summary:Factorial hidden Markov models (FHMMs) are powerful tools of modelingsequential data. Learning FHMMs yields a challenging simultaneous modelselection issue, i.e., selecting the number of multiple Markov chains and thedimensionality of each chain. Our main contribution is to address this modelselection issue by extending Factorized Asymptotic Bayesian (FAB) inference toFHMMs. First, we offer a better approximation of marginal log-likelihood thanthe previous FAB inference. Our key idea is to integrate out transitionprobabilities, yet still apply the Laplace approximation to emissionprobabilities. Second, we prove that if there are two very similar hiddenstates in an FHMM, i.e. one is redundant, then FAB will almost surely shrinkand eliminate one of them, making the model parsimonious. Experimental resultsshow that FAB for FHMMs significantly outperforms state-of-the-artnonparametric Bayesian iFHMM and Variational FHMM in model selection accuracy,with competitive held-out perplexity.
arxiv-12300-140 | A Java Implementation of Parameter-less Evolutionary Algorithms | http://arxiv.org/pdf/1506.08694v1.pdf | author:José C. Pereira, Fernando G. Lobo category:cs.MS cs.NE I.2.8 published:2015-06-26 summary:The Parameter-less Genetic Algorithm was first presented by Harik and Lobo in1999 as an alternative to the usual trial-and-error method of finding, for eachgiven problem, an acceptable set-up of the parameter values of the geneticalgorithm. Since then, the same strategy has been successfully applied tocreate parameter-less versions of other population-based search algorithms suchas the Extended Compact Genetic Algorithm and the Hierarchical BayesianOptimization Algorithm. This report describes a Java implementation,Parameter-less Evolutionary Algorithm (P-EAJava), that integrates severalparameter-less evolutionary algorithms into a single platform. Along with abrief description of P-EAJava, we also provide detailed instructions on how touse it, how to implement new problems, and how to generate new parameter-lessversions of evolutionary algorithms. At present time, P-EAJava already includes parameter-less versions of theSimple Genetic Algorithm, the Extended Compact Genetic Algorithm, theUnivariate Marginal Distribution Algorithm, and the Hierarchical BayesianOptimization Algorithm. The source and binary files of the Java implementationof P-EAJava are available for free download athttps://github.com/JoseCPereira/2015ParameterlessEvolutionaryAlgorithmsJava.
arxiv-12300-141 | Automagically encoding Adverse Drug Reactions in MedDRA | http://arxiv.org/pdf/1506.08052v2.pdf | author:Carlo Combi, Riccardo Lora, Ugo Moretti, Marco Pagliarini, Margherita Zorzi category:cs.CL published:2015-06-26 summary:Pharmacovigilance is the field of science devoted to the collection, analysisand prevention of Adverse Drug Reactions (ADRs). Efficient strategies for theextraction of information about ADRs from free text resources are essential tosupport the work of experts, employed in the crucial task of detecting andclassifying unexpected pathologies possibly related to drug assumptions.Narrative ADR descriptions may be collected in several way, e.g. by monitoringsocial networks or through the so called spontaneous reporting, the main methodpharmacovigilance adopts in order to identify ADRs. The encoding of free-textADR descriptions according to MedDRA standard terminology is central for reportanalysis. It is a complex work, which has to be manually implemented by thepharmacovigilance experts. The manual encoding is expensive (in terms of time).Moreover, a problem about the accuracy of the encoding may occur, since thenumber of reports is growing up day by day. In this paper, we proposeMagiCoder, an efficient Natural Language Processing algorithm able toautomatically derive MedDRA terminologies from free-text ADR descriptions.MagiCoder is part of VigiWork, a web application for online ADR reporting andanalysis. From a practical view-point, MagiCoder radically reduces the revisiontime of ADR reports: the pharmacologist has simply to revise and validate theautomatic solution versus the hard task of choosing solutions in the 70k termsof MedDRA. This improvement of the expert work efficiency has a meaningfulimpact on the quality of data analysis. Moreover, our procedure is generalpurpose. We developed MagiCoder for the Italian pharmacovigilance language, butpreliminarily analyses show that it is robust to language and dictionarychanges.
arxiv-12300-142 | Skopus: Exact discovery of the most interesting sequential patterns under Leverage | http://arxiv.org/pdf/1506.08009v2.pdf | author:Francois Petitjean, Tao Li, Nikolaj Tatti, Geoffrey I. Webb category:cs.AI cs.LG stat.ML published:2015-06-26 summary:This paper presents a framework for exact discovery of the most interestingsequential patterns. It combines (1) a novel definition of the expected supportfor a sequential pattern - a concept on which most interestingness measuresdirectly rely - with (2) SkOPUS: a new branch-and-bound algorithm for the exactdiscovery of top-k sequential patterns under a given measure of interest. Ourinterestingness measure is based on comparing the pattern support with theaverage support of its sister patterns, obtained by permuting (to certainextent) the items of the pattern. The larger the support compared to theexpectation, the more interesting is the pattern. We build on these twoelements to exactly extract the k sequential patterns with highest leverage,consistent with our definition of expected support. We conduct experiments onboth synthetic data with known patterns and real-world datasets; bothexperiments confirm the consistency and relevance of our approach with regardto the state of the art.
arxiv-12300-143 | Safe Feature Pruning for Sparse High-Order Interaction Models | http://arxiv.org/pdf/1506.08002v1.pdf | author:Kazuya Nakagawa, Shinya Suzumura, Masayuki Karasuyama, Koji Tsuda, Ichiro Takeuchi category:stat.ML published:2015-06-26 summary:Taking into account high-order interactions among covariates is valuable inmany practical regression problems. This is, however, computationallychallenging task because the number of high-order interaction features to beconsidered would be extremely large unless the number of covariates issufficiently small. In this paper, we propose a novel efficient algorithm forLASSO-based sparse learning of such high-order interaction models. Our basicstrategy for reducing the number of features is to employ the idea of recentlyproposed safe feature screening (SFS) rule. An SFS rule has a property that, ifa feature satisfies the rule, then the feature is guaranteed to be non-activein the LASSO solution, meaning that it can be safely screened-out prior to theLASSO training process. If a large number of features can be screened-outbefore training the LASSO, the computational cost and the memory requirment canbe dramatically reduced. However, applying such an SFS rule to each of theextremely large number of high-order interaction features would becomputationally infeasible. Our key idea for solving this computational issueis to exploit the underlying tree structure among high-order interactionfeatures. Specifically, we introduce a pruning condition called safe featurepruning (SFP) rule which has a property that, if the rule is satisfied in acertain node of the tree, then all the high-order interaction featurescorresponding to its descendant nodes can be guaranteed to be non-active at theoptimal solution. Our algorithm is extremely efficient, making it possible towork, e.g., with 3rd order interactions of 10,000 original covariates, wherethe number of possible high-order interaction features is greater than 10^{12}.
arxiv-12300-144 | An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process | http://arxiv.org/pdf/1506.08180v1.pdf | author:Amar Shah, David A. Knowles, Zoubin Ghahramani category:stat.ML cs.LG stat.AP stat.CO stat.ME published:2015-06-26 summary:Stochastic variational inference (SVI) is emerging as the most promisingcandidate for scaling inference in Bayesian probabilistic models to largedatasets. However, the performance of these methods has been assessed primarilyin the context of Bayesian topic models, particularly latent Dirichletallocation (LDA). Deriving several new algorithms, and using synthetic, imageand genomic datasets, we investigate whether the understanding gleaned from LDAapplies in the setting of sparse latent factor models, specifically betaprocess factor analysis (BPFA). We demonstrate that the big picture isconsistent: using Gibbs sampling within SVI to maintain certain posteriordependencies is extremely effective. However, we find that different posteriordependencies are important in BPFA relative to LDA. Particularly,approximations able to model intra-local variable dependence perform best.
arxiv-12300-145 | Spectral Collaborative Representation based Classification for Hand Gestures recognition on Electromyography Signals | http://arxiv.org/pdf/1506.08006v1.pdf | author:Ali Boyali category:cs.CV published:2015-06-26 summary:In this study, we introduce a novel variant and application of theCollaborative Representation based Classification in spectral domain forrecognition of the hand gestures using the raw surface Electromyographysignals. The intuitive use of spectral features are explained via circulantmatrices. The proposed Spectral Collaborative Representation basedClassification (SCRC) is able to recognize gestures with higher levels ofaccuracy for a fairly rich gesture set. The worst recognition result which isthe best in the literature is obtained as 97.3\% among the four sets of theexperiments for each hand gestures. The recognition results are reported with asubstantial number of experiments and labeling computation.
arxiv-12300-146 | An Efficient Post-Selection Inference on High-Order Interaction Models | http://arxiv.org/pdf/1506.07997v1.pdf | author:S. Suzumura, K. Nakagawa, K. Tsuda, I. Takeuchi category:stat.ML published:2015-06-26 summary:Finding statistically significant high-order interaction features inpredictive modeling is important but challenging task. The difficulty lies inthe fact that, for a recent applications with high-dimensional covariates, thenumber of possible high-order interaction features would be extremely large.Identifying statistically significant features from such a huge pool ofcandidates would be highly challenging both in computational and statisticalsenses. To work with this problem, we consider a two stage algorithm where wefirst select a set of high-order interaction features by marginal screening,and then make statistical inferences on the regression model fitted only withthe selected features. Such statistical inferences are called post-selectioninference (PSI), and receiving an increasing attention in the literature. Oneof the seminal recent advancements in PSI literature is the works by Lee et al.where the authors presented an algorithmic framework for computing exactsampling distributions in PSI. A main challenge when applying their approach toour high-order interaction models is to cope with the fact that PSI in generaldepends not only on the selected features but also on the unselected features,making it hard to apply to our extremely high-dimensional high-orderinteraction models. The goal of this paper is to overcome this difficulty byintroducing a novel efficient method for PSI. Our key idea is to exploit theunderlying tree structure among high-order interaction features, and to developa pruning method of the tree which enables us to quickly identify a group ofunselected features that are guaranteed to have no influence on PSI. Theexperimental results indicate that the proposed method allows us to reliablyidentify statistically significant high-order interaction features withreasonable computational cost.
arxiv-12300-147 | Finding Linear Structure in Large Datasets with Scalable Canonical Correlation Analysis | http://arxiv.org/pdf/1506.08170v1.pdf | author:Zhuang Ma, Yichao Lu, Dean Foster category:stat.ML stat.CO published:2015-06-26 summary:Canonical Correlation Analysis (CCA) is a widely used spectral technique forfinding correlation structures in multi-view datasets. In this paper, we tacklethe problem of large scale CCA, where classical algorithms, usually requiringcomputing the product of two huge matrices and huge matrix decomposition, arecomputationally and storage expensive. We recast CCA from a novel perspectiveand propose a scalable and memory efficient Augmented Approximate Gradient(AppGrad) scheme for finding top $k$ dimensional canonical subspace which onlyinvolves large matrix multiplying a thin matrix of width $k$ and small matrixdecomposition of dimension $k\times k$. Further, AppGrad achieves optimalstorage complexity $O(k(p_1+p_2))$, compared with classical algorithms whichusually require $O(p_1^2+p_2^2)$ space to store two dense whitening matrices.The proposed scheme naturally generalizes to stochastic optimization regime,especially efficient for huge datasets where batch algorithms are prohibitive.The online property of stochastic AppGrad is also well suited to the streamingscenario, where data comes sequentially. To the best of our knowledge, it isthe first stochastic algorithm for CCA. Experiments on four real data sets areprovided to show the effectiveness of the proposed methods.
arxiv-12300-148 | Bag-of-Features Image Indexing and Classification in Microsoft SQL Server Relational Database | http://arxiv.org/pdf/1506.07950v1.pdf | author:Marcin Korytkowski, Rafal Scherer, Pawel Staszewski, Piotr Woldan category:cs.DB cs.CV published:2015-06-26 summary:This paper presents a novel relational database architecture aimed to visualobjects classification and retrieval. The framework is based on thebag-of-features image representation model combined with the Support VectorMachine classification and is integrated in a Microsoft SQL Server database.
arxiv-12300-149 | Modelling of directional data using Kent distributions | http://arxiv.org/pdf/1506.08105v1.pdf | author:Parthan Kasarapu category:cs.LG stat.ML published:2015-06-26 summary:The modelling of data on a spherical surface requires the consideration ofdirectional probability distributions. To model asymmetrically distributed dataon a three-dimensional sphere, Kent distributions are often used. The momentestimates of the parameters are typically used in modelling tasks involvingKent distributions. However, these lack a rigorous statistical treatment. Thefocus of the paper is to introduce a Bayesian estimation of the parameters ofthe Kent distribution which has not been carried out in the literature, partlybecause of its complex mathematical form. We employ the Bayesianinformation-theoretic paradigm of Minimum Message Length (MML) to bridge thisgap and derive reliable estimators. The inferred parameters are subsequentlyused in mixture modelling of Kent distributions. The problem of inferring thesuitable number of mixture components is also addressed using the MMLcriterion. We demonstrate the superior performance of the derived MML-basedparameter estimates against the traditional estimators. We apply the MMLprinciple to infer mixtures of Kent distributions to model empirical datacorresponding to protein conformations. We demonstrate the effectiveness ofKent models to act as improved descriptors of protein structural data ascompared to commonly used von Mises-Fisher distributions.
arxiv-12300-150 | Collaboratively Learning Preferences from Ordinal Data | http://arxiv.org/pdf/1506.07947v1.pdf | author:Sewoong Oh, Kiran K. Thekumparampil, Jiaming Xu category:cs.LG cs.IT math.IT stat.ML published:2015-06-26 summary:In applications such as recommendation systems and revenue management, it isimportant to predict preferences on items that have not been seen by a user orpredict outcomes of comparisons among those that have never been compared. Apopular discrete choice model of multinomial logit model captures the structureof the hidden preferences with a low-rank matrix. In order to predict thepreferences, we want to learn the underlying model from noisy observations ofthe low-rank matrix, collected as revealed preferences in various forms ofordinal data. A natural approach to learn such a model is to solve a convexrelaxation of nuclear norm minimization. We present the convex relaxationapproach in two contexts of interest: collaborative ranking and bundled choicemodeling. In both cases, we show that the convex relaxation is minimax optimal.We prove an upper bound on the resulting error with finite samples, and providea matching information-theoretic lower bound.
arxiv-12300-151 | Minimax Correlation Clustering and Biclustering: Bounding Errors Locally | http://arxiv.org/pdf/1506.08189v2.pdf | author:Gregory J. Puleo, Olgica Milenkovic category:cs.DS cs.LG published:2015-06-26 summary:We introduce a new agnostic clustering model, \emph{minimax correlationclustering}, and a rounding algorithm tailored to the needs of this model.Given a graph whose edges are labeled with $+$ or $-$, we wish to partition thegraph into clusters while trying to avoid errors: $+$ edges between clusters or$-$ edges within clusters. Unlike classical correlation clustering, which seeksto minimize the total number of errors, minimax clustering instead seeks tominimize the number of errors at the \emph{worst vertex}, that is, at thevertex with the greatest number of incident errors. This minimax objectivefunction may be seen as a way to enforce individual-level quality of partitionconstraints for vertices in a graph. We study this problem on complete graphsand complete bipartite graphs, proving that the problem is NP-hard on thesegraph classes and giving polynomial-time constant-factor approximationalgorithms. The approximation algorithms rely on LP relaxation and roundingprocedures. We also discuss the broader applicability of our rounding algorithmto other (nonlinear) objective functions for correlation clustering.
arxiv-12300-152 | Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric | http://arxiv.org/pdf/1506.07944v2.pdf | author:Vivien Seguy, Marco Cuturi category:stat.ML published:2015-06-26 summary:Given a family of probability measures in P(X), the space of probabilitymeasures on a Hilbert space X, our goal in this paper is to highlight one oremore curves in P(X) that summarize efficiently that family. We propose to studythis problem under the optimal transport (Wasserstein) geometry, using curvesthat are restricted to be geodesic segments under that metric. We show thatconcepts that play a key role in Euclidean PCA, such as data centering ororthogonality of principal directions, find a natural equivalent in the optimaltransport geometry, using Wasserstein means and differential geometry. Theimplementation of these ideas is, however, computationally challenging. Toachieve scalable algorithms that can handle thousands of measures, we proposeto use a relaxed definition for geodesics and regularized optimal transportdistances. The interest of our approach is demonstrated on images seen eitheras shapes or color histograms.
arxiv-12300-153 | Clustering categorical data via ensembling dissimilarity matrices | http://arxiv.org/pdf/1506.07930v1.pdf | author:Saeid Amiri, Bertrand Clarke, Jennifer Clarke category:stat.ML published:2015-06-26 summary:We present a technique for clustering categorical data by generating manydissimilarity matrices and averaging over them. We begin by demonstrating ourtechnique on low dimensional categorical data and comparing it to several othertechniques that have been proposed. Then we give conditions under which ourmethod should yield good results in general. Our method extends to highdimensional categorical data of equal lengths by ensembling over many choicesof explanatory variables. In this context we compare our method with two othermethods. Finally, we extend our method to high dimensional categorical datavectors of unequal length by using alignment techniques to equalize thelengths. We give examples to show that our method continues to provide goodresults, in particular, better in the context of genome sequences thanclusterings suggested by phylogenetic trees.
arxiv-12300-154 | A geometric alternative to Nesterov's accelerated gradient descent | http://arxiv.org/pdf/1506.08187v1.pdf | author:Sébastien Bubeck, Yin Tat Lee, Mohit Singh category:math.OC cs.DS cs.LG cs.NA published:2015-06-26 summary:We propose a new method for unconstrained optimization of a smooth andstrongly convex function, which attains the optimal rate of convergence ofNesterov's accelerated gradient descent. The new algorithm has a simplegeometric interpretation, loosely inspired by the ellipsoid method. We providesome numerical evidence that the new method can be superior to Nesterov'saccelerated gradient descent.
arxiv-12300-155 | Analyzing statistical and computational tradeoffs of estimation procedures | http://arxiv.org/pdf/1506.07925v1.pdf | author:Daniel L. Sussman, Alexander Volfovsky, Edoardo M. Airoldi category:stat.CO stat.ML published:2015-06-25 summary:The recent explosion in the amount and dimensionality of data has exacerbatedthe need of trading off computational and statistical efficiency carefully, sothat inference is both tractable and meaningful. We propose a framework thatprovides an explicit opportunity for practitioners to specify how muchstatistical risk they are willing to accept for a given computational cost, andleads to a theoretical risk-computation frontier for any given inferenceproblem. We illustrate the tradeoff between risk and computation and illustratethe frontier in three distinct settings. First, we derive analytic forms forthe risk of estimating parameters in the classical setting of estimating themean and variance for normally distributed data and for the more generalsetting of parameters of an exponential family. The second example concentrateson computationally constrained Hodges-Lehmann estimators. We conclude with anevaluation of risk associated with early termination of iterative matrixinversion algorithms in the context of linear regression.
arxiv-12300-156 | Camera Calibration from Dynamic Silhouettes Using Motion Barcodes | http://arxiv.org/pdf/1506.07866v3.pdf | author:Gil Ben-Artzi, Yoni Kasten, Shmuel Peleg, Michael Werman category:cs.CV published:2015-06-25 summary:Computing the epipolar geometry between cameras with very differentviewpoints is often problematic as matching points are hard to find. In thesecases, it has been proposed to use information from dynamic objects in thescene for suggesting point and line correspondences. We propose a speed up of about two orders of magnitude, as well as anincrease in robustness and accuracy, to methods computing epipolar geometryfrom dynamic silhouettes. This improvement is based on a new temporalsignature: motion barcode for lines. Motion barcode is a binary temporalsequence for lines, indicating for each frame the existence of at least oneforeground pixel on that line. The motion barcodes of two correspondingepipolar lines are very similar, so the search for corresponding epipolar linescan be limited only to lines having similar barcodes. The use of motionbarcodes leads to increased speed, accuracy, and robustness in computing theepipolar geometry.
arxiv-12300-157 | Minimax Structured Normal Means Inference | http://arxiv.org/pdf/1506.07902v2.pdf | author:Akshay Krishnamurthy category:stat.ML cs.IT math.IT published:2015-06-25 summary:We provide a unified treatment of a broad class of noisy structure recoveryproblems, known as structured normal means problems. In this setting, the goalis to identify, from a finite collection of Gaussian distributions withdifferent means, the distribution that produced some observed data. Recent workhas studied several special cases including sparse vectors, biclusters, andgraph-based structures. We establish nearly matching upper and lower bounds onthe minimax probability of error for any structured normal means problem, andwe derive an optimality certificate for the maximum likelihood estimator, whichcan be applied to many instantiations. We also consider an experimental designsetting, where we generalize our minimax bounds and derive an algorithm forcomputing a design strategy with a certain optimality property. We show thatour results give tight minimax bounds for many structure recovery problems andconsider some consequences for interactive sampling.
arxiv-12300-158 | CRAFT: ClusteR-specific Assorted Feature selecTion | http://arxiv.org/pdf/1506.07609v1.pdf | author:Vikas K. Garg, Cynthia Rudin, Tommi Jaakkola category:cs.LG stat.ML published:2015-06-25 summary:We present a framework for clustering with cluster-specific featureselection. The framework, CRAFT, is derived from asymptotic log posteriorformulations of nonparametric MAP-based clustering models. CRAFT handlesassorted data, i.e., both numeric and categorical data, and the underlyingobjective functions are intuitively appealing. The resulting algorithm issimple to implement and scales nicely, requires minimal parameter tuning,obviates the need to specify the number of clusters a priori, and comparesfavorably with other methods on real datasets.
arxiv-12300-159 | Conservativeness of untied auto-encoders | http://arxiv.org/pdf/1506.07643v3.pdf | author:Daniel Jiwoong Im, Mohamed Ishmael Diwan Belghazi, Roland Memisevic category:cs.LG published:2015-06-25 summary:We discuss necessary and sufficient conditions for an auto-encoder to definea conservative vector field, in which case it is associated with an energyfunction akin to the unnormalized log-probability of the data. We show that theconditions for conservativeness are more general than for encoder and decoderweights to be the same ("tied weights"), and that they also depend on the formof the hidden unit activation function, but that contractive training criteria,such as denoising, will enforce these conditions locally. Based on theseobservations, we show how we can use auto-encoders to extract the conservativecomponent of a vector field.
arxiv-12300-160 | Diffusion Nets | http://arxiv.org/pdf/1506.07840v1.pdf | author:Gal Mishne, Uri Shaham, Alexander Cloninger, Israel Cohen category:stat.ML cs.LG math.CA published:2015-06-25 summary:Non-linear manifold learning enables high-dimensional data analysis, butrequires out-of-sample-extension methods to process new data points. In thispaper, we propose a manifold learning algorithm based on deep learning tocreate an encoder, which maps a high-dimensional dataset and itslow-dimensional embedding, and a decoder, which takes the embedded data back tothe high-dimensional space. Stacking the encoder and decoder togetherconstructs an autoencoder, which we term a diffusion net, that performsout-of-sample-extension as well as outlier detection. We introduce new neuralnet constraints for the encoder, which preserves the local geometry of thepoints, and we prove rates of convergence for the encoder. Also, our approachis efficient in both computational complexity and memory requirements, asopposed to previous methods that require storage of all training points in boththe high-dimensional and the low-dimensional spaces to calculate theout-of-sample-extension and the pre-image.
arxiv-12300-161 | How to improve robustness in Kohonen maps and display additional information in Factorial Analysis: application to text mining | http://arxiv.org/pdf/1506.07732v1.pdf | author:Nicolas Bourgeois, Marie Cottrell, Benjamin Déruelle, Stéphane Lamassé, Patrick Letrémy category:math.ST cs.CL stat.TH published:2015-06-25 summary:This article is an extended version of a paper presented in the WSOM'2012conference [1]. We display a combination of factorial projections, SOMalgorithm and graph techniques applied to a text mining problem. The corpuscontains 8 medieval manuscripts which were used to teach arithmetic techniquesto merchants. Among the techniques for Data Analysis, those used forLexicometry (such as Factorial Analysis) highlight the discrepancies betweenmanuscripts. The reason for this is that they focus on the deviation from theindependence between words and manuscripts. Still, we also want to discover andcharacterize the common vocabulary among the whole corpus. Using the propertiesof stochastic Kohonen maps, which define neighborhood between inputs in anon-deterministic way, we highlight the words which seem to play a special rolein the vocabulary. We call them fickle and use them to improve both Kohonen maprobustness and significance of FCA visualization. Finally we use graphalgorithmic to exploit this fickleness for classification of words.
arxiv-12300-162 | Degenerate Motions in Multicamera Cluster SLAM with Non-overlapping Fields of View | http://arxiv.org/pdf/1506.07597v1.pdf | author:Michael J. Tribou, David W. L. Wang, Steven L. Waslander category:cs.CV cs.RO published:2015-06-25 summary:An analysis of the relative motion and point feature model configurationsleading to solution degeneracy is presented, for the case of a SimultaneousLocalization and Mapping system using multicamera clusters with non-overlappingfields-of-view. The SLAM optimization system seeks to minimize image spacereprojection error and is formulated for a cluster containing any number ofcomponent cameras, observing any number of point features over two keyframes.The measurement Jacobian is transformed to expose a reduced-dimensionrepresentation such that the degeneracy of the system can be determined by therank of a dense submatrix. A set of relative motions sufficient for degeneracyare identified for certain cluster configurations, independent of target modelgeometry. Furthermore, it is shown that increasing the number of cameras withinthe cluster and observing features across different cameras over the twokeyframes reduces the size of the degenerate motion sets significantly.
arxiv-12300-163 | Decentralized Q-Learning for Stochastic Teams and Games | http://arxiv.org/pdf/1506.07924v2.pdf | author:Gürdal Arslan, Serdar Yüksel category:math.OC cs.GT cs.LG published:2015-06-25 summary:There are only a few learning algorithms applicable to stochastic dynamicteams and games which generalize Markov decision processes to decentralizedstochastic control problems involving possibly self-interested decision makers.Learning in games is generally difficult because of the non-stationaryenvironment in which each decision maker aims to learn its optimal decisionswith minimal information in the presence of the other decision makers who arealso learning. In stochastic dynamic games, learning is more challengingbecause, while learning, the decision makers alter the state of the system andhence the future cost. In this paper, we present decentralized Q-learningalgorithms for stochastic games, and study their convergence for the weaklyacyclic case which includes team problems as an important special case. Thealgorithm is decentralized in that each decision maker has access to only itslocal information, the state information, and the local cost realizations;furthermore, it is completely oblivious to the presence of other decisionmakers. We show that these algorithms converge to equilibrium policies almostsurely in large classes of stochastic games.
arxiv-12300-164 | DeepMatching: Hierarchical Deformable Dense Matching | http://arxiv.org/pdf/1506.07656v2.pdf | author:Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid category:cs.CV published:2015-06-25 summary:We introduce a novel matching algorithm, called DeepMatching, to computedense correspondences between images. DeepMatching relies on a hierarchical,multi-layer, correlational architecture designed for matching images and wasinspired by deep convolutional approaches. The proposed matching algorithm canhandle non-rigid deformations and repetitive textures and efficientlydetermines dense correspondences in the presence of significant changes betweenimages. We evaluate the performance of DeepMatching, in comparison withstate-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013)datasets. DeepMatching outperforms the state-of-the-art algorithms and showsexcellent results in particular for repetitive textures.We also propose amethod for estimating optical flow, called DeepFlow, by integratingDeepMatching in the large displacement optical flow (LDOF) approach of Brox andMalik (2011). Compared to existing matching algorithms, additional robustnessto large displacements and complex motion is obtained thanks to our matchingapproach. DeepFlow obtains competitive performance on public benchmarks foroptical flow estimation.
arxiv-12300-165 | Fairness-Aware Learning with Restriction of Universal Dependency using f-Divergences | http://arxiv.org/pdf/1506.07721v1.pdf | author:Kazuto Fukuchi, Jun Sakuma category:stat.ML cs.LG published:2015-06-25 summary:Fairness-aware learning is a novel framework for classification tasks. Likeregular empirical risk minimization (ERM), it aims to learn a classifier with alow error rate, and at the same time, for the predictions of the classifier tobe independent of sensitive features, such as gender, religion, race, andethnicity. Existing methods can achieve low dependencies on given samples, butthis is not guaranteed on unseen samples. The existing fairness-aware learningalgorithms employ different dependency measures, and each algorithm isspecifically designed for a particular one. Such diversity makes it difficultto theoretically analyze and compare them. In this paper, we propose a generalframework for fairness-aware learning that uses f-divergences and that coversmost of the dependency measures employed in the existing methods. We introducea way to estimate the f-divergences that allows us to give a unified analysisfor the upper bound of the estimation error; this bound is tighter than that ofthe existing convergence rate analysis of the divergence estimation. With ourdivergence estimate, we propose a fairness-aware learning algorithm, andperform a theoretical analysis of its generalization error. Our analysisreveals that, under mild assumptions and even with enforcement of fairness, thegeneralization error of our method is $O(\sqrt{1/n})$, which is the same asthat of the regular ERM. In addition, and more importantly, we show that, forany f-divergence, the upper bound of the estimation error of the divergence is$O(\sqrt{1/n})$. This indicates that our fairness-aware learning algorithmguarantees low dependencies on unseen samples for any dependency measurerepresented by an f-divergence.
arxiv-12300-166 | AttentionNet: Aggregating Weak Directions for Accurate Object Detection | http://arxiv.org/pdf/1506.07704v2.pdf | author:Donggeun Yoo, Sunggyun Park, Joon-Young Lee, Anthony S. Paek, In So Kweon category:cs.CV cs.LG published:2015-06-25 summary:We present a novel detection method using a deep convolutional neural network(CNN), named AttentionNet. We cast an object detection problem as an iterativeclassification problem, which is the most suitable form of a CNN. AttentionNetprovides quantized weak directions pointing a target object and the ensemble ofiterative predictions from AttentionNet converges to an accurate objectboundary box. Since AttentionNet is a unified network for object detection, itdetects objects without any separated models from the object proposal to thepost bounding-box regression. We evaluate AttentionNet by a human detectiontask and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC2007/2012 with an 8-layered architecture only.
arxiv-12300-167 | Generalized Majorization-Minimization | http://arxiv.org/pdf/1506.07613v1.pdf | author:Sobhan Naderi Parizi, Kun He, Stan Sclaroff, Pedro Felzenszwalb category:cs.CV cs.IT cs.LG math.IT stat.ML published:2015-06-25 summary:Non-convex optimization is ubiquitous in machine learning. TheMajorization-Minimization (MM) procedure systematically optimizes non-convexfunctions through an iterative construction and optimization of upper bounds onthe objective function. The bound at each iteration is required to \emph{touch}the objective function at the optimizer of the previous bound. We show thatthis touching constraint is unnecessary and overly restrictive. We generalizeMM by relaxing this constraint, and propose a new framework for designingoptimization algorithms, named Generalized Majorization-Minimization (G-MM).Compared to MM, G-MM is much more flexible. For instance, it can incorporateapplication-specific biases into the optimization procedure without changingthe objective function. We derive G-MM algorithms for several latent variablemodels and show that they consistently outperform their MM counterparts inoptimizing non-convex objectives. In particular, G-MM algorithms appear to beless sensitive to initialization.
arxiv-12300-168 | Joint community and anomaly tracking in dynamic networks | http://arxiv.org/pdf/1506.07611v1.pdf | author:Brian Baingana, Georgios B. Giannakis category:stat.ML cs.SI physics.soc-ph published:2015-06-25 summary:Most real-world networks exhibit community structure, a phenomenoncharacterized by existence of node clusters whose intra-edge connectivity isstronger than edge connectivities between nodes belonging to differentclusters. In addition to facilitating a better understanding of networkbehavior, community detection finds many practical applications in diversesettings. Communities in online social networks are indicative of sharedfunctional roles, or affiliation to a common socio-economic status, theknowledge of which is vital for targeted advertisement. In buyer-sellernetworks, community detection facilitates better product recommendations.Unfortunately, reliability of community assignments is hindered by anomaloususer behavior often observed as unfair self-promotion, or "fake"highly-connected accounts created to promote fraud. The present paper advocatesa novel approach for jointly tracking communities while detecting suchanomalous nodes in time-varying networks. By postulating edge creation as theresult of mutual community participation by node pairs, a dynamic factor modelwith anomalous memberships captured through a sparse outlier matrix is putforth. Efficient tracking algorithms suitable for both online and decentralizedoperation are developed. Experiments conducted on both synthetic and realnetwork time series successfully unveil underlying communities and anomalousnodes.
arxiv-12300-169 | The local convexity of solving systems of quadratic equations | http://arxiv.org/pdf/1506.07868v4.pdf | author:Chris D. White, Sujay Sanghavi, Rachel Ward category:math.NA math.OC stat.ML published:2015-06-25 summary:This paper considers the recovery of a rank $r$ positive semidefinite matrix$X X^T\in\mathbb{R}^{n\times n}$ from $m$ scalar measurements of the form $y_i:= a_i^T X X^T a_i$ (i.e., quadratic measurements of $X$). Such problems arisein a variety of applications, including covariance sketching ofhigh-dimensional data streams, quadratic regression, quantum state tomography,among others. A natural approach to this problem is to minimize the lossfunction $f(U) = \sum_i (y_i - a_i^TUU^Ta_i)^2$ which has an entire manifold ofsolutions given by $\{XO\}_{O\in\mathcal{O}_r}$ where $\mathcal{O}_r$ is theorthogonal group of $r\times r$ orthogonal matrices; this is {\it non-convex}in the $n\times r$ matrix $U$, but methods like gradient descent are simple andeasy to implement (as compared to semidefinite relaxation approaches). In this paper we show that once we have $m \geq C nr \log^2(n)$ samples fromisotropic gaussian $a_i$, with high probability {\em (a)} this function admitsa dimension-independent region of {\em local strong convexity} on linesperpendicular to the solution manifold, and {\em (b)} with an additionalpolynomial factor of $r$ samples, a simple spectral initialization will landwithin the region of convexity with high probability. Together, this impliesthat gradient descent with initialization (but no re-sampling) will convergelinearly to the correct $X$, up to an orthogonal transformation. We believethat this general technique (local convexity reachable by spectralinitialization) should prove applicable to a broader class of nonconvexoptimization problems.
arxiv-12300-170 | Completing Low-Rank Matrices with Corrupted Samples from Few Coefficients in General Basis | http://arxiv.org/pdf/1506.07615v1.pdf | author:Hongyang Zhang, Zhouchen Lin, Chao Zhang category:cs.IT cs.LG cs.NA math.IT math.NA stat.ML 68T05 G.1.6; K.3.2 published:2015-06-25 summary:Subspace recovery from corrupted and missing data is crucial for variousapplications in signal processing and information theory. To complete missingvalues and detect column corruptions, existing robust Matrix Completion (MC)methods mostly concentrate on recovering a low-rank matrix from few corruptedcoefficients w.r.t. the standard basis, which, however, does not apply to moregeneral basis, e.g., the Fourier basis. In this paper, we prove that the rangespace of an $m\times n$ matrix with rank $r$ can be exactly recovered from fewcoefficients w.r.t. general basis, though the rank $r$ and the number ofcorrupted samples are both as high as $O(\min\{m,n\}/\log^3 (m+n))$. Thus ourresults cover previous work as special cases, and robust MC can recover theintrinsic matrix with a higher rank. Moreover, we suggest a universal choice ofthe regularization parameter, which is $\lambda=1/\sqrt{\log n}$. By our$\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we canfurther reduce the computational cost of our model. As an application, we alsofind that the solutions to extended robust Low-Rank Representation and to ourextended robust MC are mutually expressible, so both our theory and algorithmcan be immediately applied to the subspace clustering problem with missingvalues. Experiments verify our theories.
arxiv-12300-171 | Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling | http://arxiv.org/pdf/1506.07650v1.pdf | author:Kun Xu, Yansong Feng, Songfang Huang, Dongyan Zhao category:cs.CL cs.LG published:2015-06-25 summary:Syntactic features play an essential role in identifying relationship in asentence. Previous neural network models often suffer from irrelevantinformation introduced when subjects and objects are in a long distance. Inthis paper, we propose to learn more robust relation representations from theshortest dependency path through a convolution neural network. We furtherpropose a straightforward negative sampling strategy to improve the assignmentof subjects and objects. Experimental results show that our method outperformsthe state-of-the-art methods on the SemEval-2010 Task 8 dataset.
arxiv-12300-172 | Manifold Optimization for Gaussian Mixture Models | http://arxiv.org/pdf/1506.07677v1.pdf | author:Reshad Hosseini, Suvrit Sra category:stat.ML cs.LG math.OC published:2015-06-25 summary:We take a new look at parameter estimation for Gaussian Mixture Models(GMMs). In particular, we propose using \emph{Riemannian manifold optimization}as a powerful counterpart to Expectation Maximization (EM). An out-of-the-boxinvocation of manifold optimization, however, fails spectacularly: it convergesto the same solution but vastly slower. Driven by intuition from manifoldconvexity, we then propose a reparamerization that has remarkable empiricalconsequences. It makes manifold optimization not only match EM---a highlyencouraging result in itself given the poor record nonlinear programmingmethods have had against EM so far---but also outperform EM in many practicalsettings, while displaying much less variability in running times. We furtherhighlight the strengths of manifold optimization by developing a somewhat tunedmanifold LBFGS method that proves even more competitive and reliable thanexisting manifold optimization tools. We hope that our results encourage awider consideration of manifold optimization for parameter estimation problems.
arxiv-12300-173 | Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation | http://arxiv.org/pdf/1506.07452v1.pdf | author:Marijn F. Stollenga, Wonmin Byeon, Marcus Liwicki, Juergen Schmidhuber category:cs.CV cs.LG published:2015-06-24 summary:Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3Dvideos to segment them. They have a fixed input size and typically perceiveonly small local contexts of the pixels to be classified as foreground orbackground. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceivethe entire spatio-temporal context of each pixel in a few sweeps through allpixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despitethese theoretical advantages, however, unlike CNNs, previous MD-LSTM variantswere hard to parallelize on GPUs. Here we re-arrange the traditional cuboidorder of computations in MD-LSTM in pyramidal fashion. The resultingPyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks ofbrain slice images. PyraMiD-LSTM achieved best known pixel-wise brain imagesegmentation results on MRBrainS13 (and competitive results on EM-ISBI12).
arxiv-12300-174 | Efficient Learning for Undirected Topic Models | http://arxiv.org/pdf/1506.07477v1.pdf | author:Jiatao Gu, Victor O. K. Li category:cs.LG cs.CL cs.IR stat.ML published:2015-06-24 summary:Replicated Softmax model, a well-known undirected topic model, is powerful inextracting semantic representations of documents. Traditional learningstrategies such as Contrastive Divergence are very inefficient. This paperprovides a novel estimator to speed up the learning based on Noise ContrastiveEstimate, extended for documents of variant lengths and weighted inputs.Experiments on two benchmarks show that the new estimator achieves greatlearning efficiency and high accuracy on document retrieval and classification.
arxiv-12300-175 | Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation | http://arxiv.org/pdf/1506.07405v2.pdf | author:Dejiao Zhang, Laura Balzano category:cs.NA math.NA stat.ML 90C52, 65Y20 G.1.6; F.2.1 published:2015-06-24 summary:It has been observed in a variety of contexts that gradient descent methodshave great success in solving low-rank matrix factorization problems, despitethe relevant problem formulation being non-convex. We tackle a particularinstance of this scenario, where we seek the $d$-dimensional subspace spannedby a streaming data matrix. We apply the natural first order incrementalgradient descent method, constraining the gradient method to the Grassmannian.In this paper, we propose an adaptive step size scheme that is greedy for thenoiseless case, that maximizes the improvement of our metric of convergence ateach data index $t$, and yields an expected improvement for the noisy case. Weshow that, with noise-free data, this method converges from any randominitialization to the global minimum of the problem. For noisy data, we providethe expected convergence rate of the proposed algorithm per iteration.
arxiv-12300-176 | Targeting Ultimate Accuracy: Face Recognition via Deep Embedding | http://arxiv.org/pdf/1506.07310v4.pdf | author:Jingtuo Liu, Yafeng Deng, Tao Bai, Zhengping Wei, Chang Huang category:cs.CV published:2015-06-24 summary:Face Recognition has been studied for many decades. As opposed to traditionalhand-crafted features such as LBP and HOG, much more sophisticated features canbe learned automatically by deep learning methods in a data-driven way. In thispaper, we propose a two-stage approach that combines a multi-patch deep CNN anddeep metric learning, which extracts low dimensional but very discriminativefeatures for face verification and recognition. Experiments show that thismethod outperforms other state-of-the-art methods on LFW dataset, achieving99.77% pair-wise verification accuracy and significantly better accuracy underother two more practical protocols. This paper also discusses the importance ofdata size and the number of patches, showing a clear path to practicalhigh-performance face recognition systems in real world.
arxiv-12300-177 | Attention-Based Models for Speech Recognition | http://arxiv.org/pdf/1506.07503v1.pdf | author:Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML published:2015-06-24 summary:Recurrent sequence generators conditioned on input data through an attentionmechanism have recently shown very good performance on a range of tasks in-cluding machine translation, handwriting synthesis and image caption gen-eration. We extend the attention-mechanism with features needed for speechrecognition. We show that while an adaptation of the model used for machinetranslation in reaches a competitive 18.7% phoneme error rate (PER) on theTIMIT phoneme recognition task, it can only be applied to utterances which areroughly as long as the ones it was trained on. We offer a qualitativeexplanation of this failure and propose a novel and generic method of addinglocation-awareness to the attention mechanism to alleviate this issue. The newmethod yields a model that is robust to long inputs and achieves 18% PER insingle utterances and 20% in 10-times longer (repeated) utterances. Finally, wepropose a change to the at- tention mechanism that prevents it fromconcentrating too much on single frames, which further reduces PER to 17.6%level.
arxiv-12300-178 | Flexible Multi-layer Sparse Approximations of Matrices and Applications | http://arxiv.org/pdf/1506.07300v2.pdf | author:Luc Le Magoarou, Rémi Gribonval category:cs.LG published:2015-06-24 summary:The computational cost of many signal processing and machine learningtechniques is often dominated by the cost of applying certain linear operatorsto high-dimensional vectors. This paper introduces an algorithm aimed atreducing the complexity of applying linear operators in high dimension byapproximately factorizing the corresponding matrix into few sparse factors. Theapproach relies on recent advances in non-convex optimization. It is firstexplained and analyzed in details and then demonstrated experimentally onvarious problems including dictionary learning for image denoising, and theapproximation of large matrices arising in inverse problems.
arxiv-12300-179 | Nonnegative Matrix Factorization applied to reordered pixels of single images based on patches to achieve structured nonnegative dictionaries | http://arxiv.org/pdf/1506.08110v1.pdf | author:Richard M. Charles, Kye M. Taylor, James H. Curry category:cs.CV math.NA 65K02 published:2015-06-24 summary:Recent improvements in computing allow for the processing and analysis ofvery large datasets in a variety of fields. Often the analysis requires thecreation of low-rank approximations to the datasets leading to efficientstorage. This article presents and analyzes a novel approach for creatingnonnegative, structured dictionaries using NMF applied to reordered pixels ofsingle, natural images. We reorder the pixels based on patches and present ourapproach in general. We investigate our approach when using the Singular ValueDecomposition (SVD) and Nonnegative Matrix Factorizations (NMF) as low-rankapproximations. Peak Signal-to-Noise Ratio (PSNR) and Mean StructuralSimilarity Index (MSSIM) are used to evaluate the algorithm. We report thatwhile the SVD provides the best reconstructions, its dictionary of vectors loseboth the sign structure of the original image and details of localized imagecontent. In contrast, the dictionaries produced using NMF preserves the signstructure of the original image matrix and offer a nonnegative, parts-baseddictionary.
arxiv-12300-180 | Global Optimality in Tensor Factorization, Deep Learning, and Beyond | http://arxiv.org/pdf/1506.07540v1.pdf | author:Benjamin D. Haeffele, Rene Vidal category:cs.NA cs.LG stat.ML published:2015-06-24 summary:Techniques involving factorization are found in a wide range of applicationsand have enjoyed significant empirical success in many fields. However, commonto a vast majority of these problems is the significant disadvantage that theassociated optimization problems are typically non-convex due to a multilinearform or other convexity destroying transformation. Here we build on ideas fromconvex relaxations of matrix factorizations and present a very generalframework which allows for the analysis of a wide range of non-convexfactorization problems - including matrix factorization, tensor factorization,and deep neural network training formulations. We derive sufficient conditionsto guarantee that a local minimum of the non-convex optimization problem is aglobal minimum and show that if the size of the factorized variables is largeenough then from any initialization it is possible to find a global minimizerusing a purely local descent algorithm. Our framework also provides a partialtheoretical justification for the increasingly common use of Rectified LinearUnits (ReLUs) in deep neural networks and offers guidance on deep networkarchitectures and regularization strategies to facilitate efficientoptimization.
arxiv-12300-181 | Deep CNN Ensemble with Data Augmentation for Object Detection | http://arxiv.org/pdf/1506.07224v1.pdf | author:Jian Guo, Stephen Gould category:cs.CV published:2015-06-24 summary:We report on the methods used in our recent DeepEnsembleCoco submission tothe PASCAL VOC 2012 challenge, which achieves state-of-the-art performance onthe object detection task. Our method is a variant of the R-CNN model proposedGirshick:CVPR14 with two key improvements to training and evaluation. First,our method constructs an ensemble of deep CNN models with differentarchitectures that are complementary to each other. Second, we augment thePASCAL VOC training set with images from the Microsoft COCO dataset tosignificantly enlarge the amount training data. Importantly, we select a subsetof the Microsoft COCO images to be consistent with the PASCAL VOC task. Resultson the PASCAL VOC evaluation server show that our proposed method outperformall previous methods on the PASCAL VOC 2012 detection task at time ofsubmission.
arxiv-12300-182 | Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images | http://arxiv.org/pdf/1506.07365v3.pdf | author:Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin Riedmiller category:cs.LG cs.CV stat.ML published:2015-06-24 summary:We introduce Embed to Control (E2C), a method for model learning and controlof non-linear dynamical systems from raw pixel images. E2C consists of a deepgenerative model, belonging to the family of variational autoencoders, thatlearns to generate image trajectories from a latent space in which the dynamicsis constrained to be locally linear. Our model is derived directly from anoptimal control formulation in latent space, supports long-term prediction ofimage sequences and exhibits strong performance on a variety of complex controlproblems.
arxiv-12300-183 | Natural Scene Recognition Based on Superpixels and Deep Boltzmann Machines | http://arxiv.org/pdf/1506.07271v1.pdf | author:Jinfu Yang, Jingyu Gao, Guanghui Wang, Shanshan Zhang category:cs.CV published:2015-06-24 summary:The Deep Boltzmann Machines (DBM) is a state-of-the-art unsupervised learningmodel, which has been successfully applied to handwritten digit recognitionand, as well as object recognition. However, the DBM is limited in scenerecognition due to the fact that natural scene images are usually very large.In this paper, an efficient scene recognition approach is proposed based onsuperpixels and the DBMs. First, a simple linear iterative clustering (SLIC)algorithm is employed to generate superpixels of input images, where eachsuperpixel is regarded as an input of a learning model. Then, a two-layer DBMmodel is constructed by stacking two restricted Boltzmann machines (RBMs), anda greedy layer-wise algorithm is applied to train the DBM model. Finally, asoftmax regression is utilized to categorize scene images. The proposedtechnique can effectively reduce the computational complexity and enhance theperformance for large natural image recognition. The approach is verified andevaluated by extensive experiments, including the fifteen-scene categoriesdataset the UIUC eight-sports dataset, and the SIFT flow dataset, are used toevaluate the proposed method. The experimental results show that the proposedapproach outperforms other state-of-the-art methods in terms of recognitionrate.
arxiv-12300-184 | Benchmark of structured machine learning methods for microbial identification from mass-spectrometry data | http://arxiv.org/pdf/1506.07251v1.pdf | author:Kévin Vervier, Pierre Mahé, Jean-Baptiste Veyrieras, Jean-Philippe Vert category:stat.ML cs.LG q-bio.QM published:2015-06-24 summary:Microbial identification is a central issue in microbiology, in particular inthe fields of infectious diseases diagnosis and industrial quality control. Theconcept of species is tightly linked to the concept of biological and clinicalclassification where the proximity between species is generally measured interms of evolutionary distances and/or clinical phenotypes. Surprisingly, theinformation provided by this well-known hierarchical structure is rarely usedby machine learning-based automatic microbial identification systems.Structured machine learning methods were recently proposed for taking intoaccount the structure embedded in a hierarchy and using it as additional apriori information, and could therefore allow to improve microbialidentification systems. We test and compare several state-of-the-art machinelearning methods for microbial identification on a new Matrix-Assisted LaserDesorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset.We include in the benchmark standard and structured methods, that leverage theknowledge of the underlying hierarchical structure in the learning process. Ourresults show that although some methods perform better than others, structuredmethods do not consistently perform better than their "flat" counterparts. Wepostulate that this is partly due to the fact that standard methods alreadyreach a high level of accuracy in this context, and that they mainly confusespecies close to each other in the tree, a case where using the known hierarchyis not helpful.
arxiv-12300-185 | Unconfused ultraconservative multiclass algorithms | http://arxiv.org/pdf/1506.07254v1.pdf | author:Ugo Louche, Liva Ralaivola category:cs.LG published:2015-06-24 summary:We tackle the problem of learning linear classifiers from noisy datasets in amulticlass setting. The two-class version of this problem was studied a fewyears ago where the proposed approaches to combat the noise revolve around aPer-ceptron learning scheme fed with peculiar examples computed through aweighted average of points from the noisy training set. We propose to buildupon these approaches and we introduce a new algorithm called UMA (forUnconfused Multiclass additive Algorithm) which may be seen as a generalizationto the multiclass setting of the previous approaches. In order to characterizethe noise we use the confusion matrix as a multiclass extension of theclassification noise studied in the aforemen-tioned literature. Theoreticallywell-founded, UMA furthermore displays very good empirical noise robustness, asevidenced by numerical simulations conducted on both synthetic and real data.
arxiv-12300-186 | Unshredding of Shredded Documents: Computational Framework and Implementation | http://arxiv.org/pdf/1506.07440v1.pdf | author:Lei Kristoffer R. Lactuan, Jaderick P. Pabico category:cs.CV published:2015-06-24 summary:A shredded document $D$ is a document whose pages have been cut into stripsfor the purpose of destroying private, confidential, or sensitive information$I$ contained in $D$. Shredding has become a standard means of governmentorganizations, businesses, and private individuals to destroy archival recordsthat have been officially classified for disposal. It can also be used todestroy documentary evidence of wrongdoings by entities who are trying to hide$I$. In this paper, we present an optimal $O((n\times m)^2)$ algorithm $A$ thatreconstructs an $n$-page $D$, where each page $p$ is shredded into $m$ strips.We also present the efficacy of $A$ in reconstructing three document types:hand-written, machine typed-set, and images.
arxiv-12300-187 | Objective Variables for Probabilistic Revenue Maximization in Second-Price Auctions with Reserve | http://arxiv.org/pdf/1506.07504v1.pdf | author:Maja R. Rudolph, Joseph G. Ellis, David M. Blei category:stat.ML cs.AI cs.GT cs.LG stat.AP published:2015-06-24 summary:Many online companies sell advertisement space in second-price auctions withreserve. In this paper, we develop a probabilistic method to learn a profitablestrategy to set the reserve price. We use historical auction data with featuresto fit a predictor of the best reserve price. This problem is delicate - thestructure of the auction is such that a reserve price set too high is muchworse than a reserve price set too low. To address this we develop objectivevariables, a new framework for combining probabilistic modeling with optimaldecision-making. Objective variables are "hallucinated observations" thattransform the revenue maximization task into a regularized maximum likelihoodestimation problem, which we solve with an EM algorithm. This framework enablesa variety of prediction mechanisms to set the reserve price. As examples, westudy objective variable methods with regression, kernelized regression, andneural networks on simulated and real data. Our methods outperform previousapproaches both in terms of scalability and profit.
arxiv-12300-188 | Salient Object Detection via Objectness Measure | http://arxiv.org/pdf/1506.07363v1.pdf | author:Sai Srivatsa R, R. Venkatesh Babu category:cs.CV published:2015-06-24 summary:Salient object detection has become an important task in many imageprocessing applications. The existing approaches exploit background prior andcontrast prior to attain state of the art results. In this paper, instead ofusing background cues, we estimate the foreground regions in an image usingobjectness proposals and utilize it to obtain smooth and accurate saliencymaps. We propose a novel saliency measure called `foreground connectivity'which determines how tightly a pixel or a region is connected to the estimatedforeground. We use the values assigned by this measure as foreground weightsand integrate these in an optimization framework to obtain the final saliencymaps. We extensively evaluate the proposed approach on two benchmark databasesand demonstrate that the results obtained are better than the existing state ofthe art approaches.
arxiv-12300-189 | Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization | http://arxiv.org/pdf/1506.07512v1.pdf | author:Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford category:stat.ML cs.DS cs.LG published:2015-06-24 summary:We develop a family of accelerated stochastic algorithms that minimize sumsof convex functions. Our algorithms improve upon the fastest running time forempirical risk minimization (ERM), and in particular linear least-squaresregression, across a wide range of problem settings. To achieve this, weestablish a framework based on the classical proximal point algorithm. Namely,we provide several algorithms that reduce the minimization of a strongly convexfunction to approximate minimizations of regularizations of the function. Usingthese results, we accelerate recent fast stochastic algorithms in a black-boxfashion. Empirically, we demonstrate that the resulting algorithms exhibitnotions of stability that are advantageous in practice. Both in theory and inpractice, the provided algorithms reap the computational benefits of adding alarge strongly convex regularization term, without incurring a correspondingbias to the original problem.
arxiv-12300-190 | Splash: User-friendly Programming Interface for Parallelizing Stochastic Algorithms | http://arxiv.org/pdf/1506.07552v2.pdf | author:Yuchen Zhang, Michael I. Jordan category:cs.LG published:2015-06-24 summary:Stochastic algorithms are efficient approaches to solving machine learningand optimization problems. In this paper, we propose a general framework calledSplash for parallelizing stochastic algorithms on multi-node distributedsystems. Splash consists of a programming interface and an execution engine.Using the programming interface, the user develops sequential stochasticalgorithms without concerning any detail about distributed computing. Thealgorithm is then automatically parallelized by a communication-efficientexecution engine. We provide theoretical justifications on the optimal rate ofconvergence for parallelizing stochastic gradient descent. Splash is built ontop of Apache Spark. The real-data experiments on logistic regression,collaborative filtering and topic modeling verify that Splash yieldsorder-of-magnitude speedup over single-thread stochastic algorithms and overstate-of-the-art implementations on Spark.
arxiv-12300-191 | Ask Me Anything: Dynamic Memory Networks for Natural Language Processing | http://arxiv.org/pdf/1506.07285v5.pdf | author:Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher category:cs.CL cs.LG cs.NE published:2015-06-24 summary:Most tasks in natural language processing can be cast into question answering(QA) problems over language input. We introduce the dynamic memory network(DMN), a neural network architecture which processes input sequences andquestions, forms episodic memories, and generates relevant answers. Questionstrigger an iterative attention process which allows the model to condition itsattention on the inputs and the result of previous iterations. These resultsare then reasoned over in a hierarchical recurrent sequence model to generateanswers. The DMN can be trained end-to-end and obtains state-of-the-art resultson several types of tasks and datasets: question answering (Facebook's bAbIdataset), text classification for sentiment analysis (Stanford SentimentTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). Thetraining for these different tasks relies exclusively on trained word vectorrepresentations and input-question-answer triplets.
arxiv-12300-192 | A Novel Feature Extraction Method for Scene Recognition Based on Centered Convolutional Restricted Boltzmann Machines | http://arxiv.org/pdf/1506.07257v1.pdf | author:Jingyu Gao, Jinfu Yang, Guanghui Wang, Mingai Li category:cs.CV published:2015-06-24 summary:Scene recognition is an important research topic in computer vision, whilefeature extraction is a key step of object recognition. Although classicalRestricted Boltzmann machines (RBM) can efficiently represent complicated data,it is hard to handle large images due to its complexity in computation. In thispaper, a novel feature extraction method, named Centered ConvolutionalRestricted Boltzmann Machines (CCRBM), is proposed for scene recognition. Theproposed model is an improved Convolutional Restricted Boltzmann Machines(CRBM) by introducing centered factors in its learning strategy to reduce thesource of instabilities. First, the visible units of the network are redefinedusing centered factors. Then, the hidden units are learned with a modifiedenergy function by utilizing a distribution function, and the visible units arereconstructed using the learned hidden units. In order to achieve bettergenerative ability, the Centered Convolutional Deep Belief Networks (CCDBN) istrained in a greedy layer-wise way. Finally, a softmax regression isincorporated for scene recognition. Extensive experimental evaluations usingnatural scenes, MIT-indoor scenes, and Caltech 101 datasets show that theproposed approach performs better than other counterparts in terms ofstability, generalization, and discrimination. The CCDBN model is more suitablefor natural scene image recognition by virtue of convolutional property.
arxiv-12300-193 | Multiresolution Approach to Acceleration of Iterative Image Reconstruction for X-Ray Imaging for Security Applications | http://arxiv.org/pdf/1508.04458v1.pdf | author:S. Degirmenci, Joseph A. O'Sullivan, David G. Politte category:cs.CV published:2015-06-24 summary:Three-dimensional x-ray CT image reconstruction in baggage scanning insecurity applications is an important research field. The variety of materialsto be reconstructed is broader than medical x-ray imaging. Presence of highattenuating materials such as metal may cause artifacts if analyticalreconstruction methods are used. Statistical modeling and the resultantiterative algorithms are known to reduce these artifacts and present goodquantitative accuracy in estimates of linear attenuation coefficients. However,iterative algorithms may require computations in order to achievequantitatively accurate results. For the case of baggage scanning, in order toprovide fast accurate inspection throughput, they must be accelerateddrastically. There are many approaches proposed in the literature to increasespeed of convergence. This paper presents a new method that estimates thewavelet coefficients of the images in the discrete wavelet transform domaininstead of the image space itself. Initially, surrogate functions are createdaround approximation coefficients only. As the iterations proceed, the wavelettree on which the updates are made is expanded based on a criterion and detailcoefficients at each level are updated and the tree is expanded this way. Forexample, in the smooth regions of the image the detail coefficients are notupdated while the coefficients that represent the high-frequency componentaround edges are being updated, thus saving time by focusing computations wherethey are needed. This approach is implemented on real data from a SureScan (TM)x1000 Explosive Detection System and compared to straightforward implementationof the unregularized alternating minimization of O'Sullivan and Benac [1].
arxiv-12300-194 | Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks | http://arxiv.org/pdf/1506.07220v1.pdf | author:Yangtuo Peng, Hui Jiang category:cs.CE cs.AI cs.CL published:2015-06-24 summary:Financial news contains useful information on public companies and themarket. In this paper we apply the popular word embedding methods and deepneural networks to leverage financial news to predict stock price movements inthe market. Experimental results have shown that our proposed methods aresimple but very effective, which can significantly improve the stock predictionaccuracy on a standard financial database over the baseline system using onlythe historical price information.
arxiv-12300-195 | Secrets of GrabCut and Kernel K-means | http://arxiv.org/pdf/1506.07439v1.pdf | author:Meng Tang, Ismail Ben Ayed, Dmitrii Marin, Yuri Boykov category:cs.CV published:2015-06-24 summary:The log-likelihood energy term in popular model-fitting segmentation methods,e.g. Zhu-Yuille, Chan-Vese, GrabCut, etc., is presented as a generalized"probabilistic" K-means energy for color space clustering. This interpretationreveals some limitations, e.g. over-fitting. We propose an alternative approachto color clustering using kernel K-means energy with well-known properties suchas non-linear separation and scalability to higher-dimensional feature spaces.Similarly to log-likelihoods, our kernel energy term for color space clusteringcan be combined with image grid regularization, e.g. boundary smoothness, andminimized using (pseudo-) bound optimization and max-flow algorithm. Unlikehistogram or GMM fitting and implicit entropy minimization, our approach isclosely related to general pairwise clustering such as average association andnormalized cut. But, in contrast to previous pairwise clustering algorithms,our approach can incorporate any standard geometric regularization in the imagedomain. We analyze extreme cases for kernel bandwidth (e.g. Gini bias) andpropose adaptive strategies. Our general kernel-based approach opens the doorfor many extensions/applications.
arxiv-12300-196 | Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality | http://arxiv.org/pdf/1506.07216v3.pdf | author:Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, David P. Woodruff category:cs.LG cs.CC cs.IT math.IT stat.ML published:2015-06-24 summary:We study the tradeoff between the statistical error and communication cost ofdistributed statistical estimation problems in high dimensions. In thedistributed sparse Gaussian mean estimation problem, each of the $m$ machinesreceives $n$ data points from a $d$-dimensional Gaussian distribution withunknown mean $\theta$ which is promised to be $k$-sparse. The machinescommunicate by message passing and aim to estimate the mean $\theta$. Weprovide a tight (up to logarithmic factors) tradeoff between the estimationerror and the number of bits communicated between the machines. This directlyleads to a lower bound for the distributed \textit{sparse linear regression}problem: to achieve the statistical minimax error, the total communication isat least $\Omega(\min\{n,d\}m)$, where $n$ is the number of observations thateach machine receives and $d$ is the ambient dimension. These lower resultsimprove upon [Sha14,SD'14] by allowing multi-round iterative communicationmodel. We also give the first optimal simultaneous protocol in the dense casefor mean estimation. As our main technique, we prove a \textit{distributed data processinginequality}, as a generalization of usual data processing inequalities, whichmight be of independent interest and useful for other problems.
arxiv-12300-197 | Incremental RANSAC for Online Relocation in Large Dynamic Environments | http://arxiv.org/pdf/1506.07236v2.pdf | author:Kanji Tanaka, Eiji Kondo category:cs.RO cs.CV published:2015-06-24 summary:Vehicle relocation is the problem in which a mobile robot has to estimate theself-position with respect to an a priori map of landmarks using the perceptionand the motion measurements without using any knowledge of the initialself-position. Recently, RANdom SAmple Consensus (RANSAC), a robustmulti-hypothesis estimator, has been successfully applied to offline relocationin static environments. On the other hand, online relocation in dynamicenvironments is still a difficult problem, for available computation time isalways limited, and for measurement include many outliers. To realize real timealgorithm for such an online process, we have developed an incremental versionof RANSAC algorithm by extending an efficient preemption RANSAC scheme. Thisnovel scheme named incremental RANSAC is able to find inlier hypotheses ofself-positions out of large number of outlier hypotheses contaminated byoutlier measurements.
arxiv-12300-198 | Learning Representations from Deep Networks Using Mode Synthesizers | http://arxiv.org/pdf/1506.07545v1.pdf | author:N. E. Osegi, P. Enyindah category:cs.NE published:2015-06-24 summary:Deep learning Networks play a crucial role in the evolution of a vast numberof current machine learning models for solving a variety of real worldnon-trivial tasks. Such networks use big data which is generally unlabeledunsupervised and multi-layered requiring no form of supervision for trainingand learning data and has been used to successfully build automatic supervisoryneural networks. However the question still remains how well the learned datarepresents interestingness, and their effectiveness i.e. efficiency in deeplearning models or applications. If the output of a network of deep learningmodels can be beamed unto a scene of observables, we could learn thevariational frequencies of these stacked networks in a parallel anddistributive way.This paper seeks to discover and represent interestingpatterns in an efficient and less complex way by incorporating the concept ofMode synthesizers in the deep learning process models
arxiv-12300-199 | Strategic Classification | http://arxiv.org/pdf/1506.06980v2.pdf | author:Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, Mary Wootters category:cs.LG published:2015-06-23 summary:Machine learning relies on the assumption that unseen test instances of aclassification problem follow the same distribution as observed training data.However, this principle can break down when machine learning is used to makeimportant decisions about the welfare (employment, education, health) ofstrategic individuals. Knowing information about the classifier, suchindividuals may manipulate their attributes in order to obtain a betterclassification outcome. As a result of this behavior---often referred to asgaming---the performance of the classifier may deteriorate sharply. Indeed,gaming is a well-known obstacle for using machine learning methods in practice;in financial policy-making, the problem is widely known as Goodhart's law. Inthis paper, we formalize the problem, and pursue algorithms for learningclassifiers that are robust to gaming. We model classification as a sequential game between a player named "Jury"and a player named "Contestant." Jury designs a classifier, and Contestantreceives an input to the classifier, which he may change at some cost. Jury'sgoal is to achieve high classification accuracy with respect to Contestant'soriginal input and some underlying target classification function. Contestant'sgoal is to achieve a favorable classification outcome while taking into accountthe cost of achieving it. For a natural class of cost functions, we obtain computationally efficientlearning algorithms which are near-optimal. Surprisingly, our algorithms areefficient even on concept classes that are computationally hard to learn. Forgeneral cost functions, designing an approximately optimal strategy-proofclassifier, for inverse-polynomial approximation, is NP-hard.
arxiv-12300-200 | deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets | http://arxiv.org/pdf/1506.06863v2.pdf | author:Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, Bill Dolan category:cs.CL published:2015-06-23 summary:We introduce Discriminative BLEU (deltaBLEU), a novel metric for intrinsicevaluation of generated text in tasks that admit a diverse range of possibleoutputs. Reference strings are scored for quality by human raters on a scale of[-1, +1] to weight multi-reference BLEU. In tasks involving generation ofconversational responses, deltaBLEU correlates reasonably with human judgmentsand outperforms sentence-level and IBM BLEU in terms of both Spearman's rho andKendall's tau.
arxiv-12300-201 | On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants | http://arxiv.org/pdf/1506.06840v2.pdf | author:Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabás Póczos, Alex Smola category:cs.LG stat.ML published:2015-06-23 summary:We study optimization algorithms based on variance reduction for stochasticgradient descent (SGD). Remarkable recent progress has been made in thisdirection through development of algorithms like SAG, SVRG, SAGA. Thesealgorithms have been shown to outperform SGD, both theoretically andempirically. However, asynchronous versions of these algorithms---a crucialrequirement for modern large-scale applications---have not been studied. Webridge this gap by presenting a unifying framework for many variance reductiontechniques. Subsequently, we propose an asynchronous algorithm grounded in ourframework, and prove its fast convergence. An important consequence of ourgeneral approach is that it yields asynchronous versions of variance reductionalgorithms such as SVRG and SAGA as a byproduct. Our method achieves nearlinear speedup in sparse settings common to machine learning. We demonstratethe empirical performance of our method through a concrete realization ofasynchronous SVRG.
arxiv-12300-202 | Detection and Analysis of Emotion From Speech Signals | http://arxiv.org/pdf/1506.06832v1.pdf | author:Assel Davletcharova, Sherin Sugathan, Bibia Abraham, Alex Pappachen James category:cs.SD cs.CL cs.HC published:2015-06-23 summary:Recognizing emotion from speech has become one the active research themes inspeech processing and in applications based on human-computer interaction. Thispaper conducts an experimental study on recognizing emotions from human speech.The emotions considered for the experiments include neutral, anger, joy andsadness. The distinuishability of emotional features in speech were studiedfirst followed by emotion classification performed on a custom dataset. Theclassification was performed for different classifiers. One of the main featureattribute considered in the prepared dataset was the peak-to-peak distanceobtained from the graphical representation of the speech signals. Afterperforming the classification tests on a dataset formed from 30 differentsubjects, it was found that for getting better accuracy, one should considerthe data collected from one person rather than considering the data from agroup of people.
arxiv-12300-203 | A Survey of Current Datasets for Vision and Language Research | http://arxiv.org/pdf/1506.06833v2.pdf | author:Francis Ferraro, Nasrin Mostafazadeh, Ting-Hao, Huang, Lucy Vanderwende, Jacob Devlin, Michel Galley, Margaret Mitchell category:cs.CL cs.AI cs.CV cs.GL published:2015-06-23 summary:Integrating vision and language has long been a dream in work on artificialintelligence (AI). In the past two years, we have witnessed an explosion ofwork that brings together vision and language from images to videos and beyond.The available corpora have played a crucial role in advancing this area ofresearch. In this paper, we propose a set of quality metrics for evaluating andanalyzing the vision & language datasets and categorize them accordingly. Ouranalyses show that the most recent datasets have been using more complexlanguage and more abstract concepts, however, there are different strengths andweaknesses in each.
arxiv-12300-204 | A Feature-Based Analysis on the Impact of Set of Constraints for e-Constrained Differential Evolution | http://arxiv.org/pdf/1506.06848v1.pdf | author:Shayan Poursoltan, FranK Neumann category:cs.NE published:2015-06-23 summary:Different types of evolutionary algorithms have been developed forconstrained continuous optimization. We carry out a feature-based analysis ofevolved constrained continuous optimization instances to understand thecharacteristics of constraints that make problems hard for evolutionaryalgorithm. In our study, we examine how various sets of constraints caninfluence the behaviour of e-Constrained Differential Evolution. Investigatingthe evolved instances, we obtain knowledge of what type of constraints andtheir features make a problem difficult for the examined algorithm.
arxiv-12300-205 | Learning Discriminative Bayesian Networks from High-dimensional Continuous Neuroimaging Data | http://arxiv.org/pdf/1506.06868v1.pdf | author:Luping Zhou, Lei Wang, Lingqiao Liu, Philip Ogunbona, Dinggang Shen category:cs.CV cs.LG published:2015-06-23 summary:Due to its causal semantics, Bayesian networks (BN) have been widely employedto discover the underlying data relationship in exploratory studies, such asbrain research. Despite its success in modeling the probability distribution ofvariables, BN is naturally a generative model, which is not necessarilydiscriminative. This may cause the ignorance of subtle but critical networkchanges that are of investigation values across populations. In this paper, wepropose to improve the discriminative power of BN models for continuousvariables from two different perspectives. This brings two generaldiscriminative learning frameworks for Gaussian Bayesian networks (GBN). In thefirst framework, we employ Fisher kernel to bridge the generative models of GBNand the discriminative classifiers of SVMs, and convert the GBN parameterlearning to Fisher kernel learning via minimizing a generalization error boundof SVMs. In the second framework, we employ the max-margin criterion and buildit directly upon GBN models to explicitly optimize the classificationperformance of the GBNs. The advantages and disadvantages of the two frameworksare discussed and experimentally compared. Both of them demonstrate strongpower in learning discriminative parameters of GBNs for neuroimaging basedbrain network analysis, as well as maintaining reasonable representationcapacity. The contributions of this paper also include a new Directed AcyclicGraph (DAG) constraint with theoretical guarantee to ensure the graph validityof GBN.
arxiv-12300-206 | Advanced statistical methods for eye movement analysis and modeling: a gentle introduction | http://arxiv.org/pdf/1506.07194v3.pdf | author:Giuseppe Boccignone category:cs.CV q-bio.NC G.3; I.5; I.4 published:2015-06-23 summary:In this Chapter we show that by considering eye movements, and in particular,the resulting sequence of gaze shifts, a stochastic process, a wide variety oftools become available for analyses and modelling beyond conventionalstatistical methods. Such tools encompass random walk analyses and more complextechniques borrowed from the pattern recognition and machine learning fields. After a brief, though critical, probabilistic tour of current computationalmodels of eye movements and visual attention, we lay down the basis for gazeshift pattern analysis. To this end, the concepts of Markov Processes, theWiener process and related random walks within the Gaussian framework of theCentral Limit Theorem will be introduced. Then, we will deliberately violatefundamental assumptions of the Central Limit Theorem to elicit a largerperspective, rooted in statistical physics, for analysing and modelling eyemovements in terms of anomalous, non-Gaussian, random walks and modern foragingtheory. Eventually, by resorting to machine learning techniques, we discuss how theanalyses of movement patterns can develop into the inference of hidden patternsof the mind: inferring the observer's task, assessing cognitive impairments,classifying expertise.
arxiv-12300-207 | Autonomous 3D Reconstruction Using a MAV | http://arxiv.org/pdf/1506.06876v1.pdf | author:Alexander Popov, Dimitrios Zermas, Nikolaos Papanikolopoulos category:cs.CV published:2015-06-23 summary:An approach is proposed for high resolution 3D reconstruction of an objectusing a Micro Air Vehicle (MAV). A system is described which autonomouslycaptures images and performs a dense 3D reconstruction via structure frommotion with no prior knowledge of the environment. Only the MAVs own sensors,the front facing camera and the Inertial Measurement Unit (IMU) are utilized.Precision agriculture is considered as an example application for the system.
arxiv-12300-208 | On Elicitation Complexity and Conditional Elicitation | http://arxiv.org/pdf/1506.07212v1.pdf | author:Rafael Frongillo, Ian A. Kash category:cs.LG math.OC math.ST q-fin.MF stat.TH published:2015-06-23 summary:Elicitation is the study of statistics or properties which are computable viaempirical risk minimization. While several recent papers have approached thegeneral question of which properties are elicitable, we suggest that this isthe wrong question---all properties are elicitable by first eliciting theentire distribution or data set, and thus the important question is howelicitable. Specifically, what is the minimum number of regression parametersneeded to compute the property? Building on previous work, we introduce a new notion of elicitationcomplexity and lay the foundations for a calculus of elicitation. We establishseveral general results and techniques for proving upper and lower bounds onelicitation complexity. These results provide tight bounds for eliciting theBayes risk of any loss, a large class of properties which includes spectralrisk measures and several new properties of interest. Finally, we extend ourcalculus to conditionally elicitable properties, which are elicitableconditioned on knowing the value of another property, giving a necessarycondition for the elicitability of both properties together.
arxiv-12300-209 | Multi-domain Dialog State Tracking using Recurrent Neural Networks | http://arxiv.org/pdf/1506.07190v1.pdf | author:Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG published:2015-06-23 summary:Dialog state tracking is a key component of many modern dialog systems, mostof which are designed with a single, well-defined domain in mind. This papershows that dialog data drawn from different dialog domains can be used to traina general belief tracking model which can operate across all of these domains,exhibiting superior performance to each of the domain-specific models. Wepropose a training procedure which uses out-of-domain data to initialise belieftracking models for entirely new domains. This procedure leads to improvementsin belief tracking performance regardless of the amount of in-domain dataavailable for training the model.
arxiv-12300-210 | Coercive functions from a topological viewpoint and properties of minimizing sets of convex functions appearing in image restoration | http://arxiv.org/pdf/1506.08615v1.pdf | author:René Ciak category:math.OC cs.CV math.CA math.FA published:2015-06-23 summary:Many tasks in image processing can be tackled by modeling an appropriate datafidelity term $\Phi: \mathbb{R}^n \rightarrow \mathbb{R} \cup \{+\infty\}$ andthen solve one of the regularized minimization problems \begin{align*} &{}(P_{1,\tau}) \qquad \mathop{\rm argmin}_{x \in \mathbb R^n} \big\{ \Phi(x)\;{\rm s.t.}\; \Psi(x) \leq \tau \big\} \\ &{}(P_{2,\lambda}) \qquad\mathop{\rm argmin}_{x \in \mathbb R^n} \{ \Phi(x) + \lambda \Psi(x) \}, \;\lambda > 0 \end{align*} with some function $\Psi: \mathbb{R}^n \rightarrow\mathbb{R} \cup \{+\infty\}$ and a good choice of the parameter(s). Two tasksarise naturally here: \begin{align*} {}& \text{1. Study the solver sets ${\rmSOL}(P_{1,\tau})$ and ${\rm SOL}(P_{2,\lambda})$ of the minimization problems.} \\ {}& \text{2.Ensure that the minimization problems have solutions.} \end{align*} This thesisprovides contributions to both tasks: Regarding the first task for a morespecial setting we prove that there are intervals $(0,c)$ and $(0,d)$ such thatthe setvalued curves \begin{align*} \tau \mapsto {}& {\rm SOL}(P_{1,\tau}), \; \tau \in (0,c) \\ {} \lambda\mapsto {}& {\rm SOL}(P_{2,\lambda}), \; \lambda \in (0,d) \end{align*} are thesame, besides an order reversing parameter change $g: (0,c) \rightarrow (0,d)$.Moreover we show that the solver sets are changing all the time while $\tau$runs from $0$ to $c$ and $\lambda$ runs from $d$ to $0$. In the presence of lower semicontinuity the second task is done if we haveadditionally coercivity. We regard lower semicontinuity and coercivity from atopological point of view and develop a new technique for proving lowersemicontinuity plus coercivity. Dropping any lower semicontinuity assumption we also prove a theorem on thecoercivity of a sum of functions.
arxiv-12300-211 | Efficient approximate Bayesian inference for models with intractable likelihoods | http://arxiv.org/pdf/1506.06975v1.pdf | author:Johan Dahlin, Mattias Villani, Thomas B. Schön category:stat.CO q-fin.RM stat.ML published:2015-06-23 summary:We consider the problem of approximate Bayesian parameter inference innonlinear state space models with intractable likelihoods. Sequential MonteCarlo with approximate Bayesian computations (SMC-ABC) is an approach toapproximate the likelihood in this type of models. However, such approximationscan be noisy and computationally costly which hinders efficient implementationsusing standard methods based on optimisation and statistical simulation. Wepropose a novel method based on the combination of Gaussian processoptimisation (GPO) and SMC-ABC to create a Laplace approximation of theintractable posterior. The properties of the resulting GPO-ABC method arestudied using stochastic volatility (SV) models with both synthetic andreal-world data. We conclude that the algorithm enjoys: good accuracycomparable to particle Markov chain Monte Carlo with a significant reduction incomputational cost and better robustness to noise in the estimates comparedwith a gradient-based optimisation algorithm. Finally, we make use of GPO-ABCto estimate the Value-at-Risk for a portfolio using a copula model with SVmodels for the margins.
arxiv-12300-212 | GEFCOM 2014 - Probabilistic Electricity Price Forecasting | http://arxiv.org/pdf/1506.06972v1.pdf | author:Gergo Barta, Gyula Borbely, Gabor Nagy, Sandor Kazi, Tamas Henk category:stat.ML cs.CE cs.LG stat.AP published:2015-06-23 summary:Energy price forecasting is a relevant yet hard task in the field ofmulti-step time series forecasting. In this paper we compare a well-known andestablished method, ARMA with exogenous variables with a relatively newtechnique Gradient Boosting Regression. The method was tested on data fromGlobal Energy Forecasting Competition 2014 with a year long rolling windowforecast. The results from the experiment reveal that a multi-model approach issignificantly better performing in terms of error metrics. Gradient Boostingcan deal with seasonality and auto-correlation out-of-the box and achieve lowerrate of normalized mean absolute error on real-world data.
arxiv-12300-213 | Automatic vehicle tracking and recognition from aerial image sequences | http://arxiv.org/pdf/1506.06881v1.pdf | author:Ognjen Arandjelovic category:cs.CV published:2015-06-23 summary:This paper addresses the problem of automated vehicle tracking andrecognition from aerial image sequences. Motivated by its successes in theexisting literature focus on the use of linear appearance subspaces to describemulti-view object appearance and highlight the challenges involved in theirapplication as a part of a practical system. A working solution which includessteps for data extraction and normalization is described. In experiments onreal-world data the proposed methodology achieved promising results with a highcorrect recognition rate and few, meaningful errors (type II errors wherebygenuinely similar targets are sometimes being confused with one another).Directions for future research and possible improvements of the proposed methodare discussed.
arxiv-12300-214 | Segmentation of Three-dimensional Images with Parametric Active Surfaces and Topology Changes | http://arxiv.org/pdf/1506.07136v1.pdf | author:Heike Benninghoff, Harald Garcke category:cs.CV published:2015-06-23 summary:In this paper, we introduce a novel parametric method for segmentation ofthree-dimensional images. We consider a piecewise constant version of theMumford-Shah and the Chan-Vese functionals and perform a region-basedsegmentation of 3D image data. An evolution law is derived from energyminimization problems which push the surfaces to the boundaries of 3D objectsin the image. We propose a parametric scheme which describes the evolution ofparametric surfaces. An efficient finite element scheme is proposed for anumerical approximation of the evolution equations. Since standard parametricmethods cannot handle topology changes automatically, an efficient method ispresented to detect, identify and perform changes in the topology of thesurfaces. One main focus of this paper are the algorithmic details to handletopology changes like splitting and merging of surfaces and change of the genusof a surface. Different artificial images are studied to demonstrate theability to detect the different types of topology changes. Finally, theparametric method is applied to segmentation of medical 3D images.
arxiv-12300-215 | SALSA: A Novel Dataset for Multimodal Group Behavior Analysis | http://arxiv.org/pdf/1506.06882v1.pdf | author:Xavier Alameda-Pineda, Jacopo Staiano, Ramanathan Subramanian, Ligia Batrinca, Elisa Ricci, Bruno Lepri, Oswald Lanz, Nicu Sebe category:cs.CV published:2015-06-23 summary:Studying free-standing conversational groups (FCGs) in unstructured socialsettings (e.g., cocktail party ) is gratifying due to the wealth of informationavailable at the group (mining social networks) and individual (recognizingnative behavioral and personality traits) levels. However, analyzing socialscenes involving FCGs is also highly challenging due to the difficulty inextracting behavioral cues such as target locations, their speaking activityand head/body pose due to crowdedness and presence of extreme occlusions. Tothis end, we propose SALSA, a novel dataset facilitating multimodal andSynergetic sociAL Scene Analysis, and make two main contributions to researchon automated social interaction analysis: (1) SALSA records social interactionsamong 18 participants in a natural, indoor environment for over 60 minutes,under the poster presentation and cocktail party contexts presentingdifficulties in the form of low-resolution images, lighting variations,numerous occlusions, reverberations and interfering sound sources; (2) Toalleviate these problems we facilitate multimodal analysis by recording thesocial interplay using four static surveillance cameras and sociometric badgesworn by each participant, comprising the microphone, accelerometer, bluetoothand infrared sensors. In addition to raw data, we also provide annotationsconcerning individuals' personality as well as their position, head, bodyorientation and F-formation information over the entire event duration. Throughextensive experiments with state-of-the-art approaches, we show (a) thelimitations of current methods and (b) how the recorded multiple cuessynergetically aid automatic analysis of social interactions. SALSA isavailable at http://tev.fbk.eu/salsa.
arxiv-12300-216 | R-CNN minus R | http://arxiv.org/pdf/1506.06981v1.pdf | author:Karel Lenc, Andrea Vedaldi category:cs.CV published:2015-06-23 summary:Deep convolutional neural networks (CNNs) have had a major impact in mostareas of image understanding, including object category detection. In objectdetection, methods such as R-CNN have obtained excellent results by integratingCNNs with region proposal generation algorithms such as selective search. Inthis paper, we investigate the role of proposal generation in CNN-baseddetectors in order to determine whether it is a necessary modelling component,carrying essential geometric information not contained in the CNN, or whetherit is merely a way of accelerating detection. We do so by designing andevaluating a detector that uses a trivial region generation scheme, constantfor each image. Combined with SPP, this results in an excellent and fastdetector that does not require to process an image with algorithms other thanthe CNN itself. We also streamline and simplify the training of CNN-baseddetectors by integrating several learning steps in a single algorithm, as wellas by proposing a number of improvements that accelerate detection.
arxiv-12300-217 | Improving Fiber Alignment in HARDI by Combining Contextual PDE Flow with Constrained Spherical Deconvolution | http://arxiv.org/pdf/1506.07062v1.pdf | author:J. M. Portegies, R. H. J. Fick, G. R. Sanguinetti, S. P. L. Meesters, G. Girard, R. Duits category:cs.CV published:2015-06-23 summary:We propose two strategies to improve the quality of tractography resultscomputed from diffusion weighted magnetic resonance imaging (DW-MRI) data. Bothmethods are based on the same PDE framework, defined in the coupled space ofpositions and orientations, associated with a stochastic process describing theenhancement of elongated structures while preserving crossing structures. Inthe first method we use the enhancement PDE for contextual regularization of afiber orientation distribution (FOD) that is obtained on individual voxels fromhigh angular resolution diffusion imaging (HARDI) data via constrainedspherical deconvolution (CSD). Thereby we improve the FOD as input forsubsequent tractography. Secondly, we introduce the fiber to bundle coherence(FBC), a measure for quantification of fiber alignment. The FBC is computedfrom a tractography result using the same PDE framework and provides acriterion for removing the spurious fibers. We validate the proposedcombination of CSD and enhancement on phantom data and on human data, acquiredwith different scanning protocols. On the phantom data we find that PDEenhancements improve both local metrics and global metrics of tractographyresults, compared to CSD without enhancements. On the human data we show thatthe enhancements allow for a better reconstruction of crossing fiber bundlesand they reduce the variability of the tractography output with respect to theacquisition parameters. Finally, we show that both the enhancement of the FODsand the use of the FBC measure on the tractography improve the stability withrespect to different stochastic realizations of probabilistic tractography.This is shown in a clinical application: the reconstruction of the opticradiation for epilepsy surgery planning.
arxiv-12300-218 | New Approach to translation of Isolated Units in English-Korean Machine Translation | http://arxiv.org/pdf/1506.06904v1.pdf | author:Kim Song Jon, An Hae Gum category:cs.CL published:2015-06-23 summary:It is the most effective way for quick translation of tremendous amount ofexplosively increasing science and technique information material to develop apracticable machine translation system and introduce it into translationpractice. This essay treats problems arising from translation of isolated unitson the basis of the practical materials and experiments obtained in thedevelopment and introduction of English-Korean machine translation system. Inother words, this essay considers establishment of information for isolatedunits and their Korean equivalents and word order.
arxiv-12300-219 | Graphs in machine learning: an introduction | http://arxiv.org/pdf/1506.06962v1.pdf | author:Pierre Latouche, Fabrice Rossi category:stat.ML cs.LG cs.SI physics.soc-ph published:2015-06-23 summary:Graphs are commonly used to characterise interactions between objects ofinterest. Because they are based on a straightforward formalism, they are usedin many scientific fields from computer science to historical sciences. In thispaper, we give an introduction to some methods relying on graphs for learning.This includes both unsupervised and supervised methods. Unsupervised learningalgorithms usually aim at visualising graphs in latent spaces and/or clusteringthe nodes. Both focus on extracting knowledge from graph topologies. While mostexisting techniques are only applicable to static graphs, where edges do notevolve through time, recent developments have shown that they could be extendedto deal with evolving networks. In a supervised context, one generally aims atinferring labels or numerical values attached to nodes using both the graphand, when they are available, node characteristics. Balancing the two sourcesof information can be challenging, especially as they can disagree locally orglobally. In both contexts, supervised and un-supervised, data can berelational (augmented with one or several global graphs) as described above, orgraph valued. In this latter case, each object of interest is given as a fullgraph (possibly completed by other characteristics). In this context, naturaltasks include graph clustering (as in producing clusters of graphs rather thanclusters of nodes in a single graph), graph classification, etc. 1 Realnetworks One of the first practical studies on graphs can be dated back to theoriginal work of Moreno [51] in the 30s. Since then, there has been a growinginterest in graph analysis associated with strong developments in the modellingand the processing of these data. Graphs are now used in many scientificfields. In Biology [54, 2, 7], for instance, metabolic networks can describepathways of biochemical reactions [41], while in social sciences networks areused to represent relation ties between actors [66, 56, 36, 34]. Other examplesinclude powergrids [71] and the web [75]. Recently, networks have also beenconsidered in other areas such as geography [22] and history [59, 39]. Inmachine learning, networks are seen as powerful tools to model problems inorder to extract information from data and for prediction purposes. This is theobject of this paper. For more complete surveys, we refer to [28, 62, 49, 45].In this section, we introduce notations and highlight properties shared by mostreal networks. In Section 2, we then consider methods aiming at extractinginformation from a unique network. We will particularly focus on clusteringmethods where the goal is to find clusters of vertices. Finally, in Section 3,techniques that take a series of networks into account, where each network is
arxiv-12300-220 | Person re-identification via efficient inference in fully connected CRF | http://arxiv.org/pdf/1506.06905v1.pdf | author:Jiuqing Wan, Menglin Xing category:cs.CV published:2015-06-23 summary:In this paper, we address the problem of person re-identification problem,i.e., retrieving instances from gallery which are generated by the same personas the given probe image. This is very challenging because the person'sappearance usually undergoes significant variations due to changes inillumination, camera angle and view, background clutter, and occlusion over thecamera network. In this paper, we assume that the matched gallery images shouldnot only be similar to the probe, but also be similar to each other, undersuitable metric. We express this assumption with a fully connected CRF model inwhich each node corresponds to a gallery and every pair of nodes are connectedby an edge. A label variable is associated with each node to indicate whetherthe corresponding image is from target person. We define unary potential foreach node using existing feature calculation and matching techniques, whichreflect the similarity between probe and gallery image, and define pairwisepotential for each edge in terms of a weighed combination of Gaussian kernels,which encode appearance similarity between pair of gallery images. The specificform of pairwise potential allows us to exploit an efficient inferencealgorithm to calculate the marginal distribution of each label variable forthis dense connected CRF. We show the superiority of our method by applying itto public datasets and comparing with the state of the art.
arxiv-12300-221 | Nonparametric Bayesian Double Articulation Analyzer for Direct Language Acquisition from Continuous Speech Signals | http://arxiv.org/pdf/1506.06646v2.pdf | author:Tadahiro Taniguchi, Ryo Nakashima, Shogo Nagasaka category:cs.AI cs.CL cs.LG stat.ML published:2015-06-22 summary:Human infants can discover words directly from unsegmented speech signalswithout any explicitly labeled data. In this paper, we develop a novel machinelearning method called nonparametric Bayesian double articulation analyzer(NPB-DAA) that can directly acquire language and acoustic models from observedcontinuous speech signals. For this purpose, we propose an integrativegenerative model that combines a language model and an acoustic model into asingle generative model called the "hierarchical Dirichlet process hiddenlanguage model" (HDP-HLM). The HDP-HLM is obtained by extending thehierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed byJohnson et al. An inference procedure for the HDP-HLM is derived using theblocked Gibbs sampler originally proposed for the HDP-HSMM. This procedureenables the simultaneous and direct inference of language and acoustic modelsfrom continuous speech signals. Based on the HDP-HLM and its inferenceprocedure, we developed a novel double articulation analyzer. By assumingHDP-HLM as a generative model of observed time series data, and by inferringlatent variables of the model, the method can analyze latent doublearticulation structure, i.e., hierarchically organized latent words andphonemes, of the data in an unsupervised manner. The novel unsupervised doublearticulation analyzer is called NPB-DAA. The NPB-DAA can automatically estimate double articulation structure embeddedin speech signals. We also carried out two evaluation experiments usingsynthetic data and actual human continuous speech signals representing Japanesevowel sequences. In the word acquisition and phoneme categorization tasks, theNPB-DAA outperformed a conventional double articulation analyzer (DAA) andbaseline automatic speech recognition system whose acoustic model was trainedin a supervised manner.
arxiv-12300-222 | A Neural Network Approach to Context-Sensitive Generation of Conversational Responses | http://arxiv.org/pdf/1506.06714v1.pdf | author:Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, Bill Dolan category:cs.CL cs.AI cs.LG cs.NE published:2015-06-22 summary:We present a novel response generation system that can be trained end to endon large quantities of unstructured Twitter conversations. A neural networkarchitecture is used to address sparsity issues that arise when integratingcontextual information into classic statistical models, allowing the system totake into account previous dialog utterances. Our dynamic-context generativemodels show consistent gains over both context-sensitive andnon-context-sensitive Machine Translation and Information Retrieval baselines.
arxiv-12300-223 | Adaptive Digital Scan Variable Pixels | http://arxiv.org/pdf/1506.06681v1.pdf | author:Sherin Sugathan, Reshma Scaria, Alex Pappachen James category:cs.CV published:2015-06-22 summary:The square and rectangular shape of the pixels in the digital images forsensing and display purposes introduces several inaccuracies in therepresentation of digital images. The major disadvantage of square pixel shapesis the inability to accurately capture and display the details in the objectshaving variable orientations to edges, shapes and regions. This effect can beobserved by the inaccurate representation of diagonal edges in low resolutionsquare pixel images. This paper explores a less investigated idea of usingvariable shaped pixels for improving visual quality of image scans withoutincreasing the square pixel resolution. The proposed adaptive filteringtechnique reports an improvement in image PSNR.
arxiv-12300-224 | Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books | http://arxiv.org/pdf/1506.06724v1.pdf | author:Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler category:cs.CV cs.CL published:2015-06-22 summary:Books are a rich source of both fine-grained information, how a character, anobject or a scene looks like, as well as high-level semantics, what someone isthinking, feeling and how these states evolve through a story. This paper aimsto align books to their movie releases in order to provide rich descriptiveexplanations for visual content that go semantically far beyond the captionsavailable in current datasets. To align movies and books we exploit a neuralsentence embedding that is trained in an unsupervised way from a large corpusof books, as well as a video-text neural embedding for computing similaritiesbetween movie clips and sentences in the book. We propose a context-aware CNNto combine information from multiple sources. We demonstrate good quantitativeperformance for movie/book alignment and show several qualitative examples thatshowcase the diversity of tasks our model can be used for.
arxiv-12300-225 | Skip-Thought Vectors | http://arxiv.org/pdf/1506.06726v1.pdf | author:Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler category:cs.CL cs.LG published:2015-06-22 summary:We describe an approach for unsupervised learning of a generic, distributedsentence encoder. Using the continuity of text from books, we train anencoder-decoder model that tries to reconstruct the surrounding sentences of anencoded passage. Sentences that share semantic and syntactic properties arethus mapped to similar vector representations. We next introduce a simplevocabulary expansion method to encode words that were not seen as part oftraining, allowing us to expand our vocabulary to a million words. Aftertraining our model, we extract and evaluate our vectors with linear models on 8tasks: semantic relatedness, paraphrase detection, image-sentence ranking,question-type classification and 4 benchmark sentiment and subjectivitydatasets. The end result is an off-the-shelf encoder that can produce highlygeneric sentence representations that are robust and perform well in practice.We will make our encoder publicly available.
arxiv-12300-226 | Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms | http://arxiv.org/pdf/1506.06438v2.pdf | author:Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher Ré category:cs.LG math.OC stat.ML published:2015-06-22 summary:Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety ofmachine learning problems. Researchers and industry have developed severaltechniques to optimize SGD's runtime performance, including asynchronousexecution and reduced precision. Our main result is a martingale-based analysisthat enables us to capture the rich noise models that may arise from suchtechniques. Specifically, we use our new analysis in three ways: (1) we deriveconvergence rates for the convex case (Hogwild!) with relaxed assumptions onthe sparsity of the problem; (2) we analyze asynchronous SGD algorithms fornon-convex matrix problems including matrix completion; and (3) we design andanalyze an asynchronous SGD algorithm, called Buckwild!, that useslower-precision arithmetic. We show experimentally that our algorithms runefficiently for a variety of problems on modern hardware.
arxiv-12300-227 | DeepStereo: Learning to Predict New Views from the World's Imagery | http://arxiv.org/pdf/1506.06825v1.pdf | author:John Flynn, Ivan Neulander, James Philbin, Noah Snavely category:cs.CV published:2015-06-22 summary:Deep networks have recently enjoyed enormous success when applied torecognition and classification problems in computer vision, but their use ingraphics problems has been limited. In this work, we present a novel deeparchitecture that performs new view synthesis directly from pixels, trainedfrom a large number of posed image sets. In contrast to traditional approacheswhich consist of multiple complex stages of processing, each of which requirecareful tuning and can fail in unexpected ways, our system is trainedend-to-end. The pixels from neighboring views of a scene are presented to thenetwork which then directly produces the pixels of the unseen view. Thebenefits of our approach include generality (we only require posed image setsand can easily apply our method to different domains), and high quality resultson traditionally difficult scenes. We believe this is due to the end-to-endnature of our system which is able to plausibly generate pixels according tocolor, depth, and texture priors learnt automatically from the training data.To verify our method we show that it can convincingly reproduce known testviews from nearby imagery. Additionally we show images rendered from novelviewpoints. To our knowledge, our work is the first to apply deep learning tothe problem of new view synthesis from sets of real-world, natural imagery.
arxiv-12300-228 | Target Tracking In Real Time Surveillance Cameras and Videos | http://arxiv.org/pdf/1506.06659v1.pdf | author:Nayyab Naseem, Mehreen Sirshar category:cs.CV published:2015-06-22 summary:Security concerns has been kept on increasing, so it is important foreveryone to keep their property safe from thefts and destruction. So the needfor surveillance techniques are also increasing. The system has been developedto detect the motion in a video. A system has been developed for real timeapplications by using the techniques of background subtraction and framedifferencing. In this system, motion is detected from the webcam or from thereal time video. Background subtraction and frames differencing method has beenused to detect the moving target. In background subtraction method, currentframe is subtracted from the referenced frame and then the threshold isapplied. If the difference is greater than the threshold then it is consideredas the pixel from the moving object, otherwise it is considered as backgroundpixel. Similarly, two frames difference method takes difference between twocontinuous frames. Then that resultant difference frame is thresholded and theamount of difference pixels is calculated.
arxiv-12300-229 | Multi-Modulus Algorithms Using Hyperbolic and Givens Rotations for MIMO Deconvolution | http://arxiv.org/pdf/1506.06650v1.pdf | author:Syed A. W. Shah, Karim Abed-Meraim, Tareq Y. Al-Naffouri category:cs.IT math.IT stat.ML published:2015-06-22 summary:This paper addresses the problem of blind multiple-input multiple-outputdeconvolution of a communication system. Two new iterative blind sourceseparation (BSS) algorithms are presented, based on the minimization ofmulti-modulus criterion. Further, we show that the design of algorithm in thecomplex domain is quite complicated, so a special structure of real filteringmatrix is suggested and maintained throughout the design. Then, a firstmulti-modulus algorithm based on data whitening and Givens rotations isproposed. An improved version of the latter is introduced for small samplesizes by combining Hyperbolic (Shear) with Givens rotations to compensate forthe ill whitening that occurs in this case. Proposed methods are finallycompared with several BSS algorithms in terms of signal-to-interference andnoise ratio, symbol error rate and convergence rate. Simulation results showthat the proposed methods outperform the contemporary BSS algorithms.
arxiv-12300-230 | Understanding Neural Networks Through Deep Visualization | http://arxiv.org/pdf/1506.06579v1.pdf | author:Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson category:cs.CV cs.LG cs.NE published:2015-06-22 summary:Recent years have produced great advances in training large, deep neuralnetworks (DNNs), including notable successes in training convolutional neuralnetworks (convnets) to recognize natural images. However, our understanding ofhow these models work, especially what computations they perform atintermediate layers, has lagged behind. Progress in the field will be furtheraccelerated by the development of better tools for visualizing and interpretingneural nets. We introduce two such tools here. The first is a tool thatvisualizes the activations produced on each layer of a trained convnet as itprocesses an image or video (e.g. a live webcam stream). We have found thatlooking at live activations that change in response to user input helps buildvaluable intuitions about how convnets work. The second tool enablesvisualizing features at each layer of a DNN via regularized optimization inimage space. Because previous versions of this idea produced less recognizableimages, here we introduce several new regularization methods that combine toproduce qualitatively clearer, more interpretable visualizations. Both toolsare open source and work on a pre-trained convnet with minimal setup.
arxiv-12300-231 | Distributional Sentence Entailment Using Density Matrices | http://arxiv.org/pdf/1506.06534v2.pdf | author:Esma Balkir, Mehrnoosh Sadrzadeh, Bob Coecke category:cs.CL cs.IT cs.LO math.CT math.IT published:2015-06-22 summary:Categorical compositional distributional model of Coecke et al. (2010)suggests a way to combine grammatical composition of the formal, type logicalmodels with the corpus based, empirical word representations of distributionalsemantics. This paper contributes to the project by expanding the model to alsocapture entailment relations. This is achieved by extending the representationsof words from points in meaning space to density operators, which areprobability distributions on the subspaces of the space. A symmetric measure ofsimilarity and an asymmetric measure of entailment is defined, where lexicalentailment is measured using von Neumann entropy, the quantum variant ofKullback-Leibler divergence. Lexical entailment, combined with the compositionmap on word representations, provides a method to obtain entailment relationson the level of sentences. Truth theoretic and corpus-based examples areprovided.
arxiv-12300-232 | Modality-dependent Cross-media Retrieval | http://arxiv.org/pdf/1506.06628v2.pdf | author:Yunchao Wei, Yao Zhao, Zhenfeng Zhu, Shikui Wei, Yanhui Xiao, Jiashi Feng, Shuicheng Yan category:cs.CV cs.IR cs.LG published:2015-06-22 summary:In this paper, we investigate the cross-media retrieval between images andtext, i.e., using image to search text (I2T) and using text to search images(T2I). Existing cross-media retrieval methods usually learn one couple ofprojections, by which the original features of images and text can be projectedinto a common latent space to measure the content similarity. However, usingthe same projections for the two different retrieval tasks (I2T and T2I) maylead to a tradeoff between their respective performances, rather than theirbest performances. Different from previous works, we propose amodality-dependent cross-media retrieval (MDCR) model, where two couples ofprojections are learned for different cross-media retrieval tasks instead ofone couple of projections. Specifically, by jointly optimizing the correlationbetween images and text and the linear regression from one modal space (imageor text) to the semantic space, two couples of mappings are learned to projectimages and text from their original feature spaces into two common latentsubspaces (one for I2T and the other for T2I). Extensive experiments show thesuperiority of the proposed MDCR compared with other methods. In particular,based the 4,096 dimensional convolutional neural network (CNN) visual featureand 100 dimensional LDA textual feature, the mAP of the proposed methodachieves 41.5\%, which is a new state-of-the-art performance on the Wikipediadataset.
arxiv-12300-233 | PAC-Bayes Iterated Logarithm Bounds for Martingale Mixtures | http://arxiv.org/pdf/1506.06573v1.pdf | author:Akshay Balsubramani category:cs.LG math.PR stat.ML published:2015-06-22 summary:We give tight concentration bounds for mixtures of martingales that aresimultaneously uniform over (a) mixture distributions, in a PAC-Bayes sense;and (b) all finite times. These bounds are proved in terms of the martingalevariance, extending classical Bernstein inequalities, and sharpening andsimplifying prior work.
arxiv-12300-234 | Non-Normal Mixtures of Experts | http://arxiv.org/pdf/1506.06707v2.pdf | author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML published:2015-06-22 summary:Mixture of Experts (MoE) is a popular framework for modeling heterogeneity indata for regression, classification and clustering. For continuous data whichwe consider here in the context of regression and cluster analysis, MoE usuallyuse normal experts, that is, expert components following the Gaussiandistribution. However, for a set of data containing a group or groups ofobservations with asymmetric behavior, heavy tails or atypical observations,the use of normal experts may be unsuitable and can unduly affect the fit ofthe MoE model. In this paper, we introduce new non-normal mixture of experts(NNMoE) which can deal with these issues regarding possibly skewed,heavy-tailed data and with outliers. The proposed models are the skew-normalMoE and the robust $t$ MoE and skew $t$ MoE, respectively named SNMoE, TMoE andSTMoE. We develop dedicated expectation-maximization (EM) and expectationconditional maximization (ECM) algorithms to estimate the parameters of theproposed models by monotonically maximizing the observed data log-likelihood.We describe how the presented models can be used in prediction and inmodel-based clustering of regression data. Numerical experiments carried out onsimulated data show the effectiveness and the robustness of the proposed modelsin terms modeling non-linear regression functions as well as in model-basedclustering. Then, to show their usefulness for practical applications, theproposed models are applied to the real-world data of tone perception formusical data analysis, and the one of temperature anomalies for the analysis ofclimate change data.
arxiv-12300-235 | Answer Sequence Learning with Neural Networks for Answer Selection in Community Question Answering | http://arxiv.org/pdf/1506.06490v1.pdf | author:Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou Tang, Xiaolong Wang category:cs.CL cs.IR cs.LG published:2015-06-22 summary:In this paper, the answer selection problem in community question answering(CQA) is regarded as an answer sequence labeling task, and a novel approach isproposed based on the recurrent architecture for this problem. Our approachapplies convolution neural networks (CNNs) to learning the joint representationof question-answer pair firstly, and then uses the joint representation asinput of the long short-term memory (LSTM) to learn the answer sequence of aquestion for labeling the matching quality of each answer. Experimentsconducted on the SemEval 2015 CQA dataset shows the effectiveness of ourapproach.
arxiv-12300-236 | A Deep Memory-based Architecture for Sequence-to-Sequence Learning | http://arxiv.org/pdf/1506.06442v4.pdf | author:Fandong Meng, Zhengdong Lu, Zhaopeng Tu, Hang Li, Qun Liu category:cs.CL cs.LG cs.NE published:2015-06-22 summary:We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequencelearning, which performs the task through a series of nonlinear transformationsfrom the representation of the input sequence (e.g., a Chinese sentence) to thefinal output sequence (e.g., translation to English). Inspired by the recentlyproposed Neural Turing Machine (Graves et al., 2014), we store the intermediaterepresentations in stacked layers of memories, and use read-write operations onthe memories to realize the nonlinear transformations between therepresentations. The types of transformations are designed in advance but theparameters are learned from data. Through layer-by-layer transformations,DEEPMEMORY can model complicated relations between sequences necessary forapplications such as machine translation between distant languages. Thearchitecture can be trained with normal back-propagation on sequenceto-sequencedata, and the learning can be easily scaled up to a large corpus. DEEPMEMORY isbroad enough to subsume the state-of-the-art neural translation model in(Bahdanau et al., 2015) as its special case, while significantly improving uponthe model with its deeper architecture. Remarkably, DEEPMEMORY, being purelyneural network-based, can achieve performance comparable to the traditionalphrase-based machine translation system Moses with a small vocabulary and amodest parameter size.
arxiv-12300-237 | When slower is faster | http://arxiv.org/pdf/1506.06796v2.pdf | author:Carlos Gershenson, Dirk Helbing category:nlin.AO cs.NE physics.soc-ph q-bio.QM published:2015-06-22 summary:The slower is faster (SIF) effect occurs when a system performs worse as itscomponents try to do better. Thus, a moderate individual efficiency actuallyleads to a better systemic performance. The SIF effect takes place in a varietyof phenomena. We review studies and examples of the SIF effect in pedestriandynamics, vehicle traffic, traffic light control, logistics, public transport,social dynamics, ecological systems, and adaptation. Drawing on these examples,we generalize common features of the SIF effect and suggest possible futurelines of research.
arxiv-12300-238 | The Ebb and Flow of Deep Learning: a Theory of Local Learning | http://arxiv.org/pdf/1506.06472v1.pdf | author:Pierre Baldi, Peter Sadowski category:cs.LG cs.NE stat.ML published:2015-06-22 summary:In a physical neural system, where storage and processing are intimatelyintertwined, the rules for adjusting the synaptic weights can only depend onvariables that are available locally, such as the activity of the pre- andpost-synaptic neurons, resulting in local learning rules. A systematicframework for studying the space of local learning rules must first define thenature of the local variables, and then the functional form that ties themtogether into each learning rule. We consider polynomial local learning rulesand analyze their behavior and capabilities in both linear and non-linearnetworks. As a byproduct, this framework enables also the discovery of newlearning rules as well as important relationships between learning rules andgroup symmetries. Stacking local learning rules in deep feedforward networksleads to deep local learning. While deep local learning can learn interestingrepresentations, it cannot learn complex input-output functions, even whentargets are available for the top layer. Learning complex input-outputfunctions requires local deep learning where target information is propagatedto the deep layers through a backward channel. The nature of the propagatedinformation about the targets, and the backward channel through which thisinformation is propagated, partition the space of learning algorithms. For anylearning algorithm, the capacity of the backward channel can be defined as thenumber of bits provided about the gradient per weight, divided by the number ofrequired operations per weight. We estimate the capacity associated withseveral learning algorithms and show that backpropagation outperforms them andachieves the maximum possible capacity. The theory clarifies the concept ofHebbian learning, what is learnable by Hebbian learning, and explains thesparsity of the space of learning rules discovered so far.
arxiv-12300-239 | DeepOrgan: Multi-level Deep Convolutional Networks for Automated Pancreas Segmentation | http://arxiv.org/pdf/1506.06448v1.pdf | author:Holger R. Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin Liu, Evrim Turkbey, Ronald M. Summers category:cs.CV published:2015-06-22 summary:Automatic organ segmentation is an important yet challenging problem formedical image analysis. The pancreas is an abdominal organ with very highanatomical variability. This inhibits previous segmentation methods fromachieving high accuracies, especially compared to other organs such as theliver, heart or kidneys. In this paper, we present a probabilistic bottom-upapproach for pancreas segmentation in abdominal computed tomography (CT) scans,using multi-level deep convolutional networks (ConvNets). We propose andevaluate several variations of deep ConvNets in the context of hierarchical,coarse-to-fine classification on image patches and regions, i.e. superpixels.We first present a dense labeling of local image patches via$P{-}\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regionalConvNet ($R_1{-}\mathrm{ConvNet}$) that samples a set of bounding boxes aroundeach image superpixel at different scales of contexts in a "zoom-out" fashion.Our ConvNets learn to assign class probabilities for each superpixel region ofbeing pancreas. Last, we study a stacked $R_2{-}\mathrm{ConvNet}$ leveragingthe joint space of CT intensities and the $P{-}\mathrm{ConvNet}$ denseprobability maps. Both 3D Gaussian smoothing and 2D conditional random fieldsare exploited as structured predictions for post-processing. We evaluate on CTimages of 82 patients in 4-fold cross-validation. We achieve a Dice SimilarityCoefficient of 83.6$\pm$6.3% in training and 71.8$\pm$10.7% in testing.
arxiv-12300-240 | Extreme Extraction: Only One Hour per Relation | http://arxiv.org/pdf/1506.06418v1.pdf | author:Raphael Hoffmann, Luke Zettlemoyer, Daniel S. Weld category:cs.CL cs.AI cs.IR published:2015-06-21 summary:Information Extraction (IE) aims to automatically generate a large knowledgebase from natural language text, but progress remains slow. Supervised learningrequires copious human annotation, while unsupervised and weakly supervisedapproaches do not deliver competitive accuracy. As a result, most fieldedapplications of IE, as well as the leading TAC-KBP systems, rely on significantamounts of manual engineering. Even "Extreme" methods, such as those reportedin Freedman et al. 2011, require about 10 hours of expert labor per relation. This paper shows how to reduce that effort by an order of magnitude. Wepresent a novel system, InstaRead, that streamlines authoring with an ensembleof methods: 1) encoding extraction rules in an expressive and compositionalrepresentation, 2) guiding the user to promising rules based on corpusstatistics and mined resources, and 3) introducing a new interactivedevelopment cycle that provides immediate feedback --- even on large datasets.Experiments show that experts can create quality extractors in under an hourand even NLP novices can author good extractors. These extractors equal oroutperform ones obtained by comparably supervised and state-of-the-artdistantly supervised approaches.
arxiv-12300-241 | A Novel Method for Stock Forecasting based on Fuzzy Time Series Combined with the Longest Common/Repeated Sub-sequence | http://arxiv.org/pdf/1506.06366v1.pdf | author:He-Wen Chen, Zih-Ci Wang, Shu-Yu Kuo, Yao-Hsin Chou category:cs.CE cs.AI cs.NE published:2015-06-21 summary:Stock price forecasting is an important issue for investors since extremeaccuracy in forecasting can bring about high profits. Fuzzy Time Series (FTS)and Longest Common/Repeated Sub-sequence (LCS/LRS) are two important issues forforecasting prices. However, to the best of our knowledge, there are nosignificant studies using LCS/LRS to predict stock prices. It is impossiblethat prices stay exactly the same as historic prices. Therefore, this paperproposes a state-of-the-art method which combines FTS and LCS/LRS to predictstock prices. This method is based on the principle that history will repeatitself. It uses different interval lengths in FTS to fuzzify the prices, andLCS/LRS to look for the same pattern in the historical prices to predict futurestock prices. In the experiment, we examine various intervals of fuzzy timesets in order to achieve high prediction accuracy. The proposed methodoutperforms traditional methods in terms of prediction accuracy and,furthermore, it is easy to implement.
arxiv-12300-242 | Communication Efficient Distributed Agnostic Boosting | http://arxiv.org/pdf/1506.06318v1.pdf | author:Shang-Tse Chen, Maria-Florina Balcan, Duen Horng Chau category:cs.LG stat.ML published:2015-06-21 summary:We consider the problem of learning from distributed data in the agnosticsetting, i.e., in the presence of arbitrary forms of noise. Our maincontribution is a general distributed boosting-based procedure for learning anarbitrary concept space, that is simultaneously noise tolerant, communicationefficient, and computationally efficient. This improves significantly overprior works that were either communication efficient only in noise-freescenarios or computationally prohibitive. Empirical results on large syntheticand real-world datasets demonstrate the effectiveness and scalability of theproposed approach.
arxiv-12300-243 | Beyond Hartigan Consistency: Merge Distortion Metric for Hierarchical Clustering | http://arxiv.org/pdf/1506.06422v2.pdf | author:Justin Eldridge, Mikhail Belkin, Yusu Wang category:stat.ML math.ST stat.TH published:2015-06-21 summary:Hierarchical clustering is a popular method for analyzing data whichassociates a tree to a dataset. Hartigan consistency has been used extensivelyas a framework to analyze such clustering algorithms from a statistical pointof view. Still, as we show in the paper, a tree which is Hartigan consistentwith a given density can look very different than the correct limit tree.Specifically, Hartigan consistency permits two types of undesirableconfigurations which we term over-segmentation and improper nesting. Moreover,Hartigan consistency is a limit property and does not directly quantifydifference between trees. In this paper we identify two limit properties, separation and minimality,which address both over-segmentation and improper nesting and together imply(but are not implied by) Hartigan consistency. We proceed to introduce a mergedistortion metric between hierarchical clusterings and show that convergence inour distance implies both separation and minimality. We also prove that uniformseparation and minimality imply convergence in the merge distortion metric.Furthermore, we show that our merge distortion metric is stable underperturbations of the density. Finally, we demonstrate applicability of these concepts by provingconvergence results for two clustering algorithms. First, we show convergence(and hence separation and minimality) of the recent robust single linkagealgorithm of Chaudhuri and Dasgupta (2010). Second, we provide convergenceresults on manifolds for topological split tree clustering.
arxiv-12300-244 | Mining Mid-level Visual Patterns with Deep CNN Activations | http://arxiv.org/pdf/1506.06343v2.pdf | author:Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV published:2015-06-21 summary:The purpose of mid-level visual element discovery is to find clusters ofimage patches that are both representative and discriminative. Here we studythis problem from the prospective of pattern mining while relying on therecently popularized Convolutional Neural Networks (CNNs). We observe that afully-connected CNN activation extracted from an image patch typicallypossesses two appealing properties that enable its seamless integration withpattern mining techniques. The marriage between CNN activations and associationrule mining, a well-known pattern mining technique in the literature, leads tofast and effective discovery of representative and discriminative patterns froma huge number of image patches. When we retrieve and visualize image patcheswith the same pattern, surprisingly, they are not only visually similar butalso semantically consistent, and thus give rise to a mid-level visual elementin our work. Given the patterns and retrieved mid-level visual elements, wepropose two methods to generate image feature representations for each. Thefirst method is to use the patterns as codewords in a dictionary, similar tothe Bag-of-Visual-Words model, we compute a Bag-of-Patterns representation. Thesecond one relies on the retrieved mid-level visual elements to construct aBag-of-Elements representation. We evaluate the two encoding methods on sceneand object classification tasks, and demonstrate that our approach outperformsor matches recent works using CNN activations for these tasks.
arxiv-12300-245 | Collective Mind, Part II: Towards Performance- and Cost-Aware Software Engineering as a Natural Science | http://arxiv.org/pdf/1506.06256v1.pdf | author:Grigori Fursin, Abdul Memon, Christophe Guillon, Anton Lokhmotov category:cs.SE cs.LG cs.PF published:2015-06-20 summary:Nowadays, engineers have to develop software often without even knowing whichhardware it will eventually run on in numerous mobile phones, tablets,desktops, laptops, data centers, supercomputers and cloud services.Unfortunately, optimizing compilers are not keeping pace with ever increasingcomplexity of computer systems anymore and may produce severely underperformingexecutable codes while wasting expensive resources and energy. We present our practical and collaborative solution to this problem vialight-weight wrappers around any software piece when more than oneimplementation or optimization choice available. These wrappers are connectedwith a public Collective Mind autotuning infrastructure and repository ofknowledge (c-mind.org/repo) to continuously monitor various importantcharacteristics of these pieces (computational species) across numerousexisting hardware configurations together with randomly selected optimizations.Similar to natural sciences, we can now continuously track winning solutions(optimizations for a given hardware) that minimize all costs of a computation(execution time, energy spent, code size, failures, memory and storagefootprint, optimization time, faults, contentions, inaccuracy and so on) of agiven species on a Pareto frontier along with any unexpected behavior. Thecommunity can then collaboratively classify solutions, prune redundant ones,and correlate them with various features of software, its inputs (data sets)and used hardware either manually or using powerful predictive analyticstechniques. Our approach can then help create a large, realistic, diverse,representative, and continuously evolving benchmark with related optimizationknowledge while gradually covering all possible software and hardware to beable to predict best optimizations and improve compilers and hardware dependingon usage scenarios and requirements.
arxiv-12300-246 | Aligning where to see and what to tell: image caption with region-based attention and scene factorization | http://arxiv.org/pdf/1506.06272v1.pdf | author:Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha, Changshui Zhang category:cs.CV cs.LG stat.ML published:2015-06-20 summary:Recent progress on automatic generation of image captions has shown that itis possible to describe the most salient information conveyed by images withaccurate and meaningful sentences. In this paper, we propose an image captionsystem that exploits the parallel structures between images and sentences. Inour model, the process of generating the next word, given the previouslygenerated ones, is aligned with the visual perception experience where theattention shifting among the visual regions imposes a thread of visualordering. This alignment characterizes the flow of "abstract meaning", encodingwhat is semantically shared by both the visual scene and the text description.Our system also makes another novel modeling contribution by introducingscene-specific contexts that capture higher-level semantic information encodedin an image. The contexts adapt language models for word generation to specificscene types. We benchmark our system and contrast to published results onseveral popular datasets. We show that using either region-based attention orscene-specific contexts improves systems without those components. Furthermore,combining these two modeling ingredients attains the state-of-the-artperformance.
arxiv-12300-247 | 3D Reconstruction from Full-view Fisheye Camera | http://arxiv.org/pdf/1506.06273v1.pdf | author:Chuiwen Ma, Liang Shi, Hanlu Huang, Mengyuan Yan category:cs.CV published:2015-06-20 summary:In this report, we proposed a 3D reconstruction method for the full-viewfisheye camera. The camera we used is Ricoh Theta, which captures sphericalimages and has a wide field of view (FOV). The conventional stereo apporachbased on perspective camera model cannot be directly applied and instead weused a spherical camera model to depict the relation between 3D point and itscorresponding observation in the image. We implemented a system that canreconstruct the 3D scene using captures from two or more cameras. A GUI is alsocreated to allow users to control the view perspective and obtain a betterintuition of how the scene is rebuilt. Experiments showed that ourreconstruction results well preserved the structure of the scene in the realworld.
arxiv-12300-248 | Filtrated Algebraic Subspace Clustering | http://arxiv.org/pdf/1506.06289v4.pdf | author:Manolis C. Tsakiris, Rene Vidal category:cs.CV published:2015-06-20 summary:Subspace clustering is the problem of clustering data that lie close to aunion of linear subspaces. In the abstract form of the problem, where no noiseor other corruptions are present, the data are assumed to lie in generalposition inside the algebraic variety of a union of subspaces, and theobjective is to decompose the variety into its constituent subspaces. Prioralgebraic-geometric approaches to this problem require the subspaces to be ofequal dimension, or the number of subspaces to be known. Subspaces of arbitrarydimensions can still be recovered in closed form, in terms of all homogeneouspolynomials of degree $m$ that vanish on their union, when an upper bound m onthe number of the subspaces is given. In this paper, we propose an alternative,provably correct, algorithm for addressing a union of at most $m$arbitrary-dimensional subspaces, based on the idea of descending filtrations ofsubspace arrangements. Our algorithm uses the gradient of a vanishingpolynomial at a point in the variety to find a hyperplane containing thesubspace S passing through that point. By intersecting the variety with thishyperplane, we obtain a subvariety that contains S, and recursively applyingthe procedure until no non-trivial vanishing polynomial exists, our algorithmeventually identifies S. By repeating this procedure for other points, ouralgorithm eventually identifies all the subspaces by returning a basis fortheir orthogonal complement. Finally, we develop a variant of the abstractalgorithm, suitable for computations with noisy data. We show by experiments onsynthetic and real data that the proposed algorithm outperformsstate-of-the-art methods on several occasions, thus demonstrating the merit ofthe idea of filtrations.
arxiv-12300-249 | Pose Estimation Based on 3D Models | http://arxiv.org/pdf/1506.06274v1.pdf | author:Chuiwen Ma, Hao Su, Liang Shi category:cs.CV cs.LG cs.RO published:2015-06-20 summary:In this paper, we proposed a pose estimation system based on rendered imagetraining set, which predicts the pose of objects in real image, with knowledgeof object category and tight bounding box. We developed a patch-basedmulti-class classification algorithm, and an iterative approach to improve theaccuracy. We achieved state-of-the-art performance on pose estimation task.
arxiv-12300-250 | Learning to Segment Object Candidates | http://arxiv.org/pdf/1506.06204v2.pdf | author:Pedro O. Pinheiro, Ronan Collobert, Piotr Dollar category:cs.CV published:2015-06-20 summary:Recent object detection systems rely on two critical steps: (1) a set ofobject proposals is predicted as efficiently as possible, and (2) this set ofcandidate proposals is then passed to an object classifier. Such approacheshave been shown they can be fast, while achieving the state of the art indetection performance. In this paper, we propose a new way to generate objectproposals, introducing an approach based on a discriminative convolutionalnetwork. Our model is trained jointly with two objectives: given an imagepatch, the first part of the system outputs a class-agnostic segmentation mask,while the second part of the system outputs the likelihood of the patch beingcentered on a full object. At test time, the model is efficiently applied onthe whole test image and generates a set of segmentation masks, each of thembeing assigned with a corresponding object likelihood score. We show that ourmodel yields significant improvements over state-of-the-art object proposalalgorithms. In particular, compared to previous approaches, our model obtainssubstantially higher object recall using fewer proposals. We also show that ourmodel is able to generalize to unseen categories it has not seen duringtraining. Unlike all previous approaches for generating object masks, we do notrely on edges, superpixels, or any other form of low-level segmentation.
arxiv-12300-251 | Detectability thresholds and optimal algorithms for community structure in dynamic networks | http://arxiv.org/pdf/1506.06179v1.pdf | author:Amir Ghasemian, Pan Zhang, Aaron Clauset, Cristopher Moore, Leto Peel category:stat.ML cs.LG cs.SI published:2015-06-19 summary:We study the fundamental limits on learning latent community structure indynamic networks. Specifically, we study dynamic stochastic block models wherenodes change their community membership over time, but where edges aregenerated independently at each time step. In this setting (which is a specialcase of several existing models), we are able to derive the detectabilitythreshold exactly, as a function of the rate of change and the strength of thecommunities. Below this threshold, we claim that no algorithm can identify thecommunities better than chance. We then give two algorithms that are optimal inthe sense that they succeed all the way down to this limit. The first usesbelief propagation (BP), which gives asymptotically optimal accuracy, and thesecond is a fast spectral clustering algorithm, based on linearizing the BPequations. We verify our analytic and algorithmic results via numericalsimulation, and close with a brief discussion of extensions and open questions.
arxiv-12300-252 | A simple application of FIC to model selection | http://arxiv.org/pdf/1506.06129v1.pdf | author:Paul A. Wiggins category:cs.LG stat.ML published:2015-06-19 summary:We have recently proposed a new information-based approach to modelselection, the Frequentist Information Criterion (FIC), that reconcilesinformation-based and frequentist inference. The purpose of this current paperis to provide a simple example of the application of this criterion and ademonstration of the natural emergence of model complexities with both AIC-like($N^0$) and BIC-like ($\log N$) scaling with observation number $N$. Theapplication developed is deliberately simplified to make the analysisanalytically tractable.
arxiv-12300-253 | To Know Where We Are: Vision-Based Positioning in Outdoor Environments | http://arxiv.org/pdf/1506.05870v1.pdf | author:Kuan-Wen Chen, Chun-Hsin Wang, Xiao Wei, Qiao Liang, Ming-Hsuan Yang, Chu-Song Chen, Yi-Ping Hung category:cs.CV published:2015-06-19 summary:Augmented reality (AR) displays become more and more popular recently,because of its high intuitiveness for humans and high-quality head-mounteddisplay have rapidly developed. To achieve such displays with augmentedinformation, highly accurate image registration or ego-positioning arerequired, but little attention have been paid for out-door environments. Thispaper presents a method for ego-positioning in outdoor environments with lowcost monocular cameras. To reduce the computational and memory requirements aswell as the communication overheads, we formulate the model compressionalgorithm as a weighted k-cover problem for better preserving model structures.Specifically for real-world vision-based positioning applications, we considerthe issues with large scene change and propose a model update algorithm totackle these problems. A long- term positioning dataset with more than onemonth, 106 sessions, and 14,275 images is constructed. Based on both local andup-to-date models constructed in our approach, extensive experimental resultsshow that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can beachieved, which outperforms existing vision-based algorithms.
arxiv-12300-254 | Representation Learning for Clustering: A Statistical Framework | http://arxiv.org/pdf/1506.05900v1.pdf | author:Hassan Ashtiani, Shai Ben-David category:stat.ML cs.LG published:2015-06-19 summary:We address the problem of communicating domain knowledge from a user to thedesigner of a clustering algorithm. We propose a protocol in which the userprovides a clustering of a relatively small random sample of a data set. Thealgorithm designer then uses that sample to come up with a data representationunder which $k$-means clustering results in a clustering (of the full data set)that is aligned with the user's clustering. We provide a formal statisticalmodel for analyzing the sample complexity of learning a clusteringrepresentation with this paradigm. We then introduce a notion of capacity of aclass of possible representations, in the spirit of the VC-dimension, showingthat classes of representations that have finite such dimension can besuccessfully learned with sample size error bounds, and end our discussion withan analysis of that dimension for classes of representations induced by linearembeddings.
arxiv-12300-255 | Doubly Decomposing Nonparametric Tensor Regression | http://arxiv.org/pdf/1506.05967v3.pdf | author:Masaaki Imaizumi, Kohei Hayashi category:stat.ML published:2015-06-19 summary:Nonparametric extension of tensor regression is proposed. Nonlinearity in ahigh-dimensional tensor space is broken into simple local functions byincorporating low-rank tensor decomposition. Compared to naive nonparametricapproaches, our formulation considerably improves the convergence rate ofestimation while maintaining consistency with the same function class underspecific conditions. To estimate local functions, we develop a Bayesianestimator with the Gaussian process prior. Experimental results show itstheoretical properties and high performance in terms of predicting a summarystatistic of a real complex network.
arxiv-12300-256 | Deep Knowledge Tracing | http://arxiv.org/pdf/1506.05908v1.pdf | author:Chris Piech, Jonathan Spencer, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas Guibas, Jascha Sohl-Dickstein category:cs.AI cs.CY cs.LG K.3.1 published:2015-06-19 summary:Knowledge tracing---where a machine models the knowledge of a student as theyinteract with coursework---is a well established problem in computer supportededucation. Though effectively modeling student knowledge would have higheducational impact, the task has many inherent challenges. In this paper weexplore the utility of using Recurrent Neural Networks (RNNs) to model studentlearning. The RNN family of models have important advantages over previousmethods in that they do not require the explicit encoding of human domainknowledge, and can capture more complex representations of student knowledge.Using neural networks results in substantial improvements in predictionperformance on a range of knowledge tracing datasets. Moreover the learnedmodel can be used for intelligent curriculum design and allows straightforwardinterpretation and discovery of structure in student tasks. These resultssuggest a promising new line of research for knowledge tracing and an exemplaryapplication task for RNNs.
arxiv-12300-257 | A Neural Conversational Model | http://arxiv.org/pdf/1506.05869v3.pdf | author:Oriol Vinyals, Quoc Le category:cs.CL published:2015-06-19 summary:Conversational modeling is an important task in natural languageunderstanding and machine intelligence. Although previous approaches exist,they are often restricted to specific domains (e.g., booking an airline ticket)and require hand-crafted rules. In this paper, we present a simple approach forthis task which uses the recently proposed sequence to sequence framework. Ourmodel converses by predicting the next sentence given the previous sentence orsentences in a conversation. The strength of our model is that it can betrained end-to-end and thus requires much fewer hand-crafted rules. We findthat this straightforward model can generate simple conversations given a largeconversational training dataset. Our preliminary results suggest that, despiteoptimizing the wrong objective function, the model is able to converse well. Itis able extract knowledge from both a domain specific dataset, and from alarge, noisy, and general domain dataset of movie subtitles. On adomain-specific IT helpdesk dataset, the model can find a solution to atechnical problem via conversations. On a noisy open-domain movie transcriptdataset, the model can perform simple forms of common sense reasoning. Asexpected, we also find that the lack of consistency is a common failure mode ofour model.
arxiv-12300-258 | Enhanced Lasso Recovery on Graph | http://arxiv.org/pdf/1506.05985v1.pdf | author:Xavier Bresson, Thomas Laurent, James von Brecht category:cs.LG stat.ML published:2015-06-19 summary:This work aims at recovering signals that are sparse on graphs. Compressedsensing offers techniques for signal recovery from a few linear measurementsand graph Fourier analysis provides a signal representation on graph. In thispaper, we leverage these two frameworks to introduce a new Lasso recoveryalgorithm on graphs. More precisely, we present a non-convex, non-smoothalgorithm that outperforms the standard convex Lasso technique. We carry outnumerical experiments on three benchmark graph datasets.
arxiv-12300-259 | Structured Training for Neural Network Transition-Based Parsing | http://arxiv.org/pdf/1506.06158v1.pdf | author:David Weiss, Chris Alberti, Michael Collins, Slav Petrov category:cs.CL published:2015-06-19 summary:We present structured perceptron training for neural network transition-baseddependency parsing. We learn the neural network representation using a goldcorpus augmented by a large number of automatically parsed sentences. Giventhis fixed network representation, we learn a final layer using the structuredperceptron with beam-search decoding. On the Penn Treebank, our parser reaches94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledgeis the best accuracy on Stanford Dependencies to date. We also provide in-depthablative analysis to determine which aspects of our model provide the largestgains in accuracy.
arxiv-12300-260 | Solving Problems with Unknown Solution Length at (Almost) No Extra Cost | http://arxiv.org/pdf/1506.05913v1.pdf | author:Benjamin Doerr, Carola Doerr, Timo Kötzing category:cs.NE published:2015-06-19 summary:Most research in the theory of evolutionary computation assumes that theproblem at hand has a fixed problem size. This assumption does not always applyto real-world optimization challenges, where the length of an optimal solutionmay be unknown a priori. Following up on previous work of Cathabard, Lehre, and Yao [FOGA 2011] weanalyze variants of the (1+1) evolutionary algorithm for problems with unknownsolution length. For their setting, in which the solution length is sampledfrom a geometric distribution, we provide mutation rates that yield an expectedoptimization time that is of the same order as that of the (1+1) EA knowing thesolution length. We then show that almost the same run times can be achieved even if \emph{no}a priori information on the solution length is available. Finally, we provide mutation rates suitable for settings in which neither thesolution length nor the positions of the relevant bits are known. Again weobtain almost optimal run times for the \textsc{OneMax} and\textsc{LeadingOnes} test functions, thus solving an open problem fromCathabard et al.
arxiv-12300-261 | New Descriptor for Glomerulus Detection in Kidney Microscopy Image | http://arxiv.org/pdf/1506.05920v1.pdf | author:Tsuyoshi Kato, Raissa Relator, Hayliang Ngouv, Yoshihiro Hirohashi, Tetsuhiro Kakimoto, Kinya Okada category:cs.CV published:2015-06-19 summary:Glomerulus detection is a key step in histopathological evaluation ofmicroscopy images of kidneys. However, the task of automatic detection ofglomeruli poses challenges due to the disparity in sizes and shapes ofglomeruli in renal sections. Moreover, extensive variations of theirintensities due to heterogeneity in immunohistochemistry staining are alsoencountered. Despite being widely recognized as a powerful descriptor forgeneral object detection, the rectangular histogram of oriented gradients(Rectangular HOG) suffers from many false positives due to the aforementioneddifficulties in the context of glomerulus detection. A new descriptor referred to as Segmental HOG is developed to perform acomprehensive detection of hundreds of glomeruli in images of whole kidneysections. The new descriptor possesses flexible blocks that can be adaptivelyfitted to input images to acquire robustness to deformations of glomeruli.Moreover, the novel segmentation technique employed herewith generates highquality segmentation outputs and the algorithm is assured to converge to anoptimal solution. Consequently, experiments using real world image data revealthat Segmental HOG achieves significant improvements in detection performancecompared to Rectangular HOG. The proposed descriptor and method for glomeruli detection present promisingresults and is expected to be useful in pathological evaluation.
arxiv-12300-262 | Exploring the influence of scale on artist attribution | http://arxiv.org/pdf/1506.05929v1.pdf | author:Nanne van Noord, Eric Postma category:cs.CV published:2015-06-19 summary:Previous work has shown that the artist of an artwork can be identified byuse of computational methods that analyse digital images. However, thedigitised artworks are often investigated at a coarse scale discarding many ofthe important details that may define an artist's style. In recent years highresolution images of artworks have become available, which, combined withincreased processing power and new computational techniques, allow us toanalyse digital images of artworks at a very fine scale. In this work we trainand evaluate a Convolutional Neural Network (CNN) on the task of artistattribution using artwork images of varying resolutions. To this end, wecombine two existing methods to enable the application of high resolutionimages to CNNs. By comparing the attribution performances obtained at differentscales, we find that in most cases finer scales are beneficial to theattribution performance, whereas for a minority of the artists, coarser scalesappear to be preferable. We conclude that artist attribution would benefit froma multi-scale CNN approach which vastly expands the possibilities forcomputational art forensics.
arxiv-12300-263 | Approximate Inference with the Variational Holder Bound | http://arxiv.org/pdf/1506.06100v1.pdf | author:Guillaume Bouchard, Balaji Lakshminarayanan category:stat.ML cs.LG math.FA published:2015-06-19 summary:We introduce the Variational Holder (VH) bound as an alternative toVariational Bayes (VB) for approximate Bayesian inference. Unlike VB whichtypically involves maximization of a non-convex lower bound with respect to thevariational parameters, the VH bound involves minimization of a convex upperbound to the intractable integral with respect to the variational parameters.Minimization of the VH bound is a convex optimization problem; hence the VHmethod can be applied using off-the-shelf convex optimization algorithms andthe approximation error of the VH bound can also be analyzed using tools fromconvex optimization literature. We present experiments on the task ofintegrating a truncated multivariate Gaussian distribution and compare ourmethod to VB, EP and a state-of-the-art numerical integration method for thisproblem.
arxiv-12300-264 | Expectation Particle Belief Propagation | http://arxiv.org/pdf/1506.05934v1.pdf | author:Thibaut Lienart, Yee Whye Teh, Arnaud Doucet category:stat.CO cs.AI stat.ML published:2015-06-19 summary:We propose an original particle-based implementation of the Loopy BeliefPropagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on acontinuous state space. The algorithm constructs adaptively efficient proposaldistributions approximating the local beliefs at each note of the MRF. This isachieved by considering proposal distributions in the exponential family whoseparameters are updated iterately in an Expectation Propagation (EP) framework.The proposed particle scheme provides consistent estimation of the LBPmarginals as the number of particles increases. We demonstrate that it providesmore accurate results than the Particle Belief Propagation (PBP) algorithm ofIhler and McAllester (2009) at a fraction of the computational cost and isadditionally more robust empirically. The computational complexity of ouralgorithm at each iteration is quadratic in the number of particles. We alsopropose an accelerated implementation with sub-quadratic computationalcomplexity which still provides consistent estimates of the loopy BP marginaldistributions and performs almost as well as the original procedure.
arxiv-12300-265 | Sampling constrained probability distributions using Spherical Augmentation | http://arxiv.org/pdf/1506.05936v1.pdf | author:Shiwei Lan, Babak Shahbaba category:stat.CO stat.ML published:2015-06-19 summary:Statistical models with constrained probability distributions are abundant inmachine learning. Some examples include regression models with norm constraints(e.g., Lasso), probit, many copula models, and latent Dirichlet allocation(LDA). Bayesian inference involving probability distributions confined toconstrained domains could be quite challenging for commonly used samplingalgorithms. In this paper, we propose a novel augmentation technique thathandles a wide range of constraints by mapping the constrained domain to asphere in the augmented space. By moving freely on the surface of this sphere,sampling algorithms handle constraints implicitly and generate proposals thatremain within boundaries when mapped back to the original space. Our proposedmethod, called {Spherical Augmentation}, provides a mathematically natural andcomputationally efficient framework for sampling from constrained probabilitydistributions. We show the advantages of our method over state-of-the-artsampling algorithms, such as exact Hamiltonian Monte Carlo, using severalexamples including truncated Gaussian distributions, Bayesian Lasso, Bayesianbridge regression, reconstruction of quantized stationary Gaussian process, andLDA for topic modeling.
arxiv-12300-266 | LCSTS: A Large Scale Chinese Short Text Summarization Dataset | http://arxiv.org/pdf/1506.05865v4.pdf | author:Baotian Hu, Qingcai Chen, Fangze Zhu category:cs.CL cs.IR cs.LG published:2015-06-19 summary:Automatic text summarization is widely regarded as the highly difficultproblem, partially because of the lack of large text summarization data set.Due to the great challenge of constructing the large scale summaries for fulltext, in this paper, we introduce a large corpus of Chinese short textsummarization dataset constructed from the Chinese microblogging website SinaWeibo, which is released to the public{http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over2 million real Chinese short texts with short summaries given by the author ofeach text. We also manually tagged the relevance of 10,666 short summaries withtheir corresponding short texts. Based on the corpus, we introduce recurrentneural network for the summary generation and achieve promising results, whichnot only shows the usefulness of the proposed corpus for short textsummarization research, but also provides a baseline for further research onthis topic.
arxiv-12300-267 | Graph-based compression of dynamic 3D point cloud sequences | http://arxiv.org/pdf/1506.06096v1.pdf | author:Dorina Thanou, Philip A. Chou, Pascal Frossard category:cs.CV cs.GR published:2015-06-19 summary:This paper addresses the problem of compression of 3D point cloud sequencesthat are characterized by moving 3D positions and color attributes. Astemporally successive point cloud frames are similar, motion estimation is keyto effective compression of these sequences. It however remains a challengingproblem as the point cloud frames have varying numbers of points withoutexplicit correspondence information. We represent the time-varying geometry ofthese sequences with a set of graphs, and consider 3D positions and colorattributes of the points clouds as signals on the vertices of the graphs. Wethen cast motion estimation as a feature matching problem between successivegraphs. The motion is estimated on a sparse set of representative verticesusing new spectral graph wavelet descriptors. A dense motion field iseventually interpolated by solving a graph-based regularization problem. Theestimated motion is finally used for removing the temporal redundancy in thepredictive coding of the 3D positions and the color characteristics of thepoint cloud sequences. Experimental results demonstrate that our method is ableto accurately estimate the motion between consecutive frames. Moreover, motionestimation is shown to bring significant improvement in terms of the overallcompression performance of the sequence. To the best of our knowledge, this isthe first paper that exploits both the spatial correlation inside each frame(through the graph) and the temporal correlation between the frames (throughthe motion estimation) to compress the color and the geometry of 3D point cloudsequences in an efficient way.
arxiv-12300-268 | Scene-adaptive Coded Apertures Imaging | http://arxiv.org/pdf/1506.05942v2.pdf | author:Xuehui Wang, Jinli Suo, Jingyi Yu, Yongdong Zhang, Qionghai Dai category:cs.CV published:2015-06-19 summary:Coded aperture imaging systems have recently shown great success inrecovering scene depth and extending the depth-of-field. The ideal pattern,however, would have to serve two conflicting purposes: 1) be broadband toensure robust deconvolution and 2) has sufficient zero-crossings for a highdepth discrepancy. This paper presents a simple but effective scene-adaptivecoded aperture solution to bridge this gap. We observe that the geometricstructures in a natural scene often exhibit only a few edge directions, and thesuccessive frames are closely correlated. Therefore we adopt a spatialpartitioning and temporal propagation scheme. In each frame, we address oneprincipal direction by applying depth-discriminative codes along it andbroadband codes along its orthogonal direction. Since within a frame only theregions with edge direction corresponding to its aperture code behaves well, weutilize the close among-frame correlation to propagate the high quality singleframe results temporally to obtain high performance over the whole imagelattice. To physically implement this scheme, we use a Liquid Crystal onSilicon (LCoS) microdisplay that permits fast changing pattern codes. Firstly,we capture the scene with a pinhole and analyze the scene content to determineprimary edge orientations. Secondly, we sequentially apply the proposed codingscheme with these orientations in the following frames. Experiments on bothsynthetic and real scenes show that our technique is able to combine advantagesof the state-of-the-art patterns for recovering better quality depth map andall-focus images.
arxiv-12300-269 | CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits | http://arxiv.org/pdf/1506.06155v2.pdf | author:Mohammad Norouzi, Maxwell D. Collins, David J. Fleet, Pushmeet Kohli category:cs.LG cs.CV published:2015-06-19 summary:We propose a novel algorithm for optimizing multivariate linear thresholdfunctions as split functions of decision trees to create improved Random Forestclassifiers. Standard tree induction methods resort to sampling and exhaustivesearch to find good univariate split functions. In contrast, our methodcomputes a linear combination of the features at each node, and optimizes theparameters of the linear combination (oblique) split functions by adopting avariant of latent variable SVM formulation. We develop a convex-concave upperbound on the classification loss for a one-level decision tree, and optimizethe bound by stochastic gradient descent at each internal node of the tree.Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees arecreated, which significantly outperform Random Forest with univariate splitsand previous techniques for constructing oblique trees. Experimental resultsare reported on multi-class classification benchmarks and on Labeled Faces inthe Wild (LFW) dataset.
arxiv-12300-270 | Quantifying the Effect of Sentiment on Information Diffusion in Social Media | http://arxiv.org/pdf/1506.06072v1.pdf | author:Emilio Ferrara, Zeyao Yang category:cs.SI cs.LG physics.soc-ph published:2015-06-19 summary:Social media have become the main vehicle of information production andconsumption online. Millions of users every day log on their Facebook orTwitter accounts to get updates and news, read about their topics of interest,and become exposed to new opportunities and interactions. Although recentstudies suggest that the contents users produce will affect the emotions oftheir readers, we still lack a rigorous understanding of the role and effectsof contents sentiment on the dynamics of information diffusion. This work aimsat quantifying the effect of sentiment on information diffusion, to understand:(i) whether positive conversations spread faster and/or broader than negativeones (or vice-versa); (ii) what kind of emotions are more typical of popularconversations on social media; and, (iii) what type of sentiment is expressedin conversations characterized by different temporal dynamics. Our findingsshow that, at the level of contents, negative messages spread faster thanpositive ones, but positive ones reach larger audiences, suggesting that peopleare more inclined to share and favorite positive contents, the so-calledpositive bias. As for the entire conversations, we highlight how differenttemporal dynamics exhibit different sentiment patterns: for example, positivesentiment builds up for highly-anticipated events, while unexpected events aremainly characterized by negative sentiment. Our contribution is a milestone tounderstand how the emotions expressed in short texts affect their spreading inonline social ecosystems, and may help to craft effective policies andstrategies for content generation and diffusion.
arxiv-12300-271 | A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements | http://arxiv.org/pdf/1506.06081v3.pdf | author:Qinqing Zheng, John Lafferty category:stat.ML cs.LG published:2015-06-19 summary:We propose a simple, scalable, and fast gradient descent algorithm tooptimize a nonconvex objective for the rank minimization problem and a closelyrelated family of semidefinite programs. With $O(r^3 \kappa^2 n \log n)$ randommeasurements of a positive semidefinite $n \times n$ matrix of rank $r$ andcondition number $\kappa$, our method is guaranteed to converge linearly to theglobal optimum.
arxiv-12300-272 | The Extreme Value Machine | http://arxiv.org/pdf/1506.06112v3.pdf | author:Ethan M. Rudd, Lalit P. Jain, Walter J. Scheirer, Terrance E. Boult category:cs.LG published:2015-06-19 summary:It is often desirable to be able to recognize when inputs to a recognitionfunction correspond to classes unseen at training time. With this ability,these inputs could be re-labeled by a human, and later incorporated into therecognition function -- ideally under an efficient incremental updatemechanism. While good models that assume inputs from a fixed set of classesexist, e.g., artificial neural networks and kernel machines, it is notimmediately obvious how to extend them to perform incremental learning in thepresence of unknown query classes. Models that do so take little otherdistributional information into account when constructing recognition functionsand lack strong theoretical foundations. We take steps to address this gap byformulating a novel, theoretically grounded classifier -- the Extreme ValueMachine (EVM) -- which is capable of performing open world recognition. The EVMhas a well-grounded interpretation derived from statistical extreme valuetheory (EVT), and is the first classifier of its kind to be able to performnonlinear, kernel-free, variable bandwidth, incremental learning. Wedemonstrate experimentally that, compared to other classifiers in the same deepnetwork derived feature space, the EVM is accurate and efficient on anestablished benchmark partition of the ImageNet dataset.
arxiv-12300-273 | A Tight Runtime Analysis of the $(1+(λ, λ))$ Genetic Algorithm on OneMax | http://arxiv.org/pdf/1506.05937v1.pdf | author:Benjamin Doerr, Carola Doerr category:cs.NE published:2015-06-19 summary:Understanding how crossover works is still one of the big challenges inevolutionary computation research, and making our understanding precise andproven by mathematical means might be an even bigger one. As one of fewexamples where crossover provably is useful, the $(1+(\lambda, \lambda))$Genetic Algorithm (GA) was proposed recently in [Doerr, Doerr, Ebel: TCS 2015].Using the fitness level method, the expected optimization time on generalOneMax functions was analyzed and a $O(\max\{n\log(n)/\lambda, \lambda n\})$bound was proven for any offspring population size $\lambda \in [1..n]$. We improve this work in several ways, leading to sharper bounds and a betterunderstanding of how the use of crossover speeds up the runtime in thisalgorithm. We first improve the upper bound on the runtime to$O(\max\{n\log(n)/\lambda, n\lambda \log\log(\lambda)/\log(\lambda)\})$. Thisimprovement is made possible from observing that in the parallel generation of$\lambda$ offspring via crossover (but not mutation), the best of these oftenis better than the expected value, and hence several fitness levels can begained in one iteration. We then present the first lower bound for this problem. It matches our upperbound for all values of $\lambda$. This allows to determine the asymptoticallyoptimal value for the population size. It is $\lambda =\Theta(\sqrt{\log(n)\log\log(n)/\log\log\log(n)})$, which gives an optimizationtime of $\Theta(n \sqrt{\log(n)\log\log\log(n)/\log\log(n)})$. Hence theimproved runtime analysis gives a better runtime guarantee along with a bettersuggestion for the parameter $\lambda$. We finally give a tail bound for the upper tail of the runtime distribution,which shows that the actual runtime exceeds our runtime guarantee by a factorof $(1+\delta)$ with probability $O((n/\lambda^2)^{-\delta})$ only.
arxiv-12300-274 | Spectral Analysis of Symmetric and Anti-Symmetric Pairwise Kernels | http://arxiv.org/pdf/1506.05950v1.pdf | author:Tapio Pahikkala, Markus Viljanen, Antti Airola, Willem Waegeman category:cs.LG stat.ML published:2015-06-19 summary:We consider the problem of learning regression functions from pairwise datawhen there exists prior knowledge that the relation to be learned is symmetricor anti-symmetric. Such prior knowledge is commonly enforced by symmetrizing oranti-symmetrizing pairwise kernel functions. Through spectral analysis, we showthat these transformations reduce the kernel's effective dimension. Further, weprovide an analysis of the approximation properties of the resulting kernels,and bound the regularization bias of the kernels in terms of the correspondingbias of the original kernel.
arxiv-12300-275 | A general framework for the IT-based clustering methods | http://arxiv.org/pdf/1506.06068v1.pdf | author:Teng Qiu, Yongjie Li category:cs.CV cs.LG stat.ML published:2015-06-19 summary:Previously, we proposed a physically inspired rule to organize the datapoints in a sparse yet effective structure, called the in-tree (IT) graph,which is able to capture a wide class of underlying cluster structures in thedatasets, especially for the density-based datasets. Although there are someredundant edges or lines between clusters requiring to be removed by computer,this IT graph has a big advantage compared with the k-nearest-neighborhood(k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges inthe IT graph are much more distinguishable and thus can be easily determined byseveral methods previously proposed by us. In this paper, we propose a general framework to re-construct the IT graph,based on an initial neighborhood graph, such as the k-NN or MST, etc, and thecorresponding graph distances. For this general framework, our previous way ofconstructing the IT graph turns out to be a special case of it. This generalframework 1) can make the IT graph capture a wider class of underlying clusterstructures in the datasets, especially for the manifolds, and 2) should be moreeffective to cluster the sparse or graph-based datasets.
arxiv-12300-276 | Tensor Analysis and Fusion of Multimodal Brain Images | http://arxiv.org/pdf/1506.06040v1.pdf | author:Esin Karahan, Pedro A. Rojas-Lopez, Maria L. Bringas-Vega, Pedro A. Valdes-Hernandez, Pedro A. Valdes-Sosa category:stat.ME cs.NA stat.AP stat.ML published:2015-06-19 summary:Current high-throughput data acquisition technologies probe dynamical systemswith different imaging modalities, generating massive data sets at differentspatial and temporal resolutions posing challenging problems in multimodal datafusion. A case in point is the attempt to parse out the brain structures andnetworks that underpin human cognitive processes by analysis of differentneuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that themultimodal, multi-scale nature of neuroimaging data is well reflected by amulti-way (tensor) structure where the underlying processes can be summarizedby a relatively small number of components or "atoms". We introduceMarkov-Penrose diagrams - an integration of Bayesian DAG and tensor networknotation in order to analyze these models. These diagrams not only clarifymatrix and tensor EEG and fMRI time/frequency analysis and inverse problems,but also help understand multimodal fusion via Multiway Partial Least Squaresand Coupled Matrix-Tensor Factorization. We show here, for the first time, thatGranger causal analysis of brain networks is a tensor regression problem, thusallowing the atomic decomposition of brain networks. Analysis of EEG and fMRIrecordings shows the potential of the methods and suggests their use in otherscientific domains.
arxiv-12300-277 | Information-based inference in sloppy and singular models | http://arxiv.org/pdf/1506.05855v3.pdf | author:Colin H. LaMont, Paul A. Wiggins category:stat.ML cs.LG published:2015-06-19 summary:A central problem in statistics is model selection: the choice betweencompeting models of a stochastic process whose observables are corrupted bynoise. In information-based inference, model selection is performed bymaximizing the estimated predictive performance. We propose a frequen- tistinformation criterion (FIC) which extends the applicability ofinformation-based inference to the analysis of singular and sloppy models. Inthese scenarios, the Akaike information criterion (AIC) can result insignificant under or over-estimates of the predictive complexity. Two importantmechanisms for this failure are examined: an implicit multiple testing problemand the presence of unidentifiable parameters. FIC rectifies this failure byapplying a frequentist approximation to compute the com- plexity. For regularmodels in the large-sample-size limit, AIC and FIC are equal, but in generalthe complexity exhibits a sample-size dependent scaling. In the context ofsingular models, FIC can ex- hibit Bayesian information criterion-like orHannan-Quinn-like scalings with sample size. FIC does not depend on ad hocprior distributions or exogenous regularization and can be applied when struc-tured data complicates the use of cross-validatation.
arxiv-12300-278 | moco: Fast Motion Correction for Calcium Imaging | http://arxiv.org/pdf/1506.06039v1.pdf | author:Alexander Dubbs, James Guevara, Darcy S. Peterka, Rafael Yuste category:cs.CV published:2015-06-19 summary:Motion correction is the first in a pipeline of algorithms to analyze calciumimaging videos and extract biologically relevant information, for example thenetwork structure of the neurons therein. Fast motion correction would beespecially critical for closed-loop activity triggered stimulation experiments,where accurate detection and targeting of specific cells in necessary. Ouralgorithm uses a Fourier-transform approach, and its efficiency derives from acombination of judicious downsampling and the accelerated computation of many$L_2$ norms using dynamic programming and two-dimensional, fft-acceleratedconvolutions. Its accuracy is comparable to that of established community-usedalgorithms, and it is more stable to large translational motions. It isprogrammed in Java and is compatible with ImageJ.
arxiv-12300-279 | Variational Gaussian Copula Inference | http://arxiv.org/pdf/1506.05860v3.pdf | author:Shaobo Han, Xuejun Liao, David B. Dunson, Lawrence Carin category:stat.ML cs.LG stat.CO published:2015-06-19 summary:We utilize copulas to constitute a unified framework for constructing andoptimizing variational proposals in hierarchical Bayesian models. For modelswith continuous and non-Gaussian hidden variables, we propose a semiparametricand automated variational Gaussian copula approach, in which the parametricGaussian copula family is able to preserve multivariate posterior dependence,and the nonparametric transformations based on Bernstein polynomials provideample flexibility in characterizing the univariate marginal posteriors.
arxiv-12300-280 | Measuring Emotional Contagion in Social Media | http://arxiv.org/pdf/1506.06021v1.pdf | author:Emilio Ferrara, Zeyao Yang category:cs.SI cs.LG physics.soc-ph published:2015-06-19 summary:Social media are used as main discussion channels by millions of individualsevery day. The content individuals produce in daily social-media-basedmicro-communications, and the emotions therein expressed, may impact theemotional states of others. A recent experiment performed on Facebookhypothesized that emotions spread online, even in absence of non-verbal cuestypical of in-person interactions, and that individuals are more likely toadopt positive or negative emotions if these are over-expressed in their socialnetwork. Experiments of this type, however, raise ethical concerns, as theyrequire massive-scale content manipulation with unknown consequences for theindividuals therein involved. Here, we study the dynamics of emotionalcontagion using Twitter. Rather than manipulating content, we devise a nullmodel that discounts some confounding factors (including the effect ofemotional contagion). We measure the emotional valence of content the users areexposed to before posting their own tweets. We determine that on average anegative post follows an over-exposure to 4.34% more negative content thanbaseline, while positive posts occur after an average over-exposure to 4.50%more positive contents. We highlight the presence of a linear relationshipbetween the average emotional valence of the stimuli users are exposed to, andthat of the responses they produce. We also identify two different classes ofindividuals: highly and scarcely susceptible to emotional contagion. Highlysusceptible users are significantly less inclined to adopt negative emotionsthan the scarcely susceptible ones, but equally likely to adopt positiveemotions. In general, the likelihood of adopting positive emotions is muchgreater than that of negative emotions.
arxiv-12300-281 | Crowd Flow Segmentation in Compressed Domain using CRF | http://arxiv.org/pdf/1506.06006v1.pdf | author:Srinivas S. S. Kruthiventi, R. Venkatesh Babu category:cs.CV published:2015-06-19 summary:Crowd flow segmentation is an important step in many video surveillancetasks. In this work, we propose an algorithm for segmenting flows in H.264compressed videos in a completely unsupervised manner. Our algorithm works onmotion vectors which can be obtained by partially decoding the compressed videowithout extracting any additional features. Our approach is based on modellingthe motion vector field as a Conditional Random Field (CRF) and obtainingoriented motion segments by finding the optimal labelling which minimises theglobal energy of CRF. These oriented motion segments are recursively mergedbased on gradient across their boundaries to obtain the final flow segments.This work in compressed domain can be easily extended to pixel domain bysubstituting motion vectors with motion based features like optical flow. Theproposed algorithm is experimentally evaluated on a standard crowd flow datasetand its superior performance in both accuracy and computational time aredemonstrated through quantitative results.
arxiv-12300-282 | A new Initial Centroid finding Method based on Dissimilarity Tree for K-means Algorithm | http://arxiv.org/pdf/1509.03200v1.pdf | author:Abhishek Kumar, Suresh Chandra Gupta category:cs.LG published:2015-06-19 summary:Cluster analysis is one of the primary data analysis technique in data miningand K-means is one of the commonly used partitioning clustering algorithm. InK-means algorithm, resulting set of clusters depend on the choice of initialcentroids. If we can find initial centroids which are coherent with thearrangement of data, the better set of clusters can be obtained. This paperproposes a method based on the Dissimilarity Tree to find, the better initialcentroid as well as every bit more accurate cluster with less computationaltime. Theory analysis and experimental results indicate that the proposedmethod can effectively improve the accuracy of clusters and reduce thecomputational complexity of the K-means algorithm.
arxiv-12300-283 | Design of OFDM radar pulses using genetic algorithm based techniques | http://arxiv.org/pdf/1507.01889v1.pdf | author:Gabriel Lellouch, Amit Kumar Mishra, Michael Inggs category:cs.NE published:2015-06-19 summary:The merit of evolutionary algorithms (EA) to solve convex optimizationproblems is widely acknowledged. In this paper, a genetic algorithm (GA)optimization based waveform design framework is used to improve the features ofradar pulses relying on the orthogonal frequency division multiplexing (OFDM)structure. Our optimization techniques focus on finding optimal phase codesequences for the OFDM signal. Several optimality criteria are used since weconsider two different radar processing solutions which call either for singleor multiple-objective optimizations. When minimization of the so-calledpeak-to-mean envelope power ratio (PMEPR) single-objective is tackled, wecompare our findings with existing methods and emphasize on the merit of ourapproach. In the scope of the two-objective optimization, we first addressPMEPR and peak-to-sidelobe level ratio (PSLR) and show that our approach basedon the non-dominated sorting genetic algorithm-II (NSGA-II) provides designsolutions with noticeable improvements as opposed to random sets of phasecodes. We then look at another case of interest where the objective functionsare two measures of the sidelobe level, namely PSLR and the integrated-sidelobelevel ratio (ISLR) and propose to modify the NSGA-II to include a constrain onthe PMEPR instead. In the last part, we illustrate via a case study how ourencoding solution makes it possible to minimize the single objective PMEPRwhile enabling a target detection enhancement strategy, when the SNR metricwould be chosen for the detection framework.
arxiv-12300-284 | Stereoscopic Cinema | http://arxiv.org/pdf/1506.06001v1.pdf | author:Frédéric Devernay, Paul Beardsley category:cs.CV published:2015-06-19 summary:Stereoscopic cinema has seen a surge of activity in recent years, and for thefirst time all of the major Hollywood studios released 3-D movies in 2009. Thisis happening alongside the adoption of 3-D technology for sports broadcasting,and the arrival of 3-D TVs for the home. Two previous attempts to introduce 3-Dcinema in the 1950s and the 1980s failed because the contemporary technologywas immature and resulted in viewer discomfort. But current technologies --such as accurately-adjustable 3-D camera rigs with onboard computers toautomatically inform a camera operator of inappropriate stereoscopic shots,digital processing for post-shooting rectification of the 3-D imagery, digitalprojectors for accurate positioning of the two stereo projections on the cinemascreen, and polarized silver screens to reduce cross-talk between the viewersleft- and right-eyes -- mean that the viewer experience is at a much higherlevel of quality than in the past. Even so, creation of stereoscopic cinema isan open, active research area, and there are many challenges from acquisitionto post-production to automatic adaptation for different-sized display. Thischapter describes the current state-of-the-art in stereoscopic cinema, anddirections of future work.
arxiv-12300-285 | Causality on Cross-Sectional Data: Stable Specification Search in Constrained Structural Equation Modeling | http://arxiv.org/pdf/1506.05600v2.pdf | author:Ridho Rahmadi, Perry Groot, Marianne Heins, Hans Knoop, Tom Heskes, The OPTIMISTIC consortium category:stat.ML cs.LG published:2015-06-18 summary:Causal modeling has long been an attractive topic for many researchers and inrecent decades there has seen a surge in theoretical development and discoveryalgorithms. Generally discovery algorithms can be divided into two approaches:constraint-based and score-based. The constraint-based approach is able todetect common causes of the observed variables but the use of independencetests makes it less reliable. The score-based approach produces a result thatis easier to interpret as it also measures the reliability of the inferredcausal relationships, but it is unable to detect common confounders of theobserved variables. A drawback of both score-based and constrained-basedapproaches is the inherent instability in structure estimation. With finitesamples small changes in the data can lead to completely different optimalstructures. The present work introduces a new hypothesis-free score-basedcausal discovery algorithm that is robust for finite samples based on recentadvances in stability selection using subsampling and selection algorithms.Structure search is performed over Structural Equation Models. Our approachuses exploratory search but allows incorporation of prior background knowledge.We validated our approach on one simulated data set, which we compare to theknown the ground truth, and two real-world data sets for Chronic FatigueSyndrome and Attention Deficit Hyperactivity Disorder, which we compare toearlier medical studies. The result on the simulated data set shows accuratestructure estimates and the results on the real-word data sets show consistencywith the hypothesis driven models constructed by medical experts.
arxiv-12300-286 | Scalable Semi-Supervised Aggregation of Classifiers | http://arxiv.org/pdf/1506.05790v2.pdf | author:Akshay Balsubramani, Yoav Freund category:cs.LG published:2015-06-18 summary:We present and empirically evaluate an efficient algorithm that learns toaggregate the predictions of an ensemble of binary classifiers. The algorithmuses the structure of the ensemble predictions on unlabeled data to yieldsignificant performance improvements. It does this without making assumptionson the structure or origin of the ensemble, without parameters, and as scalablyas linear learning. We empirically demonstrate these performance gains withrandom forests.
arxiv-12300-287 | "The Sum of Its Parts": Joint Learning of Word and Phrase Representations with Autoencoders | http://arxiv.org/pdf/1506.05703v1.pdf | author:Rémi Lebret, Ronan Collobert category:cs.CL published:2015-06-18 summary:Recently, there has been a lot of effort to represent words in continuousvector spaces. Those representations have been shown to capture both semanticand syntactic information about words. However, distributed representations ofphrases remain a challenge. We introduce a novel model that jointly learns wordvector representations and their summation. Word representations are learntusing the word co-occurrence statistical information. To embed sequences ofwords (i.e. phrases) with different sizes into a common semantic space, wepropose to average word vector representations. In contrast with previousmethods which reported a posteriori some compositionality aspects by simplesummation, we simultaneously train words to sum, while keeping the maximuminformation from the original vectors. We evaluate the quality of the wordrepresentations on several classical word evaluation tasks, and we introduce anovel task to evaluate the quality of the phrase representations. While ourdistributed representations compete with other methods of learning wordrepresentations on word evaluations, we show that they give better performanceon the phrase evaluation. Such representations of phrases could be interestingfor many tasks in natural language processing.
arxiv-12300-288 | Comparing the writing style of real and artificial papers | http://arxiv.org/pdf/1506.05702v2.pdf | author:Diego R. Amancio category:cs.CL published:2015-06-18 summary:Recent years have witnessed the increase of competition in science. Whilepromoting the quality of research in many cases, an intense competition amongscientists can also trigger unethical scientific behaviors. To increase thetotal number of published papers, some authors even resort to software toolsthat are able to produce grammatical, but meaningless scientific manuscripts.Because automatically generated papers can be misunderstood as real papers, itbecomes of paramount importance to develop means to identify these scientificfrauds. In this paper, I devise a methodology to distinguish real manuscriptsfrom those generated with SCIGen, an automatic paper generator. Upon modelingtexts as complex networks (CN), it was possible to discriminate real from fakepapers with at least 89\% of accuracy. A systematic analysis of featuresrelevance revealed that the accessibility and betweenness were useful inparticular cases, even though the relevance depended upon the dataset. Thesuccessful application of the methods described here show, as a proof ofprinciple, that network features can be used to identify scientific gibberishpapers. In addition, the CN-based approach can be combined in a straightforwardfashion with traditional statistical language processing methods to improve theperformance in identifying artificially generated papers.
arxiv-12300-289 | Comparing and evaluating extended Lambek calculi | http://arxiv.org/pdf/1506.05561v1.pdf | author:Richard Moot category:cs.CL cs.LO published:2015-06-18 summary:Lambeks Syntactic Calculus, commonly referred to as the Lambek calculus, wasinnovative in many ways, notably as a precursor of linear logic. But it alsoshowed that we could treat our grammatical framework as a logic (as opposed toa logical theory). However, though it was successful in giving at least a basictreatment of many linguistic phenomena, it was also clear that a slightly moreexpressive logical calculus was needed for many other cases. Therefore, manyextensions and variants of the Lambek calculus have been proposed, since theeighties and up until the present day. As a result, there is now a large classof calculi, each with its own empirical successes and theoretical results, butalso each with its own logical primitives. This raises the question: how do wecompare and evaluate these different logical formalisms? To answer thisquestion, I present two unifying frameworks for these extended Lambek calculi.Both are proof net calculi with graph contraction criteria. The first calculusis a very general system: you specify the structure of your sequents and itgives you the connectives and contractions which correspond to it. The calculuscan be extended with structural rules, which translate directly into graphrewrite rules. The second calculus is first-order (multiplicativeintuitionistic) linear logic, which turns out to have several other,independently proposed extensions of the Lambek calculus as fragments. I willillustrate the use of each calculus in building bridges between analysesproposed in different frameworks, in highlighting differences and in helping toidentify problems.
arxiv-12300-290 | An Iterative Convolutional Neural Network Algorithm Improves Electron Microscopy Image Segmentation | http://arxiv.org/pdf/1506.05849v1.pdf | author:Xundong Wu category:cs.NE cs.LG published:2015-06-18 summary:To build the connectomics map of the brain, we developed a new algorithm thatcan automatically refine the Membrane Detection Probability Maps (MDPM)generated to perform automatic segmentation of electron microscopy (EM) images.To achieve this, we executed supervised training of a convolutional neuralnetwork to recover the removed center pixel label of patches sampled from aMDPM. MDPM can be generated from other machine learning based algorithmsrecognizing whether a pixel in an image corresponds to the cell membrane. Byiteratively applying this network over MDPM for multiple rounds, we were ableto significantly improve membrane segmentation results.
arxiv-12300-291 | Hamiltonian Monte Carlo Acceleration Using Surrogate Functions with Random Bases | http://arxiv.org/pdf/1506.05555v4.pdf | author:Cheng Zhang, Babak Shahbaba, Hongkai Zhao category:stat.CO stat.ML published:2015-06-18 summary:For big data analysis, high computational cost for Bayesian methods oftenlimits their applications in practice. In recent years, there have been manyattempts to improve computational efficiency of Bayesian inference. Here wepropose an efficient and scalable computational technique for astate-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, HamiltonianMonte Carlo (HMC). The key idea is to explore and exploit the structure andregularity in parameter space for the underlying probabilistic model toconstruct an effective approximation of its geometric properties. To this end,we build a surrogate function to approximate the target distribution usingproperly chosen random bases and an efficient optimization process. Theresulting method provides a flexible, scalable, and efficient samplingalgorithm, which converges to the correct target distribution. We show that bychoosing the basis functions and optimization process differently, our methodcan be related to other approaches for the construction of surrogate functionssuch as generalized additive models or Gaussian process models. Experimentsbased on simulated and real data show that our approach leads to substantiallymore efficient sampling algorithms compared to existing state-of-the artmethods.
arxiv-12300-292 | A hybrid algorithm for Bayesian network structure learning with application to multi-label learning | http://arxiv.org/pdf/1506.05692v1.pdf | author:Maxime Gasse, Alex Aussem, Haytham Elghazel category:stat.ML cs.AI cs.LG published:2015-06-18 summary:We present a novel hybrid algorithm for Bayesian network structure learning,called H2PC. It first reconstructs the skeleton of a Bayesian network and thenperforms a Bayesian-scoring greedy hill-climbing search to orient the edges.The algorithm is based on divide-and-conquer constraint-based subroutines tolearn the local structure around a target variable. We conduct two series ofexperimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which iscurrently the most powerful state-of-the-art algorithm for Bayesian networkstructure learning. First, we use eight well-known Bayesian network benchmarkswith various data sizes to assess the quality of the learned structure returnedby the algorithms. Our extensive experiments show that H2PC outperforms MMHC interms of goodness of fit to new data and quality of the network structure withrespect to the true dependence structure of the data. Second, we investigateH2PC's ability to solve the multi-label learning problem. We providetheoretical results to characterize and identify graphically the so-calledminimal label powersets that appear as irreducible factors in the jointdistribution under the faithfulness condition. The multi-label learning problemis then decomposed into a series of multi-class classification problems, whereeach multi-class variable encodes a label powerset. H2PC is shown to comparefavorably to MMHC in terms of global classification accuracy over tenmulti-label data sets covering different application domains. Overall, ourexperiments support the conclusions that local structural learning with H2PC inthe form of local neighborhood induction is a theoretically well-motivated andempirically effective learning framework that is well suited to multi-labellearning. The source code (in R) of H2PC as well as all data sets used for theempirical tests are publicly available.
arxiv-12300-293 | A Spatial Layout and Scale Invariant Feature Representation for Indoor Scene Classification | http://arxiv.org/pdf/1506.05532v2.pdf | author:Munawar Hayat, Salman H. Khan, Mohammed Bennamoun, Senjian An category:cs.CV published:2015-06-18 summary:Unlike standard object classification, where the image to be classifiedcontains one or multiple instances of the same object, indoor sceneclassification is quite different since the image consists of multiple distinctobjects. Further, these objects can be of varying sizes and are present acrossnumerous spatial locations in different layouts. For automatic indoor scenecategorization, large scale spatial layout deformations and scale variationsare therefore two major challenges and the design of rich feature descriptorswhich are robust to these challenges is still an open problem. This paperintroduces a new learnable feature descriptor called "spatial layout and scaleinvariant convolutional activations" to deal with these challenges. For thispurpose, a new Convolutional Neural Network architecture is designed whichincorporates a novel 'Spatially Unstructured' layer to introduce robustnessagainst spatial layout deformations. To achieve scale invariance, we present apyramidal image representation. For feasible training of the proposed networkfor images of indoor scenes, the paper proposes a new methodology whichefficiently adapts a trained network model (on a large scale data) for our taskwith only a limited amount of available training data. Compared with existingstate of the art, the proposed approach achieves a relative performanceimprovement of 3.2%, 3.8%, 7.0%, 11.9% and 2.1% on MIT-67, Scene-15, Sports-8,Graz-02 and NYU datasets respectively.
arxiv-12300-294 | Dependent Multinomial Models Made Easy: Stick Breaking with the Pólya-Gamma Augmentation | http://arxiv.org/pdf/1506.05843v1.pdf | author:Scott W. Linderman, Matthew J. Johnson, Ryan P. Adams category:stat.ML published:2015-06-18 summary:Many practical modeling problems involve discrete data that are bestrepresented as draws from multinomial or categorical distributions. Forexample, nucleotides in a DNA sequence, children's names in a given state andyear, and text documents are all commonly modeled with multinomialdistributions. In all of these cases, we expect some form of dependency betweenthe draws: the nucleotide at one position in the DNA strand may depend on thepreceding nucleotides, children's names are highly correlated from year toyear, and topics in text may be correlated and dynamic. These dependencies arenot naturally captured by the typical Dirichlet-multinomial formulation. Here,we leverage a logistic stick-breaking representation and recent innovations inP\'olya-gamma augmentation to reformulate the multinomial distribution in termsof latent variables with jointly Gaussian likelihoods, enabling us to takeadvantage of a host of Bayesian inference techniques for Gaussian models withminimal overhead.
arxiv-12300-295 | Optimal model-free prediction from multivariate time series | http://arxiv.org/pdf/1506.05822v1.pdf | author:Jakob Runge, Reik V. Donner, Jürgen Kurths category:stat.ML stat.ME published:2015-06-18 summary:Forecasting a time series from multivariate predictors constitutes achallenging problem, especially using model-free approaches. Most techniques,such as nearest-neighbor prediction, quickly suffer from the curse ofdimensionality and overfitting for more than a few predictors which has limitedtheir application mostly to the univariate case. Therefore, selectionstrategies are needed that harness the available information as efficiently aspossible. Since often the right combination of predictors matters, ideally allsubsets of possible predictors should be tested for their predictive power, butthe exponentially growing number of combinations makes such an approachcomputationally prohibitive. Here a prediction scheme that overcomes thisstrong limitation is introduced utilizing a causal pre-selection step whichdrastically reduces the number of possible predictors to the most predictiveset of causal drivers making a globally optimal search scheme tractable. Theinformation-theoretic optimality is derived and practical selection criteriaare discussed. As demonstrated for multivariate nonlinear stochastic delayprocesses, the optimal scheme can even be less computationally expensive thancommonly used sub-optimal schemes like forward selection. The method suggests ageneral framework to apply the optimal model-free approach to select variablesand subsequently fit a model to further improve a prediction or learnstatistical dependencies. The performance of this framework is illustrated on aclimatological index of El Ni\~no Southern Oscillation.
arxiv-12300-296 | Point-wise Map Recovery and Refinement from Functional Correspondence | http://arxiv.org/pdf/1506.05603v1.pdf | author:Emanuele Rodolà, Michael Moeller, Daniel Cremers category:cs.CV cs.CG published:2015-06-18 summary:Since their introduction in the shape analysis community, functional mapshave met with considerable success due to their ability to compactly representdense correspondences between deformable shapes, with applications ranging fromshape matching and image segmentation, to exploration of large shapecollections. Despite the numerous advantages of such representation, however,the problem of converting a given functional map back to a point-to-point maphas received a surprisingly limited interest. In this paper we analyze thegeneral problem of point-wise map recovery from arbitrary functional maps. Indoing so, we rule out many of the assumptions required by the currentlyestablished approach -- most notably, the limiting requirement of the inputshapes being nearly-isometric. We devise an efficient recovery process based ona simple probabilistic model. Experiments confirm that this approach achievesremarkable accuracy improvements in very challenging cases.
arxiv-12300-297 | Simultaneous Estimation of Non-Gaussian Components and their Correlation Structure | http://arxiv.org/pdf/1506.05666v1.pdf | author:Hiroaki Sasaki, Michael U. Gutmann, Hayaru Shouno, Aapo Hyvärinen category:stat.ML published:2015-06-18 summary:The statistical dependencies which independent component analysis (ICA)cannot remove often provide rich information beyond the linear independentcomponents. It would thus be very useful to estimate the dependency structurefrom data. While such models have been proposed, they usually concentrated onhigher-order correlations such as energy (square) correlations. Yet, linearcorrelations are a fundamental and informative form of dependency in many realdata sets. Linear correlations are usually completely removed by ICA andrelated methods, so they can only be analyzed by developing new methods whichexplicitly allow for linearly correlated components. In this paper, we proposea probabilistic model of linear non-Gaussian components which are allowed tohave both linear and energy correlations. The precision matrix of the linearcomponents is assumed to be randomly generated by a higher-order process andexplicitly parametrized by a parameter matrix. The estimation of the parametermatrix is shown to be particularly simple because using score matching, theobjective function is a quadratic form. Using simulations with artificial data,we demonstrate that the proposed method is able to estimate non-Gaussiancomponents and their correlation structure simultaneously. Applications onsimulated complex cells with natural image input, as well as spectrograms ofnatural audio data show that the method finds new kinds of dependencies betweenthe components.
arxiv-12300-298 | Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks | http://arxiv.org/pdf/1506.05751v1.pdf | author:Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus category:cs.CV published:2015-06-18 summary:In this paper we introduce a generative parametric model capable of producinghigh quality samples of natural images. Our approach uses a cascade ofconvolutional networks within a Laplacian pyramid framework to generate imagesin a coarse-to-fine fashion. At each level of the pyramid, a separategenerative convnet model is trained using the Generative Adversarial Nets (GAN)approach (Goodfellow et al.). Samples drawn from our model are of significantlyhigher quality than alternate approaches. In a quantitative assessment by humanevaluators, our CIFAR10 samples were mistaken for real images around 40% of thetime, compared to 10% for samples drawn from a GAN baseline model. We also showsamples from models trained on the higher resolution images of the LSUN scenedataset.
arxiv-12300-299 | A tree augmented naive Bayesian network experiment for breast cancer prediction | http://arxiv.org/pdf/1506.05776v1.pdf | author:Ping Ren category:stat.ML q-bio.QM published:2015-06-18 summary:In order to investigate the breast cancer prediction problem on the agingpopulation with the grades of DCIS, we conduct a tree augmented naive Bayesiannetwork experiment trained and tested on a large clinical dataset includingconsecutive diagnostic mammography examinations, consequent biopsy outcomes andrelated cancer registry records in the population of women across all ages. Theaggregated results of our ten-fold cross validation method recommend a biopsythreshold higher than 2% for the aging population.
arxiv-12300-300 | Editorial for the First Workshop on Mining Scientific Papers: Computational Linguistics and Bibliometrics | http://arxiv.org/pdf/1506.05402v1.pdf | author:Iana Atanassova, Marc Bertin, Philipp Mayr category:cs.CL cs.DL cs.IR published:2015-06-17 summary:The workshop "Mining Scientific Papers: Computational Linguistics andBibliometrics" (CLBib 2015), co-located with the 15th International Society ofScientometrics and Informetrics Conference (ISSI 2015), brought togetherresearchers in Bibliometrics and Computational Linguistics in order to studythe ways Bibliometrics can benefit from large-scale text analytics and sensemining of scientific papers, thus exploring the interdisciplinarity ofBibliometrics and Natural Language Processing (NLP). The goals of the workshopwere to answer questions like: How can we enhance author network analysis andBibliometrics using data obtained by text analytics? What insights can NLPprovide on the structure of scientific writing, on citation networks, and onin-text citation analysis? This workshop is the first step to foster thereflection on the interdisciplinarity and the benefits that the two disciplinesBibliometrics and Natural Language Processing can drive from it.
