arxiv-7200-1 | A feasible roadmap for developing volumetric probability atlas of localized prostate cancer | http://arxiv.org/pdf/1409.4139v1.pdf | author:Liang Zhao, Jianhua Xuan, Yue Wang category:q-bio.QM cs.CV published:2014-09-15 summary:A statistical volumetric model, showing the probability map of localizedprostate cancer within the host anatomical structure, has been developed from90 optically-imaged surgical specimens. This master model permits an accuratecharacterization of prostate cancer distribution patterns and an atlas-informedbiopsy sampling strategy. The model is constructed by mapping individualprostate models onto a site model, together with localized tumors. An accuratemulti-object non-rigid warping scheme is developed based on a mixture ofprincipal-axis registrations. We report our evaluation and pilot studies on theeffectiveness of the method and its application to optimizing needle biopsystrategies.
arxiv-7200-2 | Avoiding pathologies in very deep networks | http://arxiv.org/pdf/1402.5836v2.pdf | author:David Duvenaud, Oren Rippel, Ryan P. Adams, Zoubin Ghahramani category:stat.ML cs.LG published:2014-02-24 summary:Choosing appropriate architectures and regularization strategies for deepnetworks is crucial to good predictive performance. To shed light on thisproblem, we analyze the analogous problem of constructing useful priors oncompositions of functions. Specifically, we study the deep Gaussian process, atype of infinitely-wide, deep neural network. We show that in standardarchitectures, the representational capacity of the network tends to capturefewer degrees of freedom as the number of layers increases, retaining only asingle degree of freedom in the limit. We propose an alternate networkarchitecture which does not suffer from this pathology. We also examine deepcovariance functions, obtained by composing infinitely many feature transforms.Lastly, we characterize the class of models obtained by performing dropout onGaussian processes.
arxiv-7200-3 | Learning $k$-Modal Distributions via Testing | http://arxiv.org/pdf/1107.2700v3.pdf | author:Constantinos Daskalakis, Ilias Diakonikolas, Rocco A. Servedio category:cs.DS cs.LG math.ST stat.TH published:2011-07-13 summary:A $k$-modal probability distribution over the discrete domain $\{1,...,n\}$is one whose histogram has at most $k$ "peaks" and "valleys." Suchdistributions are natural generalizations of monotone ($k=0$) and unimodal($k=1$) probability distributions, which have been intensively studied inprobability theory and statistics. In this paper we consider the problem of \emph{learning} (i.e., performingdensity estimation of) an unknown $k$-modal distribution with respect to the$L_1$ distance. The learning algorithm is given access to independent samplesdrawn from an unknown $k$-modal distribution $p$, and it must output ahypothesis distribution $\widehat{p}$ such that with high probability the totalvariation distance between $p$ and $\widehat{p}$ is at most $\epsilon.$ Ourmain goal is to obtain \emph{computationally efficient} algorithms for thisproblem that use (close to) an information-theoretically optimal number ofsamples. We give an efficient algorithm for this problem that runs in time$\mathrm{poly}(k,\log(n),1/\epsilon)$. For $k \leq \tilde{O}(\log n)$, thenumber of samples used by our algorithm is very close (within an$\tilde{O}(\log(1/\epsilon))$ factor) to being information-theoreticallyoptimal. Prior to this work computationally efficient algorithms were knownonly for the cases $k=0,1$ \cite{Birge:87b,Birge:97}. A novel feature of our approach is that our learning algorithm crucially usesa new algorithm for \emph{property testing of probability distributions} as akey subroutine. The learning algorithm uses the property tester to efficientlydecompose the $k$-modal distribution into $k$ (near-)monotone distributions,which are easier to learn.
arxiv-7200-4 | Cavlectometry: Towards Holistic Reconstruction of Large Mirror Objects | http://arxiv.org/pdf/1409.4095v1.pdf | author:Jonathan Balzer, Daniel Acevedo-Feliz, Stefano Soatto, Sebastian Höfer, Markus Hadwiger, Jürgen Beyerer category:cs.CV published:2014-09-14 summary:We introduce a method based on the deflectometry principle for thereconstruction of specular objects exhibiting significant size and geometriccomplexity. A key feature of our approach is the deployment of an AutomaticVirtual Environment (CAVE) as pattern generator. To unfold the full power ofthis extraordinary experimental setup, an optical encoding scheme is developedwhich accounts for the distinctive topology of the CAVE. Furthermore, we devisean algorithm for detecting the object of interest in raw deflectometric images.The segmented foreground is used for single-view reconstruction, the backgroundfor estimation of the camera pose, necessary for calibrating the sensor system.Experiments suggest a significant gain of coverage in single measurementscompared to previous methods. To facilitate research on specular surfacereconstruction, we will make our data set publicly available.
arxiv-7200-5 | A New Framework for Retinex based Color Image Enhancement using Particle Swarm Optimization | http://arxiv.org/pdf/1409.4046v1.pdf | author:M. C Hanumantharaju, M. Ravishankar, D. R Rameshbabu, V. N Manjunath Aradhya category:cs.CV 68T45 H.2.0 published:2014-09-14 summary:A new approach for tuning the parameters of MultiScale Retinex (MSR) basedcolor image enhancement algorithm using a popular optimization method, namely,Particle Swarm Optimization (PSO) is presented in this paper. The imageenhancement using MSR scheme heavily depends on parameters such as Gaussiansurround space constant, number of scales, gain and offset etc. Selection ofthese parameters, empirically and its application to MSR scheme to produceinevitable results are the major blemishes. The method presented here resultsin huge savings of computation time as well as improvement in the visualquality of an image, since the PSO exploited maximizes the MSR parameters. Theobjective of PSO is to validate the visual quality of the enhanced imageiteratively using an effective objective criterion based on entropy and edgeinformation of an image. The PSO method of parameter optimization of MSR schemeachieves a very good quality of reconstructed images, far better than thatpossible with the other existing methods. Finally, the quality of the enhancedcolor images obtained by the proposed method are evaluated using novel metric,namely, Wavelet Energy (WE). The experimental results presented show that colorimages enhanced using the proposed scheme are clearer, more vivid andefficient.
arxiv-7200-6 | A new approach in machine learning | http://arxiv.org/pdf/1409.4044v1.pdf | author:Alain Tapp category:stat.ML cs.LG published:2014-09-14 summary:In this technical report we presented a novel approach to machine learning.Once the new framework is presented, we will provide a simple and yet verypowerful learning algorithm which will be benchmark on various dataset. The framework we proposed is based on booleen circuits; more specifically theclassifier produced by our algorithm have that form. Using bits and booleangates instead of real numbers and multiplication enable the the learningalgorithm and classifier to use very efficient boolean vector operations. Thisenable both the learning algorithm and classifier to be extremely efficient.The accuracy of the classifier we obtain with our framework compares veryfavorably those produced by conventional techniques, both in terms ofefficiency and accuracy.
arxiv-7200-7 | Design of Novel Algorithm and Architecture for Gaussian Based Color Image Enhancement System for Real Time Applications | http://arxiv.org/pdf/1409.4043v1.pdf | author:M. C. Hanumantharaju, M. Ravishankar, D. R. Rameshbabu category:cs.AR cs.CV published:2014-09-14 summary:This paper presents the development of a new algorithm for Gaussian basedcolor image enhancement system. The algorithm has been designed intoarchitecture suitable for FPGA/ASIC implementation. The color image enhancementis achieved by first convolving an original image with a Gaussian kernel sinceGaussian distribution is a point spread function which smoothen the image.Further, logarithm-domain processing and gain/offset corrections are employedin order to enhance and translate pixels into the display range of 0 to 255.The proposed algorithm not only provides better dynamic range compression andcolor rendition effect but also achieves color constancy in an image. Thedesign exploits high degrees of pipelining and parallel processing to achievereal time performance. The design has been realized by RTL compliant Verilogcoding and fits into a single FPGA with a gate count utilization of 321,804.The proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148FPGA device and is capable of processing high resolution color motion picturesof sizes of up to 1600x1200 pixels at the real time video rate of 116 framesper second. This shows that the proposed design would work for not only stillimages but also for high resolution video sequences.
arxiv-7200-8 | EquiNMF: Graph Regularized Multiview Nonnegative Matrix Factorization | http://arxiv.org/pdf/1409.4018v1.pdf | author:Daniel Hidru, Anna Goldenberg category:cs.LG cs.NA published:2014-09-14 summary:Nonnegative matrix factorization (NMF) methods have proved to be powerfulacross a wide range of real-world clustering applications. Integrating multipletypes of measurements for the same objects/subjects allows us to gain a deeperunderstanding of the data and refine the clustering. We have developed a novelGraph-reguarized multiview NMF-based method for data integration calledEquiNMF. The parameters for our method are set in a completely automateddata-specific unsupervised fashion, a highly desirable property in real-worldapplications. We performed extensive and comprehensive experiments on multiviewimaging data. We show that EquiNMF consistently outperforms other single-viewNMF methods used on concatenated data and multi-view NMF methods with differenttypes of regularizations.
arxiv-7200-9 | Mining Mid-level Features for Action Recognition Based on Effective Skeleton Representation | http://arxiv.org/pdf/1409.4014v1.pdf | author:Pichao Wang, Wanqing Li, Philip Ogunbona, Zhimin Gao, Hanling Zhang category:cs.CV published:2014-09-14 summary:Recently, mid-level features have shown promising performance in computervision. Mid-level features learned by incorporating class-level information arepotentially more discriminative than traditional low-level local features. Inthis paper, an effective method is proposed to extract mid-level features fromKinect skeletons for 3D human action recognition. Firstly, the orientations oflimbs connected by two skeleton joints are computed and each orientation isencoded into one of the 27 states indicating the spatial relationship of thejoints. Secondly, limbs are combined into parts and the limb's states aremapped into part states. Finally, frequent pattern mining is employed to minethe most frequent and relevant (discriminative, representative andnon-redundant) states of parts in continuous several frames. These parts arereferred to as Frequent Local Parts or FLPs. The FLPs allow us to buildpowerful bag-of-FLP-based action representation. This new representation yieldsstate-of-the-art results on MSR DailyActivity3D and MSR ActionPairs3D.
arxiv-7200-10 | Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces | http://arxiv.org/pdf/1409.4011v1.pdf | author:Kevin Swersky, David Duvenaud, Jasper Snoek, Frank Hutter, Michael A. Osborne category:stat.ML published:2014-09-14 summary:In practical Bayesian optimization, we must often search over structures withdiffering numbers of parameters. For instance, we may wish to search overneural network architectures with an unknown number of layers. To relateperformance data gathered for different architectures, we define a new kernelfor conditional parameter spaces that explicitly includes information aboutwhich parameters are relevant in a given structure. We show that this kernelimproves model quality and Bayesian optimization results over several simplerbaseline kernels.
arxiv-7200-11 | Sparse Estimation with Strongly Correlated Variables using Ordered Weighted L1 Regularization | http://arxiv.org/pdf/1409.4005v1.pdf | author:Mario A. T. Figueiredo, Robert D. Nowak category:stat.ML published:2014-09-14 summary:This paper studies ordered weighted L1 (OWL) norm regularization for sparseestimation problems with strongly correlated variables. We prove sufficientconditions for clustering based on the correlation/colinearity of variablesusing the OWL norm, of which the so-called OSCAR is a particular case. Ourresults extend previous ones for OSCAR in several ways: for the squared errorloss, our conditions hold for the more general OWL norm and under weakerassumptions; we also establish clustering conditions for the absolute errorloss, which is, as far as we know, a novel result. Furthermore, we characterizethe statistical performance of OWL norm regularization for generative models inwhich certain clusters of regression variables are strongly (even perfectly)correlated, but variables in different clusters are uncorrelated. We show thatif the true p-dimensional signal generating the data involves only s of theclusters, then O(s log p) samples suffice to accurately estimate the signal,regardless of the number of coefficients within the clusters. The estimation ofs-sparse signals with completely independent variables requires just as manymeasurements. In other words, using the OWL we pay no price (in terms of thenumber of measurements) for the presence of strongly correlated variables.
arxiv-7200-12 | Polarity detection movie reviews in hindi language | http://arxiv.org/pdf/1409.3942v1.pdf | author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.CL cs.IR published:2014-09-13 summary:Nowadays peoples are actively involved in giving comments and reviews onsocial networking websites and other websites like shopping websites, newswebsites etc. large number of people everyday share their opinion on the web,results is a large number of user data is collected .users also find it trivialtask to read all the reviews and then reached into the decision. It would bebetter if these reviews are classified into some category so that the userfinds it easier to read. Opinion Mining or Sentiment Analysis is a naturallanguage processing task that mines information from various text forms such asreviews, news, and blogs and classify them on the basis of their polarity aspositive, negative or neutral. But, from the last few years, user content inHindi language is also increasing at a rapid rate on the Web. So it is veryimportant to perform opinion mining in Hindi language as well. In this paper aHindi language opinion mining system is proposed. The system classifies thereviews as positive, negative and neutral for Hindi language. Negation is alsohandled in the proposed system. Experimental results using reviews of moviesshow the effectiveness of the system
arxiv-7200-13 | A study on effectiveness of extreme learning machine | http://arxiv.org/pdf/1409.3924v1.pdf | author:Yuguang Wang, Feilong Cao, Yubo Yuan category:cs.NE cs.LG published:2014-09-13 summary:Extreme learning machine (ELM), proposed by Huang et al., has been shown apromising learning algorithm for single-hidden layer feedforward neuralnetworks (SLFNs). Nevertheless, because of the random choice of input weightsand biases, the ELM algorithm sometimes makes the hidden layer output matrix Hof SLFN not full column rank, which lowers the effectiveness of ELM. This paperdiscusses the effectiveness of ELM and proposes an improved algorithm calledEELM that makes a proper selection of the input weights and bias beforecalculating the output weights, which ensures the full column rank of H intheory. This improves to some extend the learning rate (testing accuracy,prediction accuracy, learning time) and the robustness property of thenetworks. The experimental results based on both the benchmark functionapproximation and real-world problems including classification and regressionapplications show the good performances of EELM.
arxiv-7200-14 | Concurrent Tracking of Inliers and Outliers | http://arxiv.org/pdf/1409.3913v1.pdf | author:Jae-Yeong Lee, Wonpil Yu category:cs.CV published:2014-09-13 summary:In object tracking, outlier is one of primary factors which degradeperformance of image-based tracking algorithms. In this respect, therefore,most of the existing methods simply discard detected outliers and pay little orno attention to employing them as an important source of information for motionestimation. We consider outliers as important as inliers for object trackingand propose a motion estimation algorithm based on concurrent tracking ofinliers and outliers. Our tracker makes use of pyramidal implementation of theLucas-Kanade tracker to estimate motion flows of inliers and outliers and finaltarget motion is estimated robustly based on both of these information.Experimental results from challenging benchmark video sequences confirmenhanced tracking performance, showing highly stable target tracking undersevere occlusion compared with state-of-the-art algorithms. The proposedalgorithm runs at more than 100 frames per second even without using a hardwareaccelerator, which makes the proposed method more practical and portable.
arxiv-7200-15 | Parallel Distributed Block Coordinate Descent Methods based on Pairwise Comparison Oracle | http://arxiv.org/pdf/1409.3912v1.pdf | author:Kota Matsui, Wataru Kumagai, Takafumi Kanamori category:stat.ML cs.LG published:2014-09-13 summary:This paper provides a block coordinate descent algorithm to solveunconstrained optimization problems. In our algorithm, computation of functionvalues or gradients is not required. Instead, pairwise comparison of functionvalues is used. Our algorithm consists of two steps; one is the directionestimate step and the other is the search step. Both steps require onlypairwise comparison of function values, which tells us only the order offunction values over two points. In the direction estimate step, a Newton typesearch direction is estimated. A computation method like block coordinatedescent methods is used with the pairwise comparison. In the search step, anumerical solution is updated along the estimated direction. The computation inthe direction estimate step can be easily parallelized, and thus, the algorithmworks efficiently to find the minimizer of the objective function. Also, weshow an upper bound of the convergence rate. In numerical experiments, we showthat our method efficiently finds the optimal solution compared to someexisting methods based on the pairwise comparison.
arxiv-7200-16 | Approximating rational Bezier curves by constrained Bezier curves of arbitrary degree | http://arxiv.org/pdf/1212.3385v4.pdf | author:Mao Shi, Jiansong Deng category:math.NA cs.CV published:2012-12-14 summary:In this paper, we propose a method to obtain a constrained approximation of arational B\'{e}zier curve by a polynomial B\'{e}zier curve. This problem isreformulated as an approximation problem between two polynomial B\'{e}ziercurves based on weighted least-squares method, where weight functions$\rho(t)=\omega(t)$ and $\rho(t)=\omega(t)^{2}$ are studied respectively. Theefficiency of the proposed method is tested using some examples.
arxiv-7200-17 | Structure Preserving Large Imagery Reconstruction | http://arxiv.org/pdf/1409.3906v1.pdf | author:Ju Shen, Jianjun Yang, Sami Taha-abusneineh, Bryson Payne, Markus Hitz category:cs.CV published:2014-09-13 summary:With the explosive growth of web-based cameras and mobile devices, billionsof photographs are uploaded to the internet. We can trivially collect a hugenumber of photo streams for various goals, such as image clustering, 3D scenereconstruction, and other big data applications. However, such tasks are noteasy due to the fact the retrieved photos can have large variations in theirview perspectives, resolutions, lighting, noises, and distortions.Fur-thermore, with the occlusion of unexpected objects like people, vehicles,it is even more challenging to find feature correspondences and reconstructre-alistic scenes. In this paper, we propose a structure-based image completionalgorithm for object removal that produces visually plausible content withconsistent structure and scene texture. We use an edge matching technique toinfer the potential structure of the unknown region. Driven by the estimatedstructure, texture synthesis is performed automatically along the estimatedcurves. We evaluate the proposed method on different types of images: fromhighly structured indoor environment to natural scenes. Our experimentalresults demonstrate satisfactory performance that can be potentially used forsubsequent big data processing, such as image localization, object retrieval,and scene reconstruction. Our experiments show that this approach achievesfavorable results that outperform existing state-of-the-art techniques.
arxiv-7200-18 | An Approach to Reducing Annotation Costs for BioNLP | http://arxiv.org/pdf/1409.3881v1.pdf | author:Michael Bloodgood, K. Vijay-Shanker category:cs.CL cs.LG stat.ML published:2014-09-12 summary:There is a broad range of BioNLP tasks for which active learning (AL) cansignificantly reduce annotation costs and a specific AL algorithm we havedeveloped is particularly effective in reducing annotation costs for thesetasks. We have previously developed an AL algorithm called ClosestInitPA thatworks best with tasks that have the following characteristics: redundancy intraining material, burdensome annotation costs, Support Vector Machines (SVMs)work well for the task, and imbalanced datasets (i.e. when set up as a binaryclassification problem, one class is substantially rarer than the other). ManyBioNLP tasks have these characteristics and thus our AL algorithm is a naturalapproach to apply to BioNLP tasks.
arxiv-7200-19 | Linear, Deterministic, and Order-Invariant Initialization Methods for the K-Means Clustering Algorithm | http://arxiv.org/pdf/1409.3854v1.pdf | author:M. Emre Celebi, Hassan A. Kingravi category:cs.LG cs.CV I.5.3; H.2.8 published:2014-09-12 summary:Over the past five decades, k-means has become the clustering algorithm ofchoice in many application domains primarily due to its simplicity, time/spaceefficiency, and invariance to the ordering of the data points. Unfortunately,the algorithm's sensitivity to the initial selection of the cluster centersremains to be its most serious drawback. Numerous initialization methods havebeen proposed to address this drawback. Many of these methods, however, havetime complexity superlinear in the number of data points, which makes themimpractical for large data sets. On the other hand, linear methods are oftenrandom and/or sensitive to the ordering of the data points. These methods aregenerally unreliable in that the quality of their results is unpredictable.Therefore, it is common practice to perform multiple runs of such methods andtake the output of the run that produces the best results. Such a practice,however, greatly increases the computational requirements of the otherwisehighly efficient k-means algorithm. In this chapter, we investigate theempirical performance of six linear, deterministic (non-random), andorder-invariant k-means initialization methods on a large and diversecollection of data sets from the UCI Machine Learning Repository. The resultsdemonstrate that two relatively unknown hierarchical initialization methods dueto Su and Dy outperform the remaining four methods with respect to twoobjective effectiveness criteria. In addition, a recent method due to Erisogluet al. performs surprisingly poorly.
arxiv-7200-20 | Incorporating Semi-supervised Features into Discontinuous Easy-First Constituent Parsing | http://arxiv.org/pdf/1409.3813v1.pdf | author:Yannick Versley category:cs.CL published:2014-09-12 summary:This paper describes adaptations for EaFi, a parser for easy-first parsing ofdiscontinuous constituents, to adapt it to multiple languages as well as makeuse of the unlabeled data that was provided as part of the SPMRL shared task2014.
arxiv-7200-21 | A Fast Quartet Tree Heuristic for Hierarchical Clustering | http://arxiv.org/pdf/1409.4276v1.pdf | author:Rudi L. Cilibrasi, Paul M. B. Vitanyi category:cs.LG cs.CE cs.DS published:2014-09-12 summary:The Minimum Quartet Tree Cost problem is to construct an optimal weight treefrom the $3{n \choose 4}$ weighted quartet topologies on $n$ objects, whereoptimality means that the summed weight of the embedded quartet topologies isoptimal (so it can be the case that the optimal tree embeds all quartets asnonoptimal topologies). We present a Monte Carlo heuristic, based on randomizedhill climbing, for approximating the optimal weight tree, given the quartettopology weights. The method repeatedly transforms a dendrogram, with allobjects involved as leaves, achieving a monotonic approximation to the exactsingle globally optimal tree. The problem and the solution heuristic has beenextensively used for general hierarchical clustering of nontree-like(non-phylogeny) data in various domains and across domains with heterogeneousdata. We also present a greatly improved heuristic, reducing the running timeby a factor of order a thousand to ten thousand. All this is implemented andavailable, as part of the CompLearn package. We compare performance and runningtime of the original and improved versions with those of UPGMA, BioNJ, and NJ,as implemented in the SplitsTree package on genomic data for which the latterare optimized. Keywords: Data and knowledge visualization, Patternmatching--Clustering--Algorithms/Similarity measures, Hierarchical clustering,Global optimization, Quartet tree, Randomized hill-climbing,
arxiv-7200-22 | Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection | http://arxiv.org/pdf/1409.3768v1.pdf | author:Sang-Yun Oh, Onkar Dalal, Kshitij Khare, Bala Rajaratnam category:stat.CO cs.LG stat.ML published:2014-09-12 summary:Sparse high dimensional graphical model selection is a popular topic incontemporary machine learning. To this end, various useful approaches have beenproposed in the context of $\ell_1$-penalized estimation in the Gaussianframework. Though many of these inverse covariance estimation approaches aredemonstrably scalable and have leveraged recent advances in convexoptimization, they still depend on the Gaussian functional form. To addressthis gap, a convex pseudo-likelihood based partial correlation graph estimationmethod (CONCORD) has been recently proposed. This method uses coordinate-wiseminimization of a regression based pseudo-likelihood, and has been shown tohave robust model selection properties in comparison with the Gaussianapproach. In direct contrast to the parallel work in the Gaussian settinghowever, this new convex pseudo-likelihood framework has not leveraged theextensive array of methods that have been proposed in the machine learningliterature for convex optimization. In this paper, we address this crucial gapby proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) forperforming $\ell_1$-regularized inverse covariance matrix estimation in thepseudo-likelihood framework. We present timing comparisons with coordinate-wiseminimization and demonstrate that our approach yields tremendous payoffs for$\ell_1$-penalized partial correlation graph estimation outside the Gaussiansetting, thus yielding the fastest and most scalable approach for suchproblems. We undertake a theoretical analysis of our approach and rigorouslydemonstrate convergence, and also derive rates thereof.
arxiv-7200-23 | Time-domain multiscale shape identification in electro-sensing | http://arxiv.org/pdf/1409.3714v1.pdf | author:Habib Ammari, Han Wang category:math.NA cs.CV published:2014-09-12 summary:This paper presents premier and innovative time-domain multi-scale method forshape identification in electro-sensing using pulse-type signals. The method isbased on transform-invariant shape descriptors computed from filteredpolarization tensors at multi-scales. The proposed algorithm enjoys aremarkable noise robustness even with far-field measurements at very limitedangle of view. It opens a door for pulsed imaging using echolocation andinduction data.
arxiv-7200-24 | An OvS-MultiObjective Algorithm Approach for Lane Reversal Problem | http://arxiv.org/pdf/1409.4244v1.pdf | author:Enrique Gabriel Baquela, Ana Carolina Olivera category:cs.NE published:2014-09-12 summary:The lane reversal has proven to be a useful method to mitigate trafficcongestion during rush hour or in case of specific events that affect hightraffic volumes. In this work we propose a methodology that is placed withinoptimization via Simulation, by means of which a multi-objective geneticalgorithm and simulations of traffic are used to determine the configuration ofideal lane reversal.
arxiv-7200-25 | An inertial forward-backward algorithm for monotone inclusions | http://arxiv.org/pdf/1403.3522v2.pdf | author:Dirk A. Lorenz, Thomas Pock category:cs.CV cs.NA math.NA math.OC published:2014-03-14 summary:In this paper, we propose an inertial forward backward splitting algorithm tocompute a zero of the sum of two monotone operators, with one of the twooperators being co-coercive. The algorithm is inspired by the acceleratedgradient method of Nesterov, but can be applied to a much larger class ofproblems including convex-concave saddle point problems and general monotoneinclusions. We prove convergence of the algorithm in a Hilbert space settingand show that several recently proposed first-order methods can be obtained asspecial cases of the general algorithm. Numerical results show that theproposed algorithm converges faster than existing methods, while keeping thecomputational cost of each iteration basically unchanged.
arxiv-7200-26 | A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion Systems | http://arxiv.org/pdf/1409.5671v1.pdf | author:Ebru Aydin Gol, Ezio Bartocci, Calin Belta category:cs.AI cs.CE cs.LG cs.LO cs.SY published:2014-09-12 summary:We propose a technique to detect and generate patterns in a network oflocally interacting dynamical systems. Central to our approach is a novelspatial superposition logic, whose semantics is defined over the quad-tree of apartitioned image. We show that formulas in this logic can be efficientlylearned from positive and negative examples of several types of patterns. Wealso demonstrate that pattern detection, which is implemented as a modelchecking algorithm, performs very well for test data sets different from thelearning sets. We define a quantitative semantics for the logic and integratethe model checking algorithm with particle swarm optimization in acomputational framework for synthesis of parameters leading to desired patternsin reaction-diffusion systems.
arxiv-7200-27 | Parsimonious Topic Models with Salient Word Discovery | http://arxiv.org/pdf/1401.6169v2.pdf | author:Hossein Soleimani, David J. Miller category:cs.LG cs.CL cs.IR stat.ML published:2014-01-22 summary:We propose a parsimonious topic model for text corpora. In related modelssuch as Latent Dirichlet Allocation (LDA), all words are modeledtopic-specifically, even though many words occur with similar frequenciesacross different topics. Our modeling determines salient words for each topic,which have topic-specific probabilities, with the rest explained by a universalshared model. Further, in LDA all topics are in principle present in everydocument. By contrast our model gives sparse topic representation, determiningthe (small) subset of relevant topics for each document. We derive a BayesianInformation Criterion (BIC), balancing model complexity and goodness of fit.Here, interestingly, we identify an effective sample size and correspondingpenalty specific to each parameter type in our model. We minimize BIC tojointly determine our entire model -- the topic-specific words,document-specific topics, all model parameter values, {\it and} the totalnumber of topics -- in a wholly unsupervised fashion. Results on three textcorpora and an image dataset show that our model achieves higher test setlikelihood and better agreement with ground-truth class labels, compared to LDAand to a model designed to incorporate sparsity.
arxiv-7200-28 | Analyzing the Language of Food on Social Media | http://arxiv.org/pdf/1409.2195v2.pdf | author:Daniel Fried, Mihai Surdeanu, Stephen Kobourov, Melanie Hingle, Dane Bell category:cs.CL cs.CY cs.SI published:2014-09-08 summary:We investigate the predictive power behind the language of food on socialmedia. We collect a corpus of over three million food-related posts fromTwitter and demonstrate that many latent population characteristics can bedirectly predicted from this data: overweight rate, diabetes rate, politicalleaning, and home geographical location of authors. For all tasks, ourlanguage-based models significantly outperform the majority-class baselines.Performance is further improved with more complex natural language processing,such as topic modeling. We analyze which textual features have most predictivepower for these datasets, providing insight into the connections between thelanguage of food, geographic locale, and community characteristics. Lastly, wedesign and implement an online system for real-time query and visualization ofthe dataset. Visualization tools, such as geo-referenced heatmaps,semantics-preserving wordclouds and temporal histograms, allow us to discovermore complex, global patterns mirrored in the language of food.
arxiv-7200-29 | DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection | http://arxiv.org/pdf/1409.3505v1.pdf | author:Wanli Ouyang, Ping Luo, Xingyu Zeng, Shi Qiu, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang, Yuanjun Xiong, Chen Qian, Zhenyao Zhu, Ruohui Wang, Chen-Change Loy, Xiaogang Wang, Xiaoou Tang category:cs.CV published:2014-09-11 summary:In this paper, we propose multi-stage and deformable deep convolutionalneural networks for object detection. This new deep learning object detectiondiagram has innovations in multiple aspects. In the proposed new deeparchitecture, a new deformation constrained pooling (def-pooling) layer modelsthe deformation of object parts with geometric constraint and penalty. With theproposed multi-stage training strategy, multiple classifiers are jointlyoptimized to process samples at different difficulty levels. A new pre-trainingstrategy is proposed to learn feature representations more suitable for theobject detection task and with good generalization capability. By changing thenet structures, training strategies, adding and removing some key components inthe detection pipeline, a set of models with large diversity are obtained,which significantly improves the effectiveness of modeling averaging. Theproposed approach ranked \#2 in ILSVRC 2014. It improves the mean averagedprecision obtained by RCNN, which is the state-of-the-art of object detection,from $31\%$ to $45\%$. Detailed component-wise analysis is also providedthrough extensive experimental evaluation.
arxiv-7200-30 | Consensus-Based Modelling using Distributed Feature Construction | http://arxiv.org/pdf/1409.3446v1.pdf | author:Haimonti Dutta, Ashwin Srinivasan category:cs.LG published:2014-09-11 summary:A particularly successful role for Inductive Logic Programming (ILP) is as atool for discovering useful relational features for subsequent use in apredictive model. Conceptually, the case for using ILP to construct relationalfeatures rests on treating these features as functions, the automated discoveryof which necessarily requires some form of first-order learning. Practically,there are now several reports in the literature that suggest that augmentingany existing features with ILP-discovered relational features can substantiallyimprove the predictive power of a model. While the approach is straightforwardenough, much still needs to be done to scale it up to explore more fully thespace of possible features that can be constructed by an ILP system. This is inprinciple, infinite and in practice, extremely large. Applications have beenconfined to heuristic or random selections from this space. In this paper, weaddress this computational difficulty by allowing features to be constructed ina distributed manner. That is, there is a network of computational units, eachof which employs an ILP engine to construct some small number of features andthen builds a (local) model. We then employ a consensus-based algorithm, inwhich neighboring nodes share information to update local models. For acategory of models (those with convex loss functions), it can be shown that thealgorithm will result in all nodes converging to a consensus model. Inpractice, it may be slow to achieve this convergence. Nevertheless, our resultson synthetic and real datasets that suggests that in relatively short time the"best" node in the network reaches a model whose predictive accuracy iscomparable to that obtained using more computational effort in anon-distributed setting (the best node is identified as the one whose weightsconverge first).
arxiv-7200-31 | Selection of Most Appropriate Backpropagation Training Algorithm in Data Pattern Recognition | http://arxiv.org/pdf/1409.4727v1.pdf | author:Hindayati Mustafidah, Sri Hartati, Retantyo Wardoyo, Agus Harjoko category:cs.NE published:2014-09-11 summary:There are several training algorithms for backpropagation method in neuralnetwork. Not all of these algorithms have the same accuracy level demonstratedthrough the percentage level of suitability in recognizing patterns in thedata. In this research tested 12 training algorithms specifically in recognizedata patterns of test validity. The basic network parameters used are themaximum allowable epoch = 1000, target error = 10-3, and learning rate = 0.05.Of the twelve training algorithms each performed 20 times looping. The testresults obtained that the percentage rate of the great match is trainlmalgorithm with alpha 5% have adequate levels of suitability of 87.5% at thelevel of significance of 0.000. This means the most appropriate trainingalgorithm in recognizing the the data pattern of test validity is the trainlmalgorithm.
arxiv-7200-32 | Building Program Vector Representations for Deep Learning | http://arxiv.org/pdf/1409.3358v1.pdf | author:Lili Mou, Ge Li, Yuxuan Liu, Hao Peng, Zhi Jin, Yan Xu, Lu Zhang category:cs.SE cs.LG cs.NE published:2014-09-11 summary:Deep learning has made significant breakthroughs in various fields ofartificial intelligence. Advantages of deep learning include the ability tocapture highly complicated features, weak involvement of human engineering,etc. However, it is still virtually impossible to use deep learning to analyzeprograms since deep architectures cannot be trained effectively with pure backpropagation. In this pioneering paper, we propose the "coding criterion" tobuild program vector representations, which are the premise of deep learningfor program analysis. Our representation learning approach directly makes deeplearning a reality in this new field. We evaluate the learned vectorrepresentations both qualitatively and quantitatively. We conclude, based onthe experiments, the coding criterion is successful in building programrepresentations. To evaluate whether deep learning is beneficial for programanalysis, we feed the representations to deep neural networks, and achievehigher accuracy in the program classification task than "shallow" methods, suchas logistic regression and the support vector machine. This result confirms thefeasibility of deep learning to analyze programs. It also gives primaryevidence of its success in this new field. We believe deep learning will becomean outstanding technique for program analysis in the near future.
arxiv-7200-33 | Tyler's Covariance Matrix Estimator in Elliptical Models with Convex Structure | http://arxiv.org/pdf/1404.1935v2.pdf | author:Ilya Soloveychik, Ami Wiesel category:stat.ML published:2014-04-07 summary:We address structured covariance estimation in elliptical distributions byassuming that the covariance is a priori known to belong to a given convex set,e.g., the set of Toeplitz or banded matrices. We consider the General Method ofMoments (GMM) optimization applied to robust Tyler's scatter M-estimatorsubject to these convex constraints. Unfortunately, GMM turns out to benon-convex due to the objective. Instead, we propose a new COCA estimator - aconvex relaxation which can be efficiently solved. We prove that the relaxationis tight in the unconstrained case for a finite number of samples, and in theconstrained case asymptotically. We then illustrate the advantages of COCA insynthetic simulations with structured compound Gaussian distributions. In theseexamples, COCA outperforms competing methods such as Tyler's estimator and itsprojection onto the structure set.
arxiv-7200-34 | Intelligent Indoor Mobile Robot Navigation Using Stereo Vision | http://arxiv.org/pdf/1412.6153v1.pdf | author:Arjun B. Krishnan, Jayaram Kollipara category:cs.RO cs.AI cs.CV published:2014-09-10 summary:Majority of the existing robot navigation systems, which facilitate the useof laser range finders, sonar sensors or artificial landmarks, has the abilityto locate itself in an unknown environment and then build a map of thecorresponding environment. Stereo vision, while still being a rapidlydeveloping technique in the field of autonomous mobile robots, are currentlyless preferable due to its high implementation cost. This paper aims atdescribing an experimental approach for the building of a stereo vision systemthat helps the robots to avoid obstacles and navigate through indoorenvironments and at the same time remaining very much cost effective. Thispaper discusses the fusion techniques of stereo vision and ultrasound sensorswhich helps in the successful navigation through different types of complexenvironments. The data from the sensor enables the robot to create the twodimensional topological map of unknown environments and stereo vision systemsmodels the three dimension model of the same environment.
arxiv-7200-35 | Word Sense Disambiguation using WSD specific Wordnet of Polysemy Words | http://arxiv.org/pdf/1409.3512v1.pdf | author:Udaya Raj Dhungana, Subarna Shakya, Kabita Baral, Bharat Sharma category:cs.CL published:2014-09-10 summary:This paper presents a new model of WordNet that is used to disambiguate thecorrect sense of polysemy word based on the clue words. The related words foreach sense of a polysemy word as well as single sense word are referred to asthe clue words. The conventional WordNet organizes nouns, verbs, adjectives andadverbs together into sets of synonyms called synsets each expressing adifferent concept. In contrast to the structure of WordNet, we developed a newmodel of WordNet that organizes the different senses of polysemy words as wellas the single sense words based on the clue words. These clue words for eachsense of a polysemy word as well as for single sense word are used todisambiguate the correct meaning of the polysemy word in the given contextusing knowledge based Word Sense Disambiguation (WSD) algorithms. The clue wordcan be a noun, verb, adjective or adverb.
arxiv-7200-36 | Metric Learning for Temporal Sequence Alignment | http://arxiv.org/pdf/1409.3136v1.pdf | author:Damien Garreau, Rémi Lajugie, Sylvain Arlot, Francis Bach category:cs.LG published:2014-09-10 summary:In this paper, we propose to learn a Mahalanobis distance to performalignment of multivariate time series. The learning examples for this task aretime series for which the true alignment is known. We cast the alignmentproblem as a structured prediction task, and propose realistic losses betweenalignments for which the optimization is tractable. We provide experiments onreal data in the audio to audio context, where we show that the learning of asimilarity measure leads to improvements in the performance of the alignmenttask. We also propose to use this metric learning framework to perform featureselection and, from basic audio features, build a combination of these withbetter performance for the alignment.
arxiv-7200-37 | An improved genetic algorithm with a local optimization strategy and an extra mutation level for solving traveling salesman problem | http://arxiv.org/pdf/1409.3078v1.pdf | author:Keivan Borna, Vahid Haji Hashemi category:cs.NE published:2014-09-10 summary:The Traveling salesman problem (TSP) is proved to be NP-complete in mostcases. The genetic algorithm (GA) is one of the most useful algorithms forsolving this problem. In this paper a conventional GA is compared with animproved hybrid GA in solving TSP. The improved or hybrid GA consist ofconventional GA and two local optimization strategies. The first strategy isextracting all sequential groups including four cities of samples and changingthe two central cities with each other. The second local optimization strategyis similar to an extra mutation process. In this step with a low probability asample is selected. In this sample two random cities are defined and the pathbetween these cities is reversed. The computation results show that theproposed method also finds better paths than the conventional GA within anacceptable computation time.
arxiv-7200-38 | Image Denoising using New Adaptive Based Median Filters | http://arxiv.org/pdf/1410.2175v1.pdf | author:Suman Shrestha category:cs.CV published:2014-09-10 summary:Noise is a major issue while transferring images through all kinds ofelectronic communication. One of the most common noise in electroniccommunication is an impulse noise which is caused by unstable voltage. In thispaper, the comparison of known image denoising techniques is discussed and anew technique using the decision based approach has been used for the removalof impulse noise. All these methods can primarily preserve image details whilesuppressing impulsive noise. The principle of these techniques is at firstintroduced and then analysed with various simulation results using MATLAB. Mostof the previously known techniques are applicable for the denoising of imagescorrupted with less noise density. Here a new decision based technique has beenpresented which shows better performances than those already being used. Thecomparisons are made based on visual appreciation and further quantitatively byMean Square error (MSE) and Peak Signal to Noise Ratio (PSNR) of differentfiltered images..
arxiv-7200-39 | One-Dimensional Vector based Pattern Matching | http://arxiv.org/pdf/1409.3024v1.pdf | author:Y. M. Fouda category:cs.CV published:2014-09-10 summary:Template matching is a basic method in image analysis to extract usefulinformation from images. In this paper, we suggest a new method for patternmatching. Our method transform the template image from two dimensional imageinto one dimensional vector. Also all sub-windows (same size of template) inthe reference image will transform into one dimensional vectors. The threesimilarity measures SAD, SSD, and Euclidean are used to compute the likenessbetween template and all sub-windows in the reference image to find the bestmatch. The experimental results show the superior performance of the proposedmethod over the conventional methods on various template of different sizes.
arxiv-7200-40 | A Study of Association Measures and their Combination for Arabic MWT Extraction | http://arxiv.org/pdf/1409.3005v1.pdf | author:Abdelkader El Mahdaouy, Saïd EL Alaoui Ouatik, Eric Gaussier category:cs.CL published:2014-09-10 summary:Automatic Multi-Word Term (MWT) extraction is a very important issue to manyapplications, such as information retrieval, question answering, and textcategorization. Although many methods have been used for MWT extraction inEnglish and other European languages, few studies have been applied to Arabic.In this paper, we propose a novel, hybrid method which combines linguistic andstatistical approaches for Arabic Multi-Word Term extraction. The maincontribution of our method is to consider contextual information and bothtermhood and unithood for association measures at the statistical filteringstep. In addition, our technique takes into account the problem of MWTvariation in the linguistic filtering step. The performance of the proposedstatistical measure (NLC-value) is evaluated using an Arabic environment corpusby comparing it with some existing competitors. Experimental results show thatour NLC-value measure outperforms the other ones in term of precision for bothbi-grams and tri-grams.
arxiv-7200-41 | Scalable Bayesian Modelling of Paired Symbols | http://arxiv.org/pdf/1409.2824v2.pdf | author:Ulrich Paquet, Noam Koenigstein, Ole Winther category:stat.ML published:2014-09-09 summary:We present a novel, scalable and Bayesian approach to modelling theoccurrence of pairs of symbols (i,j) drawn from a large vocabulary. Observedpairs are assumed to be generated by a simple popularity based selectionprocess followed by censoring using a preference function. By basing inferenceon the well-founded principle of variational bounding, and using newsite-independent bounds, we show how a scalable inference procedure can beobtained for large data sets. State of the art results are presented onreal-world movie viewing data.
arxiv-7200-42 | "Look Ma, No Hands!" A Parameter-Free Topic Model | http://arxiv.org/pdf/1409.2993v1.pdf | author:Jian Tang, Ming Zhang, Qiaozhu Mei category:cs.LG cs.CL cs.IR published:2014-09-10 summary:It has always been a burden to the users of statistical topic models topredetermine the right number of topics, which is a key parameter of most topicmodels. Conventionally, automatic selection of this parameter is done througheither statistical model selection (e.g., cross-validation, AIC, or BIC) orBayesian nonparametric models (e.g., hierarchical Dirichlet process). Thesemethods either rely on repeated runs of the inference algorithm to searchthrough a large range of parameter values which does not suit the mining of bigdata, or replace this parameter with alternative parameters that are lessintuitive and still hard to be determined. In this paper, we explore to"eliminate" this parameter from a new perspective. We first present anonparametric treatment of the PLSA model named nonparametric probabilisticlatent semantic analysis (nPLSA). The inference procedure of nPLSA allows forthe exploration and comparison of different numbers of topics within a singleexecution, yet remains as simple as that of PLSA. This is achieved bysubstituting the parameter of the number of topics with an alternativeparameter that is the minimal goodness of fit of a document. We show that thenew parameter can be further eliminated by two parameter-free treatments:either by monitoring the diversity among the discovered topics or by a weaksupervision from users in the form of an exemplar topic. The parameter-freetopic model finds the appropriate number of topics when the diversity among thediscovered topics is maximized, or when the granularity of the discoveredtopics matches the exemplar topic. Experiments on both synthetic and real dataprove that the parameter-free topic model extracts topics with a comparablequality comparing to classical topic models with "manual transmission". Thequality of the topics outperforms those extracted through classical Bayesiannonparametric models.
arxiv-7200-43 | Quantum Edge Detection for Image Segmentation in Optical Environments | http://arxiv.org/pdf/1409.2918v1.pdf | author:Mario Mastriani category:cs.CV published:2014-09-09 summary:A quantum edge detector for image segmentation in optical environments ispresented in this work. A Boolean version of the same detector is presentedtoo. The quantum version of the new edge detector works with computationalbasis states, exclusively. This way, we can easily avoid the problem of quantummeasurement retrieving the result of applying the new detector on the image.Besides, a new criterion and logic based on projections onto vertical axis ofBloch's Sphere exclusively are presented too. This approach will allow us: 1) asimpler development of logic quantum operations, where they will closer tothose used in the classical logic operations, 2) building simple and robustclassical-to-quantum and quantum-to-classical interfaces. Said so far isextended to quantum algorithms outside image processing too. In a specialsection on metric and simulations, a new metric based on the comparison betweenthe classical and quantum versions algorithms for edge detection of images ispresented. Notable differences between the results of classical and quantumversions of such algorithms (outside and inside of quantum computer,respectively) show the existence of implementation problems involved in theexperiment, and that they have not been properly modeled for opticalenvironments. However, although they are different, the quantum results areequally valid. The latter is clearly seen in the computer simulations
arxiv-7200-44 | Non-Convex Boosting Overcomes Random Label Noise | http://arxiv.org/pdf/1409.2905v1.pdf | author:Sunsern Cheamanunkul, Evan Ettinger, Yoav Freund category:cs.LG published:2014-09-09 summary:The sensitivity of Adaboost to random label noise is a well-studied problem.LogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to beless sensitive to noise than AdaBoost. We present the results of experimentsevaluating these algorithms on both synthetic and real datasets. We compare theperformance on each of datasets when the labels are corrupted by differentlevels of independent label noise. In presence of random label noise, we foundthat BrownBoost and RobustBoost perform significantly better than AdaBoost andLogitBoost, while the difference between each pair of algorithms isinsignificant. We provide an explanation for the difference based on the margindistributions of the algorithms.
arxiv-7200-45 | Ambiguity-Driven Fuzzy C-Means Clustering: How to Detect Uncertain Clustered Records | http://arxiv.org/pdf/1409.2821v1.pdf | author:Meysam Ghaffari, Nasser Ghadiri category:cs.AI cs.CV published:2014-09-09 summary:As a well-known clustering algorithm, Fuzzy C-Means (FCM) allows each inputsample to belong to more than one cluster, providing more flexibility thannon-fuzzy clustering methods. However, the accuracy of FCM is subject to falsedetections caused by noisy records, weak feature selection and low certainty ofthe algorithm in some cases. The false detections are very important in somedecision-making application domains like network security and medicaldiagnosis, where weak decisions based on such false detections may lead tocatastrophic outcomes. They are mainly emerged from making decisions about asubset of records that do not provide enough evidence to make a good decision.In this paper, we propose a method for detecting such ambiguous records in FCMby introducing a certainty factor to decrease invalid detections. This approachenables us to send the detected ambiguous records to another discriminationmethod for a deeper investigation, thus increasing the accuracy by lowering theerror rate. Most of the records are still processed quickly and with low errorrate which prevents performance loss compared to similar hybrid methods.Experimental results of applying the proposed method on several datasets fromdifferent domains show a significant decrease in error rate as well as improvedsensitivity of the algorithm.
arxiv-7200-46 | Enforcing Label and Intensity Consistency for IR Target Detection | http://arxiv.org/pdf/1409.2800v1.pdf | author:Toufiq Parag category:cs.CV published:2014-09-09 summary:This study formulates the IR target detection as a binary classificationproblem of each pixel. Each pixel is associated with a label which indicateswhether it is a target or background pixel. The optimal label set for all thepixels of an image maximizes aposteriori distribution of label configurationgiven the pixel intensities. The posterior probability is factored into (orproportional to) a conditional likelihood of the intensity values and a priorprobability of label configuration. Each of these two probabilities arecomputed assuming a Markov Random Field (MRF) on both pixel intensities andtheir labels. In particular, this study enforces neighborhood dependency onboth intensity values, by a Simultaneous Auto Regressive (SAR) model, and onlabels, by an Auto-Logistic model. The parameters of these MRF models arelearned from labeled examples. During testing, an MRF inference technique,namely Iterated Conditional Mode (ICM), produces the optimal label for eachpixel. The detection performance is further improved by incorporating temporalinformation through background subtraction. High performances on benchmarkdatasets demonstrate effectiveness of this method for IR target detection.
arxiv-7200-47 | Short-term plasticity as cause-effect hypothesis testing in distal reward learning | http://arxiv.org/pdf/1402.0710v5.pdf | author:Andrea Soltoggio category:cs.NE q-bio.NC published:2014-02-04 summary:Asynchrony, overlaps and delays in sensory-motor signals introduce ambiguityas to which stimuli, actions, and rewards are causally related. Only therepetition of reward episodes helps distinguish true cause-effect relationshipsfrom coincidental occurrences. In the model proposed here, a novel plasticityrule employs short and long-term changes to evaluate hypotheses on cause-effectrelationships. Transient weights represent hypotheses that are consolidated inlong-term memory only when they consistently predict or cause future rewards.The main objective of the model is to preserve existing network topologies whenlearning with ambiguous information flows. Learning is also improved by biasingthe exploration of the stimulus-response space towards actions that in the pastoccurred before rewards. The model indicates under which conditions beliefs canbe consolidated in long-term memory, it suggests a solution to theplasticity-stability dilemma, and proposes an interpretation of the role ofshort-term plasticity.
arxiv-7200-48 | Automatic Dimension Selection for a Non-negative Factorization Approach to Clustering Multiple Random Graphs | http://arxiv.org/pdf/1406.6315v2.pdf | author:Nam H. Lee, I-Jeng Wang, Youngser Park, Care E. Priebe, Michael Rosen category:stat.ML published:2014-06-24 summary:We consider a problem of grouping multiple graphs into several clusters usingsingular value thesholding and non-negative factorization. We derive a modelselection information criterion to estimate the number of clusters. Wedemonstrate our approach using "Swimmer data set" as well as simulated dataset, and compare its performance with two standard clustering algorithms.
arxiv-7200-49 | Context-specific independence in graphical log-linear models | http://arxiv.org/pdf/1409.2713v1.pdf | author:Henrik Nyman, Johan Pensar, Timo Koski, Jukka Corander category:stat.ML published:2014-09-09 summary:Log-linear models are the popular workhorses of analyzing contingency tables.A log-linear parameterization of an interaction model can be more expressivethan a direct parameterization based on probabilities, leading to a powerfulway of defining restrictions derived from marginal, conditional andcontext-specific independence. However, parameter estimation is often simplerunder a direct parameterization, provided that the model enjoys certaindecomposability properties. Here we introduce a cyclical projection algorithmfor obtaining maximum likelihood estimates of log-linear parameters under anarbitrary context-specific graphical log-linear model, which needs not satisfycriteria of decomposability. We illustrate that lifting the restriction ofdecomposability makes the models more expressive, such that additionalcontext-specific independencies embedded in real data can be identified. It isalso shown how a context-specific graphical model can correspond to anon-hierarchical log-linear parameterization with a concise interpretation.This observation can pave way to further development of non-hierarchicallog-linear models, which have been largely neglected due to their believed lackof interpretability.
arxiv-7200-50 | eAnt-Miner : An Ensemble Ant-Miner to Improve the ACO Classification | http://arxiv.org/pdf/1409.2710v1.pdf | author:Gopinath Chennupati category:cs.NE published:2014-09-09 summary:Ant Colony Optimization (ACO) has been applied in supervised learning inorder to induce classification rules as well as decision trees, namedAnt-Miners. Although these are competitive classifiers, the stability of theseclassifiers is an important concern that owes to their stochastic nature. Inthis paper, to address this issue, an acclaimed machine learning techniquenamed, ensemble of classifiers is applied, where an ACO classifier is used as abase classifier to prepare the ensemble. The main trade-off is, the predictionsin the new approach are determined by discovering a group of models as opposedto the single model classification. In essence, we prepare multiple models fromthe randomly replaced samples of training data from which, a unique model isprepared by aggregating the models to test the unseen data points. The mainobjective of this new approach is to increase the stability of the Ant-Minerresults there by improving the performance of ACO classification. We found thatthe ensemble Ant-Miners significantly improved the stability by reducing theclassification error on unseen data.
arxiv-7200-51 | F-formation Detection: Individuating Free-standing Conversational Groups in Images | http://arxiv.org/pdf/1409.2702v1.pdf | author:Francesco Setti, Chris Russell, Chiara Bassetti, Marco Cristani category:cs.CV published:2014-09-09 summary:Detection of groups of interacting people is a very interesting and usefultask in many modern technologies, with application fields spanning fromvideo-surveillance to social robotics. In this paper we first furnish arigorous definition of group considering the background of the social sciences:this allows us to specify many kinds of group, so far neglected in the ComputerVision literature. On top of this taxonomy, we present a detailed state of theart on the group detection algorithms. Then, as a main contribution, we presenta brand new method for the automatic detection of groups in still images, whichis based on a graph-cuts framework for clustering individuals; in particular weare able to codify in a computational sense the sociological definition ofF-formation, that is very useful to encode a group having only proxemicinformation: position and orientation of people. We call the proposed methodGraph-Cuts for F-formation (GCFF). We show how GCFF definitely outperforms allthe state of the art methods in terms of different accuracy measures (some ofthem are brand new), demonstrating also a strong robustness to noise andversatility in recognizing groups of various cardinality.
arxiv-7200-52 | A theoretical contribution to the fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices | http://arxiv.org/pdf/1409.2579v1.pdf | author:Ting-ting Feng, Gang Wu category:cs.NA cs.CV cs.LG published:2014-09-09 summary:The null linear discriminant analysis method is a competitive approach fordimensionality reduction. The implementation of this method, however, iscomputationally expensive. Recently, a fast implementation of null lineardiscriminant analysis method using random matrix multiplication with scattermatrices was proposed. However, if the random matrix is chosen arbitrarily, theorientation matrix may be rank deficient, and some useful discriminantinformation will be lost. In this paper, we investigate how to choose therandom matrix properly, such that the two criteria of the null LDA method aresatisfied theoretically. We give a necessary and sufficient condition toguarantee full column rank of the orientation matrix. Moreover, the geometriccharacterization of the condition is also described.
arxiv-7200-53 | Combining the analytical hierarchy process and the genetic algorithm to solve the timetable problem | http://arxiv.org/pdf/1409.2650v1.pdf | author:Ihab Sbeity, Mohamed Dbouk, Habib Kobeissi category:cs.AI cs.NE published:2014-09-09 summary:The main problems of school course timetabling are time, curriculum, andclassrooms. In addition there are other problems that vary from one institutionto another. This paper is intended to solve the problem of satisfying theteachers preferred schedule in a way that regards the importance of the teacherto the supervising institute, i.e. his score according to some criteria.Genetic algorithm (GA) has been presented as an elegant method in solvingtimetable problem (TTP) in order to produce solutions with no conflict. In thispaper, we consider the analytic hierarchy process (AHP) to efficiently obtain ascore for each teacher, and consequently produce a GA-based TTP solution thatsatisfies most of the teachers preferences.
arxiv-7200-54 | Comparing Feature Detectors: A bias in the repeatability criteria, and how to correct it | http://arxiv.org/pdf/1409.2465v2.pdf | author:Ives Rey-Otero, Mauricio Delbracio, Jean-Michel Morel category:cs.CV published:2014-09-08 summary:Most computer vision application rely on algorithms finding localcorrespondences between different images. These algorithms detect and comparestable local invariant descriptors centered at scale-invariant keypoints.Because of the importance of the problem, new keypoint detectors anddescriptors are constantly being proposed, each one claiming to perform better(or to be complementary) to the preceding ones. This raises the question of afair comparison between very diverse methods. This evaluation has been mainlybased on a repeatability criterion of the keypoints under a series of imageperturbations (blur, illumination, noise, rotations, homotheties, homographies,etc). In this paper, we argue that the classic repeatability criterion isbiased towards algorithms producing redundant overlapped detections. Tocompensate this bias, we propose a variant of the repeatability rate takinginto account the descriptors overlap. We apply this variant to revisit thepopular benchmark by Mikolajczyk et al., on classic and new feature detectors.Experimental evidence shows that the hierarchy of these feature detectors isseverely disrupted by the amended comparator.
arxiv-7200-55 | Learning Machines Implemented on Non-Deterministic Hardware | http://arxiv.org/pdf/1409.2620v1.pdf | author:Suyog Gupta, Vikas Sindhwani, Kailash Gopalakrishnan category:cs.LG stat.ML published:2014-09-09 summary:This paper highlights new opportunities for designing large-scale machinelearning systems as a consequence of blurring traditional boundaries that haveallowed algorithm designers and application-level practitioners to stay -- forthe most part -- oblivious to the details of the underlying hardware-levelimplementations. The hardware/software co-design methodology advocated herehinges on the deployment of compute-intensive machine learning kernels ontocompute platforms that trade-off determinism in the computation for improvementin speed and/or energy efficiency. To achieve this, we revisit digitalstochastic circuits for approximating matrix computations that are ubiquitousin machine learning algorithms. Theoretical and empirical evaluation isundertaken to assess the impact of the hardware-induced computational noise onalgorithm performance. As a proof-of-concept, a stochastic hardware simulatoris employed for training deep neural networks for image recognition problems.
arxiv-7200-56 | Multi-scale Orderless Pooling of Deep Convolutional Activation Features | http://arxiv.org/pdf/1403.1840v3.pdf | author:Yunchao Gong, Liwei Wang, Ruiqi Guo, Svetlana Lazebnik category:cs.CV published:2014-03-07 summary:Deep convolutional neural networks (CNN) have shown their promise as auniversal representation for recognition. However, global CNN activations lackgeometric invariance, which limits their robustness for classification andmatching of highly variable scenes. To improve the invariance of CNNactivations without degrading their discriminative power, this paper presents asimple but effective scheme called multi-scale orderless pooling (MOP-CNN).This scheme extracts CNN activations for local patches at multiple scalelevels, performs orderless VLAD pooling of these activations at each levelseparately, and concatenates the result. The resulting MOP-CNN representationcan be used as a generic feature for either supervised or unsupervisedrecognition tasks, from image classification to instance-level retrieval; itconsistently outperforms global CNN activations without requiring any jointtraining of prediction layers for a particular target dataset. In absoluteterms, it achieves state-of-the-art results on the challenging SUN397 and MITIndoor Scenes classification datasets, and competitive results onILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.
arxiv-7200-57 | Exploiting Social Network Structure for Person-to-Person Sentiment Analysis | http://arxiv.org/pdf/1409.2450v1.pdf | author:Robert West, Hristo S. Paskov, Jure Leskovec, Christopher Potts category:cs.SI cs.CL physics.soc-ph published:2014-09-08 summary:Person-to-person evaluations are prevalent in all kinds of discourse andimportant for establishing reputations, building social bonds, and shapingpublic opinion. Such evaluations can be analyzed separately using signed socialnetworks and textual sentiment analysis, but this misses the rich interactionsbetween language and social context. To capture such interactions, we develop amodel that predicts individual A's opinion of individual B by synthesizinginformation from the signed social network in which A and B are embedded withsentiment analysis of the evaluative texts relating A to B. We prove that thisproblem is NP-hard but can be relaxed to an efficiently solvable hinge-lossMarkov random field, and we show that this implementation outperforms text-onlyand network-only versions in two very different datasets involvingcommunity-level decision-making: the Wikipedia Requests for Adminship corpusand the Convote U.S. Congressional speech corpus.
arxiv-7200-58 | Approximating solution structure of the Weighted Sentence Alignment problem | http://arxiv.org/pdf/1409.2433v1.pdf | author:Antonina Kolokolova, Renesa Nizamee category:cs.CL cs.CC cs.DS published:2014-09-08 summary:We study the complexity of approximating solution structure of the bijectiveweighted sentence alignment problem of DeNero and Klein (2008). In particular,we consider the complexity of finding an alignment that has a significantoverlap with an optimal alignment. We discuss ways of representing the solutionfor the general weighted sentence alignment as well as phrases-to-wordsalignment problem, and show that computing a string which agrees with theoptimal sentence partition on more than half (plus an arbitrarily smallpolynomial fraction) positions for the phrases-to-words alignment is NP-hard.For the general weighted sentence alignment we obtain such bound from theagreement on a little over 2/3 of the bits. Additionally, we generalize theHamming distance approximation of a solution structure to approximating it withrespect to the edit distance metric, obtaining similar lower bounds.
arxiv-7200-59 | Bayesian Discovery of Threat Networks | http://arxiv.org/pdf/1311.5552v3.pdf | author:Steven T. Smith, Edward K. Kao, Kenneth D. Senne, Garrett Bernstein, Scott Philips category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH published:2013-11-21 summary:A novel unified Bayesian framework for network detection is developed, underwhich a detection algorithm is derived based on random walks on graphs. Thealgorithm detects threat networks using partial observations of their activity,and is proved to be optimum in the Neyman-Pearson sense. The algorithm isdefined by a graph, at least one observation, and a diffusion model for threat.A link to well-known spectral detection methods is provided, and theequivalence of the random walk and harmonic solutions to the Bayesianformulation is proven. A general diffusion model is introduced that utilizesspatio-temporal relationships between vertices, and is used for a specificspace-time formulation that leads to significant performance improvements oncoordinated covert networks. This performance is demonstrated using a newhybrid mixed-membership blockmodel introduced to simulate random covertnetworks with realistic properties.
arxiv-7200-60 | Symbolic regression of generative network models | http://arxiv.org/pdf/1409.2390v1.pdf | author:Telmo Menezes, Camille Roth category:cs.NE cs.SI physics.soc-ph published:2014-09-08 summary:Networks are a powerful abstraction with applicability to a variety ofscientific fields. Models explaining their morphology and growth processespermit a wide range of phenomena to be more systematically analysed andunderstood. At the same time, creating such models is often challenging andrequires insights that may be counter-intuitive. Yet there currently exists nogeneral method to arrive at better models. We have developed an approach toautomatically detect realistic decentralised network growth models fromempirical data, employing a machine learning technique inspired by naturalselection and defining a unified formalism to describe such models as computerprograms. As the proposed method is completely general and does not assume anypre-existing models, it can be applied "out of the box" to any given network.To validate our approach empirically, we systematically rediscover pre-definedgrowth laws underlying several canonical network generation models and crediblelaws for diverse real-world networks. We were able to find programs that aresimple enough to lead to an actual understanding of the mechanisms proposed,namely for a simple brain and a social network.
arxiv-7200-61 | Variational Inference for Uncertainty on the Inputs of Gaussian Process Models | http://arxiv.org/pdf/1409.2287v1.pdf | author:Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence category:stat.ML cs.AI cs.CV cs.LG published:2014-09-08 summary:The Gaussian process latent variable model (GP-LVM) provides a flexibleapproach for non-linear dimensionality reduction that has been widely applied.However, the current approach for training GP-LVMs is based on maximumlikelihood, where the latent projection variables are maximized over ratherthan integrated out. In this paper we present a Bayesian method for trainingGP-LVMs by introducing a non-standard variational inference framework thatallows to approximately integrate out the latent variables and subsequentlytrain a GP-LVM by maximizing an analytic lower bound on the exact marginallikelihood. We apply this method for learning a GP-LVM from iid observationsand for learning non-linear dynamical systems where the observations aretemporally correlated. We show that a benefit of the variational Bayesianprocedure is its robustness to overfitting and its ability to automaticallyselect the dimensionality of the nonlinear latent space. The resultingframework is generic, flexible and easy to extend for other purposes, such asGaussian process regression with uncertain inputs and semi-supervised Gaussianprocesses. We demonstrate our method on synthetic data and standard machinelearning benchmarks, as well as challenging real world datasets, including highresolution video data.
arxiv-7200-62 | Spectral Clustering of Graphs with the Bethe Hessian | http://arxiv.org/pdf/1406.1880v2.pdf | author:Alaa Saade, Florent Krzakala, Lenka Zdeborová category:cs.SI physics.soc-ph stat.ML published:2014-06-07 summary:Spectral clustering is a standard approach to label nodes on a graph bystudying the (largest or lowest) eigenvalues of a symmetric real matrix such ase.g. the adjacency or the Laplacian. Recently, it has been argued that usinginstead a more complicated, non-symmetric and higher dimensional operator,related to the non-backtracking walk on the graph, leads to improvedperformance in detecting clusters, and even to optimal performance for thestochastic block model. Here, we propose to use instead a simpler object, asymmetric real matrix known as the Bethe Hessian operator, or deformedLaplacian. We show that this approach combines the performances of thenon-backtracking operator, thus detecting clusters all the way down to thetheoretical limit in the stochastic block model, with the computational,theoretical and memory advantages of real symmetric matrices.
arxiv-7200-63 | When coding meets ranking: A joint framework based on local learning | http://arxiv.org/pdf/1409.2232v1.pdf | author:Jim Jing-Yan Wang category:cs.CV cs.LG stat.ML published:2014-09-08 summary:Sparse coding, which represents a data point as a s- parse reconstructioncode with regard to a dictionary, has been a popular data representationmethod. Meanwhile, in database retrieval problems, learn the ranking scoresfrom data points plays an important role. Up to new, these two methods havealways been used individually, assuming that data coding and ranking are twoindependent and irrele- vant problems. However, is there any internalrelationship between sparse coding and ranking score learning? If yes, how toexplore this internal relationship? In this paper, we try to answer thesequestions by developing the first join- t sparse coding and ranking scorelearning algorithm. To explore the local distribution in the sparse code space,and also to bridgecoding and rankingproblems, we assume that in theneighborhood of each data points, the ranking scores can be approximated fromthe corresponding sparse codes by a local linear function. By considering thelocal approx- imation error of ranking scores, reconstruction error andsparsity of sparse coding, and the query information pro- vided by the user, weconstruct an unified objective func- tion for learning of sparse codes,dictionary and rankings scores. An iterative algorithm is developed to optimizethe objective function to jointly learn the sparse codes, dictio- nary andrankings scores.
arxiv-7200-64 | Real Time Fabric Defect Detection System on an Embedded DSP Platform | http://arxiv.org/pdf/1410.0371v1.pdf | author:J. L. Raheja, B. Ajay, Ankit Chaudhary category:cs.CV published:2014-09-08 summary:In industrial fabric productions, automated real time systems are needed tofind out the minor defects. It will save the cost by not transporting defectedproducts and also would help in making compmay image of quality fabrics bysending out only undefected products. A real time fabric defect detectionsystem (FDDS), implementd on an embedded DSP platform is presented here.Textural features of fabric image are extracted based on gray levelco-occurrence matrix (GLCM). A sliding window technique is used for defectdetection where window moves over the whole image computing a textural energyfrom the GLCM of the fabric image. The energy values are compared to areference and the deviations beyond a threshold are reported as defects andalso visually represented by a window. The implementation is carried out on aTI TMS320DM642 platform and programmed using code composer studio software. Thereal time output of this implementation was shown on a monitor.
arxiv-7200-65 | The Large Margin Mechanism for Differentially Private Maximization | http://arxiv.org/pdf/1409.2177v1.pdf | author:Kamalika Chaudhuri, Daniel Hsu, Shuang Song category:cs.LG cs.DS cs.IT math.IT math.ST stat.TH published:2014-09-07 summary:A basic problem in the design of privacy-preserving algorithms is the privatemaximization problem: the goal is to pick an item from a universe that(approximately) maximizes a data-dependent function, all under the constraintof differential privacy. This problem has been used as a sub-routine in manyprivacy-preserving algorithms for statistics and machine-learning. Previous algorithms for this problem are either range-dependent---i.e., theirutility diminishes with the size of the universe---or only apply to veryrestricted function classes. This work provides the first general-purpose,range-independent algorithm for private maximization that guaranteesapproximate differential privacy. Its applicability is demonstrated on twofundamental tasks in data mining and machine learning.
arxiv-7200-66 | A Computational Model of the Short-Cut Rule for 2D Shape Decomposition | http://arxiv.org/pdf/1409.2104v1.pdf | author:Lei Luo, Chunhua Shen, Xinwang Liu, Chunyuan Zhang category:cs.CV published:2014-09-07 summary:We propose a new 2D shape decomposition method based on the short-cut rule.The short-cut rule originates from cognition research, and states that thehuman visual system prefers to partition an object into parts using theshortest possible cuts. We propose and implement a computational model for theshort-cut rule and apply it to the problem of shape decomposition. The model weproposed generates a set of cut hypotheses passing through the points on thesilhouette which represent the negative minima of curvature. We then show thatmost part-cut hypotheses can be eliminated by analysis of local properties ofeach. Finally, the remaining hypotheses are evaluated in ascending lengthorder, which guarantees that of any pair of conflicting cuts only the shortestwill be accepted. We demonstrate that, compared with state-of-the-art shapedecomposition methods, the proposed approach achieves decomposition resultswhich better correspond to human intuition as revealed in psychologicalexperiments.
arxiv-7200-67 | An NLP Assistant for Clide | http://arxiv.org/pdf/1409.2073v1.pdf | author:Tobias Kortkamp category:cs.CL published:2014-09-07 summary:This report describes an NLP assistant for the collaborative developmentenvironment Clide, that supports the development of NLP applications byproviding easy access to some common NLP data structures. The assistantvisualizes text fragments and their dependencies by displaying the semanticgraph of a sentence, the coreference chain of a paragraph and mined triplesthat are extracted from a paragraph's semantic graphs and linked using itscoreference chain. Using this information and a logic programming library, wecreate an NLP database which is used by a series of queries to mine thetriples. The algorithm is tested by translating a natural language textdescribing a graph to an actual graph that is shown as an annotation in thetext editor.
arxiv-7200-68 | Depth image hand tracking from an overhead perspective using partially labeled, unbalanced data: Development and real-world testing | http://arxiv.org/pdf/1409.2050v1.pdf | author:Stephen Czarnuch, Alex Mihailidis category:cs.CV published:2014-09-06 summary:We present the development and evaluation of a hand tracking algorithm basedon single depth images captured from an overhead perspective for use in theCOACH prompting system. We train a random decision forest body part classifierusing approximately 5,000 manually labeled, unbalanced, partially labeledtraining images. The classifier represents a random subset of pixels in eachdepth image with a learned probability density function across all trained bodyparts. A local mode-find approach is used to search for clusters present in theunderlying feature space sampled by the classified pixels. In each frame, bodypart positions are chosen as the mode with the highest confidence. User handpositions are translated into hand washing task actions based on proximity toenvironmental objects. We validate the performance of the classifier and taskaction proposals on a large set of approximately 24,000 manually labeledimages.
arxiv-7200-69 | Global Convergence of Online Limited Memory BFGS | http://arxiv.org/pdf/1409.2045v1.pdf | author:Aryan Mokhtari, Alejandro Ribeiro category:math.OC cs.LG stat.ML published:2014-09-06 summary:Global convergence of an online (stochastic) limited memory version of theBroyden-Fletcher- Goldfarb-Shanno (BFGS) quasi-Newton method for solvingoptimization problems with stochastic objectives that arise in large scalemachine learning is established. Lower and upper bounds on the Hessianeigenvalues of the sample functions are shown to suffice to guarantee that thecurvature approximation matrices have bounded determinants and traces, which,in turn, permits establishing convergence to optimal arguments with probability1. Numerical experiments on support vector machines with synthetic datashowcase reductions in convergence time relative to stochastic gradient descentalgorithms as well as reductions in storage and computation relative to otheronline quasi-Newton methods. Experimental evaluation on a search engineadvertising problem corroborates that these advantages also manifest inpractical applications.
arxiv-7200-70 | Improving self-calibration | http://arxiv.org/pdf/1312.1349v3.pdf | author:Torsten A. Enßlin, Henrik Junklewitz, Lars Winderling, Maksim Greiner, Marco Selig category:astro-ph.IM cs.IT math.IT stat.ML published:2013-12-04 summary:Response calibration is the process of inferring how much the measured datadepend on the signal one is interested in. It is essential for any quantitativesignal estimation on the basis of the data. Here, we investigateself-calibration methods for linear signal measurements and linear dependenceof the response on the calibration parameters. The common practice is toaugment an external calibration solution using a known reference signal with aninternal calibration on the unknown measurement signal itself. Contemporaryself-calibration schemes try to find a self-consistent solution for signal andcalibration by exploiting redundancies in the measurements. This can beunderstood in terms of maximizing the joint probability of signal andcalibration. However, the full uncertainty structure of this joint probabilityaround its maximum is thereby not taken into account by these schemes.Therefore better schemes -- in sense of minimal square error -- can be designedby accounting for asymmetries in the uncertainty of signal and calibration. Weargue that at least a systematic correction of the common self-calibrationscheme should be applied in many measurement situations in order to properlytreat uncertainties of the signal on which one calibrates. Otherwise thecalibration solutions suffer from a systematic bias, which consequentlydistorts the signal reconstruction. Furthermore, we argue that non-parametric,signal-to-noise filtered calibration should provide more accuratereconstructions than the common bin averages and provide a new, improvedself-calibration scheme. We illustrate our findings with a simplistic numericalexample.
arxiv-7200-71 | A Reduction of the Elastic Net to Support Vector Machines with an Application to GPU Computing | http://arxiv.org/pdf/1409.1976v1.pdf | author:Quan Zhou, Wenlin Chen, Shiji Song, Jacob R. Gardner, Kilian Q. Weinberger, Yixin Chen category:stat.ML cs.LG published:2014-09-06 summary:The past years have witnessed many dedicated open-source projects that builtand maintain implementations of Support Vector Machines (SVM), parallelized forGPU, multi-core CPUs and distributed systems. Up to this point, no comparableeffort has been made to parallelize the Elastic Net, despite its popularity inmany high impact applications, including genetics, neuroscience and systemsbiology. The first contribution in this paper is of theoretical nature. Weestablish a tight link between two seemingly different algorithms and provethat Elastic Net regression can be reduced to SVM with squared hinge lossclassification. Our second contribution is to derive a practical algorithmbased on this reduction. The reduction enables us to utilize prior efforts inspeeding up and parallelizing SVMs to obtain a highly optimized and parallelsolver for the Elastic Net and Lasso. With a simple wrapper, consisting of only11 lines of MATLAB code, we obtain an Elastic Net implementation that naturallyutilizes GPU and multi-core CPUs. We demonstrate on twelve real world datasets, that our algorithm yields identical results as the popular (and highlyoptimized) glmnet implementation but is one or several orders of magnitudefaster.
arxiv-7200-72 | Marginal Structured SVM with Hidden Variables | http://arxiv.org/pdf/1409.1320v2.pdf | author:Wei Ping, Qiang Liu, Alexander Ihler category:stat.ML cs.LG published:2014-09-04 summary:In this work, we propose the marginal structured SVM (MSSVM) for structuredprediction with hidden variables. MSSVM properly accounts for the uncertaintyof hidden variables, and can significantly outperform the previously proposedlatent structured SVM (LSSVM; Yu & Joachims (2009)) and other state-of-artmethods, especially when that uncertainty is large. Our method also results ina smoother objective function, making gradient-based optimization of MSSVMsconverge significantly faster than for LSSVMs. We also show that our methodconsistently outperforms hidden conditional random fields (HCRFs; Quattoni etal. (2007)) on both simulated and real-world datasets. Furthermore, we proposea unified framework that includes both our and several other existing methodsas special cases, and provides insights into the comparison of different modelsin practice.
arxiv-7200-73 | Particle Swarm Optimized Fuzzy Controller for Indirect Vector Control of Multilevel Inverter Fed Induction Motor | http://arxiv.org/pdf/1409.2697v1.pdf | author:Sanjaya Kumar Sahu, T. V. Dixit, D. D. Neema category:cs.NE published:2014-09-05 summary:The Particle Swarm Optimized (PSO) fuzzy controller has been proposed forindirect vector control of induction motor. In this proposed scheme a NeutralPoint Clamped (NPC) multilevel inverter is used and hysteresis current controltechnique has been adopted for switching the IGBTs. A Mamdani type fuzzycontroller is used in place of conventional PI controller. To ensure betterperformance of fuzzy controller all parameters such as membership functions,normalizing and de-normalizing parameters are optimized using PSO. Theperformance of proposed controller is investigated under various load and speedconditions. The simulation results show its stability and robustness for highperformance derives applications.
arxiv-7200-74 | Automatic Neuron Type Identification by Neurite Localization in the Drosophila Medulla | http://arxiv.org/pdf/1409.1892v1.pdf | author:Ting Zhao, Stephen M Plaza category:q-bio.NC cs.CV published:2014-09-05 summary:Mapping the connectivity of neurons in the brain (i.e., connectomics) is achallenging problem due to both the number of connections in even the smallestorganisms and the nanometer resolution required to resolve them. Because ofthis, previous connectomes contain only hundreds of neurons, such as in theC.elegans connectome. Recent technological advances will unlock the mysteriesof increasingly large connectomes (or partial connectomes). However, the valueof these maps is limited by our ability to reason with this data and understandany underlying motifs. To aid connectome analysis, we introduce algorithms tocluster similarly-shaped neurons, where 3D neuronal shapes are represented asskeletons. In particular, we propose a novel location-sensitive clusteringalgorithm. We show clustering results on neurons reconstructed from theDrosophila medulla that show high-accuracy.
arxiv-7200-75 | Novel Methods for Activity Classification and Occupany Prediction Enabling Fine-grained HVAC Control | http://arxiv.org/pdf/1409.1917v1.pdf | author:Rajib Rana, Brano Kusy, Josh Wall, Wen Hu category:cs.LG published:2014-09-05 summary:Much of the energy consumption in buildings is due to HVAC systems, which hasmotivated several recent studies on making these systems more energy-efficient. Occupancy and activity are two important aspects, which need to becorrectly estimated for optimal HVAC control. However, state-of-the-art methodsto estimate occupancy and classify activity require infrastructure and/orwearable sensors which suffers from lower acceptability due to higher cost.Encouragingly, with the advancement of the smartphones, these are becoming moreachievable. Most of the existing occupancy estimation tech- niques have theunderlying assumption that the phone is always carried by its user. However,phones are often left at desk while attending meeting or other events, whichgenerates estimation error for the existing phone based occupancy algorithms.Similarly, in the recent days the emerging theory of Sparse Random Classifier(SRC) has been applied for activity classification on smartphone, however,there are rooms to improve the on-phone process- ing. We propose a novel sensorfusion method which offers almost 100% accuracy for occupancy estimation. Wealso propose an activity classifica- tion algorithm, which offers similaraccuracy as of the state-of-the-art SRC algorithms while offering 50% reductionin processing.
arxiv-7200-76 | KNET: A General Framework for Learning Word Embedding using Morphological Knowledge | http://arxiv.org/pdf/1407.1687v3.pdf | author:Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, Tie-Yan Liu category:cs.CL cs.LG published:2014-07-07 summary:Neural network techniques are widely applied to obtain high-qualitydistributed representations of words, i.e., word embeddings, to address textmining, information retrieval, and natural language processing tasks. Recently,efficient methods have been proposed to learn word embeddings from context thatcaptures both semantic and syntactic relationships between words. However, itis challenging to handle unseen words or rare words with insufficient context.In this paper, inspired by the study on word recognition process in cognitivepsychology, we propose to take advantage of seemingly less obvious butessentially important morphological knowledge to address these challenges. Inparticular, we introduce a novel neural network architecture called KNET thatleverages both contextual information and morphological word similarity builtbased on morphological knowledge to learn word embeddings. Meanwhile, thelearning architecture is also able to refine the pre-defined morphologicalknowledge and obtain more accurate word similarity. Experiments on ananalogical reasoning task and a word similarity task both demonstrate that theproposed KNET framework can greatly enhance the effectiveness of wordembeddings.
arxiv-7200-77 | Identifying Synapses Using Deep and Wide Multiscale Recursive Networks | http://arxiv.org/pdf/1409.1789v1.pdf | author:Gary B. Huang, Stephen Plaza category:cs.CV published:2014-09-05 summary:In this work, we propose a learning framework for identifying synapses usinga deep and wide multi-scale recursive (DAWMR) network, previously considered inimage segmentation applications. We apply this approach on electron microscopydata from invertebrate fly brain tissue. By learning features directly from thedata, we are able to achieve considerable improvements over existing techniquesthat rely on a small set of hand-designed features. We show that this systemcan reduce the amount of manual annotation required, in both acquisition oftraining data as well as verification of inferred detections.
arxiv-7200-78 | Structure of an elite co-occurrence network | http://arxiv.org/pdf/1409.1744v1.pdf | author:V. A. Traag, R. Reinanda, G. van Klinken category:physics.soc-ph cs.CL cs.SI published:2014-09-05 summary:The rise of social media allowed for rich analyses of their content and theirnetwork structure. As traditional media (i.e. newspapers and magazines) arebeing digitized, similar analyses can be undertaken. This provides a glimpse ofthe elite, as the news mostly revolves around the more influential members ofsociety. We here focus on a network structure derived from co-occurrences ofpeople in the media. This network has a strong core with peripheral clustersbeing connected to the core. Nonetheless, these characteristics seem to bemainly a result from the bipartite structure of the data. We employ a simplegrowing bipartite model that can qualitatively reproduce such a core-peripherystructure. Two self-reinforcing processes are vital: (1) more frequentlyoccurring persons are more likely to occur again; and (2) if two peopleco-occur frequently, they are more likely to co-occur again. This suggests thatthe core-periphery structure is not necessarily reflective of the elite networkin society, but might be an artefact of how they are portrayed in the media.
arxiv-7200-79 | An Experimental Study of Adaptive Control for Evolutionary Algorithms | http://arxiv.org/pdf/1409.1715v1.pdf | author:Giacomo di Tollo, Frédéric Lardeux, Jorge Maturana, Frédéric Saubion category:cs.NE published:2014-09-05 summary:The balance of exploration versus exploitation (EvE) is a key issue onevolutionary computation. In this paper we will investigate how an adaptivecontroller aimed to perform Operator Selection can be used to dynamicallymanage the EvE balance required by the search, showing that the searchstrategies determined by this control paradigm lead to an improvement ofsolution quality found by the evolutionary algorithm.
arxiv-7200-80 | Unsupervised deconvolution of dynamic imaging reveals intratumor vascular heterogeneity | http://arxiv.org/pdf/1306.3392v3.pdf | author:Li Chen, Peter L. Choyke, Niya Wang, Robert Clarke, Zaver M. Bhujwalla, Elizabeth M. C. Hillman, Yue Wang category:q-bio.QM stat.ML published:2013-06-14 summary:Intratumor heterogeneity is often manifested by vascular compartments withdistinct pharmacokinetics that cannot be resolved directly by in vivo dynamicimaging. We developed tissue-specific compartment modeling (TSCM), anunsupervised computational method of deconvolving dynamic imaging series fromheterogeneous tumors that can improve vascular phenotyping in many biologicalcontexts. Applying TSCM to dynamic contrast-enhanced MRI of breast cancersrevealed characteristic intratumor vascular heterogeneity and therapeuticresponses that were otherwise undetectable.
arxiv-7200-81 | Classification with Sparse Overlapping Groups | http://arxiv.org/pdf/1402.4512v2.pdf | author:Nikhil Rao, Robert Nowak, Christopher Cox, Timothy Rogers category:cs.LG stat.ML published:2014-02-18 summary:Classification with a sparsity constraint on the solution plays a centralrole in many high dimensional machine learning applications. In some cases, thefeatures can be grouped together so that entire subsets of features can beselected or not selected. In many applications, however, this can be toorestrictive. In this paper, we are interested in a less restrictive form ofstructured sparse feature selection: we assume that while features can begrouped according to some notion of similarity, not all features in a groupneed be selected for the task at hand. When the groups are comprised ofdisjoint sets of features, this is sometimes referred to as the "sparse group"lasso, and it allows for working with a richer class of models than traditionalgroup lasso methods. Our framework generalizes conventional sparse group lassofurther by allowing for overlapping groups, an additional flexiblity needed inmany applications and one that presents further challenges. The maincontribution of this paper is a new procedure called Sparse Overlapping Group(SOG) lasso, a convex optimization program that automatically selects similarfeatures for classification in high dimensions. We establish model selectionerror bounds for SOGlasso classification problems under a fairly generalsetting. In particular, the error bounds are the first such results forclassification using the sparse group lasso. Furthermore, the general SOGlassobound specializes to results for the lasso and the group lasso, some known andsome new. The SOGlasso is motivated by multi-subject fMRI studies in whichfunctional activity is classified using brain voxels as features, sourcelocalization problems in Magnetoencephalography (MEG), and analyzing geneactivation patterns in microarray data analysis. Experiments with real andsynthetic data demonstrate the advantages of SOGlasso compared to the lasso andgroup lasso.
arxiv-7200-82 | Gradient density estimation in arbitrary finite dimensions using the method of stationary phase | http://arxiv.org/pdf/1211.3038v4.pdf | author:Karthik S. Gurumoorthy, Anand Rangarajan, John Corring category:stat.ML published:2012-11-13 summary:We prove that the density function of the gradient of a sufficiently smoothfunction $S : \Omega \subset \mathbb{R}^d \rightarrow \mathbb{R}$, obtained viaa random variable transformation of a uniformly distributed random variable, isincreasingly closely approximated by the normalized power spectrum of$\phi=\exp\left(\frac{iS}{\tau}\right)$ as the free parameter $\tau \rightarrow0$. The result is shown using the stationary phase approximation and standardintegration techniques and requires proper ordering of limits. We highlight arelationship with the well-known characteristic function approach to densityestimation, and detail why our result is distinct from this approach.
arxiv-7200-83 | Nonlinear tensor product approximation of functions | http://arxiv.org/pdf/1409.1403v1.pdf | author:D. Bazarkhanov, V. Temlyakov category:stat.ML math.NA 41A65 published:2014-09-04 summary:We are interested in approximation of a multivariate function$f(x_1,\dots,x_d)$ by linear combinations of products $u^1(x_1)\cdots u^d(x_d)$of univariate functions $u^i(x_i)$, $i=1,\dots,d$. In the case $d=2$ it is aclassical problem of bilinear approximation. In the case of approximation inthe $L_2$ space the bilinear approximation problem is closely related to theproblem of singular value decomposition (also called Schmidt expansion) of thecorresponding integral operator with the kernel $f(x_1,x_2)$. There are knownresults on the rate of decay of errors of best bilinear approximation in $L_p$under different smoothness assumptions on $f$. The problem of multilinearapproximation (nonlinear tensor product approximation) in the case $d\ge 3$ ismore difficult and much less studied than the bilinear approximation problem.We will present results on best multilinear approximation in $L_p$ under mixedsmoothness assumption on $f$.
arxiv-7200-84 | Question Answering with Subgraph Embeddings | http://arxiv.org/pdf/1406.3676v3.pdf | author:Antoine Bordes, Sumit Chopra, Jason Weston category:cs.CL published:2014-06-14 summary:This paper presents a system which learns to answer questions on a broadrange of topics from a knowledge base using few hand-crafted features. Ourmodel learns low-dimensional embeddings of words and knowledge baseconstituents; these representations are used to score natural languagequestions against candidate answers. Training our system using pairs ofquestions and structured representations of their answers, and pairs ofquestion paraphrases, yields competitive results on a competitive benchmark ofthe literature.
arxiv-7200-85 | Tracking Dynamic Point Processes on Networks | http://arxiv.org/pdf/1409.0031v2.pdf | author:Eric C. Hall, Rebecca M. Willett category:stat.ML cs.IT cs.SI math.IT published:2014-08-29 summary:Cascading chains of events are a salient feature of many real-world social,biological, and financial networks. In social networks, social reciprocityaccounts for retaliations in gang interactions, proxy wars in nation-stateconflicts, or Internet memes shared via social media. Neuron spikes stimulateor inhibit spike activity in other neurons. Stock market shocks can trigger acontagion of volatility throughout a financial network. In these and otherexamples, only individual events associated with network nodes are observed,usually without knowledge of the underlying dynamic relationships betweennodes. This paper addresses the challenge of tracking how events within suchnetworks stimulate or influence future events. The proposed approach is anonline learning framework well-suited to streaming data, using a multivariateHawkes point process model to encapsulate autoregressive features of observedevents within the social network. Recent work on online learning in dynamicenvironments is leveraged not only to exploit the dynamics within theunderlying network, but also to track that network structure as it evolves.Regret bounds and experimental results demonstrate that the proposed methodperforms nearly as well as an oracle or batch algorithm.
arxiv-7200-86 | Domain Transfer Structured Output Learning | http://arxiv.org/pdf/1409.1200v1.pdf | author:Jim Jing-Yan Wang category:cs.LG published:2014-09-03 summary:In this paper, we propose the problem of domain transfer structured outputlearn- ing and the first solution to solve it. The problem is defined on twodifferent data domains sharing the same input and output spaces, named assource domain and target domain. The outputs are structured, and for the datasamples of the source domain, the corresponding outputs are available, whilefor most data samples of the target domain, the corresponding outputs aremissing. The input distributions of the two domains are significantlydifferent. The problem is to learn a predictor for the target domain to predictthe structured outputs from the input. Due to the limited number of outputsavailable for the samples form the target domain, it is difficult to directlylearn the predictor from the target domain, thus it is necessary to use theoutput information available in source domain. We propose to learn the targetdomain predictor by adapting a auxiliary predictor trained by using sourcedomain data to the target domain. The adaptation is implemented by adding adelta function on the basis of the auxiliary predictor. An algorithm isdeveloped to learn the parameter of the delta function to minimize lossfunctions associat- ed with the predicted outputs against the true outputs ofthe data samples with available outputs of the target domain.
arxiv-7200-87 | Focused Proofreading: Efficiently Extracting Connectomes from Segmented EM Images | http://arxiv.org/pdf/1409.1199v1.pdf | author:Stephen M. Plaza category:q-bio.QM cs.CV published:2014-09-03 summary:Identifying complex neural circuitry from electron microscopic (EM) imagesmay help unlock the mysteries of the brain. However, identifying this circuitryrequires time-consuming, manual tracing (proofreading) due to the size andintricacy of these image datasets, thus limiting state-of-the-art analysis tovery small brain regions. Potential avenues to improve scalability includeautomatic image segmentation and crowd sourcing, but current efforts have hadlimited success. In this paper, we propose a new strategy, focusedproofreading, that works with automatic segmentation and aims to limitproofreading to the regions of a dataset that are most impactful to theresulting circuit. We then introduce a novel workflow, which exploitsbiological information such as synapses, and apply it to a large dataset in thefly optic lobe. With our techniques, we achieve significant tracing speedups of3-5x without sacrificing the quality of the resulting circuit. Furthermore, ourmethodology makes the task of proofreading much more accessible and hencepotentially enhances the effectiveness of crowd sourcing.
arxiv-7200-88 | Tunably Rugged Landscapes with Known Maximum and Minimum | http://arxiv.org/pdf/1409.1143v1.pdf | author:Narine Manukyan, Margaret J. Eppstein, Jeffrey S. Buzas category:cs.NE published:2014-09-03 summary:We propose NM landscapes as a new class of tunably rugged benchmark problems.NM landscapes are well-defined on alphabets of any arity, including bothdiscrete and real-valued alphabets, include epistasis in a natural andtransparent manner, are proven to have known value and location of the globalmaximum and, with some additional constraints, are proven to also have a knownglobal minimum. Empirical studies are used to illustrate that, whencoefficients are selected from a recommended distribution, the ruggedness of NMlandscapes is smoothly tunable and correlates with several measures of searchdifficulty. We discuss why these properties make NM landscapes preferable toboth NK landscapes and Walsh polynomials as benchmark landscape models withtunable epistasis.
arxiv-7200-89 | Compressive Sensing of Sparse Tensors | http://arxiv.org/pdf/1305.5777v4.pdf | author:Shmuel Friedland, Qun Li, Dan Schonfeld category:cs.CV cs.IT math.IT published:2013-05-24 summary:Compressive sensing (CS) has triggered enormous research activity since itsfirst appearance. CS exploits the signal's sparsity or compressibility in aparticular domain and integrates data compression and acquisition, thusallowing exact reconstruction through relatively few non-adaptive linearmeasurements. While conventional CS theory relies on data representation in theform of vectors, many data types in various applications such as color imaging,video sequences, and multi-sensor networks, are intrinsically represented byhigher-order tensors. Application of CS to higher-order data representation istypically performed by conversion of the data to very long vectors that must bemeasured using very large sampling matrices, thus imposing a huge computationaland memory burden. In this paper, we propose Generalized Tensor CompressiveSensing (GTCS)--a unified framework for compressive sensing of higher-ordertensors which preserves the intrinsic structure of tensor data with reducedcomputational complexity at reconstruction. GTCS offers an efficient means forrepresentation of multidimensional data by providing simultaneous acquisitionand compression from all tensor modes. In addition, we propound tworeconstruction procedures, a serial method (GTCS-S) and a parallelizable method(GTCS-P). We then compare the performance of the proposed method with Kroneckercompressive sensing (KCS) and multi way compressive sensing (MWCS). Wedemonstrate experimentally that GTCS outperforms KCS and MWCS in terms of bothreconstruction accuracy (within a range of compression ratios) and processingspeed. The major disadvantage of our methods (and of MWCS as well), is that thecompression ratios may be worse than that offered by KCS.
arxiv-7200-90 | Gradient Distribution Priors for Biomedical Image Processing | http://arxiv.org/pdf/1408.3300v2.pdf | author:Yuanhao Gong, Ivo F. Sbalzarini category:cs.CV published:2014-08-13 summary:Ill-posed inverse problems are commonplace in biomedical image processing.Their solution typically requires imposing prior knowledge about the latentground truth. While this regularizes the problem to an extent where it can besolved, it also biases the result toward the expected. With inappropriatepriors harming more than they use, it remains unclear what prior to use for agiven practical problem. Priors are hence mostly chosen in an {\em ad hoc} orempirical fashion. We argue here that the gradient distribution ofnatural-scene images may provide a versatile and well-founded prior forbiomedical images. We provide motivation for this choice from different pointsof view, and we fully validate the resulting prior for use on biomedical imagesby showing its stability and correlation with image quality. We then provide aset of simple parametric models for the resulting prior, leading tostraightforward (quasi-)convex optimization problems for which we provideefficient solver algorithms. We illustrate the use of the present models andsolvers in a variety of common image-processing tasks, including contrastenhancement, noise level estimation, denoising, blind deconvolution,zooming/up-sampling, and dehazing. In all cases we show that the present methodleads to results that are comparable to or better than the state of the art;always using the same, simple prior. We conclude by discussing the limitationsand possible interpretations of the prior.
arxiv-7200-91 | Performance Analysis on Evolutionary Algorithms for the Minimum Label Spanning Tree Problem | http://arxiv.org/pdf/1409.1073v1.pdf | author:Xinsheng Lai, Yuren Zhou, Jun He, Jun Zhang category:cs.NE cs.DS published:2014-09-03 summary:Some experimental investigations have shown that evolutionary algorithms(EAs) are efficient for the minimum label spanning tree (MLST) problem.However, we know little about that in theory. As one step towards this issue,we theoretically analyze the performances of the (1+1) EA, a simple version ofEAs, and a multi-objective evolutionary algorithm called GSEMO on the MLSTproblem. We reveal that for the MLST$_{b}$ problem the (1+1) EA and GSEMOachieve a $\frac{b+1}{2}$-approximation ratio in expected polynomial times of$n$ the number of nodes and $k$ the number of labels. We also show that GSEMOachieves a $(2ln(n))$-approximation ratio for the MLST problem in expectedpolynomial time of $n$ and $k$. At the same time, we show that the (1+1) EA andGSEMO outperform local search algorithms on three instances of the MLSTproblem. We also construct an instance on which GSEMO outperforms the (1+1) EA.
arxiv-7200-92 | Structured Low-Rank Matrix Factorization with Missing and Grossly Corrupted Observations | http://arxiv.org/pdf/1409.1062v1.pdf | author:Fanhua Shang, Yuanyuan Liu, Hanghang Tong, James Cheng, Hong Cheng category:cs.LG cs.CV stat.ML published:2014-09-03 summary:Recovering low-rank and sparse matrices from incomplete or corruptedobservations is an important problem in machine learning, statistics,bioinformatics, computer vision, as well as signal and image processing. Intheory, this problem can be solved by the natural convex joint/mixedrelaxations (i.e., l_{1}-norm and trace norm) under certain conditions.However, all current provable algorithms suffer from superlinear per-iterationcost, which severely limits their applicability to large-scale problems. Inthis paper, we propose a scalable, provable structured low-rank matrixfactorization method to recover low-rank and sparse matrices from missing andgrossly corrupted data, i.e., robust matrix completion (RMC) problems, orincomplete and grossly corrupted measurements, i.e., compressive principalcomponent pursuit (CPCP) problems. Specifically, we first present twosmall-scale matrix trace norm regularized bilinear structured factorizationmodels for RMC and CPCP problems, in which repetitively calculating SVD of alarge-scale matrix is replaced by updating two much smaller factor matrices.Then, we apply the alternating direction method of multipliers (ADMM) toefficiently solve the RMC problems. Finally, we provide the convergenceanalysis of our algorithm, and extend it to address general CPCP problems.Experimental results verified both the efficiency and effectiveness of ourmethod compared with the state-of-the-art methods.
arxiv-7200-93 | Augmented Neural Networks for Modelling Consumer Indebtness | http://arxiv.org/pdf/1409.1057v1.pdf | author:Alexandros Ladas, Jonathan M. Garibaldi, Rodrigo Scarpel, Uwe Aickelin category:cs.CE cs.LG cs.NE published:2014-09-03 summary:Consumer Debt has risen to be an important problem of modern societies,generating a lot of research in order to understand the nature of consumerindebtness, which so far its modelling has been carried out by statisticalmodels. In this work we show that Computational Intelligence can offer a moreholistic approach that is more suitable for the complex relationships anindebtness dataset has and Linear Regression cannot uncover. In particular, asour results show, Neural Networks achieve the best performance in modellingconsumer indebtness, especially when they manage to incorporate the significantand experimentally verified results of the Data Mining process in the model,exploiting the flexibility Neural Networks offer in designing their topology.This novel method forms an elaborate framework to model Consumer indebtnessthat can be extended to any other real world application.
arxiv-7200-94 | Tuning a Multiple Classifier System for Side Effect Discovery using Genetic Algorithms | http://arxiv.org/pdf/1409.1053v1.pdf | author:Jenna M. Reps, Uwe Aickelin, Jonathan M. Garibaldi category:cs.LG cs.CE published:2014-09-03 summary:In previous work, a novel supervised framework implementing a binaryclassifier was presented that obtained excellent results for side effectdiscovery. Interestingly, unique side effects were identified when differentbinary classifiers were used within the framework, prompting the investigationof applying a multiple classifier system. In this paper we investigate tuning aside effect multiple classifying system using genetic algorithms. The resultsof this research show that the novel framework implementing a multipleclassifying system trained using genetic algorithms can obtain a higher partialarea under the receiver operating characteristic curve than implementing asingle classifier. Furthermore, the framework is able to detect side effectsefficiently and obtains a low false positive rate.
arxiv-7200-95 | Variability of Behaviour in Electricity Load Profile Clustering; Who Does Things at the Same Time Each Day? | http://arxiv.org/pdf/1409.1043v1.pdf | author:Ian Dent, Tony Craig, Uwe Aickelin, Tom Rodden category:cs.LG cs.CE published:2014-09-03 summary:UK electricity market changes provide opportunities to alter households'electricity usage patterns for the benefit of the overall electricity network.Work on clustering similar households has concentrated on daily load profilesand the variability in regular household behaviours has not been considered.Those households with most variability in regular activities may be the mostreceptive to incentives to change timing. Whether using the variability of regular behaviour allows the creation ofmore consistent groupings of households is investigated and compared with dailyload profile clustering. 204 UK households are analysed to find repeatingpatterns (motifs). Variability in the time of the motif is used as the basisfor clustering households. Different clustering algorithms are assessed by theconsistency of the results. Findings show that variability of behaviour, using motifs, provides moreconsistent groupings of households across different clustering algorithms andallows for more efficient targeting of behaviour change interventions.
arxiv-7200-96 | EVOC: A Computer Model of the Evolution of Culture | http://arxiv.org/pdf/1310.0522v2.pdf | author:Liane Gabora category:cs.MA cs.NE published:2013-10-01 summary:EVOC is a computer model of the EVOlution of Culture. It consists of neuralnetwork based agents that invent ideas for actions, and imitate neighbors'actions. EVOC replicates using a different fitness function the resultsobtained with an earlier model (MAV), including (1) an increase in mean fitnessof actions, and (2) an increase and then decrease in the diversity of actions.Diversity of actions is positively correlated with number of needs, populationsize and density, and with the erosion of borders between populations. Slowlyeroding borders maximize diversity, fostering specialization followed bysharing of fit actions. Square (as opposed to toroidal) worlds also exhibithigher diversity. Introducing a leader that broadcasts its actions throughoutthe population increases the fitness of actions but reduces diversity; theseeffects diminish the more leaders there are. Low density populations have lessfit ideas but broadcasting diminishes this effect.
arxiv-7200-97 | Aggregate channel features for multi-view face detection | http://arxiv.org/pdf/1407.4023v2.pdf | author:Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li category:cs.CV published:2014-07-15 summary:Face detection has drawn much attention in recent decades since the seminalwork by Viola and Jones. While many subsequences have improved the work withmore powerful learning algorithms, the feature representation used for facedetection still can't meet the demand for effectively and efficiently handlingfaces with large appearance variance in the wild. To solve this bottleneck, weborrow the concept of channel features to the face detection domain, whichextends the image channel to diverse types like gradient magnitude and orientedgradient histograms and therefore encodes rich information in a simple form. Weadopt a novel variant called aggregate channel features, make a fullexploration of feature design, and discover a multi-scale version of featureswith better performance. To deal with poses of faces in the wild, we propose amulti-view detection approach featuring score re-ranking and detectionadjustment. Following the learning pipelines in Viola-Jones framework, themulti-view face detector using aggregate channel features shows competitiveperformance against state-of-the-art algorithms on AFW and FDDB testsets, whileruns at 42 FPS on VGA images.
arxiv-7200-98 | A Truncated EM Approach for Spike-and-Slab Sparse Coding | http://arxiv.org/pdf/1211.3589v3.pdf | author:Abdul-Saboor Sheikh, Jacquelyn A. Shelton, Jörg Lücke category:stat.ML published:2012-11-15 summary:We study inference and learning based on a sparse coding model with`spike-and-slab' prior. As in standard sparse coding, the model used assumesindependent latent sources that linearly combine to generate data points.However, instead of using a standard sparse prior such as a Laplacedistribution, we study the application of a more flexible `spike-and-slab'distribution which models the absence or presence of a source's contributionindependently of its strength if it contributes. We investigate two approachesto optimize the parameters of spike-and-slab sparse coding: a novel truncatedEM approach and, for comparison, an approach based on standard factoredvariational distributions. The truncated approach can be regarded as avariational approach with truncated posteriors as variational distributions. Inapplications to source separation we find that both approaches improve thestate-of-the-art in a number of standard benchmarks, which argues for the useof `spike-and-slab' priors for the corresponding data domains. Furthermore, wefind that the truncated EM approach improves on the standard factored approachin source separation tasks$-$which hints to biases introduced by assumingposterior independence in the factored variational approach. Likewise, on astandard benchmark for image denoising, we find that the truncated EM approachimproves on the factored variational approach. While the performance of thefactored approach saturates with increasing numbers of hidden dimensions, theperformance of the truncated approach improves the state-of-the-art for highernoise levels.
arxiv-7200-99 | Attributes for Causal Inference in Longitudinal Observational Databases | http://arxiv.org/pdf/1409.5774v1.pdf | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.CE cs.LG published:2014-09-03 summary:The pharmaceutical industry is plagued by the problem of side effects thatcan occur anytime a prescribed medication is ingested. There has been a recentinterest in using the vast quantities of medical data available in longitudinalobservational databases to identify causal relationships between drugs andmedical events. Unfortunately the majority of existing post marketingsurveillance algorithms measure how dependant or associated an event is on thepresence of a drug rather than measuring causality. In this paper weinvestigate potential attributes that can be used in causal inference toidentify side effects based on the Bradford-Hill causality criteria. Potentialattributes are developed by considering five of the causality criteria andfeature selection is applied to identify the most suitable of these attributesfor detecting side effects. We found that attributes based on the specificitycriterion may improve side effect signalling algorithms but the experiment anddosage criteria attributes investigated in this paper did not offer sufficientadditional information.
arxiv-7200-100 | Constructing a Non-Negative Low Rank and Sparse Graph with Data-Adaptive Features | http://arxiv.org/pdf/1409.0964v1.pdf | author:Liansheng Zhuang, Shenghua Gao, Jinhui Tang, Jingjing Wang, Zhouchen Lin, Yi Ma category:cs.CV cs.LG published:2014-09-03 summary:This paper aims at constructing a good graph for discovering intrinsic datastructures in a semi-supervised learning setting. Firstly, we propose to builda non-negative low-rank and sparse (referred to as NNLRS) graph for the givendata representation. Specifically, the weights of edges in the graph areobtained by seeking a nonnegative low-rank and sparse matrix that representseach data sample as a linear combination of others. The so-obtained NNLRS-graphcan capture both the global mixture of subspaces structure (by the lowrankness) and the locally linear structure (by the sparseness) of the data,hence is both generative and discriminative. Secondly, as good features areextremely important for constructing a good graph, we propose to learn the dataembedding matrix and construct the graph jointly within one framework, which istermed as NNLRS with embedded features (referred to as NNLRS-EF). Extensiveexperiments on three publicly available datasets demonstrate that the proposedmethod outperforms the state-of-the-art graph construction method by a largemargin for both semi-supervised classification and discriminative analysis,which verifies the effectiveness of our proposed method.
arxiv-7200-101 | Convolutional Neural Networks for Sentence Classification | http://arxiv.org/pdf/1408.5882v2.pdf | author:Yoon Kim category:cs.CL cs.NE published:2014-08-25 summary:We report on a series of experiments with convolutional neural networks (CNN)trained on top of pre-trained word vectors for sentence-level classificationtasks. We show that a simple CNN with little hyperparameter tuning and staticvectors achieves excellent results on multiple benchmarks. Learningtask-specific vectors through fine-tuning offers further gains in performance.We additionally propose a simple modification to the architecture to allow forthe use of both task-specific and static vectors. The CNN models discussedherein improve upon the state of the art on 4 out of 7 tasks, which includesentiment analysis and question classification.
arxiv-7200-102 | Breakdown Point of Robust Support Vector Machine | http://arxiv.org/pdf/1409.0934v1.pdf | author:Takafumi Kanamori, Shuhei Fujiwara, Akiko Takeda category:stat.ML cs.LG published:2014-09-03 summary:The support vector machine (SVM) is one of the most successful learningmethods for solving classification problems. Despite its popularity, SVM has aserious drawback, that is sensitivity to outliers in training samples. Thepenalty on misclassification is defined by a convex loss called the hinge loss,and the unboundedness of the convex loss causes the sensitivity to outliers. Todeal with outliers, robust variants of SVM have been proposed, such as therobust outlier detection algorithm and an SVM with a bounded loss called theramp loss. In this paper, we propose a robust variant of SVM and investigateits robustness in terms of the breakdown point. The breakdown point is arobustness measure that is the largest amount of contamination such that theestimated classifier still gives information about the non-contaminated data.The main contribution of this paper is to show an exact evaluation of thebreakdown point for the robust SVM. For learning parameters such as theregularization parameter in our algorithm, we derive a simple formula thatguarantees the robustness of the classifier. When the learning parameters aredetermined with a grid search using cross validation, our formula works toreduce the number of candidate search points. The robustness of the proposedmethod is confirmed in numerical experiments. We show that the statisticalproperties of the robust SVM are well explained by a theoretical analysis ofthe breakdown point.
arxiv-7200-103 | Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation | http://arxiv.org/pdf/1406.1078v3.pdf | author:Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio category:cs.CL cs.LG cs.NE stat.ML published:2014-06-03 summary:In this paper, we propose a novel neural network model called RNNEncoder-Decoder that consists of two recurrent neural networks (RNN). One RNNencodes a sequence of symbols into a fixed-length vector representation, andthe other decodes the representation into another sequence of symbols. Theencoder and decoder of the proposed model are jointly trained to maximize theconditional probability of a target sequence given a source sequence. Theperformance of a statistical machine translation system is empirically found toimprove by using the conditional probabilities of phrase pairs computed by theRNN Encoder-Decoder as an additional feature in the existing log-linear model.Qualitatively, we show that the proposed model learns a semantically andsyntactically meaningful representation of linguistic phrases.
arxiv-7200-104 | Visual Speech Recognition | http://arxiv.org/pdf/1409.1411v1.pdf | author:Ahmad B. A. Hassanat category:cs.CV published:2014-09-03 summary:Lip reading is used to understand or interpret speech without hearing it, atechnique especially mastered by people with hearing difficulties. The abilityto lip read enables a person with a hearing impairment to communicate withothers and to engage in social activities, which otherwise would be difficult.Recent advances in the fields of computer vision, pattern recognition, andsignal processing has led to a growing interest in automating this challengingtask of lip reading. Indeed, automating the human ability to lip read, aprocess referred to as visual speech recognition (VSR) (or sometimes speechreading), could open the door for other novel related applications. VSR hasreceived a great deal of attention in the last decade for its potential use inapplications such as human-computer interaction (HCI), audio-visual speechrecognition (AVSR), speaker recognition, talking heads, sign languagerecognition and video surveillance. Its main aim is to recognise spoken word(s)by using only the visual signal that is produced during speech. Hence, VSRdeals with the visual domain of speech and involves image processing,artificial intelligence, object detection, pattern recognition, statisticalmodelling, etc.
arxiv-7200-105 | Bypassing Captcha By Machine A Proof For Passing The Turing Test | http://arxiv.org/pdf/1409.0925v1.pdf | author:Ahmad B. A. Hassanat category:cs.CV cs.AI cs.HC published:2014-09-03 summary:For the last ten years, CAPTCHAs have been widely used by websites to preventtheir data being automatically updated by machines. By supposedly allowing onlyhumans to do so, CAPTCHAs take advantage of the reverse Turing test (TT),knowing that humans are more intelligent than machines. Generally, CAPTCHAshave defeated machines, but things are changing rapidly as technology improves.Hence, advanced research into optical character recognition (OCR) is overtakingattempts to strengthen CAPTCHAs against machine-based attacks. This paperinvestigates the immunity of CAPTCHA, which was built on the failure of the TT.We show that some CAPTCHAs are easily broken using a simple OCR machine builtfor the purpose of this study. By reviewing other techniques, we show that evenmore difficult CAPTCHAs can be broken using advanced OCR machines. Currentadvances in OCR should enable machines to pass the TT in the image recognitiondomain, which is exactly where machines are seeking to overcome CAPTCHAs. Weenhance traditional CAPTCHAs by employing not only characters, but also naturallanguage and multiple objects within the same CAPTCHA. The proposed CAPTCHAsmight be able to hold out against machines, at least until the advent of amachine that passes the TT completely.
arxiv-7200-106 | Visual Passwords Using Automatic Lip Reading | http://arxiv.org/pdf/1409.0924v1.pdf | author:Ahmad Basheer Hassanat category:cs.CV cs.CR published:2014-09-02 summary:This paper presents a visual passwords system to increase security. Thesystem depends mainly on recognizing the speaker using the visual speech signalalone. The proposed scheme works in two stages: setting the visual passwordstage and the verification stage. At the setting stage the visual passwordssystem request the user to utter a selected password, a video recording of theuser face is captured, and processed by a special words-based VSR system whichextracts a sequence of feature vectors. In the verification stage, the sameprocedure is executed, the features will be sent to be compared with the storedvisual password. The proposed scheme has been evaluated using a video databaseof 20 different speakers (10 females and 10 males), and 15 more males inanother video database with different experiment sets. The evaluation hasproved the system feasibility, with average error rate in the range of 7.63% to20.51% at the worst tested scenario, and therefore, has potential to be apractical approach with the support of other conventional authenticationmethods such as the use of usernames and passwords.
arxiv-7200-107 | Dimensionality Invariant Similarity Measure | http://arxiv.org/pdf/1409.0923v1.pdf | author:Ahmad Basheer Hassanat category:cs.LG published:2014-09-02 summary:This paper presents a new similarity measure to be used for general tasksincluding supervised learning, which is represented by the K-nearest neighborclassifier (KNN). The proposed similarity measure is invariant to largedifferences in some dimensions in the feature space. The proposed metric isproved mathematically to be a metric. To test its viability for differentapplications, the KNN used the proposed metric for classifying test exampleschosen from a number of real datasets. Compared to some other well knownmetrics, the experimental results show that the proposed metric is a promisingdistance measure for the KNN classifier with strong potential for a wide rangeof applications.
arxiv-7200-108 | Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach | http://arxiv.org/pdf/1409.0919v1.pdf | author:Ahmad Basheer Hassanat, Mohammad Ali Abbadi, Ghada Awad Altarawneh, Ahmad Ali Alhasanat category:cs.LG published:2014-09-02 summary:This paper presents a new solution for choosing the K parameter in thek-nearest neighbor (KNN) algorithm, the solution depending on the idea ofensemble learning, in which a weak KNN classifier is used each time with adifferent K, starting from one to the square root of the size of the trainingset. The results of the weak classifiers are combined using the weighted sumrule. The proposed solution was tested and compared to other solutions using agroup of experiments in real life problems. The experimental results show thatthe proposed classifier outperforms the traditional KNN classifier that uses adifferent number of neighbors, is competitive with other classifiers, and is apromising classifier with strong potential for a wide range of applications.
arxiv-7200-109 | Deformation corrected compressed sensing (DC-CS): a novel framework for accelerated dynamic MRI | http://arxiv.org/pdf/1405.7718v2.pdf | author:Sajan Goud Lingala, Edward DiBella, Mathews Jacob category:cs.CV published:2014-05-29 summary:We propose a novel deformation corrected compressed sensing (DC-CS) frameworkto recover dynamic magnetic resonance images from undersampled measurements. Weintroduce a generalized formulation that is capable of handling a wide class ofsparsity/compactness priors on the deformation corrected dynamic signal. Inthis work, we consider example compactness priors such as sparsity in temporalFourier domain, sparsity in temporal finite difference domain, and nuclear normpenalty to exploit low rank structure. Using variable splitting, we decouplethe complex optimization problem to simpler and well understood sub problems;the resulting algorithm alternates between simple steps of shrinkage baseddenoising, deformable registration, and a quadratic optimization step.Additionally, we employ efficient continuation strategies to minimize the riskof convergence to local minima. The proposed formulation contrasts withexisting DC-CS schemes that are customized for free breathing cardiac cineapplications, and other schemes that rely on fully sampled reference frames ornavigator signals to estimate the deformation parameters. The efficientdecoupling enabled by the proposed scheme allows its application to a widerange of applications including contrast enhanced dynamic MRI. Throughexperiments on numerical phantom and in vivo myocardial perfusion MRI datasets,we demonstrate the utility of the proposed DC-CS scheme in providing robustreconstructions with reduced motion artifacts over classical compressed sensingschemes that utilize the compact priors on the original deformationun-corrected signal.
arxiv-7200-110 | An Approach for Text Steganography Based on Markov Chains | http://arxiv.org/pdf/1409.0915v1.pdf | author:H. Hernan Moraldo category:cs.MM cs.CL 68P25, 94A60 D.4.6 published:2014-09-02 summary:A text steganography method based on Markov chains is introduced, togetherwith a reference implementation. This method allows for information hiding intexts that are automatically generated following a given Markov model. OtherMarkov - based systems of this kind rely on big simplifications of the languagemodel to work, which produces less natural looking and more easily detectabletexts. The method described here is designed to generate texts within a goodapproximation of the original language model provided.
arxiv-7200-111 | Action Recognition in the Frequency Domain | http://arxiv.org/pdf/1409.0908v1.pdf | author:Anh Tran, Jinyan Guan, Thanima Pilantanakitti, Paul Cohen category:cs.CV published:2014-09-02 summary:In this paper, we describe a simple strategy for mitigating variability intemporal data series by shifting focus onto long-term, frequency domainfeatures that are less susceptible to variability. We apply this method to thehuman action recognition task and demonstrate how working in the frequencydomain can yield good recognition features for commonly used optical flow andarticulated pose features, which are highly sensitive to small differences inmotion, viewpoint, dynamic backgrounds, occlusion and other sources ofvariability. We show how these frequency-based features can be used incombination with a simple forest classifier to achieve good and robust resultson the popular KTH Actions dataset.
arxiv-7200-112 | Empirical Evaluation of Tree distances for Parser Evaluation | http://arxiv.org/pdf/1409.0314v2.pdf | author:Taraka Rama category:cs.CL published:2014-09-01 summary:In this empirical study, I compare various tree distance measures --originally developed in computational biology for the purpose of treecomparison -- for the purpose of parser evaluation. I will control for theparser setting by comparing the automatically generated parse trees from thestate-of-the-art parser Charniak, 2000) with the gold-standard parse trees. Thearticle describes two different tree distance measures (RF and QD) along withits variants (GRF and GQD) for the purpose of parser evaluation. The articlewill argue that RF measure captures similar information as the standard EvalBmetric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha,1989) applied by Tsarfaty et al. (2011). Finally, the article also providesempirical evidence by reporting high correlations between the different treedistances and EvalB metric's scores.
arxiv-7200-113 | CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein Tertiary Structure Retrieval | http://arxiv.org/pdf/1409.0814v1.pdf | author:Rezaul Karim, Mohd. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman, Md. Abul Kashem Mia, Farhana Zaman, Salman Rakin category:cs.CV cs.CE cs.IR published:2014-09-02 summary:Due to the advancements in technology number of entries in the structuraldatabase of proteins are increasing day by day. Methods for retrieving proteintertiary structures from this large database is the key to comparative analysisof structures which plays an important role to understand proteins and theirfunction. In this paper, we present fast and accurate methods for the retrievalof proteins from a large database with tertiary structures similar to a queryprotein. Our proposed methods borrow ideas from the field of computer vision.The speed and accuracy of our methods comes from the two newly introducedfeatures, the co-occurrence matrix of the oriented gradient and pyramidhistogram of oriented gradient and from the use of Euclidean distance as thedistance measure. Experimental results clearly indicate the superiority of ourapproach in both running time and accuracy. Our method is readily available foruse from this website: http://research.buet.ac.bd:8080/Comograd/.
arxiv-7200-114 | Axiomatic Construction of Hierarchical Clustering in Asymmetric Networks | http://arxiv.org/pdf/1301.7724v2.pdf | author:Gunnar Carlsson, Facundo Mémoli, Alejandro Ribeiro, Santiago Segarra category:cs.LG cs.SI stat.ML published:2013-01-31 summary:This paper considers networks where relationships between nodes arerepresented by directed dissimilarities. The goal is to study methods for thedetermination of hierarchical clusters, i.e., a family of nested partitionsindexed by a connectivity parameter, induced by the given dissimilaritystructures. Our construction of hierarchical clustering methods is based ondefining admissible methods to be those methods that abide by the axioms ofvalue - nodes in a network with two nodes are clustered together at the maximumof the two dissimilarities between them - and transformation - whendissimilarities are reduced, the network may become more clustered but notless. Several admissible methods are constructed and two particular methods,termed reciprocal and nonreciprocal clustering, are shown to provide upper andlower bounds in the space of admissible methods. Alternative clusteringmethodologies and axioms are further considered. Allowing the outcome ofhierarchical clustering to be asymmetric, so that it matches the asymmetry ofthe original data, leads to the inception of quasi-clustering methods. Theexistence of a unique quasi-clustering method is shown. Allowing clustering ina two-node network to proceed at the minimum of the two dissimilaritiesgenerates an alternative axiomatic construction. There is a unique clusteringmethod in this case too. The paper also develops algorithms for the computationof hierarchical clusters using matrix powers on a min-max dioid algebra andstudies the stability of the methods proposed. We proved that most of themethods introduced in this paper are such that similar networks yield similarhierarchical clustering results. Algorithms are exemplified through theirapplication to networks describing internal migration within states of theUnited States (U.S.) and the interrelation between sectors of the U.S. economy.
arxiv-7200-115 | Feature Engineering for Map Matching of Low-Sampling-Rate GPS Trajectories in Road Network | http://arxiv.org/pdf/1409.0797v1.pdf | author:Jian Yang, Liqiu Meng category:stat.ML cs.LG published:2014-09-02 summary:Map matching of GPS trajectories from a sequence of noisy observations servesthe purpose of recovering the original routes in a road network. In this workin progress, we attempt to share our experience of feature construction in aspatial database by reporting our ongoing experiment of feature extrac-tion inConditional Random Fields (CRFs) for map matching. Our preliminary results areobtained from real-world taxi GPS trajectories.
arxiv-7200-116 | Feature Selection in Conditional Random Fields for Map Matching of GPS Trajectories | http://arxiv.org/pdf/1409.0791v1.pdf | author:Jian Yang, Liqiu Meng category:stat.ML cs.AI cs.LG published:2014-09-02 summary:Map matching of the GPS trajectory serves the purpose of recovering theoriginal route on a road network from a sequence of noisy GPS observations. Itis a fundamental technique to many Location Based Services. However, mapmatching of a low sampling rate on urban road network is still a challengingtask. In this paper, the characteristics of Conditional Random Fields withregard to inducing many contextual features and feature selection are exploredfor the map matching of the GPS trajectories at a low sampling rate.Experiments on a taxi trajectory dataset show that our method may achievecompetitive results along with the success of reducing model complexity forcomputation-limited applications.
arxiv-7200-117 | Ensemble Learning of Colorectal Cancer Survival Rates | http://arxiv.org/pdf/1409.0788v1.pdf | author:Chris Roadknight, Uwe Aickelin, John Scholefield, Lindy Durrant category:cs.LG cs.CE published:2014-09-02 summary:In this paper, we describe a dataset relating to cellular and physicalconditions of patients who are operated upon to remove colorectal tumours. Thisdata provides a unique insight into immunological status at the point of tumourremoval, tumour classification and post-operative survival. We build onexisting research on clustering and machine learning facets of this data todemonstrate a role for an ensemble approach to highlighting patients withclearer prognosis parameters. Results for survival prediction using 3 differentapproaches are shown for a subset of the data which is most difficult to model.The performance of each model individually is compared with subsets of the datawhere some agreement is reached for multiple models. Significant improvementsin model accuracy on an unseen test set can be achieved for patients whereagreement between models is achieved.
arxiv-7200-118 | Feature selection in detection of adverse drug reactions from the Health Improvement Network (THIN) database | http://arxiv.org/pdf/1409.0775v1.pdf | author:Yihui Liu, Uwe Aickelin category:cs.LG cs.CE published:2014-09-02 summary:Adverse drug reaction (ADR) is widely concerned for public health issue. ADRsare one of most common causes to withdraw some drugs from market. Prescriptionevent monitoring (PEM) is an important approach to detect the adverse drugreactions. The main problem to deal with this method is how to automaticallyextract the medical events or side effects from high-throughput medical events,which are collected from day to day clinical practice. In this study we proposea novel concept of feature matrix to detect the ADRs. Feature matrix, which isextracted from big medical data from The Health Improvement Network (THIN)database, is created to characterize the medical events for the patients whotake drugs. Feature matrix builds the foundation for the irregular and bigmedical data. Then feature selection methods are performed on feature matrix todetect the significant features. Finally the ADRs can be located based on thesignificant features. The experiments are carried out on three drugs:Atorvastatin, Alendronate, and Metoclopramide. Major side effects for each drugare detected and better performance is achieved compared to other computerizedmethods. The detected ADRs are based on computerized methods, furtherinvestigation is needed.
arxiv-7200-119 | Signalling Paediatric Side Effects using an Ensemble of Simple Study Designs | http://arxiv.org/pdf/1409.0772v1.pdf | author:Jenna M. Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE published:2014-09-02 summary:Background: Children are frequently prescribed medication off-label, meaningthere has not been sufficient testing of the medication to determine its safetyor effectiveness. The main reason this safety knowledge is lacking is due toethical restrictions that prevent children from being included in the majorityof clinical trials. Objective: The objective of this paper is to investigatewhether an ensemble of simple study designs can be implemented to signalacutely occurring side effects effectively within the paediatric population byusing historical longitudinal data. The majority of pharmacovigilancetechniques are unsupervised, but this research presents a supervised framework.Methods: Multiple measures of association are calculated for each drug andmedical event pair and these are used as features that are fed into aclassiffier to determine the likelihood of the drug and medical event paircorresponding to an adverse drug reaction. The classiffier is trained usingknown adverse drug reactions or known non-adverse drug reaction relationships.Results: The novel ensemble framework obtained a false positive rate of 0:149,a sensitivity of 0:547 and a specificity of 0:851 when implemented on areference set of drug and medical event pairs. The novel framework consistentlyoutperformed each individual simple study design. Conclusion: This researchshows that it is possible to exploit the mechanism of causality and presents aframework for signalling adverse drug reactions effectively.
arxiv-7200-120 | A Novel Semi-Supervised Algorithm for Rare Prescription Side Effect Discovery | http://arxiv.org/pdf/1409.0768v1.pdf | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack E. Gibson, Richard B. Hubbard category:cs.LG cs.CE published:2014-09-02 summary:Drugs are frequently prescribed to patients with the aim of improving eachpatient's medical state, but an unfortunate consequence of most prescriptiondrugs is the occurrence of undesirable side effects. Side effects that occur inmore than one in a thousand patients are likely to be signalled efficiently bycurrent drug surveillance methods, however, these same methods may take decadesbefore generating signals for rarer side effects, risking medical morbidity ormortality in patients prescribed the drug while the rare side effect isundiscovered. In this paper we propose a novel computational meta-analysisframework for signalling rare side effects that integrates existing methods,knowledge from the web, metric learning and semi-supervised clustering. Thenovel framework was able to signal many known rare and serious side effects forthe selection of drugs investigated, such as tendon rupture when prescribedCiprofloxacin or Levofloxacin, renal failure with Naproxen and depressionassociated with Rimonabant. Furthermore, for the majority of the druginvestigated it generated signals for rare side effects at a more stringentsignalling threshold than existing methods and shows the potential to become afundamental part of post marketing surveillance to detect rare side effects.
arxiv-7200-121 | Data classification using the Dempster-Shafer method | http://arxiv.org/pdf/1409.0763v1.pdf | author:Qi Chen, Amanda Whitbrook, Uwe Aickelin, Chris Roadknight category:cs.LG published:2014-09-02 summary:In this paper, the Dempster-Shafer method is employed as the theoreticalbasis for creating data classification systems. Testing is carried out usingthree popular (multiple attribute) benchmark datasets that have two, three andfour classes. In each case, a subset of the available data is used for trainingto establish thresholds, limits or likelihoods of class membership for eachattribute, and hence create mass functions that establish probability of classmembership for each attribute of the test data. Classification of each dataitem is achieved by combination of these probabilities via Dempster's Rule ofCombination. Results for the first two datasets show extremely highclassification accuracy that is competitive with other popular methods. Thethird dataset is non-numerical and difficult to classify, but good results canbe achieved provided the system and mass functions are designed carefully andthe right attributes are chosen for combination. In all cases theDempster-Shafer method provides comparable performance to other more popularalgorithms, but the overhead of generating accurate mass functions increasesthe complexity with the addition of new attributes. Overall, the resultssuggest that the D-S approach provides a suitable framework for the design ofclassification systems and that automating the mass function design andcalculation would increase the viability of the algorithm for complexclassification problems.
arxiv-7200-122 | Image Retrieval And Classification Using Local Feature Vectors | http://arxiv.org/pdf/1409.0749v1.pdf | author:Vikas Verma category:cs.IR cs.CV cs.MM published:2014-09-02 summary:Content Based Image Retrieval(CBIR) is one of the important subfield in thefield of Information Retrieval. The goal of a CBIR algorithm is to retrievesemantically similar images in response to a query image submitted by the enduser. CBIR is a hard problem because of the phenomenon known as $\textit{semantic gap}$. In this thesis, we aim at analyzing the performance of a CBIR system buildusing local feature vectors and Intermediate Matching Kernel. We also propose aTwo-Step Matching process for reducing the response time of the CBIR systems.Further, we develop a Meta-Learning framework for improving the retrievalperformance of these systems. Our results show that the Two-Step Matchingprocess significantly reduces response time and the Meta-Learning Frameworkimproves the retrieval performance by more than two fold. We also analyze theperformance of various image classification systems that use different imagerepresentations constructed from the local feature vectors.
arxiv-7200-123 | Comparison of algorithms that detect drug side effects using electronic healthcare databases | http://arxiv.org/pdf/1409.0748v1.pdf | author:Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack Gibson, Richard Hubbard category:cs.LG cs.CE published:2014-09-02 summary:The electronic healthcare databases are starting to become more readilyavailable and are thought to have excellent potential for generating adversedrug reaction signals. The Health Improvement Network (THIN) database is anelectronic healthcare database containing medical information on over 11million patients that has excellent potential for detecting ADRs. In this paperwe apply four existing electronic healthcare database signal detectingalgorithms (MUTARA, HUNT, Temporal Pattern Discovery and modified ROR) on theTHIN database for a selection of drugs from six chosen drug families. This isthe first comparison of ADR signalling algorithms that includes MUTARA and HUNTand enabled us to set a benchmark for the adverse drug reaction signallingability of the THIN database. The drugs were selectively chosen to enable acomparison with previous work and for variety. It was found that no algorithmwas generally superior and the algorithms' natural thresholds act at variablestringencies. Furthermore, none of the algorithms perform well at detectingrare ADRs.
arxiv-7200-124 | A natural framework for sparse hierarchical clustering | http://arxiv.org/pdf/1409.0745v1.pdf | author:Hongyang Zhang, Ruben H. Zamar category:stat.ML cs.LG 62H30 published:2014-09-02 summary:There has been a surge in the number of large and flat data sets - data setscontaining a large number of features and a relatively small number ofobservations - due to the growing ability to collect and store information inmedical research and other fi?elds. Hierarchical clustering is a widely usedclustering tool. In hierarchical clustering, large and flat data sets may allowfor a better coverage of clustering features (features that help explain thetrue underlying clusters) but, such data sets usually include a large fractionof noise features (non-clustering features) that may hide the underlyingclusters. Witten and Tibshirani (2010) proposed a sparse hierarchicalclustering framework to cluster the observations using an adaptively chosensubset of the features, however, we show that this framework has somelimitations when the data sets contain clustering features with complexstructure. In this paper, another sparse hierarchical clustering (SHC)framework is proposed. We show that, using simulation studies and real dataexamples, the proposed framework produces superior feature selection andclustering performance comparing to the classical (of-the-shelf) hierarchicalclustering and the existing sparse hierarchical clustering framework.
arxiv-7200-125 | Transferring Landmark Annotations for Cross-Dataset Face Alignment | http://arxiv.org/pdf/1409.0602v1.pdf | author:Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang category:cs.CV published:2014-09-02 summary:Dataset bias is a well known problem in object recognition domain. Thisissue, nonetheless, is rarely explored in face alignment research. In thisstudy, we show that dataset plays an integral part of face alignmentperformance. Specifically, owing to face alignment dataset bias, training onone database and testing on another or unseen domain would lead to poorperformance. Creating an unbiased dataset through combining various existingdatabases, however, is non-trivial as one has to exhaustively re-label thelandmarks for standardisation. In this work, we propose a simple and yeteffective method to bridge the disparate annotation spaces between databases,making datasets fusion possible. We show extensive results on combining variouspopular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset andunseen data alignment.
arxiv-7200-126 | On the Equivalence Between Deep NADE and Generative Stochastic Networks | http://arxiv.org/pdf/1409.0585v1.pdf | author:Li Yao, Sherjil Ozair, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG published:2014-09-02 summary:Neural Autoregressive Distribution Estimators (NADEs) have recently beenshown as successful alternatives for modeling high dimensional multimodaldistributions. One issue associated with NADEs is that they rely on aparticular order of factorization for $P(\mathbf{x})$. This issue has beenrecently addressed by a variant of NADE called Orderless NADEs and its deeperversion, Deep Orderless NADE. Orderless NADEs are trained based on a criterionthat stochastically maximizes $P(\mathbf{x})$ with all possible orders offactorizations. Unfortunately, ancestral sampling from deep NADE is veryexpensive, corresponding to running through a neural net separately predictingeach of the visible variables given some others. This work makes a connectionbetween this criterion and the training criterion for Generative StochasticNetworks (GSNs). It shows that training NADEs in this way also trains a GSN,which defines a Markov chain associated with the NADE model. Based on thisconnection, we show an alternative way to sample from a trained Orderless NADEthat allows to trade-off computing time and quality of the samples: a 3 to10-fold speedup (taking into account the waste due to correlations betweenconsecutive samples of the chain) can be obtained without noticeably reducingthe quality of the samples. This is achieved using a novel sampling procedurefor GSNs called annealed GSN sampling, similar to tempering methods thatcombines fast mixing (obtained thanks to steps at high noise levels) withaccurate samples (obtained thanks to steps at low noise levels).
arxiv-7200-127 | Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks | http://arxiv.org/pdf/1311.1780v7.pdf | author:Caglar Gulcehre, Kyunghyun Cho, Razvan Pascanu, Yoshua Bengio category:cs.NE cs.LG stat.ML published:2013-11-07 summary:In this paper we propose and investigate a novel nonlinear unit, called $L_p$unit, for deep neural networks. The proposed $L_p$ unit receives signals fromseveral projections of a subset of units in the layer below and computes anormalized $L_p$ norm. We notice two interesting interpretations of the $L_p$unit. First, the proposed unit can be understood as a generalization of anumber of conventional pooling operators such as average, root-mean-square andmax pooling widely used in, for instance, convolutional neural networks (CNN),HMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certaindegree, similar to the recently proposed maxout unit (Goodfellow et al., 2013)which achieved the state-of-the-art object recognition results on a number ofbenchmark datasets. Secondly, we provide a geometrical interpretation of theactivation function based on which we argue that the $L_p$ unit is moreefficient at representing complex, nonlinear separating boundaries. Each $L_p$unit defines a superelliptic boundary, with its exact shape defined by theorder $p$. We claim that this makes it possible to model arbitrarily shaped,curved boundaries more efficiently by combining a few $L_p$ units of differentorders. This insight justifies the need for learning different orders for eachunit in the model. We empirically evaluate the proposed $L_p$ units on a numberof datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$units achieve the state-of-the-art results on a number of benchmark datasets.Furthermore, we evaluate the proposed $L_p$ unit on the recently proposed deeprecurrent neural networks (RNN).
arxiv-7200-128 | Multi-task Sparse Structure Learning | http://arxiv.org/pdf/1409.0272v2.pdf | author:Andre R. Goncalves, Puja Das, Soumyadeep Chatterjee, Vidyashankar Sivakumar, Fernando J. Von Zuben, Arindam Banerjee category:cs.LG stat.ML I.5.1, J.2 published:2014-09-01 summary:Multi-task learning (MTL) aims to improve generalization performance bylearning multiple related tasks simultaneously. While sometimes the underlyingtask relationship structure is known, often the structure needs to be estimatedfrom data at hand. In this paper, we present a novel family of models for MTL,applicable to regression and classification problems, capable of learning thestructure of task relationships. In particular, we consider a joint estimationproblem of the task relationship structure and the individual task parameters,which is solved using alternating minimization. The task relationship structurelearning component builds on recent advances in structure learning of Gaussiangraphical models based on sparse estimators of the precision (inversecovariance) matrix. We illustrate the effectiveness of the proposed model on avariety of synthetic and benchmark datasets for regression and classification.We also consider the problem of combining climate model outputs for betterprojections of future climate, with focus on temperature in South America, andshow that the proposed model outperforms several existing methods for theproblem.
arxiv-7200-129 | Measures of Entropy from Data Using Infinitely Divisible Kernels | http://arxiv.org/pdf/1211.2459v3.pdf | author:Luis G. Sanchez Giraldo, Murali Rao, Jose C. Principe category:cs.LG cs.IT math.IT stat.ML published:2012-11-11 summary:Information theory provides principled ways to analyze different inferenceand learning problems such as hypothesis testing, clustering, dimensionalityreduction, classification, among others. However, the use of informationtheoretic quantities as test statistics, that is, as quantities obtained fromempirical data, poses a challenging estimation problem that often leads tostrong simplifications such as Gaussian models, or the use of plug in densityestimators that are restricted to certain representation of the data. In thispaper, a framework to non-parametrically obtain measures of entropy directlyfrom data using operators in reproducing kernel Hilbert spaces defined byinfinitely divisible kernels is presented. The entropy functionals, which bearresemblance with quantum entropies, are defined on positive definite matricesand satisfy similar axioms to those of Renyi's definition of entropy.Convergence of the proposed estimators follows from concentration results onthe difference between the ordered spectrum of the Gram matrices and theintegral operators associated to the population quantities. In this way,capitalizing on both the axiomatic definition of entropy and on therepresentation power of positive definite kernels, the proposed measure ofentropy avoids the estimation of the probability distribution underlying thedata. Moreover, estimators of kernel-based conditional entropy and mutualinformation are also defined. Numerical experiments on independence testscompare favourably with state of the art.
arxiv-7200-130 | Neural coordination can be enhanced by occasional interruption of normal firing patterns: A self-optimizing spiking neural network model | http://arxiv.org/pdf/1409.0470v1.pdf | author:Alexander Woodward, Tom Froese, Takashi Ikegami category:nlin.AO cs.NE q-bio.NC 92B20 published:2014-09-01 summary:The state space of a conventional Hopfield network typically exhibits manydifferent attractors of which only a small subset satisfy constraints betweenneurons in a globally optimal fashion. It has recently been demonstrated thatcombining Hebbian learning with occasional alterations of normal neural statesavoids this problem by means of self-organized enlargement of the best basinsof attraction. However, so far it is not clear to what extent this process ofself-optimization is also operative in real brains. Here we demonstrate that itcan be transferred to more biologically plausible neural networks byimplementing a self-optimizing spiking neural network model. In addition, byusing this spiking neural network to emulate a Hopfield network with Hebbianlearning, we attempt to make a connection between rate-based and temporalcoding based neural systems. Although further work is required to make thismodel more realistic, it already suggests that the efficacy of theself-optimizing process is independent from the simplifying assumptions of aconventional Hopfield network. We also discuss natural and cultural processesthat could be responsible for occasional alteration of neural firing patternsin actual brains
arxiv-7200-131 | Classifying and Visualizing Motion Capture Sequences using Deep Neural Networks | http://arxiv.org/pdf/1306.3874v2.pdf | author:Kyunghyun Cho, Xi Chen category:cs.CV published:2013-06-17 summary:The gesture recognition using motion capture data and depth sensors hasrecently drawn more attention in vision recognition. Currently most systemsonly classify dataset with a couple of dozens different actions. Moreover,feature extraction from the data is often computational complex. In this paper,we propose a novel system to recognize the actions from skeleton data withsimple, but effective, features using deep neural networks. Features areextracted for each frame based on the relative positions of joints (PO),temporal differences (TD), and normalized trajectories of motion (NT). Giventhese features a hybrid multi-layer perceptron is trained, which simultaneouslyclassifies and reconstructs input data. We use deep autoencoder to visualizelearnt features, and the experiments show that deep neural networks can capturemore discriminative information than, for instance, principal componentanalysis can. We test our system on a public database with 65 classes and morethan 2,000 motion sequences. We obtain an accuracy above 95% which is, to ourknowledge, the state of the art result for such a large dataset.
arxiv-7200-132 | Spectral Sparse Representation for Clustering: Evolved from PCA, K-means, Laplacian Eigenmap, and Ratio Cut | http://arxiv.org/pdf/1403.6290v2.pdf | author:Zhenfang Hu, Gang Pan, Yueming Wang, Zhaohui Wu category:cs.CV published:2014-03-25 summary:Dimensionality reduction methods, e.g. PCA and Laplacian eigenmap (LE), andcluster analysis methods, e.g. K-means and ratio cut (Rcut), are two kinds ofwidely-used unsupervised data analysis tools. The former concerns highrepresentation fidelity, while the latter semantics. Some preliminary relationsbetween these methods have been established in the literature, but they are notyet integrated into a unified framework. In this paper, we show that under anideal condition, the four methods: PCA, K-means, LE, and Rcut, are unifiedtogether; and when the ideal condition is relaxed, the unification evolves to anew sparse representation method, called spectral sparse representation (SSR).It achieves the same representation fidelity as PCA/LE, and is able to revealthe cluster structure of data as K-means/Rcut. SSR is inherently related tocluster analysis, and the sparse codes can be directly used to do clustering.An efficient algorithm NSCrt is developed to solve the sparse codes of SSR. Itis observed that NSCrt is able to effectively recover the underlying solutions.As a direct application of SSR, a new clustering algorithm Scut is devised. Itreaches the start-of-the-art performance among spectral clustering methods.Compared with K-means based clustering methods, Scut does not depend oninitialization and avoids getting trapped in local minima; and the solutionsare comparable to the optimal ones of K-means run many times. Extensiveexperiments using data sets of different nature demonstrate the properties andstrengths of SSR, NSCrt, and Scut.
arxiv-7200-133 | Multi-tensor Completion for Estimating Missing Values in Video Data | http://arxiv.org/pdf/1409.0347v1.pdf | author:Chao Li, Lili Guo, Andrzej Cichocki category:cs.CV published:2014-09-01 summary:Many tensor-based data completion methods aim to solve image and videoin-painting problems. But, all methods were only developed for a singledataset. In most of real applications, we can usually obtain more than onedataset to reflect one phenomenon, and all the datasets are mutually related insome sense. Thus one question raised whether such the relationship can improvethe performance of data completion or not? In the paper, we proposed a noveland efficient method by exploiting the relationship among datasets formulti-video data completion. Numerical results show that the proposed methodsignificantly improve the performance of video in-painting, particularly in thecase of very high missing percentage.
arxiv-7200-134 | Storing sequences in binary tournament-based neural networks | http://arxiv.org/pdf/1409.0334v1.pdf | author:Xiaoran Jiang, Vincent Gripon, Claude Berrou, Michael Rabbat category:cs.NE published:2014-09-01 summary:An extension to a recently introduced architecture of clique-based neuralnetworks is presented. This extension makes it possible to store sequences withhigh efficiency. To obtain this property, network connections are provided withorientation and with flexible redundancy carried by both spatial and temporalredundancy, a mechanism of anticipation being introduced in the model. Inaddition to the sequence storage with high efficiency, this new scheme alsooffers biological plausibility. In order to achieve accurate sequenceretrieval, a double layered structure combining hetero-association andauto-association is also proposed.
arxiv-7200-135 | A Geometric Approach For Fully Automatic Chromosome Segmentation | http://arxiv.org/pdf/1112.4164v5.pdf | author:Shervin Minaee, Mehran Fotouhi, Babak Hossein Khalaj category:cs.CV published:2011-12-18 summary:A fundamental task in human chromosome analysis is chromosome segmentation.Segmentation plays an important role in chromosome karyotyping. The first stepin segmentation is to remove intrusive objects such as stain debris and othernoises. The next step is detection of touching and overlapping chromosomes, andthe final step is separation of such chromosomes. Common methods for separationbetween touching chromosomes are interactive and require human intervention forcorrect separation between touching and overlapping chromosomes. In this paper,a geometric-based method is used for automatic detection of touching andoverlapping chromosomes and separating them. The proposed scheme performssegmentation in two phases. In the first phase, chromosome clusters aredetected using three geometric criteria, and in the second phase, chromosomeclusters are separated using a cut-line. Most of earlier methods did not workproperly in case of chromosome clusters that contained more than twochromosomes. Our method, on the other hand, is quite efficient in separation ofsuch chromosome clusters. At each step, one separation will be performed andthis algorithm is repeated until all individual chromosomes are separated.Another important point about the proposed method is that it uses the geometricfeatures of chromosomes which are independent of the type of images and it caneasily be applied to any type of images such as binary images and does notrequire multispectral images as well. We have applied our method to a databasecontaining 62 touching and partially overlapping chromosomes and a success rateof 91.9% is achieved.
arxiv-7200-136 | Towards a Calculus of Echo State Networks | http://arxiv.org/pdf/1409.0280v1.pdf | author:Alireza Goudarzi, Darko Stefanovic category:cs.NE published:2014-09-01 summary:Reservoir computing is a recent trend in neural networks which uses thedynamical perturbations on the phase space of a system to compute a desiredtarget function. We present how one can formulate an expectation of systemperformance in a simple class of reservoir computing called echo statenetworks. In contrast with previous theoretical frameworks, which only revealan upper bound on the total memory in the system, we analytically calculate theentire memory curve as a function of the structure of the system and theproperties of the input and the target function. We demonstrate the precisionof our framework by validating its result for a wide range of system sizes andspectral radii. Our analytical calculation agrees with numerical simulations.To the best of our knowledge this work presents the first exact analyticalcharacterization of the memory curve in echo state networks.
arxiv-7200-137 | Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix Completion Algorithm and Theoretical Guarantees | http://arxiv.org/pdf/1409.0203v1.pdf | author:Mohammad J. Taghizadeh, Reza Parhizkar, Philip N. Garner, Herve Bourlard, Afsaneh Asaei category:cs.SD cs.LG published:2014-08-31 summary:This paper addresses the problem of ad hoc microphone array calibration whereonly partial information about the distances between microphones is available.We construct a matrix consisting of the pairwise distances and propose toestimate the missing entries based on a novel Euclidean distance matrixcompletion algorithm by alternative low-rank matrix completion and projectiononto the Euclidean distance space. This approach confines the recovered matrixto the EDM cone at each iteration of the matrix completion algorithm. Thetheoretical guarantees of the calibration performance are obtained consideringthe random and locally structured missing entries as well as the measurementnoise on the known distances. This study elucidates the links between thecalibration error and the number of microphones along with the noise level andthe ratio of missing distances. Thorough experiments on real data recordingsand simulated setups are conducted to demonstrate these theoretical insights. Asignificant improvement is achieved by the proposed Euclidean distance matrixcompletion algorithm over the state-of-the-art techniques for ad hoc microphonearray calibration.
arxiv-7200-138 | Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models | http://arxiv.org/pdf/1207.3538v3.pdf | author:Quan Wang category:cs.CV published:2012-07-15 summary:Principal component analysis (PCA) is a popular tool for lineardimensionality reduction and feature extraction. Kernel PCA is the nonlinearform of PCA, which better exploits the complicated spatial structure ofhigh-dimensional features. In this paper, we first review the basic ideas ofPCA and kernel PCA. Then we focus on the reconstruction of pre-images forkernel PCA. We also give an introduction on how PCA is used in active shapemodels (ASMs), and discuss how kernel PCA can be applied to improve traditionalASMs. Then we show some experimental results to compare the performance ofkernel PCA and standard PCA for classification problems. We also implement thekernel PCA-based ASMs, and use it to construct human face models.
arxiv-7200-139 | A Plug&Play P300 BCI Using Information Geometry | http://arxiv.org/pdf/1409.0107v1.pdf | author:Alexandre Barachant, Marco Congedo category:cs.LG cs.HC stat.ML published:2014-08-30 summary:This paper presents a new classification methods for Event Related Potentials(ERP) based on an Information geometry framework. Through a new estimation ofcovariance matrices, this work extend the use of Riemannian geometry, which waspreviously limited to SMR-based BCI, to the problem of classification of ERPs.As compared to the state-of-the-art, this new method increases performance,reduces the number of data needed for the calibration and features goodgeneralisation across sessions and subjects. This method is illustrated on datarecorded with the P300-based game brain invaders. Finally, an online andadaptive implementation is described, where the BCI is initialized with genericparameters derived from a database and continuously adapt to the individual,allowing the user to play the game without any calibration while keeping a highaccuracy.
arxiv-7200-140 | Kernel Coding: General Formulation and Special Cases | http://arxiv.org/pdf/1409.0084v1.pdf | author:Mehrtash Harandi, Mathieu Salzmann category:cs.CV published:2014-08-30 summary:Representing images by compact codes has proven beneficial for many visualrecognition tasks. Most existing techniques, however, perform this coding stepdirectly in image feature space, where the distributions of the differentclasses are typically entangled. In contrast, here, we study the problem ofperforming coding in a high-dimensional Hilbert space, where the classes areexpected to be more easily separable. To this end, we introduce a generalcoding formulation that englobes the most popular techniques, such as bag ofwords, sparse coding and locality-based coding, and show how this formulationand its special cases can be kernelized. Importantly, we address severalaspects of learning in our general formulation, such as kernel learning,dictionary learning and supervised kernel coding. Our experimental evaluationon several visual recognition tasks demonstrates the benefits of performingcoding in Hilbert space, and in particular of jointly learning the kernel, thedictionary and the classifier.
arxiv-7200-141 | Sparse Coding on Symmetric Positive Definite Manifolds using Bregman Divergences | http://arxiv.org/pdf/1409.0083v1.pdf | author:Mehrtash Harandi, Richard Hartley, Brian Lovell, Conrad Sanderson category:cs.CV published:2014-08-30 summary:This paper introduces sparse coding and dictionary learning for SymmetricPositive Definite (SPD) matrices, which are often used in machine learning,computer vision and related areas. Unlike traditional sparse coding schemesthat work in vector spaces, in this paper we discuss how SPD matrices can bedescribed by sparse combination of dictionary atoms, where the atoms are alsoSPD matrices. We propose to seek sparse coding by embedding the space of SPDmatrices into Hilbert spaces through two types of Bregman matrix divergences.This not only leads to an efficient way of performing sparse coding, but alsoan online and iterative scheme for dictionary learning. We apply the proposedmethods to several computer vision tasks where images are represented by regioncovariance matrices. Our proposed algorithms outperform state-of-the-artmethods on a wide range of classification tasks, including face recognition,action recognition, material classification and texture categorization.
arxiv-7200-142 | Data Stability in Clustering: A Closer Look | http://arxiv.org/pdf/1107.2379v5.pdf | author:Shalev Ben-David, Lev Reyzin category:cs.LG cs.DS published:2011-07-12 summary:We consider the model introduced by Bilu and Linial (2010), who studyproblems for which the optimal clustering does not change when distances areperturbed. They show that even when a problem is NP-hard, it is sometimespossible to obtain efficient algorithms for instances resilient to certainmultiplicative perturbations, e.g. on the order of $O(\sqrt{n})$ for max-cutclustering. Awasthi et al. (2010) consider center-based objectives, and Balcanand Liang (2011) analyze the $k$-median and min-sum objectives, givingefficient algorithms for instances resilient to certain constant multiplicativeperturbations. Here, we are motivated by the question of to what extent these assumptionscan be relaxed while allowing for efficient algorithms. We show there is littleroom to improve these results by giving NP-hardness lower bounds for both the$k$-median and min-sum objectives. On the other hand, we show that constantmultiplicative resilience parameters can be so strong as to make the clusteringproblem trivial, leaving only a narrow range of resilience parameters for whichclustering is interesting. We also consider a model of additive perturbationsand give a correspondence between additive and multiplicative notions ofstability. Our results provide a close examination of the consequences ofassuming stability in data.
arxiv-7200-143 | Temporal Extension of Scale Pyramid and Spatial Pyramid Matching for Action Recognition | http://arxiv.org/pdf/1408.7071v1.pdf | author:Zhenzhong Lan, Xuanchong Li, Alexandar G. Hauptmann category:cs.CV published:2014-08-29 summary:Historically, researchers in the field have spent a great deal of effort tocreate image representations that have scale invariance and retain spatiallocation information. This paper proposes to encode equivalent temporalcharacteristics in video representations for action recognition. To achievetemporal scale invariance, we develop a method called temporal scale pyramid(TSP). To encode temporal information, we present and compare two methodscalled temporal extension descriptor (TED) and temporal division pyramid (TDP). Our purpose is to suggest solutions for matching complex actions that havelarge variation in velocity and appearance, which is missing from most currentaction representations. The experimental results on four benchmark datasets,UCF50, HMDB51, Hollywood2 and Olympic Sports, support our approach andsignificantly outperform state-of-the-art methods. Most noticeably, we achieve65.0% mean accuracy and 68.2% mean average precision on the challenging HMDB51and Hollywood2 datasets which constitutes an absolute improvement over thestate-of-the-art by 7.8% and 3.9%, respectively.
arxiv-7200-144 | An Information Retrieval Approach to Short Text Conversation | http://arxiv.org/pdf/1408.6988v1.pdf | author:Zongcheng Ji, Zhengdong Lu, Hang Li category:cs.IR cs.CL published:2014-08-29 summary:Human computer conversation is regarded as one of the most difficult problemsin artificial intelligence. In this paper, we address one of its keysub-problems, referred to as short text conversation, in which given a messagefrom human, the computer returns a reasonable response to the message. Weleverage the vast amount of short conversation data available on social mediato study the issue. We propose formalizing short text conversation as a searchproblem at the first step, and employing state-of-the-art information retrieval(IR) techniques to carry out the task. We investigate the significance as wellas the limitation of the IR approach. Our experiments demonstrate that theretrieval-based model can make the system behave rather "intelligently", whencombined with a huge repository of conversation data from social media.
arxiv-7200-145 | Augmentation Schemes for Particle MCMC | http://arxiv.org/pdf/1408.6980v1.pdf | author:Paul Fearnhead, Loukia Meligkotsidou category:stat.CO stat.ML published:2014-08-29 summary:Particle MCMC involves using a particle filter within an MCMC algorithm. Forinference of a model which involves an unobserved stochastic process, thestandard implementation uses the particle filter to propose new values for thestochastic process, and MCMC moves to propose new values for the parameters. Weshow how particle MCMC can be generalised beyond this. Our key idea is tointroduce new latent variables. We then use the MCMC moves to update the latentvariables, and the particle filter to propose new values for the parameters andstochastic process given the latent variables. A generic way of defining theselatent variables is to model them as pseudo-observations of the parameters orof the stochastic process. By choosing the amount of information these latentvariables have about the parameters and the stochastic process we can oftenimprove the mixing of the particle MCMC algorithm by trading off the MonteCarlo error of the particle filter and the mixing of the MCMC moves. We showthat using pseudo-observations within particle MCMC can improve its efficiencyin certain scenarios: dealing with initialisation problems of the particlefilter; speeding up the mixing of particle Gibbs when there is strongdependence between the parameters and the stochastic process; and enablingfurther MCMC steps to be used within the particle filter.
arxiv-7200-146 | Fast Disk Conformal Parameterization of Simply-connected Open Surfaces | http://arxiv.org/pdf/1408.6974v1.pdf | author:Pui Tung Choi, Lok Ming Lui category:cs.CG cs.CV cs.GR cs.MM math.DG published:2014-08-29 summary:Surface parameterizations have been widely used in computer graphics andgeometry processing. In particular, as simply-connected open surfaces areconformally equivalent to the unit disk, it is desirable to compute the diskconformal parameterizations of the surfaces. In this paper, we propose a novelalgorithm for the conformal parameterization of a simply-connected open surfaceonto the unit disk, which significantly speeds up the computation, enhances theconformality and stability, and guarantees the bijectivity. The conformalitydistortions at the inner region and on the boundary are corrected by two steps,with the aid of an iterative scheme using quasi-conformal theories.Experimental results demonstrate the effectiveness of our proposed method.
arxiv-7200-147 | Comment on "Ensemble Projection for Semi-supervised Image Classification" | http://arxiv.org/pdf/1408.6963v1.pdf | author:Xavier Boix, Gemma Roig, Luc Van Gool category:cs.CV published:2014-08-29 summary:In a series of papers by Dai and colleagues [1,2], a feature map (or kernel)was introduced for semi- and unsupervised learning. This feature map is buildfrom the output of an ensemble of classifiers trained without using theground-truth class labels. In this critique, we analyze the latest version ofthis series of papers, which is called Ensemble Projections [2]. We show thatthe results reported in [2] were not well conducted, and that EnsembleProjections performs poorly for semi-supervised learning.
arxiv-7200-148 | Strongly Incremental Repair Detection | http://arxiv.org/pdf/1408.6788v2.pdf | author:Julian Hough, Matthew Purver category:cs.CL published:2014-08-28 summary:We present STIR (STrongly Incremental Repair detection), a system thatdetects speech repairs and edit terms on transcripts incrementally with minimallatency. STIR uses information-theoretic measures from n-gram models as itsprincipal decision features in a pipeline of classifiers detecting thedifferent stages of repairs. Results on the Switchboard disfluency taggedcorpus show utterance-final accuracy on a par with state-of-the-art incrementalrepair detection methods, but with better incremental accuracy, fastertime-to-detection and less computational overhead. We evaluate its performanceusing incremental metrics and propose new repair processing evaluationstandards.
arxiv-7200-149 | A sequential reduction method for inference in generalized linear mixed models | http://arxiv.org/pdf/1312.1903v2.pdf | author:Helen Ogden category:stat.CO stat.ME stat.ML published:2013-12-06 summary:The likelihood for the parameters of a generalized linear mixed modelinvolves an integral which may be of very high dimension. Because of thisintractability, many approximations to the likelihood have been proposed, butall can fail when the model is sparse, in that there is only a small amount ofinformation available on each random effect. The sequential reduction methoddescribed in this paper exploits the dependence structure of the posteriordistribution of the random effects to reduce substantially the cost of findingan accurate approximation to the likelihood in models with sparse structure.
arxiv-7200-150 | Binary matrices of optimal autocorrelations as alignment marks | http://arxiv.org/pdf/1408.6915v1.pdf | author:Scott A. Skirlo, Ling Lu, Marin Soljačić category:cs.CV cs.IT math.IT published:2014-08-29 summary:We define a new class of binary matrices by maximizing the peak-sidelobedistances in the aperiodic autocorrelations. These matrices can be used asrobust position marks for in-plane spatial alignment. The optimal squarematrices of dimensions up to 7 by 7 and optimal diagonally-symmetric matricesof 8 by 8 and 9 by 9 were found by exhaustive searches.
arxiv-7200-151 | Text Line Identification in Tagore's Manuscript | http://arxiv.org/pdf/1408.6911v1.pdf | author:Chandranath Adak, Bidyut B. Chaudhuri category:cs.CV published:2014-08-29 summary:In this paper, a text line identification method is proposed. The text linesof printed document are easy to segment due to uniform straightness of thelines and sufficient gap between the lines. But in handwritten documents, theline is non-uniform and interline gaps are variable. We take RabindranathTagore's manuscript as it is one of the most difficult manuscripts that containdoodles. Our method consists of a pre-processing stage to clean the documentimage. Then we separate doodles from the manuscript to get the textual region.After that we identify the text lines on the manuscript. For text lineidentification, we use window examination, black run-length smearing,horizontal histogram and connected component analysis.
arxiv-7200-152 | Chatbot for admissions | http://arxiv.org/pdf/1408.6762v1.pdf | author:Nikolaos Polatidis category:cs.CY cs.CL published:2014-08-28 summary:The communication of potential students with a university department isperformed manually and it is a very time consuming procedure. The opportunityto communicate with on a one-to-one basis is highly valued. However with manyhundreds of applications each year, one-to-one conversations are not feasiblein most cases. The communication will require a member of academic staff toexpend several hours to find suitable answers and contact each student. Itwould be useful to reduce his costs and time. The project aims to reduce the burden on the head of admissions, andpotentially other users, by developing a convincing chatbot. A suitablealgorithm must be devised to search through the set of data and find apotential answer. The program then replies to the user and provides a relevantweb link if the user is not satisfied by the answer. Furthermore a webinterface is provided for both users and an administrator. The achievements of the project can be summarised as follows. To prepare thebackground of the project a literature review was undertaken, together with aninvestigation of existing tools, and consultation with the head of admissions.The requirements of the system were established and a range of algorithms andtools were investigated, including keyword and template matching. An algorithmthat combines keyword matching with string similarity has been developed. Ausable system using the proposed algorithm has been implemented. The system wasevaluated by keeping logs of questions and answers and by feedback received bypotential students that used it.
arxiv-7200-153 | PCANet: A Simple Deep Learning Baseline for Image Classification? | http://arxiv.org/pdf/1404.3606v2.pdf | author:Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, Yi Ma category:cs.CV cs.LG cs.NE published:2014-04-14 summary:In this work, we propose a very simple deep learning network for imageclassification which comprises only the very basic data processing components:cascaded principal component analysis (PCA), binary hashing, and block-wisehistograms. In the proposed architecture, PCA is employed to learn multistagefilter banks. It is followed by simple binary hashing and block histograms forindexing and pooling. This architecture is thus named as a PCA network (PCANet)and can be designed and learned extremely easily and efficiently. Forcomparison and better understanding, we also introduce and study two simplevariations to the PCANet, namely the RandNet and LDANet. They share the sametopology of PCANet but their cascaded filters are either selected randomly orlearned from LDA. We have tested these basic networks extensively on manybenchmark visual datasets for different tasks, such as LFW for faceverification, MultiPIE, Extended Yale B, AR, FERET datasets for facerecognition, as well as MNIST for hand-written digits recognition.Surprisingly, for all tasks, such a seemingly naive PCANet model is on par withthe state of the art features, either prefixed, highly hand-crafted orcarefully learned (by DNNs). Even more surprisingly, it sets new records formany classification tasks in Extended Yale B, AR, FERET datasets, and MNISTvariations. Additional experiments on other public datasets also demonstratethe potential of the PCANet serving as a simple but highly competitive baselinefor texture classification and object recognition.
arxiv-7200-154 | Memcomputing and Swarm Intelligence | http://arxiv.org/pdf/1408.6741v1.pdf | author:Y. V. Pershin, M. Di Ventra category:cs.NE cs.ET published:2014-08-28 summary:We explore the relation between memcomputing, namely computing with and inmemory, and swarm intelligence algorithms. In particular, we show that one candesign memristive networks to solve short-path optimization problems that canalso be solved by ant-colony algorithms. By employing appropriate memristiveelements one can demonstrate an almost one-to-one correspondence betweenmemcomputing and ant colony optimization approaches. However, the memristivenetwork has the capability of finding the solution in one deterministic step,compared to the stochastic multi-step ant colony optimization. This resultpaves the way for nanoscale hardware implementations of several swarmintelligence algorithms that are presently explored, from scheduling problemsto robotics.
arxiv-7200-155 | A study of the fixed points and spurious solutions of the FastICA algorithm | http://arxiv.org/pdf/1408.6693v1.pdf | author:Tianwen Wei category:stat.ML published:2014-08-28 summary:The FastICA algorithm is one of the most popular iterative algorithms in thedomain of linear independent component analysis. Despite its success, it isobserved that FastICA occasionally yields outcomes that do not correspond toany true solutions (known as demixing vectors) of the ICA problem. Theseoutcomes are commonly referred to as spurious solutions. Although FastICA isamong the most extensively studied ICA algorithms, the occurrence of spurioussolutions are not yet completely understood by the community. In thiscontribution, we aim at addressing this issue. In the first part of this work,we are interested in the relationship between demixing vectors, localoptimizers of the contrast function and (attractive or unattractive) fixedpoints of FastICA algorithm. Characterizations of these sets are given, and aninclusion relationship is discovered. In the second part, we investigate thepossible scenarios where spurious solutions occur. We show that when certainbimodal Gaussian mixtures distributions are involved, there may exist spurioussolutions that are attractive fixed points of FastICA. In this case, popularnonlinearities such as "gauss" or "tanh" tend to yield spurious solutions,whereas only "kurtosis" may give reliable results. Some advices are given forthe practical choice of nonlinearity function.
arxiv-7200-156 | Nonparametric ridge estimation | http://arxiv.org/pdf/1212.5156v3.pdf | author:Christopher R. Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH published:2012-12-20 summary:We study the problem of estimating the ridges of a density function. Ridgeestimation is an extension of mode finding and is useful for understanding thestructure of a density. It can also be used to find hidden structure in pointcloud data. We show that, under mild regularity conditions, the ridges of thekernel density estimator consistently estimate the ridges of the true density.When the data are noisy measurements of a manifold, we show that the ridges areclose and topologically similar to the hidden manifold. To find the estimatedridges in practice, we adapt the modified mean-shift algorithm proposed byOzertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numericalexperiments verify that the algorithm is accurate.
arxiv-7200-157 | Falsifiable implies Learnable | http://arxiv.org/pdf/1408.6618v1.pdf | author:David Balduzzi category:cs.LG math.ST stat.ML stat.TH published:2014-08-28 summary:The paper demonstrates that falsifiability is fundamental to learning. Weprove the following theorem for statistical learning and sequential prediction:If a theory is falsifiable then it is learnable -- i.e. admits a strategy thatpredicts optimally. An analogous result is shown for universal induction.
arxiv-7200-158 | Task-group Relatedness and Generalization Bounds for Regularized Multi-task Learning | http://arxiv.org/pdf/1408.6617v1.pdf | author:Chao Zhang, Dacheng Tao, Tao Hu, Xiang Li category:cs.LG published:2014-08-28 summary:In this paper, we study the generalization performance of regularizedmulti-task learning (RMTL) in a vector-valued framework, where MTL isconsidered as a learning process for vector-valued functions. We are mainlyconcerned with two theoretical questions: 1) under what conditions does RMTLperform better with a smaller task sample size than STL? 2) under whatconditions is RMTL generalizable and can guarantee the consistency of each taskduring simultaneous learning? In particular, we investigate two types of task-group relatedness: theobserved discrepancy-dependence measure (ODDM) and the empiricaldiscrepancy-dependence measure (EDDM), both of which detect the dependencebetween two groups of multiple related tasks (MRTs). We then introduce theCartesian product-based uniform entropy number (CPUEN) to measure thecomplexities of vector-valued function classes. By applying the specificdeviation and the symmetrization inequalities to the vector-valued framework,we obtain the generalization bound for RMTL, which is the upper bound of thejoint probability of the event that there is at least one task with a largeempirical discrepancy between the expected and empirical risks. Finally, wepresent a sufficient condition to guarantee the consistency of each task in thesimultaneous learning process, and we discuss how task relatedness affects thegeneralization performance of RMTL. Our theoretical findings answer theaforementioned two questions.
arxiv-7200-159 | To lie or not to lie in a subspace | http://arxiv.org/pdf/1408.5544v2.pdf | author:Daniel L. Pimentel-Alarcón category:stat.ML cs.LG published:2014-08-24 summary:Give deterministic necessary and sufficient conditions to guarantee that if asubspace fits certain partially observed data from a union of subspaces, it isbecause such data really lies in a subspace. Furthermore, Give deterministic necessary and sufficient conditions toguarantee that if a subspace fits certain partially observed data, suchsubspace is unique. Do this by characterizing when and only when a set of incomplete vectorsbehaves as a single but complete one.
arxiv-7200-160 | Compression, Restoration, Re-sampling, Compressive Sensing: Fast Transforms in Digital Imaging | http://arxiv.org/pdf/1408.6335v1.pdf | author:Leonid Yaroslavsky category:cs.CV physics.optics published:2014-08-27 summary:Transform image processing methods are methods that work in domains of imagetransforms, such as Discrete Fourier, Discrete Cosine, Wavelet and alike. Theyare the basic tool in image compression, in image restoration, in imagere-sampling and geometrical transformations and can be traced back to early1970-ths. The paper presents a review of these methods with emphasis on theircomparison and relationships, from the very first steps of transform imagecompression methods to adaptive and local adaptive transform domain filters forimage restoration, to methods of precise image re-sampling and imagereconstruction from sparse samples and up to "compressive sensing" approachthat has gained popularity in last few years. The review has a tutorialcharacter and purpose.
arxiv-7200-161 | LARSEN-ELM: Selective Ensemble of Extreme Learning Machines using LARS for Blended Data | http://arxiv.org/pdf/1408.2003v2.pdf | author:Bo Han, Bo He, Rui Nian, Mengmeng Ma, Shujing Zhang, Minghui Li, Amaury Lendasse category:cs.LG stat.ML published:2014-08-09 summary:Extreme learning machine (ELM) as a neural network algorithm has shown itsgood performance, such as fast speed, simple structure etc, but also, weakrobustness is an unavoidable defect in original ELM for blended data. Wepresent a new machine learning framework called LARSEN-ELM for overcoming thisproblem. In our paper, we would like to show two key steps in LARSEN-ELM. Inthe first step, preprocessing, we select the input variables highly related tothe output using least angle regression (LARS). In the second step, training,we employ Genetic Algorithm (GA) based selective ensemble and original ELM. Inthe experiments, we apply a sum of two sines and four datasets from UCIrepository to verify the robustness of our approach. The experimental resultsshow that compared with original ELM and other methods such as OP-ELM,GASEN-ELM and LSBoost, LARSEN-ELM significantly improve robustness performancewhile keeping a relatively high speed.
arxiv-7200-162 | A Model of Plant Identification System Using GLCM, Lacunarity And Shen Features | http://arxiv.org/pdf/1410.0969v1.pdf | author:Abdul Kadir category:cs.CV published:2014-08-27 summary:Recently, many approaches have been introduced by several researchers toidentify plants. Now, applications of texture, shape, color and vein featuresare common practices. However, there are many possibilities of methods can bedeveloped to improve the performance of such identification systems. Therefore,several experiments had been conducted in this research. As a result, a newnovel approach by using combination of Gray-Level Co-occurrence Matrix,lacunarity and Shen features and a Bayesian classifier gives a better resultcompared to other plant identification systems. For comparison, this researchused two kinds of several datasets that were usually used for testing theperformance of each plant identification system. The results show that thesystem gives an accuracy rate of 97.19% when using the Flavia dataset and95.00% when using the Foliage dataset and outperforms other approaches.
arxiv-7200-163 | Adaptive Multinomial Matrix Completion | http://arxiv.org/pdf/1408.6218v1.pdf | author:Olga Klopp, Jean Lafond, Eric Moulines, Joseph Salmon category:math.ST stat.ML stat.TH published:2014-08-26 summary:The task of estimating a matrix given a sample of observed entries is knownas the \emph{matrix completion problem}. Most works on matrix completion havefocused on recovering an unknown real-valued low-rank matrix from a randomsample of its entries. Here, we investigate the case of highly quantizedobservations when the measurements can take only a small number of values.These quantized outputs are generated according to a probability distributionparametrized by the unknown matrix of interest. This model corresponds, forexample, to ratings in recommender systems or labels in multi-classclassification. We consider a general, non-uniform, sampling scheme and givetheoretical guarantees on the performance of a constrained, nuclear normpenalized maximum likelihood estimator. One important advantage of thisestimator is that it does not require knowledge of the rank or an upper boundon the nuclear norm of the unknown matrix and, thus, it is adaptive. We providelower bounds showing that our estimator is minimax optimal. An efficientalgorithm based on lifted coordinate gradient descent is proposed to computethe estimator. A limited Monte-Carlo experiment, using both simulated and realdata is provided to support our claims.
arxiv-7200-164 | A Methodology for the Diagnostic of Aircraft Engine Based on Indicators Aggregation | http://arxiv.org/pdf/1408.6214v1.pdf | author:Tsirizo Rabenoro, Jérôme Lacaille, Marie Cottrell, Fabrice Rossi category:stat.ML cs.LG published:2014-08-26 summary:Aircraft engine manufacturers collect large amount of engine related dataduring flights. These data are used to detect anomalies in the engines in orderto help companies optimize their maintenance costs. This article introduces andstudies a generic methodology that allows one to build automatic early signs ofanomaly detection in a way that is understandable by human operators who makethe final maintenance decision. The main idea of the method is to generate avery large number of binary indicators based on parametric anomaly scoresdesigned by experts, complemented by simple aggregations of those scores. Thebest indicators are selected via a classical forward scheme, leading to a muchreduced number of indicators that are tuned to a data set. We illustrate theinterest of the method on simulated data which contain realistic early signs ofanomalies.
arxiv-7200-165 | Resolving Lexical Ambiguity in Tensor Regression Models of Meaning | http://arxiv.org/pdf/1408.6181v1.pdf | author:Dimitri Kartsaklis, Nal Kalchbrenner, Mehrnoosh Sadrzadeh category:cs.CL published:2014-08-26 summary:This paper provides a method for improving tensor-based compositionaldistributional models of meaning by the addition of an explicit disambiguationstep prior to composition. In contrast with previous research where thishypothesis has been successfully tested against relatively simple compositionalmodels, in our work we use a robust model trained with linear regression. Theresults we get in two experiments show the superiority of the priordisambiguation method and suggest that the effectiveness of this approach ismodel-independent.
arxiv-7200-166 | Evaluating Neural Word Representations in Tensor-Based Compositional Settings | http://arxiv.org/pdf/1408.6179v1.pdf | author:Dmitrijs Milajevs, Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Matthew Purver category:cs.CL published:2014-08-26 summary:We provide a comparative study between neural word representations andtraditional vector spaces based on co-occurrence counts, in a number ofcompositional tasks. We use three different semantic spaces and implement seventensor-based compositional models, which we then test (together with simpleradditive and multiplicative approaches) in tasks involving verb disambiguationand sentence similarity. To check their scalability, we additionally evaluatethe spaces using simple compositional methods on larger-scale tasks with lessconstrained language: paraphrase detection and dialogue act tagging. In themore constrained tasks, co-occurrence vectors are competitive, although choiceof compositional method is important; on the larger-scale tasks, they areoutperformed by neural word embeddings, which show robust, stable performanceacross the tasks.
arxiv-7200-167 | Bayesian Fusion of Multi-Band Images | http://arxiv.org/pdf/1307.5996v2.pdf | author:Qi Wei, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV stat.ME published:2013-07-23 summary:In this paper, a Bayesian fusion technique for remotely sensed multi-bandimages is presented. The observed images are related to the high spectral andhigh spatial resolution image to be recovered through physical degradations,e.g., spatial and spectral blurring and/or subsampling defined by the sensorcharacteristics. The fusion problem is formulated within a Bayesian estimationframework. An appropriate prior distribution exploiting geometricalconsideration is introduced. To compute the Bayesian estimator of the scene ofinterest from its posterior distribution, a Markov chain Monte Carlo algorithmis designed to generate samples asymptotically distributed according to thetarget distribution. To efficiently sample from this high-dimensiondistribution, a Hamiltonian Monte Carlo step is introduced in the Gibbssampling strategy. The efficiency of the proposed fusion method is evaluatedwith respect to several state-of-the-art fusion techniques. In particular, lowspatial resolution hyperspectral and multispectral images are fused to producea high spatial resolution hyperspectral image.
arxiv-7200-168 | Inference of Cancer Progression Models with Biological Noise | http://arxiv.org/pdf/1408.6032v1.pdf | author:Ilya Korsunsky, Daniele Ramazzotti, Giulio Caravagna, Bud Mishra category:stat.ML cs.LG q-bio.QM published:2014-08-26 summary:Many applications in translational medicine require the understanding of howdiseases progress through the accumulation of persistent events. SpecializedBayesian networks called monotonic progression networks offer a statisticalframework for modeling this sort of phenomenon. Current machine learning toolsto reconstruct Bayesian networks from data are powerful but not suited toprogression models. We combine the technological advances in machine learningwith a rigorous philosophical theory of causation to produce Polaris, ascalable algorithm for learning progression networks that accounts for causalor biological noise as well as logical relations among genetic events, makingthe resulting models easy to interpret qualitatively. We tested Polaris onsynthetically generated data and showed that it outperforms a widely usedmachine learning algorithm and approaches the performance of the competingspecial-purpose, albeit clairvoyant algorithm that is given a prioriinformation about the model parameters. We also prove that under certain rathermild conditions, Polaris is guaranteed to converge for sufficiently largesample sizes. Finally, we applied Polaris to point mutation and copy numbervariation data in Prostate cancer from The Cancer Genome Atlas (TCGA) and foundthat there are likely three distinct progressions, one major androgen drivenprogression, one major non-androgen driven progression, and one novel minorandrogen driven progression.
arxiv-7200-169 | Learn Convolutional Neural Network for Face Anti-Spoofing | http://arxiv.org/pdf/1408.5601v2.pdf | author:Jianwei Yang, Zhen Lei, Stan Z. Li category:cs.CV published:2014-08-24 summary:Though having achieved some progresses, the hand-crafted texture features,e.g., LBP [23], LBP-TOP [11] are still unable to capture the mostdiscriminative cues between genuine and fake faces. In this paper, instead ofdesigning feature by ourselves, we rely on the deep convolutional neuralnetwork (CNN) to learn features of high discriminative ability in a supervisedmanner. Combined with some data pre-processing, the face anti-spoofingperformance improves drastically. In the experiments, over 70% relativedecrease of Half Total Error Rate (HTER) is achieved on two challengingdatasets, CASIA [36] and REPLAY-ATTACK [7] compared with the state-of-the-art.Meanwhile, the experimental results from inter-tests between two datasetsindicates CNN can obtain features with better generalization ability. Moreover,the nets trained using combined data from two datasets have less biases betweentwo datasets.
arxiv-7200-170 | Introduction to Clustering Algorithms and Applications | http://arxiv.org/pdf/1408.4576v2.pdf | author:Sibei Yang, Liangde Tao, Bingchen Gong category:cs.LG cs.CV published:2014-08-20 summary:Data clustering is the process of identifying natural groupings or clusterswithin multidimensional data based on some similarity measure. Clustering is afundamental process in many different disciplines. Hence, researchers fromdifferent fields are actively working on the clustering problem. This paperprovides an overview of the different representative clustering methods. Inaddition, application of clustering in different field is briefly introduced.
arxiv-7200-171 | ARTOS -- Adaptive Real-Time Object Detection System | http://arxiv.org/pdf/1407.2721v2.pdf | author:Björn Barz, Erik Rodner, Joachim Denzler category:cs.CV published:2014-07-10 summary:ARTOS is all about creating, tuning, and applying object detection modelswith just a few clicks. In particular, ARTOS facilitates learning of models forvisual object detection by eliminating the burden of having to collect andannotate a large set of positive and negative samples manually and in additionit implements a fast learning technique to reduce the time needed for thelearning step. A clean and friendly GUI guides the user through the process of modelcreation, adaptation of learned models to different domains using in-situimages, and object detection on both offline images and images from a videostream. A library written in C++ provides the main functionality of ARTOS witha C-style procedural interface, so that it can be easily integrated with anyother project.
arxiv-7200-172 | Image processing | http://arxiv.org/pdf/1409.2413v1.pdf | author:Franco Rino category:cs.CV published:2014-08-25 summary:Gabor filters can extract multi-orientation and multiscale features from faceimages. Researchers have designed different ways to use the magnitude of thefiltered results for face recognition: Gabor Fisher classifier exploited onlythe magnitude information of Gabor magnitude pictures (GMPs); Local GaborBinary Pattern uses only the gradient information. In this paper, we regardGMPs as smooth surfaces. By completely describing the shape of GMPs, we get aface representation method called Gabor Surface Feature (GSF). First, wecompute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them andtransform them into decimal values. Finally we construct joint histograms anduse subspace methods for classification. Experiments on FERET, ORL and FRGC1.0.4 database show the effectiveness of GSF.
arxiv-7200-173 | An application of topological graph clustering to protein function prediction | http://arxiv.org/pdf/1408.5634v1.pdf | author:R. Sean Bowman, Douglas Heisterkamp, Jesse Johnson, Danielle O'Donnol category:cs.CE cs.LG q-bio.QM stat.ML published:2014-08-24 summary:We use a semisupervised learning algorithm based on a topological dataanalysis approach to assign functional categories to yeast proteins usingsimilarity graphs. This new approach to analyzing biological networks yieldsresults that are as good as or better than state of the art existingapproaches.
arxiv-7200-174 | Fuzzy and entropy facial recognition | http://arxiv.org/pdf/1408.5552v1.pdf | author:Jaejun Lee, Taeseon Yun category:cs.CV 68T10 published:2014-08-24 summary:This paper suggests an effective method for facial recognition using fuzzytheory and Shannon entropy. Combination of fuzzy theory and Shannon entropyeliminates the complication of other methods. Shannon entropy calculates theratio of an element between faces, and fuzzy theory calculates the member shipof the entropy with 1. More details will be mentioned in Section 3. Thelearning performance is better than others as it is very simple, and only needtwo data per learning. By using factors that don't usually change during thelife, the method will have a high accuracy.
arxiv-7200-175 | Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation | http://arxiv.org/pdf/1408.5516v1.pdf | author:Sanja Fidler, Marko Boben, Ales Leonardis category:cs.CV published:2014-08-23 summary:Hierarchies allow feature sharing between objects at multiple levels ofrepresentation, can code exponential variability in a very compact way andenable fast inference. This makes them potentially suitable for learning andrecognizing a higher number of object classes. However, the success of thehierarchical approaches so far has been hindered by the use of hand-craftedfeatures or predetermined grouping rules. This paper presents a novel frameworkfor learning a hierarchical compositional shape vocabulary for representingmultiple object classes. The approach takes simple contour fragments and learnstheir frequent spatial configurations. These are recursively combined intoincreasingly more complex and class-specific shape compositions, each exertinga high degree of shape variability. At the top-level of the vocabulary, thecompositions are sufficiently large and complex to represent the whole shapesof the objects. We learn the vocabulary layer after layer, by graduallyincreasing the size of the window of analysis and reducing the spatialresolution at which the shape configurations are learned. The lower layers arelearned jointly on images of all classes, whereas the higher layers of thevocabulary are learned incrementally, by presenting the algorithm with oneobject class after another. The experimental results show that the learnedmulti-class object representation scales favorably with the number of objectclasses and achieves a state-of-the-art detection performance at both, fasterinference as well as shorter training times.
arxiv-7200-176 | Interpreting Tree Ensembles with inTrees | http://arxiv.org/pdf/1408.5456v1.pdf | author:Houtao Deng category:cs.LG stat.ML published:2014-08-23 summary:Tree ensembles such as random forests and boosted trees are accurate butdifficult to understand, debug and deploy. In this work, we provide the inTrees(interpretable trees) framework that extracts, measures, prunes and selectsrules from a tree ensemble, and calculates frequent variable interactions. Anrule-based learner, referred to as the simplified tree ensemble learner (STEL),can also be formed and used for future prediction. The inTrees framework canapplied to both classification and regression problems, and is applicable tomany types of tree ensembles, e.g., random forests, regularized random forests,and boosted trees. We implemented the inTrees algorithms in the "inTrees" Rpackage.
arxiv-7200-177 | A Wild Bootstrap for Degenerate Kernel Tests | http://arxiv.org/pdf/1408.5404v1.pdf | author:Kacper Chwialkowski, Dino Sejdinovic, Arthur Gretton category:stat.ML 62G10 published:2014-08-23 summary:A wild bootstrap method for nonparametric hypothesis tests based on kerneldistribution embeddings is proposed. This bootstrap method is used to constructprovably consistent tests that apply to random processes, for which the naivepermutation-based bootstrap fails. It applies to a large group of kernel testsbased on V-statistics, which are degenerate under the null hypothesis, andnon-degenerate elsewhere. To illustrate this approach, we construct atwo-sample test, an instantaneous independence test and a multiple lagindependence test for time series. In experiments, the wild bootstrap givesstrong performance on synthetic examples, on audio data, and in performancebenchmarking for the Gibbs sampler.
arxiv-7200-178 | Stretchy Polynomial Regression | http://arxiv.org/pdf/1408.5449v1.pdf | author:Kar-Ann Toh category:cs.LG stat.ML published:2014-08-23 summary:This article proposes a novel solution for stretchy polynomial regressionlearning. The solution comes in primal and dual closed-forms similar to that ofridge regression. Essentially, the proposed solution stretches the covariancecomputation via a power term thereby compresses or amplifies the estimation.Our experiments on both synthetic data and real-world data show effectivenessof the proposed method for compressive learning.
arxiv-7200-179 | Hierarchical Adaptive Structural SVM for Domain Adaptation | http://arxiv.org/pdf/1408.5400v1.pdf | author:Jiaolong Xu, Sebastian Ramos, David Vazquez, Antonio M. Lopez category:cs.CV cs.LG published:2014-08-22 summary:A key topic in classification is the accuracy loss produced when the datadistribution in the training (source) domain differs from that in the testing(target) domain. This is being recognized as a very relevant problem for manycomputer vision tasks such as image classification, object detection, andobject category recognition. In this paper, we present a novel domainadaptation method that leverages multiple target domains (or sub-domains) in ahierarchical adaptation tree. The core idea is to exploit the commonalities anddifferences of the jointly considered target domains. Given the relevance of structural SVM (SSVM) classifiers, we apply our ideato the adaptive SSVM (A-SSVM), which only requires the target domain samplestogether with the existing source-domain classifier for performing the desiredadaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM). As proof of concept we use HA-SSVM for pedestrian detection and objectcategory recognition. In the former we apply HA-SSVM to the deformablepart-based model (DPM) while in the latter HA-SSVM is applied to multi-categoryclassifiers. In both cases, we show how HA-SSVM is effective in increasing thedetection/recognition accuracy with respect to adaptation strategies thatignore the structure of the target data. Since, the sub-domains of the targetdata are not always known a priori, we shown how HA-SSVM can incorporatesub-domain structure discovery for object category recognition.
arxiv-7200-180 | Computing Multi-Relational Sufficient Statistics for Large Databases | http://arxiv.org/pdf/1408.5389v1.pdf | author:Zhensong Qian, Oliver Schulte, Yan Sun category:cs.LG cs.DB H.2.8; H.2.4 published:2014-08-22 summary:Databases contain information about which relationships do and do not holdamong entities. To make this information accessible for statistical analysisrequires computing sufficient statistics that combine information fromdifferent database tables. Such statistics may involve any number of {\empositive and negative} relationships. With a naive enumeration approach,computing sufficient statistics for negative relationships is feasible only forsmall databases. We solve this problem with a new dynamic programming algorithmthat performs a virtual join, where the requisite counts are computed withoutmaterializing join tables. Contingency table algebra is a new extension ofrelational algebra, that facilitates the efficient implementation of thisM\"obius virtual join operation. The M\"obius Join scales to large datasets(over 1M tuples) with complex schemas. Empirical evaluation with sevenbenchmark datasets showed that information about the presence and absence oflinks can be exploited in feature selection, association rule mining, andBayesian network learning.
arxiv-7200-181 | Statistical and computational trade-offs in estimation of sparse principal components | http://arxiv.org/pdf/1408.5369v1.pdf | author:Tengyao Wang, Quentin Berthet, Richard J. Samworth category:math.ST stat.ML stat.TH 62H25, 68Q17 published:2014-08-22 summary:In recent years, Sparse Principal Component Analysis has emerged as anextremely popular dimension reduction technique for high-dimensional data. Thetheoretical challenge, in the simplest case, is to estimate the leadingeigenvector of a population covariance matrix under the assumption that thiseigenvector is sparse. An impressive range of estimators have been proposed;some of these are fast to compute, while others are known to achieve theminimax optimal rate over certain Gaussian or subgaussian classes. In thispaper we show that, under a widely-believed assumption from computationalcomplexity theory, there is a fundamental trade-off between statistical andcomputational performance in this problem. More precisely, working with new,larger classes satisfying a Restricted Covariance Concentration condition, weshow that no randomised polynomial time algorithm can achieve the minimaxoptimal rate. On the other hand, we also study a (polynomial time) variant ofthe well-known semidefinite relaxation estimator, and show that it attainsessentially the optimal rate among all randomised polynomial time algorithms.
arxiv-7200-182 | Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in Polynomial Time | http://arxiv.org/pdf/1408.5352v1.pdf | author:Zhaoran Wang, Huanran Lu, Han Liu category:stat.ML cs.LG published:2014-08-22 summary:Sparse principal component analysis (PCA) involves nonconvex optimization forwhich the global solution is hard to obtain. To address this issue, one popularapproach is convex relaxation. However, such an approach may produce suboptimalestimators due to the relaxation effect. To optimally estimate sparse principalsubspaces, we propose a two-stage computational framework named "tighten afterrelax": Within the 'relax' stage, we approximately solve a convex relaxation ofsparse PCA with early stopping to obtain a desired initial estimator; For the'tighten' stage, we propose a novel algorithm called sparse orthogonaliteration pursuit (SOAP), which iteratively refines the initial estimator bydirectly solving the underlying nonconvex problem. A key concept of thistwo-stage framework is the basin of attraction. It represents a local regionwithin which the `tighten' stage has desired computational and statisticalguarantees. We prove that, the initial estimator obtained from the 'relax'stage falls into such a region, and hence SOAP geometrically converges to aprincipal subspace estimator which is minimax-optimal within a certain modelclass. Unlike most existing sparse PCA estimators, our approach applies to thenon-spiked covariance models, and adapts to non-Gaussianity as well asdependent data settings. Moreover, through analyzing the computationalcomplexity of the two stages, we illustrate an interesting phenomenon thatlarger sample size can reduce the total iteration complexity. Our frameworkmotivates a general paradigm for solving many complex statistical problemswhich involve nonconvex optimization with provable guarantees.
arxiv-7200-183 | Structural bias in population-based algorithms | http://arxiv.org/pdf/1408.5350v1.pdf | author:Anna V. Kononova, David W. Corne, Philippe De Wilde, Vsevolod Shneer, Fabio Caraffini category:cs.NE published:2014-08-22 summary:Challenging optimisation problems are abundant in all areas of science. Sincethe 1950s, scientists have developed ever-diversifying families of black boxoptimisation algorithms designed to address any optimisation problem, requiringonly that quality of a candidate solution is calculated via a fitness functionspecific to the problem. For such algorithms to be successful, at least threeproperties are required: an effective informed sampling strategy, that guidesgeneration of new candidates on the basis of fitnesses and locations ofpreviously visited candidates; mechanisms to ensure efficiency, so that samecandidates are not repeatedly visited; absence of structural bias, which, ifpresent, would predispose the algorithm towards limiting its search to someregions of solution space. The first two of these properties have beenextensively investigated, however the third is little understood. In thisarticle we provide theoretical and empirical analyses that contribute to theunderstanding of structural bias. We prove a theorem concerning dynamics ofpopulation variance in the case of real-valued search spaces. This reveals howstructural bias can manifest as non-uniform clustering of population over time.Theory predicts that structural bias is exacerbated with increasing populationsize and problem difficulty. These predictions reveal two previouslyunrecognised aspects of structural bias. Respectively, increasing populationsize, though ostensibly promoting diversity, will magnify any inherentstructural bias, and effects of structural bias are more apparent when facedwith difficult problems. Our theoretical result also suggests that two commonlyused approaches to enhancing exploration, increasing population size andincreasing disruptiveness of search operators, have quite distinct implicationsin terms of structural bias.
arxiv-7200-184 | Bat Algorithm is Better Than Intermittent Search Strategy | http://arxiv.org/pdf/1408.5348v1.pdf | author:Xin-She Yang, Suash Deb, Simon Fong category:math.OC cs.NE published:2014-08-22 summary:The efficiency of any metaheuristic algorithm largely depends on the way ofbalancing local intensive exploitation and global diverse exploration. Studiesshow that bat algorithm can provide a good balance between these two keycomponents with superior efficiency. In this paper, we first review somecommonly used metaheuristic algorithms, and then compare the performance of batalgorithm with the so-called intermittent search strategy. From simulations, wefound that bat algorithm is better than the optimal intermittent searchstrategy. We also analyse the comparison results and their implications forhigher dimensional optimization problems. In addition, we also apply batalgorithm in solving business optimization and engineering design problems.
arxiv-7200-185 | Cuckoo Search: A Brief Literature Review | http://arxiv.org/pdf/1408.5343v1.pdf | author:I. Fister Jr., X. S. Yang, D. Fister, I. Fister category:math.OC cs.NE nlin.AO 90C26 published:2014-08-22 summary:Cuckoo search (CS) was introduced in 2009, and it has attracted greatattention due to its promising efficiency in solving many optimization problemsand real-world applications. In the last few years, many papers have beenpublished regarding cuckoo search, and the relevant literature has expandedsignificantly. This chapter summarizes briefly the majority of the literatureabout cuckoo search in peer-reviewed journals and conferences found so far.These references can be systematically classified into appropriate categories,which can be used as a basis for further research.
arxiv-7200-186 | Flower Pollination Algorithm: A Novel Approach for Multiobjective Optimization | http://arxiv.org/pdf/1408.5332v1.pdf | author:Xin-She Yang, M. Karamanoglu, X. S. He category:math.OC cs.NE nlin.AO 90C26 published:2014-08-22 summary:Multiobjective design optimization problems require multiobjectiveoptimization techniques to solve, and it is often very challenging to obtainhigh-quality Pareto fronts accurately. In this paper, the recently developedflower pollination algorithm (FPA) is extended to solve multiobjectiveoptimization problems. The proposed method is used to solve a set ofmultobjective test functions and two bi-objective design benchmarks, and acomparison of the proposed algorithm with other algorithms has been made, whichshows that FPA is efficient with a good convergence rate. Finally, theimportance for further parametric studies and theoretical analysis arehighlighted and discussed.
arxiv-7200-187 | Applications and Analysis of Bio-Inspired Eagle Strategy for Engineering Optimization | http://arxiv.org/pdf/1408.5320v1.pdf | author:Xin-She Yang, M. Karamanoglu, T. O. Ting, Y. X. Zhao category:math.OC cs.NE nlin.AO 90C26 published:2014-08-22 summary:All swarm-intelligence-based optimization algorithms use some stochasticcomponents to increase the diversity of solutions during the search process.Such randomization is often represented in terms of random walks. However, itis not yet clear why some randomization techniques (and thus why somealgorithms) may perform better than others for a given set of problems. In thiswork, we analyze these randomization methods in the context of nature-inspiredalgorithms. We also use eagle strategy to provide basic observations and relatestep sizes and search efficiency using Markov theory. Then, we apply ouranalysis and observations to solve four design benchmarks, including thedesigns of a pressure vessel, a speed reducer, a PID controller and a heatexchanger. Our results demonstrate that eagle strategy with L\'evy flights canperform extremely well in reducing the overall computational efforts.
arxiv-7200-188 | Cuckoo Search: Recent Advances and Applications | http://arxiv.org/pdf/1408.5316v1.pdf | author:Xin-She Yang, Suash Deb category:math.OC cs.NE nlin.AO 90-XX published:2014-08-22 summary:Cuckoo search (CS) is a relatively new algorithm, developed by Yang and Debin 2009, and CS is efficient in solving global optimization problems. In thispaper, we review the fundamental ideas of cuckoo search and the latestdevelopments as well as its applications. We analyze the algorithm and gaininsight into its search mechanisms and find out why it is efficient. We alsodiscuss the essence of algorithms and its link to self-organizing systems, andfinally we propose some important topics for further research.
arxiv-7200-189 | Continuum armed bandit problem of few variables in high dimensions | http://arxiv.org/pdf/1304.5793v4.pdf | author:Hemant Tyagi, Bernd Gärtner category:cs.LG published:2013-04-21 summary:We consider the stochastic and adversarial settings of continuum armedbandits where the arms are indexed by [0,1]^d. The reward functions r:[0,1]^d-> R are assumed to intrinsically depend on at most k coordinate variablesimplying r(x_1,..,x_d) = g(x_{i_1},..,x_{i_k}) for distinct and unknowni_1,..,i_k from {1,..,d} and some locally Holder continuous g:[0,1]^k -> R withexponent 0 < alpha <= 1. Firstly, assuming (i_1,..,i_k) to be fixed acrosstime, we propose a simple modification of the CAB1 algorithm where we constructthe discrete set of sampling points to obtain a bound ofO(n^((alpha+k)/(2*alpha+k)) (log n)^((alpha)/(2*alpha+k)) C(k,d)) on theregret, with C(k,d) depending at most polynomially in k and sub-logarithmicallyin d. The construction is based on creating partitions of {1,..,d} into kdisjoint subsets and is probabilistic, hence our result holds with highprobability. Secondly we extend our results to also handle the more generalcase where (i_1,...,i_k) can change over time and derive regret bounds for thesame.
arxiv-7200-190 | Joint Hierarchical Gaussian Process Model with Application to Forecast in Medical Monitoring | http://arxiv.org/pdf/1408.4660v2.pdf | author:Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak category:stat.ME stat.ML published:2014-08-20 summary:A novel extrapolation method is proposed for longitudinal forecasting. Ahierarchical Gaussian process model is used to combine nonlinear populationchange and individual memory of the past to make prediction. The predictionerror is minimized through the hierarchical design. The method is furtherextended to joint modeling of continuous measurements and survival events. Thebaseline hazard, covariate and joint effects are conveniently modeled in thishierarchical structure. The estimation and inference are implemented in fullyBayesian framework using the objective and shrinkage priors. In simulationstudies, this model shows robustness in latent estimation, correlationdetection and high accuracy in forecasting. The model is illustrated withmedical monitoring data from cystic fibrosis (CF) patients. Estimation andforecasts are obtained in the measurement of lung function and records of acuterespiratory events. Keyword: Extrapolation, Joint Model, Longitudinal Model, HierarchicalGaussian Process, Cystic Fibrosis, Medical Monitoring
arxiv-7200-191 | Automatic Extraction of Protein Interaction in Literature | http://arxiv.org/pdf/1406.1953v2.pdf | author:Peilei Liu, Ting Wang category:cs.CL cs.CE H.2.8; H.3.5 published:2014-06-08 summary:Protein-protein interaction extraction is the key precondition of theconstruction of protein knowledge network, and it is very important for theresearch in the biomedicine. This paper extracted directional protein-proteininteraction from the biological text, using the SVM-based method. Experimentswere evaluated on the LLL05 corpus with good results. The results show thatdependency features are import for the protein-protein interaction extractionand features related to the interaction word are effective for the interactiondirection judgment. At last, we analyzed the effects of different features andplaned for the next step.
arxiv-7200-192 | Neural Mechanism of Language | http://arxiv.org/pdf/1408.5403v1.pdf | author:Peilei Liu, Ting Wang category:cs.NE cs.CL q-bio.NC published:2014-08-22 summary:This paper is based on our previous work on neural coding. It is aself-organized model supported by existing evidences. Firstly, we brieflyintroduce this model in this paper, and then we explain the neural mechanism oflanguage and reasoning with it. Moreover, we find that the position of an areadetermines its importance. Specifically, language relevant areas are in thecapital position of the cortical kingdom. Therefore they are closely relatedwith autonomous consciousness and working memories. In essence, language is aminiature of the real world. Briefly, this paper would like to bridge the gapbetween molecule mechanism of neurons and advanced functions such as languageand reasoning.
arxiv-7200-193 | Unsupervised Spike Sorting Based on Discriminative Subspace Learning | http://arxiv.org/pdf/1408.5275v1.pdf | author:Mohammad Reza Keshtkaran, Zhi Yang category:cs.CV physics.med-ph published:2014-08-22 summary:Spike sorting is a fundamental preprocessing step for many neurosciencestudies which rely on the analysis of spike trains. In this paper, we presenttwo unsupervised spike sorting algorithms based on discriminative subspacelearning. The first algorithm simultaneously learns the discriminative featuresubspace and performs clustering. It uses histogram of features in the mostdiscriminative projection to detect the number of neurons. The second algorithmperforms hierarchical divisive clustering that learns a discriminative1-dimensional subspace for clustering in each level of the hierarchy untilachieving almost unimodal distribution in the subspace. The algorithms aretested on synthetic and in-vivo data, and are compared against two widely usedspike sorting methods. The comparative results demonstrate that our spikesorting methods can achieve substantially higher accuracy in lower dimensionalfeature space, and they are highly robust to noise. Moreover, they providesignificantly better cluster separability in the learned subspace than in thesubspace obtained by principal component analysis or wavelet transform.
arxiv-7200-194 | Improving the Interpretability of Support Vector Machines-based Fuzzy Rules | http://arxiv.org/pdf/1408.5246v1.pdf | author:Duc-Hien Nguyen, Manh-Thanh Le category:cs.LG cs.AI 68U35 I.2.3 published:2014-08-22 summary:Support vector machines (SVMs) and fuzzy rule systems are functionallyequivalent under some conditions. Therefore, the learning algorithms developedin the field of support vector machines can be used to adapt the parameters offuzzy systems. Extracting fuzzy models from support vector machines has theinherent advantage that the model does not need to determine the number ofrules in advance. However, after the support vector machine learning, thecomplexity is usually high, and interpretability is also impaired. This papernot only proposes a complete framework for extracting interpretable SVM-basedfuzzy modeling, but also provides optimization issues of the models.Simulations examples are given to embody the idea of this paper.
arxiv-7200-195 | A two-stage architecture for stock price forecasting by combining SOM and fuzzy-SVM | http://arxiv.org/pdf/1408.5241v1.pdf | author:Duc-Hien Nguyen, Manh-Thanh Le category:cs.AI cs.LG 68U35 I.2.3 published:2014-08-22 summary:This paper proposed a model to predict the stock price based on combiningSelf-Organizing Map (SOM) and fuzzy-Support Vector Machines (f-SVM). Extractionof fuzzy rules from raw data based on the combining of statistical machinelearning models is base of this proposed approach. In the proposed model, SOMis used as a clustering algorithm to partition the whole input space into theseveral disjoint regions. For each partition, a set of fuzzy rules is extractedbased on a f-SVM combining model. Then fuzzy rules sets are used to predict thetest data using fuzzy inference algorithms. The performance of the proposedapproach is compared with other models using four data sets
arxiv-7200-196 | Gap-weighted subsequences for automatic cognate identification and phylogenetic inference | http://arxiv.org/pdf/1408.2359v2.pdf | author:Taraka Rama category:cs.CL published:2014-08-11 summary:In this paper, we describe the problem of cognate identification and itsrelation to phylogenetic inference. We introduce subsequence based features fordiscriminating cognates from non-cognates. We show that subsequence basedfeatures perform better than the state-of-the-art string similarity measuresfor the purpose of cognate identification. We use the cognate judgments for thepurpose of phylogenetic inference and observe that these classifiers infer atree which is close to the gold standard tree. The contribution of this paperis the use of subsequence features for cognate identification and to employ thecognate judgments for phylogenetic inference.
arxiv-7200-197 | Uniform Sampling for Matrix Approximation | http://arxiv.org/pdf/1408.5099v1.pdf | author:Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, Aaron Sidford category:cs.DS cs.LG stat.ML published:2014-08-21 summary:Random sampling has become a critical tool in solving massive matrixproblems. For linear regression, a small, manageable set of data rows can berandomly selected to approximate a tall, skinny data matrix, improvingprocessing time significantly. For theoretical performance guarantees, each rowmust be sampled with probability proportional to its statistical leveragescore. Unfortunately, leverage scores are difficult to compute. A simple alternative is to sample rows uniformly at random. While this oftenworks, uniform sampling will eliminate critical row information for manynatural instances. We take a fresh look at uniform sampling by examining whatinformation it does preserve. Specifically, we show that uniform samplingyields a matrix that, in some sense, well approximates a large fraction of theoriginal. While this weak form of approximation is not enough for solvinglinear regression directly, it is enough to compute a better approximation. This observation leads to simple iterative row sampling algorithms for matrixapproximation that run in input-sparsity time and preserve row structure andsparsity at all intermediate steps. In addition to an improved understanding ofuniform sampling, our main proof introduces a structural result of independentinterest: we show that every matrix can be made to have low coherence byreweighting a small subset of its rows.
arxiv-7200-198 | Deep Learning for Detecting Robotic Grasps | http://arxiv.org/pdf/1301.3592v6.pdf | author:Ian Lenz, Honglak Lee, Ashutosh Saxena category:cs.LG cs.CV cs.RO published:2013-01-16 summary:We consider the problem of detecting robotic grasps in an RGB-D view of ascene containing objects. In this work, we apply a deep learning approach tosolve this problem, which avoids time-consuming hand-design of features. Thispresents two main challenges. First, we need to evaluate a huge number ofcandidate grasps. In order to make detection fast, as well as robust, wepresent a two-step cascaded structure with two deep networks, where the topdetections from the first are re-evaluated by the second. The first network hasfewer features, is faster to run, and can effectively prune out unlikelycandidate grasps. The second, with more features, is slower but has to run onlyon the top few detections. Second, we need to handle multimodal inputs well,for which we present a method to apply structured regularization on the weightsbased on multimodal group regularization. We demonstrate that our methodoutperforms the previous state-of-the-art methods in robotic grasp detection,and can be used to successfully execute grasps on two different roboticplatforms.
arxiv-7200-199 | A Case Study in Text Mining: Interpreting Twitter Data From World Cup Tweets | http://arxiv.org/pdf/1408.5427v1.pdf | author:Daniel Godfrey, Caley Johns, Carl Meyer, Shaina Race, Carol Sadek category:stat.ML cs.CL cs.IR cs.LG published:2014-08-21 summary:Cluster analysis is a field of data analysis that extracts underlyingpatterns in data. One application of cluster analysis is in text-mining, theanalysis of large collections of text to find similarities between documents.We used a collection of about 30,000 tweets extracted from Twitter just beforethe World Cup started. A common problem with real world text data is thepresence of linguistic noise. In our case it would be extraneous tweets thatare unrelated to dominant themes. To combat this problem, we created analgorithm that combined the DBSCAN algorithm and a consensus matrix. This waywe are left with the tweets that are related to those dominant themes. We thenused cluster analysis to find those topics that the tweets describe. Weclustered the tweets using k-means, a commonly used clustering algorithm, andNon-Negative Matrix Factorization (NMF) and compared the results. The twoalgorithms gave similar results, but NMF proved to be faster and provided moreeasily interpreted results. We explored our results using two visualizationtools, Gephi and Wordle.
arxiv-7200-200 | On the Sample Complexity of Subspace Learning | http://arxiv.org/pdf/1408.5032v1.pdf | author:Alessandro Rudi, Guille D. Canas, Lorenzo Rosasco category:stat.ML published:2014-08-21 summary:A large number of algorithms in machine learning, from principal componentanalysis (PCA), and its non-linear (kernel) extensions, to more recent spectralembedding and support estimation methods, rely on estimating a linear subspacefrom samples. In this paper we introduce a general formulation of this problemand derive novel learning error estimates. Our results rely on naturalassumptions on the spectral properties of the covariance operator associated tothe data distribu- tion, and hold for a wide class of metrics betweensubspaces. As special cases, we discuss sharp error estimates for thereconstruction properties of PCA and spectral support estimation. Key to ouranalysis is an operator theoretic approach that has broad applicability tospectral learning methods.
arxiv-7200-201 | Image Retargeting by Content-Aware Synthesis | http://arxiv.org/pdf/1403.6566v2.pdf | author:Weiming Dong, Fuzhang Wu, Yan Kong, Xing Mei, Tong-Yee Lee, Xiaopeng Zhang category:cs.GR cs.CV published:2014-03-26 summary:Real-world images usually contain vivid contents and rich textural details,which will complicate the manipulation on them. In this paper, we design a newframework based on content-aware synthesis to enhance content-aware imageretargeting. By detecting the textural regions in an image, the textural imagecontent can be synthesized rather than simply distorted or cropped. This methodenables the manipulation of textural & non-textural regions with differentstrategy since they have different natures. We propose to retarget the texturalregions by content-aware synthesis and non-textural regions by fastmulti-operators. To achieve practical retargeting applications for generalimages, we develop an automatic and fast texture detection method that candetect multiple disjoint textural regions. We adjust the saliency of the imageaccording to the features of the textural regions. To validate the proposedmethod, comparisons with state-of-the-art image targeting techniques and a userstudy were conducted. Convincing visual results are shown to demonstrate theeffectiveness of the proposed method.
arxiv-7200-202 | Ranking via Robust Binary Classification and Parallel Parameter Estimation in Large-Scale Data | http://arxiv.org/pdf/1402.2676v4.pdf | author:Hyokun Yun, Parameswaran Raman, S. V. N. Vishwanathan category:stat.ML cs.DC cs.LG stat.CO published:2014-02-11 summary:We propose RoBiRank, a ranking algorithm that is motivated by observing aclose connection between evaluation metrics for learning to rank and lossfunctions for robust classification. The algorithm shows a very competitiveperformance on standard benchmark datasets against other representativealgorithms in the literature. On the other hand, in large scale problems whereexplicit feature vectors and scores are not given, our algorithm can beefficiently parallelized across a large number of machines; for a task thatrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,our algorithm finds solutions that are of dramatically higher quality than thatcan be found by a state-of-the-art competitor algorithm, given the same amountof wall-clock time for computation.
arxiv-7200-203 | Swarm Intelligence Based Multi-phase OPF For Peak Power Loss Reduction In A Smart Grid | http://arxiv.org/pdf/1408.4849v1.pdf | author:Adnan Anwar, A. N. Mahmood category:cs.CE cs.NE published:2014-08-21 summary:Recently there has been increasing interest in improving smart gridsefficiency using computational intelligence. A key challenge in future smartgrid is designing Optimal Power Flow tool to solve important planning problemsincluding optimal DG capacities. Although, a number of OPF tools exists forbalanced networks there is a lack of research for unbalanced multi-phasedistribution networks. In this paper, a new OPF technique has been proposed forthe DG capacity planning of a smart grid. During the formulation of theproposed algorithm, multi-phase power distribution system is considered whichhas unbalanced loadings, voltage control and reactive power compensationdevices. The proposed algorithm is built upon a co-simulation framework thatoptimizes the objective by adapting a constriction factor Particle Swarmoptimization. The proposed multi-phase OPF technique is validated using IEEE8500-node benchmark distribution system.
arxiv-7200-204 | Enhanced Estimation of Autoregressive Wind Power Prediction Model Using Constriction Factor Particle Swarm Optimization | http://arxiv.org/pdf/1408.4792v1.pdf | author:Adnan Anwar, Abdun Naser Mahmood category:cs.CE cs.NE published:2014-08-21 summary:Accurate forecasting is important for cost-effective and efficient monitoringand control of the renewable energy based power generation. Wind based power isone of the most difficult energy to predict accurately, due to the widelyvarying and unpredictable nature of wind energy. Although Autoregressive (AR)techniques have been widely used to create wind power models, they have shownlimited accuracy in forecasting, as well as difficulty in determining thecorrect parameters for an optimized AR model. In this paper, ConstrictionFactor Particle Swarm Optimization (CF-PSO) is employed to optimally determinethe parameters of an Autoregressive (AR) model for accurate prediction of thewind power output behaviour. Appropriate lag order of the proposed model isselected based on Akaike information criterion. The performance of the proposedPSO based AR model is compared with four well-established approaches;Forward-backward approach, Geometric lattice approach, Least-squares approachand Yule-Walker approach, that are widely used for error minimization of the ARmodel. To validate the proposed approach, real-life wind power data of\textit{Capital Wind Farm} was obtained from Australian Energy Market Operator.Experimental evaluation based on a number of different datasets demonstratethat the performance of the AR model is significantly improved compared withbenchmark methods.
arxiv-7200-205 | A Bi-clustering Framework for Consensus Problems | http://arxiv.org/pdf/1405.6159v3.pdf | author:Mariano Tepper, Guillermo Sapiro category:cs.CV cs.LG stat.ML published:2014-04-30 summary:We consider grouping as a general characterization for problems such asclustering, community detection in networks, and multiple parametric modelestimation. We are interested in merging solutions from different groupingalgorithms, distilling all their good qualities into a consensus solution. Inthis paper, we propose a bi-clustering framework and perspective for reachingconsensus in such grouping problems. In particular, this is the first time thatthe task of finding/fitting multiple parametric models to a dataset is formallyposed as a consensus problem. We highlight the equivalence of these tasks andestablish the connection with the computational Gestalt program, that seeks toprovide a psychologically-inspired detection theory for visual events. We alsopresent a simple but powerful bi-clustering algorithm, specially tuned to thenature of the problem we address, though general enough to handle manydifferent instances inscribed within our characterization. The presentation isaccompanied with diverse and extensive experimental results in clustering,community detection, and multiple parametric model estimation in imageprocessing applications.
arxiv-7200-206 | Be Careful When Assuming the Obvious: Commentary on "The placement of the head that minimizes online memory: a complex systems approach" | http://arxiv.org/pdf/1408.4753v1.pdf | author:Phillip M. Alday category:cs.CL published:2014-08-20 summary:Ferrer-i-Cancho (2015) presents a mathematical model of both the synchronicand diachronic nature of word order based on the assumption that memory costsare a never decreasing function of distance and a few very general linguisticassumptions. However, even these minimal and seemingly obvious assumptions arenot as safe as they appear in light of recent typological and psycholinguisticevidence. The interaction of word order and memory has further depths to beexplored.
arxiv-7200-207 | DeepPose: Human Pose Estimation via Deep Neural Networks | http://arxiv.org/pdf/1312.4659v3.pdf | author:Alexander Toshev, Christian Szegedy category:cs.CV published:2013-12-17 summary:We propose a method for human pose estimation based on Deep Neural Networks(DNNs). The pose estimation is formulated as a DNN-based regression problemtowards body joints. We present a cascade of such DNN regressors which resultsin high precision pose estimates. The approach has the advantage of reasoningabout pose in a holistic fashion and has a simple but yet powerful formulationwhich capitalizes on recent advances in Deep Learning. We present a detailedempirical analysis with state-of-art or better performance on four academicbenchmarks of diverse real-world images.
arxiv-7200-208 | Code Generation for High-Level Synthesis of Multiresolution Applications on FPGAs | http://arxiv.org/pdf/1408.4721v1.pdf | author:Moritz Schmid, Oliver Reiche, Christian Schmitt, Frank Hannig, Jürgen Teich category:cs.CV cs.DC cs.PL published:2014-08-20 summary:Multiresolution Analysis (MRA) is a mathematical method that is based onworking on a problem at different scales. One of its applications is medicalimaging where processing at multiple scales, based on the concept of Gaussianand Laplacian image pyramids, is a well-known technique. It is often applied toreduce noise while preserving image detail on different levels of granularitywithout modifying the filter kernel. In scientific computing, multigrid methodsare a popular choice, as they are asymptotically optimal solvers for ellipticPartial Differential Equations (PDEs). As such algorithms have a very highcomputational complexity that would overwhelm CPUs in the presence of real-timeconstraints, application-specific processors come into consideration forimplementation. Despite of huge advancements in leveraging productivity in therespective fields, designers are still required to have detailed knowledgeabout coding techniques and the targeted architecture to achieve efficientsolutions. Recently, the HIPAcc framework was proposed as a means for automaticcode generation of image processing algorithms, based on a Domain-SpecificLanguage (DSL). From the same code base, it is possible to generate code forefficient implementations on several accelerator technologies includingdifferent types of Graphics Processing Units (GPUs) as well as reconfigurablelogic (FPGAs). In this work, we demonstrate the ability of HIPAcc to generatecode for the implementation of multiresolution applications on FPGAs andembedded GPUs.
arxiv-7200-209 | Conic Multi-Task Classification | http://arxiv.org/pdf/1408.4714v1.pdf | author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG published:2014-08-20 summary:Traditionally, Multi-task Learning (MTL) models optimize the average oftask-related objective functions, which is an intuitive approach and which wewill be referring to as Average MTL. However, a more general framework,referred to as Conic MTL, can be formulated by considering conic combinationsof the objective functions instead; in this framework, Average MTL arises as aspecial case, when all combination coefficients equal 1. Although the advantageof Conic MTL over Average MTL has been shown experimentally in previous works,no theoretical justification has been provided to date. In this paper, wederive a generalization bound for the Conic MTL method, and demonstrate thatthe tightest bound is not necessarily achieved, when all combinationcoefficients equal 1; hence, Average MTL may not always be the optimal choice,and it is important to consider Conic MTL. As a byproduct of the generalizationbound, it also theoretically explains the good experimental results of previousrelevant works. Finally, we propose a new Conic MTL model, whose coniccombination coefficients minimize the generalization bound, instead of choosingthem heuristically as has been done in previous methods. The rationale andadvantage of our model is demonstrated and verified via a series of experimentsby comparing with several other methods.
arxiv-7200-210 | GIMP and Wavelets for Medical Image Processing: Enhancing Images of the Fundus of the Eye | http://arxiv.org/pdf/1408.4703v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2014-08-20 summary:The visual analysis of retina and of its vascular characteristics isimportant in the diagnosis and monitoring of diseases of visual perception. Inthe related medical diagnoses, the digital processing of the fundus images isused to obtain the segmentation of retinal vessels. However, an imagesegmentation is often requiring methods based on peculiar or complexalgorithms: in this paper we will show some alternative approaches obtained byapplying freely available tools to enhance, without a specific segmentation,the images of the fundus of the eye. We will see in particular, that combiningthe use of GIMP, the GNU Image Manipulation Program, with the wavelet filter ofIris, a program well-known for processing astronomical images, the result isgiving images which can be alternative of those obtained from segmentation.
arxiv-7200-211 | Seeing through bag-of-visual-word glasses: towards understanding quantization effects in feature extraction methods | http://arxiv.org/pdf/1408.4692v1.pdf | author:Alexander Freytag, Johannes Rühle, Paul Bodesheim, Erik Rodner, Joachim Denzler category:cs.CV published:2014-08-20 summary:Vector-quantized local features frequently used in bag-of-visual-wordsapproaches are the backbone of popular visual recognition systems due to boththeir simplicity and their performance. Despite their success,bag-of-words-histograms basically contain low-level image statistics (e.g.,number of edges of different orientations). The question remains how muchvisual information is "lost in quantization" when mapping visual features tocode words? To answer this question, we present an in-depth analysis of theeffect of local feature quantization on human recognition performance. Ouranalysis is based on recovering the visual information by inverting quantizedlocal features and presenting these visualizations with different codebooksizes to human observers. Although feature inversion techniques are around forquite a while, to the best of our knowledge, our technique is the firstvisualizing especially the effect of feature quantization. Thereby, we are nowable to compare single steps in common image classification pipelines to humancounterparts.
arxiv-7200-212 | Horn functions and the AFP Algorithm | http://arxiv.org/pdf/1408.4673v1.pdf | author:Rooholah Majdodin category:cs.LG 68Q32, 68T27 published:2014-08-20 summary:It is described why multiple refinements with each negative counterexampledoes not improve the complexity of the AFP Algorithm. Also Canonical normalformulas for Horn functions are discussed.
arxiv-7200-213 | A new integral loss function for Bayesian optimization | http://arxiv.org/pdf/1408.4622v1.pdf | author:Emmanuel Vazquez, Julien Bect category:stat.CO cs.LG math.OC stat.ML published:2014-08-20 summary:We consider the problem of maximizing a real-valued continuous function $f$using a Bayesian approach. Since the early work of Jonas Mockus and Antanas\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated byconsidering the loss function $\max f - M_n$ (where $M_n$ denotes the bestfunction value observed after $n$ evaluations of $f$). This loss function putsemphasis on the value of the maximum, at the expense of the location of themaximizer. In the special case of a one-step Bayes-optimal strategy, it leadsto the classical Expected Improvement (EI) sampling criterion. This is aspecial case of a Stepwise Uncertainty Reduction (SUR) strategy, where the riskassociated to a certain uncertainty measure (here, the expected loss) on thequantity of interest is minimized at each step of the algorithm. In thisarticle, assuming that $f$ is defined over a measure space $(\mathbb{X},\lambda)$, we propose to consider instead the integral loss function$\int_{\mathbb{X}} (f - M_n)_{+}\, d\lambda$, and we show that this leads, inthe case of a Gaussian process prior, to a new numerically tractable samplingcriterion that we call $\rm EI^2$ (for Expected Integrated ExpectedImprovement). A numerical experiment illustrates that a SUR strategy based onthis new sampling criterion reduces the error on both the value and thelocation of the maximizer faster than the EI-based strategy.
arxiv-7200-214 | EURETILE D7.3 - Dynamic DAL benchmark coding, measurements on MPI version of DPSNN-STDP (distributed plastic spiking neural net) and improvements to other DAL codes | http://arxiv.org/pdf/1408.4587v1.pdf | author:Pier Stanislao Paolucci, Iuliana Bacivarov, Devendra Rai, Lars Schor, Lothar Thiele, Hoeseok Yang, Elena Pastorelli, Roberto Ammendola, Andrea Biagioni, Ottorino Frezza, Francesca Lo Cicero, Alessandro Lonardo, Francesco Simula, Laura Tosoratto, Piero Vicini category:cs.DC cs.CE cs.MS cs.NE q-bio.NC published:2014-08-20 summary:The EURETILE project required the selection and coding of a set of dedicatedbenchmarks. The project is about the software and hardware architecture offuture many-tile distributed fault-tolerant systems. We focus on dynamicworkloads characterised by heavy numerical processing requirements. Theambition is to identify common techniques that could be applied to both theEmbedded Systems and HPC domains. This document is the first public deliverableof Work Package 7: Challenging Tiled Applications.
arxiv-7200-215 | Optimal rates for zero-order convex optimization: the power of two function evaluations | http://arxiv.org/pdf/1312.2139v2.pdf | author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright, Andre Wibisono category:math.OC cs.IT math.IT stat.ML published:2013-12-07 summary:We consider derivative-free algorithms for stochastic and non-stochasticconvex optimization problems that use only function values rather thangradients. Focusing on non-asymptotic bounds on convergence rates, we show thatif pairs of function values are available, algorithms for $d$-dimensionaloptimization that use gradient estimates based on random perturbations suffer afactor of at most $\sqrt{d}$ in convergence rate over traditional stochasticgradient methods. We establish such results for both smooth and non-smoothcases, sharpening previous analyses that suggested a worse dimensiondependence, and extend our results to the case of multiple ($m \ge 2$)evaluations. We complement our algorithmic development withinformation-theoretic lower bounds on the minimax convergence rate of suchproblems, establishing the sharpness of our achievable results up to constant(sometimes logarithmic) factors.
arxiv-7200-216 | Modulation Classification via Gibbs Sampling Based on a Latent Dirichlet Bayesian Network | http://arxiv.org/pdf/1408.0765v2.pdf | author:Yu Liu, Osvaldo Simeone, Alexander M. Haimovich, Wei Su category:cs.IT cs.CV math.IT published:2014-08-04 summary:A novel Bayesian modulation classification scheme is proposed for asingle-antenna system over frequency-selective fading channels. The method isbased on Gibbs sampling as applied to a latent Dirichlet Bayesian network (BN).The use of the proposed latent Dirichlet BN provides a systematic solution tothe convergence problem encountered by the conventional Gibbs sampling approachfor modulation classification. The method generalizes, and is shown to improveupon, the state of the art.
arxiv-7200-217 | Unsupervised Parallel Extraction based Texture for Efficient Image Representation | http://arxiv.org/pdf/1408.4504v1.pdf | author:Mohammed M. Abdelsamea category:cs.CV published:2014-08-20 summary:SOM is a type of unsupervised learning where the goal is to discover someunderlying structure of the data. In this paper, a new extraction method basedon the main idea of Concurrent Self-Organizing Maps (CSOM), representing awinner-takes-all collection of small SOM networks is proposed. Each SOM of thesystem is trained individually to provide best results for one class only. Theexperiments confirm that the proposed features based CSOM is capable torepresent image content better than extracted features based on a single bigSOM and these proposed features improve the final decision of the CAD.Experiments held on Mammographic Image Analysis Society (MIAS) dataset.
arxiv-7200-218 | On Optimal Decision-Making in Ant Colonies | http://arxiv.org/pdf/1408.4487v1.pdf | author:Mahnush Movahedi, Mahdi Zamani category:cs.DC cs.NE published:2014-08-19 summary:Colonies of ants can collectively choose the best of several nests, even whenmany of the active ants who organize the move visit only one site.Understanding such a behavior can help us design efficient distributed decisionmaking algorithms. Marshall et al. propose a model for house-hunting incolonies of ant Temnothorax albipennis. Unfortunately, their model does notachieve optimal decision-making while laboratory experiments show that, infact, colonies usually achieve optimality during the house-hunting process. Inthis paper, we argue that the model of Marshall et al. can achieve optimalityby including nest size information in their mathematical model. We use labresults of Pratt et al. to re-define the differential equations of Marshall etal. Finally, we sketch our strategy for testing the optimality of the newmodel.
arxiv-7200-219 | PGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical Data Problems | http://arxiv.org/pdf/1407.5656v2.pdf | author:Khalifeh AlJadda, Mohammed Korayem, Camilo Ortiz, Trey Grainger, John A. Miller, William S. York category:cs.AI cs.LG published:2014-07-21 summary:In the big data era, scalability has become a crucial requirement for anyuseful computational model. Probabilistic graphical models are very useful formining and discovering data insights, but they are not scalable enough to besuitable for big data problems. Bayesian Networks particularly demonstrate thislimitation when their data is represented using few random variables while eachrandom variable has a massive set of values. With hierarchical data - data thatis arranged in a treelike structure with several levels - one would expect tosee hundreds of thousands or millions of values distributed over even just asmall number of levels. When modeling this kind of hierarchical data acrosslarge data sets, Bayesian networks become infeasible for representing theprobability distributions for the following reasons: i) Each level represents asingle random variable with hundreds of thousands of values, ii) The number oflevels is usually small, so there are also few random variables, and iii) Thestructure of the network is predefined since the dependency is modeled top-downfrom each parent to each of its child nodes, so the network would contain asingle linear path for the random variables from each parent to each childnode. In this paper we present a scalable probabilistic graphical model toovercome these limitations for massive hierarchical data. We believe theproposed model will lead to an easily-scalable, more readable, and expressiveimplementation for problems that require probabilistic-based solutions formassive amounts of hierarchical data. We successfully applied this model tosolve two different challenging probabilistic-based problems on massivehierarchical data sets for different domains, namely, bioinformatics and latentsemantic discovery over search logs.
arxiv-7200-220 | Object Detection with Pixel Intensity Comparisons Organized in Decision Trees | http://arxiv.org/pdf/1305.4537v5.pdf | author:Nenad Markuš, Miroslav Frljak, Igor S. Pandžić, Jörgen Ahlberg, Robert Forchheimer category:cs.CV published:2013-05-20 summary:We describe a method for visual object detection based on an ensemble ofoptimized decision trees organized in a cascade of rejectors. The trees usepixel intensity comparisons in their internal nodes and this makes them able toprocess image regions very fast. Experimental analysis is provided through aface detection problem. The obtained results are encouraging and demonstratethat the method has practical value. Additionally, we analyse its sensitivityto noise and show how to perform fast rotation invariant object detection.Complete source code is provided at https://github.com/nenadmarkus/pico.
arxiv-7200-221 | Object Segmentation in Images using EEG Signals | http://arxiv.org/pdf/1408.4363v1.pdf | author:Eva Mohedano, Graham Healy, Kevin McGuinness, Xavier Giro-i-Nieto, Noel E. O'Connor, Alan F. Smeaton category:cs.CV cs.MM published:2014-08-19 summary:This paper explores the potential of brain-computer interfaces in segmentingobjects from images. Our approach is centered around designing an effectivemethod for displaying the image parts to the users such that they generatemeasurable brain reactions. When an image region, specifically a block ofpixels, is displayed we estimate the probability of the block containing theobject of interest using a score based on EEG activity. After several suchblocks are displayed, the resulting probability map is binarized and combinedwith the GrabCut algorithm to segment the image into object and backgroundregions. This study shows that BCI and simple EEG analysis are useful inlocating object boundaries in images.
arxiv-7200-222 | What Regularized Auto-Encoders Learn from the Data Generating Distribution | http://arxiv.org/pdf/1211.4246v5.pdf | author:Guillaume Alain, Yoshua Bengio category:cs.LG stat.ML published:2012-11-18 summary:What do auto-encoders learn about the underlying data generatingdistribution? Recent work suggests that some auto-encoder variants do a goodjob of capturing the local manifold structure of data. This paper clarifiessome of these previous observations by showing that minimizing a particularform of regularized reconstruction error yields a reconstruction function thatlocally characterizes the shape of the data generating density. We show thatthe auto-encoder captures the score (derivative of the log-density with respectto the input). It contradicts previous interpretations of reconstruction erroras an energy function. Unlike previous results, the theorems provided here arecompletely generic and do not depend on the parametrization of theauto-encoder: they show what the auto-encoder would tend to if given enoughcapacity and examples. These results are for a contractive training criterionwe show to be similar to the denoising auto-encoder training criterion withsmall corruption noise, but with contraction applied on the wholereconstruction function rather than just encoder. Similarly to score matching,one can consider the proposed training criterion as a convenient alternative tomaximum likelihood because it does not involve a partition function. Finally,we show how an approximate Metropolis-Hastings MCMC can be setup to recoversamples from the estimated distribution, and this is confirmed in samplingexperiments.
arxiv-7200-223 | The Algebraic Combinatorial Approach for Low-Rank Matrix Completion | http://arxiv.org/pdf/1211.4116v4.pdf | author:Franz J. Király, Louis Theran, Ryota Tomioka category:cs.LG cs.NA math.AG math.CO stat.ML published:2012-11-17 summary:We present a novel algebraic combinatorial view on low-rank matrix completionbased on studying relations between a few entries with tools from algebraicgeometry and matroid theory. The intrinsic locality of the approach allows forthe treatment of single entries in a closed theoretical and practicalframework. More specifically, apart from introducing an algebraic combinatorialtheory of low-rank matrix completion, we present probability-one algorithms todecide whether a particular entry of the matrix can be completed. We alsodescribe methods to complete that entry from a few others, and to estimate theerror which is incurred by any method completing that entry. Furthermore, weshow how known results on matrix completion and their sampling assumptions canbe related to our new perspective and interpreted in terms of a completabilityphase transition.
arxiv-7200-224 | What makes an Image Iconic? A Fine-Grained Case Study | http://arxiv.org/pdf/1408.4325v1.pdf | author:Yangmuzi Zhang, Diane Larlus, Florent Perronnin category:cs.CV published:2014-08-19 summary:A natural approach to teaching a visual concept, e.g. a bird species, is toshow relevant images. However, not all relevant images represent a conceptequally well. In other words, they are not necessarily iconic. This observationraises three questions. Is iconicity a subjective property? If not, can wepredict iconicity? And what exactly makes an image iconic? We provide answersto these questions through an extensive experimental study on a challengingfine-grained dataset of birds. We first show that iconicity ratings areconsistent across individuals, even when they are not domain experts, thusdemonstrating that iconicity is not purely subjective. We then consider anexhaustive list of properties that are intuitively related to iconicity andmeasure their correlation with these iconicity ratings. We combine them topredict iconicity of new unseen images. We also propose a direct iconicitypredictor that is discriminatively trained with iconicity ratings. By combiningboth systems, we get an iconicity prediction that approaches human performance.
arxiv-7200-225 | Hierarchical Clustering of Hyperspectral Images using Rank-Two Nonnegative Matrix Factorization | http://arxiv.org/pdf/1310.7441v4.pdf | author:Nicolas Gillis, Da Kuang, Haesun Park category:cs.CV cs.IR math.OC published:2013-09-14 summary:In this paper, we design a hierarchical clustering algorithm forhigh-resolution hyperspectral images. At the core of the algorithm, a newrank-two nonnegative matrix factorizations (NMF) algorithm is used to split theclusters, which is motivated by convex geometry concepts. The method startswith a single cluster containing all pixels, and, at each step, (i) selects acluster in such a way that the error at the next step is minimized, and (ii)splits the selected cluster into two disjoint clusters using rank-two NMF insuch a way that the clusters are well balanced and stable. The proposed methodcan also be used as an endmember extraction algorithm in the presence of purepixels. The effectiveness of this approach is illustrated on several syntheticand real-world hyperspectral images, and shown to outperform standardclustering techniques such as k-means, spherical k-means and standard NMF.
arxiv-7200-226 | Towards crowdsourcing and cooperation in linguistic resources | http://arxiv.org/pdf/1408.4245v1.pdf | author:Dmitry Ustalov category:cs.SI cs.CL K.4.3 published:2014-08-19 summary:Linguistic resources can be populated with data through the use of suchapproaches as crowdsourcing and gamification when motivated people areinvolved. However, current crowdsourcing genre taxonomies lack the concept ofcooperation, which is the principal element of modern video games and maypotentially drive the annotators' interest. This survey on crowdsourcingtaxonomies and cooperation in linguistic resources provides recommendations onusing cooperation in existent genres of crowdsourcing and an evidence of theefficiency of cooperation using a popular Russian linguistic resource createdthrough crowdsourcing as an example.
arxiv-7200-227 | Can Artificial Neural Networks be Applied in Seismic Predicition? Preliminary Analysis Applying Radial Topology. Case: Mexico | http://arxiv.org/pdf/1408.4222v1.pdf | author:Cinthya Mota-Hernandez, Luis Esquivel-Rodriguez, Rafael Alvarado-Corona category:cs.NE physics.geo-ph published:2014-08-19 summary:Tectonic earthquakes of high magnitude can cause considerable losses in termsof human lives, economic and infrastructure, among others. According to anevaluation published by the U.S. Geological Survey, 30 is the number ofearthquakes which have greatly impacted Mexico from the end of the XIX centuryto this one. Based upon data from the National Seismological Service, on theperiod between January 1, 2006 and May 1, 2013 there have occurred 5,826earthquakes which magnitude has been greater than 4.0 degrees on the Richtermagnitude scale (25.54% of the total of earthquakes registered on the nationalterritory), being the Pacific Plate and the Cocos Plate the most importantones. This document describes the development of an Artificial Neural Network(ANN) based on the radial topology which seeks to generate a prediction with anerror margin lower than 20% which can inform about the probability of a futureearthquake one of the main questions is: can artificial neural networks beapplied in seismic forecasting? It can be argued that research has thepotential to bring in the forecast seismic, more research is needed toconsolidate data and help mitigate the impact caused by such events linked withsociety. Keywords--- Analysis, Mexico, Neural Artificial Networks, Seismicity.
arxiv-7200-228 | BET: Bayesian Ensemble Trees for Clustering and Prediction in Heterogeneous Data | http://arxiv.org/pdf/1408.4140v1.pdf | author:Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak category:stat.ML stat.CO published:2014-08-18 summary:We propose a novel "tree-averaging" model that utilizes the ensemble ofclassification and regression trees (CART). Each constituent tree is estimatedwith a subset of similar data. We treat this grouping of subsets as Bayesianensemble trees (BET) and model them as an infinite mixture Dirichlet process.We show that BET adapts to data heterogeneity and accurately estimates eachcomponent. Compared with the bootstrap-aggregating approach, BET shows improvedprediction performance with fewer trees. We develop an efficient estimatingprocedure with improved sampling strategies in both CART and mixture models. Wedemonstrate these advantages of BET with simulations, classification of breastcancer and regression of lung function measurement of cystic fibrosis patients. Keywords: Bayesian CART; Dirichlet Process; Ensemble Approach; Heterogeneity;Mixture of Trees.
arxiv-7200-229 | Brain: Biological noise-based logic | http://arxiv.org/pdf/1408.4077v1.pdf | author:Laszlo B. Kish, Claes-Goran Granqvist, Sergey M. Bezrukov, Tamas Horvath category:cs.NE cs.ET published:2014-08-18 summary:Neural spikes in the brain form stochastic sequences, i.e., belong to theclass of pulse noises. This stochasticity is a counterintuitive feature becauseextracting information - such as the commonly supposed neural information ofmean spike frequency - requires long times for reasonably low errorprobability. The mystery could be solved by noise-based logic, whereinrandomness has an important function and allows large speed enhancements forspecial-purpose tasks, and the same mechanism is at work for the brain logicversion of this concept.
arxiv-7200-230 | Offline Signature-Based Fuzzy Vault (OSFV: Review and New Results | http://arxiv.org/pdf/1408.3985v1.pdf | author:George S. Eskander, Robert Sabourin, Eric Granger category:cs.CV cs.CR published:2014-08-18 summary:An offline signature-based fuzzy vault (OSFV) is a bio-cryptographicimplementation that uses handwritten signature images as biometrics instead oftraditional passwords to secure private cryptographic keys. Having a reliableOSFV implementation is the first step towards automating financial and legalauthentication processes, as it provides greater security of confidentialdocuments by means of the embedded handwritten signatures. The authors haverecently proposed the first OSFV implementation which is reviewed in thispaper. In this system, a machine learning approach based on the dissimilarityrepresentation concept is employed to select a reliable feature representationadapted for the fuzzy vault scheme. Some variants of this system are proposedfor enhanced accuracy and security. In particular, a new method that adaptsuser key size is presented. Performance of proposed methods are compared usingthe Brazilian PUCPR and GPDS signature databases and results indicate that thekey-size adaptation method achieves a good compromise between security andaccuracy. While average system entropy is increased from 45-bits to about51-bits, the AER (average error rate) is decreased by about 21%.
arxiv-7200-231 | On Detecting Messaging Abuse in Short Text Messages using Linguistic and Behavioral patterns | http://arxiv.org/pdf/1408.3934v1.pdf | author:Alejandro Mosquera, Lamine Aouad, Slawomir Grzonkowski, Dylan Morss category:cs.CL cs.AI cs.SI published:2014-08-18 summary:The use of short text messages in social media and instant messaging hasbecome a popular communication channel during the last years. This risingpopularity has caused an increment in messaging threats such as spam, phishingor malware as well as other threats. The processing of these short text messagethreats could pose additional challenges such as the presence of lexicalvariants, SMS-like contractions or advanced obfuscations which can degrade theperformance of traditional filtering solutions. By using a real-world SMS dataset from a large telecommunications operator from the US and a social mediacorpus, in this paper we analyze the effectiveness of machine learning filtersbased on linguistic and behavioral patterns in order to detect short text spamand abusive users in the network. We have also explored different ways to dealwith short text message challenges such as tokenization and entity detection byusing text normalization and substring clustering techniques. The obtainedresults show the validity of the proposed solution by enhancing baselineapproaches.
arxiv-7200-232 | Bayesian image segmentations by Potts prior and loopy belief propagation | http://arxiv.org/pdf/1404.3012v5.pdf | author:Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Yuji Waizumi, Chiou-Ting Hsu category:cs.CV cs.LG stat.ML published:2014-04-11 summary:This paper presents a Bayesian image segmentation model based on Potts priorand loopy belief propagation. The proposed Bayesian model involves severalterms, including the pairwise interactions of Potts models, and the averagevectors and covariant matrices of Gauss distributions in color image modeling.These terms are often referred to as hyperparameters in statistical machinelearning theory. In order to determine these hyperparameters, we propose a newscheme for hyperparameter estimation based on conditional maximization ofentropy in the Potts prior. The algorithm is given based on loopy beliefpropagation. In addition, we compare our conditional maximum entropy frameworkwith the conventional maximum likelihood framework, and also clarify how thefirst order phase transitions in LBP's for Potts models influence ourhyperparameter estimation procedures.
arxiv-7200-233 | Opinion mining of movie reviews at document level | http://arxiv.org/pdf/1408.3829v1.pdf | author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.IR cs.CL published:2014-08-17 summary:The whole world is changed rapidly and using the current technologiesInternet becomes an essential need for everyone. Web is used in every field.Most of the people use web for a common purpose like online shopping, chattingetc. During an online shopping large number of reviews/opinions are given bythe users that reflect whether the product is good or bad. These reviews needto be explored, analyse and organized for better decision making. OpinionMining is a natural language processing task that deals with findingorientation of opinion in a piece of text with respect to a topic. In thispaper a document based opinion mining system is proposed that classify thedocuments as positive, negative and neutral. Negation is also handled in theproposed system. Experimental results using reviews of movies show theeffectiveness of the system.
arxiv-7200-234 | Unsupervised learning segmentation for dynamic speckle activity images | http://arxiv.org/pdf/1408.3818v1.pdf | author:Lucia I. Passoni, Ana I. Dai Pra, Gustavo J. Meschino, MArcelo Guzman, Chistian Weber, Héctor Rabal, Marcelo Trivi category:physics.optics cs.CV published:2014-08-17 summary:This paper proposes the design of decision models based on ComputationalIntelligence techniques applied to image sequences of dynamic laser speckle.These models aim to identify image regions of biological specimens illuminatedby a coherent beam coming from a laser. The field image is pseudo colored usinga Self Organizing Map projection. This process is carried out using a set ofdescriptors applied to the intensity variations along time in every pixel of animage sequence. The models use descriptors selected to improve effectiveness,depending on the specific application. We present two examples of theapplication of the proposed techniques to assess biological tissues. Theresults obtained are encouraging and significantly improve those obtained usinga single descriptor.
arxiv-7200-235 | Robust Statistical Approach for Extraction of Moving Human Silhouettes from Videos | http://arxiv.org/pdf/1408.3814v1.pdf | author:Oinam Binarani Devi, Nissi S. Paul, Y. Jayanta Singh category:cs.CV published:2014-08-17 summary:Human pose estimation is one of the key problems in computer vision that hasbeen studied in the recent years. The significance of human pose estimation isin the higher level tasks of understanding human actions applications such asrecognition of anomalous actions present in videos and many other relatedapplications. The human poses can be estimated by extracting silhouettes ofhumans as silhouettes are robust to variations and it gives the shapeinformation of the human body. Some common challenges include illuminationchanges, variation in environments, and variation in human appearances. Thusthere is a need for a robust method for human pose estimation. This paperpresents a study and analysis of approaches existing for silhouette extractionand proposes a robust technique for extracting human silhouettes in videosequences. Gaussian Mixture Model (GMM) A statistical approach is combined withHSV (Hue, Saturation and Value) color space model for a robust background modelthat is used for background subtraction to produce foreground blobs, calledhuman silhouettes. Morphological operations are then performed on foregroundblobs from background subtraction. The silhouettes obtained from this work canbe used in further tasks associated with human action interpretation andactivity processes like human action classification, human pose estimation andaction recognition or action interpretation.
arxiv-7200-236 | On solving Ordinary Differential Equations using Gaussian Processes | http://arxiv.org/pdf/1408.3807v1.pdf | author:David Barber category:stat.ME cs.NA math.NA stat.CO stat.ML published:2014-08-17 summary:We describe a set of Gaussian Process based approaches that can be used tosolve non-linear Ordinary Differential Equations. We suggest an explicitprobabilistic solver and two implicit methods, one analogous to Picarditeration and the other to gradient matching. All methods have greater accuracythan previously suggested Gaussian Process approaches. We also suggest ageneral approach that can yield error estimates from any standard ODE solver.
arxiv-7200-237 | Real-time emotion recognition for gaming using deep convolutional network features | http://arxiv.org/pdf/1408.3750v1.pdf | author:Sébastien Ouellet category:cs.CV cs.LG cs.NE published:2014-08-16 summary:The goal of the present study is to explore the application of deepconvolutional network features to emotion recognition. Results indicate thatthey perform similarly to other published models at a best recognition rate of94.4%, and do so with a single still image rather than a video stream. Animplementation of an affective feedback game is also described, where aclassifier using these features tracks the facial expressions of a player inreal-time.
arxiv-7200-238 | A fast patch-dictionary method for whole image recovery | http://arxiv.org/pdf/1408.3740v1.pdf | author:Yangyang Xu, Wotao Yin category:cs.CV math.OC 94A08, 94A12 published:2014-08-16 summary:Various algorithms have been proposed for dictionary learning. Among thosefor image processing, many use image patches to form dictionaries. This paperfocuses on whole-image recovery from corrupted linear measurements. We addressthe open issue of representing an image by overlapping patches: the overlappingleads to an excessive number of dictionary coefficients to determine. With veryfew exceptions, this issue has limited the applications of image-patch methodsto the local kind of tasks such as denoising, inpainting, cartoon-texturedecomposition, super-resolution, and image deblurring, for which one canprocess a few patches at a time. Our focus is global imaging tasks such ascompressive sensing and medical image recovery, where the whole image isencoded together, making it either impossible or very ineffective to update afew patches at a time. Our strategy is to divide the sparse recovery into multiple subproblems, eachof which handles a subset of non-overlapping patches, and then the results ofthe subproblems are averaged to yield the final recovery. This simple strategyis surprisingly effective in terms of both quality and speed. In addition, weaccelerate computation of the learned dictionary by applying a recent blockproximal-gradient method, which not only has a lower per-iteration complexitybut also takes fewer iterations to converge, compared to the currentstate-of-the-art. We also establish that our algorithm globally converges to astationary point. Numerical results on synthetic data demonstrate that ouralgorithm can recover a more faithful dictionary than two state-of-the-artmethods. Combining our whole-image recovery and dictionary-learning methods, wenumerically simulate image inpainting, compressive sensing recovery, anddeblurring. Our recovery is more faithful than those of a total variationmethod and a method based on overlapping patches.
arxiv-7200-239 | Analysis of a chaotic spiking neural model: The NDS neuron | http://arxiv.org/pdf/1408.3735v1.pdf | author:Mohammad Alhawarat, Waleed Nazih, Mohammad Eldesouki category:cs.NE nlin.CD published:2014-08-16 summary:Further analysis and experimentation is carried out in this paper for achaotic dynamic model, viz. the Nonlinear Dynamic State neuron (NDS). Theanalysis and experimentations are performed to further understand theunderlying dynamics of the model and enhance it as well. Chaos provides manyinteresting properties that can be exploited to achieve computational tasks.Such properties are sensitivity to initial conditions, space filling, controland synchronization.Chaos might play an important role in informationprocessing tasks in human brain as suggested by biologists. If artificialneural networks (ANNs) is equipped with chaos then it will enrich the dynamicbehaviours of such networks. The NDS model has some limitations and can beovercome in different ways. In this paper different approaches are followed topush the boundaries of the NDS model in order to enhance it. One way is tostudy the effects of scaling the parameters of the chaotic equations of the NDSmodel and study the resulted dynamics. Another way is to study the method thatis used in discretization of the original R\"{o}ssler that the NDS model isbased on. These approaches have revealed some facts about the NDS attractor andsuggest why such a model can be stabilized to large number of unstable periodicorbits (UPOs) which might correspond to memories in phase space.
arxiv-7200-240 | Multi-Sensor Event Detection using Shape Histograms | http://arxiv.org/pdf/1408.3733v1.pdf | author:Ehtesham Hassan, Gautam Shroff, Puneet Agarwal category:cs.LG published:2014-08-16 summary:Vehicular sensor data consists of multiple time-series arising from a numberof sensors. Using such multi-sensor data we would like to detect occurrences ofspecific events that vehicles encounter, e.g., corresponding to particularmaneuvers that a vehicle makes or conditions that it encounters. Events arecharacterized by similar waveform patterns re-appearing within one or moresensors. Further such patterns can be of variable duration. In this work, wepropose a method for detecting such events in time-series data using a novelfeature descriptor motivated by similar ideas in image processing. We definethe shape histogram: a constant dimension descriptor that nevertheless capturespatterns of variable duration. We demonstrate the efficacy of using shapehistograms as features to detect events in an SVM-based, multi-sensor,supervised learning scenario, i.e., multiple time-series are used to detect anevent. We present results on real-life vehicular sensor data and show that ourtechnique performs better than available pattern detection implementations onour data, and that it can also be used to combine features from multiplesensors resulting in better accuracy than using any single sensor. Sinceprevious work on pattern detection in time-series has been in the single seriescontext, we also present results using our technique on multiple standardtime-series datasets and show that it is the most versatile in terms of how itranks compared to other published results.
arxiv-7200-241 | Robust 3D face recognition in presence of pose and partial occlusions or missing parts | http://arxiv.org/pdf/1408.3709v1.pdf | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri category:cs.CV published:2014-08-16 summary:In this paper, we propose a robust 3D face recognition system which canhandle pose as well as occlusions in real world. The system at first takes asinput, a 3D range image, simultaneously registers it using ICP(IterativeClosest Point) algorithm. ICP used in this work, registers facial surfaces to acommon model by minimizing distances between a probe model and a gallery model.However the performance of ICP relies heavily on the initial conditions. Hence,it is necessary to provide an initial registration, which will be improvediteratively and finally converge to the best alignment possible. Once the facesare registered, the occlusions are automatically extracted by thresholding thedepth map values of the 3D image. After the occluded regions are detected,restoration is done by Principal Component Analysis (PCA). The restored images,after the removal of occlusions, are then fed to the recognition system forclassification purpose. Features are extracted from the reconstructednon-occluded face images in the form of face normals. The experimental resultswhich were obtained on the occluded facial images from the Bosphorus 3D facedatabase, illustrate that our occlusion compensation scheme has attained arecognition accuracy of 91.30%.
arxiv-7200-242 | Turkish Presidential Elections TRT Publicity Speech Facial Expression Analysis | http://arxiv.org/pdf/1408.3573v1.pdf | author:H. Emrah Tasli, Paul Ivan category:cs.CV published:2014-08-15 summary:In this paper, facial expressions of the three Turkish presidentialcandidates Demirtas, Erdogan and Ihsanoglu (in alphabetical order) are analyzedduring the publicity speeches featured at TRT (Turkish Radio and Television) on03.08.2014. FaceReader is used for the analysis where 3D modeling of the faceis achieved using the active appearance models (AAM). Over 500 landmark pointsare tracked and analyzed for obtaining the facial expressions during the wholespeech. All source videos and the data are publicly available for researchpurposes.
arxiv-7200-243 | Detection is the central problem in real-word spelling correction | http://arxiv.org/pdf/1408.3153v2.pdf | author:L. Amber Wilcox-O'Hearn category:cs.CL published:2014-08-13 summary:Real-word spelling correction differs from non-word spelling correction inits aims and its challenges. Here we show that the central problem in real-wordspelling correction is detection. Methods from non-word spelling correction,which focus instead on selection among candidate corrections, do not addressdetection adequately, because detection is either assumed in advance or heavilyconstrained. As we demonstrate in this paper, merely discriminating between theintended word and a random close variation of it within the context of asentence is a task that can be performed with high accuracy usingstraightforward models. Trigram models are sufficient in almost all cases. Thedifficulty comes when every word in the sentence is a potential error, with alarge set of possible candidate corrections. Despite their strengths, trigrammodels cannot reliably find true errors without introducing many more, at leastnot when used in the obvious sequential way without added structure. Thedetection task exposes weakness not visible in the selection task.
arxiv-7200-244 | Indexing Cost Sensitive Prediction | http://arxiv.org/pdf/1408.4072v1.pdf | author:Leilani Battle, Edward Benson, Aditya Parameswaran, Eugene Wu category:cs.LG cs.DB cs.DS published:2014-08-15 summary:Predictive models are often used for real-time decision making. However,typical machine learning techniques ignore feature evaluation cost, and focussolely on the accuracy of the machine learning models obtained utilizing allthe features available. We develop algorithms and indexes to supportcost-sensitive prediction, i.e., making decisions using machine learning modelstaking feature evaluation cost into account. Given an item and a onlinecomputation cost (i.e., time) budget, we present two approaches to return anappropriately chosen machine learning model that will run within the specifiedtime on the given item. The first approach returns the optimal machine learningmodel, i.e., one with the highest accuracy, that runs within the specifiedtime, but requires significant up-front precomputation time. The secondapproach returns a possibly sub- optimal machine learning model, but requireslittle up-front precomputation time. We study these two algorithms in detailand characterize the scenarios (using real and synthetic data) in which eachperforms well. Unlike prior work that focuses on a narrow domain or a specificalgorithm, our techniques are very general: they apply to any cost-sensitiveprediction scenario on any machine learning algorithm.
arxiv-7200-245 | Robust Statistical Ranking: Theory and Algorithms | http://arxiv.org/pdf/1408.3467v1.pdf | author:Qianqian Xu, Jiechao Xiong, Qingming Huang, Yuan Yao category:stat.ME cs.LG stat.ML published:2014-08-15 summary:Deeply rooted in classical social choice and voting theory, statisticalranking with paired comparison data experienced its renaissance with the widespread of crowdsourcing technique. As the data quality might be significantlydamaged in an uncontrolled crowdsourcing environment, outlier detection androbust ranking have become a hot topic in such data analysis. In this paper, wepropose a robust ranking framework based on the principle of Huber's robuststatistics, which formulates outlier detection as a LASSO problem to findsparse approximations of the cyclic ranking projection in Hodge decomposition.Moreover, simple yet scalable algorithms are developed based on LinearizedBregman Iteration to achieve an even less biased estimator than LASSO.Statistical consistency of outlier detection is established in both cases whichstates that when the outliers are strong enough and in Erdos-Renyi random graphsampling settings, outliers can be faithfully detected. Our studies aresupported by experiments with both simulated examples and real-world data. Theproposed framework provides us a promising tool for robust ranking with largescale crowdsourcing data arising from computer vision, multimedia, machinelearning, sociology, etc.
arxiv-7200-246 | Classifiers fusion method to recognize handwritten persian numerals | http://arxiv.org/pdf/1407.2572v2.pdf | author:Reza Azad, Babak Azad, Iraj Mogharreb, Shahram Jamali category:cs.CV published:2014-07-09 summary:Recognition of Persian handwritten characters has been considered as asignificant field of research for the last few years under pattern analysingtechnique. In this paper, a new approach for robust handwritten Persiannumerals recognition using strong feature set and a classifier fusion method isscrutinized to increase the recognition percentage. For implementing theclassifier fusion technique, we have considered k nearest neighbour (KNN),linear classifier (LC) and support vector machine (SVM) classifiers. Theinnovation of this tactic is to attain better precision with few features usingclassifier fusion method. For evaluation of the proposed method we considered aPersian numerals database with 20,000 handwritten samples. Spending 15,000samples for training stage, we verified our technique on other 5,000 samples,and the correct recognition ratio achieved approximately 99.90%. Additional, wegot 99.97% exactness using four-fold cross validation procedure on 20,000databases.
arxiv-7200-247 | SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation | http://arxiv.org/pdf/1408.3456v1.pdf | author:Felix Hill, Roi Reichart, Anna Korhonen category:cs.CL published:2014-08-15 summary:We present SimLex-999, a gold standard resource for evaluating distributionalsemantic models that improves on existing resources in several important ways.First, in contrast to gold standards such as WordSim-353 and MEN, it explicitlyquantifies similarity rather than association or relatedness, so that pairs ofentities that are associated but not actually similar [Freud, psychology] havea low rating. We show that, via this focus on similarity, SimLex-999incentivizes the development of models with a different, and arguably widerrange of applications than those which reflect conceptual association. Second,SimLex-999 contains a range of concrete and abstract adjective, noun and verbpairs, together with an independent rating of concreteness and (free)association strength for each pair. This diversity enables fine-grainedanalyses of the performance of models on concepts of different types, andconsequently greater insight into how architectures can be improved. Further,unlike existing gold standard evaluations, for which automatic approaches havereached or surpassed the inter-annotator agreement ceiling, state-of-the-artmodels perform well below this ceiling on SimLex-999. There is therefore plentyof scope for SimLex-999 to quantify future improvements to distributionalsemantic models, guiding the development of the next generation ofrepresentation-learning architectures.
arxiv-7200-248 | A convex pseudo-likelihood framework for high dimensional partial correlation estimation with convergence guarantees | http://arxiv.org/pdf/1307.5381v3.pdf | author:Kshitij Khare, Sang-Yun Oh, Bala Rajaratnam category:stat.ME stat.CO stat.ML published:2013-07-20 summary:Sparse high dimensional graphical model selection is a topic of much interestin modern day statistics. A popular approach is to apply l1-penalties to either(1) parametric likelihoods, or, (2) regularized regression/pseudo-likelihoods,with the latter having the distinct advantage that they do not explicitlyassume Gaussianity. As none of the popular methods proposed for solvingpseudo-likelihood based objective functions have provable convergenceguarantees, it is not clear if corresponding estimators exist or are evencomputable, or if they actually yield correct partial correlation graphs. Thispaper proposes a new pseudo-likelihood based graphical model selection methodthat aims to overcome some of the shortcomings of current methods, but at thesame time retain all their respective strengths. In particular, we introduce anovel framework that leads to a convex formulation of the partial covarianceregression graph problem, resulting in an objective function comprised ofquadratic forms. The objective is then optimized via a coordinate-wiseapproach. The specific functional form of the objective function facilitatesrigorous convergence analysis leading to convergence guarantees; an importantproperty that cannot be established using standard results, when the dimensionis larger than the sample size, as is often the case in high dimensionalapplications. These convergence guarantees ensure that estimators arewell-defined under very general conditions, and are always computable. Inaddition, the approach yields estimators that have good large sample propertiesand also respect symmetry. Furthermore, application to simulated/real data,timing comparisons and numerical convergence is demonstrated. We also present anovel unifying framework that places all graphical pseudo-likelihood methods asspecial cases of a more general formulation, leading to important insights.
arxiv-7200-249 | An Adaptive Statistical Non-uniform Quantizer for Detail Wavelet Components in Lossy JPEG2000 Image Compression | http://arxiv.org/pdf/1305.1986v3.pdf | author:Madhur Srivastava, Satish K. Singh, Prasanta K. Panigrahi category:cs.MM cs.CV published:2013-05-09 summary:The paper presents a non-uniform quantization method for the Detailcomponents in the JPEG2000 standard. Incorporating the fact that thecoefficients lying towards the ends of the histogram plot of each Detailcomponent represent the structural information of an image, the quantizationstep sizes become smaller at they approach the ends of the histogram plot. Thevariable quantization step sizes are determined by the actual statistics of thewavelet coefficients. Mean and standard deviation are the two statisticalparameters used iteratively to obtain the variable step sizes. Moreover, themean of the coefficients lying within the step size is chosen as the quantizedvalue, contrary to the deadzone uniform quantizer which selects the midpoint ofthe quantization step size as the quantized value. The experimental results ofthe deadzone uniform quantizer and the proposed non-uniform quantizer areobjectively compared by using Mean-Squared Error (MSE) and Mean StructuralSimilarity Index Measure (MSSIM), to evaluate the quantization error andreconstructed image quality, respectively. Subjective analysis of thereconstructed images is also carried out. Through the objective and subjectiveassessments, it is shown that the non-uniform quantizer performs better thanthe deadzone uniform quantizer in the perceptual quality of the reconstructedimage, especially at low bitrates. More importantly, unlike the deadzoneuniform quantizer, the non-uniform quantizer accomplishes better visual qualitywith a few quantized values.
arxiv-7200-250 | Likely to stop? Predicting Stopout in Massive Open Online Courses | http://arxiv.org/pdf/1408.3382v1.pdf | author:Colin Taylor, Kalyan Veeramachaneni, Una-May O'Reilly category:cs.CY cs.LG published:2014-08-14 summary:Understanding why students stopout will help in understanding how studentslearn in MOOCs. In this report, part of a 3 unit compendium, we describe how webuild accurate predictive models of MOOC student stopout. We document ascalable, stopout prediction methodology, end to end, from raw source data tomodel analysis. We attempted to predict stopout for the Fall 2012 offering of6.002x. This involved the meticulous and crowd-sourced engineering of over 25predictive features extracted for thousands of students, the creation oftemporal and non-temporal data representations for use in predictive modeling,the derivation of over 10 thousand models with a variety of state-of-the-artmachine learning techniques and the analysis of feature importance by examiningover 70000 models. We found that stop out prediction is a tractable problem.Our models achieved an AUC (receiver operating characteristicarea-under-the-curve) as high as 0.95 (and generally 0.88) when predicting oneweek in advance. Even with more difficult prediction problems, such aspredicting stop out at the end of the course with only one weeks' data, themodels attained AUCs of 0.7.
arxiv-7200-251 | 2D View Aggregation for Lymph Node Detection Using a Shallow Hierarchy of Linear Classifiers | http://arxiv.org/pdf/1408.3337v1.pdf | author:Ari Seff, Le Lu, Kevin M. Cherry, Holger Roth, Jiamin Liu, Shijun Wang, Joanne Hoffman, Evrim B. Turkbey, Ronald M. Summers category:cs.CV cs.LG published:2014-08-14 summary:Enlarged lymph nodes (LNs) can provide important information for cancerdiagnosis, staging, and measuring treatment reactions, making automateddetection a highly sought goal. In this paper, we propose a new algorithmrepresentation of decomposing the LN detection problem into a set of 2D objectdetection subtasks on sampled CT slices, largely alleviating the curse ofdimensionality issue. Our 2D detection can be effectively formulated as linearclassification on a single image feature type of Histogram of OrientedGradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. Weexploit both simple pooling and sparse linear fusion schemes to aggregate these2D detection scores for the final 3D LN detection. In this manner, detection ismore tractable and does not need to perform perfectly at instance level (asweak hypotheses) since our aggregation process will robustly harness collectiveinformation for LN detection. Two datasets (90 patients with 389 mediastinalLNs and 86 patients with 595 abdominal LNs) are used for validation.Cross-validation demonstrates 78.0% sensitivity at 6 false positives/volume(FP/vol.) (86.1% at 10 FP/vol.) and 73.1% sensitivity at 6 FP/vol. (87.2% at 10FP/vol.), for the mediastinal and abdominal datasets respectively. Our resultscompare favorably to previous state-of-the-art methods.
arxiv-7200-252 | Exact and empirical estimation of misclassification probability | http://arxiv.org/pdf/1408.3332v1.pdf | author:Victor Nedelko category:stat.ML cs.LG published:2014-08-14 summary:We discuss the problem of risk estimation in the classification problem, withspecific focus on finding distributions that maximize the confidence intervalsof risk estimation. We derived simple analytic approximations for the maximumbias of empirical risk for histogram classifier. We carry out a detailed studyon using these analytic estimates for empirical estimation of risk.
arxiv-7200-253 | Analysis of Gait Pattern to Recognize the Human Activities | http://arxiv.org/pdf/1407.4867v2.pdf | author:Jay Prakash Gupta, Pushkar Dixit, Nishant Singh, Vijay Bhaskar Semwal category:cs.CV published:2014-07-18 summary:Human activity recognition based on the computer vision is the process oflabelling image sequences with action labels. Accurate systems for this problemare applied in areas such as visual surveillance, human computer interactionand video retrieval.
arxiv-7200-254 | Recognition of Handwritten Persian/Arabic Numerals Based on Robust Feature Set and K-NN Classifier | http://arxiv.org/pdf/1407.6492v2.pdf | author:Reza Azad, Fatemeh Davami, Hamid Reza Shayegh category:cs.CV published:2014-07-24 summary:This paper has been withdrawn by the author due to a crucial sign error inequation 2 and some mistake in Table 1 information. please let me for changingthis information and updating this paper.
arxiv-7200-255 | Addendum on the scoring of Gaussian directed acyclic graphical models | http://arxiv.org/pdf/1402.6863v3.pdf | author:Jack Kuipers, Giusi Moffa, David Heckerman category:stat.ML published:2014-02-27 summary:We provide a correction to the expression for scoring Gaussian directedacyclic graphical models derived in Geiger and Heckerman [Ann. Statist. 30(2002) 1414-1440] and discuss how to evaluate the score efficiently.
arxiv-7200-256 | Toward Automated Discovery of Artistic Influence | http://arxiv.org/pdf/1408.3218v1.pdf | author:Babak Saleh, Kanako Abe, Ravneet Singh Arora, Ahmed Elgammal category:cs.CV cs.LG published:2014-08-14 summary:Considering the huge amount of art pieces that exist, there is valuableinformation to be discovered. Examining a painting, an expert can determine itsstyle, genre, and the time period that the painting belongs. One important taskfor art historians is to find influences and connections between artists. Isinfluence a task that a computer can measure? The contribution of this paper isin exploring the problem of computer-automated suggestion of influences betweenartists, a problem that was not addressed before in a general setting. We firstpresent a comparative study of different classification methodologies for thetask of fine-art style classification. A two-level comparative study isperformed for this classification problem. The first level reviews theperformance of discriminative vs. generative models, while the second leveltouches the features aspect of the paintings and compares semantic-levelfeatures vs. low-level and intermediate-level features present in the painting.Then, we investigate the question "Who influenced this artist?" by looking athis masterpieces and comparing them to others. We pose this interestingquestion as a knowledge discovery problem. For this purpose, we investigatedseveral painting-similarity and artist-similarity measures. As a result, weprovide a visualization of artists (Map of Artists) based on the similaritybetween their works
arxiv-7200-257 | Cortical Processing with Thermodynamic-RAM | http://arxiv.org/pdf/1408.3215v1.pdf | author:M. Alexander Nugent, Timothy W. Molter category:cs.NE cs.ET published:2014-08-14 summary:AHaH computing forms a theoretical framework from which abiologically-inspired type of computing architecture can be built where, unlikevon Neumann systems, memory and processor are physically combined. In thispaper we report on an incremental step beyond the theoretical framework of AHaHcomputing toward the development of a memristor-based physical neuralprocessing unit (NPU), which we call Thermodynamic-RAM (kT-RAM). While thepower consumption and speed dominance of such an NPU over von Neumannarchitectures for machine learning applications is well appreciated,Thermodynamic-RAM offers several advantages over other hardware approaches toadaptation and learning. Benefits include general-purpose use, a simple yetflexible instruction set and easy integration into existing digital platforms.We present a high level design of kT-RAM and a formal definition of itsinstruction set. We report the completion of a kT-RAM emulator and thesuccessful port of all previous machine learning benchmark applicationsincluding unsupervised clustering, supervised and unsupervised classification,complex signal prediction, unsupervised robotic actuation and combinatorialoptimization. Lastly, we extend a previous MNIST hand written digits benchmarkapplication, to show that an extra step of reading the synaptic states of AHaHnodes during the train phase (healing) alone results in plasticity thatimproves the classifier's performance, bumping our best F1 score up to 99.5%.
arxiv-7200-258 | A Parallel Algorithm for Exact Bayesian Structure Discovery in Bayesian Networks | http://arxiv.org/pdf/1408.1664v2.pdf | author:Yetian Chen, Jin Tian, Olga Nikolova, Srinivas Aluru category:cs.AI cs.DC cs.LG published:2014-08-07 summary:Exact Bayesian structure discovery in Bayesian networks requires exponentialtime and space. Using dynamic programming (DP), the fastest known serialalgorithm computes the exact posterior probabilities of structural features in$O(n2^n)$ time and space, if the number of parents per node or indegree isbounded by a constant $d$. Here we present a parallel algorithm capable ofcomputing the exact posterior probabilities for all $n(n-1)$ edges with optimalparallel time and space efficiency. That is, if $p=2^k$ processors are used,the run-time and space usage reduce to $O(n2^{n-k}+k(n-k)^d)$ and$O(n2^{n-k})$, respectively. Our algorithm is based the observation that theoriginal DP steps constitute a $n$-$D$ hypercube. In our algorithm, we take adelicate way to coordinate the computations of correlated DP procedures suchthat large amount of data exchange is suppressed. Further, we develop paralleltechniques for two variants of the well-known zeta transform, which haveapplications outside the context of Bayesian networks. We demonstrate thecapability of our algorithm on datasets with up to 33 variables and itsscalability on up to 2048 processors.
arxiv-7200-259 | Indefinitely Oscillating Martingales | http://arxiv.org/pdf/1408.3169v1.pdf | author:Jan Leike, Marcus Hutter category:cs.LG math.PR math.ST stat.TH published:2014-08-14 summary:We construct a class of nonnegative martingale processes that oscillateindefinitely with high probability. For these processes, we state a uniformrate of the number of oscillations and show that this rate is asymptoticallyclose to the theoretical upper bound. These bounds on probability andexpectation of the number of upcrossings are compared to classical bounds fromthe martingale literature. We discuss two applications. First, our resultsimply that the limit of the minimum description length operator may not exist.Second, we give bounds on how often one can change one's belief in a givenhypothesis when observing a stream of data.
arxiv-7200-260 | Convergence rate of Bayesian tensor estimator: Optimal rate without restricted strong convexity | http://arxiv.org/pdf/1408.3092v1.pdf | author:Taiji Suzuki category:stat.ML cs.LG published:2014-08-13 summary:In this paper, we investigate the statistical convergence rate of a Bayesianlow-rank tensor estimator. Our problem setting is the regression problem wherea tensor structure underlying the data is estimated. This problem settingoccurs in many practical applications, such as collaborative filtering,multi-task learning, and spatio-temporal data analysis. The convergence rate isanalyzed in terms of both in-sample and out-of-sample predictive accuracies. Itis shown that a near optimal rate is achieved without any strong convexity ofthe observation. Moreover, we show that the method has adaptivity to theunknown rank of the true tensor, that is, the near optimal rate depending onthe true rank is achieved even if it is not known a priori.
arxiv-7200-261 | Fastfood: Approximate Kernel Expansions in Loglinear Time | http://arxiv.org/pdf/1408.3060v1.pdf | author:Quoc Viet Le, Tamas Sarlos, Alexander Johannes Smola category:cs.LG stat.ML published:2014-08-13 summary:Despite their successes, what makes kernel methods difficult to use in manylarge scale problems is the fact that storing and computing the decisionfunction is typically expensive, especially at prediction time. In this paper,we overcome this difficulty by proposing Fastfood, an approximation thataccelerates such computation significantly. Key to Fastfood is the observationthat Hadamard matrices, when combined with diagonal Gaussian matrices, exhibitproperties similar to dense Gaussian random matrices. Yet unlike the latter,Hadamard and diagonal matrices are inexpensive to multiply and store. These twomatrices can be used in lieu of Gaussian matrices in Random Kitchen Sinksproposed by Rahimi and Recht (2009) and thereby speeding up the computation fora large range of kernel functions. Specifically, Fastfood requires O(n log d)time and O(n) storage to compute n non-linear basis functions in d dimensions,a significant improvement from O(nd) computation and storage, withoutsacrificing accuracy. Our method applies to any translation invariant and any dot-product kernel,such as the popular RBF kernels and polynomial kernels. We prove that theapproximation is unbiased and has low variance. Experiments show that weachieve similar accuracy to full kernel expansions and Random Kitchen Sinkswhile being 100x faster and using 1000x less memory. These improvements,especially in terms of memory usage, make kernel methods more practical forapplications that have large training sets and/or require real-time prediction.
arxiv-7200-262 | Learning a hyperplane classifier by minimizing an exact bound on the VC dimension | http://arxiv.org/pdf/1408.2803v2.pdf | author:Jayadeva category:cs.LG I.5.1; I.5.2 published:2014-08-12 summary:The VC dimension measures the capacity of a learning machine, and a low VCdimension leads to good generalization. While SVMs produce state-of-the-artlearning performance, it is well known that the VC dimension of a SVM can beunbounded; despite good results in practice, there is no guarantee of goodgeneralization. In this paper, we show how to learn a hyperplane classifier byminimizing an exact, or \boldmath{$\Theta$} bound on its VC dimension. Theproposed approach, termed as the Minimal Complexity Machine (MCM), involvessolving a simple linear programming problem. Experimental results show, that ona number of benchmark datasets, the proposed approach learns classifiers witherror rates much less than conventional SVMs, while often using fewer supportvectors. On many benchmark datasets, the number of support vectors is less thanone-tenth the number used by SVMs, indicating that the MCM does indeed learnsimpler representations.
arxiv-7200-263 | Marginal Likelihoods for Distributed Parameter Estimation of Gaussian Graphical Models | http://arxiv.org/pdf/1303.4756v6.pdf | author:Zhaoshi Meng, Dennis Wei, Ami Wiesel, Alfred O. Hero III category:stat.ML cs.LG published:2013-03-19 summary:We consider distributed estimation of the inverse covariance matrix, alsocalled the concentration or precision matrix, in Gaussian graphical models.Traditional centralized estimation often requires global inference of thecovariance matrix, which can be computationally intensive in large dimensions.Approximate inference based on message-passing algorithms, on the other hand,can lead to unstable and biased estimation in loopy graphical models. In thispaper, we propose a general framework for distributed estimation based on amaximum marginal likelihood (MML) approach. This approach computes localparameter estimates by maximizing marginal likelihoods defined with respect todata collected from local neighborhoods. Due to the non-convexity of the MMLproblem, we introduce and solve a convex relaxation. The local estimates arethen combined into a global estimate without the need for iterativemessage-passing between neighborhoods. The proposed algorithm is naturallyparallelizable and computationally efficient, thereby making it suitable forhigh-dimensional problems. In the classical regime where the number ofvariables $p$ is fixed and the number of samples $T$ increases to infinity, theproposed estimator is shown to be asymptotically consistent and to improvemonotonically as the local neighborhood size increases. In the high-dimensionalscaling regime where both $p$ and $T$ increase to infinity, the convergencerate to the true parameters is derived and is seen to be comparable tocentralized maximum likelihood estimation. Extensive numerical experimentsdemonstrate the improved performance of the two-hop version of the proposedestimator, which suffices to almost close the gap to the centralized maximumlikelihood estimator at a reduced computational cost.
arxiv-7200-264 | An Improved Approach for Contrast Enhancement of Spinal Cord Images based on Multiscale Retinex Algorithm | http://arxiv.org/pdf/1408.2997v1.pdf | author:Sreenivasa Setty, N. K Srinath, M. C Hanumantharaju category:cs.CV published:2014-08-13 summary:This paper presents a new approach for contrast enhancement of spinal cordmedical images based on multirate scheme incorporated into multiscale retinexalgorithm. The proposed work here uses HSV color space, since HSV color spaceseparates color details from intensity. The enhancement of medical image isachieved by down sampling the original image into five versions, namely, tiny,small, medium, fine, and normal scale. This is due to the fact that the eachversions of the image when independently enhanced and reconstructed results inenormous improvement in the visual quality. Further, the contrast stretchingand MultiScale Retinex (MSR) techniques are exploited in order to enhance eachof the scaled version of the image. Finally, the enhanced image is obtained bycombining each of these scales in an efficient way to obtain the compositeenhanced image. The efficiency of the proposed algorithm is validated by usinga wavelet energy metric in the wavelet domain. Reconstructed image usingproposed method highlights the details (edges and tissues), reduces image noise(Gaussian and Speckle) and improves the overall contrast. The proposedalgorithm also enhances sharp edges of the tissue surrounding the spinal cordregions which is useful for diagnosis of spinal cord lesions. Elaboratedexperiments are conducted on several medical images and results presented showthat the enhanced medical pictures are of good quality and is found to bebetter compared with other researcher methods.
arxiv-7200-265 | Learning Multi-Scale Representations for Material Classification | http://arxiv.org/pdf/1408.2938v1.pdf | author:Wenbin Li, Mario Fritz category:cs.CV cs.LG cs.NE published:2014-08-13 summary:The recent progress in sparse coding and deep learning has made unsupervisedfeature learning methods a strong competitor to hand-crafted descriptors. Incomputer vision, success stories of learned features have been predominantlyreported for object recognition tasks. In this paper, we investigate if and howfeature learning can be used for material recognition. We propose twostrategies to incorporate scale information into the learning procedureresulting in a novel multi-scale coding procedure. Our results show that ourlearned features for material recognition outperform hand-crafted descriptorson the FMD and the KTH-TIPS2 material classification benchmarks.
arxiv-7200-266 | Hashing for Similarity Search: A Survey | http://arxiv.org/pdf/1408.2927v1.pdf | author:Jingdong Wang, Heng Tao Shen, Jingkuan Song, Jianqiu Ji category:cs.DS cs.CV cs.DB published:2014-08-13 summary:Similarity search (nearest neighbor search) is a problem of pursuing the dataitems whose distances to a query item are the smallest from a large database.Various methods have been developed to address this problem, and recently a lotof efforts have been devoted to approximate search. In this paper, we present asurvey on one of the main solutions, hashing, which has been widely studiedsince the pioneering work locality sensitive hashing. We divide the hashingalgorithms two main categories: locality sensitive hashing, which designs hashfunctions without exploring the data distribution and learning to hash, whichlearns hash functions according the data distribution, and review them fromvarious aspects, including hash function design and distance measure and searchscheme in the hash coding space.
arxiv-7200-267 | Linear Contour Learning: A Method for Supervised Dimension Reduction | http://arxiv.org/pdf/1408.3359v1.pdf | author:Bing Li, Hongyuan Zha, Francesca Chiaromonte category:cs.LG published:2014-08-13 summary:We propose a novel approach to sufficient dimension reduction in regression,based on estimating contour directions of negligible variation for the responsesurface. These directions span the orthogonal complement of the minimal spacerelevant for the regression, and can be extracted according to a measure of thevariation in the response, leading to General Contour Regression(GCR). Incomparison to exiisting sufficient dimension reduction techniques, thissontour-based mothology guarantees exhaustive estimation of the central spaceunder ellipticity of the predictoor distribution and very mild additionalassumptions, while maintaining vn-consisytency and somputational ease.Moreover, it proves to be robust to departures from ellipticity. We alsoestablish some useful population properties for GCR. Simulations to compareperformance with that of standard techniques such as ordinary least squares,sliced inverse regression, principal hessian directions, and sliced averagevariance estimation confirm the advntages anticipated by theoretical analyses.We also demonstrate the use of contour-based methods on a data set concerninggrades of students from Massachusetts colleges.
arxiv-7200-268 | Robust OS-ELM with a novel selective ensemble based on particle swarm optimization | http://arxiv.org/pdf/1408.2890v1.pdf | author:Yang Liu, Bo He, Diya Dong, Yue Shen, Tianhong Yan, Rui Nian, Amaury Lendase category:cs.LG published:2014-08-13 summary:In this paper, a robust online sequential extreme learning machine (ROS-ELM)is proposed. It is based on the original OS-ELM with an adaptive selectiveensemble framework. Two novel insights are proposed in this paper. First, anovel selective ensemble algorithm referred to as particle swarm optimizationselective ensemble (PSOSEN) is proposed. Noting that PSOSEN is a generalselective ensemble method which is applicable to any learning algorithms,including batch learning and online learning. Second, an adaptive selectiveensemble framework for online learning is designed to balance the robustnessand complexity of the algorithm. Experiments for both regression andclassification problems with UCI data sets are carried out. Comparisons betweenOS-ELM, simple ensemble OS-ELM (EOS-ELM) and the proposed ROS-ELM empiricallyshow that ROS-ELM significantly improves the robustness and stability.
arxiv-7200-269 | A Classifier-free Ensemble Selection Method based on Data Diversity in Random Subspaces | http://arxiv.org/pdf/1408.2889v1.pdf | author:Albert H. R. Ko, Robert Sabourin, Alceu S. Britto Jr, Luiz E. S. Oliveira category:cs.LG cs.NE I.5.2; I.5.3 published:2014-08-13 summary:The Ensemble of Classifiers (EoC) has been shown to be effective in improvingthe performance of single classifiers by combining their outputs, and one ofthe most important properties involved in the selection of the best EoC from apool of classifiers is considered to be classifier diversity. In general,classifier diversity does not occur randomly, but is generated systematicallyby various ensemble creation methods. By using diverse data subsets to trainclassifiers, these methods can create diverse classifiers for the EoC. In thiswork, we propose a scheme to measure data diversity directly from randomsubspaces, and explore the possibility of using it to select the best datasubsets for the construction of the EoC. Our scheme is the first ensembleselection method to be presented in the literature based on the concept of datadiversity. Its main advantage over the traditional framework (ensemble creationthen selection) is that it obviates the need for classifier training prior toensemble selection. A single Genetic Algorithm (GA) and a Multi-ObjectiveGenetic Algorithm (MOGA) were evaluated to search for the best solutions forthe classifier-free ensemble selection. In both cases, objective functionsbased on different clustering diversity measures were implemented and tested.All the results obtained with the proposed classifier-free ensemble selectionmethod were compared with the traditional classifier-based ensemble selectionusing Mean Classifier Error (ME) and Majority Voting Error (MVE). Theapplicability of the method is tested on UCI machine learning problems and NISTSD19 handwritten numerals.
arxiv-7200-270 | Cluster based RBF Kernel for Support Vector Machines | http://arxiv.org/pdf/1408.2869v1.pdf | author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG stat.ML published:2014-08-12 summary:In the classical Gaussian SVM classification we use the feature spaceprojection transforming points to normal distributions with fixed covariancematrices (identity in the standard RBF and the covariance of the whole datasetin Mahalanobis RBF). In this paper we add additional information to GaussianSVM by considering local geometry-dependent feature space projection. Weemphasize that our approach is in fact an algorithm for a construction of thenew Gaussian-type kernel. We show that better (compared to standard RBF and Mahalanobis RBF)classification results are obtained in the simple case when the space ispreliminary divided by k-means into two sets and points are represented asnormal distributions with a covariances calculated according to the datasetpartitioning. We call the constructed method C$_k$RBF, where $k$ stands for the amount ofclusters used in k-means. We show empirically on nine datasets from UCIrepository that C$_2$RBF increases the stability of the grid search (measuredas the probability of finding good parameters).
arxiv-7200-271 | Spectral Unmixing of Hyperspectral Imagery using Multilayer NMF | http://arxiv.org/pdf/1408.2810v1.pdf | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2014-08-12 summary:Hyperspectral images contain mixed pixels due to low spatial resolution ofhyperspectral sensors. Spectral unmixing problem refers to decomposing mixedpixels into a set of endmembers and abundance fractions. Due to nonnegativityconstraint on abundance fractions, nonnegative matrix factorization (NMF)methods have been widely used for solving spectral unmixing problem. In thisletter we proposed using multilayer NMF (MLNMF) for the purpose ofhyperspectral unmixing. In this approach, spectral signature matrix can bemodeled as a product of sparse matrices. In fact MLNMF decomposes theobservation matrix iteratively in a number of layers. In each layer, we appliedsparseness constraint on spectral signature matrix as well as on abundancefractions matrix. In this way signatures matrix can be sparsely decomposeddespite the fact that it is not generally a sparse matrix. The proposedalgorithm is applied on synthetic and real datasets. Synthetic data isgenerated based on endmembers from USGS spectral library. AVIRIS Cupritedataset has been used as a real dataset for evaluation of proposed method.Results of experiments are quantified based on SAD and AAD measures. Results incomparison with previously proposed methods show that the multilayer approachcan unmix data more effectively.
arxiv-7200-272 | Online Learning via Sequential Complexities | http://arxiv.org/pdf/1006.1138v3.pdf | author:Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari category:cs.LG stat.ML published:2010-06-06 summary:We consider the problem of sequential prediction and provide tools to studythe minimax value of the associated game. Classical statistical learning theoryprovides several useful complexity measures to study learning with i.i.d. data.Our proposed sequential complexities can be seen as extensions of thesemeasures to the sequential setting. The developed theory is shown to yieldprecise learning guarantees for the problem of sequential prediction. Inparticular, we show necessary and sufficient conditions for online learnabilityin the setting of supervised learning. Several examples show the utility of ourframework: we can establish learnability without having to exhibit an explicitonline learning algorithm.
arxiv-7200-273 | Generalization and Robustness of Batched Weighted Average Algorithm with V-geometrically Ergodic Markov Data | http://arxiv.org/pdf/1406.3166v2.pdf | author:Nguyen Viet Cuong, Lam Si Tung Ho, Vu Dinh category:stat.ML published:2014-06-12 summary:We analyze the generalization and robustness of the batched weighted averagealgorithm for V-geometrically ergodic Markov data. This algorithm is a goodalternative to the empirical risk minimization algorithm when the lattersuffers from overfitting or when optimizing the empirical risk is hard. For thegeneralization of the algorithm, we prove a PAC-style bound on the trainingsample size for the expected $L_1$-loss to converge to the optimal loss whentraining data are V-geometrically ergodic Markov chains. For the robustness, weshow that if the training target variable's values contain bounded noise, thenthe generalization bound of the algorithm deviates at most by the range of thenoise. Our results can be applied to the regression problem, the classificationproblem, and the case where there exists an unknown deterministic targethypothesis.
arxiv-7200-274 | Internal and external dynamics in language: Evidence from verb regularity in a historical corpus of English | http://arxiv.org/pdf/1408.2699v1.pdf | author:Christine F. Cuskley, Martina Pugliese, Claudio Castellano, Francesca Colaiori, Vittorio Loreto, Francesca Tria category:physics.soc-ph cs.CL published:2014-08-12 summary:Human languages are rule governed, but almost invariably these rules haveexceptions in the form of irregularities. Since rules in language are efficientand productive, the persistence of irregularity is an anomaly. How doesirregularity linger in the face of internal (endogenous) and external(exogenous) pressures to conform to a rule? Here we address this problem bytaking a detailed look at simple past tense verbs in the Corpus of HistoricalAmerican English. The data show that the language is open, with many new verbsentering. At the same time, existing verbs might tend to regularize orirregularize as a consequence of internal dynamics, but overall, the amount ofirregularity sustained by the language stays roughly constant over time.Despite continuous vocabulary growth, and presumably, an attendant increase inexpressive power, there is no corresponding growth in irregularity. We analyzethe set of irregulars, showing they may adhere to a set of minority rules,allowing for increased stability of irregularity over time. These findingscontribute to the debate on how language systems become rule governed, and howand why they sustain exceptions to rules, providing insight into the interplaybetween the emergence and maintenance of rules and exceptions in language.
arxiv-7200-275 | Multiple Instance Learning with Bag Dissimilarities | http://arxiv.org/pdf/1309.5643v3.pdf | author:Veronika Cheplygina, David M. J. Tax, Marco Loog category:stat.ML cs.LG published:2013-09-22 summary:Multiple instance learning (MIL) is concerned with learning from sets (bags)of objects (instances), where the individual instance labels are ambiguous. Inthis setting, supervised learning cannot be applied directly. Often,specialized MIL methods learn by making additional assumptions about therelationship of the bag labels and instance labels. Such assumptions may fit aparticular dataset, but do not generalize to the whole range of MIL problems.Other MIL methods shift the focus of assumptions from the labels to the overall(dis)similarity of bags, and therefore learn from bags directly. We propose torepresent each bag by a vector of its dissimilarities to other bags in thetraining set, and treat these dissimilarities as a feature representation. Weshow several alternatives to define a dissimilarity between bags and discusswhich definitions are more suitable for particular MIL problems. Theexperimental results show that the proposed approach is computationallyinexpensive, yet very competitive with state-of-the-art algorithms on a widerange of MIL datasets.
arxiv-7200-276 | Mapping the stereotyped behaviour of freely-moving fruit flies | http://arxiv.org/pdf/1310.4249v2.pdf | author:Gordon J. Berman, Daniel M. Choi, William Bialek, Joshua W. Shaevitz category:q-bio.QM cs.CV physics.bio-ph stat.ML published:2013-10-16 summary:Most animals possess the ability to actuate a vast diversity of movements,ostensibly constrained only by morphology and physics. In practice, however, afrequent assumption in behavioral science is that most of an animal'sactivities can be described in terms of a small set of stereotyped motifs. Herewe introduce a method for mapping the behavioral space of organisms, relyingonly upon the underlying structure of postural movement data to organize andclassify behaviors. We find that six different drosophilid species each performa mix of non-stereotyped actions and over one hundred hierarchically-organized,stereotyped behaviors. Moreover, we use this approach to compare these species'behavioral spaces, systematically identifying subtle behavioral differencesbetween closely-related species.
arxiv-7200-277 | Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction of Tumors | http://arxiv.org/pdf/1408.2552v1.pdf | author:Amit G. Deshwar, Shankar Vembu, Quaid Morris category:q-bio.PE cs.LG stat.ML published:2014-08-11 summary:Statistical machine learning methods, especially nonparametric Bayesianmethods, have become increasingly popular to infer clonal population structureof tumors. Here we describe the treeCRP, an extension of the Chinese restaurantprocess (CRP), a popular construction used in nonparametric mixture models, toinfer the phylogeny and genotype of major subclonal lineages represented in thepopulation of cancer cells. We also propose new split-merge updates tailored tothe subclonal reconstruction problem that improve the mixing time of Markovchains. In comparisons with the tree-structured stick breaking prior used inPhyloSub, we demonstrate superior mixing and running time using the treeCRPwith our new split-merge procedures. We also show that given the same number ofsamples, TSSB and treeCRP have similar ability to recover the subclonalstructure of a tumor.
arxiv-7200-278 | Compressed Sensing with Very Sparse Gaussian Random Projections | http://arxiv.org/pdf/1408.2504v1.pdf | author:Ping Li, Cun-Hui Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT published:2014-08-11 summary:We study the use of very sparse random projections for compressed sensing(sparse signal recovery) when the signal entries can be either positive ornegative. In our setting, the entries of a Gaussian design matrix are randomlysparsified so that only a very small fraction of the entries are nonzero. Ourproposed decoding algorithm is simple and efficient in that the major cost isone linear scan of the coordinates. We have developed two estimators: (i) the{\em tie estimator}, and (ii) the {\em absolute minimum estimator}. Using onlythe tie estimator, we are able to recover a $K$-sparse signal of length $N$using $1.551 eK \log K/\delta$ measurements (where $\delta\leq 0.05$ is theconfidence). Using only the absolute minimum estimator, we can detect thesupport of the signal using $eK\log N/\delta$ measurements. For a particularcoordinate, the absolute minimum estimator requires fewer measurements (i.e.,with a constant $e$ instead of $1.551e$). Thus, the two estimators can becombined to form an even more practical decoding framework. Prior studies have shown that existing one-scan (or roughly one-scan)recovery algorithms using sparse matrices would require substantially more(e.g., one order of magnitude) measurements than L1 decoding by linearprogramming, when the nonzero entries of signals can be either negative orpositive. In this paper, following a known experimental setup, we show that, atthe same number of measurements, the recovery accuracies of our proposed methodare (at least) similar to the standard L1 decoding.
arxiv-7200-279 | Robust computation of linear models by convex relaxation | http://arxiv.org/pdf/1202.4044v2.pdf | author:Gilad Lerman, Michael McCoy, Joel A. Tropp, Teng Zhang category:cs.IT math.IT stat.CO stat.ML published:2012-02-18 summary:Consider a dataset of vector-valued observations that consists of noisyinliers, which are explained well by a low-dimensional subspace, along withsome number of outliers. This work describes a convex optimization problem,called REAPER, that can reliably fit a low-dimensional model to this type ofdata. This approach parameterizes linear subspaces using orthogonal projectors,and it uses a relaxation of the set of orthogonal projectors to reach theconvex formulation. The paper provides an efficient algorithm for solving theREAPER problem, and it documents numerical experiments which confirm thatREAPER can dependably find linear structure in synthetic and natural data. Inaddition, when the inliers lie near a low-dimensional subspace, there is arigorous theory that describes when REAPER can approximate this subspace.
arxiv-7200-280 | Learning to see like children: proof of concept | http://arxiv.org/pdf/1408.2478v1.pdf | author:Marco Gori, Marco Lippi, Marco Maggini, Stefano Melacci category:cs.CV published:2014-08-11 summary:In the last few years we have seen a growing interest in machine learningapproaches to computer vision and, especially, to semantic labeling. Nowadaysstate of the art systems use deep learning on millions of labeled images withvery successful results on benchmarks, though it is unlikely to expect similarresults in unrestricted visual environments. Most learning schemes essentiallyignore the inherent sequential structure of videos: this might be a criticalissue, since any visual recognition process is remarkably more complex whenshuffling video frames. Based on this remark, we propose a re-foundation of thecommunication protocol between visual agents and the environment, which isreferred to as learning to see like children. Like for human interaction,visual concepts are acquired by the agents solely by processing their ownvisual stream along with human supervisions on selected pixels. We give a proofof concept that remarkable semantic labeling can emerge within this protocol byusing only a few supervised examples. This is made possible by exploiting aconstraint of motion coherent labeling that virtually offers tons ofsupervisions. Additional visual constraints, including those associated withobject supervisions, are used within the context of learning from constraints.The framework is extended in the direction of lifelong learning, so as ourvisual agents live in their own visual environment without distinguishinglearning and test set. Learning takes place in deep architectures under aprogressive developmental scheme. In order to evaluate our Developmental VisualAgents (DVAs), in addition to classic benchmarks, we open the doors of our lab,allowing people to evaluate DVAs by crowd-sourcing. Such assessment mechanismmight result in a paradigm shift in methodologies and algorithms for computervision, encouraging truly novel solutions within the proposed framework.
arxiv-7200-281 | Optimizing Component Combination in a Multi-Indexing Paragraph Retrieval System | http://arxiv.org/pdf/1408.2430v1.pdf | author:Boris Iolis, Gianluca Bontempi category:cs.IR cs.CL 68T50 published:2014-08-11 summary:We demonstrate a method to optimize the combination of distinct components ina paragraph retrieval system. Our system makes use of several indices, querygenerators and filters, each of them potentially contributing to the quality ofthe returned list of results. The components are combined with a weighed sum,and we optimize the weights using a heuristic optimization algorithm. Thisallows us to maximize the quality of our results, but also to determine whichcomponents are most valuable in our system. We evaluate our approach on theparagraph selection task of a Question Answering dataset.
arxiv-7200-282 | Video Face Editing Using Temporal-Spatial-Smooth Warping | http://arxiv.org/pdf/1408.2380v1.pdf | author:Xiaoyan Li, Dacheng Tao category:cs.CV cs.AI cs.MM published:2014-08-11 summary:Editing faces in videos is a popular yet challenging aspect of computervision and graphics, which encompasses several applications including facialattractiveness enhancement, makeup transfer, face replacement, and expressionmanipulation. Simply applying image-based warping algorithms to video-basedface editing produces temporal incoherence in the synthesized videos because itis impossible to consistently localize facial features in two framesrepresenting two different faces in two different videos (or even twoconsecutive frames representing the same face in one video). Therefore, highperformance face editing usually requires significant manual manipulation. Inthis paper we propose a novel temporal-spatial-smooth warping (TSSW) algorithmto effectively exploit the temporal information in two consecutive frames, aswell as the spatial smoothness within each frame. TSSW precisely estimates twocontrol lattices in the horizontal and vertical directions respectively fromthe corresponding control lattices in the previous frame, by minimizing a novelenergy function that unifies a data-driven term, a smoothness term, and featurepoint constraints. Corresponding warping surfaces then precisely map sourceframes to the target frames. Experimental testing on facial attractivenessenhancement, makeup transfer, face replacement, and expression manipulationdemonstrates that the proposed approaches can effectively preserve spatialsmoothness and temporal coherence in editing facial geometry, skin detail,identity, and expression, which outperform the existing face editing methods.In particular, TSSW is robust to subtly inaccurate localization of featurepoints and is a vast improvement over image-based warping methods.
arxiv-7200-283 | On the Complexity of Bandit Linear Optimization | http://arxiv.org/pdf/1408.2368v1.pdf | author:Ohad Shamir category:cs.LG published:2014-08-11 summary:We study the attainable regret for online linear optimization problems withbandit feedback, where unlike the full-information setting, the player can onlyobserve its own loss rather than the full loss vector. We show that the priceof bandit information in this setting can be as large as $d$, disproving thewell-known conjecture that the regret for bandit linear optimization is at most$\sqrt{d}$ times the full-information regret. Surprisingly, this is shown using"trivial" modifications of standard domains, which have no effect in thefull-information setting. This and other results we present highlight someinteresting differences between full-information and bandit learning, whichwere not considered in previous literature.
arxiv-7200-284 | Physical Computing With No Clock to Implement the Gaussian Pyramid of SIFT Algorithm | http://arxiv.org/pdf/1408.2289v1.pdf | author:Yi Li, Qi Wei, Fei Qiao, Huazhong Yang category:cs.CV published:2014-08-11 summary:Physical computing is a technology utilizing the nature of electronic devicesand circuit topology to cope with computing tasks. In this paper, we propose anactive circuit network to implement multi-scale Gaussian filter, which is alsocalled Gaussian Pyramid in image preprocessing. Various kinds of methods havebeen tried to accelerate the key stage in image feature extracting algorithmthese years. Compared with existing technologies, GPU parallel computing andFPGA accelerating technology, physical computing has great advantage onprocessing speed as well as power consumption. We have verified that processingtime to implement the Gaussian pyramid of the SIFT algorithm stands onnanosecond level through the physical computing technology, while otherexisting methods all need at least hundreds of millisecond. With an estimate onthe stray capacitance of the circuit, the power consumption is around 670pJ tofilter a 256x256 image. To the best of our knowledge, this is the most fastprocessing technology to accelerate the SIFT algorithm, and it is also a ratherenergy-efficient method, thanks to the proposed physical computing technology.
arxiv-7200-285 | Genetic Programming for Smart Phone Personalisation | http://arxiv.org/pdf/1408.2288v1.pdf | author:Philip Valencia, Aiden Haak, Alban Cotillon, Raja Jurdak category:cs.NE cs.CY published:2014-08-11 summary:Personalisation in smart phones requires adaptability to dynamic contextbased on user mobility, application usage and sensor inputs. Currentpersonalisation approaches, which rely on static logic that is developed apriori, do not provide sufficient adaptability to dynamic and unexpectedcontext. This paper proposes genetic programming (GP), which can evolve programlogic in realtime, as an online learning method to deal with the highly dynamiccontext in smart phone personalisation. We introduce the concept ofcollaborative smart phone personalisation through the GP Island Model, in orderto exploit shared context among co-located phone users and reduce convergencetime. We implement these concepts on real smartphones to demonstrate thecapability of personalisation through GP and to explore the benefits of theIsland Model. Our empirical evaluations on two example applications confirmthat the Island Model can reduce convergence time by up to two-thirds overstandalone GP personalisation.
arxiv-7200-286 | Exponentiated Gradient Exploration for Active Learning | http://arxiv.org/pdf/1408.2196v1.pdf | author:Djallel Bouneffouf category:cs.LG cs.AI I.2 published:2014-08-10 summary:Active learning strategies respond to the costly labelling task in asupervised classification by selecting the most useful unlabelled examples intraining a predictive model. Many conventional active learning algorithms focuson refining the decision boundary, rather than exploring new regions that canbe more informative. In this setting, we propose a sequential algorithm namedEG-Active that can improve any Active learning algorithm by an optimal randomexploration. Experimental results show a statistically significant andappreciable improvement in the performance of our new approach over theexisting active feedback methods.
arxiv-7200-287 | R-UCB: a Contextual Bandit Algorithm for Risk-Aware Recommender Systems | http://arxiv.org/pdf/1408.2195v1.pdf | author:Djallel Bouneffouf category:cs.IR cs.LG I.2 published:2014-08-10 summary:Mobile Context-Aware Recommender Systems can be naturally modelled as anexploration/exploitation trade-off (exr/exp) problem, where the system has tochoose between maximizing its expected rewards dealing with its currentknowledge (exploitation) and learning more about the unknown user's preferencesto improve its knowledge (exploration). This problem has been addressed by thereinforcement learning community but they do not consider the risk level of thecurrent user's situation, where it may be dangerous to recommend items the usermay not desire in her current situation if the risk level is high. We introducein this paper an algorithm named R-UCB that considers the risk level of theuser's situation to adaptively balance between exr and exp. The detailedanalysis of the experimental results reveals several important discoveries inthe exr/exp behaviour.
arxiv-7200-288 | Statistical guarantees for the EM algorithm: From population to sample-based analysis | http://arxiv.org/pdf/1408.2156v1.pdf | author:Sivaraman Balakrishnan, Martin J. Wainwright, Bin Yu category:math.ST cs.LG stat.ML stat.TH published:2014-08-09 summary:We develop a general framework for proving rigorous guarantees on theperformance of the EM algorithm and a variant known as gradient EM. Ouranalysis is divided into two parts: a treatment of these algorithms at thepopulation level (in the limit of infinite data), followed by results thatapply to updates based on a finite set of samples. First, we characterize thedomain of attraction of any global maximizer of the population likelihood. Thischaracterization is based on a novel view of the EM updates as a perturbed formof likelihood ascent, or in parallel, of the gradient EM updates as a perturbedform of standard gradient ascent. Leveraging this characterization, we thenprovide non-asymptotic guarantees on the EM and gradient EM algorithms whenapplied to a finite set of samples. We develop consequences of our generaltheory for three canonical examples of incomplete-data problems: mixture ofGaussians, mixture of regressions, and linear regression with covariatesmissing completely at random. In each case, our theory guarantees that with asuitable initialization, a relatively small number of EM (or gradient EM) stepswill yield (with high probability) an estimate that is within statistical errorof the MLE. We provide simulations to confirm this theoretically predictedbehavior.
arxiv-7200-289 | Learning Graphical Models With Hubs | http://arxiv.org/pdf/1402.7349v2.pdf | author:Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel, Daniela Witten category:stat.ML stat.CO stat.ME published:2014-02-28 summary:We consider the problem of learning a high-dimensional graphical model inwhich certain hub nodes are highly-connected to many other nodes. Many authorshave studied the use of an l1 penalty in order to learn a sparse graph inhigh-dimensional setting. However, the l1 penalty implicitly assumes that eachedge is equally likely and independent of all other edges. We propose a generalframework to accommodate more realistic networks with hub nodes, using a convexformulation that involves a row-column overlap norm penalty. We apply thisgeneral framework to three widely-used probabilistic graphical models: theGaussian graphical model, the covariance graph model, and the binary Isingmodel. An alternating direction method of multipliers algorithm is used tosolve the corresponding convex optimization problems. On synthetic data, wedemonstrate that our proposed framework outperforms competitors that do notexplicitly model hub nodes. We illustrate our proposal on a webpage data setand a gene expression data set.
arxiv-7200-290 | Probabilistic inverse reinforcement learning in unknown environments | http://arxiv.org/pdf/1408.2067v1.pdf | author:Aristide Tossou, Christos Dimitrakakis category:cs.LG stat.ML published:2014-08-09 summary:We consider the problem of learning by demonstration from agents acting inunknown stochastic Markov environments or games. Our aim is to estimate agentpreferences in order to construct improved policies for the same task that theagents are trying to solve. To do so, we extend previous probabilisticapproaches for inverse reinforcement learning in known MDPs to the case ofunknown dynamics or opponents. We do this by deriving two simplifiedprobabilistic models of the demonstrator's policy and utility. Fortractability, we use maximum a posteriori estimation rather than full Bayesianinference. Under a flat prior, this results in a convex optimisation problem.We find that the resulting algorithms are highly competitive against a varietyof other methods for inverse reinforcement learning that do have knowledge ofthe dynamics.
arxiv-7200-291 | Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear Multivariate Regression and Granger Causality | http://arxiv.org/pdf/1408.2066v1.pdf | author:Vikas Sindhwani, Ha Quang Minh, Aurelie Lozano category:cs.LG stat.ML published:2014-08-09 summary:We propose a general matrix-valued multiple kernel learning framework forhigh-dimensional nonlinear multivariate regression problems. This frameworkallows a broad class of mixed norm regularizers, including those that inducesparsity, to be imposed on a dictionary of vector-valued Reproducing KernelHilbert Spaces. We develop a highly scalable and eigendecomposition-freealgorithm that orchestrates two inexact solvers for simultaneously learningboth the input and output components of separable matrix-valued kernels. As akey application enabled by our framework, we show how high-dimensional causalinference tasks can be naturally cast as sparse function estimation problems,leading to novel nonlinear extensions of a class of Graphical Granger Causalitytechniques. Our algorithmic developments and extensive empirical studies arecomplemented by theoretical analyses in terms of Rademacher generalizationbounds.
arxiv-7200-292 | Normalized Online Learning | http://arxiv.org/pdf/1408.2065v1.pdf | author:Stephane Ross, Paul Mineiro, John Langford category:cs.LG stat.ML published:2014-08-09 summary:We introduce online learning algorithms which are independent of featurescales, proving regret bounds dependent on the ratio of scales existent in thedata rather than the absolute scale. This has several useful effects: there isno need to pre-normalize data, the test-time and test-space complexity arereduced, and the algorithms are more robust.
arxiv-7200-293 | One-Class Support Measure Machines for Group Anomaly Detection | http://arxiv.org/pdf/1408.2064v1.pdf | author:Krikamol Muandet, Bernhard Schoelkopf category:cs.LG stat.ML published:2014-08-09 summary:We propose one-class support measure machines (OCSMMs) for group anomalydetection which aims at recognizing anomalous aggregate behaviors of datapoints. The OCSMMs generalize well-known one-class support vector machines(OCSVMs) to a space of probability measures. By formulating the problem asquantile estimation on distributions, we can establish an interestingconnection to the OCSVMs and variable kernel density estimators (VKDEs) overthe input space on which the distributions are defined, bridging the gapbetween large-margin methods and kernel density estimators. In particular, weshow that various types of VKDEs can be considered as solutions to a class ofregularization problems studied in this paper. Experiments on Sloan Digital SkySurvey dataset and High Energy Particle Physics dataset demonstrate thebenefits of the proposed framework in real-world applications.
arxiv-7200-294 | The Lovasz-Bregman Divergence and connections to rank aggregation, clustering, and web ranking | http://arxiv.org/pdf/1408.2062v1.pdf | author:Rishabh Iyer, Jeff A. Bilmes category:cs.LG stat.ML published:2014-08-09 summary:We extend the recently introduced theory of Lovasz-Bregman (LB) divergences(Iyer & Bilmes 2012) in several ways. We show that they represent a distortionbetween a "score" and an "ordering", thus providing a new view of rankaggregation and order based clustering with interesting connections to webranking. We show how the LB divergences have a number of properties akin tomany permutation based metrics, and in fact have as special cases forms verysimilar to the Kendall-tau metric. We also show how the LB divergences subsumea number of commonly used ranking measures in information retrieval, like NDCGand AUC. Unlike the traditional permutation based metrics, however, the LBdivergence naturally captures a notion of "confidence" in the orderings, thusproviding a new representation to applications involving aggregating scores asopposed to just orderings. We show how a number of recently used web rankingmodels are forms of Lovasz-Bregman rank aggregation and also observe that anatural form of Mallow's model using the LB divergence has been used asconditional ranking models for the "Learning to Rank" problem.
arxiv-7200-295 | Warped Mixtures for Nonparametric Cluster Shapes | http://arxiv.org/pdf/1408.2061v1.pdf | author:Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani category:cs.LG stat.ML published:2014-08-09 summary:A mixture of Gaussians fit to a single curved or heavy-tailed cluster willreport that the data contains many clusters. To produce more appropriateclusterings, we introduce a model which warps a latent mixture of Gaussians toproduce nonparametric cluster shapes. The possibly low-dimensional latentmixture model allows us to summarize the properties of the high-dimensionalclusters (or density manifolds) describing the data. The number of manifolds,as well as the shape and dimension of each manifold is automatically inferred.We derive a simple inference scheme for this model which analyticallyintegrates out both the mixture parameters and the warping function. We showthat our model is effective for density estimation, performs better thaninfinite Gaussian mixture models at recovering the true number of clusters, andproduces interpretable summaries of high-dimensional datasets.
arxiv-7200-296 | Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations | http://arxiv.org/pdf/1408.2060v1.pdf | author:Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan Tan, Patrick Jaillet category:cs.LG cs.DC stat.ML published:2014-08-09 summary:Gaussian processes (GP) are Bayesian non-parametric models that are widelyused for probabilistic regression. Unfortunately, it cannot scale well withlarge data nor perform real-time predictions due to its cubic time cost in thedata size. This paper presents two parallel GP regression methods that exploitlow-rank covariance matrix approximations for distributing the computationalload among parallel machines to achieve time efficiency and scalability. Wetheoretically guarantee the predictive performances of our proposed parallelGPs to be equivalent to that of some centralized approximate GP regressionmethods: The computation of their centralized counterparts can be distributedamong parallel machines, hence achieving greater time efficiency andscalability. We analytically compare the properties of our parallel GPs such astime, space, and communication complexity. Empirical evaluation on tworeal-world datasets in a cluster of 20 computing nodes shows that our parallelGPs are significantly more time-efficient and scalable than their centralizedcounterparts and exact/full GP while achieving predictive performancescomparable to full GP.
arxiv-7200-297 | Guess Who Rated This Movie: Identifying Users Through Subspace Clustering | http://arxiv.org/pdf/1408.2055v1.pdf | author:Amy Zhang, Nadia Fawaz, Stratis Ioannidis, Andrea Montanari category:cs.LG cs.IR stat.ML published:2014-08-09 summary:It is often the case that, within an online recommender system, multipleusers share a common account. Can such shared accounts be identified solely onthe basis of the userprovided ratings? Once a shared account is identified, canthe different users sharing it be identified as well? Whenever such useridentification is feasible, it opens the way to possible improvements inpersonalized recommendations, but also raises privacy concerns. We develop amodel for composite accounts based on unions of linear subspaces, and usesubspace clustering for carrying out the identification task. We show that asignificant fraction of such accounts is identifiable in a reliable manner, andillustrate potential uses for personalized recommendation.
arxiv-7200-298 | Non-Convex Rank Minimization via an Empirical Bayesian Approach | http://arxiv.org/pdf/1408.2054v1.pdf | author:David Wipf category:cs.LG cs.NA stat.ML published:2014-08-09 summary:In many applications that require matrix solutions of minimal rank, theunderlying cost function is non-convex leading to an intractable, NP-hardoptimization problem. Consequently, the convex nuclear norm is frequently usedas a surrogate penalty term for matrix rank. The problem is that in manypractical scenarios there is no longer any guarantee that we can correctlyestimate generative low-rank matrices of interest, theoretical special casesnotwithstanding. Consequently, this paper proposes an alternative empiricalBayesian procedure build upon a variational approximation that, unlike thenuclear norm, retains the same globally minimizing point estimate as the rankfunction under many useful constraints. However, locally minimizing solutionsare largely smoothed away via marginalization, allowing the algorithm tosucceed when standard convex relaxations completely fail. While the proposedmethodology is generally applicable to a wide range of low-rank applications,we focus our attention on the robust principal component analysis problem(RPCA), which involves estimating an unknown low-rank matrix with unknownsparse corruptions. Theoretical and empirical evidence are presented to showthat our method is potentially superior to related MAP-based approaches, forwhich the convex principle component pursuit (PCP) algorithm (Candes et al.,2011) can be viewed as a special case.
arxiv-7200-299 | Algorithms for Approximate Minimization of the Difference Between Submodular Functions, with Applications | http://arxiv.org/pdf/1408.2051v1.pdf | author:Rishabh Iyer, Jeff A. Bilmes category:cs.LG stat.ML published:2014-08-09 summary:We extend the work of Narasimhan and Bilmes [30] for minimizing set functionsrepresentable as a dierence between submodular functions. Similar to [30], ournew algorithms are guaranteed to monotonically reduce the objective function atevery step. We empirically and theoretically show that the per-iteration costof our algorithms is much less than [30], and our algorithms can be used toefficiently minimize a dierence between submodular functions under variouscombinatorial constraints, a problem not previously addressed. We providecomputational bounds and a hardness result on the multiplicativeinapproximability of minimizing the dierence between submodular functions. Weshow, however, that it is possible to give worst-case additive bounds byproviding a polynomial time computable lower-bound on the minima. Finally weshow how a number of machine learning problems can be modeled as minimizing thedierence between submodular functions. We experimentally show the validity ofour algorithms by testing them on the problem of feature selection withsubmodular cost features.
arxiv-7200-300 | Optimally-Weighted Herding is Bayesian Quadrature | http://arxiv.org/pdf/1408.2049v1.pdf | author:Ferenc Huszar, David Duvenaud category:cs.LG stat.ML published:2014-08-09 summary:Herding and kernel herding are deterministic methods of choosing sampleswhich summarise a probability distribution. A related task is choosing samplesfor estimating integrals using Bayesian quadrature. We show that the criterionminimised when selecting samples in kernel herding is equivalent to theposterior variance in Bayesian quadrature. We then show that sequentialBayesian quadrature can be viewed as a weighted version of kernel herding whichachieves performance superior to any other weighted herding method. Wedemonstrate empirically a rate of convergence faster than O(1/N). Our resultsalso imply an upper bound on the empirical error of the Bayesian quadratureestimate.
