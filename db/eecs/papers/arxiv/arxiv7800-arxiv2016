arxiv-7800-1 | Stacked Quantizers for Compositional Vector Compression | http://arxiv.org/pdf/1411.2173v1.pdf | author:Julieta Martinez, Holger H. Hoos, James J. Little category:cs.CV published:2014-11-08 summary:Recently, Babenko and Lempitsky introduced Additive Quantization (AQ), ageneralization of Product Quantization (PQ) where a non-independent set ofcodebooks is used to compress vectors into small binary codes. Unfortunately,under this scheme encoding cannot be done independently in each codebook, andoptimal encoding is an NP-hard problem. In this paper, we observe that PQ andAQ are both compositional quantizers that lie on the extremes of the codebookdependence-independence assumption, and explore an intermediate approach thatexploits a hierarchical structure in the codebooks. This results in a methodthat achieves quantization error on par with or lower than AQ, while beingseveral orders of magnitude faster. We perform a complexity analysis of PQ, AQand our method, and evaluate our approach on standard benchmarks of SIFT andGIST descriptors, as well as on new datasets of features obtained fromstate-of-the-art convolutional neural networks.
arxiv-7800-2 | Evolving intraday foreign exchange trading strategies utilizing multiple instruments price series | http://arxiv.org/pdf/1411.2153v1.pdf | author:Simone Cirillo, Stefan Lloyd, Peter Nordin category:cs.NE q-fin.TR I.2.2 published:2014-11-08 summary:We propose a Genetic Programming architecture for the generation of foreignexchange trading strategies. The system's principal features are the evolutionof free-form strategies which do not rely on any prior models and theutilization of price series from multiple instruments as input data. Thislatter feature constitutes an innovation with respect to previous worksdocumented in literature. In this article we utilize Open, High, Low, Close bardata at a 5 minutes frequency for the AUD.USD, EUR.USD, GBP.USD and USD.JPYcurrency pairs. We will test the implementation analyzing the in-sample andout-of-sample performance of strategies for trading the USD.JPY obtained acrossmultiple algorithm runs. We will also evaluate the differences betweenstrategies selected according to two different criteria: one relies on thefitness obtained on the training set only, the second one makes use of anadditional validation dataset. Strategy activity and trade accuracy areremarkably stable between in and out of sample results. From a profitabilityaspect, the two criteria both result in strategies successful on out-of-sampledata but exhibiting different characteristics. The overall best performingout-of-sample strategy achieves a yearly return of 19%.
arxiv-7800-3 | Fast Mesh-Based Medical Image Registration | http://arxiv.org/pdf/1411.2141v1.pdf | author:Ahmadreza Baghaie, Zeyun Yu, Roshan M. D'souza category:cs.CV published:2014-11-08 summary:In this paper a fast triangular mesh based registration method is proposed.Having Template and Reference images as inputs, the template image istriangulated using a content adaptive mesh generation algorithm. Consideringthe pixel values at mesh nodes, interpolated using spline interpolation methodfor both of the images, the energy functional needed for image registration isminimized. The minimization process was achieved using a mesh baseddiscretization of the distance measure and regularization term which resultedin a sparse system of linear equations, which due to the smaller size incomparison to the pixel-wise registration method, can be solved directly. MeanSquared Di?erence (MSD) is used as a metric for evaluating the results. Usingthe mesh based technique, higher speed was achieved compared to pixel-basedcurvature registration technique with fast DCT solver. The implementation wasdone in MATLAB without any speci?c optimization. Higher speeds can be achievedusing C/C++ implementations.
arxiv-7800-4 | Dimensionality Reduction of Affine Variational Inequalities Using Random Projections | http://arxiv.org/pdf/1408.4551v2.pdf | author:Bharat Prabhakar, Ankur A. Kulkarni category:math.OC cs.LG cs.SY published:2014-08-20 summary:We present a method for dimensionality reduction of an affine variationalinequality (AVI) defined over a compact feasible region. Centered around theJohnson Lindenstrauss lemma, our method is a randomized algorithm that produceswith high probability an approximate solution for the given AVI by solving alower-dimensional AVI. The algorithm allows the lower dimension to be chosenbased on the quality of approximation desired. The algorithm can also be usedas a subroutine in an exact algorithm for generating an initial point close tothe solution. The lower-dimensional AVI is obtained by appropriately projectingthe original AVI on a randomly chosen subspace. The lower-dimensional AVI issolved using standard solvers and from this solution an approximate solution tothe original AVI is recovered through an inexpensive process. Our numericalexperiments corroborate the theoretical results and validate that the algorithmprovides a good approximation at low dimensions and substantial savings in timefor an exact solution.
arxiv-7800-5 | Parallax Effect Free Mosaicing of Underwater Video Sequence Based on Texture Features | http://arxiv.org/pdf/1411.2090v1.pdf | author:Nagaraja S., Prabhakar C. J., Praveen Kumar P. U category:cs.CV published:2014-11-08 summary:In this paper, we present feature-based technique for construction of mosaicimage from underwater video sequence, which suffers from parallax distortiondue to propagation properties of light in the underwater environment. The mostof the available mosaic tools and underwater image mosaicing techniques yieldsfinal result with some artifacts such as blurring, ghosting and seam due topresence of parallax in the input images. The removal of parallax from inputimages may not reduce its effects instead it must be corrected in successivesteps of mosaicing. Thus, our approach minimizes the parallax effects byadopting an efficient local alignment technique after global registration. Weextract texture features using Centre Symmetric Local Binary Pattern (CS-LBP)descriptor in order to find feature correspondences, which are used further forestimation of homography through RANSAC. In order to increase the accuracy ofglobal registration, we perform preprocessing such as colour alignment betweentwo selected frames based on colour distribution adjustment. Because ofexistence of 100% overlap in consecutive frames of underwater video, we selectframes with minimum overlap based on mutual offset in order to reduce thecomputation cost during mosaicing. Our approach minimizes the parallax effectsconsiderably in final mosaic constructed using our own underwater videosequences.
arxiv-7800-6 | On Communication Cost of Distributed Statistical Estimation and Dimensionality | http://arxiv.org/pdf/1405.1665v2.pdf | author:Ankit Garg, Tengyu Ma, Huy L. Nguyen category:cs.LG cs.IT math.IT published:2014-05-07 summary:We explore the connection between dimensionality and communication cost indistributed learning problems. Specifically we study the problem of estimatingthe mean $\vec{\theta}$ of an unknown $d$ dimensional gaussian distribution inthe distributed setting. In this problem, the samples from the unknowndistribution are distributed among $m$ different machines. The goal is toestimate the mean $\vec{\theta}$ at the optimal minimax rate whilecommunicating as few bits as possible. We show that in this setting, thecommunication cost scales linearly in the number of dimensions i.e. one needsto deal with different dimensions individually. Applying this result toprevious lower bounds for one dimension in the interactive setting\cite{ZDJW13} and to our improved bounds for the simultaneous setting, we provenew lower bounds of $\Omega(md/\log(m))$ and $\Omega(md)$ for the bits ofcommunication needed to achieve the minimax squared loss, in the interactiveand simultaneous settings respectively. To complement, we also demonstrate aninteractive protocol achieving the minimax squared loss with $O(md)$ bits ofcommunication, which improves upon the simple simultaneous protocol by alogarithmic factor. Given the strong lower bounds in the general setting, weinitiate the study of the distributed parameter estimation problems withstructured parameters. Specifically, when the parameter is promised to be$s$-sparse, we show a simple thresholding based protocol that achieves the samesquared loss while saving a $d/s$ factor of communication. We conjecture thatthe tradeoff between communication and squared loss demonstrated by thisprotocol is essentially optimal up to logarithmic factor.
arxiv-7800-7 | A Novel Approach to Develop a New Hybrid Technique for Trademark Image Retrieval | http://arxiv.org/pdf/1504.03315v1.pdf | author:Saurabh Agarwal, Punit Kumar Johari category:cs.CV published:2014-11-08 summary:Trademark Image Retrieval is playing a vital role as a part of CBIR System.Trademark is of great significance because it carries the status value of anycompany. To retrieve such a fake or copied trademark we design a retrievalsystem which is based on hybrid techniques. It contains a mixture of twodifferent feature vector which combined together to give a suitable retrievalsystem. In the proposed system we extract the corner feature which is appliedon an edge pixel image. This feature is used to extract the relevant image andto more purify the result we apply other feature which is the invariant momentfeature. From the experimental result we conclude that the system is 85 percentefficient.
arxiv-7800-8 | Online Collaborative-Filtering on Graphs | http://arxiv.org/pdf/1411.2057v1.pdf | author:Siddhartha Banerjee, Sujay Sanghavi, Sanjay Shakkottai category:cs.LG published:2014-11-07 summary:A common phenomena in modern recommendation systems is the use of feedbackfrom one user to infer the `value' of an item to other users. This results inan exploration vs. exploitation trade-off, in which items of possibly low valuehave to be presented to users in order to ascertain their value. Existingapproaches to solving this problem focus on the case where the number of itemsare small, or admit some underlying structure -- it is unclear, however, ifgood recommendation is possible when dealing with content-rich settings withunstructured content. We consider this problem under a simple natural model, wherein the number ofitems and the number of item-views are of the same order, and an `access-graph'constrains which user is allowed to see which item. Our main insight is thatthe presence of the access-graph in fact makes good recommendation possible --however this requires the exploration policy to be designed to take advantageof the access-graph. Our results demonstrate the importance of `serendipity' inexploration, and how higher graph-expansion translates to a higher quality ofrecommendations; it also suggests a reason why in some settings, simplepolicies like Twitter's `Latest-First' policy achieve a good performance. From a technical perspective, our model presents a way to studyexploration-exploitation tradeoffs in settings where the number of `trials' and`strategies' are large (potentially infinite), and more importantly, of thesame order. Our algorithms admit competitive-ratio guarantees which hold forthe worst-case user, under both finite-population and infinite-horizonsettings, and are parametrized in terms of properties of the underlying graph.Conversely, we also demonstrate that improperly-designed policies can be highlysub-optimal, and that in many settings, our results are order-wise optimal.
arxiv-7800-9 | Multivariate f-Divergence Estimation With Confidence | http://arxiv.org/pdf/1411.2045v1.pdf | author:Kevin R. Moon, Alfred O. Hero III category:cs.IT math.IT stat.ML published:2014-11-07 summary:The problem of f-divergence estimation is important in the fields of machinelearning, information theory, and statistics. While several nonparametricdivergence estimators exist, relatively few have known convergence properties.In particular, even for those estimators whose MSE convergence rates are known,the asymptotic distributions are unknown. We establish the asymptotic normalityof a recently proposed ensemble estimator of f-divergence between twodistributions from a finite number of samples. This estimator has MSEconvergence rate of O(1/T), is simple to implement, and performs well in highdimensions. This theory enables us to perform divergence-based inference taskssuch as testing equality of pairs of distributions based on empirical samples.We experimentally validate our theoretical results and, as an illustration, usethem to empirically bound the best achievable classification error.
arxiv-7800-10 | Dictionary Learning and Tensor Decomposition via the Sum-of-Squares Method | http://arxiv.org/pdf/1407.1543v2.pdf | author:Boaz Barak, Jonathan A. Kelner, David Steurer category:cs.DS cs.LG stat.ML published:2014-07-06 summary:We give a new approach to the dictionary learning (also known as "sparsecoding") problem of recovering an unknown $n\times m$ matrix $A$ (for $m \geqn$) from examples of the form \[ y = Ax + e, \] where $x$ is a random vector in$\mathbb R^m$ with at most $\tau m$ nonzero coordinates, and $e$ is a randomnoise vector in $\mathbb R^n$ with bounded magnitude. For the case $m=O(n)$,our algorithm recovers every column of $A$ within arbitrarily good constantaccuracy in time $m^{O(\log m/\log(\tau^{-1}))}$, in particular achievingpolynomial time if $\tau = m^{-\delta}$ for any $\delta>0$, and time $m^{O(\logm)}$ if $\tau$ is (a sufficiently small) constant. Prior algorithms withcomparable assumptions on the distribution required the vector $x$ to be muchsparser---at most $\sqrt{n}$ nonzero coordinates---and there were intrinsicbarriers preventing these algorithms from applying for denser $x$. We achieve this by designing an algorithm for noisy tensor decomposition thatcan recover, under quite general conditions, an approximate rank-onedecomposition of a tensor $T$, given access to a tensor $T'$ that is$\tau$-close to $T$ in the spectral norm (when considered as a matrix). To ourknowledge, this is the first algorithm for tensor decomposition that works inthe constant spectral-norm noise regime, where there is no guarantee that thelocal optima of $T$ and $T'$ have similar structures. Our algorithm is based on a novel approach to using and analyzing the Sum ofSquares semidefinite programming hierarchy (Parrilo 2000, Lasserre 2001), andit can be viewed as an indication of the utility of this very general andpowerful tool for unsupervised learning problems.
arxiv-7800-11 | Scalable Variational Gaussian Process Classification | http://arxiv.org/pdf/1411.2005v1.pdf | author:James Hensman, Alex Matthews, Zoubin Ghahramani category:stat.ML published:2014-11-07 summary:Gaussian process classification is a popular method with a number ofappealing properties. We show how to scale the model within a variationalinducing point framework, outperforming the state of the art on benchmarkdatasets. Importantly, the variational formulation can be exploited to allowclassification in problems with millions of data points, as we demonstrate inexperiments.
arxiv-7800-12 | Azhary: An Arabic Lexical Ontology | http://arxiv.org/pdf/1411.1999v1.pdf | author:Hossam Ishkewy, Hany Harb, Hassan Farahat category:cs.AI cs.CL published:2014-11-07 summary:Arabic language is the most spoken languages in the Semitic languages group,and one of the most common languages in the world spoken by more than 422million. It is also of paramount importance to Muslims, it is a sacred languageof the Islamic Holly Book (Quran) and prayer (and other acts of worship) inIslam is performed only by mastering some of Arabic words. Arabic is also amajor ritual language of a number of Christian churches in the Arab world andit is also used in writing several intellectual and religious Jewish books inthe Middle Ages. Despite this, there is no semantic Arabic lexicon whichresearchers can depend on. In this paper we introduce Azhary as a lexicalontology for the Arabic language. It groups Arabic words into sets of synonymscalled synsets, and records a number of relationships between words such assynonym, antonym, hypernym, hyponym, meronym, holonym and associationrelations. The ontology contains 26,195 words organized in 13,328 synsets. Ithas been developed and contrasted against AWN which is the most commonavailable Arabic lexical ontology.
arxiv-7800-13 | Differential gene co-expression networks via Bayesian biclustering models | http://arxiv.org/pdf/1411.1997v1.pdf | author:Chuan Gao, Shiwen Zhao, Ian C. McDowell, Christopher D. Brown, Barbara E. Engelhardt category:stat.ME q-bio.GN q-bio.MN stat.ML published:2014-11-07 summary:Identifying latent structure in large data matrices is essential forexploring biological processes. Here, we consider recovering gene co-expressionnetworks from gene expression data, where each network encodes relationshipsbetween genes that are locally co-regulated by shared biological mechanisms. Todo this, we develop a Bayesian statistical model for biclustering to infersubsets of co-regulated genes whose covariation may be observed in only asubset of the samples. Our biclustering method, BicMix, has desirableproperties, including allowing overcomplete representations of the data,computational tractability, and jointly modeling unknown confounders andbiological signals. Compared with related biclustering methods, BicMix recoverslatent structure with higher precision across diverse simulation scenarios.Further, we develop a method to recover gene co-expression networks from theestimated sparse biclustering matrices. We apply BicMix to breast cancer geneexpression data and recover a gene co-expression network that is differentialacross ER+ and ER- samples.
arxiv-7800-14 | Data-driven HRF estimation for encoding and decoding models | http://arxiv.org/pdf/1402.7015v6.pdf | author:Fabian Pedregosa, Michael Eickenberg, Philippe Ciuciu, Bertrand Thirion, Alexandre Gramfort category:cs.CE cs.LG published:2014-02-27 summary:Despite the common usage of a canonical, data-independent, hemodynamicresponse function (HRF), it is known that the shape of the HRF varies acrossbrain regions and subjects. This suggests that a data-driven estimation of thisfunction could lead to more statistical power when modeling BOLD fMRI data.However, unconstrained estimation of the HRF can yield highly unstable resultswhen the number of free parameters is large. We develop a method for the jointestimation of activation and HRF using a rank constraint causing the estimatedHRF to be equal across events/conditions, yet permitting it to be differentacross voxels. Model estimation leads to an optimization problem that wepropose to solve with an efficient quasi-Newton method exploiting fast gradientcomputations. This model, called GLM with Rank-1 constraint (R1-GLM), can beextended to the setting of GLM with separate designs which has been shown toimprove decoding accuracy in brain activity decoding experiments. We compare 10different HRF modeling methods in terms of encoding and decoding score in twodifferent datasets. Our results show that the R1-GLM model significantlyoutperforms competing methods in both encoding and decoding settings,positioning it as an attractive method both from the points of view of accuracyand computational efficiency.
arxiv-7800-15 | Marginal AMP Chain Graphs | http://arxiv.org/pdf/1305.0751v6.pdf | author:Jose M. Peña category:stat.ML cs.AI published:2013-05-03 summary:We present a new family of models that is based on graphs that may haveundirected, directed and bidirected edges. We name these new models marginalAMP (MAMP) chain graphs because each of them is Markov equivalent to some AMPchain graph under marginalization of some of its nodes. However, MAMP chaingraphs do not only subsume AMP chain graphs but also multivariate regressionchain graphs. We describe global and pairwise Markov properties for MAMP chaingraphs and prove their equivalence for compositional graphoids. We alsocharacterize when two MAMP chain graphs are Markov equivalent. For Gaussian probability distributions, we also show that every MAMP chaingraph is Markov equivalent to some directed and acyclic graph withdeterministic nodes under marginalization and conditioning on some of itsnodes. This is important because it implies that the independence modelrepresented by a MAMP chain graph can be accounted for by some data generatingprocess that is partially observed and has selection bias. Finally, we modifyMAMP chain graphs so that they are closed under marginalization for Gaussianprobability distributions. This is a desirable feature because it guaranteesparsimonious models under marginalization.
arxiv-7800-16 | Large-Margin Determinantal Point Processes | http://arxiv.org/pdf/1411.1537v2.pdf | author:Boqing Gong, Wei-lun Chao, Kristen Grauman, Fei Sha category:stat.ML cs.CV cs.LG published:2014-11-06 summary:Determinantal point processes (DPPs) offer a powerful approach to modelingdiversity in many applications where the goal is to select a diverse subset. Westudy the problem of learning the parameters (the kernel matrix) of a DPP fromlabeled training data. We make two contributions. First, we show how toreparameterize a DPP's kernel matrix with multiple kernel functions, thusenhancing modeling flexibility. Second, we propose a novel parameter estimationtechnique based on the principle of large margin separation. In contrast to thestate-of-the-art method of maximum likelihood estimation, our large-margin lossfunction explicitly models errors in selecting the target subsets, and it canbe customized to trade off different types of errors (precision vs. recall).Extensive empirical studies validate our contributions, including applicationson challenging document and video summarization, where flexibility in modelingthe kernel matrix and balancing different errors is indispensable.
arxiv-7800-17 | Sparse Polynomial Learning and Graph Sketching | http://arxiv.org/pdf/1402.3902v4.pdf | author:Murat Kocaoglu, Karthikeyan Shanmugam, Alexandros G. Dimakis, Adam Klivans category:cs.LG published:2014-02-17 summary:Let $f:\{-1,1\}^n$ be a polynomial with at most $s$ non-zero realcoefficients. We give an algorithm for exactly reconstructing f given randomexamples from the uniform distribution on $\{-1,1\}^n$ that runs in timepolynomial in $n$ and $2s$ and succeeds if the function satisfies the uniquesign property: there is one output value which corresponds to a unique set ofvalues of the participating parities. This sufficient condition is satisfiedwhen every coefficient of f is perturbed by a small random noise, or satisfiedwith high probability when s parity functions are chosen randomly or when allthe coefficients are positive. Learning sparse polynomials over the Booleandomain in time polynomial in $n$ and $2s$ is considered notoriously hard in theworst-case. Our result shows that the problem is tractable for almost allsparse polynomials. Then, we show an application of this result to hypergraphsketching which is the problem of learning a sparse (both in the number ofhyperedges and the size of the hyperedges) hypergraph from uniformly drawnrandom cuts. We also provide experimental results on a real world dataset.
arxiv-7800-18 | How transferable are features in deep neural networks? | http://arxiv.org/pdf/1411.1792v1.pdf | author:Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson category:cs.LG cs.NE published:2014-11-06 summary:Many deep neural networks trained on natural images exhibit a curiousphenomenon in common: on the first layer they learn features similar to Gaborfilters and color blobs. Such first-layer features appear not to be specific toa particular dataset or task, but general in that they are applicable to manydatasets and tasks. Features must eventually transition from general tospecific by the last layer of the network, but this transition has not beenstudied extensively. In this paper we experimentally quantify the generalityversus specificity of neurons in each layer of a deep convolutional neuralnetwork and report a few surprising results. Transferability is negativelyaffected by two distinct issues: (1) the specialization of higher layer neuronsto their original task at the expense of performance on the target task, whichwas expected, and (2) optimization difficulties related to splitting networksbetween co-adapted neurons, which was not expected. In an example networktrained on ImageNet, we demonstrate that either of these two issues maydominate, depending on whether features are transferred from the bottom,middle, or top of the network. We also document that the transferability offeatures decreases as the distance between the base task and target taskincreases, but that transferring features even from distant tasks can be betterthan using random features. A final surprising result is that initializing anetwork with transferred features from almost any number of layers can producea boost to generalization that lingers even after fine-tuning to the targetdataset.
arxiv-7800-19 | Computer vision-based recognition of liquid surfaces and phase boundaries in transparent vessels, with emphasis on chemistry applications | http://arxiv.org/pdf/1404.7174v7.pdf | author:Sagi Eppel, Tal Kachman category:cs.CV published:2014-04-28 summary:The ability to recognize the liquid surface and the liquid level intransparent containers is perhaps the most commonly used evaluation method whendealing with fluids. Such recognition is essential in determining the liquidvolume, fill level, phase boundaries and phase separation in various fluidsystems. The recognition of liquid surfaces is particularly important insolution chemistry, where it is essential to many laboratory techniques (e.g.,extraction, distillation, titration). A general method for the recognition ofinterfaces between liquid and air or between phase-separating liquids couldhave a wide range of applications and contribute to the understanding of thevisual properties of such interfaces. This work examines a computer visionmethod for the recognition of liquid surfaces and liquid levels in varioustransparent containers. The method can be applied to recognition of bothliquid-air and liquid-liquid surfaces. No prior knowledge of the number ofphases is required. The method receives the image of the liquid container andthe boundaries of the container in the image and scans all possible curves thatcould correspond to the outlines of liquid surfaces in the image. The methodthen compares each curve to the image to rate its correspondence with theoutline of the real liquid surface by examining various image properties in thearea surrounding each point of the curve. The image properties that were foundto give the best indication of the liquid surface are the relative intensitychange, the edge density change and the gradient direction relative to thecurve normal.
arxiv-7800-20 | Conditional Generative Adversarial Nets | http://arxiv.org/pdf/1411.1784v1.pdf | author:Mehdi Mirza, Simon Osindero category:cs.LG cs.AI cs.CV stat.ML published:2014-11-06 summary:Generative Adversarial Nets [8] were recently introduced as a novel way totrain generative models. In this work we introduce the conditional version ofgenerative adversarial nets, which can be constructed by simply feeding thedata, y, we wish to condition on to both the generator and discriminator. Weshow that this model can generate MNIST digits conditioned on class labels. Wealso illustrate how this model could be used to learn a multi-modal model, andprovide preliminary examples of an application to image tagging in which wedemonstrate how this approach can generate descriptive tags which are not partof training labels.
arxiv-7800-21 | Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets | http://arxiv.org/pdf/1411.1752v1.pdf | author:Adarsh Prasad, Stefanie Jegelka, Dhruv Batra category:cs.LG cs.AI cs.CV cs.IR stat.ML published:2014-11-06 summary:To cope with the high level of ambiguity faced in domains such as ComputerVision or Natural Language processing, robust prediction methods often searchfor a diverse set of high-quality candidate solutions or proposals. Instructured prediction problems, this becomes a daunting task, as the solutionspace (image labelings, sentence parses, etc.) is exponentially large. We studygreedy algorithms for finding a diverse subset of solutions instructured-output spaces by drawing new connections between submodularfunctions over combinatorial item sets and High-Order Potentials (HOPs) studiedfor graphical models. Specifically, we show via examples that when marginalgains of submodular diversity functions allow structured representations, thisenables efficient (sub-linear time) approximate maximization by reducing thegreedy augmentation step to inference in a factor graph with appropriatelyconstructed HOPs. We discuss benefits, tradeoffs, and show that ourconstructions lead to significantly better proposals.
arxiv-7800-22 | Stochastic Variational Inference for Hidden Markov Models | http://arxiv.org/pdf/1411.1670v1.pdf | author:Nicholas J. Foti, Jason Xu, Dillon Laird, Emily B. Fox category:stat.ML published:2014-11-06 summary:Variational inference algorithms have proven successful for Bayesian analysisin large data settings, with recent advances using stochastic variationalinference (SVI). However, such methods have largely been studied in independentor exchangeable data settings. We develop an SVI algorithm to learn theparameters of hidden Markov models (HMMs) in a time-dependent data setting. Thechallenge in applying stochastic optimization in this setting arises fromdependencies in the chain, which must be broken to consider minibatches ofobservations. We propose an algorithm that harnesses the memory decay of thechain to adaptively bound errors arising from edge effects. We demonstrate theeffectiveness of our algorithm on synthetic experiments and a large genomicsdataset where a batch algorithm is computationally infeasible.
arxiv-7800-23 | Learning Word Representations with Hierarchical Sparse Coding | http://arxiv.org/pdf/1406.2035v2.pdf | author:Dani Yogatama, Manaal Faruqui, Chris Dyer, Noah A. Smith category:cs.CL cs.LG stat.ML published:2014-06-08 summary:We propose a new method for learning word representations using hierarchicalregularization in sparse coding inspired by the linguistic study of wordmeanings. We show an efficient learning algorithm based on stochastic proximalmethods that is significantly faster than previous approaches, making itpossible to perform hierarchical sparse coding on a corpus of billions of wordtokens. Experiments on various benchmark tasks---word similarity ranking,analogies, sentence completion, and sentiment analysis---demonstrate that themethod outperforms or is competitive with state-of-the-art methods. Our wordrepresentations are available at\url{http://www.ark.cs.cmu.edu/dyogatam/wordvecs/}.
arxiv-7800-24 | A Hybrid Recurrent Neural Network For Music Transcription | http://arxiv.org/pdf/1411.1623v1.pdf | author:Siddharth Sigtia, Emmanouil Benetos, Nicolas Boulanger-Lewandowski, Tillman Weyde, Artur S. d'Avila Garcez, Simon Dixon category:cs.LG published:2014-11-06 summary:We investigate the problem of incorporating higher-level symbolic score-likeinformation into Automatic Music Transcription (AMT) systems to improve theirperformance. We use recurrent neural networks (RNNs) and their variants asmusic language models (MLMs) and present a generative architecture forcombining these models with predictions from a frame level acoustic classifier.We also compare different neural network architectures for acoustic modeling.The proposed model computes a distribution over possible output sequences giventhe acoustic input signal and we present an algorithm for performing a globalsearch for good candidate transcriptions. The performance of the proposed modelis evaluated on piano music from the MAPS dataset and we observe that theproposed model consistently outperforms existing transcription methods.
arxiv-7800-25 | Rapid Skill Capture in a First-Person Shooter | http://arxiv.org/pdf/1411.1316v2.pdf | author:David Buckley, Ke Chen, Joshua Knowles category:cs.HC cs.LG published:2014-11-05 summary:Various aspects of computer game design, including adaptive elements of gamelevels, characteristics of 'bot' behavior, and player matching in multiplayergames, would ideally be sensitive to a player's skill level. Yet, whiledifficulty and player learning have been explored in the context of games,there has been little work analyzing skill per se, and how it pertains to aplayer's input. To this end, we present a data set of 476 game logs from over40 players of a first-person shooter game (Red Eclipse) as a basis of a casestudy. We then analyze different metrics of skill and show that some of thesecan be predicted using only a few seconds of keyboard and mouse input. We arguethat the techniques used here are useful for adapting games to match players'skill levels rapidly, perhaps more rapidly than solutions based on performanceaveraging such as TrueSkill.
arxiv-7800-26 | Proof Supplement - Learning Sparse Causal Models is not NP-hard (UAI2013) | http://arxiv.org/pdf/1411.1557v1.pdf | author:Tom Claassen, Joris M. Mooij, Tom Heskes category:stat.ML published:2014-11-06 summary:This article contains detailed proofs and additional examples related to theUAI-2013 submission `Learning Sparse Causal Models is not NP-hard'. Itdescribes the FCI+ algorithm: a method for sound and complete causal modeldiscovery in the presence of latent confounders and/or selection bias, that hasworst case polynomial complexity of order $N^{2(k+1)}$ in the number ofindependence tests, for sparse graphs over $N$ nodes, bounded by node degree$k$. The algorithm is an adaptation of the well-known FCI algorithm by (Spirteset al., 2000) that is also sound and complete, but has worst case complexityexponential in $N$.
arxiv-7800-27 | Convolutional Neural Network-based Place Recognition | http://arxiv.org/pdf/1411.1509v1.pdf | author:Zetao Chen, Obadiah Lam, Adam Jacobson, Michael Milford category:cs.CV cs.LG cs.NE published:2014-11-06 summary:Recently Convolutional Neural Networks (CNNs) have been shown to achievestate-of-the-art performance on various classification tasks. In this paper, wepresent for the first time a place recognition technique based on CNN models,by combining the powerful features learnt by CNNs with a spatial and sequentialfilter. Applying the system to a 70 km benchmark place recognition dataset weachieve a 75% increase in recall at 100% precision, significantly outperformingall previous state of the art techniques. We also conduct a comprehensiveperformance comparison of the utility of features from all 21 layers for placerecognition, both for the benchmark dataset and for a second dataset with moresignificant viewpoint changes.
arxiv-7800-28 | Learning to Discover Efficient Mathematical Identities | http://arxiv.org/pdf/1406.1584v3.pdf | author:Wojciech Zaremba, Karol Kurach, Rob Fergus category:cs.LG published:2014-06-06 summary:In this paper we explore how machine learning techniques can be applied tothe discovery of efficient mathematical identities. We introduce an attributegrammar framework for representing symbolic expressions. Given a set of grammarrules we build trees that combine different rules, looking for branches whichyield compositions that are analytically equivalent to a target expression, butof lower computational complexity. However, as the size of the trees growsexponentially with the complexity of the target expression, brute force searchis impractical for all but the simplest of expressions. Consequently, weintroduce two novel learning approaches that are able to learn from simplerexpressions to guide the tree search. The first of these is a simple n-grammodel, the other being a recursive neural-network. We show how these approachesenable us to derive complex identities, beyond reach of brute-force search, orhuman derivation.
arxiv-7800-29 | A Generic Sample Splitting Approach for Refined Community Recovery in Stochastic Block Models | http://arxiv.org/pdf/1411.1469v1.pdf | author:Jing Lei, Lingxue Zhu category:stat.ML math.ST stat.TH published:2014-11-06 summary:We propose and analyze a generic method for community recovery in stochasticblock models and degree corrected block models. This approach can exactlyrecover the hidden communities with high probability when the expected nodedegrees are of order $\log n$ or higher. Starting from a roughly correctcommunity partition given by some conventional community recovery algorithm,this method refines the partition in a cross clustering step. Our resultssimplify and extend some of the previous work on exact community recovery,discovering the key role played by sample splitting. The proposed method issimple and can be implemented with many practical community recoveryalgorithms.
arxiv-7800-30 | Electrocardiography Separation of Mother and Baby | http://arxiv.org/pdf/1411.1446v1.pdf | author:Wei Wang category:cs.CV cs.LG published:2014-11-05 summary:Extraction of Electrocardiography (ECG or EKG) signals of mother and baby isa challenging task, because one single device is used and it receives a mixtureof multiple heart beats. In this paper, we would like to design a filter toseparate the signals from each other.
arxiv-7800-31 | Optical Character Recognition, Using K-Nearest Neighbors | http://arxiv.org/pdf/1411.1442v1.pdf | author:Wei Wang category:cs.CV published:2014-11-05 summary:The problem of optical character recognition, OCR, has been widely discussedin the literature. Having a hand-written text, the program aims at recognizingthe text. Even though there are several approaches to this issue, it is stillan open problem. In this paper we would like to propose an approach that usesK-nearest neighbors algorithm, and has the accuracy of more than 90%. Thetraining and run time is also very short.
arxiv-7800-32 | Distributed Policy Evaluation Under Multiple Behavior Strategies | http://arxiv.org/pdf/1312.7606v2.pdf | author:Sergio Valcarcel Macua, Jianshu Chen, Santiago Zazo, Ali H. Sayed category:cs.MA cs.AI cs.DC cs.LG published:2013-12-30 summary:We apply diffusion strategies to develop a fully-distributed cooperativereinforcement learning algorithm in which agents in a network communicate onlywith their immediate neighbors to improve predictions about their environment.The algorithm can also be applied to off-policy learning, meaning that theagents can predict the response to a behavior different from the actualpolicies they are following. The proposed distributed strategy is efficient,with linear complexity in both computation time and memory footprint. Weprovide a mean-square-error performance analysis and establish convergenceunder constant step-size updates, which endow the network with continuouslearning capabilities. The results show a clear gain from cooperation: when theindividual agents can estimate the solution, cooperation increases stabilityand reduces bias and variance of the prediction error; but, more importantly,the network is able to approach the optimal solution even when none of theindividual agents can (e.g., when the individual behavior policies restricteach agent to sample a small portion of the state space).
arxiv-7800-33 | Online SLAM with Any-time Self-calibration and Automatic Change Detection | http://arxiv.org/pdf/1411.1372v1.pdf | author:Nima Keivan, Gabe Sibley category:cs.CV cs.RO published:2014-11-05 summary:A framework for online simultaneous localization, mapping andself-calibration is presented which can detect and handle significant change inthe calibration parameters. Estimates are computed in constant-time byfactoring the problem and focusing on segments of the trajectory that are mostinformative for the purposes of calibration. A novel technique is presented todetect the probability that a significant change is present in the calibrationparameters. The system is then able to re-calibrate. Maximum likelihoodtrajectory and map estimates are computed using an asynchronous and adaptiveoptimization. The system requires no prior information and is able toinitialize without any special motions or routines, or in the case whereobservability over calibration parameters is delayed. The system isexperimentally validated to calibrate camera intrinsic parameters for anonlinear camera model on a monocular dataset featuring a significant zoomevent partway through, and achieves high accuracy despite unknown initialcalibration parameters. Self-calibration and re-calibration parameters areshown to closely match estimates computed using a calibration target. Theaccuracy of the system is demonstrated with SLAM results that achieve sub-1%distance-travel error even in the presence of significant re-calibrationevents.
arxiv-7800-34 | Application of Multi-core Parallel Programming to a Combination of Ant Colony Optimization and Genetic Algorithm | http://arxiv.org/pdf/1411.4297v1.pdf | author:Rishita Kalyani category:cs.NE published:2014-11-05 summary:This Paper will deal with a combination of Ant Colony and Genetic ProgrammingAlgorithm to optimize Travelling Salesmen problem (NP-Hard). However, thecomplexity of the algorithm requires considerable computational time andresources. Parallel implementation can reduce the computational time. In thispaper, emphasis in the parallelizing section is given to Multi-corearchitecture and Multi-Processor Systems which is developed and used almosteverywhere today and hence, multi-core parallelization to the combination ofalgorithm is achieved by OpenMP library by Intel Corporation.
arxiv-7800-35 | Edge Detection based on Kernel Density Estimation | http://arxiv.org/pdf/1411.1297v1.pdf | author:Osvaldo Pereira, Esley Torre, Yasel Garcés, Roberto Rodríguez category:cs.CV published:2014-11-05 summary:Edges of an image are considered a crucial type of information. These can beextracted by applying edge detectors with different methodology. Edge detectionis a vital step in computer vision tasks, because it is an essential issue forpattern recognition and visual interpretation. In this paper, we propose a newmethod for edge detection in images, based on the estimation by kernel of theprobability density function. In our algorithm, pixels in the image withminimum value of density function are labeled as edges. The boundary betweentwo homogeneous regions is defined in two domains: the spatial/lattice domainand the range/color domain. Extensive experimental evaluations proved that ouredge detection method is significantly a competitive algorithm.
arxiv-7800-36 | Controlling false discoveries in high-dimensional situations: Boosting with stability selection | http://arxiv.org/pdf/1411.1285v1.pdf | author:Benjamin Hofner, Luigi Boccuto, Markus Göker category:stat.ML stat.AP stat.CO published:2014-11-05 summary:Modern biotechnologies often result in high-dimensional data sets with muchmore variables than observations (n $\ll$ p). These data sets pose newchallenges to statistical analysis: Variable selection becomes one of the mostimportant tasks in this setting. We assess the recently proposed flexibleframework for variable selection called stability selection. By the use ofresampling procedures, stability selection adds a finite sample error controlto high-dimensional variable selection procedures such as Lasso or boosting. Weconsider the combination of boosting and stability selection and presentresults from a detailed simulation study that provides insights into theusefulness of this combination. Limitations are discussed and guidance on thespecification and tuning of stability selection is given. The interpretation ofthe used error bounds is elaborated and insights for practical data analysisare given. The results will be used to detect differentially expressedphenotype measurements in patients with autism spectrum disorders. All methodsare implemented in the freely available R package stabs.
arxiv-7800-37 | Using Twitter to predict football outcomes | http://arxiv.org/pdf/1411.1243v1.pdf | author:Stylianos Kampakis, Andreas Adamides category:stat.ML cs.CL cs.SI I.2.m published:2014-11-05 summary:Twitter has been proven to be a notable source for predictive modelling onvarious domains such as the stock market, the dissemination of diseases orsports outcomes. However, such a study has not been conducted in football(soccer) so far. The purpose of this research was to study whether data minedfrom Twitter can be used for this purpose. We built a set of predictive modelsfor the outcome of football games of the English Premier League for a 3 monthperiod based on tweets and we studied whether these models can overcomepredictive models which use only historical data and simple footballstatistics. Moreover, combined models are constructed using both Twitter andhistorical data. The final results indicate that data mined from Twitter canindeed be a useful source for predicting games in the Premier League. The finalTwitter-based model performs significantly better than chance when measured byCohen's kappa and is comparable to the model that uses simple statistics andhistorical data. Combining both models raises the performance higher than itwas achieved by each individual model. Thereby, this study provides evidencethat Twitter derived features can indeed provide useful information for theprediction of football (soccer) outcomes.
arxiv-7800-38 | Variational Depth from Focus Reconstruction | http://arxiv.org/pdf/1408.0173v2.pdf | author:Michael Moeller, Martin Benning, Carola Schönlieb, Daniel Cremers category:cs.CV math.OC published:2014-08-01 summary:This paper deals with the problem of reconstructing a depth map from asequence of differently focused images, also known as depth from focus or shapefrom focus. We propose to state the depth from focus problem as a variationalproblem including a smooth but nonconvex data fidelity term, and a convexnonsmooth regularization, which makes the method robust to noise and leads tomore realistic depth maps. Additionally, we propose to solve the nonconvexminimization problem with a linearized alternating directions method ofmultipliers (ADMM), allowing to minimize the energy very efficiently. Anumerical comparison to classical methods on simulated as well as on real datais presented.
arxiv-7800-39 | A Gesture Recognition System for Detecting Behavioral Patterns of ADHD | http://arxiv.org/pdf/1410.4485v2.pdf | author:Miguel Ángel Bautista, Antonio Hernández-Vela, Sergio Escalera, Laura Igual, Oriol Pujol, Josep Moya, Verónica Violant, María Teresa Anguera category:cs.CV published:2014-10-16 summary:We present an application of gesture recognition using an extension ofDynamic Time Warping (DTW) to recognize behavioural patterns of AttentionDeficit Hyperactivity Disorder (ADHD). We propose an extension of DTW usingone-class classifiers in order to be able to encode the variability of agesture category, and thus, perform an alignment between a gesture sample and agesture class. We model the set of gesture samples of a certain gesturecategory using either GMMs or an approximation of Convex Hulls. Thus, we add atheoretical contribution to classical warping path in DTW by including localmodeling of intra-class gesture variability. This methodology is applied in aclinical context, detecting a group of ADHD behavioural patterns defined byexperts in psychology/psychiatry, to provide support to clinicians in thediagnose procedure. The proposed methodology is tested on a novel multi-modaldataset (RGB plus Depth) of ADHD children recordings with behavioural patterns.We obtain satisfying results when compared to standard state-of-the-artapproaches in the DTW context.
arxiv-7800-40 | Return of the Devil in the Details: Delving Deep into Convolutional Nets | http://arxiv.org/pdf/1405.3531v4.pdf | author:Ken Chatfield, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV published:2014-05-14 summary:The latest generation of Convolutional Neural Networks (CNN) have achievedimpressive results in challenging benchmarks on image recognition and objectdetection, significantly raising the interest of the community in thesemethods. Nevertheless, it is still unclear how different CNN methods comparewith each other and with previous state-of-the-art shallow representations suchas the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conductsa rigorous evaluation of these new techniques, exploring different deeparchitectures and comparing them on a common ground, identifying and disclosingimportant implementation details. We identify several useful properties ofCNN-based representations, including the fact that the dimensionality of theCNN output layer can be reduced significantly without having an adverse effecton performance. We also identify aspects of deep and shallow methods that canbe successfully shared. In particular, we show that the data augmentationtechniques commonly applied to CNN-based methods can also be applied to shallowmethods, and result in an analogous performance boost. Source code and modelsto reproduce the experiments in the paper is made publicly available.
arxiv-7800-41 | Tensor object classification via multilinear discriminant analysis network | http://arxiv.org/pdf/1411.1172v1.pdf | author:Rui Zeng, Jiasong Wu, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2014-11-05 summary:This paper proposes a multilinear discriminant analysis network (MLDANet) forthe recognition of multidimensional objects, known as tensor objects. TheMLDANet is a variation of linear discriminant analysis network (LDANet) andprincipal component analysis network (PCANet), both of which are the recentlyproposed deep learning algorithms. The MLDANet consists of three parts: 1) Theencoder learned by MLDA from tensor data. 2) Features maps ob-tained fromdecoder. 3) The use of binary hashing and histogram for feature pooling. Alearning algorithm for MLDANet is described. Evaluations on UCF11 databaseindicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA,and MLDA in terms of classification for tensor objects.
arxiv-7800-42 | Multilinear Principal Component Analysis Network for Tensor Object Classification | http://arxiv.org/pdf/1411.1171v1.pdf | author:Rui Zeng, Jiasong Wu, Zhuhong Shao, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2014-11-05 summary:The recently proposed principal component analysis network (PCANet) has beenproved high performance for visual content classification. In this letter, wedevelop a tensorial extension of PCANet, namely, multilinear principal analysiscomponent network (MPCANet), for tensor object classification. Compared toPCANet, the proposed MPCANet uses the spatial structure and the relationshipbetween each dimension of tensor objects much more efficiently. Experimentswere conducted on different visual content datasets including UCF sports actionvideo sequences database and UCF11 database. The experimental results haverevealed that the proposed MPCANet achieves higher classification accuracy thanPCANet for tensor object classification.
arxiv-7800-43 | On the Convergence Rate of Decomposable Submodular Function Minimization | http://arxiv.org/pdf/1406.6474v3.pdf | author:Robert Nishihara, Stefanie Jegelka, Michael I. Jordan category:math.OC cs.DM cs.DS cs.LG cs.NA published:2014-06-25 summary:Submodular functions describe a variety of discrete problems in machinelearning, signal processing, and computer vision. However, minimizingsubmodular functions poses a number of algorithmic challenges. Recent workintroduced an easy-to-use, parallelizable algorithm for minimizing submodularfunctions that decompose as the sum of "simple" submodular functions.Empirically, this algorithm performs extremely well, but no theoreticalanalysis was given. In this paper, we show that the algorithm convergeslinearly, and we provide upper and lower bounds on the rate of convergence. Ourproof relies on the geometry of submodular polyhedra and draws on results fromspectral graph theory.
arxiv-7800-44 | A Probabilistic Translation Method for Dictionary-based Cross-lingual Information Retrieval in Agglutinative Languages | http://arxiv.org/pdf/1411.1006v2.pdf | author:Javid Dadashkarimi, Azadeh Shakery, Heshaam Faili category:cs.IR cs.CL published:2014-11-04 summary:Translation ambiguity, out of vocabulary words and missing some translationsin bilingual dictionaries make dictionary-based Cross-language InformationRetrieval (CLIR) a challenging task. Moreover, in agglutinative languages whichdo not have reliable stemmers, missing various lexical formations in bilingualdictionaries degrades CLIR performance. This paper aims to introduce aprobabilistic translation model to solve the ambiguity problem, and also toprovide most likely formations of a dictionary candidate. We propose MinimumEdit Support Candidates (MESC) method that exploits a monolingual corpus and abilingual dictionary to translate users' native language queries to documents'language. Our experiments show that the proposed method outperformsstate-of-the-art dictionary-based English-Persian CLIR.
arxiv-7800-45 | Statistical Active Learning Algorithms for Noise Tolerance and Differential Privacy | http://arxiv.org/pdf/1307.3102v4.pdf | author:Maria Florina Balcan, Vitaly Feldman category:cs.LG cs.DS stat.ML published:2013-07-11 summary:We describe a framework for designing efficient active learning algorithmsthat are tolerant to random classification noise and aredifferentially-private. The framework is based on active learning algorithmsthat are statistical in the sense that they rely on estimates of expectationsof functions of filtered random examples. It builds on the powerful statisticalquery framework of Kearns (1993). We show that any efficient active statistical learning algorithm can beautomatically converted to an efficient active learning algorithm which istolerant to random classification noise as well as other forms of"uncorrelated" noise. The complexity of the resulting algorithms hasinformation-theoretically optimal quadratic dependence on $1/(1-2\eta)$, where$\eta$ is the noise rate. We show that commonly studied concept classes including thresholds,rectangles, and linear separators can be efficiently actively learned in ourframework. These results combined with our generic conversion lead to the firstcomputationally-efficient algorithms for actively learning some of theseconcept classes in the presence of random classification noise that provideexponential improvement in the dependence on the error $\epsilon$ over theirpassive counterparts. In addition, we show that our algorithms can beautomatically converted to efficient active differentially-private algorithms.This leads to the first differentially-private active learning algorithms withexponential label savings over the passive case.
arxiv-7800-46 | On the Complexity of Learning with Kernels | http://arxiv.org/pdf/1411.1158v1.pdf | author:Nicolò Cesa-Bianchi, Yishay Mansour, Ohad Shamir category:cs.LG stat.ML published:2014-11-05 summary:A well-recognized limitation of kernel learning is the requirement to handlea kernel matrix, whose size is quadratic in the number of training examples.Many methods have been proposed to reduce this computational cost, mostly byusing a subset of the kernel matrix entries, or some form of low-rank matrixapproximation, or a random projection method. In this paper, we study lowerbounds on the error attainable by such methods as a function of the number ofentries observed in the kernel matrix or the rank of an approximate kernelmatrix. We show that there are kernel learning problems where no such methodwill lead to non-trivial computational savings. Our results also quantify howthe problem difficulty depends on parameters such as the nature of the lossfunction, the regularization parameter, the norm of the desired predictor, andthe kernel matrix rank. Our results also suggest cases where more efficientkernel learning might be possible.
arxiv-7800-47 | Classification with the nearest neighbor rule in general finite dimensional spaces: necessary and sufficient conditions | http://arxiv.org/pdf/1411.0894v2.pdf | author:Sébastien Gadat, Thierry Klein, Clément Marteau category:math.ST stat.ML stat.TH published:2014-11-04 summary:Given an $n$-sample of random vectors $(X_i,Y_i)_{1 \leq i \leq n}$ whosejoint law is unknown, the long-standing problem of supervised classificationaims to \textit{optimally} predict the label $Y$ of a given a new observation$X$. In this context, the nearest neighbor rule is a popular flexible andintuitive method in non-parametric situations. Even if this algorithm is commonly used in the machine learning andstatistics communities, less is known about its prediction ability in generalfinite dimensional spaces, especially when the support of the density of theobservations is $\mathbb{R}^d$. This paper is devoted to the study of thestatistical properties of the nearest neighbor rule in various situations. Inparticular, attention is paid to the marginal law of $X$, as well as thesmoothness and margin properties of the \textit{regression function} $\eta(X) =\mathbb{E}[Y X]$. We identify two necessary and sufficient conditions toobtain uniform consistency rates of classification and to derive sharpestimates in the case of the nearest neighbor rule. Some numerical experimentsare proposed at the end of the paper to help illustrate the discussion.
arxiv-7800-48 | Adaptive Learning in Cartesian Product of Reproducing Kernel Hilbert Spaces | http://arxiv.org/pdf/1408.0853v2.pdf | author:Masahiro Yukawa category:cs.LG stat.ML published:2014-08-05 summary:We propose a novel adaptive learning algorithm based on iterative orthogonalprojections in the Cartesian product of multiple reproducing kernel Hilbertspaces (RKHSs). The task is estimating/tracking nonlinear functions which aresupposed to contain multiple components such as (i) linear and nonlinearcomponents, (ii) high- and low- frequency components etc. In this case, the useof multiple RKHSs permits a compact representation of multicomponent functions.The proposed algorithm is where two different methods of the author meet:multikernel adaptive filtering and the algorithm of hyperplane projection alongaffine subspace (HYPASS). In a certain particular case, the sum space of theRKHSs is isomorphic to the product space and hence the proposed algorithm canalso be regarded as an iterative projection method in the sum space. Theefficacy of the proposed algorithm is shown by numerical examples.
arxiv-7800-49 | High-Speed Tracking with Kernelized Correlation Filters | http://arxiv.org/pdf/1404.7584v3.pdf | author:João F. Henriques, Rui Caseiro, Pedro Martins, Jorge Batista category:cs.CV published:2014-04-30 summary:The core component of most modern trackers is a discriminative classifier,tasked with distinguishing between the target and the surrounding environment.To cope with natural image changes, this classifier is typically trained withtranslated and scaled sample patches. Such sets of samples are riddled withredundancies -- any overlapping pixels are constrained to be the same. Based onthis simple observation, we propose an analytic model for datasets of thousandsof translated patches. By showing that the resulting data matrix is circulant,we can diagonalize it with the Discrete Fourier Transform, reducing bothstorage and computation by several orders of magnitude. Interestingly, forlinear regression our formulation is equivalent to a correlation filter, usedby some of the fastest competitive trackers. For kernel regression, however, wederive a new Kernelized Correlation Filter (KCF), that unlike other kernelalgorithms has the exact same complexity as its linear counterpart. Building onit, we also propose a fast multi-channel extension of linear correlationfilters, via a linear kernel, which we call Dual Correlation Filter (DCF). BothKCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50videos benchmark, despite running at hundreds of frames-per-second, and beingimplemented in a few lines of code (Algorithm 1). To encourage furtherdevelopments, our tracking framework was made open-source.
arxiv-7800-50 | Distributed Low-Rank Estimation Based on Joint Iterative Optimization in Wireless Sensor Networks | http://arxiv.org/pdf/1411.1125v1.pdf | author:S. Xu, R. C. de Lamare, H. V. Poor category:cs.IT cs.LG math.IT published:2014-11-05 summary:This paper proposes a novel distributed reduced--rank scheme and an adaptivealgorithm for distributed estimation in wireless sensor networks. The proposeddistributed scheme is based on a transformation that performs dimensionalityreduction at each agent of the network followed by a reduced-dimensionparameter vector. A distributed reduced-rank joint iterative estimationalgorithm is developed, which has the ability to achieve significantly reducedcommunication overhead and improved performance when compared with existingtechniques. Simulation results illustrate the advantages of the proposedstrategy in terms of convergence rate and mean square error performance.
arxiv-7800-51 | Do Convnets Learn Correspondence? | http://arxiv.org/pdf/1411.1091v1.pdf | author:Jonathan Long, Ning Zhang, Trevor Darrell category:cs.CV cs.LG cs.NE published:2014-11-04 summary:Convolutional neural nets (convnets) trained from massive labeled datasetshave substantially improved the state-of-the-art in image classification andobject detection. However, visual understanding requires establishingcorrespondence on a finer level than object category. Given their large poolingregions and training from whole-image labels, it is not clear that convnetsderive their success from an accurate correspondence model which could be usedfor precise localization. In this paper, we study the effectiveness of convnetactivation features for tasks requiring correspondence. We present evidencethat convnet features localize at a much finer scale than their receptive fieldsizes, that they can be used to perform intraclass alignment as well asconventional hand-engineered features, and that they outperform conventionalfeatures in keypoint prediction on objects from PASCAL VOC 2011.
arxiv-7800-52 | Expectation-Maximization for Learning Determinantal Point Processes | http://arxiv.org/pdf/1411.1088v1.pdf | author:Jennifer Gillenwater, Alex Kulesza, Emily Fox, Ben Taskar category:stat.ML cs.LG published:2014-11-04 summary:A determinantal point process (DPP) is a probabilistic model of set diversitycompactly parameterized by a positive semi-definite kernel matrix. To fit a DPPto a given task, we would like to learn the entries of its kernel matrix bymaximizing the log-likelihood of the available data. However, log-likelihood isnon-convex in the entries of the kernel matrix, and this learning problem isconjectured to be NP-hard. Thus, previous work has instead focused on morerestricted convex learning settings: learning only a single weight for each rowof the kernel matrix, or learning weights for a linear combination of DPPs withfixed kernel matrices. In this work we propose a novel algorithm for learningthe full kernel matrix. By changing the kernel parameterization from matrixentries to eigenvalues and eigenvectors, and then lower-bounding the likelihoodin the manner of expectation-maximization algorithms, we obtain an effectiveoptimization procedure. We test our method on a real-world productrecommendation task, and achieve relative gains of up to 16.5% in testlog-likelihood compared to the naive approach of maximizing likelihood byprojected gradient ascent on the entries of the kernel matrix.
arxiv-7800-53 | Fast Exact Matrix Completion with Finite Samples | http://arxiv.org/pdf/1411.1087v1.pdf | author:Prateek Jain, Praneeth Netrapalli category:cs.NA cs.DS cs.IT cs.LG math.IT stat.ML published:2014-11-04 summary:Matrix completion is the problem of recovering a low rank matrix by observinga small fraction of its entries. A series of recent works [KOM12,JNS13,HW14]have proposed fast non-convex optimization based iterative algorithms to solvethis problem. However, the sample complexity in all these results issub-optimal in its dependence on the rank, condition number and the desiredaccuracy. In this paper, we present a fast iterative algorithm that solves the matrixcompletion problem by observing $O(nr^5 \log^3 n)$ entries, which isindependent of the condition number and the desired accuracy. The run time ofour algorithm is $O(nr^7\log^3 n\log 1/\epsilon)$ which is near linear in thedimension of the matrix. To the best of our knowledge, this is the first nearlinear time algorithm for exact matrix completion with finite sample complexity(i.e. independent of $\epsilon$). Our algorithm is based on a well known projected gradient descent method,where the projection is onto the (non-convex) set of low rank matrices. Thereare two key ideas in our result: 1) our argument is based on a $\ell_{\infty}$norm potential function (as opposed to the spectral norm) and provides a novelway to obtain perturbation bounds for it. 2) we prove and use a naturalextension of the Davis-Kahan theorem to obtain perturbation bounds on the bestlow rank approximation of matrices with good eigen-gap. Both of these ideas maybe of independent interest.
arxiv-7800-54 | A statistical model for tensor PCA | http://arxiv.org/pdf/1411.1076v1.pdf | author:Andrea Montanari, Emile Richard category:cs.LG cs.IT math.IT stat.ML published:2014-11-04 summary:We consider the Principal Component Analysis problem for large tensors ofarbitrary order $k$ under a single-spike (or rank-one plus noise) model. On theone hand, we use information theory, and recent results in probability theory,to establish necessary and sufficient conditions under which the principalcomponent can be estimated using unbounded computational resources. It turnsout that this is possible as soon as the signal-to-noise ratio $\beta$ becomeslarger than $C\sqrt{k\log k}$ (and in particular $\beta$ can remain bounded asthe problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms,based on tensor unfolding, power iteration and message passing ideas fromgraphical models. We show that, unless the signal-to-noise ratio diverges inthe system dimensions, none of these approaches succeeds. This is possiblyrelated to a fundamental limitation of computationally tractable estimators forthis problem. We discuss various initializations for tensor power iteration, and show thata tractable initialization based on the spectrum of the matricized tensoroutperforms significantly baseline methods, statistically and computationally.Finally, we consider the case in which additional side information is availableabout the unknown signal. We characterize the amount of side information thatallows the iterative algorithms to converge to a good estimate.
arxiv-7800-55 | Randomized Dimensionality Reduction for k-means Clustering | http://arxiv.org/pdf/1110.2897v3.pdf | author:Christos Boutsidis, Anastasios Zouzias, Michael W. Mahoney, Petros Drineas category:cs.DS cs.LG published:2011-10-13 summary:We study the topic of dimensionality reduction for $k$-means clustering.Dimensionality reduction encompasses the union of two approaches: \emph{featureselection} and \emph{feature extraction}. A feature selection based algorithmfor $k$-means clustering selects a small subset of the input features and thenapplies $k$-means clustering on the selected features. A feature extractionbased algorithm for $k$-means clustering constructs a small set of newartificial features and then applies $k$-means clustering on the constructedfeatures. Despite the significance of $k$-means clustering as well as thewealth of heuristic methods addressing it, provably accurate feature selectionmethods for $k$-means clustering are not known. On the other hand, two provablyaccurate feature extraction methods for $k$-means clustering are known in theliterature; one is based on random projections and the other is based on thesingular value decomposition (SVD). This paper makes further progress towards a better understanding ofdimensionality reduction for $k$-means clustering. Namely, we present the firstprovably accurate feature selection method for $k$-means clustering and, inaddition, we present two feature extraction methods. The first featureextraction method is based on random projections and it improves upon theexisting results in terms of time complexity and number of features needed tobe extracted. The second feature extraction method is based on fast approximateSVD factorizations and it also improves upon the existing results in terms oftime complexity. The proposed algorithms are randomized and provideconstant-factor approximation guarantees with respect to the optimal $k$-meansobjective value.
arxiv-7800-56 | Iterated geometric harmonics for data imputation and reconstruction of missing data | http://arxiv.org/pdf/1411.0997v1.pdf | author:Chad Eckman, Jonathan A. Lindgren, Erin P. J. Pearse, David J. Sacco, Zachariah Zhang category:cs.LG stat.ML published:2014-11-04 summary:The method of geometric harmonics is adapted to the situation of incompletedata by means of the iterated geometric harmonics (IGH) scheme. The method istested on natural and synthetic data sets with 50--500 data points anddimensionality of 400--10,000. Experiments suggest that the algorithm convergesto a near optimal solution within 4--6 iterations, at runtimes of less than 30minutes on a medium-grade desktop computer. The imputation of missing datavalues is applied to collections of damaged images (suffering from dataannihilation rates of up to 70\%) which are reconstructed with a surprisingdegree of accuracy.
arxiv-7800-57 | Complexity theoretic limitations on learning DNF's | http://arxiv.org/pdf/1404.3378v2.pdf | author:Amit Daniely, Shai Shalev-Shwatz category:cs.LG cs.CC published:2014-04-13 summary:Using the recently developed framework of [Daniely et al, 2014], we show thatunder a natural assumption on the complexity of refuting random K-SAT formulas,learning DNF formulas is hard. Furthermore, the same assumption implies thehardness of learning intersections of $\omega(\log(n))$ halfspaces,agnostically learning conjunctions, as well as virtually all (distributionfree) learning problems that were previously shown hard (under complexityassumptions).
arxiv-7800-58 | Articulated Pose Estimation by a Graphical Model with Image Dependent Pairwise Relations | http://arxiv.org/pdf/1407.3399v2.pdf | author:Xianjie Chen, Alan Yuille category:cs.CV published:2014-07-12 summary:We present a method for estimating articulated human pose from a singlestatic image based on a graphical model with novel pairwise relations that makeadaptive use of local image measurements. More precisely, we specify agraphical model for human pose which exploits the fact the local imagemeasurements can be used both to detect parts (or joints) and also to predictthe spatial relationships between them (Image Dependent Pairwise Relations).These spatial relationships are represented by a mixture model. We use DeepConvolutional Neural Networks (DCNNs) to learn conditional probabilities forthe presence of parts and their spatial relationships within image patches.Hence our model combines the representational flexibility of graphical modelswith the efficiency and statistical power of DCNNs. Our method significantlyoutperforms the state of the art methods on the LSP and FLIC datasets and alsoperforms very well on the Buffy dataset without any training.
arxiv-7800-59 | Convex Optimization for Big Data | http://arxiv.org/pdf/1411.0972v1.pdf | author:Volkan Cevher, Stephen Becker, Mark Schmidt category:math.OC cs.LG stat.ML published:2014-11-04 summary:This article reviews recent advances in convex optimization algorithms forBig Data, which aim to reduce the computational, storage, and communicationsbottlenecks. We provide an overview of this emerging field, describecontemporary approximation techniques like first-order methods andrandomization for scalability, and survey the important role of parallel anddistributed computation. The new Big Data algorithms are based on surprisinglysimple principles and attain staggering accelerations even on classicalproblems.
arxiv-7800-60 | Simple approximate MAP Inference for Dirichlet processes | http://arxiv.org/pdf/1411.0939v1.pdf | author:Yordan P. Raykov, Alexis Boukouvalas, Max A. Little category:stat.ML published:2014-11-04 summary:The Dirichlet process mixture (DPM) is a ubiquitous, flexible Bayesiannonparametric statistical model. However, full probabilistic inference in thismodel is analytically intractable, so that computationally intensive techniquessuch as Gibb's sampling are required. As a result, DPM-based methods, whichhave considerable potential, are restricted to applications in whichcomputational resources and time for inference is plentiful. For example, theywould not be practical for digital signal processing on embedded hardware,where computational resources are at a serious premium. Here, we developsimplified yet statistically rigorous approximate maximum a-posteriori (MAP)inference algorithms for DPMs. This algorithm is as simple as K-meansclustering, performs in experiments as well as Gibb's sampling, while requiringonly a fraction of the computational effort. Unlike related small varianceasymptotics, our algorithm is non-degenerate and so inherits the "rich getricher" property of the Dirichlet process. It also retains a non-degenerateclosed-form likelihood which enables standard tools such as cross-validation tobe used. This is a well-posed approximation to the MAP solution of theprobabilistic DPM model.
arxiv-7800-61 | Best-Arm Identification in Linear Bandits | http://arxiv.org/pdf/1409.6110v2.pdf | author:Marta Soare, Alessandro Lazaric, Rémi Munos category:cs.LG published:2014-09-22 summary:We study the best-arm identification problem in linear bandit, where therewards of the arms depend linearly on an unknown parameter $\theta^*$ and theobjective is to return the arm with the largest reward. We characterize thecomplexity of the problem and introduce sample allocation strategies that pullarms to identify the best arm with a fixed confidence, while minimizing thesample budget. In particular, we show the importance of exploiting the globallinear structure to improve the estimate of the reward of near-optimal arms. Weanalyze the proposed strategies and compare their empirical performance.Finally, as a by-product of our analysis, we point out the connection to the$G$-optimality criterion used in optimal experimental design.
arxiv-7800-62 | Scaling laws and fluctuations in the statistics of word frequencies | http://arxiv.org/pdf/1406.4441v2.pdf | author:Martin Gerlach, Eduardo G. Altmann category:physics.soc-ph cs.CL published:2014-06-17 summary:In this paper we combine statistical analysis of large text databases andsimple stochastic models to explain the appearance of scaling laws in thestatistics of word frequencies. Besides the sublinear scaling of the vocabularysize with database size (Heaps' law), here we report a new scaling of thefluctuations around this average (fluctuation scaling analysis). We explainboth scaling laws by modeling the usage of words by simple stochastic processesin which the overall distribution of word-frequencies is fat tailed (Zipf'slaw) and the frequency of a single word is subject to fluctuations acrossdocuments (as in topic models). In this framework, the mean and the variance ofthe vocabulary size can be expressed as quenched averages, implying that: i)the inhomogeneous dissemination of words cause a reduction of the averagevocabulary size in comparison to the homogeneous case, and ii) correlations inthe co-occurrence of words lead to an increase in the variance and thevocabulary size becomes a non-self-averaging quantity. We address theimplications of these observations to the measurement of lexical richness. Wetest our results in three large text databases (Google-ngram, EnlgishWikipedia, and a collection of scientific articles).
arxiv-7800-63 | Kernel Mean Estimation via Spectral Filtering | http://arxiv.org/pdf/1411.0900v1.pdf | author:Krikamol Muandet, Bharath Sriperumbudur, Bernhard Schölkopf category:stat.ML math.ST stat.TH published:2014-11-04 summary:The problem of estimating the kernel mean in a reproducing kernel Hilbertspace (RKHS) is central to kernel methods in that it is used by classicalapproaches (e.g., when centering a kernel PCA matrix), and it also forms thecore inference step of modern kernel methods (e.g., kernel-based non-parametrictests) that rely on embedding probability distributions in RKHSs. Muandet etal. (2014) has shown that shrinkage can help in constructing "better"estimators of the kernel mean than the empirical estimator. The present paperstudies the consistency and admissibility of the estimators in Muandet et al.(2014), and proposes a wider class of shrinkage estimators that improve uponthe empirical estimator by considering appropriate basis functions. Using thekernel PCA basis, we show that some of these estimators can be constructedusing spectral filtering algorithms which are shown to be consistent under sometechnical assumptions. Our theoretical analysis also reveals a fundamentalconnection to the kernel-based supervised learning framework. The proposedestimators are simple to implement and perform well in practice.
arxiv-7800-64 | Tied Probabilistic Linear Discriminant Analysis for Speech Recognition | http://arxiv.org/pdf/1411.0895v1.pdf | author:Liang Lu, Steve Renals category:cs.CL cs.AI published:2014-11-04 summary:Acoustic models using probabilistic linear discriminant analysis (PLDA)capture the correlations within feature vectors using subspaces which do notvastly expand the model. This allows high dimensional and correlated featurespaces to be used, without requiring the estimation of multiple high dimensioncovariance matrices. In this letter we extend the recently presented PLDAmixture model for speech recognition through a tied PLDA approach, which isbetter able to control the model size to avoid overfitting. We carried outexperiments using the Switchboard corpus, with both mel frequency cepstralcoefficient features and bottleneck feature derived from a deep neural network.Reductions in word error rate were obtained by using tied PLDA, compared withthe PLDA mixture model, subspace Gaussian mixture models, and deep neuralnetworks.
arxiv-7800-65 | Using Linguistic Features to Estimate Suicide Probability of Chinese Microblog Users | http://arxiv.org/pdf/1411.0861v1.pdf | author:Lei Zhang, Xiaolei Huang, Tianli Liu, Zhenxiang Chen, Tingshao Zhu category:cs.SI cs.CL published:2014-11-04 summary:If people with high risk of suicide can be identified through social medialike microblog, it is possible to implement an active intervention system tosave their lives. Based on this motivation, the current study administered theSuicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is aleading microblog service provider in China. Two NLP (Natural LanguageProcessing) methods, the Chinese edition of Linguistic Inquiry and Word Count(LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extractlinguistic features from the Sina Weibo data. We trained predicting models bymachine learning algorithm based on these two types of features, to estimatesuicide probability based on linguistic features. The experiment resultsindicate that LDA can find topics that relate to suicide probability, andimprove the performance of prediction. Our study adds value in prediction ofsuicidal probability of social network users with their behaviors.
arxiv-7800-66 | CUR Algorithm for Partially Observed Matrices | http://arxiv.org/pdf/1411.0860v1.pdf | author:Miao Xu, Rong Jin, Zhi-Hua Zhou category:cs.LG published:2014-11-04 summary:CUR matrix decomposition computes the low rank approximation of a givenmatrix by using the actual rows and columns of the matrix. It has been a veryuseful tool for handling large matrices. One limitation with the existingalgorithms for CUR matrix decomposition is that they need an access to the {\itfull} matrix, a requirement that can be difficult to fulfill in many real worldapplications. In this work, we alleviate this limitation by developing a CURdecomposition algorithm for partially observed matrices. In particular, theproposed algorithm computes the low rank approximation of the target matrixbased on (i) the randomly sampled rows and columns, and (ii) a subset ofobserved entries that are randomly sampled from the matrix. Our analysis showsthe relative error bound, measured by spectral norm, for the proposed algorithmwhen the target matrix is of full rank. We also show that only $O(n r\ln r)$observed entries are needed by the proposed algorithm to perfectly recover arank $r$ matrix of size $n\times n$, which improves the sample complexity ofthe existing algorithms for matrix completion. Empirical studies on bothsynthetic and real-world datasets verify our theoretical claims and demonstratethe effectiveness of the proposed algorithm.
arxiv-7800-67 | A random algorithm for low-rank decomposition of large-scale matrices with missing entries | http://arxiv.org/pdf/1411.0814v1.pdf | author:Yiguang Liu category:cs.NA cs.CV published:2014-11-04 summary:A Random SubMatrix method (RSM) is proposed to calculate the low-rankdecomposition of large-scale matrices with known entry percentage \rho. RSM isvery fast as the floating-point operations (flops) required are comparedfavorably with the state-of-the-art algorithms. Meanwhile RSM is verymemory-saving. With known entries homogeneously distributed in the givenmatrix, sub-matrices formed by known entries are randomly selected. Accordingto the just proved theorem that subspace related to smaller singular values isless perturbed by noise, the null vectors or the right singular vectorsassociated with the minor singular values are calculated for each submatrix.The vectors are the null vectors of the corresponding submatrix in the groundtruth of the given large-scale matrix. If enough sub-matrices are randomlychosen, the low-rank decomposition is estimated. The experimental results onrandom synthetical matrices with sizes such as 131072X1024 and on real datasets indicate that RSM is much faster and memory-saving, and, meanwhile, hasconsiderable high precision achieving or approximating to the best.
arxiv-7800-68 | Simultaneous Localization, Mapping, and Manipulation for Unsupervised Object Discovery | http://arxiv.org/pdf/1411.0802v1.pdf | author:Lu Ma, Mahsa Ghafarianzadeh, Dave Coleman, Nikolaus Correll, Gabe Sibley category:cs.RO cs.CV published:2014-11-04 summary:We present an unsupervised framework for simultaneous appearance-based objectdiscovery, detection, tracking and reconstruction using RGBD cameras and arobot manipulator. The system performs dense 3D simultaneous localization andmapping concurrently with unsupervised object discovery. Putative objects thatare spatially and visually coherent are manipulated by the robot to gainadditional motion-cues. The robot uses appearance alone, followed by structureand motion cues, to jointly discover, verify, learn and improve models ofobjects. Induced motion segmentation reinforces learned models which arerepresented implicitly as 2D and 3D level sets to capture both shape andappearance. We compare three different approaches for appearance-based objectdiscovery and find that a novel form of spatio-temporal super-pixels gives thehighest quality candidate object models in terms of precision and recall. Liveexperiments with a Baxter robot demonstrate a holistic pipeline capable ofautomatic discovery, verification, detection, tracking and reconstruction ofunknown objects.
arxiv-7800-69 | A Robust Point Sets Matching Method | http://arxiv.org/pdf/1411.0791v1.pdf | author:Xiao Liu, Congying Han, Tiande Guo category:cs.CV published:2014-11-04 summary:Point sets matching method is very important in computer vision, featureextraction, fingerprint matching, motion estimation and so on. This paperproposes a robust point sets matching method. We present an iterative algorithmthat is robust to noise case. Firstly, we calculate all transformations betweentwo points. Then similarity matrix are computed to measure the possibility thattwo transformation are both true. We iteratively update the matching scorematrix by using the similarity matrix. By using matching algorithm on graph, weobtain the matching result. Experimental results obtained by our approach showrobustness to outlier and jitter.
arxiv-7800-70 | The Information Theoretically Efficient Model (ITEM): A model for computerized analysis of large datasets | http://arxiv.org/pdf/1409.6075v3.pdf | author:Tyler Ward category:cs.LG published:2014-09-22 summary:This document discusses the Information Theoretically Efficient Model (ITEM),a computerized system to generate an information theoretically efficientmultinomial logistic regression from a general dataset. More specifically, thismodel is designed to succeed even where the logit transform of the dependentvariable is not necessarily linear in the independent variables. This researchshows that for large datasets, the resulting models can be produced on moderncomputers in a tractable amount of time. These models are also resistant tooverfitting, and as such they tend to produce interpretable models with only alimited number of features, all of which are designed to be well behaved.
arxiv-7800-71 | A provable SVD-based algorithm for learning topics in dominant admixture corpus | http://arxiv.org/pdf/1410.6991v3.pdf | author:Trapit Bansal, Chiranjib Bhattacharyya, Ravindran Kannan category:stat.ML cs.LG published:2014-10-26 summary:Topic models, such as Latent Dirichlet Allocation (LDA), posit that documentsare drawn from admixtures of distributions over words, known as topics. Theinference problem of recovering topics from admixtures, is NP-hard. Assumingseparability, a strong assumption, [4] gave the first provable algorithm forinference. For LDA model, [6] gave a provable algorithm using tensor-methods.But [4,6] do not learn topic vectors with bounded $l_1$ error (a naturalmeasure for probability vectors). Our aim is to develop a model which makesintuitive and empirically supported assumptions and to design an algorithm withnatural, simple components such as SVD, which provably solves the inferenceproblem for the model with bounded $l_1$ error. A topic in LDA and other modelsis essentially characterized by a group of co-occurring words. Motivated bythis, we introduce topic specific Catchwords, group of words which occur withstrictly greater frequency in a topic than any other topic individually and arerequired to have high frequency together rather than individually. A majorcontribution of the paper is to show that under this more realistic assumption,which is empirically verified on real corpora, a singular value decomposition(SVD) based algorithm with a crucial pre-processing step of thresholding, canprovably recover the topics from a collection of documents drawn from Dominantadmixtures. Dominant admixtures are convex combination of distributions inwhich one distribution has a significantly higher contribution than others.Apart from the simplicity of the algorithm, the sample complexity has nearoptimal dependence on $w_0$, the lowest probability that a topic is dominant,and is better than [4]. Empirical evidence shows that on several real worldcorpora, both Catchwords and Dominant admixture assumptions hold and theproposed algorithm substantially outperforms the state of the art [5].
arxiv-7800-72 | Detecting Suicidal Ideation in Chinese Microblogs with Psychological Lexicons | http://arxiv.org/pdf/1411.0778v1.pdf | author:Xiaolei Huang, Lei Zhang, Tianli Liu, David Chiu, Tingshao Zhu, Xin Li category:cs.CL published:2014-11-04 summary:Suicide is among the leading causes of death in China. However, technicalapproaches toward preventing suicide are challenging and remaining underdevelopment. Recently, several actual suicidal cases were preceded by users whoposted microblogs with suicidal ideation to Sina Weibo, a Chinese social medianetwork akin to Twitter. It would therefore be desirable to detect suicidalideations from microblogs in real-time, and immediately alert appropriatesupport groups, which may lead to successful prevention. In this paper, wepropose a real-time suicidal ideation detection system deployed over Weibo,using machine learning and known psychological techniques. Currently, we haveidentified 53 known suicidal cases who posted suicide notes on Weibo prior totheir deaths.We explore linguistic features of these known cases using apsychological lexicon dictionary, and train an effective suicidal Weibo postdetection model. 6714 tagged posts and several classifiers are used to verifythe model. By combining both machine learning and psychological knowledge, SVMclassifier has the best performance of different classifiers, yielding anF-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.
arxiv-7800-73 | Local Decorrelation For Improved Detection | http://arxiv.org/pdf/1406.1134v2.pdf | author:Woonhyun Nam, Piotr Dollár, Joon Hee Han category:cs.CV published:2014-06-04 summary:Even with the advent of more sophisticated, data-hungry methods, boosteddecision trees remain extraordinarily successful for fast rigid objectdetection, achieving top accuracy on numerous datasets. While effective, mostboosted detectors use decision trees with orthogonal (single feature) splits,and the topology of the resulting decision boundary may not be well matched tothe natural topology of the data. Given highly correlated data, decision treeswith oblique (multiple feature) splits can be effective. Use of oblique splits,however, comes at considerable computational expense. Inspired by recent workon discriminative decorrelation of HOG features, we instead propose anefficient feature transform that removes correlations in local neighborhoods.The result is an overcomplete but locally decorrelated representation ideallysuited for use with orthogonal decision trees. In fact, orthogonal trees withour locally decorrelated features outperform oblique trees trained over theoriginal features at a fraction of the computational cost. The overallimprovement in accuracy is dramatic: on the Caltech Pedestrian Dataset, wereduce false positives nearly tenfold over the previous state-of-the-art.
arxiv-7800-74 | A Weighted Common Subgraph Matching Algorithm | http://arxiv.org/pdf/1411.0763v1.pdf | author:Xu Yang, Hong Qiao, Zhi-Yong Liu category:cs.DS cs.CV published:2014-11-04 summary:We propose a weighted common subgraph (WCS) matching algorithm to find themost similar subgraphs in two labeled weighted graphs. WCS matching, as anatural generalization of the equal-sized graph matching or subgraph matching,finds wide applications in many computer vision and machine learning tasks. Inthis paper, the WCS matching is first formulated as a combinatorialoptimization problem over the set of partial permutation matrices. Then it isapproximately solved by a recently proposed combinatorial optimizationframework - Graduated NonConvexity and Concavity Procedure (GNCCP).Experimental comparisons on both synthetic graphs and real world imagesvalidate its robustness against noise level, problem size, outlier number, andedge density.
arxiv-7800-75 | Universal Algorithm for Online Trading Based on the Method of Calibration | http://arxiv.org/pdf/1205.3767v3.pdf | author:Vladimir V'yugin, Vladimir Trunov category:cs.LG q-fin.PM published:2012-05-16 summary:We present a universal algorithm for online trading in Stock Market whichperforms asymptotically at least as good as any stationary trading strategythat computes the investment at each step using a fixed function of the sideinformation that belongs to a given RKHS (Reproducing Kernel Hilbert Space).Using a universal kernel, we extend this result for any continuous stationarystrategy. In this learning process, a trader rationally chooses his gamblesusing predictions made by a randomized well-calibrated algorithm. Our strategyis based on Dawid's notion of calibration with more general checking rules andon some modification of Kakade and Foster's randomized rounding algorithm forcomputing the well-calibrated forecasts. We combine the method of randomizedcalibration with Vovk's method of defensive forecasting in RKHS. Unlike thestatistical theory, no stochastic assumptions are made about the stock prices.Our empirical results on historical markets provide strong evidence that thistype of technical trading can "beat the market" if transaction costs areignored.
arxiv-7800-76 | A Learning Scheme for Approachability in MDPs and Stackelberg Stochastic Games | http://arxiv.org/pdf/1411.0728v1.pdf | author:Dileep Kalathil, Vivek Borkar, Rahul Jain category:cs.LG cs.GT cs.SY math.OC published:2014-11-03 summary:The notion of approachability was introduced by Blackwell in the context ofvector-valued repeated games. The famous approachability theorem prescribes astrategy for approachability, i.e., for `steering' the average vector-cost of agiven player towards a given target set, irrespective of the strategies of theother players. In this paper, motivated from the multi-objectiveoptimization/decision making problems in dynamically changing environments, weaddress the approachability problem in Markov Decision Processes (MDPs) andStackelberg stochastic games with vector-valued cost functions. We make twomain contributions. Firstly, we give simple and computationally tractablestrategy for approachability for MDPs and Stackelberg stochastic games.Secondly, we give reinforcement learning algorithms to learn the approachablestrategy when the transition kernel is unknown. We also show that theconditions that we give for approachability are both necessary and sufficientfor convex sets and thus a complete characterization. We also give sufficientconditions for non-convex sets.
arxiv-7800-77 | A Nonparametric Adaptive Nonlinear Statistical Filter | http://arxiv.org/pdf/1411.0707v1.pdf | author:Michael Busch, Jeff Moehlis category:stat.ML published:2014-11-03 summary:We use statistical learning methods to construct an adaptive state estimatorfor nonlinear stochastic systems. Optimal state estimation, in the form of aKalman filter, requires knowledge of the system's process and measurementuncertainty. We propose that these uncertainties can be estimated from(conditioned on) past observed data, and without making any assumptions of thesystem's prior distribution. The system's prior distribution at each time stepis constructed from an ensemble of least-squares estimates on sub-sampled setsof the data via jackknife sampling. As new data is acquired, the stateestimates, process uncertainty, and measurement uncertainty are updatedaccordingly, as described in this manuscript.
arxiv-7800-78 | Clustering memes in social media streams | http://arxiv.org/pdf/1411.0652v1.pdf | author:Mohsen JafariAsbagh, Emilio Ferrara, Onur Varol, Filippo Menczer, Alessandro Flammini category:cs.SI cs.CY cs.LG physics.soc-ph published:2014-11-03 summary:The problem of clustering content in social media has pervasive applications,including the identification of discussion topics, event detection, and contentrecommendation. Here we describe a streaming framework for online detection andclustering of memes in social media, specifically Twitter. A pre-clusteringprocedure, namely protomeme detection, first isolates atomic tokens ofinformation carried by the tweets. Protomemes are thereafter aggregated, basedon multiple similarity measures, to obtain memes as cohesive groups of tweetsreflecting actual concepts or topics of discussion. The clustering algorithmtakes into account various dimensions of the data and metadata, includingnatural language, the social network, and the patterns of informationdiffusion. As a result, our system can build clusters of semantically,structurally, and topically related tweets. The clustering process is based ona variant of Online K-means that incorporates a memory mechanism, used to"forget" old memes and replace them over time with the new ones. The evaluationof our framework is carried out by using a dataset of Twitter trending topics.Over a one-week period, we systematically determined whether our algorithm wasable to recover the trending hashtags. We show that the proposed methodoutperforms baseline algorithms that only use content features, as well as astate-of-the-art event detection method that assumes full knowledge of theunderlying follower network. We finally show that our online learning frameworkis flexible, due to its independence of the adopted clustering algorithm, andbest suited to work in a streaming scenario.
arxiv-7800-79 | Active Inference for Binary Symmetric Hidden Markov Models | http://arxiv.org/pdf/1411.0630v1.pdf | author:Armen E. Allahverdyan, Aram Galstyan category:stat.ML cs.IT cs.LG math.IT published:2014-11-03 summary:We consider active maximum a posteriori (MAP) inference problem for HiddenMarkov Models (HMM), where, given an initial MAP estimate of the hiddensequence, we select to label certain states in the sequence to improve theestimation accuracy of the remaining states. We develop an analytical approachto this problem for the case of binary symmetric HMMs, and obtain a closed formsolution that relates the expected error reduction to model parameters underthe specified active inference scheme. We then use this solution to determinemost optimal active inference scheme in terms of error reduction, and examinethe relation of those schemes to heuristic principles of uncertainty reductionand solution unicity.
arxiv-7800-80 | Factorbird - a Parameter Server Approach to Distributed Matrix Factorization | http://arxiv.org/pdf/1411.0602v1.pdf | author:Sebastian Schelter, Venu Satuluri, Reza Zadeh category:cs.LG published:2014-11-03 summary:We present Factorbird, a prototype of a parameter server approach forfactorizing large matrices with Stochastic Gradient Descent-based algorithms.We designed Factorbird to meet the following desiderata: (a) scalability totall and wide matrices with dozens of billions of non-zeros, (b) extensibilityto different kinds of models and loss functions as long as they can beoptimized using Stochastic Gradient Descent (SGD), and (c) adaptability to bothbatch and streaming scenarios. Factorbird uses a parameter server in order toscale to models that exceed the memory of an individual machine, and employslock-free Hogwild!-style learning with a special partitioning scheme todrastically reduce conflicting updates. We also discuss other aspects of thedesign of our system such as how to efficiently grid search for hyperparametersat scale. We present experiments of Factorbird on a matrix built from a subsetof Twitter's interaction graph, consisting of more than 38 billion non-zerosand about 200 million rows and columns, which is to the best of our knowledgethe largest matrix on which factorization results have been reported in theliterature.
arxiv-7800-81 | Bayesian feature selection with strongly-regularizing priors maps to the Ising Model | http://arxiv.org/pdf/1411.0591v1.pdf | author:Charles K. Fisher, Pankaj Mehta category:cs.LG stat.ML published:2014-11-03 summary:Identifying small subsets of features that are relevant for prediction and/orclassification tasks is a central problem in machine learning and statistics.The feature selection task is especially important, and computationallydifficult, for modern datasets where the number of features can be comparableto, or even exceed, the number of samples. Here, we show that feature selectionwith Bayesian inference takes a universal form and reduces to calculating themagnetizations of an Ising model, under some mild conditions. Our resultsexploit the observation that the evidence takes a universal form forstrongly-regularizing priors --- priors that have a large effect on theposterior probability even in the infinite data limit. We derive explicitexpressions for feature selection for generalized linear models, a large classof statistical techniques that include linear and logistic regression. Weillustrate the power of our approach by analyzing feature selection in alogistic regression-based classifier trained to distinguish between the lettersB and D in the notMNIST dataset.
arxiv-7800-82 | On Detecting Noun-Adjective Agreement Errors in Bulgarian Language Using GATE | http://arxiv.org/pdf/1411.0588v1.pdf | author:Nadezhda Borisova, Grigor Iliev, Elena Karashtranova category:cs.CL published:2014-11-03 summary:In this article, we describe an approach for automatic detection ofnoun-adjective agreement errors in Bulgarian texts by explaining the necessarysteps required to develop a simple Java-based language processing application.For this purpose, we use the GATE language processing framework, which iscapable of analyzing texts in Bulgarian language and can be embedded insoftware applications, accessed through a set of Java APIs. In our exampleapplication we also demonstrate how to use the functionality of GATE to performregular expressions over annotations for detecting agreement errors in simplenoun phrases formed by two words - attributive adjective and a noun, where theattributive adjective precedes the noun. The provided code samples can also beused as a starting point for implementing natural language processingfunctionalities in software applications related to language processing taskslike detection, annotation and retrieval of word groups meeting a specific setof criteria.
arxiv-7800-83 | Affective Facial Expression Processing via Simulation: A Probabilistic Model | http://arxiv.org/pdf/1411.0582v1.pdf | author:Jonathan Vitale, Mary-Anne Williams, Benjamin Johnston, Giuseppe Boccignone category:cs.CV published:2014-11-03 summary:Understanding the mental state of other people is an important skill forintelligent agents and robots to operate within social environments. However,the mental processes involved in `mind-reading' are complex. One explanation ofsuch processes is Simulation Theory - it is supported by a large body ofneuropsychological research. Yet, determining the best computational model ortheory to use in simulation-style emotion detection, is far from beingunderstood. In this work, we use Simulation Theory and neuroscience findings onMirror-Neuron Systems as the basis for a novel computational model, as a way tohandle affective facial expressions. The model is based on a probabilisticmapping of observations from multiple identities onto a single fixed identity(`internal transcoding of external stimuli'), and then onto a latent space(`phenomenological response'). Together with the proposed architecture wepresent some promising preliminary results
arxiv-7800-84 | Efficient Implementations of the Generalized Lasso Dual Path Algorithm | http://arxiv.org/pdf/1405.3222v2.pdf | author:Taylor Arnold, Ryan Tibshirani category:stat.CO cs.LG stat.ML published:2014-05-13 summary:We consider efficient implementations of the generalized lasso dual pathalgorithm of Tibshirani and Taylor (2011). We first describe a generic approachthat covers any penalty matrix D and any (full column rank) matrix X ofpredictor variables. We then describe fast implementations for the specialcases of trend filtering problems, fused lasso problems, and sparse fused lassoproblems, both with X=I and a general matrix X. These specializedimplementations offer a considerable improvement over the genericimplementation, both in terms of numerical stability and efficiency of thesolution path computation. These algorithms are all available for use in thegenlasso R package, which can be found in the CRAN repository.
arxiv-7800-85 | Personalized News Recommendation with Context Trees | http://arxiv.org/pdf/1303.0665v2.pdf | author:Florent Garcin, Christos Dimitrakakis, Boi Faltings category:cs.IR cs.LG stat.ML published:2013-03-04 summary:The profusion of online news articles makes it difficult to find interestingarticles, a problem that can be assuaged by using a recommender system to bringthe most relevant news stories to readers. However, news recommendation ischallenging because the most relevant articles are often new content seen byfew users. In addition, they are subject to trends and preference changes overtime, and in many cases we do not have sufficient information to profile thereader. In this paper, we introduce a class of news recommendation systems based oncontext trees. They can provide high-quality news recommendation to anonymousvisitors based on present browsing behaviour. We show that context-treerecommender systems provide good prediction accuracy and recommendationnovelty, and they are sufficiently flexible to capture the unique properties ofnews articles.
arxiv-7800-86 | Distributed Submodular Maximization | http://arxiv.org/pdf/1411.0541v1.pdf | author:Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, Andreas Krause category:cs.LG cs.AI cs.DC cs.IR published:2014-11-03 summary:Many large-scale machine learning problems -- clustering, non-parametriclearning, kernel machines, etc. -- require selecting a small yet representativesubset from a large dataset. Such problems can often be reduced to maximizing asubmodular set function subject to various constraints. Classical approaches tosubmodular optimization require centralized access to the full dataset, whichis impractical for truly large-scale problems. In this paper, we consider theproblem of submodular function maximization in a distributed fashion. Wedevelop a simple, two-stage protocol GreeDi, that is easily implemented usingMapReduce style computations. We theoretically analyze our approach, and showthat under certain natural conditions, performance close to the centralizedapproach can be achieved. We begin with monotone submodular maximizationsubject to a cardinality constraint, and then extend this approach to obtainapproximation guarantees for (not necessarily monotone) submodular maximizationsubject to more general constraints including matroid or knapsackconstraints.In our extensive experiments, we demonstrate the effectiveness ofour approach on several applications, including sparse Gaussian processinference and exemplar based clustering on tens of millions of examples usingHadoop.
arxiv-7800-87 | Unsupervised Deep Haar Scattering on Graphs | http://arxiv.org/pdf/1406.2390v2.pdf | author:Xu Chen, Xiuyuan Cheng, Stéphane Mallat category:cs.LG cs.CV published:2014-06-09 summary:The classification of high-dimensional data defined on graphs is particularlydifficult when the graph geometry is unknown. We introduce a Haar scatteringtransform on graphs, which computes invariant signal descriptors. It isimplemented with a deep cascade of additions, subtractions and absolute values,which iteratively compute orthogonal Haar wavelet transforms. Multiscaleneighborhoods of unknown graphs are estimated by minimizing an average totalvariation, with a pair matching algorithm of polynomial complexity. Supervisedclassification with dimension reduction is tested on data bases of scrambledimages, and for signals sampled on unknown irregular grids on a sphere.
arxiv-7800-88 | Tagging Scientific Publications using Wikipedia and Natural Language Processing Tools. Comparison on the ArXiv Dataset | http://arxiv.org/pdf/1309.0326v3.pdf | author:Michał Łopuszyński, Łukasz Bolikowski category:cs.CL cs.DL published:2013-09-02 summary:In this work, we compare two simple methods of tagging scientificpublications with labels reflecting their content. As a first source of labelsWikipedia is employed, second label set is constructed from the noun phrasesoccurring in the analyzed corpus. We examine the statistical properties and theeffectiveness of both approaches on the dataset consisting of abstracts from0.7 million of scientific documents deposited in the ArXiv preprint collection.We believe that obtained tags can be later on applied as useful documentfeatures in various machine learning tasks (document similarity, clustering,topic modelling, etc.).
arxiv-7800-89 | Unsupervised Keyword Extraction from Polish Legal Texts | http://arxiv.org/pdf/1408.3731v2.pdf | author:Michał Jungiewicz, Michał Łopuszyński category:cs.CL published:2014-08-16 summary:In this work, we present an application of the recently proposed unsupervisedkeyword extraction algorithm RAKE to a corpus of Polish legal texts from thefield of public procurement. RAKE is essentially a language and domainindependent method. Its only language-specific input is a stoplist containing aset of non-content words. The performance of the method heavily depends on thechoice of such a stoplist, which should be domain adopted. Therefore, wecomplement RAKE algorithm with an automatic approach to selecting non-contentwords, which is based on the statistical properties of term distribution.
arxiv-7800-90 | Non Binary Local Gradient Contours for Face Recognition | http://arxiv.org/pdf/1411.0442v1.pdf | author:Abdullah Gubbi, Mohammad Fazle Azeem, M Sharmila Kumari category:cs.CV published:2014-11-03 summary:As the features from the traditional Local Binary Patterns (LBP) and LocalDirectional Patterns (LDP) are found to be ineffective for face recognition, wehave proposed a new approach derived on the basis of Information sets wherebythe loss of information that occurs during the binarization is eliminated. Theinformation sets expand the scope of fuzzy sets by connecting the attribute andthe corresponding membership function value as a product. Since face is havingsmooth texture in a limited area, the extracted features must be highlydiscernible. To limit the number of features, we consider only the nonoverlapping windows. By the application of the information set theory we canreduce the number of feature of an image. The derived features are shown towork fairly well over eigenface, fisherface and LBP methods.
arxiv-7800-91 | Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature | http://arxiv.org/pdf/1411.0439v1.pdf | author:Tom Gunter, Michael A. Osborne, Roman Garnett, Philipp Hennig, Stephen J. Roberts category:stat.ML published:2014-11-03 summary:We propose a novel sampling framework for inference in probabilistic models:an active learning approach that converges more quickly (in wall-clock time)than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge inprobabilistic inference is numerical integration, to average over ensembles ofmodels or unknown (hyper-)parameters (for example to compute the marginallikelihood or a partition function). MCMC has provided approaches to numericalintegration that deliver state-of-the-art inference, but can suffer from sampleinefficiency and poor convergence diagnostics. Bayesian quadrature techniquesoffer a model-based solution to such problems, but their uptake has beenhindered by prohibitive computation costs. We introduce a warped model forprobabilistic integrands (likelihoods) that are known to be non-negative,permitting a cheap active learning scheme to optimally select sample locations.Our algorithm is demonstrated to offer faster convergence (in seconds) relativeto simple Monte Carlo and annealed importance sampling on both synthetic andreal-world examples.
arxiv-7800-92 | Sparsity Constrained Graph Regularized NMF for Spectral Unmixing of Hyperspectral Data | http://arxiv.org/pdf/1411.0392v1.pdf | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2014-11-03 summary:Hyperspectral images contain mixed pixels due to low spatial resolution ofhyperspectral sensors. Mixed pixels are pixels containing more than onedistinct material called endmembers. The presence percentages of endmembers inmixed pixels are called abundance fractions. Spectral unmixing problem refersto decomposing these pixels into a set of endmembers and abundance fractions.Due to nonnegativity constraint on abundance fractions, nonnegative matrixfactorization methods (NMF) have been widely used for solving spectral unmixingproblem. In this paper we have used graph regularized NMF (GNMF) methodcombined with sparseness constraint to decompose mixed pixels in hyperspectralimagery. This method preserves the geometrical structure of data whilerepresenting it in low dimensional space. Adaptive regularization parameterbased on temperature schedule in simulated annealing method also has been usedin this paper for the sparseness term. Proposed algorithm is applied onsynthetic and real datasets. Synthetic data is generated based on endmembersfrom USGS spectral library. AVIRIS Cuprite dataset is used as real dataset forevaluation of proposed method. Results are quantified based on spectral angledistance (SAD) and abundance angle distance (AAD) measures. Results incomparison with other methods show that the proposed method can unmix data moreeffectively. Specifically for the Cuprite dataset, performance of the proposedmethod is approximately 10% better than the VCA and Sparse NMF in terms of rootmean square of SAD.
arxiv-7800-93 | Variational Gaussian Process State-Space Models | http://arxiv.org/pdf/1406.4905v2.pdf | author:Roger Frigola, Yutian Chen, Carl E. Rasmussen category:cs.LG cs.RO cs.SY stat.ML published:2014-06-18 summary:State-space models have been successfully used for more than fifty years indifferent areas of science and engineering. We present a procedure forefficient variational Bayesian learning of nonlinear state-space models basedon sparse Gaussian processes. The result of learning is a tractable posteriorover nonlinear dynamical systems. In comparison to conventional parametricmodels, we offer the possibility to straightforwardly trade off model capacityand computational cost whilst avoiding overfitting. Our main algorithm uses ahybrid inference approach combining variational Bayes and sequential MonteCarlo. We also present stochastic variational inference and online learningapproaches for fast learning with long time series.
arxiv-7800-94 | Adaptive Image Denoising by Targeted Databases | http://arxiv.org/pdf/1407.5055v3.pdf | author:Enming Luo, Stanley H. Chan, Truong Q. Nguyen category:cs.CV stat.ME published:2014-06-30 summary:We propose a data-dependent denoising procedure to restore noisy images.Different from existing denoising algorithms which search for patches fromeither the noisy image or a generic database, the new algorithm finds patchesfrom a database that contains only relevant patches. We formulate the denoisingproblem as an optimal filter design problem and make two contributions. First,we determine the basis function of the denoising filter by solving a groupsparsity minimization problem. The optimization formulation generalizesexisting denoising algorithms and offers systematic analysis of theperformance. Improvement methods are proposed to enhance the patch searchprocess. Second, we determine the spectral coefficients of the denoising filterby considering a localized Bayesian prior. The localized prior leverages thesimilarity of the targeted database, alleviates the intensive Bayesiancomputation, and links the new method to the classical linear minimum meansquared error estimation. We demonstrate applications of the proposed method ina variety of scenarios, including text images, multiview images and faceimages. Experimental results show the superiority of the new algorithm overexisting methods.
arxiv-7800-95 | Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares | http://arxiv.org/pdf/1411.0347v1.pdf | author:Mert Pilanci, Martin J. Wainwright category:math.OC cs.IT cs.LG math.IT stat.ML published:2014-11-03 summary:We study randomized sketching methods for approximately solving least-squaresproblem with a general convex constraint. The quality of a least-squaresapproximation can be assessed in different ways: either in terms of the valueof the quadratic objective function (cost approximation), or in terms of somedistance measure between the approximate minimizer and the true minimizer(solution approximation). Focusing on the latter criterion, our first mainresult provides a general lower bound on any randomized method that sketchesboth the data matrix and vector in a least-squares problem; as a surprisingconsequence, the most widely used least-squares sketch is sub-optimal forsolution approximation. We then present a new method known as the iterativeHessian sketch, and show that it can be used to obtain approximations to theoriginal least-squares problem using a projection dimension proportional to thestatistical complexity of the least-squares minimizer, and a logarithmic numberof iterations. We illustrate our general theory with simulations for bothunconstrained and constrained versions of least-squares, including$\ell_1$-regularization and nuclear norm constraints. We also numericallydemonstrate the practicality of our approach in a real face expressionclassification experiment.
arxiv-7800-96 | Machine learning for many-body physics: The case of the Anderson impurity model | http://arxiv.org/pdf/1408.1143v2.pdf | author:Louis-François Arsenault, Alejandro Lopez-Bezanilla, O. Anatole von Lilienfeld, Andrew J. Millis category:stat.ML published:2014-08-05 summary:Machine learning methods are applied to finding the Green's function of theAnderson impurity model, a basic model system of quantum many-bodycondensed-matter physics. Different methods of parametrizing the Green'sfunction are investigated; a representation in terms of Legendre polynomials isfound to be superior due to its limited number of coefficients and itsapplicability to state of the art methods of solution. The dependence of theerrors on the size of the training set is determined. The results indicate thata machine learning approach to dynamical mean-field theory may be feasible.
arxiv-7800-97 | What is usual in unusual videos? Trajectory snippet histograms for discovering unusualness | http://arxiv.org/pdf/1401.0730v2.pdf | author:Ahmet Iscen, Anil Armagan, Pinar Duygulu category:cs.CV published:2014-01-03 summary:Unusual events are important as being possible indicators of undesiredconsequences. Moreover, unusualness in everyday life activities may also beamusing to watch as proven by the popularity of such videos shared in socialmedia. Discovery of unusual events in videos is generally attacked as a problemof finding usual patterns, and then separating the ones that do not resemble tothose. In this study, we address the problem from the other side, and try toanswer what type of patterns are shared among unusual videos that make themresemble to each other regardless of the ongoing event. With this challengingproblem at hand, we propose a novel descriptor to encode the rapid motions invideos utilizing densely extracted trajectories. The proposed descriptor, whichis referred to as trajectory snipped histograms, is used to distinguish unusualvideos from usual videos, and further exploited to discover snapshots in whichunusualness happen. Experiments on domain specific people falling videos andunrestricted funny videos show the effectiveness of our method in capturingunusualness.
arxiv-7800-98 | A General Framework for Mixed Graphical Models | http://arxiv.org/pdf/1411.0288v1.pdf | author:Eunho Yang, Pradeep Ravikumar, Genevera I. Allen, Yulia Baker, Ying-Wooi Wan, Zhandong Liu category:math.ST stat.ML stat.TH published:2014-11-02 summary:"Mixed Data" comprising a large number of heterogeneous variables (e.g.count, binary, continuous, skewed continuous, among other data types) areprevalent in varied areas such as genomics and proteomics, imaging genetics,national security, social networking, and Internet advertising. There have beenlimited efforts at statistically modeling such mixed data jointly, in partbecause of the lack of computationally amenable multivariate distributions thatcan capture direct dependencies between such mixed variables of differenttypes. In this paper, we address this by introducing a novel class of BlockDirected Markov Random Fields (BDMRFs). Using the basic building block ofnode-conditional univariate exponential families from Yang et al. (2012), weintroduce a class of mixed conditional random field distributions, that arethen chained according to a block-directed acyclic graph to form our class ofBlock Directed Markov Random Fields (BDMRFs). The Markov independence graphstructure underlying a BDMRF thus has both directed and undirected edges. Weintroduce conditions under which these distributions exist and arenormalizable, study several instances of our models, and propose scalablepenalized conditional likelihood estimators with statistical guarantees forrecovering the underlying network structure. Simulations as well as anapplication to learning mixed genomic networks from next generation sequencingexpression data and mutation data demonstrate the versatility of our methods.
arxiv-7800-99 | Noisy Matrix Completion under Sparse Factor Models | http://arxiv.org/pdf/1411.0282v1.pdf | author:Akshay Soni, Swayambhoo Jain, Jarvis Haupt, Stefano Gonella category:stat.ML cs.IT math.IT stat.AP published:2014-11-02 summary:This paper examines a general class of noisy matrix completion tasks wherethe goal is to estimate a matrix from observations obtained at a subset of itsentries, each of which is subject to random noise or corruption. Our specificfocus is on settings where the matrix to be estimated is well-approximated by aproduct of two (a priori unknown) matrices, one of which is sparse. Suchstructural models - referred to here as "sparse factor models" - have beenwidely used, for example, in subspace clustering applications, as well as incontemporary sparse modeling and dictionary learning tasks. Our maintheoretical contributions are estimation error bounds for sparsity-regularizedmaximum likelihood estimators for problems of this form, which are applicableto a number of different observation noise or corruption models. Severalspecific implications are examined, including scenarios where observations arecorrupted by additive Gaussian noise or additive heavier-tailed (Laplace)noise, Poisson-distributed observations, and highly-quantized (e.g., one-bit)observations. We also propose a simple algorithmic approach based on thealternating direction method of multipliers for these tasks, and provideexperimental evidence to support our error analyses.
arxiv-7800-100 | Deterministic Bayesian Information Fusion and the Analysis of its Performance | http://arxiv.org/pdf/1311.3755v4.pdf | author:Gaurav Thakur category:math.ST stat.ML stat.TH published:2013-11-15 summary:This paper develops a mathematical and computational framework for analyzingthe expected performance of Bayesian data fusion, or joint statisticalinference, within a sensor network. We use variational techniques to obtain theposterior expectation as the optimal fusion rule under a deterministicconstraint and a quadratic cost, and study the smoothness and other propertiesof its classification performance. For a certain class of fusion problems, weprove that this fusion rule is also optimal in a much wider sense and satisfiesstrong asymptotic convergence results. We show how these results apply to avariety of examples with Gaussian, exponential and other statistics, anddiscuss computational methods for determining the fusion system's performancein more general, large-scale problems. These results are motivated by studyingthe performance of fusing multi-modal radar and acoustic sensors for detectingexplosive substances, but have broad applicability to other Bayesian decisionproblems.
arxiv-7800-101 | Random feedback weights support learning in deep neural networks | http://arxiv.org/pdf/1411.0247v1.pdf | author:Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, Colin J. Akerman category:q-bio.NC cs.NE published:2014-11-02 summary:The brain processes information through many layers of neurons. This deeparchitecture is representationally powerful, but it complicates learning bymaking it hard to identify the responsible neurons when a mistake is made. Inmachine learning, the backpropagation algorithm assigns blame to a neuron bycomputing exactly how it contributed to an error. To do this, it multiplieserror signals by matrices consisting of all the synaptic weights on theneuron's axon and farther downstream. This operation requires a preciselychoreographed transport of synaptic weight information, which is thought to beimpossible in the brain. Here we present a surprisingly simple algorithm fordeep learning, which assigns blame by multiplying error signals by randomsynaptic weights. We show that a network can learn to extract usefulinformation from signals sent through these random feedback connections. Inessence, the network learns to learn. We demonstrate that this new mechanismperforms as quickly and accurately as backpropagation on a variety of problemsand describe the principles which underlie its function. Our demonstrationprovides a plausible basis for how a neuron can be adapted using error signalsgenerated at distal locations in the brain, and thus dispels long-heldassumptions about the algorithmic constraints on learning in neural circuits.
arxiv-7800-102 | Cuckoo Search Inspired Hybridization of the Nelder-Mead Simplex Algorithm Applied to Optimization of Photovoltaic Cells | http://arxiv.org/pdf/1411.0217v1.pdf | author:Raka Jovanovic, Sabre Kais, Fahhad H. Alharbi category:cs.NE published:2014-11-02 summary:A new hybridization of the Cuckoo Search (CS) is developed and applied tooptimize multi-cell solar systems; namely multi-junction and split spectrumcells. The new approach consists of combining the CS with the Nelder-Meadmethod. More precisely, instead of using single solutions as nests for the CS,we use the concept of a simplex which is used in the Nelder-Mead algorithm.This makes it possible to use the flip operation introduces in the Nelder-Meadalgorithm instead of the Levy flight which is a standard part of the CS. Inthis way, the hybridized algorithm becomes more robust and less sensitive toparameter tuning which exists in CS. The goal of our work was to optimize theperformance of multi-cell solar systems. Although the underlying problemconsists of the minimization of a function of a relatively small number ofparameters, the difficulty comes from the fact that the evaluation of thefunction is complex and only a small number of evaluations is possible. In ourtest, we show that the new method has a better performance when compared tosimilar but more compex hybridizations of Nelder-Mead algorithm using geneticalgorithms or particle swarm optimization on standard benchmark functions.Finally, we show that the new method outperforms some standard meta-heuristicsfor the problem of interest.
arxiv-7800-103 | Extension of SBL Algorithms for the Recovery of Block Sparse Signals with Intra-Block Correlation | http://arxiv.org/pdf/1201.0862v5.pdf | author:Zhilin Zhang, Bhaskar D. Rao category:stat.ML stat.ME published:2012-01-04 summary:We examine the recovery of block sparse signals and extend the framework intwo important directions; one by exploiting signals' intra-block correlationand the other by generalizing signals' block structure. We propose two familiesof algorithms based on the framework of block sparse Bayesian learning (BSBL).One family, directly derived from the BSBL framework, requires knowledge of theblock structure. Another family, derived from an expanded BSBL framework, isbased on a weaker assumption on the block structure, and can be used when theblock structure is completely unknown. Using these algorithms we show thatexploiting intra-block correlation is very helpful in improving recoveryperformance. These algorithms also shed light on how to modify existingalgorithms or design new ones to exploit such correlation and improveperformance.
arxiv-7800-104 | Compressed Sensing of EEG for Wireless Telemonitoring with Low Energy Consumption and Inexpensive Hardware | http://arxiv.org/pdf/1206.3493v3.pdf | author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao category:stat.AP cs.IT math.IT stat.ML published:2012-06-13 summary:Telemonitoring of electroencephalogram (EEG) through wireless body-areanetworks is an evolving direction in personalized medicine. Among variousconstraints in designing such a system, three important constraints are energyconsumption, data compression, and device cost. Conventional data compressionmethodologies, although effective in data compression, consumes significantenergy and cannot reduce device cost. Compressed sensing (CS), as an emergingdata compression methodology, is promising in catering to these constraints.However, EEG is non-sparse in the time domain and also non-sparse intransformed domains (such as the wavelet domain). Therefore, it is extremelydifficult for current CS algorithms to recover EEG with the quality thatsatisfies the requirements of clinical diagnosis and engineering applications.Recently, Block Sparse Bayesian Learning (BSBL) was proposed as a new method tothe CS problem. This study introduces the technique to the telemonitoring ofEEG. Experimental results show that its recovery quality is better thanstate-of-the-art CS algorithms, and sufficient for practical use. These resultssuggest that BSBL is very promising for telemonitoring of EEG and othernon-sparse physiological signals.
arxiv-7800-105 | Compressed Sensing for Energy-Efficient Wireless Telemonitoring of Noninvasive Fetal ECG via Block Sparse Bayesian Learning | http://arxiv.org/pdf/1205.1287v7.pdf | author:Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao category:stat.ML cs.LG stat.AP published:2012-05-07 summary:Fetal ECG (FECG) telemonitoring is an important branch in telemedicine. Thedesign of a telemonitoring system via a wireless body-area network with lowenergy consumption for ambulatory use is highly desirable. As an emergingtechnique, compressed sensing (CS) shows great promise incompressing/reconstructing data with low energy consumption. However, due tosome specific characteristics of raw FECG recordings such as non-sparsity andstrong noise contamination, current CS algorithms generally fail in thisapplication. This work proposes to use the block sparse Bayesian learning (BSBL) frameworkto compress/reconstruct non-sparse raw FECG recordings. Experimental resultsshow that the framework can reconstruct the raw recordings with high quality.Especially, the reconstruction does not destroy the interdependence relationamong the multichannel recordings. This ensures that the independent componentanalysis decomposition of the reconstructed recordings has high fidelity.Furthermore, the framework allows the use of a sparse binary sensing matrixwith much fewer nonzero entries to compress recordings. Particularly, eachcolumn of the matrix can contain only two nonzero entries. This shows theframework, compared to other algorithms such as current CS algorithms andwavelet algorithms, can greatly reduce code execution in CPU in the datacompression stage.
arxiv-7800-106 | Image color transfer to evoke different emotions based on color combinations | http://arxiv.org/pdf/1307.3581v2.pdf | author:Li He, Hairong Qi, Russell Zaretzki category:cs.CV cs.GR published:2013-07-12 summary:In this paper, a color transfer framework to evoke different emotions forimages based on color combinations is proposed. The purpose of this colortransfer is to change the "look and feel" of images, i.e., evoking differentemotions. Colors are confirmed as the most attractive factor in images. Inaddition, various studies in both art and science areas have concluded thatother than single color, color combinations are necessary to evoke specificemotions. Therefore, we propose a novel framework to transfer color of imagesbased on color combinations, using a predefined color emotion model. Thecontribution of this new framework is three-fold. First, users do not need toprovide reference images as used in traditional color transfer algorithms. Inmost situations, users may not have enough aesthetic knowledge or path tochoose desired reference images. Second, because of the usage of colorcombinations instead of single color for emotions, a new color transferalgorithm that does not require an image library is proposed. Third, againbecause of the usage of color combinations, artifacts that are normally seen intraditional frameworks using single color are avoided. We present encouragingresults generated from this new framework and its potential in several possibleapplications including color transfer of photos and paintings.
arxiv-7800-107 | Density Estimation in Infinite Dimensional Exponential Families | http://arxiv.org/pdf/1312.3516v3.pdf | author:Bharath Sriperumbudur, Kenji Fukumizu, Revant Kumar, Arthur Gretton, Aapo Hyvärinen category:math.ST stat.ME stat.ML stat.TH published:2013-12-12 summary:In this paper, we consider an infinite dimensional exponential family,$\mathcal{P}$ of probability densities, which are parametrized by functions ina reproducing kernel Hilbert space, $H$ and show it to be quite rich in thesense that a broad class of densities on $\mathbb{R}^d$ can be approximatedarbitrarily well in Kullback-Leibler (KL) divergence by elements in$\mathcal{P}$. The main goal of the paper is to estimate an unknown density,$p_0$ through an element in $\mathcal{P}$. Standard techniques like maximumlikelihood estimation (MLE) or pseudo MLE (based on the method of sieves),which are based on minimizing the KL divergence between $p_0$ and$\mathcal{P}$, do not yield practically useful estimators because of theirinability to efficiently handle the log-partition function. Instead, we proposean estimator, $\hat{p}_n$ based on minimizing the \emph{Fisher divergence},$J(p_0\Vert p)$ between $p_0$ and $p\in \mathcal{P}$, which involves solving asimple finite-dimensional linear system. When $p_0\in\mathcal{P}$, we show thatthe proposed estimator is consistent, and provide a convergence rate of$n^{-\min\left\{\frac{2}{3},\frac{2\beta+1}{2\beta+2}\right\}}$ in Fisherdivergence under the smoothness assumption that $\logp_0\in\mathcal{R}(C^\beta)$ for some $\beta\ge 0$, where $C$ is a certainHilbert-Schmidt operator on $H$ and $\mathcal{R}(C^\beta)$ denotes the image of$C^\beta$. We also investigate the misspecified case of $p_0\notin\mathcal{P}$and show that $J(p_0\Vert\hat{p}_n)\rightarrow \inf_{p\in\mathcal{P}}J(p_0\Vertp)$ as $n\rightarrow\infty$, and provide a rate for this convergence under asimilar smoothness condition as above. Through numerical simulations wedemonstrate that the proposed estimator outperforms the non-parametric kerneldensity estimator, and that the advantage with the proposed estimator grows as$d$ increases.
arxiv-7800-108 | Synchronization Clustering based on a Linearized Version of Vicsek model | http://arxiv.org/pdf/1411.0189v1.pdf | author:Xinquan Chen category:cs.LG cs.DB published:2014-11-02 summary:This paper presents a kind of effective synchronization clustering methodbased on a linearized version of Vicsek model. This method can be representedby an Effective Synchronization Clustering algorithm (ESynC), an Improvedversion of ESynC algorithm (IESynC), a Shrinking Synchronization Clusteringalgorithm based on another linear Vicsek model (SSynC), and an effectiveMulti-level Synchronization Clustering algorithm (MSynC). After some analysisand comparisions, we find that ESynC algorithm based on the Linearized versionof the Vicsek model has better synchronization effect than SynC algorithm basedon an extensive Kuramoto model and a similar synchronization clusteringalgorithm based on the original Vicsek model. By simulated experiments of someartificial data sets, we observe that ESynC algorithm, IESynC algorithm, andSSynC algorithm can get better synchronization effect although it needs lessiterative times and less time than SynC algorithm. In some simulations, we alsoobserve that IESynC algorithm and SSynC algorithm can get some improvements intime cost than ESynC algorithm. At last, it gives some research expectations topopularize this algorithm.
arxiv-7800-109 | A Novel Method to Extract Rocks from Mars Images | http://arxiv.org/pdf/1403.3083v2.pdf | author:Shuliang Wang, Yasen Chen category:cs.CV published:2014-03-13 summary:In this paper, a novel method is proposed to extract rocks from Martiansurface images by using 8 data field. It models the interaction between twopixels of an image in the context of imagery 9 characteristics. First,foreground rocks are differed from background information by binarizing 10image on roughly partitioned images. Second, foreground rocks are grouped intoclusters by 11 locating the centers and edges of clusters in data field viahierarchical grids. Third, the target 12 rocks are discovered for the MarsExploration Rover (MER) to keep healthy paths. The 13 experiment with imagestaken by MER shows the proposed method is practical and potential.
arxiv-7800-110 | Spectral Methods meet EM: A Provably Optimal Algorithm for Crowdsourcing | http://arxiv.org/pdf/1406.3824v3.pdf | author:Yuchen Zhang, Xi Chen, Dengyong Zhou, Michael I. Jordan category:stat.ML published:2014-06-15 summary:Crowdsourcing is a popular paradigm for effectively collecting labels at lowcost. The Dawid-Skene estimator has been widely used for inferring the truelabels from the noisy labels provided by non-expert crowdsourcing workers.However, since the estimator maximizes a non-convex log-likelihood function, itis hard to theoretically justify its performance. In this paper, we propose atwo-stage efficient algorithm for multi-class crowd labeling problems. Thefirst stage uses the spectral method to obtain an initial estimate ofparameters. Then the second stage refines the estimation by optimizing theobjective function of the Dawid-Skene estimator via the EM algorithm. We showthat our algorithm achieves the optimal convergence rate up to a logarithmicfactor. We conduct extensive experiments on synthetic and real datasets.Experimental results demonstrate that the proposed algorithm is comparable tothe most accurate empirical approach, while outperforming several otherrecently proposed methods.
arxiv-7800-111 | Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms | http://arxiv.org/pdf/1411.0169v1.pdf | author:Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS math.ST stat.TH published:2014-11-01 summary:Let $p$ be an unknown and arbitrary probability distribution over $[0,1)$. Weconsider the problem of {\em density estimation}, in which a learning algorithmis given i.i.d. draws from $p$ and must (with high probability) output ahypothesis distribution that is close to $p$. The main contribution of thispaper is a highly efficient density estimation algorithm for learning using avariable-width histogram, i.e., a hypothesis distribution with a piecewiseconstant probability density function. In more detail, for any $k$ and $\epsilon$, we give an algorithm that makes$\tilde{O}(k/\epsilon^2)$ draws from $p$, runs in $\tilde{O}(k/\epsilon^2)$time, and outputs a hypothesis distribution $h$ that is piecewise constant with$O(k \log^2(1/\epsilon))$ pieces. With high probability the hypothesis $h$satisfies $d_{\mathrm{TV}}(p,h) \leq C \cdot \mathrm{opt}_k(p) + \epsilon$,where $d_{\mathrm{TV}}$ denotes the total variation distance (statisticaldistance), $C$ is a universal constant, and $\mathrm{opt}_k(p)$ is the smallesttotal variation distance between $p$ and any $k$-piecewise constantdistribution. The sample size and running time of our algorithm are optimal upto logarithmic factors. The "approximation factor" $C$ in our result isinherent in the problem, as we prove that no algorithm with sample size boundedin terms of $k$ and $\epsilon$ can achieve $C<2$ regardless of what kind ofhypothesis distribution it uses.
arxiv-7800-112 | Entropy of Overcomplete Kernel Dictionaries | http://arxiv.org/pdf/1411.0161v1.pdf | author:Paul Honeine category:cs.IT cs.CV cs.LG cs.NE math.IT stat.ML published:2014-11-01 summary:In signal analysis and synthesis, linear approximation theory considers alinear decomposition of any given signal in a set of atoms, collected into aso-called dictionary. Relevant sparse representations are obtained by relaxingthe orthogonality condition of the atoms, yielding overcomplete dictionarieswith an extended number of atoms. More generally than the linear decomposition,overcomplete kernel dictionaries provide an elegant nonlinear extension bydefining the atoms through a mapping kernel function (e.g., the gaussiankernel). Models based on such kernel dictionaries are used in neural networks,gaussian processes and online learning with kernels. The quality of an overcomplete dictionary is evaluated with a diversitymeasure the distance, the approximation, the coherence and the Babel measures.In this paper, we develop a framework to examine overcomplete kerneldictionaries with the entropy from information theory. Indeed, a higher valueof the entropy is associated to a further uniform spread of the atoms over thespace. For each of the aforementioned diversity measures, we derive lowerbounds on the entropy. Several definitions of the entropy are examined, with anextensive analysis in both the input space and the mapped feature space.
arxiv-7800-113 | A Two-phase Decision Support Framework for the Automatic Screening of Digital Fundus Images | http://arxiv.org/pdf/1411.0130v1.pdf | author:Balint Antal, Andras Hajdu, Zsuzsanna Maros-Szabo, Zsolt Torok, Adrienne Csutak, Tunde Peto category:cs.CV published:2014-11-01 summary:In this paper we give a brief review on the present status of automateddetection systems describe for the screening of diabetic retinopathy. Wefurther detail an enhanced detection procedure that consists of two steps.First, a pre-screening algorithm is considered to classify the input digitalfundus images based on the severity of abnormalities. If an image is found tobe seriously abnormal, it will not be analysed further with robust lesiondetector algorithms. As a further improvement, we introduce a novel featureextraction approach based on clinical observations. The second step of theproposed method detects regions of interest with possible lesions on the imagesthat previously passed the pre-screening step. These regions will serve asinput to the specific lesion detectors for detailed analysis. This procedurecan increase the computational performance of a screening system. Experimentalresults show that both two steps of the proposed approach are capable toefficiently exclude a large amount of data from further processing, thus, todecrease the computational burden of the automatic screening system.
arxiv-7800-114 | Detection of texts in natural images | http://arxiv.org/pdf/1411.0126v1.pdf | author:Gowtham Rangarajan Raman category:cs.CV published:2014-11-01 summary:A framework that makes use of Connected components and supervised Supportmachine to recognise texts is proposed. The image is preprocessed and and edgegraph is calculated using a probabilistic framework to compensate forphotometric noise. Connected components over the resultant image is calculated,which is bounded and then pruned using geometric constraints. Finally a GaborFeature based SVM is used to classify the presence of text in the candidates.The proposed method was tested with ICDAR 10 dataset and few other imagesavailable on the internet. It resulted in a recall and precision metric of 0.72and 0.88 comfortably better than the benchmark Eiphstein's algorithm. Theproposed method recorded a 0.70 and 0.74 in natural images which issignificantly better than current methods on natural images. The proposedmethod also scales almost linearly for high resolution, cluttered images.
arxiv-7800-115 | Combining pattern-based CRFs and weighted context-free grammars | http://arxiv.org/pdf/1404.5475v2.pdf | author:Rustem Takhanov, Vladimir Kolmogorov category:cs.FL cs.DS cs.LG I.2.7 published:2014-04-22 summary:We consider two models for the sequence labeling (tagging) problem. The firstone is a {\em Pattern-Based Conditional Random Field }(\PB), in which theenergy of a string (chain labeling) $x=x_1\ldots x_n\in D^n$ is a sum of termsover intervals $[i,j]$ where each term is non-zero only if the substring$x_i\ldots x_j$ equals a prespecified word $w\in \Lambda$. The second model isa {\em Weighted Context-Free Grammar }(\WCFG) frequently used for naturallanguage processing. \PB and \WCFG encode local and non-local interactionsrespectively, and thus can be viewed as complementary. We propose a {\em Grammatical Pattern-Based CRF model }(\GPB) that combinesthe two in a natural way. We argue that it has certain advantages over existingapproaches such as the {\em Hybrid model} of Bened{\'i} and Sanchez thatcombines {\em $\mbox{$N$-grams}$} and \WCFGs. The focus of this paper is toanalyze the complexity of inference tasks in a \GPB such as computing MAP. Wepresent a polynomial-time algorithm for general \GPBs and a faster version fora special case that we call {\em Interaction Grammars}.
arxiv-7800-116 | Complex Events Recognition under Uncertainty in a Sensor Network | http://arxiv.org/pdf/1411.0085v1.pdf | author:Atul Kanaujia, Tae Eun Choe, Hongli Deng category:cs.CV published:2014-11-01 summary:Automated extraction of semantic information from a network of sensors forcognitive analysis and human-like reasoning is a desired capability in futureground surveillance systems. We tackle the problem of complex decision makingunder uncertainty in network information environment, where lack of effectivevisual processing tools, incomplete domain knowledge frequently causeuncertainty in the visual primitives, leading to sub-optimal decisions. Whilestate-of-the-art vision techniques exist in detecting visual entities (humans,vehicles and scene elements) in an image, a missing functionality is theability to merge the information to reveal meaningful information for highlevel inference. In this work, we develop a probabilistic first order predicatelogic(FOPL) based reasoning system for recognizing complex events insynchronized stream of videos, acquired from sensors with non-overlappingfields of view. We adopt Markov Logic Network(MLN) as a tool to modeluncertainty in observations, and fuse information extracted from heterogeneousdata in a probabilistically consistent way. MLN overcomes strong dependence onpure empirical learning by incorporating domain knowledge, in the form ofuser-defined rules and confidences associated with them. This work demonstratesthat the MLN based decision control system can be made scalable to modelstatistical relations between a variety of entities and over long videosequences. Experiments with real-world data, under a variety of settings,illustrate the mathematical soundness and wide-ranging applicability of ourapproach.
arxiv-7800-117 | Learning Mixed Multinomial Logit Model from Ordinal Data | http://arxiv.org/pdf/1411.0073v1.pdf | author:Sewoong Oh, Devavrat Shah category:stat.ML published:2014-11-01 summary:Motivated by generating personalized recommendations using ordinal (orpreference) data, we study the question of learning a mixture of MultiNomialLogit (MNL) model, a parameterized class of distributions over permutations,from partial ordinal or preference data (e.g. pair-wise comparisons). Despiteits long standing importance across disciplines including social choice,operations research and revenue management, little is known about thisquestion. In case of single MNL models (no mixture), computationally andstatistically tractable learning from pair-wise comparisons is feasible.However, even learning mixture with two MNL components is infeasible ingeneral. Given this state of affairs, we seek conditions under which it is feasible tolearn the mixture model in both computationally and statistically efficientmanner. We present a sufficient condition as well as an efficient algorithm forlearning mixed MNL models from partial preferences/comparisons data. Inparticular, a mixture of $r$ MNL components over $n$ objects can be learntusing samples whose size scales polynomially in $n$ and $r$ (concretely,$r^{3.5}n^3(log n)^4$, with $r\ll n^{2/7}$ when the model parameters aresufficiently incoherent). The algorithm has two phases: first, learn thepair-wise marginals for each component using tensor decomposition; second,learn the model parameters for each component using Rank Centrality introducedby Negahban et al. In the process of proving these results, we obtain ageneralization of existing analysis for tensor decomposition to a morerealistic regime where only partial information about each sample is available.
arxiv-7800-118 | Sparse principal component regression with adaptive loading | http://arxiv.org/pdf/1402.6455v4.pdf | author:Shuichi Kawano, Hironori Fujisawa, Toyoyuki Takada, Toshihiko Shiroishi category:stat.ML stat.ME 62H25, 62J07 published:2014-02-26 summary:Principal component regression (PCR) is a two-stage procedure that selectssome principal components and then constructs a regression model regarding themas new explanatory variables. Note that the principal components are obtainedfrom only explanatory variables and not considered with the response variable.To address this problem, we propose the sparse principal component regression(SPCR) that is a one-stage procedure for PCR. SPCR enables us to adaptivelyobtain sparse principal component loadings that are related to the responsevariable and select the number of principal components simultaneously. SPCR canbe obtained by the convex optimization problem for each of parameters with thecoordinate descent algorithm. Monte Carlo simulations and real data analysesare performed to illustrate the effectiveness of SPCR.
arxiv-7800-119 | Feedback Detection for Live Predictors | http://arxiv.org/pdf/1310.2931v2.pdf | author:Stefan Wager, Nick Chamandy, Omkar Muralidharan, Amir Najmi category:stat.ME cs.LG stat.ML published:2013-10-10 summary:A predictor that is deployed in a live production system may perturb thefeatures it uses to make predictions. Such a feedback loop can occur, forexample, when a model that predicts a certain type of behavior ends up causingthe behavior it predicts, thus creating a self-fulfilling prophecy. In thispaper we analyze predictor feedback detection as a causal inference problem,and introduce a local randomization scheme that can be used to detectnon-linear feedback in real-world problems. We conduct a pilot study for ourproposed methodology using a predictive system currently deployed as a part ofa search engine.
arxiv-7800-120 | LSDA: Large Scale Detection Through Adaptation | http://arxiv.org/pdf/1407.5035v3.pdf | author:Judy Hoffman, Sergio Guadarrama, Eric Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko category:cs.CV published:2014-07-18 summary:A major challenge in scaling object detection is the difficulty of obtaininglabeled images for large numbers of categories. Recently, deep convolutionalneural networks (CNNs) have emerged as clear winners on object classificationbenchmarks, in part due to training with 1.2M+ labeled classification images.Unfortunately, only a small fraction of those labels are available for thedetection task. It is much cheaper and easier to collect large quantities ofimage-level labels from search engines than it is to collect detection data andlabel it with precise bounding boxes. In this paper, we propose Large ScaleDetection through Adaptation (LSDA), an algorithm which learns the differencebetween the two tasks and transfers this knowledge to classifiers forcategories without bounding box annotated data, turning them into detectors.Our method has the potential to enable detection for the tens of thousands ofcategories that lack bounding box annotations, yet have plenty ofclassification data. Evaluation on the ImageNet LSVRC-2013 detection challengedemonstrates the efficacy of our approach. This algorithm enables us to producea >7.6K detector by using available classification data from leaf nodes in theImageNet tree. We additionally demonstrate how to modify our architecture toproduce a fast detector (running at 2fps for the 7.6K detector). Models andsoftware are available at
arxiv-7800-121 | Model-based Reinforcement Learning and the Eluder Dimension | http://arxiv.org/pdf/1406.1853v2.pdf | author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG published:2014-06-07 summary:We consider the problem of learning to optimize an unknown Markov decisionprocess (MDP). We show that, if the MDP can be parameterized within some knownfunction class, we can obtain regret bounds that scale with the dimensionality,rather than cardinality, of the system. We characterize this dependenceexplicitly as $\tilde{O}(\sqrt{d_K d_E T})$ where $T$ is time elapsed, $d_K$ isthe Kolmogorov dimension and $d_E$ is the \emph{eluder dimension}. Theserepresent the first unified regret bounds for model-based reinforcementlearning and provide state of the art guarantees in several important settings.Moreover, we present a simple and computationally efficient algorithm\emph{posterior sampling for reinforcement learning} (PSRL) that satisfiesthese bounds.
arxiv-7800-122 | Near-optimal Reinforcement Learning in Factored MDPs | http://arxiv.org/pdf/1403.3741v3.pdf | author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG published:2014-03-15 summary:Any reinforcement learning algorithm that applies to all Markov decisionprocesses (MDPs) will suffer $\Omega(\sqrt{SAT})$ regret on some MDP, where $T$is the elapsed time and $S$ and $A$ are the cardinalities of the state andaction spaces. This implies $T = \Omega(SA)$ time to guarantee a near-optimalpolicy. In many settings of practical interest, due to the curse ofdimensionality, $S$ and $A$ can be so enormous that this learning time isunacceptable. We establish that, if the system is known to be a \emph{factored}MDP, it is possible to achieve regret that scales polynomially in the number of\emph{parameters} encoding the factored MDP, which may be exponentially smallerthan $S$ or $A$. We provide two algorithms that satisfy near-optimal regretbounds in this context: posterior sampling reinforcement learning (PSRL) and anupper confidence bound algorithm (UCRL-Factored).
arxiv-7800-123 | Semi-Supervised Learning with Deep Generative Models | http://arxiv.org/pdf/1406.5298v2.pdf | author:Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling category:cs.LG stat.ML published:2014-06-20 summary:The ever-increasing size of modern data sets combined with the difficulty ofobtaining label information has made semi-supervised learning one of theproblems of significant practical importance in modern data analysis. Werevisit the approach to semi-supervised learning with generative models anddevelop new models that allow for effective generalisation from small labelleddata sets to large unlabelled ones. Generative approaches have thus far beeneither inflexible, inefficient or non-scalable. We show that deep generativemodels and approximate Bayesian inference exploiting recent advances invariational methods can be used to provide significant improvements, makinggenerative approaches highly competitive for semi-supervised learning.
arxiv-7800-124 | Generalized Adaptive Dictionary Learning via Domain Shift Minimization | http://arxiv.org/pdf/1411.0022v1.pdf | author:Varun Panaganti category:cs.CV published:2014-10-31 summary:Visual data driven dictionaries have been successfully employed for variousobject recognition and classification tasks. However, the task becomes morechallenging if the training and test data are from contrasting domains. In thispaper, we propose a novel and generalized approach towards learning an adaptiveand common dictionary for multiple domains. Precisely, we project the data fromdifferent domains onto a low dimensional space while preserving the intrinsicstructure of data from each domain. We also minimize the domain-shift among thedata from each pair of domains. Simultaneously, we learn a common adaptivedictionary. Our algorithm can also be modified to learn class-specificdictionaries which can be used for classification. We additionally propose adiscriminative manifold regularization which imposes the intrinsic structure ofclass specific features onto the sparse coefficients. Experiments on imageclassification show that our approach fares better compared to the existingstate-of-the-art methods.
arxiv-7800-125 | Rapid Adaptation of POS Tagging for Domain Specific Uses | http://arxiv.org/pdf/1411.0007v1.pdf | author:John E. Miller, Michael Bloodgood, Manabu Torii, K. Vijay-Shanker category:cs.CL cs.LG stat.ML published:2014-10-31 summary:Part-of-speech (POS) tagging is a fundamental component for performingnatural language tasks such as parsing, information extraction, and questionanswering. When POS taggers are trained in one domain and applied insignificantly different domains, their performance can degrade dramatically. Wepresent a methodology for rapid adaptation of POS taggers to new domains. Ourtechnique is unsupervised in that a manually annotated corpus for the newdomain is not necessary. We use suffix information gathered from large amountsof raw text as well as orthographic information to increase the lexicalcoverage. We present an experiment in the Biological domain where our POStagger achieves results comparable to POS taggers specifically trained to thisdomain.
arxiv-7800-126 | A Latent Source Model for Online Collaborative Filtering | http://arxiv.org/pdf/1411.6591v1.pdf | author:Guy Bresler, George H. Chen, Devavrat Shah category:cs.LG cs.IR stat.ML published:2014-10-31 summary:Despite the prevalence of collaborative filtering in recommendation systems,there has been little theoretical development on why and how well it works,especially in the "online" setting, where items are recommended to users overtime. We address this theoretical gap by introducing a model for onlinerecommendation systems, cast item recommendation under the model as a learningproblem, and analyze the performance of a cosine-similarity collaborativefiltering method. In our model, each of $n$ users either likes or dislikes eachof $m$ items. We assume there to be $k$ types of users, and all the users of agiven type share a common string of probabilities determining the chance ofliking each item. At each time step, we recommend an item to each user, where akey distinction from related bandit literature is that once a user consumes anitem (e.g., watches a movie), then that item cannot be recommended to the sameuser again. The goal is to maximize the number of likable items recommended tousers over time. Our main result establishes that after nearly $\log(km)$initial learning time steps, a simple collaborative filtering algorithmachieves essentially optimal performance without knowing $k$. The algorithm hasan exploitation step that uses cosine similarity and two types of explorationsteps, one to explore the space of items (standard in the literature) and theother to explore similarity between users (novel to this work).
arxiv-7800-127 | Greedy Subspace Clustering | http://arxiv.org/pdf/1410.8864v1.pdf | author:Dohyung Park, Constantine Caramanis, Sujay Sanghavi category:stat.ML cs.IT cs.LG math.IT published:2014-10-31 summary:We consider the problem of subspace clustering: given points that lie on ornear the union of many low-dimensional linear subspaces, recover the subspaces.To this end, one first identifies sets of points close to the same subspace anduses the sets to estimate the subspaces. As the geometric structure of theclusters (linear subspaces) forbids proper performance of general distancebased approaches such as K-means, many model-specific methods have beenproposed. In this paper, we provide new simple and efficient algorithms forthis problem. Our statistical analysis shows that the algorithms are guaranteedexact (perfect) clustering performance under certain conditions on the numberof points and the affinity between subspaces. These conditions are weaker thanthose considered in the standard statistical literature. Experimental resultson synthetic data generated from the standard unions of subspaces modeldemonstrate our theory. We also show that our algorithm performs competitivelyagainst state-of-the-art algorithms on real-world applications such as motionsegmentation and face clustering, with much simpler implementation and lowercomputational cost.
arxiv-7800-128 | Altitude Training: Strong Bounds for Single-Layer Dropout | http://arxiv.org/pdf/1407.3289v2.pdf | author:Stefan Wager, William Fithian, Sida Wang, Percy Liang category:stat.ML cs.LG math.ST stat.TH published:2014-07-11 summary:Dropout training, originally designed for deep neural networks, has beensuccessful on high-dimensional single-layer natural language tasks. This paperproposes a theoretical explanation for this phenomenon: we show that, under agenerative Poisson topic model with long documents, dropout training improvesthe exponent in the generalization bound for empirical risk minimization.Dropout achieves this gain much like a marathon runner who practices ataltitude: once a classifier learns to perform reasonably well on trainingexamples that have been artificially corrupted by dropout, it will do very wellon the uncorrupted test set. We also show that, under similar conditions,dropout preserves the Bayes decision boundary and should therefore induceminimal bias in high dimensions.
arxiv-7800-129 | A non-linear learning & classification algorithm that achieves full training accuracy with stellar classification accuracy | http://arxiv.org/pdf/1409.6440v2.pdf | author:Rashid Khogali category:cs.CV cs.LG published:2014-09-23 summary:A fast Non-linear and non-iterative learning and classification algorithm issynthesized and validated. This algorithm named the "Reverse RippleEffect(R.R.E)", achieves 100% learning accuracy but is computationallyexpensive upon classification. The R.R.E is a (deterministic) algorithm thatsuper imposes Gaussian weighted functions on training points. In this work, theR.R.E algorithm is compared against known learning and classificationtechniques/algorithms such as: the Perceptron Criterion algorithm, LinearSupport Vector machines, the Linear Fisher Discriminant and a simple NeuralNetwork. The classification accuracy of the R.R.E algorithm is evaluated usingsimulations conducted in MATLAB. The R.R.E algorithm's behaviour is analyzedunder linearly and non-linearly separable data sets. For the comparison withthe Neural Network, the classical XOR problem is considered.
arxiv-7800-130 | Supervised learning model for parsing Arabic language | http://arxiv.org/pdf/1410.8783v1.pdf | author:Nabil Khoufi, Chafik Aloulou, Lamia Hadrich Belguith category:cs.CL cs.LG I.2.7 published:2014-10-31 summary:Parsing the Arabic language is a difficult task given the specificities ofthis language and given the scarcity of digital resources (grammars andannotated corpora). In this paper, we suggest a method for Arabic parsing basedon supervised machine learning. We used the SVMs algorithm to select thesyntactic labels of the sentence. Furthermore, we evaluated our parserfollowing the cross validation method by using the Penn Arabic Treebank. Theobtained results are very encouraging.
arxiv-7800-131 | Learning Mixtures of Ranking Models | http://arxiv.org/pdf/1410.8750v1.pdf | author:Pranjal Awasthi, Avrim Blum, Or Sheffet, Aravindan Vijayaraghavan category:cs.LG published:2014-10-31 summary:This work concerns learning probabilistic models for ranking data in aheterogeneous population. The specific problem we study is learning theparameters of a Mallows Mixture Model. Despite being widely studied, currentheuristics for this problem do not have theoretical guarantees and can getstuck in bad local optima. We present the first polynomial time algorithm whichprovably learns the parameters of a mixture of two Mallows models. A keycomponent of our algorithm is a novel use of tensor decomposition techniques tolearn the top-k prefix in both the rankings. Before this work, even thequestion of identifiability in the case of a mixture of two Mallows models wasunresolved.
arxiv-7800-132 | Partition-wise Linear Models | http://arxiv.org/pdf/1410.8675v1.pdf | author:Hidekazu Oiwa, Ryohei Fujimaki category:stat.ML cs.LG published:2014-10-31 summary:Region-specific linear models are widely used in practical applicationsbecause of their non-linear but highly interpretable model representations. Oneof the key challenges in their use is non-convexity in simultaneousoptimization of regions and region-specific models. This paper proposes novelconvex region-specific linear models, which we refer to as partition-wiselinear models. Our key ideas are 1) assigning linear models not to regions butto partitions (region-specifiers) and representing region-specific linearmodels by linear combinations of partition-specific models, and 2) optimizingregions via partition selection from a large number of given partitioncandidates by means of convex structured regularizations. In addition toproviding initialization-free globally-optimal solutions, our convexformulation makes it possible to derive a generalization bound and to use suchadvanced optimization techniques as proximal methods and decomposition of theproximal maps for sparsity-inducing regularizations. Experimental resultsdemonstrate that our partition-wise linear models perform better than or are atleast competitive with state-of-the-art region-specific or locally linearmodels.
arxiv-7800-133 | Experiments to Improve Named Entity Recognition on Turkish Tweets | http://arxiv.org/pdf/1410.8668v1.pdf | author:Dilek Küçük, Ralf Steinberger category:cs.CL published:2014-10-31 summary:Social media texts are significant information sources for severalapplication areas including trend analysis, event monitoring, and opinionmining. Unfortunately, existing solutions for tasks such as named entityrecognition that perform well on formal texts usually perform poorly whenapplied to social media texts. In this paper, we report on experiments thathave the purpose of improving named entity recognition on Turkish tweets, usingtwo different annotated data sets. In these experiments, starting with abaseline named entity recognition system, we adapt its recognition rules andresources to better fit Twitter language by relaxing its capitalizationconstraint and by diacritics-based expansion of its lexical resources, and weemploy a simplistic normalization scheme on tweets to observe the effects ofthese on the overall named entity recognition performance on Turkish tweets.The evaluation results of the system with these different settings are providedwith discussions of these results.
arxiv-7800-134 | Addressing the non-functional requirements of computer vision systems: A case study | http://arxiv.org/pdf/1410.8623v1.pdf | author:Shannon Fenn, Alexandre Mendes, David Budden category:cs.CV cs.RO cs.SE published:2014-10-31 summary:Computer vision plays a major role in the robotics industry, where visiondata is frequently used for navigation and high-level decision making. Althoughthere is significant research in algorithms and functional requirements, thereis a comparative lack of emphasis on how best to map these abstract conceptsonto an appropriate software architecture. In this study, we distinguish between the functional and non-functionalrequirements of a computer vision system. Using a RoboCup humanoid robot systemas a case study, we propose and develop a software architecture that fulfillsthe latter criteria. The modifiability of the proposed architecture is demonstrated by detailing anumber of feature detection algorithms and emphasizing which aspects of theunderlying framework were modified to support their integration. To demonstrateportability, we port our vision system (designed for an application-specificDARwIn-OP humanoid robot) to a general-purpose, Raspberry Pi computer. Weevaluate performance on both platforms and compare them to a vision systemoptimised for functional requirements only. The architecture and implementation presented in this study provide a highlygeneralisable framework for computer vision system design that is of particularbenefit in research and development, competition and other environments inwhich rapid system evolution is necessary.
arxiv-7800-135 | Discovering Structure in High-Dimensional Data Through Correlation Explanation | http://arxiv.org/pdf/1406.1222v2.pdf | author:Greg Ver Steeg, Aram Galstyan category:cs.LG cs.AI stat.ML published:2014-06-04 summary:We introduce a method to learn a hierarchy of successively more abstractrepresentations of complex data based on optimizing an information-theoreticobjective. Intuitively, the optimization searches for a set of latent factorsthat best explain the correlations in the data as measured by multivariatemutual information. The method is unsupervised, requires no model assumptions,and scales linearly with the number of variables which makes it an attractiveapproach for very high dimensional systems. We demonstrate that CorrelationExplanation (CorEx) automatically discovers meaningful structure for data fromdiverse sources including personality tests, DNA, and human language.
arxiv-7800-136 | A Comparison of learning algorithms on the Arcade Learning Environment | http://arxiv.org/pdf/1410.8620v1.pdf | author:Aaron Defazio, Thore Graepel category:cs.LG cs.AI published:2014-10-31 summary:Reinforcement learning agents have traditionally been evaluated on small toyproblems. With advances in computing power and the advent of the ArcadeLearning Environment, it is now possible to evaluate algorithms on diverse anddifficult problems within a consistent framework. We discuss some challengesposed by the arcade learning environment which do not manifest in simplerenvironments. We then provide a comparison of model-free, linear learningalgorithms on this challenging problem set.
arxiv-7800-137 | DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks | http://arxiv.org/pdf/1410.8586v1.pdf | author:Tao Chen, Damian Borth, Trevor Darrell, Shih-Fu Chang category:cs.CV cs.LG cs.MM cs.NE H.3.3 published:2014-10-30 summary:This paper introduces a visual sentiment concept classification method basedon deep convolutional neural networks (CNNs). The visual sentiment concepts areadjective noun pairs (ANPs) automatically discovered from the tags of webphotos, and can be utilized as effective statistical cues for detectingemotions depicted in the images. Nearly one million Flickr images tagged withthese ANPs are downloaded to train the classifiers of the concepts. We adoptthe popular model of deep convolutional neural networks which recently showsgreat performance improvement on classifying large-scale web-based imagedataset such as ImageNet. Our deep CNNs model is trained based on Caffe, anewly developed deep learning framework. To deal with the biased training datawhich only contains images with strong sentiment and to prevent overfitting, weinitialize the model with the model weights trained from ImageNet. Performanceevaluation shows the newly trained deep CNNs model SentiBank 2.0 (or calledDeepSentiBank) is significantly improved in both annotation accuracy andretrieval performance, compared to its predecessors which mainly use binary SVMclassification models.
arxiv-7800-138 | Semi-Automatic Construction of a Domain Ontology for Wind Energy Using Wikipedia Articles | http://arxiv.org/pdf/1410.8581v1.pdf | author:Dilek Küçük, Yusuf Arslan category:cs.CL cs.CE published:2014-10-30 summary:Domain ontologies are important information sources for knowledge-basedsystems. Yet, building domain ontologies from scratch is known to be a verylabor-intensive process. In this study, we present our semi-automatic approachto building an ontology for the domain of wind energy which is an importanttype of renewable energy with a growing share in electricity generation allover the world. Related Wikipedia articles are first processed in an automatedmanner to determine the basic concepts of the domain together with theirproperties and next the concepts, properties, and relationships are organizedto arrive at the ultimate ontology. We also provide pointers to otherengineering ontologies which could be utilized together with the proposed windenergy ontology in addition to its prospective application areas. The currentstudy is significant as, to the best of our knowledge, it proposes the firstconsiderably wide-coverage ontology for the wind energy domain and the ontologyis built through a semi-automatic process which makes use of the related Webresources, thereby reducing the overall cost of the ontology building process.
arxiv-7800-139 | An Online Algorithm for Learning Selectivity to Mixture Means | http://arxiv.org/pdf/1410.8580v1.pdf | author:Matthew Lawlor, Steven Zucker category:q-bio.NC cs.LG published:2014-10-30 summary:We develop a biologically-plausible learning rule called Triplet BCM thatprovably converges to the class means of general mixture models. This rulegeneralizes the classical BCM neural rule, and provides a novel interpretationof classical BCM as performing a kind of tensor decomposition. It achieves asubstantial generalization over classical BCM by incorporating triplets ofsamples from the mixtures, which provides a novel information processinginterpretation to spike-timing-dependent plasticity. We provide complete proofsof convergence of this learning rule, and an extended discussion of theconnection between BCM and tensor learning.
arxiv-7800-140 | An Ensemble-based System for Microaneurysm Detection and Diabetic Retinopathy Grading | http://arxiv.org/pdf/1410.8577v1.pdf | author:Balint Antal, Andras Hajdu category:cs.CV cs.AI stat.AP stat.ML published:2014-10-30 summary:Reliable microaneurysm detection in digital fundus images is still an openissue in medical image processing. We propose an ensemble-based framework toimprove microaneurysm detection. Unlike the well-known approach of consideringthe output of multiple classifiers, we propose a combination of internalcomponents of microaneurysm detectors, namely preprocessing methods andcandidate extractors. We have evaluated our approach for microaneurysmdetection in an online competition, where this algorithm is currently ranked asfirst and also on two other databases. Since microaneurysm detection isdecisive in diabetic retinopathy grading, we also tested the proposed methodfor this task on the publicly available Messidor database, where a promisingAUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classificationbased on the presence or absence of the microaneurysms.
arxiv-7800-141 | An ensemble-based system for automatic screening of diabetic retinopathy | http://arxiv.org/pdf/1410.8576v1.pdf | author:Balint Antal, Andras Hajdu category:cs.CV cs.LG stat.AP stat.ML published:2014-10-30 summary:In this paper, an ensemble-based method for the screening of diabeticretinopathy (DR) is proposed. This approach is based on features extracted fromthe output of several retinal image processing algorithms, such as image-level(quality assessment, pre-screening, AM/FM), lesion-specific (microaneurysms,exudates) and anatomical (macula, optic disc) components. The actual decisionabout the presence of the disease is then made by an ensemble of machinelearning classifiers. We have tested our approach on the publicly availableMessidor database, where 90% sensitivity, 91% specificity and 90% accuracy and0.989 AUC are achieved in a disease/no-disease setting. These results arehighly competitive in this field and suggest that retinal image processing is avalid approach for automatic DR screening.
arxiv-7800-142 | Causal Inference through a Witness Protection Program | http://arxiv.org/pdf/1406.0531v2.pdf | author:Ricardo Silva, Robin Evans category:stat.ML published:2014-06-02 summary:One of the most fundamental problems in causal inference is the estimation ofa causal effect when variables are confounded. This is difficult in anobservational study, because one has no direct evidence that all confoundershave been adjusted for. We introduce a novel approach for estimating causaleffects that exploits observational conditional independencies to suggest"weak" paths in a unknown causal graph. The widely used faithfulness conditionof Spirtes et al. is relaxed to allow for varying degrees of "pathcancellations" that imply conditional independencies but do not rule out theexistence of confounding causal paths. The outcome is a posterior distributionover bounds on the average causal effect via a linear programming approach andBayesian inference. We claim this approach should be used in regular practicealong with other default tools in observational studies.
arxiv-7800-143 | A random forest system combination approach for error detection in digital dictionaries | http://arxiv.org/pdf/1410.8553v1.pdf | author:Michael Bloodgood, Peng Ye, Paul Rodrigues, David Zajic, David Doermann category:cs.CL cs.LG stat.ML published:2014-10-30 summary:When digitizing a print bilingual dictionary, whether via optical characterrecognition or manual entry, it is inevitable that errors are introduced intothe electronic version that is created. We investigate automating the processof detecting errors in an XML representation of a digitized print dictionaryusing a hybrid approach that combines rule-based, feature-based, and languagemodel-based methods. We investigate combining methods and show that usingrandom forests is a promising approach. We find that in isolation, unsupervisedmethods rival the performance of supervised methods. Random forests typicallyrequire training data so we investigate how we can apply random forests tocombine individual base methods that are themselves unsupervised withoutrequiring large amounts of training data. Experiments reveal empirically that arelatively small amount of data is sufficient and can potentially be furtherreduced through specific selection criteria.
arxiv-7800-144 | What a Nasty day: Exploring Mood-Weather Relationship from Twitter | http://arxiv.org/pdf/1410.8749v1.pdf | author:Jiwei Li, Xun Wang, Eduard Hovy category:cs.SI cs.CL published:2014-10-30 summary:While it has long been believed in psychology that weather somehow influenceshuman's mood, the debates have been going on for decades about how they arecorrelated. In this paper, we try to study this long-lasting topic byharnessing a new source of data compared from traditional psychologicalresearches: Twitter. We analyze 2 years' twitter data collected by twitter APIwhich amounts to $10\%$ of all postings and try to reveal the correlationsbetween multiple dimensional structure of human mood with meteorologicaleffects. Some of our findings confirm existing hypotheses, while otherscontradict them. We are hopeful that our approach, along with the new datasource, can shed on the long-going debates on weather-mood correlation.
arxiv-7800-145 | Novel methods for multilinear data completion and de-noising based on tensor-SVD | http://arxiv.org/pdf/1407.1785v2.pdf | author:Zemin Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, Misha Kilmer category:cs.CV published:2014-07-07 summary:In this paper we propose novel methods for completion (from limited samples)and de-noising of multilinear (tensor) data and as an application consider 3-Dand 4- D (color) video data completion and de-noising. We exploit the recentlyproposed tensor-Singular Value Decomposition (t-SVD)[11]. Based on t-SVD, thenotion of multilinear rank and a related tensor nuclear norm was proposed in[11] to characterize informational and structural complexity of multilineardata. We first show that videos with linear camera motion can be representedmore efficiently using t-SVD compared to the approaches based on vectorizing orflattening of the tensors. Since efficiency in representation impliesefficiency in recovery, we outline a tensor nuclear norm penalized algorithmfor video completion from missing entries. Application of the proposedalgorithm for video recovery from missing entries is shown to yield a superiorperformance over existing methods. We also consider the problem of tensorrobust Principal Component Analysis (PCA) for de-noising 3-D video data fromsparse random corruptions. We show superior performance of our method comparedto the matrix robust PCA adapted to this setting as proposed in [4].
arxiv-7800-146 | A Drifting-Games Analysis for Online Learning and Applications to Boosting | http://arxiv.org/pdf/1406.1856v2.pdf | author:Haipeng Luo, Robert E. Schapire category:cs.LG published:2014-06-07 summary:We provide a general mechanism to design online learning algorithms based ona minimax analysis within a drifting-games framework. Different online learningsettings (Hedge, multi-armed bandit problems and online convex optimization)are studied by converting into various kinds of drifting games. The originalminimax analysis for drifting games is then used and generalized by applying aseries of relaxations, starting from choosing a convex surrogate of the 0-1loss function. With different choices of surrogates, we not only recoverexisting algorithms, but also propose new algorithms that are totallyparameter-free and enjoy other useful properties. Moreover, our drifting-gamesframework naturally allows us to study high probability bounds withoutresorting to any concentration results, and also a generalized notion of regretthat measures how good the algorithm is compared to all but the top smallfraction of candidates. Finally, we translate our new Hedge algorithm into anew adaptive boosting algorithm that is computationally faster as shown inexperiments, since it ignores a large number of examples on each round.
arxiv-7800-147 | Extracting information from S-curves of language change | http://arxiv.org/pdf/1406.4498v2.pdf | author:Fakhteh Ghanbarnejad, Martin Gerlach, Jose M. Miotto, Eduardo G. Altmann category:physics.soc-ph cs.CL published:2014-06-17 summary:It is well accepted that adoption of innovations are described by S-curves(slow start, accelerating period, and slow end). In this paper, we analyze howmuch information on the dynamics of innovation spreading can be obtained from aquantitative description of S-curves. We focus on the adoption of linguisticinnovations for which detailed databases of written texts from the last 200years allow for an unprecedented statistical precision. Combining data analysiswith simulations of simple models (e.g., the Bass dynamics on complex networks)we identify signatures of endogenous and exogenous factors in the S-curves ofadoption. We propose a measure to quantify the strength of these factors andthree different methods to estimate it from S-curves. We obtain cases in whichthe exogenous factors are dominant (in the adoption of German orthographicreforms and of one irregular verb) and cases in which endogenous factors aredominant (in the adoption of conventions for romanization of Russian names andin the regularization of most studied verbs). These results show that the shapeof S-curve is not universal and contains information on the adoption mechanism.(published at "J. R. Soc. Interface, vol. 11, no. 101, (2014) 1044"; DOI:http://dx.doi.org/10.1098/rsif.2014.1044)
arxiv-7800-148 | Learning circuits with few negations | http://arxiv.org/pdf/1410.8420v1.pdf | author:Eric Blais, Clément L. Canonne, Igor C. Oliveira, Rocco A. Servedio, Li-Yang Tan category:cs.CC cs.DM cs.LG published:2014-10-30 summary:Monotone Boolean functions, and the monotone Boolean circuits that computethem, have been intensively studied in complexity theory. In this paper westudy the structure of Boolean functions in terms of the minimum number ofnegations in any circuit computing them, a complexity measure that interpolatesbetween monotone functions and the class of all functions. We study thisgeneralization of monotonicity from the vantage point of learning theory,giving near-matching upper and lower bounds on the uniform-distributionlearnability of circuits in terms of the number of negations they contain. Ourupper bounds are based on a new structural characterization of negation-limitedcircuits that extends a classical result of A. A. Markov. Our lower bounds,which employ Fourier-analytic tools from hardness amplification, give newresults even for circuits with no negations (i.e. monotone functions).
arxiv-7800-149 | On Estimating $L_2^2$ Divergence | http://arxiv.org/pdf/1410.8372v1.pdf | author:Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, Larry Wasserman category:stat.ML published:2014-10-30 summary:We give a comprehensive theoretical characterization of a nonparametricestimator for the $L_2^2$ divergence between two continuous distributions. Wefirst bound the rate of convergence of our estimator, showing that it is$\sqrt{n}$-consistent provided the densities are sufficiently smooth. In thissmooth regime, we then show that our estimator is asymptotically normal,construct asymptotic confidence intervals, and establish a Berry-Ess\'{e}enstyle inequality characterizing the rate of convergence to normality. We alsoshow that this estimator is minimax optimal.
arxiv-7800-150 | Estimating the Accuracies of Multiple Classifiers Without Labeled Data | http://arxiv.org/pdf/1407.7644v2.pdf | author:Ariel Jaffe, Boaz Nadler, Yuval Kluger category:stat.ML cs.LG published:2014-07-29 summary:In various situations one is given only the predictions of multipleclassifiers over a large unlabeled test data. This scenario raises thefollowing questions: Without any labeled data and without any a-prioriknowledge about the reliability of these different classifiers, is it possibleto consistently and computationally efficiently estimate their accuracies?Furthermore, also in a completely unsupervised manner, can one construct a moreaccurate unsupervised ensemble classifier? In this paper, focusing on thebinary case, we present simple, computationally efficient algorithms to solvethese questions. Furthermore, under standard classifier independenceassumptions, we prove our methods are consistent and study their asymptoticerror. Our approach is spectral, based on the fact that the off-diagonalentries of the classifiers' covariance matrix and 3-d tensor are rank-one. Weillustrate the competitive performance of our algorithms via extensiveexperiments on both artificial and real datasets.
arxiv-7800-151 | Towards Learning Object Affordance Priors from Technical Texts | http://arxiv.org/pdf/1410.8326v1.pdf | author:Nicholas H. Kirk category:cs.LG cs.AI cs.CL cs.RO 68T05 published:2014-10-30 summary:Everyday activities performed by artificial assistants can potentially beexecuted naively and dangerously given their lack of common sense knowledge.This paper presents conceptual work towards obtaining prior knowledge on theusual modality (passive or active) of any given entity, and their affordanceestimates, by extracting high-confidence ability modality semantic relations (Xcan Y relationship) from non-figurative texts, by analyzing co-occurrence ofgrammatical instances of subjects and verbs, and verbs and objects. Thediscussion includes an outline of the concept, potential and limitations, andpossible feature and learning framework adoption.
arxiv-7800-152 | Subspace Clustering by Exploiting a Low-Rank Representation with a Symmetric Constraint | http://arxiv.org/pdf/1403.2330v2.pdf | author:Jie Chen, Zhang Yi category:cs.CV published:2014-03-07 summary:In this paper, we propose a low-rank representation with symmetric constraint(LRRSC) method for robust subspace clustering. Given a collection of datapoints approximately drawn from multiple subspaces, the proposed technique cansimultaneously recover the dimension and members of each subspace. LRRSCextends the original low-rank representation algorithm by integrating asymmetric constraint into the low-rankness property of high-dimensional datarepresentation. The symmetric low-rank representation, which preserves thesubspace structures of high-dimensional data, guarantees weight consistency foreach pair of data points so that highly correlated data points of subspaces arerepresented together. Moreover, it can be efficiently calculated by solving aconvex optimization problem. We provide a rigorous proof for minimizing thenuclear-norm regularized least square problem with a symmetric constraint. Theaffinity matrix for spectral clustering can be obtained by further exploitingthe angular information of the principal directions of the symmetric low-rankrepresentation. This is a critical step towards evaluating the membershipsbetween data points. Experimental results on benchmark databases demonstratethe effectiveness and robustness of LRRSC compared with severalstate-of-the-art subspace clustering algorithms.
arxiv-7800-153 | Efficient Decision-Making by Volume-Conserving Physical Object | http://arxiv.org/pdf/1412.6141v1.pdf | author:Song-Ju Kim, Masashi Aono, Etsushi Nameda category:cs.AI cs.LG nlin.AO published:2014-10-30 summary:We demonstrate that any physical object, as long as its volume is conservedwhen coupled with suitable operations, provides a sophisticated decision-makingcapability. We consider the problem of finding, as accurately and quickly aspossible, the most profitable option from a set of options that givesstochastic rewards. These decisions are made as dictated by a physical object,which is moved in a manner similar to the fluctuations of a rigid body in atug-of-war game. Our analytical calculations validate statistical reasons whyour method exhibits higher efficiency than conventional algorithms.
arxiv-7800-154 | Robust sketching for multiple square-root LASSO problems | http://arxiv.org/pdf/1411.0024v1.pdf | author:Vu Pham, Laurent El Ghaoui, Arturo Fernandez category:math.OC cs.LG cs.SY stat.ML published:2014-10-30 summary:Many learning tasks, such as cross-validation, parameter search, orleave-one-out analysis, involve multiple instances of similar problems, eachinstance sharing a large part of learning data with the others. We introduce arobust framework for solving multiple square-root LASSO problems, based on asketch of the learning data that uses low-rank approximations. Our approachallows a dramatic reduction in computational effort, in effect reducing thenumber of observations from $m$ (the number of observations to start with) to$k$ (the number of singular values retained in the low-rank model), while notsacrificing---sometimes even improving---the statistical performance.Theoretical analysis, as well as numerical experiments on both synthetic andreal data, illustrate the efficiency of the method in large scale applications.
arxiv-7800-155 | Notes on Noise Contrastive Estimation and Negative Sampling | http://arxiv.org/pdf/1410.8251v1.pdf | author:Chris Dyer category:cs.LG published:2014-10-30 summary:Estimating the parameters of probabilistic models of language such as maxentmodels and probabilistic neural models is computationally difficult since itinvolves evaluating partition functions by summing over an entire vocabulary,which may be millions of word types in size. Two closely relatedstrategies---noise contrastive estimation (Mnih and Teh, 2012; Mnih andKavukcuoglu, 2013; Vaswani et al., 2013) and negative sampling (Mikolov et al.,2012; Goldberg and Levy, 2014)---have emerged as popular solutions to thiscomputational problem, but some confusion remains as to which is moreappropriate and when. This document explicates their relationships to eachother and to other estimation techniques. The analysis shows that, althoughthey are superficially similar, NCE is a general parameter estimation techniquethat is asymptotically unbiased, while negative sampling is best understood asa family of binary classification models that are useful for learning wordrepresentations but not as a general-purpose estimator.
arxiv-7800-156 | ConceptVision: A Flexible Scene Classification Framework | http://arxiv.org/pdf/1401.0733v2.pdf | author:Ahmet Iscen, Eren Golge, Ilker Sarac, Pinar Duygulu category:cs.CV published:2014-01-03 summary:We introduce ConceptVision, a method that aims for high accuracy incategorizing large number of scenes, while keeping the model relatively simplerand efficient for scalability. The proposed method combines the advantages ofboth low-level representations and high-level semantic categories, andeliminates the distinctions between different levels through the definition ofconcepts. The proposed framework encodes the perspectives brought throughdifferent concepts by considering them in concept groups. Differentperspectives are ensembled for the final decision. Extensive experiments arecarried out on benchmark datasets to test the effects of different concepts,and methods used to ensemble. Comparisons with state-of-the-art studies showthat we can achieve better results with incorporation of concepts in differentlevels with different perspectives.
arxiv-7800-157 | A Unified Framework for Approximating and Clustering Data | http://arxiv.org/pdf/1106.1379v3.pdf | author:Dan Feldman, Michael Langberg category:cs.LG published:2011-06-07 summary:Given a set $F$ of $n$ positive functions over a ground set $X$, we considerthe problem of computing $x^*$ that minimizes the expression $\sum_{f\inF}f(x)$, over $x\in X$. A typical application is \emph{shape fitting}, where wewish to approximate a set $P$ of $n$ elements (say, points) by a shape $x$ froma (possibly infinite) family $X$ of shapes. Here, each point $p\in P$corresponds to a function $f$ such that $f(x)$ is the distance from $p$ to $x$,and we seek a shape $x$ that minimizes the sum of distances from each point in$P$. In the $k$-clustering variant, each $x\in X$ is a tuple of $k$ shapes, and$f(x)$ is the distance from $p$ to its closest shape in $x$. Our main result is a unified framework for constructing {\em coresets} and{\em approximate clustering} for such general sets of functions. To achieve ourresults, we forge a link between the classic and well defined notion of$\varepsilon$-approximations from the theory of PAC Learning and VC dimension,to the relatively new (and not so consistent) paradigm of coresets, which aresome kind of "compressed representation" of the input set $F$. Usingtraditional techniques, a coreset usually implies an LTAS (linear timeapproximation scheme) for the corresponding optimization problem, which can becomputed in parallel, via one pass over the data, and using onlypolylogarithmic space (i.e, in the streaming model). We show how to generalize the results of our framework for squared distances(as in $k$-mean), distances to the $q$th power, and deterministicconstructions.
arxiv-7800-158 | Detecting Structural Irregularity in Electronic Dictionaries Using Language Modeling | http://arxiv.org/pdf/1410.8149v1.pdf | author:Paul Rodrigues, David Zajic, David Doermann, Michael Bloodgood, Peng Ye category:cs.CL cs.LG published:2014-10-29 summary:Dictionaries are often developed using tools that save to Extensible MarkupLanguage (XML)-based standards. These standards often allow high-levelrepeating elements to represent lexical entries, and utilize descendants ofthese repeating elements to represent the structure within each lexical entry,in the form of an XML tree. In many cases, dictionaries are published that haveerrors and inconsistencies that are expensive to find manually. This paperdiscusses a method for dictionary writers to quickly audit structuralregularity across entries in a dictionary by using statistical languagemodeling. The approach learns the patterns of XML nodes that could occur withinan XML tree, and then calculates the probability of each XML tree in thedictionary against these patterns to look for entries that diverge from thenorm.
arxiv-7800-159 | High-Performance Distributed ML at Scale through Parameter Server Consistency Models | http://arxiv.org/pdf/1410.8043v1.pdf | author:Wei Dai, Abhimanu Kumar, Jinliang Wei, Qirong Ho, Garth Gibson, Eric P. Xing category:cs.LG stat.ML published:2014-10-29 summary:As Machine Learning (ML) applications increase in data size and modelcomplexity, practitioners turn to distributed clusters to satisfy the increasedcomputational and memory demands. Unfortunately, effective use of clusters forML requires considerable expertise in writing distributed code, whilehighly-abstracted frameworks like Hadoop have not, in practice, approached theperformance seen in specialized ML implementations. The recent Parameter Server(PS) paradigm is a middle ground between these extremes, allowing easyconversion of single-machine parallel ML applications into distributed ones,while maintaining high throughput through relaxed "consistency models" thatallow inconsistent parameter reads. However, due to insufficient theoreticalstudy, it is not clear which of these consistency models can really ensurecorrect ML algorithm output; at the same time, there remain manytheoretically-motivated but undiscovered opportunities to maximizecomputational throughput. Motivated by this challenge, we study both thetheoretical guarantees and empirical behavior of iterative-convergent MLalgorithms in existing PS consistency models. We then use the gleaned insightsto improve a consistency model using an "eager" PS communication mechanism, andimplement it as a new PS system that enables ML algorithms to reach theirsolution more quickly.
arxiv-7800-160 | Latent Feature Based FM Model For Rating Prediction | http://arxiv.org/pdf/1410.8034v1.pdf | author:Xudong Liu, Bin Zhang, Ting Zhang, Chang Liu category:cs.LG cs.IR stat.ML 68-XX H.2.8 published:2014-10-29 summary:Rating Prediction is a basic problem in Recommender System, and one of themost widely used method is Factorization Machines(FM). However, traditionalmatrix factorization methods fail to utilize the benefit of implicit feedback,which has been proved to be important in Rating Prediction problem. In thiswork, we consider a specific situation, movie rating prediction, where weassume that watching history has a big influence on his/her rating behavior onan item. We introduce two models, Latent Dirichlet Allocation(LDA) andword2vec, both of which perform state-of-the-art results in training latentfeatures. Based on that, we propose two feature based models. One is theTopic-based FM Model which provides the implicit feedback to the matrixfactorization. The other is the Vector-based FM Model which expresses the orderinfo of watching history. Empirical results on three datasets demonstrate thatour method performs better than the baseline model and confirm thatVector-based FM Model usually works better as it contains the order info.
arxiv-7800-161 | A Semantic Web of Know-How: Linked Data for Community-Centric Tasks | http://arxiv.org/pdf/1410.8808v1.pdf | author:Paolo Pareti, Ewan Klein, Adam Barker category:cs.AI cs.CL published:2014-10-29 summary:This paper proposes a novel framework for representing community know-how onthe Semantic Web. Procedural knowledge generated by web communities typicallytakes the form of natural language instructions or videos and is largelyunstructured. The absence of semantic structure impedes the deployment of manyuseful applications, in particular the ability to discover and integrateknow-how automatically. We discuss the characteristics of community know-howand argue that existing knowledge representation frameworks fail to representit adequately. We present a novel framework for representing the semanticstructure of community know-how and demonstrate the feasibility of our approachby providing a concrete implementation which includes a method forautomatically acquiring procedural knowledge for real-world tasks.
arxiv-7800-162 | Extended Dynamic Programming and Fast Multidimensional Search Algorithm for Energy Minization in Stereo and Motion | http://arxiv.org/pdf/1410.7922v1.pdf | author:Mikhail G. Mozerov category:cs.CV published:2014-10-29 summary:This paper presents a novel extended dynamic programming approach for energyminimization (EDP) to solve the correspondence problem for stereo and motion. Asignificant speedup is achieved using a recursive minimum search strategy(RMS). The mentioned speedup is particularly important if the disparity spaceis 2D as well as 3D. The proposed RMS can also be applied in the well-knowndynamic programming (DP) approach for stereo and motion. In this case, thegeneral 2D problem of the global discrete energy minimization is reduced toseveral mutually independent sub-problems of the one-dimensional minimization.The EDP method is used when the approximation of the general 2D discrete energyminimization problem is considered. Then the RMS algorithm is an essential partof the EDP method. Using the EDP algorithm we obtain a lower energy bound thanthe graph cuts (GC) expansion technique on stereo and motion problems. Theproposed calculation scheme possesses natural parallelism and can be realizedon graphics processing unit (GPU) platforms, and can be potentially restrictedfurther by the number of scanlines in the image plane. Furthermore, the RMS andEDP methods can be used in any optimization problem where the objectivefunction meets specific conditions in the smoothness term.
arxiv-7800-163 | Global Bandits with Holder Continuity | http://arxiv.org/pdf/1410.7890v1.pdf | author:Onur Atan, Cem Tekin, Mihaela van der Schaar category:cs.LG published:2014-10-29 summary:Standard Multi-Armed Bandit (MAB) problems assume that the arms areindependent. However, in many application scenarios, the information obtainedby playing an arm provides information about the remainder of the arms. Hence,in such applications, this informativeness can and should be exploited toenable faster convergence to the optimal solution. In this paper, we introduceand formalize the Global MAB (GMAB), in which arms are globally informativethrough a global parameter, i.e., choosing an arm reveals information about allthe arms. We propose a greedy policy for the GMAB which always selects the armwith the highest estimated expected reward, and prove that it achieves boundedparameter-dependent regret. Hence, this policy selects suboptimal arms onlyfinitely many times, and after a finite number of initial time steps, theoptimal arm is selected in all of the remaining time steps with probabilityone. In addition, we also study how the informativeness of the arms about eachother's rewards affects the speed of learning. Specifically, we prove that theparameter-free (worst-case) regret is sublinear in time, and decreases with theinformativeness of the arms. We also prove a sublinear in time Bayesian riskbound for the GMAB which reduces to the well-known Bayesian risk bound forlinearly parameterized bandits when the arms are fully informative. GMABs haveapplications ranging from drug and treatment discovery to dynamic pricing.
arxiv-7800-164 | Sub-threshold CMOS Spiking Neuron Circuit Design for Navigation Inspired by C. elegans Chemotaxis | http://arxiv.org/pdf/1410.7883v1.pdf | author:Shibani Santurkar, Bipin Rajendran category:cs.NE q-bio.NC published:2014-10-29 summary:We demonstrate a spiking neural network for navigation motivated by thechemotaxis network of Caenorhabditis elegans. Our network uses informationregarding temporal gradients in the tracking variable's concentration to makenavigational decisions. The gradient information is determined by mimicking theunderlying mechanisms of the ASE neurons of C. elegans. Simulations show thatour model is able to forage and track a target set-point in extremely noisyenvironments. We develop a VLSI implementation for the main gradient detectorneurons, which could be integrated with standard comparator circuitry todevelop a robust circuit for navigation and contour tracking.
arxiv-7800-165 | A neural circuit for navigation inspired by C. elegans Chemotaxis | http://arxiv.org/pdf/1410.7881v1.pdf | author:Shibani Santurkar, Bipin Rajendran category:cs.NE q-bio.NC published:2014-10-29 summary:We develop an artificial neural circuit for contour tracking and navigationinspired by the chemotaxis of the nematode Caenorhabditis elegans. In order toharness the computational advantages spiking neural networks promise over theirnon-spiking counterparts, we develop a network comprising 7-spiking neuronswith non-plastic synapses which we show is extremely robust in tracking a rangeof concentrations. Our worm uses information regarding local temporal gradientsin sodium chloride concentration to decide the instantaneous path for foraging,exploration and tracking. A key neuron pair in the C. elegans chemotaxisnetwork is the ASEL & ASER neuron pair, which capture the gradient ofconcentration sensed by the worm in their graded membrane potentials. Theprimary sensory neurons for our network are a pair of artificial spikingneurons that function as gradient detectors whose design is adapted from acomputational model of the ASE neuron pair in C. elegans. Simulations show thatour worm is able to detect the set-point with approximately four times higherprobability than the optimal memoryless Levy foraging model. We also show thatour spiking neural network is much more efficient and noise-resilient whilenavigating and tracking a contour, as compared to an equivalent non-spikingnetwork. We demonstrate that our model is extremely robust to noise and withslight modifications can be used for other practical applications such asobstacle avoidance. Our network model could also be extended for use inthree-dimensional contour tracking or obstacle avoidance.
arxiv-7800-166 | Collaborative Multi-sensor Classification via Sparsity-based Representation | http://arxiv.org/pdf/1410.7876v1.pdf | author:Minh Dao, Nam H. Nguyen, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV cs.LG stat.ML published:2014-10-29 summary:In this paper, we propose a general collaborative sparse representationframework for multi-sensor classification, which takes into account thecorrelations as well as complementary information between heterogeneous sensorssimultaneously while considering joint sparsity within each sensor'sobservations. We also robustify our models to deal with the presence of sparsenoise and low-rank interference signals. Specifically, we demonstrate thatincorporating the noise or interference signal as a low-rank component in ourmodels is essential in a multi-sensor classification problem when multipleco-located sources/sensors simultaneously record the same physical event. Wefurther extend our frameworks to kernelized models which rely on sparselyrepresenting a test sample in terms of all the training samples in a featurespace induced by a kernel function. A fast and efficient algorithm based onalternative direction method is proposed where its convergence to optimalsolution is guaranteed. Extensive experiments are conducted on real data setscollected by researchers at the U.S. Army Research Laboratory and the resultsare compared with the conventional classifiers to verify the effectiveness ofthe proposed methods in the application of automatic multi-sensor border patrolcontrol, where we often have to discriminate between human and animalfootsteps.
arxiv-7800-167 | Faster graphical model identification of tandem mass spectra using peptide word lattices | http://arxiv.org/pdf/1410.7875v1.pdf | author:Shengjie Wang, John T. Halloran, Jeff A. Bilmes, William S. Noble category:q-bio.MN stat.ML published:2014-10-29 summary:Liquid chromatography coupled with tandem mass spectrometry, also known asshotgun proteomics, is a widely-used high-throughput technology for identifyingproteins in complex biological samples. Analysis of the tens of thousands offragmentation spectra produced by a typical shotgun proteomics experimentbegins by assigning to each observed spectrum the peptide hypothesized to beresponsible for generating the spectrum, typically done by searching eachspectrum against a database of peptides. We have recently described a machinelearning method---Dynamic Bayesian Network for Rapid Identification of Peptides(DRIP)---that not only achieves state-of-the-art spectrum identificationperformance on a variety of datasets but also provides a trainable modelcapable of returning valuable auxiliary information regarding specificpeptide-spectrum matches. In this work, we present two significant improvementsto DRIP. First, we describe how to use word lattices, which are widely used innatural language processing, to significantly speed up DRIP's computations. Toour knowledge, all existing shotgun proteomics search engines computeindependent scores between a given observed spectrum and each possiblecandidate peptide from the database. The key idea of the word lattice is torepresent the set of candidate peptides in a single data structure, therebyallowing sharing of redundant computations among the different candidates. Wedemonstrate that using lattices in conjunction with DRIP leads to speedups onthe order of tens across yeast and worm data sets. Second, we introduce avariant of DRIP that uses a discriminative training framework, performingmaximum mutual entropy estimation rather than maximum likelihood estimation.This modification improves DRIP's statistical power, enabling us to increasethe number of identified spectrum at a 1% false discovery rate on yeast andworm data sets.
arxiv-7800-168 | A Direct Estimation of High Dimensional Stationary Vector Autoregressions | http://arxiv.org/pdf/1307.0293v3.pdf | author:Fang Han, Huanran Lu, Han Liu category:stat.ML published:2013-07-01 summary:The vector autoregressive (VAR) model is a powerful tool in modeling complextime series and has been exploited in many fields. However, fitting highdimensional VAR model poses some unique challenges: On one hand, thedimensionality, caused by modeling a large number of time series and higherorder autoregressive processes, is usually much higher than the time serieslength; On the other hand, the temporal dependence structure in the VAR modelgives rise to extra theoretical challenges. In high dimensions, one popularapproach is to assume the transition matrix is sparse and fit the VAR modelusing the "least squares" method with a lasso-type penalty. In this manuscript,we propose an alternative way in estimating the VAR model. The main idea is,via exploiting the temporal dependence structure, to formulate the estimatingproblem into a linear program. There is instant advantage for the proposedapproach over the lasso-type estimators: The estimation equation can bedecomposed into multiple sub-equations and accordingly can be efficientlysolved in a parallel fashion. In addition, our method brings new theoreticalinsights into the VAR model analysis. So far the theoretical results developedin high dimensions (e.g., Song and Bickel (2011) and Kock and Callot (2012))mainly pose assumptions on the design matrix of the formulated regressionproblems. Such conditions are indirect about the transition matrices and nottransparent. In contrast, our results show that the operator norm of thetransition matrices plays an important role in estimation accuracy. We provideexplicit rates of convergence for both estimation and prediction. In addition,we provide thorough experiments on both synthetic and real-world equity data toshow that there are empirical advantages of our method over the lasso-typeestimators in both parameter estimation and forecasting.
arxiv-7800-169 | A Markov Decision Process Analysis of the Cold Start Problem in Bayesian Information Filtering | http://arxiv.org/pdf/1410.7852v1.pdf | author:Xiaoting Zhao, Peter I. Frazier category:cs.LG cs.IR math.OC published:2014-10-29 summary:We consider the information filtering problem, in which we face a stream ofitems, and must decide which ones to forward to a user to maximize the numberof relevant items shown, minus a penalty for each irrelevant item shown.Forwarding decisions are made separately in a personalized way for each user.We focus on the cold-start setting for this problem, in which we have limitedhistorical data on the user's preferences, and must rely on feedback fromforwarded articles to learn which the fraction of items relevant to the user ineach of several item categories. Performing well in this setting requirestrading exploration vs. exploitation, forwarding items that are likely to beirrelevant, to allow learning that will improve later performance. In aBayesian setting, and using Markov decision processes, we show how theBayes-optimal forwarding algorithm can be computed efficiently when the userwill examine each forwarded article, and how an upper bound on theBayes-optimal procedure and a heuristic index policy can be obtained for thesetting when the user will examine only a limited number of forwarded items. Wepresent results from simulation experiments using parameters estimated usinghistorical data from arXiv.org.
arxiv-7800-170 | Correcting Errors in Digital Lexicographic Resources Using a Dictionary Manipulation Language | http://arxiv.org/pdf/1410.7787v1.pdf | author:David Zajic, Michael Maxwell, David Doermann, Paul Rodrigues, Michael Bloodgood category:cs.CL published:2014-10-28 summary:We describe a paradigm for combining manual and automatic error correction ofnoisy structured lexicographic data. Modifications to the structure andunderlying text of the lexicographic data are expressed in a simple,interpreted programming language. Dictionary Manipulation Language (DML)commands identify nodes by unique identifiers, and manipulations are performedusing simple commands such as create, move, set text, etc. Corrected lexiconsare produced by applying sequences of DML commands to the source version of thelexicon. DML commands can be written manually to repair one-off errors orgenerated automatically to correct recurring problems. We discuss advantages ofthe paradigm for the task of editing digital bilingual dictionaries.
arxiv-7800-171 | Graphical LASSO Based Model Selection for Time Series | http://arxiv.org/pdf/1410.1184v3.pdf | author:Alexander Jung, Gabor Hannak, Norbert Görtz category:stat.ML published:2014-10-05 summary:We propose a novel graphical model selection (GMS) scheme forhigh-dimensional stationary time series or discrete time process. The method isbased on a natural generalization of the graphical LASSO (gLASSO), introducedoriginally for GMS based on i.i.d. samples, and estimates the conditionalindependence graph (CIG) of a time series from a finite length observation. ThegLASSO for time series is defined as the solution of an l1-regularized maximum(approximate) likelihood problem. We solve this optimization problem using thealternating direction method of multipliers (ADMM). Our approach isnonparametric as we do not assume a finite dimensional (e.g., anautoregressive) parametric model for the observed process. Instead, we requirethe process to be sufficiently smooth in the spectral domain. For Gaussianprocesses, we characterize the performance of our method theoretically byderiving an upper bound on the probability that our algorithm fails tocorrectly identify the CIG. Numerical experiments demonstrate the ability ofour method to recover the correct CIG from a limited amount of samples.
arxiv-7800-172 | A hierarchical framework for object recognition | http://arxiv.org/pdf/1410.7762v1.pdf | author:Reza Moazzezi category:cs.CV published:2014-10-28 summary:Object recognition in the presence of background clutter and distractors is acentral problem both in neuroscience and in machine learning. However, theperformance level of the models that are inspired by cortical mechanisms,including deep networks such as convolutional neural networks and deep beliefnetworks, is shown to significantly decrease in the presence of noise andbackground objects [19, 24]. Here we develop a computational framework that ishierarchical, relies heavily on key properties of the visual cortex includingmid-level feature selectivity in visual area V4 and Inferotemporal cortex (IT)[4, 9, 12, 18], high degrees of selectivity and invariance in IT [13, 17, 18]and the prior knowledge that is built into cortical circuits (such as theemergence of edge detector neurons in primary visual cortex before the onset ofthe visual experience) [1, 21], and addresses the problem of object recognitionin the presence of background noise and distractors. Our approach isspecifically designed to address large deformations, allows flexiblecommunication between different layers of representation and learns highlyselective filters from a small number of training examples.
arxiv-7800-173 | On the Computational Efficiency of Training Neural Networks | http://arxiv.org/pdf/1410.1141v2.pdf | author:Roi Livni, Shai Shalev-Shwartz, Ohad Shamir category:cs.LG cs.AI stat.ML published:2014-10-05 summary:It is well-known that neural networks are computationally hard to train. Onthe other hand, in practice, modern day neural networks are trained efficientlyusing SGD and a variety of tricks that include different activation functions(e.g. ReLU), over-specification (i.e., train networks which are larger thanneeded), and regularization. In this paper we revisit the computationalcomplexity of training neural networks from a modern perspective. We provideboth positive and negative results, some of them yield new provably efficientand practical algorithms for training certain types of neural networks.
arxiv-7800-174 | New similarity index based on entropy and group theory | http://arxiv.org/pdf/1410.7730v1.pdf | author:Yasel Garcés, Esley Torres, Osvaldo Pereira, Roberto Rodríguez category:cs.CV published:2014-10-28 summary:In this work, we propose a new similarity index for images considering theentropy function and group theory. This index considers an algebraic group ofimages, it is defined by an inner law that provides a novel approach for thesubtraction of images. Through an equivalence relationship in the field ofimages, we prove the existence of the quotient group, on which the newsimilarity index is defined. We also present the main properties of the newindex, and the immediate application thereof as a stopping criterion of the"Mean Shift Iterative Algorithm".
arxiv-7800-175 | Finite Element Based Tracking of Deforming Surfaces | http://arxiv.org/pdf/1306.4478v3.pdf | author:Stefanie Wuhrer, Jochen Lang, Motahareh Tekieh, Chang Shu category:cs.CV cs.GR published:2013-06-19 summary:We present an approach to robustly track the geometry of an object thatdeforms over time from a set of input point clouds captured from a singleviewpoint. The deformations we consider are caused by applying forces to knownlocations on the object's surface. Our method combines the use of priorinformation on the geometry of the object modeled by a smooth template and theuse of a linear finite element method to predict the deformation. This allowsthe accurate reconstruction of both the observed and the unobserved sides ofthe object. We present tracking results for noisy low-quality point cloudsacquired by either a stereo camera or a depth camera, and simulations withpoint clouds corrupted by different error terms. We show that our method isalso applicable to large non-linear deformations.
arxiv-7800-176 | Anomaly Detection Framework Using Rule Extraction for Efficient Intrusion Detection | http://arxiv.org/pdf/1410.7709v1.pdf | author:Antti Juvonen, Tuomo Sipola category:cs.LG cs.CR published:2014-10-28 summary:Huge datasets in cyber security, such as network traffic logs, can beanalyzed using machine learning and data mining methods. However, the amount ofcollected data is increasing, which makes analysis more difficult. Many machinelearning methods have not been designed for big datasets, and consequently areslow and difficult to understand. We address the issue of efficient networktraffic classification by creating an intrusion detection framework thatapplies dimensionality reduction and conjunctive rule extraction. The systemcan perform unsupervised anomaly detection and use this information to createconjunctive rules that classify huge amounts of traffic in real time. We testthe implemented system with the widely used KDD Cup 99 dataset and real-worldnetwork logs to confirm that the performance is satisfactory. This system istransparent and does not work like a black box, making it intuitive for domainexperts, such as network administrators.
arxiv-7800-177 | Non-convex Robust PCA | http://arxiv.org/pdf/1410.7660v1.pdf | author:Praneeth Netrapalli, U N Niranjan, Sujay Sanghavi, Animashree Anandkumar, Prateek Jain category:cs.IT cs.LG math.IT stat.ML published:2014-10-28 summary:We propose a new method for robust PCA -- the task of recovering a low-rankmatrix from sparse corruptions that are of unknown value and support. Ourmethod involves alternating between projecting appropriate residuals onto theset of low-rank matrices, and the set of sparse matrices; each projection is{\em non-convex} but easy to compute. In spite of this non-convexity, weestablish exact recovery of the low-rank matrix, under the same conditions thatare required by existing methods (which are based on convex optimization). Foran $m \times n$ input matrix ($m \leq n)$, our method has a running time of$O(r^2mn)$ per iteration, and needs $O(\log(1/\epsilon))$ iterations to reachan accuracy of $\epsilon$. This is close to the running time of simple PCA viathe power method, which requires $O(rmn)$ per iteration, and$O(\log(1/\epsilon))$ iterations. In contrast, existing methods for robust PCA,which are based on convex optimization, have $O(m^2n)$ complexity periteration, and take $O(1/\epsilon)$ iterations, i.e., exponentially moreiterations for the same accuracy. Experiments on both synthetic and real data establishes the improved speedand accuracy of our method over existing convex implementations.
arxiv-7800-178 | A Short Image Series Based Scheme for Time Series Digital Image Correlation | http://arxiv.org/pdf/1410.7613v1.pdf | author:Xian Wang, Shaopeng Ma category:physics.optics cs.CV 78Mxx published:2014-10-28 summary:A new scheme for digital image correlation, i.e., short time series DIC(STS-DIC) is proposed. Instead of processing the original deformed speckleimages individually, STS-DIC combines several adjacent deformed speckle imagesfrom a short time series and then processes the averaged image, for whichdeformation continuity over time is introduced. The deformation of severaladjacent images is assumed to be linear in time and a new spatial-temporaldisplacement representation method with eight unknowns is presented based onthe subset-based representation method. Then, the model of STS-DIC is createdand a solving scheme is developed based on the Newton-Raphson iteration. Theproposed method is verified for numerical and experimental cases. The resultsshow that the proposed STS-DIC greatly improves the accuracy of traditionalDIC, both under simple and complicated deformation conditions, while retainingacceptable actual computational cost.
arxiv-7800-179 | Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation | http://arxiv.org/pdf/1311.3494v6.pdf | author:Ohad Shamir category:cs.LG stat.ML published:2013-11-14 summary:Many machine learning approaches are characterized by information constraintson how they interact with the training data. These include memory andsequential access constraints (e.g. fast first-order methods to solvestochastic optimization problems); communication constraints (e.g. distributedlearning); partial access to the underlying data (e.g. missing features andmulti-armed bandits) and more. However, currently we have little understandinghow such information constraints fundamentally affect our performance,independent of the learning problem semantics. For example, are there learningproblems where any algorithm which has small memory footprint (or can use anybounded number of bits from each example, or has certain communicationconstraints) will perform worse than what is possible without such constraints?In this paper, we describe how a single set of results implies positive answersto the above, for several different settings.
arxiv-7800-180 | Fast Algorithms for Online Stochastic Convex Programming | http://arxiv.org/pdf/1410.7596v1.pdf | author:Shipra Agrawal, Nikhil R. Devanur category:cs.LG cs.DS math.OC F.1.2; G.1.6 published:2014-10-28 summary:We introduce the online stochastic Convex Programming (CP) problem, a verygeneral version of stochastic online problems which allows arbitrary concaveobjectives and convex feasibility constraints. Many well-studied problems likeonline stochastic packing and covering, online stochastic matching with concavereturns, etc. form a special case of online stochastic CP. We present fastalgorithms for these problems, which achieve near-optimal regret guarantees forboth the i.i.d. and the random permutation models of stochastic inputs. Whenapplied to the special case online packing, our ideas yield a simpler andfaster primal-dual algorithm for this well studied problem, which achieves theoptimal competitive ratio. Our techniques make explicit the connection ofprimal-dual paradigm and online learning to online stochastic CP.
arxiv-7800-181 | Robust Piecewise-Constant Smoothing: M-Smoother Revisited | http://arxiv.org/pdf/1410.7580v1.pdf | author:Linchao Bao, Qingxiong Yang category:cs.CV published:2014-10-28 summary:A robust estimator, namely M-smoother, for piecewise-constant smoothing isrevisited in this paper. Starting from its generalized formulation, we proposea numerical scheme/framework for solving it via a series of weighted-averagefiltering (e.g., box filtering, Gaussian filtering, bilateral filtering, andguided filtering). Because of the equivalence between M-smoother andlocal-histogram-based filters (such as median filter and mode filter), theproposed framework enables fast approximation of histogram filters via a numberof box filtering or Gaussian filtering. In addition, high-qualitypiecewise-constant smoothing can be achieved via a number of bilateralfiltering or guided filtering integrated in the proposed framework. Experimentson depth map denoising show the effectiveness of our framework.
arxiv-7800-182 | Learning deep dynamical models from image pixels | http://arxiv.org/pdf/1410.7550v1.pdf | author:Niklas Wahlström, Thomas B. Schön, Marc Peter Deisenroth category:stat.ML cs.LG cs.NE cs.SY published:2014-10-28 summary:Modeling dynamical systems is important in many disciplines, e.g., control,robotics, or neurotechnology. Commonly the state of these systems is notdirectly observed, but only available through noisy and potentiallyhigh-dimensional observations. In these cases, system identification, i.e.,finding the measurement mapping and the transition mapping (system dynamics) inlatent space can be challenging. For linear system dynamics and measurementmappings efficient solutions for system identification are available. However,in practical applications, the linearity assumptions does not hold, requiringnon-linear system identification techniques. If additionally the observationsare high-dimensional (e.g., images), non-linear system identification isinherently hard. To address the problem of non-linear system identificationfrom high-dimensional observations, we combine recent advances in deep learningand system identification. In particular, we jointly learn a low-dimensionalembedding of the observation by means of deep auto-encoders and a predictivetransition model in this low-dimensional space. We demonstrate that our modelenables learning good predictive models of dynamical systems from pixelinformation only.
arxiv-7800-183 | Practical Matrix Completion and Corruption Recovery using Proximal Alternating Robust Subspace Minimization | http://arxiv.org/pdf/1309.1539v2.pdf | author:Yu-Xiang Wang, Choon Meng Lee, Loong-Fah Cheong, Kim-Chuan Toh category:cs.CV published:2013-09-06 summary:Low-rank matrix completion is a problem of immense practical importance.Recent works on the subject often use nuclear norm as a convex surrogate of therank function. Despite its solid theoretical foundation, the convex version ofthe problem often fails to work satisfactorily in real-life applications. Realdata often suffer from very few observations, with support not meeting therandom requirements, ubiquitous presence of noise and potentially grosscorruptions, sometimes with these simultaneously occurring. This paper proposes a Proximal Alternating Robust Subspace Minimization(PARSuMi) method to tackle the three problems. The proximal alternating schemeexplicitly exploits the rank constraint on the completed matrix and uses the$\ell_0$ pseudo-norm directly in the corruption recovery step. We show that theproposed method for the non-convex and non-smooth model converges to astationary point. Although it is not guaranteed to find the global optimalsolution, in practice we find that our algorithm can typically arrive at a goodlocal minimizer when it is supplied with a reasonably good starting point basedon convex optimization. Extensive experiments with challenging synthetic andreal data demonstrate that our algorithm succeeds in a much larger range ofpractical problems where convex optimization fails, and it also outperformsvarious state-of-the-art algorithms.
arxiv-7800-184 | The Price of Privacy in Untrusted Recommendation Engines | http://arxiv.org/pdf/1207.3269v2.pdf | author:Siddhartha Banerjee, Nidhi Hegde, Laurent Massoulié category:cs.LG cs.IT math.IT published:2012-07-13 summary:Recent increase in online privacy concerns prompts the following question:can a recommender system be accurate if users do not entrust it with theirprivate data? To answer this, we study the problem of learning item-clustersunder local differential privacy, a powerful, formal notion of data privacy. Wedevelop bounds on the sample-complexity of learning item-clusters fromprivatized user inputs. Significantly, our results identify a sample-complexityseparation between learning in an information-rich and an information-scarceregime, thereby highlighting the interaction between privacy and the amount ofinformation (ratings) available to each user. In the information-rich regime, where each user rates at least a constantfraction of items, a spectral clustering approach is shown to achieve asample-complexity lower bound derived from a simple information-theoreticargument based on Fano's inequality. However, the information-scarce regime,where each user rates only a vanishing fraction of items, is found to require afundamentally different approach both for lower bounds and algorithms. To thisend, we develop new techniques for bounding mutual information under a notionof channel-mismatch, and also propose a new algorithm, MaxSense, and show thatit achieves optimal sample-complexity in this setting. The techniques we develop for bounding mutual information may be of broaderinterest. To illustrate this, we show their applicability to $(i)$ learningbased on 1-bit sketches, and $(ii)$ adaptive learning, where queries can beadapted based on answers to past queries.
arxiv-7800-185 | The Falling Factorial Basis and Its Statistical Applications | http://arxiv.org/pdf/1405.0558v2.pdf | author:Yu-Xiang Wang, Alex Smola, Ryan J. Tibshirani category:stat.ML published:2014-05-03 summary:We study a novel spline-like basis, which we name the "falling factorialbasis", bearing many similarities to the classic truncated power basis. Theadvantage of the falling factorial basis is that it enables rapid, linear-timecomputations in basis matrix multiplication and basis matrix inversion. Thefalling factorial functions are not actually splines, but are close enough tosplines that they provably retain some of the favorable properties of thelatter functions. We examine their application in two problems: trend filteringover arbitrary input points, and a higher-order variant of the two-sampleKolmogorov-Smirnov test.
arxiv-7800-186 | Fast Function to Function Regression | http://arxiv.org/pdf/1410.7414v1.pdf | author:Junier Oliva, Willie Neiswanger, Barnabas Poczos, Eric Xing, Jeff Schneider category:stat.ML cs.LG published:2014-10-27 summary:We analyze the problem of regression when both input covariates and outputresponses are functions from a nonparametric function class. Function tofunction regression (FFR) covers a large range of interesting applicationsincluding time-series prediction problems, and also more general tasks likestudying a mapping between two separate types of distributions. However,previous nonparametric estimators for FFR type problems scale badlycomputationally with the number of input/output pairs in a data-set. Given thecomplexity of a mapping between general functions it may be necessary toconsider large data-sets in order to achieve a low estimation risk. To addressthis issue, we develop a novel scalable nonparametric estimator, theTriple-Basis Estimator (3BE), which is capable of operating over datasets withmany instances. To the best of our knowledge, the 3BE is the firstnonparametric FFR estimator that can scale to massive datasets. We analyze the3BE's risk and derive an upperbound rate. Furthermore, we show an improvementof several orders of magnitude in terms of prediction speed and a reduction inerror over previous estimators in various real-world data-sets.
arxiv-7800-187 | Feature Selection through Minimization of the VC dimension | http://arxiv.org/pdf/1410.7372v1.pdf | author:Jayadeva, Sanjit S. Batra, Siddharth Sabharwal category:cs.LG I.5.1; I.5.2 published:2014-10-27 summary:Feature selection involes identifying the most relevant subset of inputfeatures, with a view to improving generalization of predictive models byreducing overfitting. Directly searching for the most relevant combination ofattributes is NP-hard. Variable selection is of critical importance in manyapplications, such as micro-array data analysis, where selecting a small numberof discriminative features is crucial to developing useful models of diseasemechanisms, as well as for prioritizing targets for drug discovery. Therecently proposed Minimal Complexity Machine (MCM) provides a way to learn ahyperplane classifier by minimizing an exact (\boldmath{$\Theta$}) bound on itsVC dimension. It is well known that a lower VC dimension contributes to goodgeneralization. For a linear hyperplane classifier in the input space, the VCdimension is upper bounded by the number of features; hence, a linearclassifier with a small VC dimension is parsimonious in the set of features itemploys. In this paper, we use the linear MCM to learn a classifier in which alarge number of weights are zero; features with non-zero weights are the onesthat are chosen. Selected features are used to learn a kernel SVM classifier.On a number of benchmark datasets, the features chosen by the linear MCM yieldcomparable or better test set accuracy than when methods such as ReliefF andFCBF are used for the task. The linear MCM typically chooses one-tenth thenumber of attributes chosen by the other methods; on some very high dimensionaldatasets, the MCM chooses about $0.6\%$ of the features; in comparison, ReliefFand FCBF choose 70 to 140 times more features, thus demonstrating thatminimizing the VC dimension may provide a new, and very effective route forfeature selection and for learning sparse representations.
arxiv-7800-188 | A General Statistic Framework for Genome-based Disease Risk Prediction | http://arxiv.org/pdf/1410.7371v1.pdf | author:L. Ma, N. Lin, C. I. Amos, M. M. Xiong category:stat.ML published:2014-10-27 summary:Advances of modern sensing and sequencing technologies generate a deluge ofhigh dimensional space-temporal physiological and next-generation sequencing(NGS) data. Physiological traits are observed either as continuous randomfunctions, or on a dense grid and referred to as function-valued traits. Bothphysiological and NGS data are highly correlated data with their inherentorder, spacing, and functional nature which are ignored by traditionalsummary-based univariate and multivariate regression methods designed forquantitative genetic analysis of scalar trait and common variants. To capturemorphological and dynamic features of the data and utilize their dependentstructure, we propose a functional linear model (FLM) in which a trait curve ismodeled as a response function, the genetic variation in a genomic region orgene is modeled as a functional predictor, and the genetic effects are modeledas a function of both time and genomic position (FLMF) for genetic analysis offunction-valued trait with both GWAS and NGS data. By extensive simulations, wedemonstrate that the FLMF has the correct type 1 error rates and much higherpower to detect association than the existing methods. The FLMF is applied tosleep data from Starr County health studies where oxygen saturation weremeasured in 22,670 seconds on average for 833 individuals. We found 65 genesthat were significantly associated with oxygen saturation functional trait withP-values ranging from 2.40E-06 to 2.53E-21. The results clearly demonstratethat the FLMF substantially outperforms the traditional genetic models withscalar trait.
arxiv-7800-189 | Topology Adaptive Graph Estimation in High Dimensions | http://arxiv.org/pdf/1410.7279v1.pdf | author:Johannes Lederer, Christian Müller category:stat.ML stat.ME published:2014-10-27 summary:We introduce Graphical TREX (GTREX), a novel method for graph estimation inhigh-dimensional Gaussian graphical models. By conducting neighborhoodselection with TREX, GTREX avoids tuning parameters and is adaptive to thegraph topology. We compare GTREX with standard methods on a new simulationset-up that is designed to assess accurately the strengths and shortcomings ofdifferent methods. These simulations show that a neighborhood selection schemebased on Lasso and an optimal (in practice unknown) tuning parameteroutperforms other standard methods over a large spectrum of scenarios.Moreover, we show that GTREX can rival this scheme and, therefore, can providecompetitive graph estimation without the need for tuning parameter calibration.
arxiv-7800-190 | An Unsupervised Ensemble-based Markov Random Field Approach to Microscope Cell Image Segmentation | http://arxiv.org/pdf/1410.7265v1.pdf | author:Balint Antal, Bence Remenyik, Andras Hajdu category:cs.CV cs.AI q-bio.QM published:2014-10-27 summary:In this paper, we propose an approach to the unsupervised segmentation ofimages using Markov Random Field. The proposed approach is based on the idea ofBit Plane Slicing. We use the planes as initial labellings for an ensemble ofsegmentations. With pixelwise voting, a robust segmentation approach can beachieved, which we demonstrate on microscope cell images. We tested ourapproach on a publicly available database, where it proven to be competitivewith other methods and manual segmentation.
arxiv-7800-191 | Iris Biometric System using a hybrid approach | http://arxiv.org/pdf/1410.7252v1.pdf | author:Abhimanyu Sarin, Dr. Jagadish Nayak category:cs.CV 47G20 published:2014-10-27 summary:Iris Recognition Systems are ocular- based biometric devices used primarilyfor security reasons. The complexity and the randomness of the Iris, amongstvarious other factors, ensure that this biometric system is inarguably an exactand reliable method of identification. The algorithm is responsible forautomatic localization and segmentation of boundaries using circular HoughTransform, noise reductions, image enhancement and feature extraction acrossnumerous distinct images present in the database. This paper delves into thevarious kinds of techniques required to approximate the pupillary and limbicboundaries of the enrolled iris image, captured using a suitable imageacquisition device and perform feature extraction on the normalized iris imagewith the help of Haar Wavelets to encode the input data into a binary stringformat. These techniques were validated using images from the CASIA database,and various other procedures were also tried and tested.
arxiv-7800-192 | A Greedy Homotopy Method for Regression with Nonconvex Constraints | http://arxiv.org/pdf/1410.7241v1.pdf | author:Fabian L. Wauthier, Peter Donnelly category:stat.ML stat.ME published:2014-10-27 summary:Constrained least squares regression is an essential tool forhigh-dimensional data analysis. Given a partition $\mathcal{G}$ of inputvariables, this paper considers a particular class of nonconvex constraintfunctions that encourage the linear model to select a small number of variablesfrom a small number of groups in $\mathcal{G}$. Such constraints are relevantin many practical applications, such as Genome-Wide Association Studies (GWAS).Motivated by the efficiency of the Lasso homotopy method, we present RepLasso,a greedy homotopy algorithm that tries to solve the induced sequence ofnonconvex problems by solving a sequence of suitably adapted convex surrogateproblems. We prove that in some situations RepLasso recovers the global minimaof the nonconvex problem. Moreover, even if it does not recover global minima,we prove that in relevant cases it will still do no worse than the Lasso interms of support and signed support recovery, while in practice outperformingit. We show empirically that the strategy can also be used to improve overother Lasso-style algorithms. Finally, a GWAS of ankylosing spondylitishighlights our method's practical utility.
arxiv-7800-193 | Offline Handwritten MODI Character Recognition Using HU, Zernike Moments and Zoning | http://arxiv.org/pdf/1406.6140v4.pdf | author:Sadanand A. Kulkarni, Prashant L. Borde, Ramesh R. Manza, Pravin L. Yannawar category:cs.CV published:2014-06-24 summary:HOCR is abbreviated as Handwritten Optical Character Recognition. HOCR is aprocess of recognition of different handwritten characters from a digital imageof documents. Handwritten automatic character recognition has attracted manyresearchers all over the world to contribute handwritten character recognitiondomain. Shape identification and feature extraction is very important part ofany character recognition system and success of method is highly dependent onselection of features. However feature extraction is the most important step indefining the shape of the character as precisely and as uniquely as possible.This is indeed the most important step and complex task as well and achievedsuccess by using invariance property, irrespective of position and orientation.Zernike moments describes shape, identify rotation invariant due to itsOrthogonality property. MODI is an ancient script of India had cursive andcomplex representation of characters. The work described in this paper presentsefficiency of Zernike moments over Hu 7 moment with zoning for automaticrecognition of handwritten MODI characters. Offline approach is used in thispaper because MODI Script was very popular and widely used for writing purposetill 19th century before Devanagari was officially adopted.
arxiv-7800-194 | Recognition of Handwritten MODI Numerals using Hu and Zernike features | http://arxiv.org/pdf/1404.1151v3.pdf | author:Sadanand A. Kulkarni, Prashant L. Borde, Ramesh R. Manza, Pravin L. Yannawar category:cs.CV published:2014-04-04 summary:Handwritten automatic character recognition has attracted many researchersall over the world to contribute automatic character recognition domain. Shapeidentification and feature extraction is very important part of any characterrecognition system and success of method is highly dependent on selection offeatures. However feature extraction is the most important step in defining theshape of the character as precisely and as uniquely as possible. This is indeedthe most important step and complex task as well and achieved success by usinginvariance property, irrespective of position and orientation. Zernike momentsdescribes shape, identify rotation invariant due to its Orthogonality property.MODI is an ancient script of India had cursive and complex representation ofcharacters. The work described in this paper presents efficiency of Zernikemoments over Hus moment for automatic recognition of handwritten MODI numerals.
arxiv-7800-195 | A method for context-based adaptive QRS clustering in real-time | http://arxiv.org/pdf/1410.7211v1.pdf | author:Daniel Castro, Paulo Félix, Jesús Presedo category:cs.CV physics.med-ph published:2014-10-27 summary:Continuous follow-up of heart condition through long-term electrocardiogrammonitoring is an invaluable tool for diagnosing some cardiac arrhythmias. Insuch context, providing tools for fast locating alterations of normalconduction patterns is mandatory and still remains an open issue. This workpresents a real-time method for adaptive clustering QRS complexes frommultilead ECG signals that provides the set of QRS morphologies that appearduring an ECG recording. The method processes the QRS complexes sequentially,grouping them into a dynamic set of clusters based on the information contentof the temporal context. The clusters are represented by templates which evolveover time and adapt to the QRS morphology changes. Rules to create, merge andremove clusters are defined along with techniques for noise detection in orderto avoid their proliferation. To cope with beat misalignment, DerivativeDynamic Time Warping is used. The proposed method has been validated againstthe MIT-BIH Arrhythmia Database and the AHA ECG Database showing a globalpurity of 98.56% and 99.56%, respectively. Results show that our proposal notonly provides better results than previous offline solutions but also fulfillsreal-time requirements.
arxiv-7800-196 | Predicting Parameters in Deep Learning | http://arxiv.org/pdf/1306.0543v2.pdf | author:Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando de Freitas category:cs.LG cs.NE stat.ML published:2013-06-03 summary:We demonstrate that there is significant redundancy in the parameterizationof several deep learning models. Given only a few weight values for eachfeature it is possible to accurately predict the remaining values. Moreover, weshow that not only can the parameter values be predicted, but many of them neednot be learned at all. We train several different architectures by learningonly a small number of weights and predicting the rest. In the best case we areable to predict more than 95% of the weights of a network without any drop inaccuracy.
arxiv-7800-197 | Analysis of Named Entity Recognition and Linking for Tweets | http://arxiv.org/pdf/1410.7182v1.pdf | author:Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Marieke van Erp, Genevieve Gorrell, Raphaël Troncy, Johann Petrak, Kalina Bontcheva category:cs.CL published:2014-10-27 summary:Applying natural language processing for mining and intelligent informationaccess to tweets (a form of microblog) is a challenging, emerging researcharea. Unlike carefully authored news text and other longer content, tweets posea number of new challenges, due to their short, noisy, context-dependent, anddynamic nature. Information extraction from tweets is typically performed in apipeline, comprising consecutive stages of language identification,tokenisation, part-of-speech tagging, named entity recognition and entitydisambiguation (e.g. with respect to DBpedia). In this work, we describe a newTwitter entity disambiguation dataset, and conduct an empirical analysis ofnamed entity recognition and disambiguation, investigating how robust a numberof state-of-the-art systems are on such noisy texts, what the main sources oferror are, and which problems should be further investigated to improve thestate of the art.
arxiv-7800-198 | Directional Bilateral Filters | http://arxiv.org/pdf/1410.7164v1.pdf | author:Manasij Venkatesh, Chandra Sekhar Seelamantula category:cs.CV published:2014-10-27 summary:We propose a bilateral filter with a locally controlled domain kernel fordirectional edge-preserving smoothing. Traditional bilateral filters use arange kernel, which is responsible for edge preservation, and a fixed domainkernel that performs smoothing. Our intuition is that orientation andanisotropy of image structures should be incorporated into the domain kernelwhile smoothing. For this purpose, we employ an oriented Gaussian domain kernellocally controlled by a structure tensor. The oriented domain kernel combinedwith a range kernel forms the directional bilateral filter. The two kernelsassist each other in effectively suppressing the influence of the outlierswhile smoothing. To find the optimal parameters of the directional bilateralfilter, we propose the use of Stein's unbiased risk estimate (SURE). We testthe capabilities of the kernels separately as well as together, first onsynthetic images, and then on real endoscopic images. The directional bilateralfilter has better denoising performance than the Gaussian bilateral filter atvarious noise levels in terms of peak signal-to-noise ratio (PSNR).
arxiv-7800-199 | Efficient MRF Energy Propagation for Video Segmentation via Bilateral Filters | http://arxiv.org/pdf/1301.5356v3.pdf | author:Ozan Sener, Kemal Ugur, A. Aydin Alatan category:cs.CV published:2013-01-22 summary:Segmentation of an object from a video is a challenging task in multimediaapplications. Depending on the application, automatic or interactive methodsare desired; however, regardless of the application type, efficient computationof video object segmentation is crucial for time-critical applications;specifically, mobile and interactive applications require near real-timeefficiencies. In this paper, we address the problem of video segmentation fromthe perspective of efficiency. We initially redefine the problem of videoobject segmentation as the propagation of MRF energies along the temporaldomain. For this purpose, a novel and efficient method is proposed to propagateMRF energies throughout the frames via bilateral filters without using anyglobal texture, color or shape model. Recently presented bi-exponential filteris utilized for efficiency, whereas a novel technique is also developed todynamically solve graph-cuts for varying, non-lattice graphs in general linearfiltering scenario. These improvements are experimented for both automatic andinteractive video segmentation scenarios. Moreover, in addition to theefficiency, segmentation quality is also tested both quantitatively andqualitatively. Indeed, for some challenging examples, significant timeefficiency is observed without loss of segmentation quality.
arxiv-7800-200 | Estimating the intrinsic dimension in fMRI space via dataset fractal analysis - Counting the `cpu cores' of the human brain | http://arxiv.org/pdf/1410.7100v1.pdf | author:Harris V. Georgiou category:cs.AI cs.CV q-bio.NC stat.ML published:2014-10-27 summary:Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive toolfor localizing and analyzing brain activity. This study focuses on one veryimportant aspect of the functional properties of human brain, specifically theestimation of the level of parallelism when performing complex cognitive tasks.Using fMRI as the main modality, the human brain activity is investigatedthrough a purely data-driven signal processing and dimensionality analysisapproach. Specifically, the fMRI signal is treated as a multi-dimensional dataspace and its intrinsic `complexity' is studied via dataset fractal analysisand blind-source separation (BSS) methods. One simulated and two real fMRIdatasets are used in combination with Independent Component Analysis (ICA) andfractal analysis for estimating the intrinsic (true) dimensionality, in orderto provide data-driven experimental evidence on the number of independent brainprocesses that run in parallel when visual or visuo-motor tasks are performed.Although this number is can not be defined as a strict threshold but rather asa continuous range, when a specific activation level is defined, acorresponding number of parallel processes or the casual equivalent of `cpucores' can be detected in normal human brain activity.
arxiv-7800-201 | Concavity of reweighted Kikuchi approximation | http://arxiv.org/pdf/1410.7098v1.pdf | author:Po-Ling Loh, Andre Wibisono category:stat.ML math.ST stat.TH published:2014-10-26 summary:We analyze a reweighted version of the Kikuchi approximation for estimatingthe log partition function of a product distribution defined over a regiongraph. We establish sufficient conditions for the concavity of our reweightedobjective function in terms of weight assignments in the Kikuchi expansion, andshow that a reweighted version of the sum product algorithm applied to theKikuchi region graph will produce global optima of the Kikuchi approximationwhenever the algorithm converges. When the region graph has two layers,corresponding to a Bethe approximation, we show that our sufficient conditionsfor concavity are also necessary. Finally, we provide an explicitcharacterization of the polytope of concavity in terms of the cycle structureof the region graph. We conclude with simulations that demonstrate theadvantages of the reweighted Kikuchi approach.
arxiv-7800-202 | On Chord and Sagitta in ${\mathbb Z}^2$: An Analysis towards Fast and Robust Circular Arc Detection | http://arxiv.org/pdf/1411.1668v1.pdf | author:Sahadev Bera, Shyamosree Pal, Partha Bhowmick, Bhargab B. Bhattacharya category:cs.CG cs.CV published:2014-10-26 summary:Although chord and sagitta, when considered in tandem, may reflect manyunderlying geometric properties of circles on the Euclidean plane, theirimplications on the digital plane are not yet well-understood. In this paper,we explore some of their fundamental properties on the digital plane that havea strong bearing on the unsupervised detection of circles and circular arcs ina digital image. We show that although the chord-and-sagitta properties of areal circle do not readily migrate to the digital plane, they can indeed beused for the analysis in the discrete domain based on certain bounds on theirdeviations, which are derived from the real domain. In particular, we derive anupper bound on the circumferential angular deviation of a point in the contextof chord property, and an upper bound on the relative error in radiusestimation with regard to the sagitta property. Using these two bounds, wedesign a novel algorithm for the detection and parameterization of circles andcircular arcs, which does not require any heuristic initialization or manualtuning. The chord property is deployed for the detection of circular arcs,whereas the sagitta property is used to estimate their centers and radii.Finally, to improve the accuracy of estimation, the notion of restricted Houghtransform is used. Experimental results demonstrate superior efficiency androbustness of the proposed methodology compared to existing techniques.
arxiv-7800-203 | Sparse Distributed Learning via Heterogeneous Diffusion Adaptive Networks | http://arxiv.org/pdf/1410.7057v1.pdf | author:Bijit Kumar Das, Mrityunjoy Chakraborty, Jerónimo Arenas-García category:cs.LG cs.DC cs.SY stat.ML published:2014-10-26 summary:In-network distributed estimation of sparse parameter vectors via diffusionLMS strategies has been studied and investigated in recent years. In all theexisting works, some convex regularization approach has been used at each nodeof the network in order to achieve an overall network performance superior tothat of the simple diffusion LMS, albeit at the cost of increased computationaloverhead. In this paper, we provide analytical as well as experimental resultswhich show that the convex regularization can be selectively applied only tosome chosen nodes keeping rest of the nodes sparsity agnostic, while stillenjoying the same optimum behavior as can be realized by deploying the convexregularization at all the nodes. Due to the incorporation of unregularizedlearning at a subset of nodes, less computational cost is needed in theproposed approach. We also provide a guideline for selection of the sparsityaware nodes and a closed form expression for the optimum regularizationparameter.
arxiv-7800-204 | Performance Guarantees for Schatten-$p$ Quasi-Norm Minimization in Recovery of Low-Rank Matrices | http://arxiv.org/pdf/1407.3716v2.pdf | author:Mohammadreza Malek-Mohammadi, Massoud Babaie-Zadeh, Mikael Skoglund category:cs.IT math.IT stat.ML published:2014-07-14 summary:We address some theoretical guarantees for Schatten-$p$ quasi-normminimization ($p \in (0,1]$) in recovering low-rank matrices from compressedlinear measurements. Firstly, using null space properties of the measurementoperator, we provide a sufficient condition for exact recovery of low-rankmatrices. This condition guarantees unique recovery of matrices of ranks equalor larger than what is guaranteed by nuclear norm minimization. Secondly, thissufficient condition leads to a theorem proving that all restricted isometryproperty (RIP) based sufficient conditions for $\ell_p$ quasi-norm minimizationgeneralize to Schatten-$p$ quasi-norm minimization. Based on this theorem, weprovide a few RIP-based recovery conditions.
arxiv-7800-205 | Learning-Assisted Automated Reasoning with Flyspeck | http://arxiv.org/pdf/1211.7012v3.pdf | author:Cezary Kaliszyk, Josef Urban category:cs.AI cs.DL cs.LG cs.LO published:2012-11-29 summary:The considerable mathematical knowledge encoded by the Flyspeck project iscombined with external automated theorem provers (ATPs) and machine-learningpremise selection methods trained on the proofs, producing an AI system capableof answering a wide range of mathematical queries automatically. Theperformance of this architecture is evaluated in a bootstrapping scenarioemulating the development of Flyspeck from axioms to the last theorem, eachtime using only the previous theorems and proofs. It is shown that 39% of the14185 theorems could be proved in a push-button mode (without any high-leveladvice and user interaction) in 30 seconds of real time on a fourteen-CPUworkstation. The necessary work involves: (i) an implementation of soundtranslations of the HOL Light logic to ATP formalisms: untyped first-order,polymorphic typed first-order, and typed higher-order, (ii) export of thedependency information from HOL Light and ATP proofs for the machine learners,and (iii) choice of suitable representations and methods for learning fromprevious proofs, and their integration as advisors with HOL Light. This work isdescribed and discussed here, and an initial analysis of the body of proofsthat were found fully automatically is provided.
arxiv-7800-206 | A Novel Statistical Method Based on Dynamic Models for Classification | http://arxiv.org/pdf/1410.7029v1.pdf | author:Lerong Li, Momiao Xiong category:stat.ML published:2014-10-26 summary:Realizations of stochastic process are often observed temporal data orfunctional data. There are growing interests in classification of dynamic orfunctional data. The basic feature of functional data is that the functionaldata have infinite dimensions and are highly correlated. An essential issue forclassifying dynamic and functional data is how to effectively reduce theirdimension and explore dynamic feature. However, few statistical methods fordynamic data classification have directly used rich dynamic features of thedata. We propose to use second order ordinary differential equation (ODE) tomodel dynamic process and principal differential analysis to estimate constantor time-varying parameters in the ODE. We examine differential dynamicproperties of the dynamic system across different conditions includingstability and transient-response, which determine how the dynamic systemsmaintain their functions and performance under a broad range of random internaland external perturbations. We use the parameters in the ODE as features forclassifiers. As a proof of principle, the proposed methods are applied toclassifying normal and abnormal QRS complexes in the electrocardiogram (ECG)data analysis, which is of great clinical values in diagnosis of cardiovasculardiseases. We show that the ODE-based classification methods in QRS complexclassification outperform the currently widely used neural networks withFourier expansion coefficients of the functional data as their features. Weexpect that the dynamic model-based classification methods may open a newavenue for functional data classification.
arxiv-7800-207 | Semantic clustering of Russian web search results: possibilities and problems | http://arxiv.org/pdf/1409.1612v2.pdf | author:Andrey Kutuzov category:cs.CL cs.IR published:2014-09-04 summary:The paper deals with word sense induction from lexical co-occurrence graphs.We construct such graphs on large Russian corpora and then apply this data tocluster Mail.ru Search results according to meanings of the query. We comparedifferent methods of performing such clustering and different source corpora.Models of applying distributional semantics to big linguistic data aredescribed.
arxiv-7800-208 | A Bayes consistent 1-NN classifier | http://arxiv.org/pdf/1407.0208v2.pdf | author:Aryeh Kontorovich, Roi Weiss category:cs.LG stat.ML published:2014-07-01 summary:We show that a simple modification of the 1-nearest neighbor classifieryields a strongly Bayes consistent learner. Prior to this work, the onlystrongly Bayes consistent proximity-based method was the k-nearest neighborclassifier, for k growing appropriately with sample size. We will argue that amargin-regularized 1-NN enjoys considerable statistical and algorithmicadvantages over the k-NN classifier. These include user-friendly finite-sampleerror bounds, as well as time- and memory-efficient learning and test-pointevaluation algorithms with a principled speed-accuracy tradeoff. Encouragingempirical results are reported.
arxiv-7800-209 | Sparse Estimation using Bayesian Hierarchical Prior Modeling for Real and Complex Linear Models | http://arxiv.org/pdf/1108.4324v3.pdf | author:Niels Lovmand Pedersen, Carles Navarro Manchón, Mihai-Alin Badiu, Dmitriy Shutin, Bernard Henri Fleury category:stat.ML published:2011-08-22 summary:In sparse Bayesian learning (SBL), Gaussian scale mixtures (GSMs) have beenused to model sparsity-inducing priors that realize a class of concave penaltyfunctions for the regression task in real-valued signal models. Motivated bythe relative scarcity of formal tools for SBL in complex-valued models, thispaper proposes a GSM model - the Bessel K model - that induces concave penaltyfunctions for the estimation of complex sparse signals. The properties of theBessel K model are analyzed when it is applied to Type I and Type IIestimation. This analysis reveals that, by tuning the parameters of the mixingpdf different penalty functions are invoked depending on the estimation typeused, the value of the noise variance, and whether real or complex signals areestimated. Using the Bessel K model, we derive a sparse estimator based on amodification of the expectation-maximization algorithm formulated for Type IIestimation. The estimator includes as a special instance the algorithmsproposed by Tipping and Faul [1] and by Babacan et al. [2]. Numerical resultsshow the superiority of the proposed estimator over these state-of-the-artestimators in terms of convergence speed, sparseness, reconstruction error, androbustness in low and medium signal-to-noise ratio regimes.
arxiv-7800-210 | Improved depth imaging by constrained full-waveform inversion | http://arxiv.org/pdf/1410.6996v1.pdf | author:Musa Maharramov, Biondo Biondi category:physics.geo-ph cs.CV published:2014-10-26 summary:We propose a formulation of full-wavefield inversion (FWI) as a constrainedoptimization problem, and describe a computationally efficient technique forsolving constrained full-wavefield inversion (CFWI). The technique is based onusing a total-variation regularization method, with the regularization weightedin favor of constraining deeper subsurface model sections. The method helps topromote "edge-preserving" blocky model inversion where fitting the seismic dataalone fails to adequately constrain the model. The method is demonstrated onsynthetic datasets with added noise, and is shown to enhance the sharpness ofthe inverted model and correctly reposition mispositioned reflectors by betterconstraining the velocity model at depth.
arxiv-7800-211 | Local Rademacher Complexity for Multi-label Learning | http://arxiv.org/pdf/1410.6990v1.pdf | author:Chang Xu, Tongliang Liu, Dacheng Tao, Chao Xu category:stat.ML cs.LG published:2014-10-26 summary:We analyze the local Rademacher complexity of empirical risk minimization(ERM)-based multi-label learning algorithms, and in doing so propose a newalgorithm for multi-label learning. Rather than using the trace norm toregularize the multi-label predictor, we instead minimize the tail sum of thesingular values of the predictor in multi-label learning. Benefiting from theuse of the local Rademacher complexity, our algorithm, therefore, has a sharpergeneralization error bound and a faster convergence rate. Compared to methodsthat minimize over all singular values, concentrating on the tail singularvalues results in better recovery of the low-rank structure of the multi-labelpredictor, which plays an import role in exploiting label correlations. Wepropose a new conditional singular value thresholding algorithm to solve theresulting objective function. Empirical studies on real-world datasets validateour theoretical results and demonstrate the effectiveness of the proposedalgorithm.
arxiv-7800-212 | Fully Automated Myocardial Infarction Classification using Ordinary Differential Equations | http://arxiv.org/pdf/1410.6984v1.pdf | author:Getie Zewdie, Momiao Xiong category:stat.ML published:2014-10-26 summary:Portable, Wearable and Wireless electrocardiogram (ECG) Systems have thepotential to be used as point-of-care for cardiovascular disease diagnosticsystems. Such wearable and wireless ECG systems require automatic detection ofcardiovascular disease. Even in the primary care, automation of ECG diagnosticsystems will improve efficiency of ECG diagnosis and reduce the minimaltraining requirement of local healthcare workers. However, few fully automaticmyocardial infarction (MI) disease detection algorithms have well beendeveloped. This paper presents a novel automatic MI classification algorithmusing second order ordinary differential equation (ODE) with time varyingcoefficients, which simultaneously captures morphological and dynamic featureof highly correlated ECG signals. By effectively estimating the unobservedstate variables and the parameters of the second order ODE, the accuracy of theclassification was significantly improved. The estimated time varyingcoefficients of the second order ODE were used as an input to the supportvector machine (SVM) for the MI classification. The proposed method was appliedto the PTB diagnostic ECG database within Physionet. The overall sensitivity,specificity, and classification accuracy of 12 lead ECGs for MI binaryclassifications were 98.7%, 96.4% and 98.3%, respectively. We also found thateven using one lead ECG signals, we can reach accuracy as high as 97%.Multiclass MI classification is a challenging task but the developed ODEapproach for 12 lead ECGs coupled with multiclass SVM reached 96.4% accuracyfor classifying 5 subgroups of MI and healthy controls.
arxiv-7800-213 | Notes on using Determinantal Point Processes for Clustering with Applications to Text Clustering | http://arxiv.org/pdf/1410.6975v1.pdf | author:Apoorv Agarwal, Anna Choromanska, Krzysztof Choromanski category:cs.LG published:2014-10-26 summary:In this paper, we compare three initialization schemes for the KMEANSclustering algorithm: 1) random initialization (KMEANSRAND), 2) KMEANS++, and3) KMEANSD++. Both KMEANSRAND and KMEANS++ have a major that the value of kneeds to be set by the user of the algorithms. (Kang 2013) recently proposed anovel use of determinantal point processes for sampling the initial centroidsfor the KMEANS algorithm (we call it KMEANSD++). They, however, do not provideany evaluation establishing that KMEANSD++ is better than other algorithms. Inthis paper, we show that the performance of KMEANSD++ is comparable to KMEANS++(both of which are better than KMEANSRAND) with KMEANSD++ having an additionalthat it can automatically approximate the value of k.
arxiv-7800-214 | A Framework for On-Line Devanagari Handwritten Character Recognition | http://arxiv.org/pdf/1410.6909v1.pdf | author:Sunil Kumar Kopparapu, Lajish V. L category:cs.CV published:2014-10-25 summary:The main challenge in on-line handwritten character recognition in Indianlan- guage is the large size of the character set, larger similarity betweendifferent characters in the script and the huge variation in writing style. Inthis paper we propose a framework for on-line handwitten script recognitiontaking cues from speech signal processing literature. The framework is based onidentify- ing strokes, which in turn lead to recognition of handwritten on-linecharacters rather that the conventional character identification. Though theframework is described for Devanagari script, the framework is general and canbe applied to any language. The proposed platform consists of pre-processing, feature extraction, recog-nition and post processing like the conventional character recognition but ap-plied to strokes. The on-line Devanagari character recognition reduces to oneof recognizing one of 69 primitives and recognition of a character is performedby recognizing a sequence of such primitives. We further show the impact ofnoise removal on on-line raw data which is usually noisy. The use of FuzzyDirec- tional Features to enhance the accuracy of stroke recognition is alsodescribed. The recognition results are compared with commonly used directionalfeatures in literature using several classifiers.
arxiv-7800-215 | Modified Mel Filter Bank to Compute MFCC of Subsampled Speech | http://arxiv.org/pdf/1410.7382v1.pdf | author:Kiran Kumar Bhuvanagiri, Sunil Kumar Kopparapu category:cs.CL cs.SD published:2014-10-25 summary:Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly usedspeech features in most speech and speaker recognition applications. In thiswork, we propose a modified Mel filter bank to extract MFCCs from subsampledspeech. We also propose a stronger metric which effectively captures thecorrelation between MFCCs of original speech and MFCC of resampled speech. Itis found that the proposed method of filter bank construction performsdistinguishably well and gives recognition performance on resampled speechclose to recognition accuracies on original speech.
arxiv-7800-216 | Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech | http://arxiv.org/pdf/1410.6903v1.pdf | author:Laxmi Narayana M., Sunil Kumar Kopparapu category:cs.SD cs.CL published:2014-10-25 summary:Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly usedspeech features in most speech and speaker recognition applications. In thispaper, we study the effect of resampling a speech signal on these speechfeatures. We first derive a relationship between the MFCC param- eters of theresampled speech and the MFCC parameters of the original speech. We propose sixmethods of calculating the MFCC parameters of downsampled speech bytransforming the Mel filter bank used to com- pute MFCC of the original speech.We then experimentally compute the MFCC parameters of the down sampled speechusing the proposed meth- ods and compute the Pearson coefficient between theMFCC parameters of the downsampled speech and that of the original speech toidentify the most effective choice of Mel-filter band that enables the computedMFCC of the resampled speech to be as close as possible to the original speechsample MFCC.
arxiv-7800-217 | Screening Rules for Overlapping Group Lasso | http://arxiv.org/pdf/1410.6880v1.pdf | author:Seunghak Lee, Eric P. Xing category:stat.ML cs.LG published:2014-10-25 summary:Recently, to solve large-scale lasso and group lasso problems, screeningrules have been developed, the goal of which is to reduce the problem size byefficiently discarding zero coefficients using simple rules independently ofthe others. However, screening for overlapping group lasso remains an openchallenge because the overlaps between groups make it infeasible to test eachgroup independently. In this paper, we develop screening rules for overlappinggroup lasso. To address the challenge arising from groups with overlaps, wetake into account overlapping groups only if they are inclusive of the groupbeing tested, and then we derive screening rules, adopting the dual polytopeprojection approach. This strategy allows us to screen each group independentlyof each other. In our experiments, we demonstrate the efficiency of ourscreening rules on various datasets.
arxiv-7800-218 | Clustering Words by Projection Entropy | http://arxiv.org/pdf/1410.6830v1.pdf | author:Işık Barış Fidaner, Ali Taylan Cemgil category:cs.CL cs.LG published:2014-10-24 summary:We apply entropy agglomeration (EA), a recently introduced algorithm, tocluster the words of a literary text. EA is a greedy agglomerative procedurethat minimizes projection entropy (PE), a function that can quantify thesegmentedness of an element set. To apply it, the text is reduced to a featureallocation, a combinatorial object to represent the word occurences in thetext's paragraphs. The experiment results demonstrate that EA, despite itsreduction and simplicity, is useful in capturing significant relationshipsamong the words in the text. This procedure was implemented in Python andpublished as a free software: REBUS.
arxiv-7800-219 | On the Challenges of Physical Implementations of RBMs | http://arxiv.org/pdf/1312.5258v2.pdf | author:Vincent Dumoulin, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG published:2013-12-18 summary:Restricted Boltzmann machines (RBMs) are powerful machine learning models,but learning and some kinds of inference in the model require sampling-basedapproximations, which, in classical digital computers, are implemented usingexpensive MCMC. Physical computation offers the opportunity to reduce the costof sampling by building physical systems whose natural dynamics correspond todrawing samples from the desired RBM distribution. Such a system avoids theburn-in and mixing cost of a Markov chain. However, hardware implementations ofthis variety usually entail limitations such as low-precision and limited rangeof the parameters and restrictions on the size and topology of the RBM. Weconduct software simulations to determine how harmful each of theserestrictions is. Our simulations are designed to reproduce aspects of theD-Wave quantum computer, but the issues we investigate arise in most forms ofphysical computation.
arxiv-7800-220 | Online and Stochastic Gradient Methods for Non-decomposable Loss Functions | http://arxiv.org/pdf/1410.6776v1.pdf | author:Purushottam Kar, Harikrishna Narasimhan, Prateek Jain category:cs.LG stat.ML published:2014-10-24 summary:Modern applications in sensitive domains such as biometrics and medicinefrequently require the use of non-decomposable loss functions such asprecision@k, F-measure etc. Compared to point loss functions such ashinge-loss, these offer much more fine grained control over prediction, but atthe same time present novel challenges in terms of algorithm design andanalysis. In this work we initiate a study of online learning techniques forsuch non-decomposable loss functions with an aim to enable incremental learningas well as design scalable solvers for batch problems. To this end, we proposean online learning framework for such loss functions. Our model enjoys severalnice properties, chief amongst them being the existence of efficient onlinelearning algorithms with sublinear regret and online to batch conversionbounds. Our model is a provable extension of existing online learning modelsfor point loss functions. We instantiate two popular losses, prec@k and pAUC,in our model and prove sublinear regret bounds for both of them. Our proofsrequire a novel structural lemma over ranked lists which may be of independentinterest. We then develop scalable stochastic gradient descent solvers fornon-decomposable loss functions. We show that for a large family of lossfunctions satisfying a certain uniform convergence property (that includesprec@k, pAUC, and F-measure), our methods provably converge to the empiricalrisk minimizer. Such uniform convergence results were not known for theselosses and we establish these using novel proof techniques. We then useextensive experimentation on real life and benchmark datasets to establish thatour method can be orders of magnitude faster than a recently proposed cuttingplane method.
arxiv-7800-221 | On The Effect of Hyperedge Weights On Hypergraph Learning | http://arxiv.org/pdf/1410.6736v1.pdf | author:Sheng Huang, Ahmed Elgammal, Dan Yang category:cs.CV published:2014-10-24 summary:Hypergraph is a powerful representation in several computer vision, machinelearning and pattern recognition problems. In the last decade, many researchershave been keen to develop different hypergraph models. In contrast, no muchattention has been paid to the design of hyperedge weights. However, manystudies on pairwise graphs show that the choice of edge weight cansignificantly influence the performances of such graph algorithms. We arguethat this also applies to hypegraphs. In this paper, we empirically discuss theinfluence of hyperedge weight on hypegraph learning via proposing three novelhyperedge weights from the perspectives of geometry, multivariate statisticalanalysis and linear regression. Extensive experiments on ORL, COIL20, JAFFE,Sheffield, Scene15 and Caltech256 databases verify our hypothesis. Similar tograph learning, several representative hyperedge weighting schemes can beconcluded by our experimental studies. Moreover, the experiments alsodemonstrate that the combinations of such weighting schemes and conventionalhypergraph models can get very promising classification and clusteringperformances in comparison with some recent state-of-the-art algorithms.
arxiv-7800-222 | Probabilistic ODE Solvers with Runge-Kutta Means | http://arxiv.org/pdf/1406.2582v2.pdf | author:Michael Schober, David Duvenaud, Philipp Hennig category:stat.ML cs.LG cs.NA math.NA published:2014-06-10 summary:Runge-Kutta methods are the classic family of solvers for ordinarydifferential equations (ODEs), and the basis for the state of the art. Likemost numerical methods, they return point estimates. We construct a family ofprobabilistic numerical methods that instead return a Gauss-Markov processdefining a probability distribution over the ODE solution. In contrast to priorwork, we construct this family such that posterior means match the outputs ofthe Runge-Kutta family exactly, thus inheriting their proven good properties.Remaining degrees of freedom not identified by the match to Runge-Kutta arechosen such that the posterior probability measure fits the observed structureof the ODE. Our results shed light on the structure of Runge-Kutta solvers froma new direction, provide a richer, probabilistic output, have low computationalcost, and raise new research questions.
arxiv-7800-223 | Median Selection Subset Aggregation for Parallel Inference | http://arxiv.org/pdf/1410.6604v1.pdf | author:Xiangyu Wang, Peichao Peng, David Dunson category:stat.ML cs.DC stat.CO stat.ME published:2014-10-24 summary:For massive data sets, efficient computation commonly relies on distributedalgorithms that store and process subsets of the data on different machines,minimizing communication costs. Our focus is on regression and classificationproblems involving many features. A variety of distributed algorithms have beenproposed in this context, but challenges arise in defining an algorithm withlow communication, theoretical guarantees and excellent practical performancein general settings. We propose a MEdian Selection Subset AGgregation Estimator(message) algorithm, which attempts to solve these problems. The algorithmapplies feature selection in parallel for each subset using Lasso or anothermethod, calculates the `median' feature inclusion index, estimates coefficientsfor the selected features in parallel for each subset, and then averages theseestimates. The algorithm is simple, involves very minimal communication, scalesefficiently in both sample and feature size, and has theoretical guarantees. Inparticular, we show model selection consistency and coefficient estimationefficiency. Extensive experiments show excellent performance in variableselection, estimation, prediction, and computation time relative to usualcompetitors.
arxiv-7800-224 | A Novel Visual Word Co-occurrence Model for Person Re-identification | http://arxiv.org/pdf/1410.6532v1.pdf | author:Ziming Zhang, Yuting Chen, Venkatesh Saligrama category:cs.CV published:2014-10-24 summary:Person re-identification aims to maintain the identity of an individual indiverse locations through different non-overlapping camera views. The problemis fundamentally challenging due to appearance variations resulting fromdiffering poses, illumination and configurations of camera views. To deal withthese difficulties, we propose a novel visual word co-occurrence model. Wefirst map each pixel of an image to a visual word using a codebook, which islearned in an unsupervised manner. The appearance transformation between cameraviews is encoded by a co-occurrence matrix of visual word joint distributionsin probe and gallery images. Our appearance model naturally accounts forspatial similarities and variations caused by pose, illumination &configuration change across camera views. Linear SVMs are then trained asclassifiers using these co-occurrence descriptors. On the VIPeR and CUHK Campusbenchmark datasets, our method achieves 83.86% and 85.49% at rank-15 on theCumulative Match Characteristic (CMC) curves, and beats the state-of-the-artresults by 10.44% and 22.27%.
arxiv-7800-225 | Non-parametric Bayesian Learning with Deep Learning Structure and Its Applications in Wireless Networks | http://arxiv.org/pdf/1410.4599v2.pdf | author:Erte Pan, Zhu Han category:cs.LG cs.NE cs.NI stat.ML published:2014-10-16 summary:In this paper, we present an infinite hierarchical non-parametric Bayesianmodel to extract the hidden factors over observed data, where the number ofhidden factors for each layer is unknown and can be potentially infinite.Moreover, the number of layers can also be infinite. We construct the modelstructure that allows continuous values for the hidden factors and weights,which makes the model suitable for various applications. We use theMetropolis-Hastings method to infer the model structure. Then the performanceof the algorithm is evaluated by the experiments. Simulation results show thatthe model fits the underlying structure of simulated data.
arxiv-7800-226 | A Framework for the Volumetric Integration of Depth Images | http://arxiv.org/pdf/1410.0925v3.pdf | author:Victor Adrian Prisacariu, Olaf Kähler, Ming Ming Cheng, Carl Yuheng Ren, Julien Valentin, Philip H. S. Torr, Ian D. Reid, David W. Murray category:cs.CV published:2014-10-03 summary:Volumetric models have become a popular representation for 3D scenes inrecent years. One of the breakthroughs leading to their popularity wasKinectFusion, where the focus is on 3D reconstruction using RGB-D sensors.However, monocular SLAM has since also been tackled with very similarapproaches. Representing the reconstruction volumetrically as a truncatedsigned distance function leads to most of the simplicity and efficiency thatcan be achieved with GPU implementations of these systems. However, thisrepresentation is also memory-intensive and limits the applicability to smallscale reconstructions. Several avenues have been explored for overcoming thislimitation. With the aim of summarizing them and providing for a fast andflexible 3D reconstruction pipeline, we propose a new, unifying frameworkcalled InfiniTAM. The core idea is that individual steps like camera tracking,scene representation and integration of new data can easily be replaced andadapted to the needs of the user. Along with the framework we also provide aset of components for scalable reconstruction: two implementations of cameratrackers, based on RGB data and on depth data, two representations of the 3Dvolumetric data, a dense volume and one based on hashes of subblocks, and anoptional module for swapping subblocks in and out of the typically limited GPUmemory.
arxiv-7800-227 | Foreground-Background Segmentation Based on Codebook and Edge Detector | http://arxiv.org/pdf/1410.6472v1.pdf | author:Mikaël A. Mousse, Eugène C. Ezin, Cina Motamed category:cs.CV published:2014-10-23 summary:Background modeling techniques are used for moving object detection in video.Many algorithms exist in the field of object detection with different purposes.In this paper, we propose an improvement of moving object detection based oncodebook segmentation. We associate the original codebook algorithm with anedge detection algorithm. Our goal is to prove the efficiency of using an edgedetection algorithm with a background modeling algorithm. Throughout our study,we compared the quality of the moving object detection when codebooksegmentation algorithm is associated with some standard edge detectors. In eachcase, we use frame-based metrics for the evaluation of the detection. Thedifferent results are presented and analyzed.
arxiv-7800-228 | Density-Based Region Search with Arbitrary Shape for Object Localization | http://arxiv.org/pdf/1410.6447v1.pdf | author:Ji Zhao, Deyu Meng, Jiayi Ma category:cs.CV published:2014-10-23 summary:Region search is widely used for object localization. Typically, the regionsearch methods project the score of a classifier into an image plane, and thensearch the region with the maximal score. The recently proposed region searchmethods, such as efficient subwindow search and efficient region search, %whichlocalize objects from the score distribution on an image are much moreefficient than sliding window search. However, for some classifiers and tasks,the projected scores are nearly all positive, and hence maximizing the score ofa region results in localizing nearly the entire images as objects, which ismeaningless. In this paper, we observe that the large scores are mainly concentrated on oraround objects. Based on this observation, we propose a method, named level setmaximum-weight connected subgraph (LS-MWCS), which localizes objects witharbitrary shapes by searching regions with the densest score rather than themaximal score. The region density can be controlled by a parameter flexibly.And we prove an important property of the proposed LS-MWCS, which guaranteesthat the region with the densest score can be searched. Moreover, the LS-MWCScan be efficiently optimized by belief propagation. The method is evaluated onthe problem of weakly-supervised object localization, and the quantitativeresults demonstrate the superiorities of our LS-MWCS compared to otherstate-of-the-art methods.
arxiv-7800-229 | Enhanced Multiobjective Evolutionary Algorithm based on Decomposition for Solving the Unit Commitment Problem | http://arxiv.org/pdf/1410.4343v2.pdf | author:Anupam Trivedi, Kunal Pal, Chiranjib Saha, Dipti Srinivasan category:cs.NE 68T04 published:2014-10-16 summary:The unit commitment (UC) problem is a nonlinear, high-dimensional, highlyconstrained, mixed-integer power system optimization problem and is generallysolved in the literature considering minimizing the system operation cost asthe only objective. However, due to increasing environmental concerns, therecent attention has shifted to incorporating emission in the problemformulation. In this paper, a multi-objective evolutionary algorithm based ondecomposition (MOEA/D) is proposed to solve the UC problem as a multi-objectiveoptimization problem considering minimizing cost and emission as the multipleobjec- tives. Since, UC problem is a mixed-integer optimization problemconsisting of binary UC variables and continuous power dispatch variables, anovel hybridization strategy is proposed within the framework of MOEA/D suchthat genetic algorithm (GA) evolves the binary variables while differentialevolution (DE) evolves the continuous variables. Further, a novel non-uniformweight vector distribution strategy is proposed and a parallel island modelbased on combination of MOEA/D with uniform and non-uniform weight vectordistribution strategy is implemented to enhance the performance of thepresented algorithm. Extensive case studies are presented on different testsystems and the effectiveness of the proposed hybridization strategy, thenon-uniform weight vector distribution strategy and parallel island model isverified through stringent simulated results. Further, exhaustive benchmarkingagainst the algorithms proposed in the literature is presented to demonstratethe superiority of the proposed algorithm in obtaining significantly betterconverged and uniformly distributed trade-off solutions.
arxiv-7800-230 | Initialization of multilayer forecasting artifical neural networks | http://arxiv.org/pdf/1410.6413v1.pdf | author:Vladimir V. Bochkarev, Yulia S. Maslennikova category:cs.NE stat.ME I.5.1 published:2014-10-23 summary:In this paper, a new method was developed for initialising artificial neuralnetworks predicting dynamics of time series. Initial weighting coefficientswere determined for neurons analogously to the case of a linear predictionfilter. Moreover, to improve the accuracy of the initialization method for amultilayer neural network, some variants of decomposition of the transformationmatrix corresponding to the linear prediction filter were suggested. Theefficiency of the proposed neural network prediction method by forecastingsolutions of the Lorentz chaotic system is shown in this paper.
arxiv-7800-231 | On Lower and Upper Bounds in Smooth Strongly Convex Optimization - A Unified Approach via Linear Iterative Methods | http://arxiv.org/pdf/1410.6387v1.pdf | author:Yossi Arjevani category:math.OC cs.LG published:2014-10-23 summary:In this thesis we develop a novel framework to study smooth and stronglyconvex optimization algorithms, both deterministic and stochastic. Focusing onquadratic functions we are able to examine optimization algorithms as arecursive application of linear operators. This, in turn, reveals a powerfulconnection between a class of optimization algorithms and the analytic theoryof polynomials whereby new lower and upper bounds are derived. In particular,we present a new and natural derivation of Nesterov's well-known AcceleratedGradient Descent method by employing simple 'economic' polynomials. This rathernatural interpretation of AGD contrasts with earlier ones which lacked asimple, yet solid, motivation. Lastly, whereas existing lower bounds are onlyvalid when the dimensionality scales with the number of iterations, our lowerbound holds in the natural regime where the dimensionality is fixed.
arxiv-7800-232 | Attribute Efficient Linear Regression with Data-Dependent Sampling | http://arxiv.org/pdf/1410.6382v1.pdf | author:Doron Kukliansky, Ohad Shamir category:cs.LG stat.ML published:2014-10-23 summary:In this paper we analyze a budgeted learning setting, in which the learnercan only choose and observe a small subset of the attributes of each trainingexample. We develop efficient algorithms for ridge and lasso linear regression,which utilize the geometry of the data by a novel data-dependent samplingscheme. When the learner has prior knowledge on the second moments of theattributes, the optimal sampling probabilities can be calculated precisely, andresult in data-dependent improvements factors for the excess risk over thestate-of-the-art that may be as large as $O(\sqrt{d})$, where $d$ is theproblem's dimension. Moreover, under reasonable assumptions our algorithms canuse less attributes than full-information algorithms, which is the main concernin budgeted learning settings. To the best of our knowledge, these are thefirst algorithms able to do so in our setting. Where no such prior knowledge isavailable, we develop a simple estimation technique that given a sufficientamount of training examples, achieves similar improvements. We complement ourtheoretical analysis with experiments on several data sets which support ourclaims.
arxiv-7800-233 | Subspace Alignment For Domain Adaptation | http://arxiv.org/pdf/1409.5241v2.pdf | author:Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars category:cs.CV published:2014-09-18 summary:In this paper, we introduce a new domain adaptation (DA) algorithm where thesource and target domains are represented by subspaces spanned by eigenvectors.Our method seeks a domain invariant feature space by learning a mappingfunction which aligns the source subspace with the target one. We show that thesolution of the corresponding optimization problem can be obtained in a simpleclosed form, leading to an extremely fast algorithm. We present two approachesto determine the only hyper-parameter in our method corresponding to the sizeof the subspaces. In the first approach we tune the size of subspaces using atheoretical bound on the stability of the obtained result. In the secondapproach, we use maximum likelihood estimation to determine the subspace size,which is particularly useful for high dimensional data. Apart from PCA, wepropose a subspace creation method that outperform partial least squares (PLS)and linear discriminant analysis (LDA) in domain adaptation. We test our methodon various datasets and show that, despite its intrinsic simplicity, itoutperforms state of the art DA methods.
arxiv-7800-234 | A General Stochastic Algorithmic Framework for Minimizing Expensive Black Box Objective Functions Based on Surrogate Models and Sensitivity Analysis | http://arxiv.org/pdf/1410.6271v1.pdf | author:Yilun Wang, Christine A. Shoemaker category:stat.ML 90C26, 90C56 published:2014-10-23 summary:We are focusing on bound constrained global optimization problems, whoseobjective functions are computationally expensive black-box functions and havemultiple local minima. The recently popular Metric Stochastic Response Surface(MSRS) algorithm proposed by \cite{Regis2007SRBF} based on adaptive orsequential learning based on response surfaces is revisited and furtherextended for better performance in case of higher dimensional problems.Specifically, we propose a new way to generate the candidate points which thenext function evaluation point is picked from according to the metric criteria,based on a new definition of distance, and prove the global convergence of thecorresponding. Correspondingly, a more adaptive implementation of MSRS, named"SO-SA", is presented. "SO-SA" is is more likely to perturb those mostsensitive coordinates when generating the candidate points, instead ofperturbing all coordinates simultaneously. Numerical experiments on both synthetic problems and real problemsdemonstrate the advantages of our new algorithm, compared with many state ofthe art alternatives.}
arxiv-7800-235 | Capturing spatial interdependence in image features: the counting grid, an epitomic representation for bags of features | http://arxiv.org/pdf/1410.6264v1.pdf | author:Alessandro Perina, Nebojsa Jojic category:cs.CV stat.ML published:2014-10-23 summary:In recent scene recognition research images or large image regions are oftenrepresented as disorganized "bags" of features which can then be analyzed usingmodels originally developed to capture co-variation of word counts in text.However, image feature counts are likely to be constrained in different waysthan word counts in text. For example, as a camera pans upwards from a buildingentrance over its first few floors and then further up into the sky Fig. 1,some feature counts in the image drop while others rise -- only to drop againgiving way to features found more often at higher elevations. The space of allpossible feature count combinations is constrained both by the properties ofthe larger scene and the size and the location of the window into it. Tocapture such variation, in this paper we propose the use of the counting gridmodel. This generative model is based on a grid of feature counts, considerablylarger than any of the modeled images, and considerably smaller than the realestate needed to tile the images next to each other tightly. Each modeled imageis assumed to have a representative window in the grid in which the featurecounts mimic the feature distribution in the image. We provide a learningprocedure that jointly maps all images in the training set to the counting gridand estimates the appropriate local counts in it. Experimentally, wedemonstrate that the resulting representation captures the space of featurecount combinations more accurately than the traditional models, not only whenthe input images come from a panning camera, but even when modeling images ofdifferent scenes from the same category.
arxiv-7800-236 | Online Group Feature Selection | http://arxiv.org/pdf/1404.4774v3.pdf | author:Wang Jing, Zhao Zhong-Qiu, Hu Xuegang, Cheung Yiu-ming, Wang Meng, Wu Xindong category:cs.CV published:2014-04-18 summary:Online feature selection with dynamic features has become an active researcharea in recent years. However, in some real-world applications such as imageanalysis and email spam filtering, features may arrive by groups. Existingonline feature selection methods evaluate features individually, while existinggroup feature selection methods cannot handle online processing. Motivated bythis, we formulate the online group feature selection problem, and propose anovel selection approach for this problem. Our proposed approach consists oftwo stages: online intra-group selection and online inter-group selection. Inthe intra-group selection, we use spectral analysis to select discriminativefeatures in each group when it arrives. In the inter-group selection, we useLasso to select a globally optimal subset of features. This 2-stage procedurecontinues until there are no more features to come or some predefined stoppingconditions are met. Extensive experiments conducted on benchmark and real-worlddata sets demonstrate that our proposed approach outperforms otherstate-of-the-art online feature selection methods.
arxiv-7800-237 | Low-Rank Modeling and Its Applications in Image Analysis | http://arxiv.org/pdf/1401.3409v3.pdf | author:Xiaowei Zhou, Can Yang, Hongyu Zhao, Weichuan Yu category:cs.CV cs.LG stat.ML published:2014-01-15 summary:Low-rank modeling generally refers to a class of methods that solve problemsby representing variables of interest as low-rank matrices. It has achievedgreat success in various fields including computer vision, data mining, signalprocessing and bioinformatics. Recently, much progress has been made intheories, algorithms and applications of low-rank modeling, such as exactlow-rank matrix recovery via convex programming and matrix completion appliedto collaborative filtering. These advances have brought more and moreattentions to this topic. In this paper, we review the recent advance oflow-rank modeling, the state-of-the-art algorithms, and related applications inimage analysis. We first give an overview to the concept of low-rank modelingand challenging problems in this area. Then, we summarize the models andalgorithms for low-rank matrix recovery and illustrate their advantages andlimitations with numerical experiments. Next, we introduce a few applicationsof low-rank modeling in the context of image analysis. Finally, we concludethis paper with some discussions.
arxiv-7800-238 | Motion Estimation via Robust Decomposition with Constrained Rank | http://arxiv.org/pdf/1410.6126v1.pdf | author:German Ros, Jose Alvarez, Julio Guerrero category:cs.CV published:2014-10-22 summary:In this work, we address the problem of outlier detection for robust motionestimation by using modern sparse-low-rank decompositions, i.e., RobustPCA-like methods, to impose global rank constraints. Robust decompositions haveshown to be good at splitting a corrupted matrix into an uncorrupted low-rankmatrix and a sparse matrix, containing outliers. However, this process onlyworks when matrices have relatively low rank with respect to their ambientspace, a property not met in motion estimation problems. As a solution, wepropose to exploit the partial information present in the decomposition todecide which matches are outliers. We provide evidences showing that even whenit is not possible to recover an uncorrupted low-rank matrix, the resultinginformation can be exploited for outlier detection. To this end we propose theRobust Decomposition with Constrained Rank (RD-CR), a proximal gradient basedmethod that enforces the rank constraints inherent to motion estimation. Wealso present a general framework to perform robust estimation for stereo VisualOdometry, based on our RD-CR and a simple but effective compressed optimizationmethod that achieves high performance. Our evaluation on synthetic data and onthe KITTI dataset demonstrates the applicability of our approach in complexscenarios and it yields state-of-the-art performance.
arxiv-7800-239 | Learning without Concentration | http://arxiv.org/pdf/1401.0304v2.pdf | author:Shahar Mendelson category:cs.LG stat.ML published:2014-01-01 summary:We obtain sharp bounds on the performance of Empirical Risk Minimizationperformed in a convex class and with respect to the squared loss, withoutassuming that class members and the target are bounded functions or haverapidly decaying tails. Rather than resorting to a concentration-based argument, the method used hererelies on a `small-ball' assumption and thus holds for classes consisting ofheavy-tailed functions and for heavy-tailed targets. The resulting estimates scale correctly with the `noise level' of theproblem, and when applied to the classical, bounded scenario, always improvethe known bounds.
arxiv-7800-240 | Rich feature hierarchies for accurate object detection and semantic segmentation | http://arxiv.org/pdf/1311.2524v5.pdf | author:Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik category:cs.CV published:2013-11-11 summary:Object detection performance, as measured on the canonical PASCAL VOCdataset, has plateaued in the last few years. The best-performing methods arecomplex ensemble systems that typically combine multiple low-level imagefeatures with high-level context. In this paper, we propose a simple andscalable detection algorithm that improves mean average precision (mAP) by morethan 30% relative to the previous best result on VOC 2012---achieving a mAP of53.3%. Our approach combines two key insights: (1) one can apply high-capacityconvolutional neural networks (CNNs) to bottom-up region proposals in order tolocalize and segment objects and (2) when labeled training data is scarce,supervised pre-training for an auxiliary task, followed by domain-specificfine-tuning, yields a significant performance boost. Since we combine regionproposals with CNNs, we call our method R-CNN: Regions with CNN features. Wealso compare R-CNN to OverFeat, a recently proposed sliding-window detectorbased on a similar CNN architecture. We find that R-CNN outperforms OverFeat bya large margin on the 200-class ILSVRC2013 detection dataset. Source code forthe complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.
arxiv-7800-241 | Online Energy Price Matrix Factorization for Power Grid Topology Tracking | http://arxiv.org/pdf/1410.6095v1.pdf | author:Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick category:stat.ML cs.LG math.OC stat.AP published:2014-10-22 summary:Grid security and open markets are two major smart grid goals. Transparencyof market data facilitates a competitive and efficient energy environment, yetit may also reveal critical physical system information. Recovering the gridtopology based solely on publicly available market data is explored here.Real-time energy prices are calculated as the Lagrange multipliers ofnetwork-constrained economic dispatch; that is, via a linear program (LP)typically solved every 5 minutes. Granted the grid Laplacian is a parameter ofthis LP, one could infer such a topology-revealing matrix upon observingsuccessive LP dual outcomes. The matrix of spatio-temporal prices is firstshown to factor as the product of the inverse Laplacian times a sparse matrix.Leveraging results from sparse matrix decompositions, topology recovery schemeswith complementary strengths are subsequently formulated. Solvers scalable tohigh-dimensional and streaming market data are devised. Numerical validationusing real load data on the IEEE 30-bus grid provide useful input for currentand future market designs.
arxiv-7800-242 | Cosine Similarity Measure According to a Convex Cost Function | http://arxiv.org/pdf/1410.6093v1.pdf | author:Osman Gunay, Cem Emre Akbas, A. Enis Cetin category:cs.LG published:2014-10-22 summary:In this paper, we describe a new vector similarity measure associated with aconvex cost function. Given two vectors, we determine the surface normals ofthe convex function at the vectors. The angle between the two surface normalsis the similarity measure. Convex cost function can be the negative entropyfunction, total variation (TV) function and filtered variation function. Theconvex cost function need not be differentiable everywhere. In general, we needto compute the gradient of the cost function to compute the surface normals. Ifthe gradient does not exist at a given vector, it is possible to use thesubgradients and the normal producing the smallest angle between the twovectors is used to compute the similarity measure.
arxiv-7800-243 | Bayesian matrix completion: prior specification | http://arxiv.org/pdf/1406.1440v3.pdf | author:Pierre Alquier, Vincent Cottet, Nicolas Chopin, Judith Rousseau category:stat.ML math.ST stat.CO stat.TH published:2014-06-05 summary:Low-rank matrix estimation from incomplete measurements recently receivedincreased attention due to the emergence of several challenging applications,such as recommender systems; see in particular the famous Netflix challenge.While the behaviour of algorithms based on nuclear norm minimization is nowwell understood, an as yet unexplored avenue of research is the behaviour ofBayesian algorithms in this context. In this paper, we briefly review thepriors used in the Bayesian literature for matrix completion. A standardapproach is to assign an inverse gamma prior to the singular values of acertain singular value decomposition of the matrix of interest; this prior isconjugate. However, we show that two other types of priors (again for thesingular values) may be conjugate for this model: a gamma prior, and a discreteprior. Conjugacy is very convenient, as it makes it possible to implementeither Gibbs sampling or Variational Bayes. Interestingly enough, the maximum aposteriori for these different priors is related to the nuclear normminimization problems. We also compare all these priors on simulated datasets,and on the classical MovieLens and Netflix datasets.
arxiv-7800-244 | Demixed principal component analysis of population activity in higher cortical areas reveals independent representation of task parameters | http://arxiv.org/pdf/1410.6031v1.pdf | author:Dmitry Kobak, Wieland Brendel, Christos Constantinidis, Claudia E. Feierstein, Adam Kepecs, Zachary F. Mainen, Ranulfo Romo, Xue-Lian Qi, Naoshige Uchida, Christian K. Machens category:q-bio.NC stat.ML published:2014-10-22 summary:Neurons in higher cortical areas, such as the prefrontal cortex, are known tobe tuned to a variety of sensory and motor variables. The resulting diversityof neural tuning often obscures the represented information. Here we introducea novel dimensionality reduction technique, demixed principal componentanalysis (dPCA), which automatically discovers and highlights the essentialfeatures in complex population activities. We reanalyze population data fromthe prefrontal areas of rats and monkeys performing a variety of working memoryand decision-making tasks. In each case, dPCA summarizes the relevant featuresof the population response in a single figure. The population activity isdecomposed into a few demixed components that capture most of the variance inthe data and that highlight dynamic tuning of the population to various taskparameters, such as stimuli, decisions, rewards, etc. Moreover, dPCA revealsstrong, condition-independent components of the population activity that remainunnoticed with conventional approaches.
arxiv-7800-245 | Salient Object Detection: A Discriminative Regional Feature Integration Approach | http://arxiv.org/pdf/1410.5926v1.pdf | author:Huaizu Jiang, Zejian Yuan, Ming-Ming Cheng, Yihong Gong, Nanning Zheng, Jingdong Wang category:cs.CV published:2014-10-22 summary:Salient object detection has been attracting a lot of interest, and recentlyvarious heuristic computational models have been designed. In this paper, weformulate saliency map computation as a regression problem. Our method, whichis based on multi-level image segmentation, utilizes the supervised learningapproach to map the regional feature vector to a saliency score. Saliencyscores across multiple levels are finally fused to produce the saliency map.The contributions lie in two-fold. One is that we propose a discriminateregional feature integration approach for salient object detection. Comparedwith existing heuristic models, our proposed method is able to automaticallyintegrate high-dimensional regional saliency features and choose discriminativeones. The other is that by investigating standard generic region properties aswell as two widely studied concepts for salient object detection, i.e.,regional contrast and backgroundness, our approach significantly outperformsstate-of-the-art methods on six benchmark datasets. Meanwhile, we demonstratethat our method runs as fast as most existing algorithms.
arxiv-7800-246 | Active Regression by Stratification | http://arxiv.org/pdf/1410.5920v1.pdf | author:Sivan Sabato, Remi Munos category:stat.ML cs.LG published:2014-10-22 summary:We propose a new active learning algorithm for parametric linear regressionwith random design. We provide finite sample convergence guarantees for generaldistributions in the misspecified model. This is the first active learner forthis setting that provably can improve over passive learning. Unlike otherlearning settings (such as classification), in regression the passive learningrate of $O(1/\epsilon)$ cannot in general be improved upon. Nonetheless, theso-called `constant' in the rate of convergence, which is characterized by adistribution-dependent risk, can be improved in many cases. For a givendistribution, achieving the optimal risk requires prior knowledge of thedistribution. Following the stratification technique advocated in Monte-Carlofunction integration, our active learner approaches the optimal risk usingpiecewise constant approximations.
arxiv-7800-247 | A Spectral Framework for Anomalous Subgraph Detection | http://arxiv.org/pdf/1401.7702v2.pdf | author:Benjamin A. Miller, Michelle S. Beard, Patrick J. Wolfe, Nadya T. Bliss category:cs.SI stat.ML published:2014-01-29 summary:A wide variety of application domains are concerned with data consisting ofentities and their relationships or connections, formally represented asgraphs. Within these diverse application areas, a common problem of interest isthe detection of a subset of entities whose connectivity is anomalous withrespect to the rest of the data. While the detection of such anomaloussubgraphs has received a substantial amount of attention, noapplication-agnostic framework exists for analysis of signal detectability ingraph-based data. In this paper, we describe a framework that enables suchanalysis using the principal eigenspace of a graph's residuals matrix, commonlycalled the modularity matrix in community detection. Leveraging this analyticaltool, we show that the framework has a natural power metric in the spectralnorm of the anomalous subgraph's adjacency matrix (signal power) and of thebackground graph's residuals matrix (noise power). We propose severalalgorithms based on spectral properties of the residuals matrix, with morecomputationally expensive techniques providing greater detection power.Detection and identification performance are presented for a number of signaland noise models, including clusters and bipartite foregrounds embedded intosimple random backgrounds as well as graphs with community structure andrealistic degree distributions. The trends observed verify intuition gleanedfrom other signal processing areas, such as greater detection power when thesignal is embedded within a less active portion of the background. Wedemonstrate the utility of the proposed techniques in detecting small, highlyanomalous subgraphs in real graphs derived from Internet traffic and productco-purchases.
arxiv-7800-248 | Vehicle Detection and Tracking Techniques: A Concise Review | http://arxiv.org/pdf/1410.5894v1.pdf | author:Raad Ahmed Hadi, Ghazali Sulong, Loay Edwar George category:cs.CV published:2014-10-22 summary:Vehicle detection and tracking applications play an important role forcivilian and military applications such as in highway traffic surveillancecontrol, management and urban traffic planning. Vehicle detection process onroad are used for vehicle tracking, counts, average speed of each individualvehicle, traffic analysis and vehicle categorizing objectives and may beimplemented under different environments changes. In this review, we present aconcise overview of image processing methods and analysis tools which used inbuilding these previous mentioned applications that involved developing trafficsurveillance systems. More precisely and in contrast with other reviews, weclassified the processing methods under three categories for more clarificationto explain the traffic systems.
arxiv-7800-249 | A Parallel and Efficient Algorithm for Learning to Match | http://arxiv.org/pdf/1410.6414v1.pdf | author:Jingbo Shang, Tianqi Chen, Hang Li, Zhengdong Lu, Yong Yu category:cs.LG cs.AI published:2014-10-22 summary:Many tasks in data mining and related fields can be formalized as matchingbetween objects in two heterogeneous domains, including collaborativefiltering, link prediction, image tagging, and web search. Machine learningtechniques, referred to as learning-to-match in this paper, have beensuccessfully applied to the problems. Among them, a class of state-of-the-artmethods, named feature-based matrix factorization, formalize the task as anextension to matrix factorization by incorporating auxiliary features into themodel. Unfortunately, making those algorithms scale to real world problems ischallenging, and simple parallelization strategies fail due to the complexcross talking patterns between sub-tasks. In this paper, we tackle thischallenge with a novel parallel and efficient algorithm for feature-basedmatrix factorization. Our algorithm, based on coordinate descent, can easilyhandle hundreds of millions of instances and features on a single machine. Thekey recipe of this algorithm is an iterative relaxation of the objective tofacilitate parallel updates of parameters, with guaranteed convergence onminimizing the original objective function. Experimental results demonstratethat the proposed method is effective on a wide range of matching problems,with efficiency significantly improved upon the baselines while accuracyretained unchanged.
arxiv-7800-250 | Mean-Field Networks | http://arxiv.org/pdf/1410.5884v1.pdf | author:Yujia Li, Richard Zemel category:cs.LG stat.ML published:2014-10-21 summary:The mean field algorithm is a widely used approximate inference algorithm forgraphical models whose exact inference is intractable. In each iteration ofmean field, the approximate marginals for each variable are updated by gettinginformation from the neighbors. This process can be equivalently converted intoa feedforward network, with each layer representing one iteration of mean fieldand with tied weights on all layers. This conversion enables a few naturalextensions, e.g. untying the weights in the network. In this paper, we studythese mean field networks (MFNs), and use them as inference tools as well asdiscriminative models. Preliminary experiment results show that MFNs can learnto do inference very efficiently and perform significantly better than meanfield as discriminative models.
arxiv-7800-251 | Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation | http://arxiv.org/pdf/1410.5877v1.pdf | author:Michael Bloodgood, Chris Callison-Burch category:cs.CL cs.LG stat.ML published:2014-10-21 summary:We explore how to improve machine translation systems by adding moretranslation data in situations where we already have substantial resources. Themain challenge is how to buck the trend of diminishing returns that is commonlyencountered. We present an active learning-style data solicitation algorithm tomeet this challenge. We test it, gathering annotations via Amazon MechanicalTurk, and find that we get an order of magnitude increase in performance ratesof improvement.
arxiv-7800-252 | Compositional Structure Learning for Action Understanding | http://arxiv.org/pdf/1410.5861v1.pdf | author:Ran Xu, Gang Chen, Caiming Xiong, Wei Chen, Jason J. Corso category:cs.CV published:2014-10-21 summary:The focus of the action understanding literature has predominately beenclassification, how- ever, there are many applications demanding richer actionunderstanding such as mobile robotics and video search, with solutions toclassification, localization and detection. In this paper, we propose acompositional model that leverages a new mid-level representation calledcompositional trajectories and a locally articulated spatiotemporal deformableparts model (LALSDPM) for fully action understanding. Our methods isadvantageous in capturing the variable structure of dynamic human activity overa long range. First, the compositional trajectories capture long-ranging,frequently co-occurring groups of trajectories in space time and represent themin discriminative hierarchies, where human motion is largely separated fromcamera motion; second, LASTDPM learns a structured model with multi-layerdeformable parts to capture multiple levels of articulated motion. We implementour methods and demonstrate state of the art performance on all three problems:action detection, localization, and recognition.
arxiv-7800-253 | A Fast Hybrid Primal Heuristic for Multiband Robust Capacitated Network Design with Multiple Time Periods | http://arxiv.org/pdf/1410.5850v1.pdf | author:Fabio D'Andreagiovanni, Jonatan Krolikowski, Jonad Pulaj category:math.OC cs.DS cs.NE published:2014-10-21 summary:We investigate the Robust Multiperiod Network Design Problem, ageneralization of the Capacitated Network Design Problem (CNDP) that, besidesestablishing flow routing and network capacity installation as in a canonicalCNDP, also considers a planning horizon made up of multiple time periods andprotection against fluctuations in traffic volumes. As a remedy against trafficvolume uncertainty, we propose a Robust Optimization model based on MultibandRobustness (B\"using and D'Andreagiovanni, 2012), a refinement of classicalGamma-Robustness by Bertsimas and Sim that uses a system of multiple deviationbands. Since the resulting optimization problem may prove very challenging evenfor instances of moderate size solved by a state-of-the-art optimizationsolver, we propose a hybrid primal heuristic that combines a randomized fixingstrategy inspired by ant colony optimization, which exploits information comingfrom linear relaxations of the problem, and an exact large neighbourhoodsearch. Computational experiments on a set of realistic instances from theSNDlib show that our original heuristic can run fast and produce solutions ofextremely high quality associated with low optimality gaps.
arxiv-7800-254 | Generalized Compression Dictionary Distance as Universal Similarity Measure | http://arxiv.org/pdf/1410.5792v1.pdf | author:Andrey Bogomolov, Bruno Lepri, Fabio Pianesi category:stat.ML cs.AI cs.CC cs.IT math.IT published:2014-10-21 summary:We present a new similarity measure based on information theoretic measureswhich is superior than Normalized Compression Distance for clustering problemsand inherits the useful properties of conditional Kolmogorov complexity. Weshow that Normalized Compression Dictionary Size and Normalized CompressionDictionary Entropy are computationally more efficient, as the need to performthe compression itself is eliminated. Also they scale linearly with exponentialvector size growth and are content independent. We show that normalizedcompression dictionary distance is compressor independent, if limited tolossless compressors, which gives space for optimizations and implementationspeed improvement for real-time and big data applications. The introducedmeasure is applicable for machine learning tasks of parameter-free unsupervisedclustering, supervised learning such as classification and regression, featureselection, and is applicable for big data problems with order of magnitudespeed increase.
arxiv-7800-255 | Daily Stress Recognition from Mobile Phone Data, Weather Conditions and Individual Traits | http://arxiv.org/pdf/1410.5816v1.pdf | author:Andrey Bogomolov, Bruno Lepri, Michela Ferron, Fabio Pianesi, Alex, Pentland category:cs.CY cs.LG stat.AP stat.ML published:2014-10-21 summary:Research has proven that stress reduces quality of life and causes manydiseases. For this reason, several researchers devised stress detection systemsbased on physiological parameters. However, these systems require thatobtrusive sensors are continuously carried by the user. In our paper, wepropose an alternative approach providing evidence that daily stress can bereliably recognized based on behavioral metrics, derived from the user's mobilephone activity and from additional indicators, such as the weather conditions(data pertaining to transitory properties of the environment) and thepersonality traits (data concerning permanent dispositions of individuals). Ourmultifactorial statistical model, which is person-independent, obtains theaccuracy score of 72.28% for a 2-class daily stress recognition problem. Themodel is efficient to implement for most of multimedia applications due tohighly reduced low-dimensional feature space (32d). Moreover, we identify anddiscuss the indicators which have strong predictive power.
arxiv-7800-256 | Regularizing Recurrent Networks - On Injected Noise and Norm-based Methods | http://arxiv.org/pdf/1410.5684v1.pdf | author:Saahil Ognawala, Justin Bayer category:stat.ML cs.LG published:2014-10-21 summary:Advancements in parallel processing have lead to a surge in multilayerperceptrons' (MLP) applications and deep learning in the past decades.Recurrent Neural Networks (RNNs) give additional representational power tofeedforward MLPs by providing a way to treat sequential data. However, RNNs arehard to train using conventional error backpropagation methods because of thedifficulty in relating inputs over many time-steps. Regularization approachesfrom MLP sphere, like dropout and noisy weight training, have beeninsufficiently applied and tested on simple RNNs. Moreover, solutions have beenproposed to improve convergence in RNNs but not enough to improve the long termdependency remembering capabilities thereof. In this study, we aim to empirically evaluate the remembering andgeneralization ability of RNNs on polyphonic musical datasets. The models aretrained with injected noise, random dropout, norm-based regularizers and theirrespective performances compared to well-initialized plain RNNs and advancedregularization methods like fast-dropout. We conclude with evidence thattraining with noise does not improve performance as conjectured by a few worksin RNN optimization before ours.
arxiv-7800-257 | Improvement of PSO algorithm by memory based gradient search - application in inventory management | http://arxiv.org/pdf/1410.5652v1.pdf | author:Tamás Varga, András Király, János Abonyi category:cs.NE published:2014-10-21 summary:Advanced inventory management in complex supply chains requires effective androbust nonlinear optimization due to the stochastic nature of supply and demandvariations. Application of estimated gradients can boost up the convergence ofParticle Swarm Optimization (PSO) algorithm but classical gradient calculationcannot be applied to stochastic and uncertain systems. In these situationsMonte-Carlo (MC) simulation can be applied to determine the gradient. Wedeveloped a memory based algorithm where instead of generating and evaluatingnew simulated samples the stored and shared former function evaluations of theparticles are sampled to estimate the gradients by local weighted least squaresregression. The performance of the resulted regional gradient-based PSO isverified by several benchmark problems and in a complex application examplewhere optimal reorder points of a supply chain are determined.
arxiv-7800-258 | A Robust Ensemble Approach to Learn From Positive and Unlabeled Data Using SVM Base Models | http://arxiv.org/pdf/1402.3144v2.pdf | author:Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor category:stat.ML cs.LG published:2014-02-13 summary:We present a novel approach to learn binary classifiers when only positiveand unlabeled instances are available (PU learning). This problem is routinelycast as a supervised task with label noise in the negative set. We use anensemble of SVM models trained on bootstrap resamples of the training data forincreased robustness against label noise. The approach can be considered in abagging framework which provides an intuitive explanation for its mechanics ina semi-supervised setting. We compared our method to state-of-the-artapproaches in simulations using multiple public benchmark data sets. Theincluded benchmark comprises three settings with increasing label noise: (i)fully supervised, (ii) PU learning and (iii) PU learning with false positives.Our approach shows a marginal improvement over existing methods in the secondsetting and a significant improvement in the third.
arxiv-7800-259 | Universality of Power Law Coding for Principal Neurons | http://arxiv.org/pdf/1410.5610v1.pdf | author:Gabriele Scheler category:q-bio.NC cs.NE published:2014-10-21 summary:In this paper we document distributions for spike rates, synaptic weights andneural gains for principal neurons in various tissues and under differentbehavioral conditions. We find a remarkable consistency of a power-law,specifically lognormal, distribution across observations from auditory orvisual cortex as well as midbrain nuclei, cerebellar Purkinje cells andstriatal medium spiny neurons. An exception is documented for fast-spikinginterneurons, as non-coding neurons, which seem to follow a normaldistribution. The difference between strongly recurrent and transferconnectivity (cortex vs. striatum and cerebellum), or the level of activation(low in cortex, high in Purkinje cells and midbrain nuclei) seems to beirrelevant for these distributions. This has certain implications on neuralcoding. In particular, logarithmic scale distribution of neuronal outputappears as a structural phenomenon that is always present in coding neurons. Wealso report data for a lognormal distribution of synaptic strengths in cortex,cerebellum and hippocampus and for intrinsic excitability in striatum, cortexand cerebellum. We present a neural model for gain, weights and spike rates,specifically matching the width of distributions. We discuss the data from theperspective of a hierarchical coding scheme with few sparse or top-levelfeatures and many additional distributed low-level features. Logarithmic-scalecoding may solve an access problem by combining a local modular structure withfew high frequency contact points. Computational models may need to incorporatethese observations as primary constraints. More data are needed to consolidatethe observations.
arxiv-7800-260 | Mobility Enhancement for Elderly | http://arxiv.org/pdf/1410.5600v1.pdf | author:Ramviyas Parasuraman category:cs.CV cs.RO published:2014-10-21 summary:Loss of Mobility is a common handicap to senior citizens. It denies them theease of movement they would like to have like outdoor visits, movement inhospitals, social outgoings, but more seriously in the day to day in-houseroutine functions necessary for living etc. Trying to overcome this handicap bymeans of servant or domestic help and simple wheel chairs is not only costly inthe long run, but forces the senior citizen to be at the mercy of sincerity ofdomestic helps and also the consequent loss of dignity. In order to give adignified life, the mobility obtained must be at the complete discretion, willand control of the senior citizen. This can be provided only by a reasonablysophisticated and versatile wheel chair, giving enhanced ability of vision,hearing through man-machine interface, and sensor aided navigation and control.More often than not senior people have poor vision which makes it difficult forthem to maker visual judgement and so calls for the use of ArtificialIntelligence in visual image analysis and guided navigation systems. In this project, we deal with two important enhancement features for mobilityenhancement, Audio command and Vision aided obstacle detection and navigation.We have implemented speech recognition algorithm using template of stored wordsfor identifying the voice command given by the user. This frees the user of anagile hand to operate joystick or mouse control. Also, we have developed a newappearance based obstacle detection system using stereo-vision cameras whichestimates the distance of nearest obstacle to the wheel chair and takesnecessary action. This helps user in making better judgement of route andnavigate obstacles. The main challenge in this project is how to navigate in anunknown/unfamiliar environment by avoiding obstacles.
arxiv-7800-261 | On Iterative Hard Thresholding Methods for High-dimensional M-Estimation | http://arxiv.org/pdf/1410.5137v2.pdf | author:Prateek Jain, Ambuj Tewari, Purushottam Kar category:cs.LG stat.ML published:2014-10-20 summary:The use of M-estimators in generalized linear regression models in highdimensional settings requires risk minimization with hard $L_0$ constraints. Ofthe known methods, the class of projected gradient descent (also known asiterative hard thresholding (IHT)) methods is known to offer the fastest andmost scalable solutions. However, the current state-of-the-art is only able toanalyze these methods in extremely restrictive settings which do not hold inhigh dimensional statistical models. In this work we bridge this gap byproviding the first analysis for IHT-style methods in the high dimensionalstatistical setting. Our bounds are tight and match known minimax lower bounds.Our results rely on a general analysis framework that enables us to analyzeseveral popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) inthe high dimensional regression setting. We also extend our analysis to a largefamily of "fully corrective methods" that includes two-stage and partialhard-thresholding algorithms. We show that our results hold for the problem ofsparse regression, as well as low-rank matrix recovery.
arxiv-7800-262 | Where do goals come from? A Generic Approach to Autonomous Goal-System Development | http://arxiv.org/pdf/1410.5557v1.pdf | author:Matthias Rolf, Minoru Asada category:cs.LG cs.AI published:2014-10-21 summary:Goals express agents' intentions and allow them to organize their behaviorbased on low-dimensional abstractions of high-dimensional world states. How canagents develop such goals autonomously? This paper proposes a detailedconceptual and computational account to this longstanding problem. We argue toconsider goals as high-level abstractions of lower-level intention mechanismssuch as rewards and values, and point out that goals need to be consideredalongside with a detection of the own actions' effects. We propose Latent GoalAnalysis as a computational learning formulation thereof, and showconstructively that any reward or value function can by explained by goals andsuch self-detection as latent mechanisms. We first show that learned goalsprovide a highly effective dimensionality reduction in a practicalreinforcement learning problem. Then, we investigate a developmental scenarioin which entirely task-unspecific rewards induced by visual saliency lead toself and goal representations that constitute goal-directed reaching.
arxiv-7800-263 | Learning to Rank Binary Codes | http://arxiv.org/pdf/1410.5524v1.pdf | author:Jie Feng, Wei Liu, Yan Wang category:cs.CV published:2014-10-21 summary:Binary codes have been widely used in vision problems as a compact featurerepresentation to achieve both space and time advantages. Various methods havebeen proposed to learn data-dependent hash functions which map a feature vectorto a binary code. However, considerable data information is inevitably lostduring the binarization step which also causes ambiguity in measuring samplesimilarity using Hamming distance. Besides, the learned hash functions cannotbe changed after training, which makes them incapable of adapting to new dataoutside the training data set. To address both issues, in this paper we proposea flexible bitwise weight learning framework based on the binary codes obtainedby state-of-the-art hashing methods, and incorporate the learned weights intothe weighted Hamming distance computation. We then formulate the proposedframework as a ranking problem and leverage the Ranking SVM model to offlinetackle the weight learning. The framework is further extended to an online modewhich updates the weights at each time new data comes, thereby making itscalable to large and dynamic data sets. Extensive experimental resultsdemonstrate significant performance gains of using binary codes with bitwiseweighting in image retrieval tasks. It is appealing that the online weightlearning leads to comparable accuracy with its offline counterpart, which thusmakes our approach practical for realistic applications.
arxiv-7800-264 | Variational Reformulation of Bayesian Inverse Problems | http://arxiv.org/pdf/1410.5522v1.pdf | author:Panagiotis Tsilifis, Ilias Bilionis, Ioannis Katsounaros, Nicholas Zabaras category:stat.ML published:2014-10-21 summary:The classical approach to inverse problems is based on the optimization of amisfit function. Despite its computational appeal, such an approach suffersfrom many shortcomings, e.g., non-uniqueness of solutions, modeling priorknowledge, etc. The Bayesian formalism to inverse problems avoids most of thedifficulties encountered by the optimization approach, albeit at an increasedcomputational cost. In this work, we use information theoretic arguments tocast the Bayesian inference problem in terms of an optimization problem. Theresulting scheme combines the theoretical soundness of fully Bayesian inferencewith the computational efficiency of a simple optimization.
arxiv-7800-265 | Using Mechanical Turk to Build Machine Translation Evaluation Sets | http://arxiv.org/pdf/1410.5491v1.pdf | author:Michael Bloodgood, Chris Callison-Burch category:cs.CL cs.LG stat.ML published:2014-10-20 summary:Building machine translation (MT) test sets is a relatively expensive task.As MT becomes increasingly desired for more and more language pairs and moreand more domains, it becomes necessary to build test sets for each case. Inthis paper, we investigate using Amazon's Mechanical Turk (MTurk) to make MTtest sets cheaply. We find that MTurk can be used to make test sets muchcheaper than professionally-produced test sets. More importantly, inexperiments with multiple MT systems, we find that the MTurk-produced test setsyield essentially the same conclusions regarding system performance as theprofessionally-produced test sets yield.
arxiv-7800-266 | Machine Learning of Coq Proof Guidance: First Experiments | http://arxiv.org/pdf/1410.5467v1.pdf | author:Cezary Kaliszyk, Lionel Mamane, Josef Urban category:cs.LO cs.LG published:2014-10-20 summary:We report the results of the first experiments with learning proofdependencies from the formalizations done with the Coq system. We explain theprocess of obtaining the dependencies from the Coq proofs, the characterizationof formulas that is used for the learning, and the evaluation method. Variousmachine learning methods are compared on a dataset of 5021 toplevel Coq proofscoming from the CoRN repository. The best resulting method covers on average75% of the needed proof dependencies among the first 100 predictions, which isa comparable performance of such initial experiments on other large-theorycorpora.
arxiv-7800-267 | Scalable Parallel Factorizations of SDD Matrices and Efficient Sampling for Gaussian Graphical Models | http://arxiv.org/pdf/1410.5392v1.pdf | author:Dehua Cheng, Yu Cheng, Yan Liu, Richard Peng, Shang-Hua Teng category:cs.DS cs.LG cs.NA math.NA stat.CO stat.ML published:2014-10-20 summary:Motivated by a sampling problem basic to computational statistical inference,we develop a nearly optimal algorithm for a fundamental problem in spectralgraph theory and numerical analysis. Given an $n\times n$ SDDM matrix ${\bf\mathbf{M}}$, and a constant $-1 \leq p \leq 1$, our algorithm gives efficientaccess to a sparse $n\times n$ linear operator $\tilde{\mathbf{C}}$ such that$${\mathbf{M}}^{p} \approx \tilde{\mathbf{C}} \tilde{\mathbf{C}}^\top.$$ Thesolution is based on factoring ${\bf \mathbf{M}}$ into a product of simple andsparse matrices using squaring and spectral sparsification. For ${\mathbf{M}}$with $m$ non-zero entries, our algorithm takes work nearly-linear in $m$, andpolylogarithmic depth on a parallel machine with $m$ processors. This gives thefirst sampling algorithm that only requires nearly linear work and $n$ i.i.d.random univariate Gaussian samples to generate i.i.d. random samples for$n$-dimensional Gaussian random fields with SDDM precision matrices. Forsampling this natural subclass of Gaussian random fields, it is optimal in therandomness and nearly optimal in the work and parallel complexity. In addition,our sampling algorithm can be directly extended to Gaussian random fields withSDD precision matrices.
arxiv-7800-268 | Prediction of Synchrostate Transitions in EEG Signals Using Markov Chain Models | http://arxiv.org/pdf/1410.5362v1.pdf | author:Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik Maharatna category:q-bio.NC physics.med-ph stat.AP stat.ML published:2014-10-20 summary:This paper proposes a stochastic model using the concept of Markov chains forthe inter-state transitions of the millisecond order quasi-stable phasesynchronized patterns or synchrostates, found in multi-channelElectroencephalogram (EEG) signals. First and second order transitionprobability matrices are estimated for Markov chain modelling from 100 trialsof 128-channel EEG signals during two different face perception tasks.Prediction accuracies with such finite Markov chain models for synchrostatetransition are also compared, under a data-partitioning based cross-validationscheme.
arxiv-7800-269 | Classification of Autism Spectrum Disorder Using Supervised Learning of Brain Connectivity Measures Extracted from Synchrostates | http://arxiv.org/pdf/1410.7795v1.pdf | author:Wasifa Jamal, Saptarshi Das, Ioana-Anastasia Oprescu, Koushik Maharatna, Fabio Apicella, Federico Sicca category:physics.med-ph cs.CV stat.AP stat.ML published:2014-10-20 summary:Objective. The paper investigates the presence of autism using the functionalbrain connectivity measures derived from electro-encephalogram (EEG) ofchildren during face perception tasks. Approach. Phase synchronized patternsfrom 128-channel EEG signals are obtained for typical children and childrenwith autism spectrum disorder (ASD). The phase synchronized states orsynchrostates temporally switch amongst themselves as an underlying process forthe completion of a particular cognitive task. We used 12 subjects in eachgroup (ASD and typical) for analyzing their EEG while processing fearful, happyand neutral faces. The minimal and maximally occurring synchrostates for eachsubject are chosen for extraction of brain connectivity features, which areused for classification between these two groups of subjects. Among differentsupervised learning techniques, we here explored the discriminant analysis andsupport vector machine both with polynomial kernels for the classificationtask. Main results. The leave one out cross-validation of the classificationalgorithm gives 94.7% accuracy as the best performance with correspondingsensitivity and specificity values as 85.7% and 100% respectively.Significance. The proposed method gives high classification accuracies andoutperforms other contemporary research results. The effectiveness of theproposed method for classification of autistic and typical children suggeststhe possibility of using it on a larger population to validate it for clinicalpractice.
arxiv-7800-270 | Artifact reduction in multichannel pervasive EEG using hybrid WPT-ICA and WPT-EMD signal decomposition techniques | http://arxiv.org/pdf/1410.5801v1.pdf | author:Valentina Bono, Wasifa Jamal, Saptarshi Das, Koushik Maharatna category:physics.med-ph cs.LG stat.AP stat.ME published:2014-10-20 summary:In order to reduce the muscle artifacts in multi-channel pervasiveElectroencephalogram (EEG) signals, we here propose and compare two hybridalgorithms by combining the concept of wavelet packet transform (WPT),empirical mode decomposition (EMD) and Independent Component Analysis (ICA).The signal cleaning performances of WPT-EMD and WPT-ICA algorithms have beencompared using a signal-to-noise ratio (SNR)-like criterion for artifacts. Thealgorithms have been tested on multiple trials of four different artifact casesviz. eye-blinking and muscle artifacts including left and right hand movementand head-shaking.
arxiv-7800-271 | On Bootstrapping Machine Learning Performance Predictors via Analytical Models | http://arxiv.org/pdf/1410.5102v1.pdf | author:Diego Didona, Paolo Romano category:cs.PF cs.LG published:2014-10-19 summary:Performance modeling typically relies on two antithetic methodologies: whitebox models, which exploit knowledge on system's internals and capture itsdynamics using analytical approaches, and black box techniques, which inferrelations among the input and output variables of a system based on theevidences gathered during an initial training phase. In this paper weinvestigate a technique, which we name Bootstrapping, which aims at reconcilingthese two methodologies and at compensating the cons of the one with the prosof the other. We thoroughly analyze the design space of this gray box modelingtechnique, and identify a number of algorithmic and parametric trade-offs whichwe evaluate via two realistic case studies, a Key-Value Store and a Total OrderBroadcast service.
arxiv-7800-272 | Learning Vague Concepts for the Semantic Web | http://arxiv.org/pdf/1410.5078v1.pdf | author:Paolo Pareti, Ewan Klein category:cs.AI cs.CL published:2014-10-19 summary:Ontologies can be a powerful tool for structuring knowledge, and they arecurrently the subject of extensive research. Updating the contents of anontology or improving its interoperability with other ontologies is animportant but difficult process. In this paper, we focus on the presence ofvague concepts, which are pervasive in natural language, within the frameworkof formal ontologies. We will adopt a framework in which vagueness is capturedvia numerical restrictions that can be automatically adjusted. Since updatingvague concepts, either through ontology alignment or ontology evolution, canlead to inconsistent sets of axioms, we define and implement a method todetecting and repairing such inconsistencies in a local fashion.
arxiv-7800-273 | Dense 3D Face Correspondence | http://arxiv.org/pdf/1410.5058v1.pdf | author:Syed Zulqarnain Gilani, Faisal Shafait, Ajmal Mian category:cs.CV published:2014-10-19 summary:We present an algorithm that automatically establishes dense correspondencesbetween a large number of 3D faces. Starting from automatically detected sparsecorrespondences on the convex hull of 3D faces, the algorithm triangulatesexisting correspondences and expands them iteratively along the triangle edges.New correspondences are established by matching keypoints on the geodesicpatches along the triangle edges and the process is repeated. After exhaustingkeypoint matches, further correspondences are established by evolving level setgeodesic curves from the centroids of large triangles. A deformable model(K3DM) is constructed from the dense corresponded faces and an algorithm isproposed for morphing the K3DM to fit unseen faces. This algorithm iteratesbetween rigid alignment of an unseen face followed by regularized morphing ofthe deformable model. We have extensively evaluated the proposed algorithms onsynthetic data and real 3D faces from the FRGCv2 and BU3DFE databases usingquantitative and qualitative benchmarks. Our algorithm achieved densecorrespondences with a mean localization error of 1.28mm on synthetic faces anddetected 18 anthropometric landmarks on unseen real faces from the FRGCv2database with 3mm precision. Furthermore, our deformable model fittingalgorithm achieved 99.8% gender classification and 98.3% face recognitionaccuracy on the FRGCv2 database.
arxiv-7800-274 | Director Field Model of the Primary Visual Cortex for Contour Detection | http://arxiv.org/pdf/1310.1341v2.pdf | author:Vijay Singh, Martin Tchernookov, Rebecca Butterfield, Ilya Nemenman category:q-bio.NC cs.CV published:2013-10-04 summary:We aim to build the simplest possible model capable of detecting long, noisycontours in a cluttered visual scene. For this, we model the neural dynamics inthe primate primary visual cortex in terms of a continuous director field thatdescribes the average rate and the average orientational preference of activeneurons at a particular point in the cortex. We then use a linear-nonlineardynamical model with long range connectivity patterns to enforce long-rangestatistical context present in the analyzed images. The resulting model hassubstantially fewer degrees of freedom than traditional models, and yet it candistinguish large contiguous objects from the background clutter by suppressingthe clutter and by filling-in occluded elements of object contours. Thisresults in high-precision, high-recall detection of large objects in clutteredscenes. Parenthetically, our model has a direct correspondence with the Landau- de Gennes theory of nematic liquid crystal in two dimensions.
arxiv-7800-275 | Gaussian Process Models with Parallelization and GPU acceleration | http://arxiv.org/pdf/1410.4984v1.pdf | author:Zhenwen Dai, Andreas Damianou, James Hensman, Neil Lawrence category:cs.DC cs.LG stat.ML published:2014-10-18 summary:In this work, we present an extension of Gaussian process (GP) models withsophisticated parallelization and GPU acceleration. The parallelization schemearises naturally from the modular computational structure w.r.t. datapoints inthe sparse Gaussian process formulation. Additionally, the computationalbottleneck is implemented with GPU acceleration for further speed up. Combiningboth techniques allows applying Gaussian process models to millions ofdatapoints. The efficiency of our algorithm is demonstrated with a syntheticdataset. Its source code has been integrated into our popular software libraryGPy.
arxiv-7800-276 | The Visualization of Change in Word Meaning over Time using Temporal Word Embeddings | http://arxiv.org/pdf/1410.4966v1.pdf | author:Chiraag Lala, Shay B. Cohen category:cs.CL published:2014-10-18 summary:We describe a visualization tool that can be used to view the change inmeaning of words over time. The tool makes use of existing (static) wordembedding datasets together with a timestamped $n$-gram corpus to create {\emtemporal} word embeddings.
arxiv-7800-277 | Discrete Dynamical Genetic Programming in XCS | http://arxiv.org/pdf/1204.4200v2.pdf | author:Richard J. Preen, Larry Bull category:cs.AI cs.LG cs.NE cs.SY I.2.6 published:2012-04-18 summary:A number of representation schemes have been presented for use withinLearning Classifier Systems, ranging from binary encodings to neural networks.This paper presents results from an investigation into using a discretedynamical system representation within the XCS Learning Classifier System. Inparticular, asynchronous random Boolean networks are used to represent thetraditional condition-action production system rules. It is shown possible touse self-adaptive, open-ended evolution to design an ensemble of such discretedynamical systems within XCS to solve a number of well-known test problems.
arxiv-7800-278 | Optimal Feature Selection from VMware ESXi 5.1 Feature Set | http://arxiv.org/pdf/1410.5784v1.pdf | author:Amartya Hatua category:cs.DC cs.LG published:2014-10-18 summary:A study of VMware ESXi 5.1 server has been carried out to find the optimalset of parameters which suggest usage of different resources of the server.Feature selection algorithms have been used to extract the optimum set ofparameters of the data obtained from VMware ESXi 5.1 server using esxtopcommand. Multiple virtual machines (VMs) are running in the mentioned server.K-means algorithm is used for clustering the VMs. The goodness of each clusteris determined by Davies Bouldin index and Dunn index respectively. The bestcluster is further identified by the determined indices. The features of thebest cluster are considered into a set of optimal parameters.
arxiv-7800-279 | Differentially Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds | http://arxiv.org/pdf/1405.7085v2.pdf | author:Raef Bassily, Adam Smith, Abhradeep Thakurta category:cs.LG cs.CR stat.ML published:2014-05-27 summary:In this paper, we initiate a systematic investigation of differentiallyprivate algorithms for convex empirical risk minimization. Variousinstantiations of this problem have been studied before. We provide newalgorithms and matching lower bounds for private ERM assuming only that eachdata point's contribution to the loss function is Lipschitz bounded and thatthe domain of optimization is bounded. We provide a separate set of algorithmsand matching lower bounds for the setting in which the loss functions are knownto also be strongly convex. Our algorithms run in polynomial time, and in some cases even match theoptimal non-private running time (as measured by oracle complexity). We giveseparate algorithms (and lower bounds) for $(\epsilon,0)$- and$(\epsilon,\delta)$-differential privacy; perhaps surprisingly, the techniquesused for designing optimal algorithms in the two cases are completelydifferent. Our lower bounds apply even to very simple, smooth function families, such aslinear and quadratic functions. This implies that algorithms from previous workcan be used to obtain optimal error rates, under the additional assumption thatthe contributions of each data point to the loss function is smooth. We showthat simple approaches to smoothing arbitrary loss functions (in order to applyprevious techniques) do not yield optimal error rates. In particular, optimalalgorithms were not previously known for problems such as training supportvector machines and the high-dimensional median.
arxiv-7800-280 | A Modality Lexicon and its use in Automatic Tagging | http://arxiv.org/pdf/1410.4868v1.pdf | author:Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Nathaniel W. Filardo, Lori Levin, Christine Piatko category:cs.CL I.2.7 published:2014-10-17 summary:This paper describes our resource-building results for an eight-week JHUHuman Language Technology Center of Excellence Summer Camp for Applied LanguageExploration (SCALE-2009) on Semantically-Informed Machine Translation.Specifically, we describe the construction of a modality annotation scheme, amodality lexicon, and two automated modality taggers that were built using thelexicon and annotation scheme. Our annotation scheme is based on identifyingthree components of modality: a trigger, a target and a holder. We describe howour modality lexicon was produced semi-automatically, expanding from an initialhand-selected list of modality trigger words and phrases. The resultingexpanded modality lexicon is being made publicly available. We demonstrate thatone tagger---a structure-based tagger---results in precision around 86%(depending on genre) for tagging of a standard LDC data set. In a machinetranslation application, using the structure-based tagger to annotate Englishmodalities on an English-Urdu training corpus improved the translation qualityscore for Urdu by 0.3 Bleu points in the face of sparse training data.
arxiv-7800-281 | Arabic Language Text Classification Using Dependency Syntax-Based Feature Selection | http://arxiv.org/pdf/1410.4863v1.pdf | author:Yannis Haralambous, Yassir Elidrissi, Philippe Lenca category:cs.CL published:2014-10-17 summary:We study the performance of Arabic text classification combining varioustechniques: (a) tfidf vs. dependency syntax, for feature selection andweighting; (b) class association rules vs. support vector machines, forclassification. The Arabic text is used in two forms: rootified and lightlystemmed. The results we obtain show that lightly stemmed text leads to betterperformance than rootified text; that class association rules are better suitedfor small feature sets obtained by dependency syntax constraints; and, finally,that support vector machines are better suited for large feature sets based onmorphological feature selection criteria.
arxiv-7800-282 | Robust Multidimensional Mean-Payoff Games are Undecidable | http://arxiv.org/pdf/1410.5703v1.pdf | author:Yaron Velner category:cs.LO cs.LG published:2014-10-17 summary:Mean-payoff games play a central role in quantitative synthesis andverification. In a single-dimensional game a weight is assigned to everytransition and the objective of the protagonist is to assure a non-negativelimit-average weight. In the multidimensional setting, a weight vector isassigned to every transition and the objective of the protagonist is to satisfya boolean condition over the limit-average weight of each dimension, e.g.,$\LimAvg(x_1) \leq 0 \vee \LimAvg(x_2)\geq 0 \wedge \LimAvg(x_3) \geq 0$. Werecently proved that when one of the players is restricted to finite-memorystrategies then the decidability of determining the winner is inter-reduciblewith Hilbert's Tenth problem over rationals (a fundamental long-standing openproblem). In this work we allow arbitrary (infinite-memory) strategies for bothplayers and we show that the problem is undecidable.
arxiv-7800-283 | Generalized Conditional Gradient for Sparse Estimation | http://arxiv.org/pdf/1410.4828v1.pdf | author:Yaoliang Yu, Xinhua Zhang, Dale Schuurmans category:math.OC cs.LG stat.ML published:2014-10-17 summary:Structured sparsity is an important modeling tool that expands theapplicability of convex formulations for data analysis, however it also createssignificant challenges for efficient algorithm design. In this paper weinvestigate the generalized conditional gradient (GCG) algorithm for solvingstructured sparse optimization problems---demonstrating that, with someenhancements, it can provide a more efficient alternative to current state ofthe art approaches. After providing a comprehensive overview of the convergenceproperties of GCG, we develop efficient methods for evaluating polar operators,a subroutine that is required in each GCG iteration. In particular, we show howthe polar operator can be efficiently evaluated in two important scenarios:dictionary learning and structured sparse estimation. A further improvement isachieved by interleaving GCG with fixed-rank local subspace optimization. Aseries of experiments on matrix completion, multi-class classification,multi-view dictionary learning and overlapping group lasso shows that theproposed method can significantly reduce the training cost of currentalternatives.
arxiv-7800-284 | Convex Optimization in Julia | http://arxiv.org/pdf/1410.4821v1.pdf | author:Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong, Steven Diamond, Stephen Boyd category:math.OC cs.MS stat.ML published:2014-10-17 summary:This paper describes Convex, a convex optimization modeling framework inJulia. Convex translates problems from a user-friendly functional language intoan abstract syntax tree describing the problem. This concise representation ofthe global structure of the problem allows Convex to infer whether the problemcomplies with the rules of disciplined convex programming (DCP), and to passthe problem to a suitable solver. These operations are carried out in Juliausing multiple dispatch, which dramatically reduces the time required to verifyDCP compliance and to parse a problem into conic form. Convex thenautomatically chooses an appropriate backend solver to solve the conic formproblem.
arxiv-7800-285 | Variational Bayes for Merging Noisy Databases | http://arxiv.org/pdf/1410.4792v1.pdf | author:Tamara Broderick, Rebecca C. Steorts category:stat.ME stat.ML published:2014-10-17 summary:Bayesian entity resolution merges together multiple, noisy databases andreturns the minimal collection of unique individuals represented, together withtheir true, latent record values. Bayesian methods allow flexible generativemodels that share power across databases as well as principled quantificationof uncertainty for queries of the final, resolved database. However, existingBayesian methods for entity resolution use Markov monte Carlo method (MCMC)approximations and are too slow to run on modern databases containing millionsor billions of records. Instead, we propose applying variational approximationsto allow scalable Bayesian inference in these models. We derive acoordinate-ascent approximation for mean-field variational Bayes, qualitativelycompare our algorithm to existing methods, note unique challenges for inferencethat arise from the expected distribution of cluster sizes in entityresolution, and discuss directions for future work in this domain.
arxiv-7800-286 | A Hierarchical Multi-Output Nearest Neighbor Model for Multi-Output Dependence Learning | http://arxiv.org/pdf/1410.4777v1.pdf | author:Richard G. Morris, Tony Martinez, Michael R. Smith category:stat.ML cs.LG published:2014-10-17 summary:Multi-Output Dependence (MOD) learning is a generalization of standardclassification problems that allows for multiple outputs that are dependent oneach other. A primary issue that arises in the context of MOD learning is thatfor any given input pattern there can be multiple correct output patterns. Thischanges the learning task from function approximation to relationapproximation. Previous algorithms do not consider this problem, and thuscannot be readily applied to MOD problems. To perform MOD learning, weintroduce the Hierarchical Multi-Output Nearest Neighbor model (HMONN) thatemploys a basic learning model for each output and a modified nearest neighborapproach to refine the initial results.
arxiv-7800-287 | mS2GD: Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting | http://arxiv.org/pdf/1410.4744v1.pdf | author:Jakub Konečný, Jie Liu, Peter Richtárik, Martin Takáč category:cs.LG stat.ML published:2014-10-17 summary:We propose a mini-batching scheme for improving the theoretical complexityand practical performance of semi-stochastic gradient descent applied to theproblem of minimizing a strongly convex composite function represented as thesum of an average of a large number of smooth convex functions, and simplenonsmooth convex function. Our method first performs a deterministic step(computation of the gradient of the objective function at the starting point),followed by a large number of stochastic steps. The process is repeated a fewtimes with the last iterate becoming the new starting point. The novelty of ourmethod is in introduction of mini-batching into the computation of stochasticsteps. In each step, instead of choosing a single function, we sample $b$functions, compute their gradients, and compute the direction based on this. Weanalyze the complexity of the method and show that the method benefits from twospeedup effects. First, we prove that as long as $b$ is below a certainthreshold, we can reach predefined accuracy with less overall work than withoutmini-batching. Second, our mini-batching scheme admits a simple parallelimplementation, and hence is suitable for further acceleration byparallelization.
arxiv-7800-288 | Heuristic algorithm for 1D and 2D unfolding | http://arxiv.org/pdf/1411.1375v1.pdf | author:Yordan Karadzhov category:stat.ML published:2014-10-17 summary:A very simple heuristic approach to the unfolding problem will be described.An iterative algorithm starts with an empty histogram and every iteration aimsto add one entry to this histogram. The entry to be added is selected accordingto a criteria which includes a $\chi^2$ test and a regularization. After arelatively small number of iterations (500 - 1000) the growing reconstructeddistribution converges to the true distribution.
arxiv-7800-289 | Learning Soft Linear Constraints with Application to Citation Field Extraction | http://arxiv.org/pdf/1403.1349v2.pdf | author:Sam Anzaroot, Alexandre Passos, David Belanger, Andrew McCallum category:cs.CL cs.DL cs.IR published:2014-03-06 summary:Accurately segmenting a citation string into fields for authors, titles, etc.is a challenging task because the output typically obeys various globalconstraints. Previous work has shown that modeling soft constraints, where themodel is encouraged, but not require to obey the constraints, can substantiallyimprove segmentation performance. On the other hand, for imposing hardconstraints, dual decomposition is a popular technique for efficient predictiongiven existing algorithms for unconstrained inference. We extend the techniqueto perform prediction subject to soft constraints. Moreover, with a techniquefor performing inference given soft constraints, it is easy to automaticallygenerate large families of constraints and learn their costs with a simpleconvex optimization problem during training. This allows us to obtainsubstantial gains in accuracy on a new, challenging citation extractiondataset.
arxiv-7800-290 | KCRC-LCD: Discriminative Kernel Collaborative Representation with Locality Constrained Dictionary for Visual Categorization | http://arxiv.org/pdf/1410.4673v1.pdf | author:Weiyang Liu, Zhiding Yu, Lijia Lu, Yandong Wen, Hui Li, Yuexian Zou category:cs.CV cs.LG published:2014-10-17 summary:We consider the image classification problem via kernel collaborativerepresentation classification with locality constrained dictionary (KCRC-LCD).Specifically, we propose a kernel collaborative representation classification(KCRC) approach in which kernel method is used to improve the discriminationability of collaborative representation classification (CRC). We then measurethe similarities between the query and atoms in the global dictionary in orderto construct a locality constrained dictionary (LCD) for KCRC. In addition, wediscuss several similarity measure approaches in LCD and further present asimple yet effective unified similarity measure whose superiority is validatedin experiments. There are several appealing aspects associated with LCD. First,LCD can be nicely incorporated under the framework of KCRC. The LCD similaritymeasure can be kernelized under KCRC, which theoretically links CRC and LCDunder the kernel method. Second, KCRC-LCD becomes more scalable to both thetraining set size and the feature dimension. Example shows that KCRC is able toperfectly classify data with certain distribution, while conventional CRC failscompletely. Comprehensive experiments on many public datasets also show thatKCRC-LCD is a robust discriminative classifier with both excellent performanceand good scalability, being comparable or outperforming many otherstate-of-the-art approaches.
arxiv-7800-291 | MKL-RT: Multiple Kernel Learning for Ratio-trace Problems via Convex Optimization | http://arxiv.org/pdf/1410.4470v2.pdf | author:Raviteja Vemulapalli, Vinay Praneeth Boda, Rama Chellappa category:cs.CV cs.LG published:2014-10-16 summary:In the recent past, automatic selection or combination of kernels (orfeatures) based on multiple kernel learning (MKL) approaches has been receivingsignificant attention from various research communities. Though MKL has beenextensively studied in the context of support vector machines (SVM), it isrelatively less explored for ratio-trace problems. In this paper, we show thatMKL can be formulated as a convex optimization problem for a general class ofratio-trace problems that encompasses many popular algorithms used in variouscomputer vision applications. We also provide an optimization procedure that isguaranteed to converge to the global optimum of the proposed optimizationproblem. We experimentally demonstrate that the proposed MKL approach, which werefer to as MKL-RT, can be successfully used to select features fordiscriminative dimensionality reduction and cross-modal retrieval. We also showthat the proposed convex MKL-RT approach performs better than the recentlyproposed non-convex MKL-DR approach.
arxiv-7800-292 | Robust Topological Feature Extraction for Mapping of Environments using Bio-Inspired Sensor Networks | http://arxiv.org/pdf/1410.4622v1.pdf | author:Alireza Dirafzoon, Edgar Lobaton category:cs.RO cs.SY math.AT stat.ML published:2014-10-17 summary:In this paper, we exploit minimal sensing information gathered frombiologically inspired sensor networks to perform exploration and mapping in anunknown environment. A probabilistic motion model of mobile sensing nodes,inspired by motion characteristics of cockroaches, is utilized to extract weakencounter information in order to build a topological representation of theenvironment. Neighbor to neighbor interactions among the nodes are exploited to buildpoint clouds representing spatial features of the manifold characterizing theenvironment based on the sampled data. To extract dominant features from sampled data, topological data analysis isused to produce persistence intervals for features, to be used for topologicalmapping. In order to improve robustness characteristics of the sampled datawith respect to outliers, density based subsampling algorithms are employed.Moreover, a robust scale-invariant classification algorithm for persistencediagrams is proposed to provide a quantitative representation of desiredfeatures in the data. Furthermore, various strategies for defining encountermetrics with different degrees of information regarding agents' motion aresuggested to enhance the precision of the estimation and classificationperformance of the topological method.
arxiv-7800-293 | Volumes of logistic regression models with applications to model selection | http://arxiv.org/pdf/1408.0881v3.pdf | author:James G. Dowty category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH published:2014-08-05 summary:Logistic regression models with $n$ observations and $q$ linearly-independentcovariates are shown to have Fisher information volumes which are bounded belowby $\pi^q$ and above by ${n \choose q} \pi^q$. This is proved with a novelgeneralization of the classical theorems of Pythagoras and de Gua, which is ofindependent interest. The finding that the volume is always finite is new, andit implies that the volume can be directly interpreted as a measure of modelcomplexity. The volume is shown to be a continuous function of the designmatrix $X$ at generic $X$, but to be discontinuous in general. This means thatmodels with sparse design matrices can be significantly less complex thannearby models, so the resulting model-selection criterion prefers sparsemodels. This is analogous to the way that $\ell^1$-regularisation tends toprefer sparse model fits, though in our case this behaviour arisesspontaneously from general principles. Lastly, an unusual topological dualityis shown to exist between the ideal boundaries of the natural and expectationparameter spaces of logistic regression models.
arxiv-7800-294 | An Overview of General Performance Metrics of Binary Classifier Systems | http://arxiv.org/pdf/1410.5330v1.pdf | author:Sebastian Raschka category:cs.LG published:2014-10-17 summary:This document provides a brief overview of different metrics and terminologythat is used to measure the performance of binary classification systems.
arxiv-7800-295 | Domain-Independent Optimistic Initialization for Reinforcement Learning | http://arxiv.org/pdf/1410.4604v1.pdf | author:Marlos C. Machado, Sriram Srinivasan, Michael Bowling category:cs.LG cs.AI published:2014-10-16 summary:In Reinforcement Learning (RL), it is common to use optimistic initializationof value functions to encourage exploration. However, such an approachgenerally depends on the domain, viz., the scale of the rewards must be known,and the feature representation must have a constant norm. We present a simpleapproach that performs optimistic initialization with less dependence on thedomain.
arxiv-7800-296 | Learning a hyperplane regressor by minimizing an exact bound on the VC dimension | http://arxiv.org/pdf/1410.4573v1.pdf | author:Jayadeva, Suresh Chandra, Siddarth Sabharwal, Sanjit S. Batra category:cs.LG I.5.1; I.5.2 published:2014-10-16 summary:The capacity of a learning machine is measured by its Vapnik-Chervonenkisdimension, and learning machines with a low VC dimension generalize better. Itis well known that the VC dimension of SVMs can be very large or unbounded,even though they generally yield state-of-the-art learning performance. In thispaper, we show how to learn a hyperplane regressor by minimizing an exact, or\boldmath{$\Theta$} bound on its VC dimension. The proposed approach, termed asthe Minimal Complexity Machine (MCM) Regressor, involves solving a simplelinear programming problem. Experimental results show, that on a number ofbenchmark datasets, the proposed approach yields regressors with error ratesmuch less than those obtained with conventional SVM regresssors, while oftenusing fewer support vectors. On some benchmark datasets, the number of supportvectors is less than one tenth the number used by SVMs, indicating that the MCMdoes indeed learn simpler representations.
arxiv-7800-297 | Reconstructive Sparse Code Transfer for Contour Detection and Semantic Labeling | http://arxiv.org/pdf/1410.4521v1.pdf | author:Michael Maire, Stella X. Yu, Pietro Perona category:cs.CV published:2014-10-16 summary:We frame the task of predicting a semantic labeling as a sparsereconstruction procedure that applies a target-specific learned transferfunction to a generic deep sparse code representation of an image. Thisstrategy partitions training into two distinct stages. First, in anunsupervised manner, we learn a set of generic dictionaries optimized forsparse coding of image patches. We train a multilayer representation viarecursive sparse dictionary learning on pooled codes output by earlier layers.Second, we encode all training images with the generic dictionaries and learn atransfer function that optimizes reconstruction of patches extracted fromannotated ground-truth given the sparse codes of their corresponding imagepatches. At test time, we encode a novel image using the generic dictionariesand then reconstruct using the transfer function. The output reconstruction isa semantic labeling of the test image. Applying this strategy to the task of contour detection, we demonstrateperformance competitive with state-of-the-art systems. Unlike almost all priorwork, our approach obviates the need for any form of hand-designed features orfilters. To illustrate general applicability, we also show initial results onsemantic part labeling of human faces. The effectiveness of our approach opens new avenues for research on deepsparse representations. Our classifiers utilize this representation in a novelmanner. Rather than acting on nodes in the deepest layer, they attach to nodesalong a slice through multiple layers of the network in order to makepredictions about local patches. Our flexible combination of a generativelylearned sparse representation with discriminatively trained transferclassifiers extends the notion of sparse reconstruction to encompass arbitrarysemantic labeling tasks.
arxiv-7800-298 | Improve CAPTCHA's Security Using Gaussian Blur Filter | http://arxiv.org/pdf/1410.4441v1.pdf | author:Ariyan Zarei category:cs.CV published:2014-10-16 summary:Providing security for webservers against unwanted and automatedregistrations has become a big concern. To prevent these kinds of falseregistrations many websites use CAPTCHAs. Among all kinds of CAPTCHAs OCR-Basedor visual CAPTCHAs are very common. Actually visual CAPTCHA is an imagecontaining a sequence of characters. So far most of visual CAPTCHAs, in orderto resist against OCR programs, use some common implementations such aswrapping the characters, random placement and rotations of characters, etc. Inthis paper we applied Gaussian Blur filter, which is an image transformation,to visual CAPTCHAs to reduce their readability by OCR programs. We concludedthat this technique made CAPTCHAs almost unreadable for OCR programs but, theirreadability by human users still remained high.
arxiv-7800-299 | Unbiased Black-Box Complexities of Jump Functions | http://arxiv.org/pdf/1403.7806v2.pdf | author:Benjamin Doerr, Carola Doerr, Timo Kötzing category:cs.NE F.2.2 published:2014-03-30 summary:We analyze the unbiased black-box complexity of jump functions with small,medium, and large sizes of the fitness plateau surrounding the optimalsolution. Among other results, we show that when the jump size is $(1/2 -\varepsilon)n$, that is, only a small constant fraction of the fitness valuesis visible, then the unbiased black-box complexities for arities $3$ and higherare of the same order as those for the simple \textsc{OneMax} function. Evenfor the extreme jump function, in which all but the two fitness values $n/2$and $n$ are blanked out, polynomial-time mutation-based (i.e., unary unbiased)black-box optimization algorithms exist. This is quite surprising given thatfor the extreme jump function almost the whole search space (all but a$\Theta(n^{-1/2})$ fraction) is a plateau of constant fitness. To prove these results, we introduce new tools for the analysis of unbiasedblack-box complexities, for example, selecting the new parent individual not bycomparing the fitnesses of the competing search points, but also by taking intoaccount the (empirical) expected fitnesses of their offspring.
arxiv-7800-300 | Multi-Agent Shape Formation and Tracking Inspired from a Social Foraging Dynamics | http://arxiv.org/pdf/1410.3864v2.pdf | author:Debdipta Goswami, Chiranjib Saha, Kunal Pal, Swagatam Das category:cs.NE cs.RO 70F04 published:2014-10-14 summary:Principle of Swarm Intelligence has recently found widespread application information control and automated tracking by the automated multi-agent system.This article proposes an elegant and effective method inspired by foragingdynamics to produce geometric-patterns by the search agents. Starting from arandom initial orientation, it is investigated how the foraging dynamics can bemodified to achieve convergence of the agents on the desired pattern withalmost uniform density. Guided through the proposed dynamics, the agents canalso track a moving point by continuously circulating around the point. Ananalytical treatment supported with computer simulation results is provided tobetter understand the convergence behaviour of the system.
