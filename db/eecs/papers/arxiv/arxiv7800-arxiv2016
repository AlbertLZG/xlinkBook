arxiv-7800-1 | Robust 3D face recognition in presence of pose and partial occlusions or missing parts | http://arxiv.org/pdf/1408.3709v1.pdf | author:Parama Bagchi, Debotosh Bhattacharjee, Mita Nasipuri category:cs.CV published:2014-08-16 summary:In this paper, we propose a robust 3D face recognition system which canhandle pose as well as occlusions in real world. The system at first takes asinput, a 3D range image, simultaneously registers it using ICP(IterativeClosest Point) algorithm. ICP used in this work, registers facial surfaces to acommon model by minimizing distances between a probe model and a gallery model.However the performance of ICP relies heavily on the initial conditions. Hence,it is necessary to provide an initial registration, which will be improvediteratively and finally converge to the best alignment possible. Once the facesare registered, the occlusions are automatically extracted by thresholding thedepth map values of the 3D image. After the occluded regions are detected,restoration is done by Principal Component Analysis (PCA). The restored images,after the removal of occlusions, are then fed to the recognition system forclassification purpose. Features are extracted from the reconstructednon-occluded face images in the form of face normals. The experimental resultswhich were obtained on the occluded facial images from the Bosphorus 3D facedatabase, illustrate that our occlusion compensation scheme has attained arecognition accuracy of 91.30%.
arxiv-7800-2 | Multi-Sensor Event Detection using Shape Histograms | http://arxiv.org/pdf/1408.3733v1.pdf | author:Ehtesham Hassan, Gautam Shroff, Puneet Agarwal category:cs.LG published:2014-08-16 summary:Vehicular sensor data consists of multiple time-series arising from a numberof sensors. Using such multi-sensor data we would like to detect occurrences ofspecific events that vehicles encounter, e.g., corresponding to particularmaneuvers that a vehicle makes or conditions that it encounters. Events arecharacterized by similar waveform patterns re-appearing within one or moresensors. Further such patterns can be of variable duration. In this work, wepropose a method for detecting such events in time-series data using a novelfeature descriptor motivated by similar ideas in image processing. We definethe shape histogram: a constant dimension descriptor that nevertheless capturespatterns of variable duration. We demonstrate the efficacy of using shapehistograms as features to detect events in an SVM-based, multi-sensor,supervised learning scenario, i.e., multiple time-series are used to detect anevent. We present results on real-life vehicular sensor data and show that ourtechnique performs better than available pattern detection implementations onour data, and that it can also be used to combine features from multiplesensors resulting in better accuracy than using any single sensor. Sinceprevious work on pattern detection in time-series has been in the single seriescontext, we also present results using our technique on multiple standardtime-series datasets and show that it is the most versatile in terms of how itranks compared to other published results.
arxiv-7800-3 | Analysis of a chaotic spiking neural model: The NDS neuron | http://arxiv.org/pdf/1408.3735v1.pdf | author:Mohammad Alhawarat, Waleed Nazih, Mohammad Eldesouki category:cs.NE nlin.CD published:2014-08-16 summary:Further analysis and experimentation is carried out in this paper for achaotic dynamic model, viz. the Nonlinear Dynamic State neuron (NDS). Theanalysis and experimentations are performed to further understand theunderlying dynamics of the model and enhance it as well. Chaos provides manyinteresting properties that can be exploited to achieve computational tasks.Such properties are sensitivity to initial conditions, space filling, controland synchronization.Chaos might play an important role in informationprocessing tasks in human brain as suggested by biologists. If artificialneural networks (ANNs) is equipped with chaos then it will enrich the dynamicbehaviours of such networks. The NDS model has some limitations and can beovercome in different ways. In this paper different approaches are followed topush the boundaries of the NDS model in order to enhance it. One way is tostudy the effects of scaling the parameters of the chaotic equations of the NDSmodel and study the resulted dynamics. Another way is to study the method thatis used in discretization of the original R\"{o}ssler that the NDS model isbased on. These approaches have revealed some facts about the NDS attractor andsuggest why such a model can be stabilized to large number of unstable periodicorbits (UPOs) which might correspond to memories in phase space.
arxiv-7800-4 | A fast patch-dictionary method for whole image recovery | http://arxiv.org/pdf/1408.3740v1.pdf | author:Yangyang Xu, Wotao Yin category:cs.CV math.OC 94A08, 94A12 published:2014-08-16 summary:Various algorithms have been proposed for dictionary learning. Among thosefor image processing, many use image patches to form dictionaries. This paperfocuses on whole-image recovery from corrupted linear measurements. We addressthe open issue of representing an image by overlapping patches: the overlappingleads to an excessive number of dictionary coefficients to determine. With veryfew exceptions, this issue has limited the applications of image-patch methodsto the local kind of tasks such as denoising, inpainting, cartoon-texturedecomposition, super-resolution, and image deblurring, for which one canprocess a few patches at a time. Our focus is global imaging tasks such ascompressive sensing and medical image recovery, where the whole image isencoded together, making it either impossible or very ineffective to update afew patches at a time. Our strategy is to divide the sparse recovery into multiple subproblems, eachof which handles a subset of non-overlapping patches, and then the results ofthe subproblems are averaged to yield the final recovery. This simple strategyis surprisingly effective in terms of both quality and speed. In addition, weaccelerate computation of the learned dictionary by applying a recent blockproximal-gradient method, which not only has a lower per-iteration complexitybut also takes fewer iterations to converge, compared to the currentstate-of-the-art. We also establish that our algorithm globally converges to astationary point. Numerical results on synthetic data demonstrate that ouralgorithm can recover a more faithful dictionary than two state-of-the-artmethods. Combining our whole-image recovery and dictionary-learning methods, wenumerically simulate image inpainting, compressive sensing recovery, anddeblurring. Our recovery is more faithful than those of a total variationmethod and a method based on overlapping patches.
arxiv-7800-5 | Real-time emotion recognition for gaming using deep convolutional network features | http://arxiv.org/pdf/1408.3750v1.pdf | author:SÃ©bastien Ouellet category:cs.CV cs.LG cs.NE published:2014-08-16 summary:The goal of the present study is to explore the application of deepconvolutional network features to emotion recognition. Results indicate thatthey perform similarly to other published models at a best recognition rate of94.4%, and do so with a single still image rather than a video stream. Animplementation of an affective feedback game is also described, where aclassifier using these features tracks the facial expressions of a player inreal-time.
arxiv-7800-6 | Inverse Reinforcement Learning with Multi-Relational Chains for Robot-Centered Smart Home | http://arxiv.org/pdf/1408.3727v5.pdf | author:Kun Li, Max Q. -H. Meng category:cs.RO cs.LG published:2014-08-16 summary:In a robot-centered smart home, the robot observes the home states with itsown sensors, and then it can change certain object states according to anoperator's commands for remote operations, or imitate the operator's behaviorsin the house for autonomous operations. To model the robot's imitation of theoperator's behaviors in a dynamic indoor environment, we use multi-relationalchains to describe the changes of environment states, and apply inversereinforcement learning to encoding the operator's behaviors with a learnedreward function. We implement this approach with a mobile robot, and do fiveexperiments to include increasing training days, object numbers, and actiontypes. Besides, a baseline method by directly recording the operator'sbehaviors is also implemented, and comparison is made on the accuracy of homestate evaluation and the accuracy of robot action selection. The results showthat the proposed approach handles dynamic environment well, and guides therobot's actions in the house more accurately.
arxiv-7800-7 | Highly Accurate Multispectral Palmprint Recognition Using Statistical and Wavelet Features | http://arxiv.org/pdf/1408.3772v2.pdf | author:Shervin Minaee, AmirAli Abdolrashidi category:cs.CV published:2014-08-16 summary:Palmprint is one of the most useful physiological biometrics that can be usedas a powerful means in personal recognition systems. The major features of thepalmprints are palm lines, wrinkles and ridges, and many approaches use them indifferent ways towards solving the palmprint recognition problem. Here we haveproposed to use a set of statistical and wavelet-based features; statistical tocapture the general characteristics of palmprints; and wavelet-based to findthose information not evident in the spatial domain. Also we use two differentclassification approaches, minimum distance classifier scheme and weightedmajority voting algorithm, to perform palmprint matching. The proposed methodis tested on a well-known palmprint dataset of 6000 samples and has shown animpressive accuracy rate of 99.65\%-100\% for most scenarios.
arxiv-7800-8 | Stability and Performance Limits of Adaptive Primal-Dual Networks | http://arxiv.org/pdf/1408.3693v3.pdf | author:Zaid J. Towfic, Ali H. Sayed category:math.OC cs.DC cs.LG cs.MA published:2014-08-16 summary:This work studies distributed primal-dual strategies for adaptation andlearning over networks from streaming data. Two first-order methods areconsidered based on the Arrow-Hurwicz (AH) and augmented Lagrangian (AL)techniques. Several revealing results are discovered in relation to theperformance and stability of these strategies when employed over adaptivenetworks. The conclusions establish that the advantages that these methods havefor deterministic optimization problems do not necessarily carry over tostochastic optimization problems. It is found that they have narrower stabilityranges and worse steady-state mean-square-error performance than primal methodsof the consensus and diffusion type. It is also found that the AH technique canbecome unstable under a partial observation model, while the other techniquesare able to recover the unknown under this scenario. A method to enhance theperformance of AL strategies is proposed by tying the selection of thestep-size to their regularization parameter. It is shown that this methodallows the AL algorithm to approach the performance of consensus and diffusionstrategies but that it remains less stable than these other strategies.
arxiv-7800-9 | Unsupervised Keyword Extraction from Polish Legal Texts | http://arxiv.org/pdf/1408.3731v2.pdf | author:MichaÅ Jungiewicz, MichaÅ ÅopuszyÅski category:cs.CL published:2014-08-16 summary:In this work, we present an application of the recently proposed unsupervisedkeyword extraction algorithm RAKE to a corpus of Polish legal texts from thefield of public procurement. RAKE is essentially a language and domainindependent method. Its only language-specific input is a stoplist containing aset of non-content words. The performance of the method heavily depends on thechoice of such a stoplist, which should be domain adopted. Therefore, wecomplement RAKE algorithm with an automatic approach to selecting non-contentwords, which is based on the statistical properties of term distribution.
arxiv-7800-10 | Parallel software implementation of recursive multidimensional digital filters for point-target detection in cluttered infrared scenes | http://arxiv.org/pdf/1408.3526v4.pdf | author:Hugh L. Kennedy category:cs.CV published:2014-08-15 summary:A technique for the enhancement of point targets in clutter is described. Thelocal 3-D spectrum at each pixel is estimated recursively. An opticalflow-field for the textured background is then generated using the 3-Dautocorrelation function and the local velocity estimates are used to applyhigh-pass velocity-selective spatiotemporal filters, with finite impulseresponses (FIRs), to subtract the background clutter signal, leaving theforeground target signal, plus noise. Parallel software implementations using amulticore central processing unit (CPU) and a graphical processing unit (GPU)are investigated.
arxiv-7800-11 | Turkish Presidential Elections TRT Publicity Speech Facial Expression Analysis | http://arxiv.org/pdf/1408.3573v1.pdf | author:H. Emrah Tasli, Paul Ivan category:cs.CV published:2014-08-15 summary:In this paper, facial expressions of the three Turkish presidentialcandidates Demirtas, Erdogan and Ihsanoglu (in alphabetical order) are analyzedduring the publicity speeches featured at TRT (Turkish Radio and Television) on03.08.2014. FaceReader is used for the analysis where 3D modeling of the faceis achieved using the active appearance models (AAM). Over 500 landmark pointsare tracked and analyzed for obtaining the facial expressions during the wholespeech. All source videos and the data are publicly available for researchpurposes.
arxiv-7800-12 | SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation | http://arxiv.org/pdf/1408.3456v1.pdf | author:Felix Hill, Roi Reichart, Anna Korhonen category:cs.CL published:2014-08-15 summary:We present SimLex-999, a gold standard resource for evaluating distributionalsemantic models that improves on existing resources in several important ways.First, in contrast to gold standards such as WordSim-353 and MEN, it explicitlyquantifies similarity rather than association or relatedness, so that pairs ofentities that are associated but not actually similar [Freud, psychology] havea low rating. We show that, via this focus on similarity, SimLex-999incentivizes the development of models with a different, and arguably widerrange of applications than those which reflect conceptual association. Second,SimLex-999 contains a range of concrete and abstract adjective, noun and verbpairs, together with an independent rating of concreteness and (free)association strength for each pair. This diversity enables fine-grainedanalyses of the performance of models on concepts of different types, andconsequently greater insight into how architectures can be improved. Further,unlike existing gold standard evaluations, for which automatic approaches havereached or surpassed the inter-annotator agreement ceiling, state-of-the-artmodels perform well below this ceiling on SimLex-999. There is therefore plentyof scope for SimLex-999 to quantify future improvements to distributionalsemantic models, guiding the development of the next generation ofrepresentation-learning architectures.
arxiv-7800-13 | Robust Statistical Ranking: Theory and Algorithms | http://arxiv.org/pdf/1408.3467v1.pdf | author:Qianqian Xu, Jiechao Xiong, Qingming Huang, Yuan Yao category:stat.ME cs.LG stat.ML published:2014-08-15 summary:Deeply rooted in classical social choice and voting theory, statisticalranking with paired comparison data experienced its renaissance with the widespread of crowdsourcing technique. As the data quality might be significantlydamaged in an uncontrolled crowdsourcing environment, outlier detection androbust ranking have become a hot topic in such data analysis. In this paper, wepropose a robust ranking framework based on the principle of Huber's robuststatistics, which formulates outlier detection as a LASSO problem to findsparse approximations of the cyclic ranking projection in Hodge decomposition.Moreover, simple yet scalable algorithms are developed based on LinearizedBregman Iteration to achieve an even less biased estimator than LASSO.Statistical consistency of outlier detection is established in both cases whichstates that when the outliers are strong enough and in Erdos-Renyi random graphsampling settings, outliers can be faithfully detected. Our studies aresupported by experiments with both simulated examples and real-world data. Theproposed framework provides us a promising tool for robust ranking with largescale crowdsourcing data arising from computer vision, multimedia, machinelearning, sociology, etc.
arxiv-7800-14 | Indexing Cost Sensitive Prediction | http://arxiv.org/pdf/1408.4072v1.pdf | author:Leilani Battle, Edward Benson, Aditya Parameswaran, Eugene Wu category:cs.LG cs.DB cs.DS published:2014-08-15 summary:Predictive models are often used for real-time decision making. However,typical machine learning techniques ignore feature evaluation cost, and focussolely on the accuracy of the machine learning models obtained utilizing allthe features available. We develop algorithms and indexes to supportcost-sensitive prediction, i.e., making decisions using machine learning modelstaking feature evaluation cost into account. Given an item and a onlinecomputation cost (i.e., time) budget, we present two approaches to return anappropriately chosen machine learning model that will run within the specifiedtime on the given item. The first approach returns the optimal machine learningmodel, i.e., one with the highest accuracy, that runs within the specifiedtime, but requires significant up-front precomputation time. The secondapproach returns a possibly sub- optimal machine learning model, but requireslittle up-front precomputation time. We study these two algorithms in detailand characterize the scenarios (using real and synthetic data) in which eachperforms well. Unlike prior work that focuses on a narrow domain or a specificalgorithm, our techniques are very general: they apply to any cost-sensitiveprediction scenario on any machine learning algorithm.
arxiv-7800-15 | On Pairwise Costs for Network Flow Multi-Object Tracking | http://arxiv.org/pdf/1408.3304v2.pdf | author:Visesh Chari, Simon Lacoste-Julien, Ivan Laptev, Josef Sivic category:cs.CV math.OC published:2014-08-14 summary:Multi-object tracking has been recently approached with the min-cost networkflow optimization techniques. Such methods simultaneously resolve multipleobject tracks in a video and enable modeling of dependencies among tracks.Min-cost network flow methods also fit well within the "tracking-by-detection"paradigm where object trajectories are obtained by connecting per-frame outputsof an object detector. Object detectors, however, often fail due to occlusionsand clutter in the video. To cope with such situations, we propose to addpairwise costs to the min-cost network flow framework. While integer solutionsto such a problem become NP-hard, we design a convex relaxation solution withan efficient rounding heuristic which empirically gives certificates of smallsuboptimality. We evaluate two particular types of pairwise costs anddemonstrate improvements over recent tracking methods in real-world videosequences.
arxiv-7800-16 | Beta diffusion trees and hierarchical feature allocations | http://arxiv.org/pdf/1408.3378v2.pdf | author:Creighton Heaukulani, David A. Knowles, Zoubin Ghahramani category:stat.ML published:2014-08-14 summary:We define the beta diffusion tree, a random tree structure with a set ofleaves that defines a collection of overlapping subsets of objects, known as afeature allocation. A generative process for the tree structure is defined interms of particles (representing the objects) diffusing in some continuousspace, analogously to the Dirichlet diffusion tree (Neal, 2003), which definesa tree structure over partitions (i.e., non-overlapping subsets) of theobjects. Unlike in the Dirichlet diffusion tree, multiple copies of a particlemay exist and diffuse along multiple branches in the beta diffusion tree, andan object may therefore belong to multiple subsets of particles. We demonstratehow to build a hierarchically-clustered factor analysis model with the betadiffusion tree and how to perform inference over the random tree structureswith a Markov chain Monte Carlo algorithm. We conclude with several numericalexperiments on missing data problems with data sets of gene expressionmicroarrays, international development statistics, and intranationalsocioeconomic measurements.
arxiv-7800-17 | Indefinitely Oscillating Martingales | http://arxiv.org/pdf/1408.3169v1.pdf | author:Jan Leike, Marcus Hutter category:cs.LG math.PR math.ST stat.TH published:2014-08-14 summary:We construct a class of nonnegative martingale processes that oscillateindefinitely with high probability. For these processes, we state a uniformrate of the number of oscillations and show that this rate is asymptoticallyclose to the theoretical upper bound. These bounds on probability andexpectation of the number of upcrossings are compared to classical bounds fromthe martingale literature. We discuss two applications. First, our resultsimply that the limit of the minimum description length operator may not exist.Second, we give bounds on how often one can change one's belief in a givenhypothesis when observing a stream of data.
arxiv-7800-18 | Likely to stop? Predicting Stopout in Massive Open Online Courses | http://arxiv.org/pdf/1408.3382v1.pdf | author:Colin Taylor, Kalyan Veeramachaneni, Una-May O'Reilly category:cs.CY cs.LG published:2014-08-14 summary:Understanding why students stopout will help in understanding how studentslearn in MOOCs. In this report, part of a 3 unit compendium, we describe how webuild accurate predictive models of MOOC student stopout. We document ascalable, stopout prediction methodology, end to end, from raw source data tomodel analysis. We attempted to predict stopout for the Fall 2012 offering of6.002x. This involved the meticulous and crowd-sourced engineering of over 25predictive features extracted for thousands of students, the creation oftemporal and non-temporal data representations for use in predictive modeling,the derivation of over 10 thousand models with a variety of state-of-the-artmachine learning techniques and the analysis of feature importance by examiningover 70000 models. We found that stop out prediction is a tractable problem.Our models achieved an AUC (receiver operating characteristicarea-under-the-curve) as high as 0.95 (and generally 0.88) when predicting oneweek in advance. Even with more difficult prediction problems, such aspredicting stop out at the end of the course with only one weeks' data, themodels attained AUCs of 0.7.
arxiv-7800-19 | 2D View Aggregation for Lymph Node Detection Using a Shallow Hierarchy of Linear Classifiers | http://arxiv.org/pdf/1408.3337v1.pdf | author:Ari Seff, Le Lu, Kevin M. Cherry, Holger Roth, Jiamin Liu, Shijun Wang, Joanne Hoffman, Evrim B. Turkbey, Ronald M. Summers category:cs.CV cs.LG published:2014-08-14 summary:Enlarged lymph nodes (LNs) can provide important information for cancerdiagnosis, staging, and measuring treatment reactions, making automateddetection a highly sought goal. In this paper, we propose a new algorithmrepresentation of decomposing the LN detection problem into a set of 2D objectdetection subtasks on sampled CT slices, largely alleviating the curse ofdimensionality issue. Our 2D detection can be effectively formulated as linearclassification on a single image feature type of Histogram of OrientedGradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. Weexploit both simple pooling and sparse linear fusion schemes to aggregate these2D detection scores for the final 3D LN detection. In this manner, detection ismore tractable and does not need to perform perfectly at instance level (asweak hypotheses) since our aggregation process will robustly harness collectiveinformation for LN detection. Two datasets (90 patients with 389 mediastinalLNs and 86 patients with 595 abdominal LNs) are used for validation.Cross-validation demonstrates 78.0% sensitivity at 6 false positives/volume(FP/vol.) (86.1% at 10 FP/vol.) and 73.1% sensitivity at 6 FP/vol. (87.2% at 10FP/vol.), for the mediastinal and abdominal datasets respectively. Our resultscompare favorably to previous state-of-the-art methods.
arxiv-7800-20 | Exact and empirical estimation of misclassification probability | http://arxiv.org/pdf/1408.3332v1.pdf | author:Victor Nedelko category:stat.ML cs.LG published:2014-08-14 summary:We discuss the problem of risk estimation in the classification problem, withspecific focus on finding distributions that maximize the confidence intervalsof risk estimation. We derived simple analytic approximations for the maximumbias of empirical risk for histogram classifier. We carry out a detailed studyon using these analytic estimates for empirical estimation of risk.
arxiv-7800-21 | Toward Automated Discovery of Artistic Influence | http://arxiv.org/pdf/1408.3218v1.pdf | author:Babak Saleh, Kanako Abe, Ravneet Singh Arora, Ahmed Elgammal category:cs.CV cs.LG published:2014-08-14 summary:Considering the huge amount of art pieces that exist, there is valuableinformation to be discovered. Examining a painting, an expert can determine itsstyle, genre, and the time period that the painting belongs. One important taskfor art historians is to find influences and connections between artists. Isinfluence a task that a computer can measure? The contribution of this paper isin exploring the problem of computer-automated suggestion of influences betweenartists, a problem that was not addressed before in a general setting. We firstpresent a comparative study of different classification methodologies for thetask of fine-art style classification. A two-level comparative study isperformed for this classification problem. The first level reviews theperformance of discriminative vs. generative models, while the second leveltouches the features aspect of the paintings and compares semantic-levelfeatures vs. low-level and intermediate-level features present in the painting.Then, we investigate the question "Who influenced this artist?" by looking athis masterpieces and comparing them to others. We pose this interestingquestion as a knowledge discovery problem. For this purpose, we investigatedseveral painting-similarity and artist-similarity measures. As a result, weprovide a visualization of artists (Map of Artists) based on the similaritybetween their works
arxiv-7800-22 | A brief survey on deep belief networks and introducing a new object oriented toolbox (DeeBNet) | http://arxiv.org/pdf/1408.3264v7.pdf | author:Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour category:cs.CV cs.LG cs.MS cs.NE 68T01 published:2014-08-14 summary:Nowadays, this is very popular to use the deep architectures in machinelearning. Deep Belief Networks (DBNs) are deep architectures that use stack ofRestricted Boltzmann Machines (RBM) to create a powerful generative model usingtraining data. DBNs have many ability like feature extraction andclassification that are used in many applications like image processing, speechprocessing and etc. This paper introduces a new object oriented MATLAB toolboxwith most of abilities needed for the implementation of DBNs. In the newversion, the toolbox can be used in Octave. According to the results of theexperiments conducted on MNIST (image), ISOLET (speech), and 20 Newsgroups(text) datasets, it was shown that the toolbox can learn automatically a goodrepresentation of the input from unlabeled data with better discriminationbetween different classes. Also on all datasets, the obtained classificationerrors are comparable to those of state of the art classifiers. In addition,the toolbox supports different sampling methods (e.g. Gibbs, CD, PCD and ournew FEPCD method), different sparsity methods (quadratic, rate distortion andour new normal method), different RBM types (generative and discriminative),using GPU, etc. The toolbox is a user-friendly open source software and isfreely available on the websitehttp://ceit.aut.ac.ir/~keyvanrad/DeeBNet%20Toolbox.html .
arxiv-7800-23 | Cortical Processing with Thermodynamic-RAM | http://arxiv.org/pdf/1408.3215v1.pdf | author:M. Alexander Nugent, Timothy W. Molter category:cs.NE cs.ET published:2014-08-14 summary:AHaH computing forms a theoretical framework from which abiologically-inspired type of computing architecture can be built where, unlikevon Neumann systems, memory and processor are physically combined. In thispaper we report on an incremental step beyond the theoretical framework of AHaHcomputing toward the development of a memristor-based physical neuralprocessing unit (NPU), which we call Thermodynamic-RAM (kT-RAM). While thepower consumption and speed dominance of such an NPU over von Neumannarchitectures for machine learning applications is well appreciated,Thermodynamic-RAM offers several advantages over other hardware approaches toadaptation and learning. Benefits include general-purpose use, a simple yetflexible instruction set and easy integration into existing digital platforms.We present a high level design of kT-RAM and a formal definition of itsinstruction set. We report the completion of a kT-RAM emulator and thesuccessful port of all previous machine learning benchmark applicationsincluding unsupervised clustering, supervised and unsupervised classification,complex signal prediction, unsupervised robotic actuation and combinatorialoptimization. Lastly, we extend a previous MNIST hand written digits benchmarkapplication, to show that an extra step of reading the synaptic states of AHaHnodes during the train phase (healing) alone results in plasticity thatimproves the classifier's performance, bumping our best F1 score up to 99.5%.
arxiv-7800-24 | On Data Preconditioning for Regularized Loss Minimization | http://arxiv.org/pdf/1408.3115v4.pdf | author:Tianbao Yang, Rong Jin, Shenghuo Zhu, Qihang Lin category:cs.NA cs.LG stat.ML published:2014-08-13 summary:In this work, we study data preconditioning, a well-known and long-existingtechnique, for boosting the convergence of first-order methods for regularizedloss minimization. It is well understood that the condition number of theproblem, i.e., the ratio of the Lipschitz constant to the strong convexitymodulus, has a harsh effect on the convergence of the first-order optimizationmethods. Therefore, minimizing a small regularized loss for achieving goodgeneralization performance, yielding an ill conditioned problem, becomes thebottleneck for big data problems. We provide a theory on data preconditioningfor regularized loss minimization. In particular, our analysis exhibits anappropriate data preconditioner and characterizes the conditions on the lossfunction and on the data under which data preconditioning can reduce thecondition number and therefore boost the convergence for minimizing theregularized loss. To make the data preconditioning practically useful, weendeavor to employ and analyze a random sampling approach to efficientlycompute the preconditioned data. The preliminary experiments validate ourtheory.
arxiv-7800-25 | Detection is the central problem in real-word spelling correction | http://arxiv.org/pdf/1408.3153v2.pdf | author:L. Amber Wilcox-O'Hearn category:cs.CL published:2014-08-13 summary:Real-word spelling correction differs from non-word spelling correction inits aims and its challenges. Here we show that the central problem in real-wordspelling correction is detection. Methods from non-word spelling correction,which focus instead on selection among candidate corrections, do not addressdetection adequately, because detection is either assumed in advance or heavilyconstrained. As we demonstrate in this paper, merely discriminating between theintended word and a random close variation of it within the context of asentence is a task that can be performed with high accuracy usingstraightforward models. Trigram models are sufficient in almost all cases. Thedifficulty comes when every word in the sentence is a potential error, with alarge set of possible candidate corrections. Despite their strengths, trigrammodels cannot reliably find true errors without introducing many more, at leastnot when used in the obvious sequential way without added structure. Thedetection task exposes weakness not visible in the selection task.
arxiv-7800-26 | Gradient Distribution Priors for Biomedical Image Processing | http://arxiv.org/pdf/1408.3300v2.pdf | author:Yuanhao Gong, Ivo F. Sbalzarini category:cs.CV published:2014-08-13 summary:Ill-posed inverse problems are commonplace in biomedical image processing.Their solution typically requires imposing prior knowledge about the latentground truth. While this regularizes the problem to an extent where it can besolved, it also biases the result toward the expected. With inappropriatepriors harming more than they use, it remains unclear what prior to use for agiven practical problem. Priors are hence mostly chosen in an {\em ad hoc} orempirical fashion. We argue here that the gradient distribution ofnatural-scene images may provide a versatile and well-founded prior forbiomedical images. We provide motivation for this choice from different pointsof view, and we fully validate the resulting prior for use on biomedical imagesby showing its stability and correlation with image quality. We then provide aset of simple parametric models for the resulting prior, leading tostraightforward (quasi-)convex optimization problems for which we provideefficient solver algorithms. We illustrate the use of the present models andsolvers in a variety of common image-processing tasks, including contrastenhancement, noise level estimation, denoising, blind deconvolution,zooming/up-sampling, and dehazing. In all cases we show that the present methodleads to results that are comparable to or better than the state of the art;always using the same, simple prior. We conclude by discussing the limitationsand possible interpretations of the prior.
arxiv-7800-27 | Implicit stochastic gradient descent | http://arxiv.org/pdf/1408.2923v5.pdf | author:Panos Toulis, Edoardo M. Airoldi category:stat.ME stat.CO stat.ML published:2014-08-13 summary:Stochastic optimization procedures, such as stochastic gradient descent, havegained popularity for parameter estimation from large data sets. However,standard stochastic optimization procedures cannot effectively combinenumerical stability with statistical and computational efficiency. Here, weintroduce an implicit stochastic gradient descent procedure, the iterates ofwhich are implicitly defined. Intuitively, implicit iterates shrink thestandard iterates. The amount of shrinkage depends on the observed Fisherinformation matrix, which does not need to be explicitly computed in practice,thus increasing stability without increasing the computational burden. Whencombined with averaging, the proposed procedure achieves statistical efficiencyas well. We derive non-asymptotic bounds and characterize the asymptoticdistribution of implicit procedures. Our analysis also reveals the asymptoticvariance of a number of existing procedures. We demonstrate implicit stochasticgradient descent by further developing theory for generalized linear models,Cox proportional hazards, and M-estimation problems, and by carrying outextensive experiments. Our results suggest that the implicit stochasticgradient descent procedure is poised to become the workhorse of estimation withlarge data sets.
arxiv-7800-28 | A Classifier-free Ensemble Selection Method based on Data Diversity in Random Subspaces | http://arxiv.org/pdf/1408.2889v1.pdf | author:Albert H. R. Ko, Robert Sabourin, Alceu S. Britto Jr, Luiz E. S. Oliveira category:cs.LG cs.NE I.5.2; I.5.3 published:2014-08-13 summary:The Ensemble of Classifiers (EoC) has been shown to be effective in improvingthe performance of single classifiers by combining their outputs, and one ofthe most important properties involved in the selection of the best EoC from apool of classifiers is considered to be classifier diversity. In general,classifier diversity does not occur randomly, but is generated systematicallyby various ensemble creation methods. By using diverse data subsets to trainclassifiers, these methods can create diverse classifiers for the EoC. In thiswork, we propose a scheme to measure data diversity directly from randomsubspaces, and explore the possibility of using it to select the best datasubsets for the construction of the EoC. Our scheme is the first ensembleselection method to be presented in the literature based on the concept of datadiversity. Its main advantage over the traditional framework (ensemble creationthen selection) is that it obviates the need for classifier training prior toensemble selection. A single Genetic Algorithm (GA) and a Multi-ObjectiveGenetic Algorithm (MOGA) were evaluated to search for the best solutions forthe classifier-free ensemble selection. In both cases, objective functionsbased on different clustering diversity measures were implemented and tested.All the results obtained with the proposed classifier-free ensemble selectionmethod were compared with the traditional classifier-based ensemble selectionusing Mean Classifier Error (ME) and Majority Voting Error (MVE). Theapplicability of the method is tested on UCI machine learning problems and NISTSD19 handwritten numerals.
arxiv-7800-29 | Robust OS-ELM with a novel selective ensemble based on particle swarm optimization | http://arxiv.org/pdf/1408.2890v1.pdf | author:Yang Liu, Bo He, Diya Dong, Yue Shen, Tianhong Yan, Rui Nian, Amaury Lendase category:cs.LG published:2014-08-13 summary:In this paper, a robust online sequential extreme learning machine (ROS-ELM)is proposed. It is based on the original OS-ELM with an adaptive selectiveensemble framework. Two novel insights are proposed in this paper. First, anovel selective ensemble algorithm referred to as particle swarm optimizationselective ensemble (PSOSEN) is proposed. Noting that PSOSEN is a generalselective ensemble method which is applicable to any learning algorithms,including batch learning and online learning. Second, an adaptive selectiveensemble framework for online learning is designed to balance the robustnessand complexity of the algorithm. Experiments for both regression andclassification problems with UCI data sets are carried out. Comparisons betweenOS-ELM, simple ensemble OS-ELM (EOS-ELM) and the proposed ROS-ELM empiricallyshow that ROS-ELM significantly improves the robustness and stability.
arxiv-7800-30 | Linear Contour Learning: A Method for Supervised Dimension Reduction | http://arxiv.org/pdf/1408.3359v1.pdf | author:Bing Li, Hongyuan Zha, Francesca Chiaromonte category:cs.LG published:2014-08-13 summary:We propose a novel approach to sufficient dimension reduction in regression,based on estimating contour directions of negligible variation for the responsesurface. These directions span the orthogonal complement of the minimal spacerelevant for the regression, and can be extracted according to a measure of thevariation in the response, leading to General Contour Regression(GCR). Incomparison to exiisting sufficient dimension reduction techniques, thissontour-based mothology guarantees exhaustive estimation of the central spaceunder ellipticity of the predictoor distribution and very mild additionalassumptions, while maintaining vn-consisytency and somputational ease.Moreover, it proves to be robust to departures from ellipticity. We alsoestablish some useful population properties for GCR. Simulations to compareperformance with that of standard techniques such as ordinary least squares,sliced inverse regression, principal hessian directions, and sliced averagevariance estimation confirm the advntages anticipated by theoretical analyses.We also demonstrate the use of contour-based methods on a data set concerninggrades of students from Massachusetts colleges.
arxiv-7800-31 | Convergence rate of Bayesian tensor estimator: Optimal rate without restricted strong convexity | http://arxiv.org/pdf/1408.3092v1.pdf | author:Taiji Suzuki category:stat.ML cs.LG published:2014-08-13 summary:In this paper, we investigate the statistical convergence rate of a Bayesianlow-rank tensor estimator. Our problem setting is the regression problem wherea tensor structure underlying the data is estimated. This problem settingoccurs in many practical applications, such as collaborative filtering,multi-task learning, and spatio-temporal data analysis. The convergence rate isanalyzed in terms of both in-sample and out-of-sample predictive accuracies. Itis shown that a near optimal rate is achieved without any strong convexity ofthe observation. Moreover, we show that the method has adaptivity to theunknown rank of the true tensor, that is, the near optimal rate depending onthe true rank is achieved even if it is not known a priori.
arxiv-7800-32 | Fastfood: Approximate Kernel Expansions in Loglinear Time | http://arxiv.org/pdf/1408.3060v1.pdf | author:Quoc Viet Le, Tamas Sarlos, Alexander Johannes Smola category:cs.LG stat.ML published:2014-08-13 summary:Despite their successes, what makes kernel methods difficult to use in manylarge scale problems is the fact that storing and computing the decisionfunction is typically expensive, especially at prediction time. In this paper,we overcome this difficulty by proposing Fastfood, an approximation thataccelerates such computation significantly. Key to Fastfood is the observationthat Hadamard matrices, when combined with diagonal Gaussian matrices, exhibitproperties similar to dense Gaussian random matrices. Yet unlike the latter,Hadamard and diagonal matrices are inexpensive to multiply and store. These twomatrices can be used in lieu of Gaussian matrices in Random Kitchen Sinksproposed by Rahimi and Recht (2009) and thereby speeding up the computation fora large range of kernel functions. Specifically, Fastfood requires O(n log d)time and O(n) storage to compute n non-linear basis functions in d dimensions,a significant improvement from O(nd) computation and storage, withoutsacrificing accuracy. Our method applies to any translation invariant and any dot-product kernel,such as the popular RBF kernels and polynomial kernels. We prove that theapproximation is unbiased and has low variance. Experiments show that weachieve similar accuracy to full kernel expansions and Random Kitchen Sinkswhile being 100x faster and using 1000x less memory. These improvements,especially in terms of memory usage, make kernel methods more practical forapplications that have large training sets and/or require real-time prediction.
arxiv-7800-33 | An Improved Approach for Contrast Enhancement of Spinal Cord Images based on Multiscale Retinex Algorithm | http://arxiv.org/pdf/1408.2997v1.pdf | author:Sreenivasa Setty, N. K Srinath, M. C Hanumantharaju category:cs.CV published:2014-08-13 summary:This paper presents a new approach for contrast enhancement of spinal cordmedical images based on multirate scheme incorporated into multiscale retinexalgorithm. The proposed work here uses HSV color space, since HSV color spaceseparates color details from intensity. The enhancement of medical image isachieved by down sampling the original image into five versions, namely, tiny,small, medium, fine, and normal scale. This is due to the fact that the eachversions of the image when independently enhanced and reconstructed results inenormous improvement in the visual quality. Further, the contrast stretchingand MultiScale Retinex (MSR) techniques are exploited in order to enhance eachof the scaled version of the image. Finally, the enhanced image is obtained bycombining each of these scales in an efficient way to obtain the compositeenhanced image. The efficiency of the proposed algorithm is validated by usinga wavelet energy metric in the wavelet domain. Reconstructed image usingproposed method highlights the details (edges and tissues), reduces image noise(Gaussian and Speckle) and improves the overall contrast. The proposedalgorithm also enhances sharp edges of the tissue surrounding the spinal cordregions which is useful for diagnosis of spinal cord lesions. Elaboratedexperiments are conducted on several medical images and results presented showthat the enhanced medical pictures are of good quality and is found to bebetter compared with other researcher methods.
arxiv-7800-34 | Hashing for Similarity Search: A Survey | http://arxiv.org/pdf/1408.2927v1.pdf | author:Jingdong Wang, Heng Tao Shen, Jingkuan Song, Jianqiu Ji category:cs.DS cs.CV cs.DB published:2014-08-13 summary:Similarity search (nearest neighbor search) is a problem of pursuing the dataitems whose distances to a query item are the smallest from a large database.Various methods have been developed to address this problem, and recently a lotof efforts have been devoted to approximate search. In this paper, we present asurvey on one of the main solutions, hashing, which has been widely studiedsince the pioneering work locality sensitive hashing. We divide the hashingalgorithms two main categories: locality sensitive hashing, which designs hashfunctions without exploring the data distribution and learning to hash, whichlearns hash functions according the data distribution, and review them fromvarious aspects, including hash function design and distance measure and searchscheme in the hash coding space.
arxiv-7800-35 | Learning Multi-Scale Representations for Material Classification | http://arxiv.org/pdf/1408.2938v1.pdf | author:Wenbin Li, Mario Fritz category:cs.CV cs.LG cs.NE published:2014-08-13 summary:The recent progress in sparse coding and deep learning has made unsupervisedfeature learning methods a strong competitor to hand-crafted descriptors. Incomputer vision, success stories of learned features have been predominantlyreported for object recognition tasks. In this paper, we investigate if and howfeature learning can be used for material recognition. We propose twostrategies to incorporate scale information into the learning procedureresulting in a novel multi-scale coding procedure. Our results show that ourlearned features for material recognition outperform hand-crafted descriptorson the FMD and the KTH-TIPS2 material classification benchmarks.
arxiv-7800-36 | Co-Localization of Audio Sources in Images Using Binaural Features and Locally-Linear Regression | http://arxiv.org/pdf/1408.2700v4.pdf | author:Antoine Deleforge, Radu Horaud, Yoav Schechner, Laurent Girin category:cs.SD cs.MM stat.AP stat.ML published:2014-08-12 summary:This paper addresses the problem of localizing audio sources using binauralmeasurements. We propose a supervised formulation that simultaneously localizesmultiple sources at different locations. The approach is intrinsicallyefficient because, contrary to prior work, it relies neither on sourceseparation, nor on monaural segregation. The method starts with a trainingstage that establishes a locally-linear Gaussian regression model between thedirectional coordinates of all the sources and the auditory features extractedfrom binaural measurements. While fixed-length wide-spectrum sounds (whitenoise) are used for training to reliably estimate the model parameters, we showthat the testing (localization) can be extended to variable-lengthsparse-spectrum sounds (such as speech), thus enabling a wide range ofrealistic applications. Indeed, we demonstrate that the method can be used foraudio-visual fusion, namely to map speech signals onto images and hence tospatially align the audio and visual modalities, thus enabling to discriminatebetween speaking and non-speaking faces. We release a novel corpus of real-roomrecordings that allow quantitative evaluation of the co-localization method inthe presence of one or two sound sources. Experiments demonstrate increasedaccuracy and speed relative to several state-of-the-art methods.
arxiv-7800-37 | Learning From Non-iid Data: Fast Rates for the One-vs-All Multiclass Plug-in Classifiers | http://arxiv.org/pdf/1408.2714v2.pdf | author:Vu Dinh, Lam Si Tung Ho, Nguyen Viet Cuong, Duy Nguyen, Binh T. Nguyen category:stat.ML published:2014-08-12 summary:We prove new fast learning rates for the one-vs-all multiclass plug-inclassifiers trained either from exponentially strongly mixing data or from datagenerated by a converging drifting distribution. These are two typicalscenarios where training data are not iid. The learning rates are obtainedunder a multiclass version of Tsybakov's margin assumption, a type of low-noiseassumption, and do not depend on the number of classes. Our results are generaland include a previous result for binary-class plug-in classifiers with iiddata as a special case. In contrast to previous works for least squares SVMsunder the binary-class setting, our results retain the optimal learning rate inthe iid case.
arxiv-7800-38 | First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs | http://arxiv.org/pdf/1408.2873v2.pdf | author:Awni Y. Hannun, Andrew L. Maas, Daniel Jurafsky, Andrew Y. Ng category:cs.CL cs.LG cs.NE published:2014-08-12 summary:We present a method to perform first-pass large vocabulary continuous speechrecognition using only a neural network and language model. Deep neural networkacoustic models are now commonplace in HMM-based speech recognition systems,but building such systems is a complex, domain-specific task. Recent workdemonstrated the feasibility of discarding the HMM sequence modeling frameworkby directly predicting transcript text from audio. This paper extends thisapproach in two ways. First, we demonstrate that a straightforward recurrentneural network architecture can achieve a high level of accuracy. Second, wepropose and evaluate a modified prefix-search decoding algorithm. This approachto decoding enables first-pass speech recognition with a language model,completely unaided by the cumbersome infrastructure of HMM-based systems.Experiments on the Wall Street Journal corpus demonstrate fairly competitiveword error rates, and the importance of bi-directional network recurrence.
arxiv-7800-39 | Convex Calibration Dimension for Multiclass Loss Matrices | http://arxiv.org/pdf/1408.2764v2.pdf | author:Harish G. Ramaswamy, Shivani Agarwal category:cs.LG stat.ML published:2014-08-12 summary:We study consistency properties of surrogate loss functions for generalmulticlass learning problems, defined by a general multiclass loss matrix. Weextend the notion of classification calibration, which has been studied forbinary and multiclass 0-1 classification problems (and for certain otherspecific learning problems), to the general multiclass setting, and derivenecessary and sufficient conditions for a surrogate loss to be calibrated withrespect to a loss matrix in this setting. We then introduce the notion ofconvex calibration dimension of a multiclass loss matrix, which measures thesmallest `size' of a prediction space in which it is possible to design aconvex surrogate that is calibrated with respect to the loss matrix. We deriveboth upper and lower bounds on this quantity, and use these results to analyzevarious loss matrices. In particular, we apply our framework to study varioussubset ranking losses, and use the convex calibration dimension as a tool toshow both the existence and non-existence of various types of convex calibratedsurrogates for these losses. Our results strengthen recent results of Duchi etal. (2010) and Calauzenes et al. (2012) on the non-existence of certain typesof convex calibrated surrogates in subset ranking. We anticipate the convexcalibration dimension may prove to be a useful tool in the study and design ofsurrogate losses for general multiclass learning problems.
arxiv-7800-40 | Multidimensional Digital Filters for Point-Target Detection in Cluttered Infrared Scenes | http://arxiv.org/pdf/1408.2590v3.pdf | author:Hugh L. Kennedy category:cs.CV published:2014-08-12 summary:A 3-D spatiotemporal prediction-error filter (PEF), is used to enhanceforeground/background contrast in (real and simulated) sensor image sequences.Relative velocity is utilized to extract point-targets that would otherwise beindistinguishable on spatial frequency alone. An optical-flow field isgenerated using local estimates of the 3-D autocorrelation function via theapplication of the fast Fourier transform (FFT) and inverse FFT. Velocityestimates are then used to tune in a background-whitening PEF that is matchedto the motion and texture of the local background. Finite-impulse-response(FIR) filters are designed and implemented in the frequency domain. Ananalytical expression for the frequency response of velocity-tuned FIR filters,of odd or even dimension, with an arbitrary delay in each dimension, isderived.
arxiv-7800-41 | Block stochastic gradient iteration for convex and nonconvex optimization | http://arxiv.org/pdf/1408.2597v3.pdf | author:Yangyang Xu, Wotao Yin category:math.OC cs.LG cs.NA math.NA stat.ML 90C06 published:2014-08-12 summary:The stochastic gradient (SG) method can minimize an objective functioncomposed of a large number of differentiable functions, or solve a stochasticoptimization problem, to a moderate accuracy. The block coordinatedescent/update (BCD) method, on the other hand, handles problems with multipleblocks of variables by updating them one at a time; when the blocks ofvariables are easier to update individually than together, BCD has a lowerper-iteration cost. This paper introduces a method that combines the featuresof SG and BCD for problems with many components in the objective and withmultiple (blocks of) variables. Specifically, a block stochastic gradient (BSG) method is proposed forsolving both convex and nonconvex programs. At each iteration, BSG approximatesthe gradient of the differentiable part of the objective by randomly sampling asmall set of data or sampling a few functions from the sum term in theobjective, and then, using those samples, it updates all the blocks ofvariables in either a deterministic or a randomly shuffled order. Itsconvergence for both convex and nonconvex cases are established in differentsenses. In the convex case, the proposed method has the same order ofconvergence rate as the SG method. In the nonconvex case, its convergence isestablished in terms of the expected violation of a first-order optimalitycondition. The proposed method was numerically tested on problems includingstochastic least squares and logistic regression, which are convex, as well aslow-rank tensor recovery and bilinear logistic regression, which are nonconvex.
arxiv-7800-42 | Internal and external dynamics in language: Evidence from verb regularity in a historical corpus of English | http://arxiv.org/pdf/1408.2699v1.pdf | author:Christine F. Cuskley, Martina Pugliese, Claudio Castellano, Francesca Colaiori, Vittorio Loreto, Francesca Tria category:physics.soc-ph cs.CL published:2014-08-12 summary:Human languages are rule governed, but almost invariably these rules haveexceptions in the form of irregularities. Since rules in language are efficientand productive, the persistence of irregularity is an anomaly. How doesirregularity linger in the face of internal (endogenous) and external(exogenous) pressures to conform to a rule? Here we address this problem bytaking a detailed look at simple past tense verbs in the Corpus of HistoricalAmerican English. The data show that the language is open, with many new verbsentering. At the same time, existing verbs might tend to regularize orirregularize as a consequence of internal dynamics, but overall, the amount ofirregularity sustained by the language stays roughly constant over time.Despite continuous vocabulary growth, and presumably, an attendant increase inexpressive power, there is no corresponding growth in irregularity. We analyzethe set of irregulars, showing they may adhere to a set of minority rules,allowing for increased stability of irregularity over time. These findingscontribute to the debate on how language systems become rule governed, and howand why they sustain exceptions to rules, providing insight into the interplaybetween the emergence and maintenance of rules and exceptions in language.
arxiv-7800-43 | Spectral Unmixing of Hyperspectral Imagery using Multilayer NMF | http://arxiv.org/pdf/1408.2810v1.pdf | author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV published:2014-08-12 summary:Hyperspectral images contain mixed pixels due to low spatial resolution ofhyperspectral sensors. Spectral unmixing problem refers to decomposing mixedpixels into a set of endmembers and abundance fractions. Due to nonnegativityconstraint on abundance fractions, nonnegative matrix factorization (NMF)methods have been widely used for solving spectral unmixing problem. In thisletter we proposed using multilayer NMF (MLNMF) for the purpose ofhyperspectral unmixing. In this approach, spectral signature matrix can bemodeled as a product of sparse matrices. In fact MLNMF decomposes theobservation matrix iteratively in a number of layers. In each layer, we appliedsparseness constraint on spectral signature matrix as well as on abundancefractions matrix. In this way signatures matrix can be sparsely decomposeddespite the fact that it is not generally a sparse matrix. The proposedalgorithm is applied on synthetic and real datasets. Synthetic data isgenerated based on endmembers from USGS spectral library. AVIRIS Cupritedataset has been used as a real dataset for evaluation of proposed method.Results of experiments are quantified based on SAD and AAD measures. Results incomparison with previously proposed methods show that the multilayer approachcan unmix data more effectively.
arxiv-7800-44 | Cluster based RBF Kernel for Support Vector Machines | http://arxiv.org/pdf/1408.2869v1.pdf | author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG stat.ML published:2014-08-12 summary:In the classical Gaussian SVM classification we use the feature spaceprojection transforming points to normal distributions with fixed covariancematrices (identity in the standard RBF and the covariance of the whole datasetin Mahalanobis RBF). In this paper we add additional information to GaussianSVM by considering local geometry-dependent feature space projection. Weemphasize that our approach is in fact an algorithm for a construction of thenew Gaussian-type kernel. We show that better (compared to standard RBF and Mahalanobis RBF)classification results are obtained in the simple case when the space ispreliminary divided by k-means into two sets and points are represented asnormal distributions with a covariances calculated according to the datasetpartitioning. We call the constructed method C$_k$RBF, where $k$ stands for the amount ofclusters used in k-means. We show empirically on nine datasets from UCIrepository that C$_2$RBF increases the stability of the grid search (measuredas the probability of finding good parameters).
arxiv-7800-45 | Learning a hyperplane classifier by minimizing an exact bound on the VC dimension | http://arxiv.org/pdf/1408.2803v2.pdf | author:Jayadeva category:cs.LG I.5.1; I.5.2 published:2014-08-12 summary:The VC dimension measures the capacity of a learning machine, and a low VCdimension leads to good generalization. While SVMs produce state-of-the-artlearning performance, it is well known that the VC dimension of a SVM can beunbounded; despite good results in practice, there is no guarantee of goodgeneralization. In this paper, we show how to learn a hyperplane classifier byminimizing an exact, or \boldmath{$\Theta$} bound on its VC dimension. Theproposed approach, termed as the Minimal Complexity Machine (MCM), involvessolving a simple linear programming problem. Experimental results show, that ona number of benchmark datasets, the proposed approach learns classifiers witherror rates much less than conventional SVMs, while often using fewer supportvectors. On many benchmark datasets, the number of support vectors is less thanone-tenth the number used by SVMs, indicating that the MCM does indeed learnsimpler representations.
arxiv-7800-46 | Optimizing Component Combination in a Multi-Indexing Paragraph Retrieval System | http://arxiv.org/pdf/1408.2430v1.pdf | author:Boris Iolis, Gianluca Bontempi category:cs.IR cs.CL 68T50 published:2014-08-11 summary:We demonstrate a method to optimize the combination of distinct components ina paragraph retrieval system. Our system makes use of several indices, querygenerators and filters, each of them potentially contributing to the quality ofthe returned list of results. The components are combined with a weighed sum,and we optimize the weights using a heuristic optimization algorithm. Thisallows us to maximize the quality of our results, but also to determine whichcomponents are most valuable in our system. We evaluate our approach on theparagraph selection task of a Question Answering dataset.
arxiv-7800-47 | Video Face Editing Using Temporal-Spatial-Smooth Warping | http://arxiv.org/pdf/1408.2380v1.pdf | author:Xiaoyan Li, Dacheng Tao category:cs.CV cs.AI cs.MM published:2014-08-11 summary:Editing faces in videos is a popular yet challenging aspect of computervision and graphics, which encompasses several applications including facialattractiveness enhancement, makeup transfer, face replacement, and expressionmanipulation. Simply applying image-based warping algorithms to video-basedface editing produces temporal incoherence in the synthesized videos because itis impossible to consistently localize facial features in two framesrepresenting two different faces in two different videos (or even twoconsecutive frames representing the same face in one video). Therefore, highperformance face editing usually requires significant manual manipulation. Inthis paper we propose a novel temporal-spatial-smooth warping (TSSW) algorithmto effectively exploit the temporal information in two consecutive frames, aswell as the spatial smoothness within each frame. TSSW precisely estimates twocontrol lattices in the horizontal and vertical directions respectively fromthe corresponding control lattices in the previous frame, by minimizing a novelenergy function that unifies a data-driven term, a smoothness term, and featurepoint constraints. Corresponding warping surfaces then precisely map sourceframes to the target frames. Experimental testing on facial attractivenessenhancement, makeup transfer, face replacement, and expression manipulationdemonstrates that the proposed approaches can effectively preserve spatialsmoothness and temporal coherence in editing facial geometry, skin detail,identity, and expression, which outperform the existing face editing methods.In particular, TSSW is robust to subtly inaccurate localization of featurepoints and is a vast improvement over image-based warping methods.
arxiv-7800-48 | On the Complexity of Bandit Linear Optimization | http://arxiv.org/pdf/1408.2368v1.pdf | author:Ohad Shamir category:cs.LG published:2014-08-11 summary:We study the attainable regret for online linear optimization problems withbandit feedback, where unlike the full-information setting, the player can onlyobserve its own loss rather than the full loss vector. We show that the priceof bandit information in this setting can be as large as $d$, disproving thewell-known conjecture that the regret for bandit linear optimization is at most$\sqrt{d}$ times the full-information regret. Surprisingly, this is shown using"trivial" modifications of standard domains, which have no effect in thefull-information setting. This and other results we present highlight someinteresting differences between full-information and bandit learning, whichwere not considered in previous literature.
arxiv-7800-49 | Physical Computing With No Clock to Implement the Gaussian Pyramid of SIFT Algorithm | http://arxiv.org/pdf/1408.2289v1.pdf | author:Yi Li, Qi Wei, Fei Qiao, Huazhong Yang category:cs.CV published:2014-08-11 summary:Physical computing is a technology utilizing the nature of electronic devicesand circuit topology to cope with computing tasks. In this paper, we propose anactive circuit network to implement multi-scale Gaussian filter, which is alsocalled Gaussian Pyramid in image preprocessing. Various kinds of methods havebeen tried to accelerate the key stage in image feature extracting algorithmthese years. Compared with existing technologies, GPU parallel computing andFPGA accelerating technology, physical computing has great advantage onprocessing speed as well as power consumption. We have verified that processingtime to implement the Gaussian pyramid of the SIFT algorithm stands onnanosecond level through the physical computing technology, while otherexisting methods all need at least hundreds of millisecond. With an estimate onthe stray capacitance of the circuit, the power consumption is around 670pJ tofilter a 256x256 image. To the best of our knowledge, this is the most fastprocessing technology to accelerate the SIFT algorithm, and it is also a ratherenergy-efficient method, thanks to the proposed physical computing technology.
arxiv-7800-50 | Genetic Programming for Smart Phone Personalisation | http://arxiv.org/pdf/1408.2288v1.pdf | author:Philip Valencia, Aiden Haak, Alban Cotillon, Raja Jurdak category:cs.NE cs.CY published:2014-08-11 summary:Personalisation in smart phones requires adaptability to dynamic contextbased on user mobility, application usage and sensor inputs. Currentpersonalisation approaches, which rely on static logic that is developed apriori, do not provide sufficient adaptability to dynamic and unexpectedcontext. This paper proposes genetic programming (GP), which can evolve programlogic in realtime, as an online learning method to deal with the highly dynamiccontext in smart phone personalisation. We introduce the concept ofcollaborative smart phone personalisation through the GP Island Model, in orderto exploit shared context among co-located phone users and reduce convergencetime. We implement these concepts on real smartphones to demonstrate thecapability of personalisation through GP and to explore the benefits of theIsland Model. Our empirical evaluations on two example applications confirmthat the Island Model can reduce convergence time by up to two-thirds overstandalone GP personalisation.
arxiv-7800-51 | Hierarchical Saliency Detection on Extended CSSD | http://arxiv.org/pdf/1408.5418v2.pdf | author:Jianping Shi, Qiong Yan, Li Xu, Jiaya Jia category:cs.CV published:2014-08-11 summary:Complex structures commonly exist in natural images. When an image containssmall-scale high-contrast patterns either in the background or foreground,saliency detection could be adversely affected, resulting erroneous andnon-uniform saliency assignment. The issue forms a fundamental challenge forprior methods. We tackle it from a scale point of view and propose amulti-layer approach to analyze saliency cues. Different from varying patchsizes or downsizing images, we measure region-based scales. The final saliencyvalues are inferred optimally combining all the saliency cues in differentscales using hierarchical inference. Through our inference model, single-scaleinformation is selected to obtain a saliency map. Our method improves detectionquality on many images that cannot be handled well traditionally. We alsoconstruct an extended Complex Scene Saliency Dataset (ECSSD) to include complexbut general natural images.
arxiv-7800-52 | Learning to see like children: proof of concept | http://arxiv.org/pdf/1408.2478v1.pdf | author:Marco Gori, Marco Lippi, Marco Maggini, Stefano Melacci category:cs.CV published:2014-08-11 summary:In the last few years we have seen a growing interest in machine learningapproaches to computer vision and, especially, to semantic labeling. Nowadaysstate of the art systems use deep learning on millions of labeled images withvery successful results on benchmarks, though it is unlikely to expect similarresults in unrestricted visual environments. Most learning schemes essentiallyignore the inherent sequential structure of videos: this might be a criticalissue, since any visual recognition process is remarkably more complex whenshuffling video frames. Based on this remark, we propose a re-foundation of thecommunication protocol between visual agents and the environment, which isreferred to as learning to see like children. Like for human interaction,visual concepts are acquired by the agents solely by processing their ownvisual stream along with human supervisions on selected pixels. We give a proofof concept that remarkable semantic labeling can emerge within this protocol byusing only a few supervised examples. This is made possible by exploiting aconstraint of motion coherent labeling that virtually offers tons ofsupervisions. Additional visual constraints, including those associated withobject supervisions, are used within the context of learning from constraints.The framework is extended in the direction of lifelong learning, so as ourvisual agents live in their own visual environment without distinguishinglearning and test set. Learning takes place in deep architectures under aprogressive developmental scheme. In order to evaluate our Developmental VisualAgents (DVAs), in addition to classic benchmarks, we open the doors of our lab,allowing people to evaluate DVAs by crowd-sourcing. Such assessment mechanismmight result in a paradigm shift in methodologies and algorithms for computervision, encouraging truly novel solutions within the proposed framework.
arxiv-7800-53 | Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction of Tumors | http://arxiv.org/pdf/1408.2552v1.pdf | author:Amit G. Deshwar, Shankar Vembu, Quaid Morris category:q-bio.PE cs.LG stat.ML published:2014-08-11 summary:Statistical machine learning methods, especially nonparametric Bayesianmethods, have become increasingly popular to infer clonal population structureof tumors. Here we describe the treeCRP, an extension of the Chinese restaurantprocess (CRP), a popular construction used in nonparametric mixture models, toinfer the phylogeny and genotype of major subclonal lineages represented in thepopulation of cancer cells. We also propose new split-merge updates tailored tothe subclonal reconstruction problem that improve the mixing time of Markovchains. In comparisons with the tree-structured stick breaking prior used inPhyloSub, we demonstrate superior mixing and running time using the treeCRPwith our new split-merge procedures. We also show that given the same number ofsamples, TSSB and treeCRP have similar ability to recover the subclonalstructure of a tumor.
arxiv-7800-54 | Compressed Sensing with Very Sparse Gaussian Random Projections | http://arxiv.org/pdf/1408.2504v1.pdf | author:Ping Li, Cun-Hui Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT published:2014-08-11 summary:We study the use of very sparse random projections for compressed sensing(sparse signal recovery) when the signal entries can be either positive ornegative. In our setting, the entries of a Gaussian design matrix are randomlysparsified so that only a very small fraction of the entries are nonzero. Ourproposed decoding algorithm is simple and efficient in that the major cost isone linear scan of the coordinates. We have developed two estimators: (i) the{\em tie estimator}, and (ii) the {\em absolute minimum estimator}. Using onlythe tie estimator, we are able to recover a $K$-sparse signal of length $N$using $1.551 eK \log K/\delta$ measurements (where $\delta\leq 0.05$ is theconfidence). Using only the absolute minimum estimator, we can detect thesupport of the signal using $eK\log N/\delta$ measurements. For a particularcoordinate, the absolute minimum estimator requires fewer measurements (i.e.,with a constant $e$ instead of $1.551e$). Thus, the two estimators can becombined to form an even more practical decoding framework. Prior studies have shown that existing one-scan (or roughly one-scan)recovery algorithms using sparse matrices would require substantially more(e.g., one order of magnitude) measurements than L1 decoding by linearprogramming, when the nonzero entries of signals can be either negative orpositive. In this paper, following a known experimental setup, we show that, atthe same number of measurements, the recovery accuracies of our proposed methodare (at least) similar to the standard L1 decoding.
arxiv-7800-55 | Bags of Affine Subspaces for Robust Object Tracking | http://arxiv.org/pdf/1408.2313v3.pdf | author:Sareh Shirazi, Conrad Sanderson, Chris McCool, Mehrtash T. Harandi category:cs.CV cs.MM cs.RO 14M15, 54B05 published:2014-08-11 summary:We propose an adaptive tracking algorithm where the object is modelled as acontinuously updated bag of affine subspaces, with each subspace constructedfrom the object's appearance over several consecutive frames. In contrast tolinear subspaces, affine subspaces explicitly model the origin of subspaces.Furthermore, instead of using a brittle point-to-subspace distance during thesearch for the object in a new frame, we propose to use a subspace-to-subspacedistance by representing candidate image areas also as affine subspaces.Distances between subspaces are then obtained by exploiting the non-Euclideangeometry of Grassmann manifolds. Experiments on challenging videos (containingobject occlusions, deformations, as well as variations in pose andillumination) indicate that the proposed method achieves higher trackingaccuracy than several recent discriminative trackers.
arxiv-7800-56 | Homotopy equivalence of finite digital images | http://arxiv.org/pdf/1408.2584v2.pdf | author:Jason Haarmann, Meg P. Murphy, Casey S. Peters, P. Christopher Staecker category:math.GN cs.CG cs.CV 55P10, 68R10 I.4.m published:2014-08-11 summary:For digital images, there is an established homotopy equivalence relationwhich parallels that of classical topology. Many classical homotopy equivalenceinvariants, such as the Euler characteristic and the homology groups, do notremain invariants in the digital setting. This paper develops a numericaldigital homotopy invariant and begins to catalog all possible connected digitalimages on a small number of points, up to homotopy equivalence.
arxiv-7800-57 | Optimum Statistical Estimation with Strategic Data Sources | http://arxiv.org/pdf/1408.2539v2.pdf | author:Yang Cai, Constantinos Daskalakis, Christos H. Papadimitriou category:stat.ML cs.GT cs.LG published:2014-08-11 summary:We propose an optimum mechanism for providing monetary incentives to the datasources of a statistical estimator such as linear regression, so that highquality data is provided at low cost, in the sense that the sum of payments andestimation error is minimized. The mechanism applies to a broad range ofestimators, including linear and polynomial regression, kernel regression, and,under some additional assumptions, ridge regression. It also generalizes toseveral objectives, including minimizing estimation error subject to budgetconstraints. Besides our concrete results for regression problems, wecontribute a mechanism design framework through which to design and analyzestatistical estimators whose examples are supplied by workers with cost forlabeling said examples.
arxiv-7800-58 | Gap-weighted subsequences for automatic cognate identification and phylogenetic inference | http://arxiv.org/pdf/1408.2359v2.pdf | author:Taraka Rama category:cs.CL published:2014-08-11 summary:In this paper, we describe the problem of cognate identification and itsrelation to phylogenetic inference. We introduce subsequence based features fordiscriminating cognates from non-cognates. We show that subsequence basedfeatures perform better than the state-of-the-art string similarity measuresfor the purpose of cognate identification. We use the cognate judgments for thepurpose of phylogenetic inference and observe that these classifiers infer atree which is close to the gold standard tree. The contribution of this paperis the use of subsequence features for cognate identification and to employ thecognate judgments for phylogenetic inference.
arxiv-7800-59 | On the Consistency of Ordinal Regression Methods | http://arxiv.org/pdf/1408.2327v7.pdf | author:Fabian Pedregosa, Francis Bach, Alexandre Gramfort category:cs.LG published:2014-08-11 summary:Many of the ordinal regression models that have been proposed in theliterature can be seen as methods that minimize a convex surrogate of thezero-one, absolute, or squared loss functions. A key property that allows tostudy the statistical implications of such approximations is that of Fisherconsistency. In this paper we will characterize the Fisher consistency of arich family of surrogate loss functions used in the context of ordinalregression, including support vector ordinal regression, ORBoosting and leastabsolute deviation. We will see that, for a family of surrogate loss functionsthat subsumes support vector ordinal regression and ORBoosting, consistency canbe fully characterized by the derivative of a real-valued function at zero, ashappens for convex margin-based surrogates in binary classification. We alsoderive excess risk bounds for a surrogate of the absolute error that generalizeexisting risk bounds for binary classification. Finally, our analysis suggestsa novel surrogate of the squared error loss. To prove the empirical performanceof such surrogate, we benchmarked it in terms of cross-validation error on 9different datasets, where it outperforms competing approaches on 7 out of 9datasets.
arxiv-7800-60 | R-UCB: a Contextual Bandit Algorithm for Risk-Aware Recommender Systems | http://arxiv.org/pdf/1408.2195v1.pdf | author:Djallel Bouneffouf category:cs.IR cs.LG I.2 published:2014-08-10 summary:Mobile Context-Aware Recommender Systems can be naturally modelled as anexploration/exploitation trade-off (exr/exp) problem, where the system has tochoose between maximizing its expected rewards dealing with its currentknowledge (exploitation) and learning more about the unknown user's preferencesto improve its knowledge (exploration). This problem has been addressed by thereinforcement learning community but they do not consider the risk level of thecurrent user's situation, where it may be dangerous to recommend items the usermay not desire in her current situation if the risk level is high. We introducein this paper an algorithm named R-UCB that considers the risk level of theuser's situation to adaptively balance between exr and exp. The detailedanalysis of the experimental results reveals several important discoveries inthe exr/exp behaviour.
arxiv-7800-61 | Exponentiated Gradient Exploration for Active Learning | http://arxiv.org/pdf/1408.2196v1.pdf | author:Djallel Bouneffouf category:cs.LG cs.AI I.2 published:2014-08-10 summary:Active learning strategies respond to the costly labelling task in asupervised classification by selecting the most useful unlabelled examples intraining a predictive model. Many conventional active learning algorithms focuson refining the decision boundary, rather than exploring new regions that canbe more informative. In this setting, we propose a sequential algorithm namedEG-Active that can improve any Active learning algorithm by an optimal randomexploration. Experimental results show a statistically significant andappreciable improvement in the performance of our new approach over theexisting active feedback methods.
arxiv-7800-62 | Quantum Annealing for Clustering | http://arxiv.org/pdf/1408.2035v1.pdf | author:Kenichi Kurihara, Shu Tanaka, Seiji Miyashita category:cs.AI cs.LG published:2014-08-09 summary:This paper studies quantum annealing (QA) for clustering, which can be seenas an extension of simulated annealing (SA). We derive a QA algorithm forclustering and propose an annealing schedule, which is crucial in practice.Experiments show the proposed QA algorithm finds better clustering assignmentsthan SA. Furthermore, QA is as easy as SA to implement.
arxiv-7800-63 | Quantum Annealing for Variational Bayes Inference | http://arxiv.org/pdf/1408.2037v1.pdf | author:Issei Sato, Kenichi Kurihara, Shu Tanaka, Hiroshi Nakagawa, Seiji Miyashita category:cs.LG stat.ML published:2014-08-09 summary:This paper presents studies on a deterministic annealing algorithm based onquantum annealing for variational Bayes (QAVB) inference, which can be seen asan extension of the simulated annealing for variational Bayes (SAVB) inference.QAVB is as easy as SAVB to implement. Experiments revealed QAVB finds a betterlocal optimum than SAVB in terms of the variational free energy in latentDirichlet allocation (LDA).
arxiv-7800-64 | A direct method for estimating a causal ordering in a linear non-Gaussian acyclic model | http://arxiv.org/pdf/1408.2038v1.pdf | author:Shohei Shimizu, Aapo Hyvarinen, Yoshinobu Kawahara category:cs.LG stat.ML published:2014-08-09 summary:Structural equation models and Bayesian networks have been widely used toanalyze causal relations between continuous variables. In such frameworks,linear acyclic models are typically used to model the datagenerating process ofvariables. Recently, it was shown that use of non-Gaussianity identifies acausal ordering of variables in a linear acyclic model without using any priorknowledge on the network structure, which is not the case with conventionalmethods. However, existing estimation methods are based on iterative searchalgorithms and may not converge to a correct solution in a finite number ofsteps. In this paper, we propose a new direct method to estimate a causalordering based on non-Gaussianity. In contrast to the previous methods, ouralgorithm requires no algorithmic parameters and is guaranteed to converge tothe right solution within a small fixed number of steps if the data strictlyfollows the model.
arxiv-7800-65 | Incorporating Side Information in Probabilistic Matrix Factorization with Gaussian Processes | http://arxiv.org/pdf/1408.2039v1.pdf | author:Ryan Prescott Adams, George E. Dahl, Iain Murray category:cs.LG stat.ML published:2014-08-09 summary:Probabilistic matrix factorization (PMF) is a powerful method for modelingdata associ- ated with pairwise relationships, Finding use in collaborativeFiltering, computational bi- ology, and document analysis, among other areas.In many domains, there are additional covariates that can assist in prediction.For example, when modeling movie ratings, we might know when the ratingoccurred, where the user lives, or what actors appear in the movie. It isdifficult, however, to incorporate this side information into the PMF model. Wepropose a framework for incorporating side information by coupling togethermulti- ple PMF problems via Gaussian process priors. We replace scalar latentfeatures with func- tions that vary over the covariate space. The GP priors onthese functions require them to vary smoothly and share information. We applythis new method to predict the scores of professional basketball games, whereside information about the venue and date of the game are relevant for theoutcome.
arxiv-7800-66 | Prediction with Advice of Unknown Number of Experts | http://arxiv.org/pdf/1408.2040v1.pdf | author:Alexey Chernov, Vladimir Vovk category:cs.LG stat.ML published:2014-08-09 summary:In the framework of prediction with expert advice, we consider a recentlyintroduced kind of regret bounds: the bounds that depend on the effectiveinstead of nominal number of experts. In contrast to the Normal- Hedge bound,which mainly depends on the effective number of experts but also weakly dependson the nominal one, we obtain a bound that does not contain the nominal numberof experts at all. We use the defensive forecasting method and introduce anapplication of defensive forecasting to multivalued supermartingales.
arxiv-7800-67 | GraphLab: A New Framework For Parallel Machine Learning | http://arxiv.org/pdf/1408.2041v1.pdf | author:Yucheng Low, Joseph E. Gonzalez, Aapo Kyrola, Danny Bickson, Carlos E. Guestrin, Joseph Hellerstein category:cs.LG cs.DC published:2014-08-09 summary:Designing and implementing efficient, provably correct parallel machinelearning (ML) algorithms is challenging. Existing high-level parallelabstractions like MapReduce are insufficiently expressive while low-level toolslike MPI and Pthreads leave ML experts repeatedly solving the same designchallenges. By targeting common patterns in ML, we developed GraphLab, whichimproves upon abstractions like MapReduce by compactly expressing asynchronousiterative algorithms with sparse computational dependencies while ensuring dataconsistency and achieving a high degree of parallel performance. We demonstratethe expressiveness of the GraphLab framework by designing and implementingparallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso andCompressed Sensing. We show that using GraphLab we can achieve excellentparallel performance on large scale real-world problems.
arxiv-7800-68 | Gaussian Process Structural Equation Models with Latent Variables | http://arxiv.org/pdf/1408.2042v1.pdf | author:Ricardo Silva, Robert B. Gramacy category:cs.LG stat.ML published:2014-08-09 summary:In a variety of disciplines such as social sciences, psychology, medicine andeconomics, the recorded data are considered to be noisy measurements of latentvariables connected by some causal structure. This corresponds to a family ofgraphical models known as the structural equation model with latent variables.While linear non-Gaussian variants have been well-studied, inference innonparametric structural equation models is still underdeveloped. We introducea sparse Gaussian process parameterization that defines a non-linear structureconnecting latent variables, unlike common formulations of Gaussian processlatent variable models. The sparse parameterization is given a full Bayesiantreatment without compromising Markov chain Monte Carlo efficiency. We comparethe stability of the sampling procedure and the predictive ability of the modelagainst the current practice.
arxiv-7800-69 | Conditional Probability Tree Estimation Analysis and Algorithms | http://arxiv.org/pdf/1408.2031v1.pdf | author:Alina Beygelzimer, John Langford, Yuri Lifshits, Gregory Sorkin, Alexander L. Strehl category:cs.LG stat.ML published:2014-08-09 summary:We consider the problem of estimating the conditional probability of a labelin time O(log n), where n is the number of possible labels. We analyze anatural reduction of this problem to a set of binary regression problemsorganized in a tree structure, proving a regret bound that scales with thedepth of the tree. Motivated by this analysis, we propose the first onlinealgorithm which provably constructs a logarithmic depth tree on the set oflabels to solve this problem. We test the algorithm empirically, showing thatit works succesfully on a dataset with roughly 106 labels.
arxiv-7800-70 | Blind Construction of Optimal Nonlinear Recursive Predictors for Discrete Sequences | http://arxiv.org/pdf/1408.2025v1.pdf | author:Cosma Shalizi, Kristina Lisa Klinkner category:cs.LG stat.ML published:2014-08-09 summary:We present a new method for nonlinear prediction of discrete random sequencesunder minimal structural assumptions. We give a mathematical construction foroptimal predictors of such processes, in the form of hidden Markov models. Wethen describe an algorithm, CSSR (Causal-State Splitting Reconstruction), whichapproximates the ideal predictor from data. We discuss the reliability of CSSR,its data requirements, and its performance in simulations. Finally, we compareour approach to existing methods using variablelength Markov models andcross-validated hidden Markov models, and show theoretically and experimentallythat our method delivers results superior to the former and at least comparableto the latter.
arxiv-7800-71 | Automatic Removal of Marginal Annotations in Printed Text Document | http://arxiv.org/pdf/1408.2015v1.pdf | author:Abdessamad Elboushaki, Rachida Hannane, P. Nagabhushan, Mohammed Javed category:cs.CV published:2014-08-09 summary:Recovering the original printed texts from a document with added handwrittenannotations in the marginal area is one of the challenging problems, especiallywhen the original document is not available. Therefore, this paper aims atsalvaging automatically the original document from the annotated document bydetecting and removing any handwritten annotations that appear in the marginalarea of the document without any loss of information. Here a two stagealgorithm is proposed, where in the first stage due to approximate marginalboundary detection with horizontal and vertical projection profiles, all of themarginal annotations along with some part of the original printed text that mayappear very close to the marginal boundary are removed. Therefore as a secondstage, using the connected components, a strategy is applied to bring back theprinted text components cropped during the first stage. The proposed method isvalidated using a dataset of 50 documents having complex handwrittenannotations, which gives an overall accuracy of 89.01% in removing the marginalannotations and 97.74% in case of retrieving the original printed textdocument.
arxiv-7800-72 | Robust Graphical Modeling with t-Distributions | http://arxiv.org/pdf/1408.2033v1.pdf | author:Michael A. Finegold, Mathias Drton category:cs.LG stat.ML published:2014-08-09 summary:Graphical Gaussian models have proven to be useful tools for exploringnetwork structures based on multivariate data. Applications to studies of geneexpression have generated substantial interest in these models, and resultingrecent progress includes the development of fitting methodology involvingpenalization of the likelihood function. In this paper we advocate the use ofthe multivariate t and related distributions for more robust inference ofgraphs. In particular, we demonstrate that penalized likelihood inferencecombined with an application of the EM algorithm provides a simple andcomputationally efficient approach to model selection in the t-distributioncase.
arxiv-7800-73 | Matrix Coherence and the Nystrom Method | http://arxiv.org/pdf/1408.2044v1.pdf | author:Ameet Talwalkar, Afshin Rostamizadeh category:cs.LG stat.ML published:2014-08-09 summary:The Nystrom method is an efficient technique used to speed up large-scalelearning applications by generating low-rank approximations. Crucial to theperformance of this technique is the assumption that a matrix can be wellapproximated by working exclusively with a subset of its columns. In this workwe relate this assumption to the concept of matrix coherence, connectingcoherence to the performance of the Nystrom method. Making use of related workin the compressed sensing and the matrix completion literature, we derive novelcoherence-based bounds for the Nystrom method in the low-rank setting. We thenpresent empirical results that corroborate these theoretical bounds. Finally,we present more general empirical results for the full-rank setting thatconvincingly demonstrate the ability of matrix coherence to measure the degreeto which information can be extracted from a subset of columns.
arxiv-7800-74 | Bayesian Structure Learning for Markov Random Fields with a Spike and Slab Prior | http://arxiv.org/pdf/1408.2047v1.pdf | author:Yutian Chen, Max Welling category:cs.LG stat.ML published:2014-08-09 summary:In recent years a number of methods have been developed for automaticallylearning the (sparse) connectivity structure of Markov Random Fields. Thesemethods are mostly based on L1-regularized optimization which has a number ofdisadvantages such as the inability to assess model uncertainty and expensivecrossvalidation to find the optimal regularization parameter. Moreover, themodel's predictive performance may degrade dramatically with a suboptimal valueof the regularization parameter (which is sometimes desirable to inducesparseness). We propose a fully Bayesian approach based on a "spike and slab"prior (similar to L0 regularization) that does not suffer from theseshortcomings. We develop an approximate MCMC method combining Langevin dynamicsand reversible jump MCMC to conduct inference in this model. Experiments showthat the proposed model learns a good combination of the structure andparameter values without the need for separate hyper-parameter tuning.Moreover, the model's predictive performance is much more robust than L1-basedmethods with hyper-parameter settings that induce highly sparse modelstructures.
arxiv-7800-75 | Video In Sentences Out | http://arxiv.org/pdf/1408.6418v1.pdf | author:Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, Sanja Fidler, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang category:cs.CV cs.CL cs.IR published:2014-08-09 summary:We present a system that produces sentential descriptions of video: who didwhat to whom, and where and how they did it. Action class is rendered as averb, participant objects as noun phrases, properties of those objects asadjectival modifiers in those noun phrases, spatial relations between thoseparticipants as prepositional phrases, and characteristics of the event asprepositional-phrase adjuncts and adverbial modifiers. Extracting theinformation needed to render these linguistic entities requires an approach toevent recognition that recovers object tracks, the trackto-role assignments,and changing body posture.
arxiv-7800-76 | Bayesian Multitask Learning with Latent Hierarchies | http://arxiv.org/pdf/1408.2032v1.pdf | author:Hal Daume III category:cs.LG stat.ML published:2014-08-09 summary:We learn multiple hypotheses for related tasks under a latent hierarchicalrelationship between tasks. We exploit the intuition that for domainadaptation, we wish to share classifier structure, but for multitask learning,we wish to share covariance structure. Our hierarchical model is seen tosubsume several previously proposed multitask learning models and performs wellon three distinct real-world data sets.
arxiv-7800-77 | Efficient Clustering with Limited Distance Information | http://arxiv.org/pdf/1408.2045v1.pdf | author:Konstantin Voevodski, Maria-Florina Balcan, Heiko Roglin, Shang-Hua Teng, Yu Xia category:cs.LG cs.AI published:2014-08-09 summary:Given a point set S and an unknown metric d on S, we study the problem ofefficiently partitioning S into k clusters while querying few distances betweenthe points. In our model we assume that we have access to one versus allqueries that given a point s 2 S return the distances between s and all otherpoints. We show that given a natural assumption about the structure of theinstance, we can efficiently find an accurate clustering using only O(k)distance queries. We use our algorithm to cluster proteins by sequencesimilarity. This setting nicely fits our model because we can use a fastsequence database search program to query a sequence against an entire dataset.We conduct an empirical study that shows that even though we query a smallfraction of the distances between the points, we produce clusterings that areclose to a desired clustering given by manual classification.
arxiv-7800-78 | Normalized Online Learning | http://arxiv.org/pdf/1408.2065v1.pdf | author:Stephane Ross, Paul Mineiro, John Langford category:cs.LG stat.ML published:2014-08-09 summary:We introduce online learning algorithms which are independent of featurescales, proving regret bounds dependent on the ratio of scales existent in thedata rather than the absolute scale. This has several useful effects: there isno need to pre-normalize data, the test-time and test-space complexity arereduced, and the algorithms are more robust.
arxiv-7800-79 | The Lovasz-Bregman Divergence and connections to rank aggregation, clustering, and web ranking | http://arxiv.org/pdf/1408.2062v1.pdf | author:Rishabh Iyer, Jeff A. Bilmes category:cs.LG stat.ML published:2014-08-09 summary:We extend the recently introduced theory of Lovasz-Bregman (LB) divergences(Iyer & Bilmes 2012) in several ways. We show that they represent a distortionbetween a "score" and an "ordering", thus providing a new view of rankaggregation and order based clustering with interesting connections to webranking. We show how the LB divergences have a number of properties akin tomany permutation based metrics, and in fact have as special cases forms verysimilar to the Kendall-tau metric. We also show how the LB divergences subsumea number of commonly used ranking measures in information retrieval, like NDCGand AUC. Unlike the traditional permutation based metrics, however, the LBdivergence naturally captures a notion of "confidence" in the orderings, thusproviding a new representation to applications involving aggregating scores asopposed to just orderings. We show how a number of recently used web rankingmodels are forms of Lovasz-Bregman rank aggregation and also observe that anatural form of Mallow's model using the LB divergence has been used asconditional ranking models for the "Learning to Rank" problem.
arxiv-7800-80 | Warped Mixtures for Nonparametric Cluster Shapes | http://arxiv.org/pdf/1408.2061v1.pdf | author:Tomoharu Iwata, David Duvenaud, Zoubin Ghahramani category:cs.LG stat.ML published:2014-08-09 summary:A mixture of Gaussians fit to a single curved or heavy-tailed cluster willreport that the data contains many clusters. To produce more appropriateclusterings, we introduce a model which warps a latent mixture of Gaussians toproduce nonparametric cluster shapes. The possibly low-dimensional latentmixture model allows us to summarize the properties of the high-dimensionalclusters (or density manifolds) describing the data. The number of manifolds,as well as the shape and dimension of each manifold is automatically inferred.We derive a simple inference scheme for this model which analyticallyintegrates out both the mixture parameters and the warping function. We showthat our model is effective for density estimation, performs better thaninfinite Gaussian mixture models at recovering the true number of clusters, andproduces interpretable summaries of high-dimensional datasets.
arxiv-7800-81 | Parallel Gaussian Process Regression with Low-Rank Covariance Matrix Approximations | http://arxiv.org/pdf/1408.2060v1.pdf | author:Jie Chen, Nannan Cao, Kian Hsiang Low, Ruofei Ouyang, Colin Keng-Yan Tan, Patrick Jaillet category:cs.LG cs.DC stat.ML published:2014-08-09 summary:Gaussian processes (GP) are Bayesian non-parametric models that are widelyused for probabilistic regression. Unfortunately, it cannot scale well withlarge data nor perform real-time predictions due to its cubic time cost in thedata size. This paper presents two parallel GP regression methods that exploitlow-rank covariance matrix approximations for distributing the computationalload among parallel machines to achieve time efficiency and scalability. Wetheoretically guarantee the predictive performances of our proposed parallelGPs to be equivalent to that of some centralized approximate GP regressionmethods: The computation of their centralized counterparts can be distributedamong parallel machines, hence achieving greater time efficiency andscalability. We analytically compare the properties of our parallel GPs such astime, space, and communication complexity. Empirical evaluation on tworeal-world datasets in a cluster of 20 computing nodes shows that our parallelGPs are significantly more time-efficient and scalable than their centralizedcounterparts and exact/full GP while achieving predictive performancescomparable to full GP.
arxiv-7800-82 | Guess Who Rated This Movie: Identifying Users Through Subspace Clustering | http://arxiv.org/pdf/1408.2055v1.pdf | author:Amy Zhang, Nadia Fawaz, Stratis Ioannidis, Andrea Montanari category:cs.LG cs.IR stat.ML published:2014-08-09 summary:It is often the case that, within an online recommender system, multipleusers share a common account. Can such shared accounts be identified solely onthe basis of the userprovided ratings? Once a shared account is identified, canthe different users sharing it be identified as well? Whenever such useridentification is feasible, it opens the way to possible improvements inpersonalized recommendations, but also raises privacy concerns. We develop amodel for composite accounts based on unions of linear subspaces, and usesubspace clustering for carrying out the identification task. We show that asignificant fraction of such accounts is identifiable in a reliable manner, andillustrate potential uses for personalized recommendation.
arxiv-7800-83 | Non-Convex Rank Minimization via an Empirical Bayesian Approach | http://arxiv.org/pdf/1408.2054v1.pdf | author:David Wipf category:cs.LG cs.NA stat.ML published:2014-08-09 summary:In many applications that require matrix solutions of minimal rank, theunderlying cost function is non-convex leading to an intractable, NP-hardoptimization problem. Consequently, the convex nuclear norm is frequently usedas a surrogate penalty term for matrix rank. The problem is that in manypractical scenarios there is no longer any guarantee that we can correctlyestimate generative low-rank matrices of interest, theoretical special casesnotwithstanding. Consequently, this paper proposes an alternative empiricalBayesian procedure build upon a variational approximation that, unlike thenuclear norm, retains the same globally minimizing point estimate as the rankfunction under many useful constraints. However, locally minimizing solutionsare largely smoothed away via marginalization, allowing the algorithm tosucceed when standard convex relaxations completely fail. While the proposedmethodology is generally applicable to a wide range of low-rank applications,we focus our attention on the robust principal component analysis problem(RPCA), which involves estimating an unknown low-rank matrix with unknownsparse corruptions. Theoretical and empirical evidence are presented to showthat our method is potentially superior to related MAP-based approaches, forwhich the convex principle component pursuit (PCP) algorithm (Candes et al.,2011) can be viewed as a special case.
arxiv-7800-84 | Algorithms for Approximate Minimization of the Difference Between Submodular Functions, with Applications | http://arxiv.org/pdf/1408.2051v1.pdf | author:Rishabh Iyer, Jeff A. Bilmes category:cs.LG stat.ML published:2014-08-09 summary:We extend the work of Narasimhan and Bilmes [30] for minimizing set functionsrepresentable as a dierence between submodular functions. Similar to [30], ournew algorithms are guaranteed to monotonically reduce the objective function atevery step. We empirically and theoretically show that the per-iteration costof our algorithms is much less than [30], and our algorithms can be used toefficiently minimize a dierence between submodular functions under variouscombinatorial constraints, a problem not previously addressed. We providecomputational bounds and a hardness result on the multiplicativeinapproximability of minimizing the dierence between submodular functions. Weshow, however, that it is possible to give worst-case additive bounds byproviding a polynomial time computable lower-bound on the minima. Finally weshow how a number of machine learning problems can be modeled as minimizing thedierence between submodular functions. We experimentally show the validity ofour algorithms by testing them on the problem of feature selection withsubmodular cost features.
arxiv-7800-85 | Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear Multivariate Regression and Granger Causality | http://arxiv.org/pdf/1408.2066v1.pdf | author:Vikas Sindhwani, Ha Quang Minh, Aurelie Lozano category:cs.LG stat.ML published:2014-08-09 summary:We propose a general matrix-valued multiple kernel learning framework forhigh-dimensional nonlinear multivariate regression problems. This frameworkallows a broad class of mixed norm regularizers, including those that inducesparsity, to be imposed on a dictionary of vector-valued Reproducing KernelHilbert Spaces. We develop a highly scalable and eigendecomposition-freealgorithm that orchestrates two inexact solvers for simultaneously learningboth the input and output components of separable matrix-valued kernels. As akey application enabled by our framework, we show how high-dimensional causalinference tasks can be naturally cast as sparse function estimation problems,leading to novel nonlinear extensions of a class of Graphical Granger Causalitytechniques. Our algorithmic developments and extensive empirical studies arecomplemented by theoretical analyses in terms of Rademacher generalizationbounds.
arxiv-7800-86 | LARSEN-ELM: Selective Ensemble of Extreme Learning Machines using LARS for Blended Data | http://arxiv.org/pdf/1408.2003v2.pdf | author:Bo Han, Bo He, Rui Nian, Mengmeng Ma, Shujing Zhang, Minghui Li, Amaury Lendasse category:cs.LG stat.ML published:2014-08-09 summary:Extreme learning machine (ELM) as a neural network algorithm has shown itsgood performance, such as fast speed, simple structure etc, but also, weakrobustness is an unavoidable defect in original ELM for blended data. Wepresent a new machine learning framework called LARSEN-ELM for overcoming thisproblem. In our paper, we would like to show two key steps in LARSEN-ELM. Inthe first step, preprocessing, we select the input variables highly related tothe output using least angle regression (LARS). In the second step, training,we employ Genetic Algorithm (GA) based selective ensemble and original ELM. Inthe experiments, we apply a sum of two sines and four datasets from UCIrepository to verify the robustness of our approach. The experimental resultsshow that compared with original ELM and other methods such as OP-ELM,GASEN-ELM and LSBoost, LARSEN-ELM significantly improve robustness performancewhile keeping a relatively high speed.
arxiv-7800-87 | Probabilistic inverse reinforcement learning in unknown environments | http://arxiv.org/pdf/1408.2067v1.pdf | author:Aristide Tossou, Christos Dimitrakakis category:cs.LG stat.ML published:2014-08-09 summary:We consider the problem of learning by demonstration from agents acting inunknown stochastic Markov environments or games. Our aim is to estimate agentpreferences in order to construct improved policies for the same task that theagents are trying to solve. To do so, we extend previous probabilisticapproaches for inverse reinforcement learning in known MDPs to the case ofunknown dynamics or opponents. We do this by deriving two simplifiedprobabilistic models of the demonstrator's policy and utility. Fortractability, we use maximum a posteriori estimation rather than full Bayesianinference. Under a flat prior, this results in a convex optimisation problem.We find that the resulting algorithms are highly competitive against a varietyof other methods for inverse reinforcement learning that do have knowledge ofthe dynamics.
arxiv-7800-88 | Statistical guarantees for the EM algorithm: From population to sample-based analysis | http://arxiv.org/pdf/1408.2156v1.pdf | author:Sivaraman Balakrishnan, Martin J. Wainwright, Bin Yu category:math.ST cs.LG stat.ML stat.TH published:2014-08-09 summary:We develop a general framework for proving rigorous guarantees on theperformance of the EM algorithm and a variant known as gradient EM. Ouranalysis is divided into two parts: a treatment of these algorithms at thepopulation level (in the limit of infinite data), followed by results thatapply to updates based on a finite set of samples. First, we characterize thedomain of attraction of any global maximizer of the population likelihood. Thischaracterization is based on a novel view of the EM updates as a perturbed formof likelihood ascent, or in parallel, of the gradient EM updates as a perturbedform of standard gradient ascent. Leveraging this characterization, we thenprovide non-asymptotic guarantees on the EM and gradient EM algorithms whenapplied to a finite set of samples. We develop consequences of our generaltheory for three canonical examples of incomplete-data problems: mixture ofGaussians, mixture of regressions, and linear regression with covariatesmissing completely at random. In each case, our theory guarantees that with asuitable initialization, a relatively small number of EM (or gradient EM) stepswill yield (with high probability) an estimate that is within statistical errorof the MLE. We provide simulations to confirm this theoretically predictedbehavior.
arxiv-7800-89 | Optimally-Weighted Herding is Bayesian Quadrature | http://arxiv.org/pdf/1408.2049v1.pdf | author:Ferenc Huszar, David Duvenaud category:cs.LG stat.ML published:2014-08-09 summary:Herding and kernel herding are deterministic methods of choosing sampleswhich summarise a probability distribution. A related task is choosing samplesfor estimating integrals using Bayesian quadrature. We show that the criterionminimised when selecting samples in kernel herding is equivalent to theposterior variance in Bayesian quadrature. We then show that sequentialBayesian quadrature can be viewed as a weighted version of kernel herding whichachieves performance superior to any other weighted herding method. Wedemonstrate empirically a rate of convergence faster than O(1/N). Our resultsalso imply an upper bound on the empirical error of the Bayesian quadratureestimate.
arxiv-7800-90 | RMSE-ELM: Recursive Model based Selective Ensemble of Extreme Learning Machines for Robustness Improvement | http://arxiv.org/pdf/1408.2004v3.pdf | author:Bo Han, Bo He, Mengmeng Ma, Tingting Sun, Tianhong Yan, Amaury Lendasse category:cs.LG cs.NE published:2014-08-09 summary:Extreme learning machine (ELM) as an emerging branch of shallow networks hasshown its excellent generalization and fast learning speed. However, forblended data, the robustness of ELM is weak because its weights and biases ofhidden nodes are set randomly. Moreover, the noisy data exert a negativeeffect. To solve this problem, a new framework called RMSE-ELM is proposed inthis paper. It is a two-layer recursive model. In the first layer, theframework trains lots of ELMs in different groups concurrently, then employsselective ensemble to pick out an optimal set of ELMs in each group, which canbe merged into a large group of ELMs called candidate pool. In the secondlayer, selective ensemble is recursively used on candidate pool to acquire thefinal ensemble. In the experiments, we apply UCI blended datasets to confirmthe robustness of our new approach in two key aspects (mean square error andstandard deviation). The space complexity of our method is increased to somedegree, but the results have shown that RMSE-ELM significantly improvesrobustness with slightly computational time compared with representativemethods (ELM, OP-ELM, GASEN-ELM, GASEN-BP and E-GASEN). It becomes a potentialframework to solve robustness issue of ELM for high-dimensional blended data inthe future.
arxiv-7800-91 | One-Class Support Measure Machines for Group Anomaly Detection | http://arxiv.org/pdf/1408.2064v1.pdf | author:Krikamol Muandet, Bernhard Schoelkopf category:cs.LG stat.ML published:2014-08-09 summary:We propose one-class support measure machines (OCSMMs) for group anomalydetection which aims at recognizing anomalous aggregate behaviors of datapoints. The OCSMMs generalize well-known one-class support vector machines(OCSVMs) to a space of probability measures. By formulating the problem asquantile estimation on distributions, we can establish an interestingconnection to the OCSVMs and variable kernel density estimators (VKDEs) overthe input space on which the distributions are defined, bridging the gapbetween large-margin methods and kernel density estimators. In particular, weshow that various types of VKDEs can be considered as solutions to a class ofregularization problems studied in this paper. Experiments on Sloan Digital SkySurvey dataset and High Energy Particle Physics dataset demonstrate thebenefits of the proposed framework in real-world applications.
arxiv-7800-92 | Characterizing predictable classes of processes | http://arxiv.org/pdf/1408.2036v2.pdf | author:Daniil Ryabko category:cs.LG stat.ML published:2014-08-09 summary:The problem is sequence prediction in the following setting. A sequencex1,..., xn,... of discrete-valued observations is generated according to someunknown probabilistic law (measure) mu. After observing each outcome, it isrequired to give the conditional probabilities of the next observation. Themeasure mu belongs to an arbitrary class C of stochastic processes. We areinterested in predictors ? whose conditional probabilities converge to the'true' mu-conditional probabilities if any mu { C is chosen to generate thedata. We show that if such a predictor exists, then a predictor can also beobtained as a convex combination of a countably many elements of C. In otherwords, it can be obtained as a Bayesian predictor whose prior is concentratedon a countable set. This result is established for two very different measuresof performance of prediction, one of which is very strong, namely, totalvariation, and the other is very weak, namely, prediction in expected averageKullback-Leibler divergence.
arxiv-7800-93 | Neighborhood Rank Order Coding for Robust Texture Analysis and Feature Extraction | http://arxiv.org/pdf/1408.1984v1.pdf | author:C. Mayr, R. SchÃ¼ffny category:cs.CV published:2014-08-08 summary:Research into the visual cortex and general neural information processing hasled to various attempts to integrate pulse computation schemes in imageanalysis systems. Of interest is especially the robustness of representing ananalogue signal in the phase or duration of a pulsed, quasi-digital signal, aswell as the possibility of direct digital interaction, i.e. computation, amongthese signals. Such a computation can also achieve information compaction forsubsequent processing stages. By using a pulse order encoding scheme motivatedby dendritic pulse interaction, we will show that a powerful low-level featureand texture extraction operator, called Pulsed Local Orientation Coding (PLOC),can be implemented. Feature extraction results are being presented, and apossible VLSI implementation is detailed.
arxiv-7800-94 | Microtask crowdsourcing for disease mention annotation in PubMed abstracts | http://arxiv.org/pdf/1408.1928v1.pdf | author:Benjamin M Good, Max Nanis, Andrew I. Su category:cs.CL 9208 H.5.3; I.2.7 published:2014-08-08 summary:Identifying concepts and relationships in biomedical text enables knowledgeto be applied in computational analyses. Many biological natural languageprocess (BioNLP) projects attempt to address this challenge, but the state ofthe art in BioNLP still leaves much room for improvement. Progress in BioNLPresearch depends on large, annotated corpora for evaluating informationextraction systems and training machine learning models. Traditionally, suchcorpora are created by small numbers of expert annotators often working overextended periods of time. Recent studies have shown that workers on microtaskcrowdsourcing platforms such as Amazon's Mechanical Turk (AMT) can, inaggregate, generate high-quality annotations of biomedical text. Here, weinvestigated the use of the AMT in capturing disease mentions in PubMedabstracts. We used the NCBI Disease corpus as a gold standard for refining andbenchmarking our crowdsourcing protocol. After several iterations, we arrivedat a protocol that reproduced the annotations of the 593 documents in thetraining set of this gold standard with an overall F measure of 0.872(precision 0.862, recall 0.883). The output can also be tuned to optimize forprecision (max = 0.984 when recall = 0.269) or recall (max = 0.980 whenprecision = 0.436). Each document was examined by 15 workers, and theirannotations were merged based on a simple voting method. In total 145 workerscombined to complete all 593 documents in the span of 1 week at a cost of $.06per abstract per worker. The quality of the annotations, as judged with the Fmeasure, increases with the number of workers assigned to each task such thatthe system can be tuned to balance cost against quality. These resultsdemonstrate that microtask crowdsourcing can be a valuable tool for generatingwell-annotated corpora in BioNLP.
arxiv-7800-95 | Using Learned Predictions as Feedback to Improve Control and Communication with an Artificial Limb: Preliminary Findings | http://arxiv.org/pdf/1408.1913v1.pdf | author:Adam S. R. Parker, Ann L. Edwards, Patrick M. Pilarski category:cs.AI cs.HC cs.LG cs.RO published:2014-08-08 summary:Many people suffer from the loss of a limb. Learning to get by without an armor hand can be very challenging, and existing prostheses do not yet fulfil theneeds of individuals with amputations. One promising solution is to providegreater communication between a prosthesis and its user. Towards this end, wepresent a simple machine learning interface to supplement the control of arobotic limb with feedback to the user about what the limb will be experiencingin the near future. A real-time prediction learner was implemented to predictimpact-related electrical load experienced by a robot limb; the learningsystem's predictions were then communicated to the device's user to aid intheir interactions with a workspace. We tested this system with fiveable-bodied subjects. Each subject manipulated the robot arm while receivingdifferent forms of vibrotactile feedback regarding the arm's contact with itsworkspace. Our trials showed that communicable predictions could be learnedquickly during human control of the robot arm. Using these predictions as abasis for feedback led to a statistically significant improvement in taskperformance when compared to purely reactive feedback from the device. Ourstudy therefore contributes initial evidence that prediction learning andmachine intelligence can benefit not just control, but also feedback from anartificial limb. We expect that a greater level of acceptance and ownership canbe achieved if the prosthesis itself takes an active role in transmittinglearned knowledge about its state and its situation of use.
arxiv-7800-96 | Gabor-like Image Filtering using a Neural Microcircuit | http://arxiv.org/pdf/1408.1986v1.pdf | author:C. Mayr, A. Heittmann, R. SchÃ¼ffny category:cs.CV cs.ET q-bio.NC published:2014-08-08 summary:In this letter, we present an implementation of a neural microcircuit forimage processing employing Hebbian-adaptive learning. The neuronal circuitutilizes only excitatory synapses to correlate action potentials, extractingthe uncorrelated ones, which contain significant image information. Thiscircuit is capable of approximating Gabor-like image filtering and other imageprocessing functions
arxiv-7800-97 | Exploring the evolution of a trade-off between vigilance and foraging in group-living organisms | http://arxiv.org/pdf/1408.1906v1.pdf | author:Randal S. Olson, Patrick B. Haley, Fred C. Dyer, Christoph Adami category:q-bio.PE cs.GT cs.NE published:2014-08-08 summary:Despite the fact that grouping behavior has been actively studied for over acentury, the relative importance of the numerous proposed fitness benefits ofgrouping remain unclear. We use a digital model of evolving prey undersimulated predation to directly explore the evolution of gregarious foragingbehavior according to one such benefit, the "many eyes" hypothesis. Accordingto this hypothesis, collective vigilance allows prey in large groups to detectpredators more efficiently by making alarm signals or behavioral cues to eachother, thereby allowing individuals within the group to spend more timeforaging. Here, we find that collective vigilance is sufficient to select forgregarious foraging behavior as long there is not a direct cost for grouping(e.g., competition for limited food resources), even when controlling forconfounding factors such as the dilution effect. Further, we explore the roleof the genetic relatedness and reproductive strategy of the prey, and find thathighly related groups of prey with a semelparous reproductive strategy are themost likely to evolve gregarious foraging behavior mediated by the benefit ofvigilance. These findings, combined with earlier studies with evolving digitalorganisms, further sharpen our understanding of the factors favoring groupingbehavior.
arxiv-7800-98 | A model of grassroots changes in linguistic systems | http://arxiv.org/pdf/1408.1985v1.pdf | author:Janet B. Pierrehumbert, Forrest Stonedahl, Robert Daland category:cs.CL nlin.AO physics.soc-ph published:2014-08-08 summary:Linguistic norms emerge in human communities because people imitate eachother. A shared linguistic system provides people with the benefits of sharedknowledge and coordinated planning. Once norms are in place, why would theyever change? This question, echoing broad questions in the theory of socialdynamics, has particular force in relation to language. By definition, aninnovator is in the minority when the innovation first occurs. In some areas ofsocial dynamics, important minorities can strongly influence the majoritythrough their power, fame, or use of broadcast media. But most linguisticchanges are grassroots developments that originate with ordinary people. Here,we develop a novel model of communicative behavior in communities, and identifya mechanism for arbitrary innovations by ordinary people to have a good chanceof being widely adopted. To imitate each other, people must form a mental representation of what otherpeople do. Each time they speak, they must also decide which form to producethemselves. We introduce a new decision function that enables us to smoothlyexplore the space between two types of behavior: probability matching (matchingthe probabilities of incoming experience) and regularization (producing someforms disproportionately often). Using Monte Carlo methods, we explore theinteractions amongst the degree of regularization, the distribution of biasesin a network, and the network position of the innovator. We identify tworegimes for the widespread adoption of arbritrary innovations, viewed asinformational cascades in the network. With moderate regularization ofexperienced input, average people (not well-connected people) are the mostlikely source of successful innovations. Our results shed light on a majoroutstanding puzzle in the theory of language change. The framework also holdspromise for understanding the dynamics of other social norms.
arxiv-7800-99 | Origin of the computational hardness for learning with binary synapses | http://arxiv.org/pdf/1408.1784v1.pdf | author:Haiping Huang, Yoshiyuki Kabashima category:cs.LG q-bio.NC published:2014-08-08 summary:Supervised learning in a binary perceptron is able to classify an extensivenumber of random patterns by a proper assignment of binary synaptic weights.However, to find such assignments in practice, is quite a nontrivial task. Therelation between the weight space structure and the algorithmic hardness hasnot yet been fully understood. To this end, we analytically derive theFranz-Parisi potential for the binary preceptron problem, by starting from anequilibrium solution of weights and exploring the weight space structure aroundit. Our result reveals the geometrical organization of the weightspace\textemdash the weight space is composed of isolated solutions, ratherthan clusters of exponentially many close-by solutions. The point-like clustersfar apart from each other in the weight space explain the previously observedglassy behavior of stochastic local search heuristics.
arxiv-7800-100 | Beyond description. Comment on "Approaching human language with complex networks" by Cong & Liu | http://arxiv.org/pdf/1408.1774v1.pdf | author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph published:2014-08-08 summary:Comment on "Approaching human language with complex networks" by Cong & Liu
arxiv-7800-101 | Real-Time and Robust Method for Hand Gesture Recognition System Based on Cross-Correlation Coefficient | http://arxiv.org/pdf/1408.1759v1.pdf | author:Reza Azad, Babak Azad, Iman Tavakoli Kazerooni category:cs.CV published:2014-08-08 summary:Hand gesture recognition possesses extensive applications in virtual reality,sign language recognition, and computer games. The direct interface of handgestures provides us a new way for communicating with the virtual environment.In this paper a novel and real-time approach for hand gesture recognitionsystem is presented. In the suggested method, first, the hand gesture isextracted from the main image by the image segmentation and morphologicaloperation and then is sent to feature extraction stage. In feature extractionstage the Cross-correlation coefficient is applied on the gesture to recognizeit. In the result part, the proposed approach is applied on American SignLanguage (ASL) database and the accuracy rate obtained 98.34%.
arxiv-7800-102 | A Parallel Algorithm for Exact Bayesian Structure Discovery in Bayesian Networks | http://arxiv.org/pdf/1408.1664v2.pdf | author:Yetian Chen, Jin Tian, Olga Nikolova, Srinivas Aluru category:cs.AI cs.DC cs.LG published:2014-08-07 summary:Exact Bayesian structure discovery in Bayesian networks requires exponentialtime and space. Using dynamic programming (DP), the fastest known serialalgorithm computes the exact posterior probabilities of structural features in$O(n2^n)$ time and space, if the number of parents per node or indegree isbounded by a constant $d$. Here we present a parallel algorithm capable ofcomputing the exact posterior probabilities for all $n(n-1)$ edges with optimalparallel time and space efficiency. That is, if $p=2^k$ processors are used,the run-time and space usage reduce to $O(n2^{n-k}+k(n-k)^d)$ and$O(n2^{n-k})$, respectively. Our algorithm is based the observation that theoriginal DP steps constitute a $n$-$D$ hypercube. In our algorithm, we take adelicate way to coordinate the computations of correlated DP procedures suchthat large amount of data exchange is suppressed. Further, we develop paralleltechniques for two variants of the well-known zeta transform, which haveapplications outside the context of Bayesian networks. We demonstrate thecapability of our algorithm on datasets with up to 33 variables and itsscalability on up to 2048 processors.
arxiv-7800-103 | Low-rank SIFT: An Affine Invariant Feature for Place Recognition | http://arxiv.org/pdf/1408.1688v1.pdf | author:Chao Yang, Shengnan Caih, Jingdong Wang, Long Quan category:cs.CV published:2014-08-07 summary:In this paper, we present a novel affine-invariant feature based on SIFT,leveraging the regular appearance of man-made objects. The feature achievesfull affine invariance without needing to simulate over affine parameter space.Low-rank SIFT, as we name the feature, is based on our observation that localtilt, which are caused by changes of camera axis orientation, could benormalized by converting local patches to standard low-rank forms. Rotation,translation and scaling invariance could be achieved in ways similar to SIFT.As an extension of SIFT, our method seeks to add prior to solve the ill-posedaffine parameter estimation problem and normalizes them directly, and isapplicable to objects with regular structures. Furthermore, owing to recentbreakthrough in convex optimization, such parameter could be computedefficiently. We will demonstrate its effectiveness in place recognition as ourmajor application. As extra contributions, we also describe our pipeline ofconstructing geotagged building database from the ground up, as well as anefficient scheme for automatic feature selection.
arxiv-7800-104 | Real-Time Human-Computer Interaction Based on Face and Hand Gesture Recognition | http://arxiv.org/pdf/1408.1549v1.pdf | author:Reza Azad, Babak Azad, Nabil Belhaj Khalifa, Shahram Jamali category:cs.CV published:2014-08-07 summary:At the present time, hand gestures recognition system could be used as a moreexpected and useable approach for human computer interaction. Automatic handgesture recognition system provides us a new tactic for interactive with thevirtual environment. In this paper, a face and hand gesture recognition systemwhich is able to control computer media player is offered. Hand gesture andhuman face are the key element to interact with the smart system. We used theface recognition scheme for viewer verification and the hand gesturerecognition in mechanism of computer media player, for instance, volumedown/up, next music and etc. In the proposed technique, first, the hand gestureand face location is extracted from the main image by combination of skin andcascade detector and then is sent to recognition stage. In recognition stage,first, the threshold condition is inspected then the extracted face and gesturewill be recognized. In the result stage, the proposed technique is applied onthe video dataset and the high precision ratio acquired. Additional therecommended hand gesture recognition method is applied on static American SignLanguage (ASL) database and the correctness rate achieved nearby 99.40%. alsothe planned method could be used in gesture based computer games and virtualreality.
arxiv-7800-105 | Matrix Completion on Graphs | http://arxiv.org/pdf/1408.1717v3.pdf | author:Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre Vandergheynst category:cs.LG stat.ML published:2014-08-07 summary:The problem of finding the missing values of a matrix given a few of itsentries, called matrix completion, has gathered a lot of attention in therecent years. Although the problem under the standard low rank assumption isNP-hard, Cand\`es and Recht showed that it can be exactly relaxed if the numberof observed entries is sufficiently large. In this work, we introduce a novelmatrix completion model that makes use of proximity information about rows andcolumns by assuming they form communities. This assumption makes sense inseveral real-world problems like in recommender systems, where there arecommunities of people sharing preferences, while products form clusters thatreceive similar ratings. Our main goal is thus to find a low-rank solution thatis structured by the proximities of rows and columns encoded by graphs. Weborrow ideas from manifold learning to constrain our solution to be smooth onthese graphs, in order to implicitly force row and column proximities. Ourmatrix recovery model is formulated as a convex non-smooth optimizationproblem, for which a well-posed iterative scheme is provided. We study andevaluate the proposed matrix completion on synthetic and real data, showingthat the proposed structured low-rank recovery model outperforms the standardmatrix completion model in many situations.
arxiv-7800-106 | Face Detection Using Radial Basis Functions Neural Networks With Fixed Spread | http://arxiv.org/pdf/1410.2173v1.pdf | author:K. A. A. Aziz, S. S. Abdullah category:cs.CV published:2014-08-07 summary:This paper presented a face detection system using Radial Basis FunctionNeural Networks With Fixed Spread Value. Face detection is the first step inface recognition system. The purpose is to localize and extract the face regionfrom the background that will be fed into the face recognition system foridentification. General preprocessing approach was used for normalizing theimage and Radial Basis Function (RBF) Neural Network was used to distinguishbetween face and non-face. RBF Neural Networks offer several advantagescompared to other neural network architecture such as they can be trained usingfast two stages training algorithm and the network possesses the property ofbest approximation. The output of the network can be optimized by settingsuitable value of center and spread of the RBF. In this paper, fixed spreadvalue will be used. The Radial Basis Function Neural Network (RBFNN) used todistinguish faces and non-faces and the evaluation of the system will be theperformance of detection, False Acceptance Rate (FAR), False Rejection Rate(FRR) and the discriminative properties.
arxiv-7800-107 | Racing to Learn: Statistical Inference and Learning in a Single Spiking Neuron with Adaptive Kernels | http://arxiv.org/pdf/1408.1245v4.pdf | author:Saeed Afshar, Libin George, Jonathan Tapson, Andre van Schaik, Tara Julia Hamilton category:cs.NE q-bio.NC published:2014-08-06 summary:This paper describes the Synapto-dendritic Kernel Adapting Neuron (SKAN), asimple spiking neuron model that performs statistical inference andunsupervised learning of spatiotemporal spike patterns. SKAN is the firstproposed neuron model to investigate the effects of dynamic synapto-dendritickernels and demonstrate their computational power even at the single neuronscale. The rule-set defining the neuron is simple there are no complexmathematical operations such as normalization, exponentiation or evenmultiplication. The functionalities of SKAN emerge from the real-timeinteraction of simple additive and binary processes. Like a biological neuron,SKAN is robust to signal and parameter noise, and can utilize both in itsoperations. At the network scale neurons are locked in a race with each otherwith the fastest neuron to spike effectively hiding its learnt pattern from itsneighbors. The robustness to noise, high speed and simple building blocks notonly make SKAN an interesting neuron model in computational neuroscience, butalso make it ideal for implementation in digital and analog neuromorphicsystems which is demonstrated through an implementation in a Field ProgrammableGate Array (FPGA).
arxiv-7800-108 | Scalable Greedy Algorithms for Transfer Learning | http://arxiv.org/pdf/1408.1292v3.pdf | author:Ilja Kuzborskij, Francesco Orabona, Barbara Caputo category:cs.CV cs.LG published:2014-08-06 summary:In this paper we consider the binary transfer learning problem, focusing onhow to select and combine sources from a large pool to yield a good performanceon a target task. Constraining our scenario to real world, we do not assume thedirect access to the source data, but rather we employ the source hypothesestrained from them. We propose an efficient algorithm that selects relevantsource hypotheses and feature dimensions simultaneously, building on theliterature on the best subset selection problem. Our algorithm achievesstate-of-the-art results on three computer vision datasets, substantiallyoutperforming both transfer learning and popular feature selection baselines ina small-sample setting. We also present a randomized variant that achieves thesame results with a fraction of the computational cost. Also, we theoreticallyprove that, under reasonable assumptions on the source hypotheses, ouralgorithm can learn effectively from few examples.
arxiv-7800-109 | Preventing False Discovery in Interactive Data Analysis is Hard | http://arxiv.org/pdf/1408.1655v1.pdf | author:Moritz Hardt, Jonathan Ullman category:cs.LG cs.CC cs.DS published:2014-08-06 summary:We show that, under a standard hardness assumption, there is nocomputationally efficient algorithm that given $n$ samples from an unknowndistribution can give valid answers to $n^{3+o(1)}$ adaptively chosenstatistical queries. A statistical query asks for the expectation of apredicate over the underlying distribution, and an answer to a statisticalquery is valid if it is "close" to the correct expectation over thedistribution. Our result stands in stark contrast to the well known fact that exponentiallymany statistical queries can be answered validly and efficiently if the queriesare chosen non-adaptively (no query may depend on the answers to previousqueries). Moreover, a recent work by Dwork et al. shows how to accuratelyanswer exponentially many adaptively chosen statistical queries via acomputationally inefficient algorithm; and how to answer a quadratic number ofadaptive queries via a computationally efficient algorithm. The latter resultimplies that our result is tight up to a linear factor in $n.$ Conceptually, our result demonstrates that achieving statistical validityalone can be a source of computational intractability in adaptive settings. Forexample, in the modern large collaborative research environment, data analyststypically choose a particular approach based on previous findings. Falsediscovery occurs if a research finding is supported by the data but not by theunderlying distribution. While the study of preventing false discovery inStatistics is decades old, to the best of our knowledge our result is the firstto demonstrate a computational barrier. In particular, our result suggests thatthe perceived difficulty of preventing false discovery in today's collaborativeresearch environment may be inherent.
arxiv-7800-110 | When does Active Learning Work? | http://arxiv.org/pdf/1408.1319v1.pdf | author:Lewis Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG published:2014-08-06 summary:Active Learning (AL) methods seek to improve classifier performance whenlabels are expensive or scarce. We consider two central questions: Where doesAL work? How much does it help? To address these questions, a comprehensiveexperimental simulation study of Active Learning is presented. We consider avariety of tasks, classifiers and other AL factors, to present a broadexploration of AL performance in various settings. A precise way to quantifyperformance is needed in order to know when AL works. Thus we also present adetailed methodology for tackling the complexities of assessing AL performancein the context of this experimental study.
arxiv-7800-111 | New crossover operators for multiple subset selection tasks | http://arxiv.org/pdf/1408.1297v1.pdf | author:Arnab Roy, J. David Schaffer, Craig B. Laramee category:cs.NE 68 I.5.2, I.5.4 published:2014-08-06 summary:We have introduced two crossover operators, MMX-BLXexploit andMMX-BLXexplore, for simultaneously solving multiple feature/subset selectionproblems where the features may have numeric attributes and the subset sizesare not predefined. These operators differ on the level of exploration andexploitation they perform; one is designed to produce convergence controlledmutation and the other exhibits a quasi-constant mutation rate. We illustratethe characteristic of these operators by evolving pattern detectors todistinguish alcoholics from controls using their visually evoked responsepotentials (VERPs). This task encapsulates two groups of subset selectionproblems; choosing a subset of EEG leads along with the lead-weights (featureswith attributes) and the other that defines the temporal pattern thatcharacterizes the alcoholic VERPs. We observed better generalizationperformance from MMX-BLXexplore. Perhaps, MMX-BLXexploit was handicapped by nothaving a restart mechanism. These operators are novel and appears to holdpromise for solving simultaneous feature selection problems.
arxiv-7800-112 | A Population Background for Nonparametric Density-Based Clustering | http://arxiv.org/pdf/1408.1381v2.pdf | author:JosÃ© E. ChacÃ³n category:math.ST math.CA math.DG math.GT stat.ML stat.TH published:2014-08-06 summary:Despite its popularity, it is widely recognized that the investigation ofsome theoretical aspects of clustering has been relatively sparse. One of themain reasons for this lack of theoretical results is surely the fact that,whereas for other statistical problems the theoretical population goal isclearly defined (as in regression or classification), for some of theclustering methodologies it is difficult to specify the population goal towhich the data-based clustering algorithms should try to get close. This paperaims to provide some insight into the theoretical foundations of clustering byfocusing on two main objectives: to provide an explicit formulation for theideal population goal of the modal clustering methodology, which understandsclusters as regions of high density; and to present two new loss functions,applicable in fact to any clustering methodology, to evaluate the performanceof a data-based clustering algorithm with respect to the ideal population goal.In particular, it is shown that only mild conditions on a sequence of densityestimators are needed to ensure that the sequence of modal clusterings thatthey induce is consistent.
arxiv-7800-113 | The functional mean-shift algorithm for mode hunting and clustering in infinite dimensions | http://arxiv.org/pdf/1408.1187v1.pdf | author:Mattia Ciollaro, Christopher Genovese, Jing Lei, Larry Wasserman category:stat.ME stat.AP stat.ML published:2014-08-06 summary:We introduce the functional mean-shift algorithm, an iterative algorithm forestimating the local modes of a surrogate density from functional data. We showthat the algorithm can be used for cluster analysis of functional data. Wepropose a test based on the bootstrap for the significance of the estimatedlocal modes of the surrogate density. We present two applications of ourmethodology. In the first application, we demonstrate how the functionalmean-shift algorithm can be used to perform spike sorting, i.e. cluster neuralactivity curves. In the second application, we use the functional mean-shiftalgorithm to distinguish between original and fake signatures.
arxiv-7800-114 | Mixed-Variate Restricted Boltzmann Machines | http://arxiv.org/pdf/1408.1160v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG stat.ME published:2014-08-06 summary:Modern datasets are becoming heterogeneous. To this end, we present in thispaper Mixed-Variate Restricted Boltzmann Machines for simultaneously modellingvariables of multiple types and modalities, including binary and continuousresponses, categorical options, multicategorical choices, ordinal assessmentand category-ranked preferences. Dependency among variables is modeled usinglatent binary variables, each of which can be interpreted as a particularhidden aspect of the data. The proposed model, similar to the standard RBMs,allows fast evaluation of the posterior for the latent variables. Hence, it isnaturally suitable for many common tasks including, but not limited to, (a) asa pre-processing step to convert complex input data into a more convenientvectorial representation through the latent posteriors, thereby offering adimensionality reduction capacity, (b) as a classifier supporting binary,multiclass, multilabel, and label-ranking outputs, or a regression tool forcontinuous outputs and (c) as a data completion tool for multimodal andheterogeneous data. We evaluate the proposed model on a large-scale datasetusing the world opinion survey results on three tasks: feature extraction andvisualization, data completion and prediction.
arxiv-7800-115 | On the Generalization of the C-Bound to Structured Output Ensemble Methods | http://arxiv.org/pdf/1408.1336v2.pdf | author:FranÃ§ois Laviolette, Emilie Morvant, Liva Ralaivola, Jean-Francis Roy category:stat.ML published:2014-08-06 summary:This paper generalizes an important result from the PAC-Bayesian literaturefor binary classification to the case of ensemble methods for structuredoutputs. We prove a generic version of the \Cbound, an upper bound over therisk of models expressed as a weighted majority vote that is based on the firstand second statistical moments of the vote's margin. This bound mayadvantageously $(i)$ be applied on more complex outputs such as multiclasslabels and multilabel, and $(ii)$ allow to consider margin relaxations. Theseresults open the way to develop new ensemble methods for structured outputprediction with PAC-Bayesian guarantees.
arxiv-7800-116 | Boosted Markov Networks for Activity Recognition | http://arxiv.org/pdf/1408.1167v1.pdf | author:Truyen Tran, Hung Bui, Svetha Venkatesh category:cs.LG cs.CV stat.ML published:2014-08-06 summary:We explore a framework called boosted Markov networks to combine the learningcapacity of boosting and the rich modeling semantics of Markov networks andapplying the framework for video-based activity recognition. Importantly, weextend the framework to incorporate hidden variables. We show how the frameworkcan be applied for both model learning and feature selection. We demonstratethat boosted Markov networks with hidden variables perform comparably with thestandard maximum likelihood estimation. However, our framework is able to learnsparse models, and therefore can provide computational savings when the learnedmodels are used for classification.
arxiv-7800-117 | Human Activity Learning and Segmentation using Partially Hidden Discriminative Models | http://arxiv.org/pdf/1408.3081v1.pdf | author:Truyen Tran, Hung Bui, Svetha Venkatesh category:cs.LG cs.CV stat.ML published:2014-08-06 summary:Learning and understanding the typical patterns in the daily activities androutines of people from low-level sensory data is an important problem in manyapplication domains such as building smart environments, or providingintelligent assistance. Traditional approaches to this problem typically relyon supervised learning and generative models such as the hidden Markov modelsand its extensions. While activity data can be readily acquired from pervasivesensors, e.g. in smart environments, providing manual labels to supportsupervised training is often extremely expensive. In this paper, we propose anew approach based on semi-supervised training of partially hiddendiscriminative models such as the conditional random field (CRF) and themaximum entropy Markov model (MEMM). We show that these models allow us toincorporate both labeled and unlabeled data for learning, and at the same time,provide us with the flexibility and accuracy of the discriminative framework.Our experimental results in the video surveillance domain illustrate that thesemodels can perform better than their generative counterpart, the partiallyhidden Markov model, even when a substantial amount of labels are unavailable.
arxiv-7800-118 | MCMC for Hierarchical Semi-Markov Conditional Random Fields | http://arxiv.org/pdf/1408.1162v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh, Hung H. Bui category:stat.ML cs.LG stat.ME published:2014-08-06 summary:Deep architecture such as hierarchical semi-Markov models is an importantclass of models for nested sequential data. Current exact inference schemeseither cost cubic time in sequence length, or exponential time in model depth.These costs are prohibitive for large-scale problems with arbitrary length anddepth. In this contribution, we propose a new approximation technique that mayhave the potential to achieve sub-cubic time complexity in length and lineartime depth, at the cost of some loss of quality. The idea is based on twowell-known methods: Gibbs sampling and Rao-Blackwellisation. We provide somesimulation-based evaluation of the quality of the RGBS with respect to run timeand sequence length.
arxiv-7800-119 | A Fast and Accurate Unconstrained Face Detector | http://arxiv.org/pdf/1408.1656v3.pdf | author:Shengcai Liao, Anil K. Jain, Stan Z. Li category:cs.CV published:2014-08-06 summary:We propose a method to address challenges in unconstrained face detection,such as arbitrary pose variations and occlusions. First, a new image featurecalled Normalized Pixel Difference (NPD) is proposed. NPD feature is computedas the difference to sum ratio between two pixel values, inspired by the WeberFraction in experimental psychology. The new feature is scale invariant,bounded, and is able to reconstruct the original image. Second, we propose adeep quadratic tree to learn the optimal subset of NPD features and theircombinations, so that complex face manifolds can be partitioned by the learnedrules. This way, only a single soft-cascade classifier is needed to handleunconstrained face detection. Furthermore, we show that the NPD features can beefficiently obtained from a look up table, and the detection template can beeasily scaled, making the proposed face detector very fast. Experimentalresults on three public face datasets (FDDB, GENKI, and CMU-MIT) show that theproposed method achieves state-of-the-art performance in detectingunconstrained faces with arbitrary pose variations and occlusions in clutteredscenes.
arxiv-7800-120 | Empirical non-parametric estimation of the Fisher Information | http://arxiv.org/pdf/1408.1182v2.pdf | author:Visar Berisha, Alfred O. Hero category:stat.CO cs.IT math.IT stat.ML published:2014-08-06 summary:The Fisher information matrix (FIM) is a foundational concept in statisticalsignal processing. The FIM depends on the probability distribution, assumed tobelong to a smooth parametric family. Traditional approaches to estimating theFIM require estimating the probability distribution function (PDF), or itsparameters, along with its gradient or Hessian. However, in many practicalsituations the PDF of the data is not known but the statistician has access toan observation sample for any parameter value. Here we propose a method ofestimating the FIM directly from sampled data that does not require knowledgeof the underlying PDF. The method is based on non-parametric estimation of an$f$-divergence over a local neighborhood of the parameter space and a relationbetween curvature of the $f$-divergence and the FIM. Thus we obtain anempirical estimator of the FIM that does not require density estimation and isasymptotically consistent. We empirically evaluate the validity of our approachusing two experiments.
arxiv-7800-121 | Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing | http://arxiv.org/pdf/1408.1387v3.pdf | author:Nihar B. Shah, Dengyong Zhou category:cs.GT cs.HC cs.LG published:2014-08-06 summary:Crowdsourcing has gained immense popularity in machine learning applicationsfor obtaining large amounts of labeled data. Crowdsourcing is cheap and fast,but suffers from the problem of low-quality data. To address this fundamentalchallenge in crowdsourcing, we propose a simple payment mechanism toincentivize workers to answer only the questions that they are sure of and skipthe rest. We show that surprisingly, under a mild and natural "no-free-lunch"requirement, this mechanism is the one and only incentive-compatible paymentmechanism possible. We also show that among all possible incentive-compatiblemechanisms (that may or may not satisfy no-free-lunch), our mechanism makes thesmallest possible payment to spammers. We further extend our results to a moregeneral setting in which workers are required to provide a quantized confidencefor each question. Interestingly, this unique mechanism takes a"multiplicative" form. The simplicity of the mechanism is an added benefit. Inpreliminary experiments involving over 900 worker-task pairs, we observe asignificant drop in the error rates under this unique mechanism for the same orlower monetary expenditure.
arxiv-7800-122 | L0 Sparse Inverse Covariance Estimation | http://arxiv.org/pdf/1408.0850v5.pdf | author:Goran Marjanovic, Alfred O. Hero III category:stat.ML published:2014-08-05 summary:Recently, there has been focus on penalized log-likelihood covarianceestimation for sparse inverse covariance (precision) matrices. The penalty isresponsible for inducing sparsity, and a very common choice is the convex $l_1$norm. However, the best estimator performance is not always achieved with thispenalty. The most natural sparsity promoting "norm" is the non-convex $l_0$penalty but its lack of convexity has deterred its use in sparse maximumlikelihood estimation. In this paper we consider non-convex $l_0$ penalizedlog-likelihood inverse covariance estimation and present a novel cyclic descentalgorithm for its optimization. Convergence to a local minimizer is proved,which is highly non-trivial, and we demonstrate via simulations the reducedbias and superior quality of the $l_0$ penalty as compared to the $l_1$penalty.
arxiv-7800-123 | Open-set Person Re-identification | http://arxiv.org/pdf/1408.0872v2.pdf | author:Shengcai Liao, Zhipeng Mo, Jianqing Zhu, Yang Hu, Stan Z. Li category:cs.CV published:2014-08-05 summary:Person re-identification is becoming a hot research for developing bothmachine learning algorithms and video surveillance applications. The task ofperson re-identification is to determine which person in a gallery has the sameidentity to a probe image. This task basically assumes that the subject of theprobe image belongs to the gallery, that is, the gallery contains this person.However, in practical applications such as searching a suspect in a video, thisassumption is usually not true. In this paper, we consider the open-set personre-identification problem, which includes two sub-tasks, detection andidentification. The detection sub-task is to determine the presence of theprobe subject in the gallery, and the identification sub-task is to determinewhich person in the gallery has the same identity as the accepted probe. Wepresent a database collected from a video surveillance setting of 6 cameras,with 200 persons and 7,413 images segmented. Based on this database, we developa benchmark protocol for evaluating the performance under the open-set personre-identification scenario. Several popular metric learning algorithms forperson re-identification have been evaluated as baselines. From the baselineperformance, we observe that the open-set person re-identification problem isstill largely unresolved, thus further attention and effort is needed.
arxiv-7800-124 | Convex Biclustering | http://arxiv.org/pdf/1408.0856v4.pdf | author:Eric C. Chi, Genevera I. Allen, Richard G. Baraniuk category:stat.ME stat.ML published:2014-08-05 summary:In the biclustering problem, we seek to simultaneously group observations andfeatures. While biclustering has applications in a wide array of domains,ranging from text mining to collaborative filtering, the problem of identifyingstructure in high dimensional genomic data motivates this work. In thiscontext, biclustering enables us to identify subsets of genes that areco-expressed only within a subset of experimental conditions. We present aconvex formulation of the biclustering problem that possesses a unique globalminimizer and an iterative algorithm, COBRA, that is guaranteed to identify it.Our approach generates an entire solution path of possible biclusters as asingle tuning parameter is varied. We also show how to reduce the problem ofselecting this tuning parameter to solving a trivial modification of the convexbiclustering problem. The key contributions of our work are its simplicity,interpretability, and algorithmic guarantees - features that arguably arelacking in the current alternative algorithms. We demonstrate the advantages ofour approach, which includes stably and reproducibly identifying biclusterings,on simulated and real microarray data.
arxiv-7800-125 | Multilayer bootstrap networks | http://arxiv.org/pdf/1408.0848v6.pdf | author:Xiao-Lei Zhang category:cs.LG cs.NE stat.ML published:2014-08-05 summary:We describe a simple multilayer bootstrap network for unsuperviseddimensionality reduction that each layer of the network is a group of mutuallyindependent k-centers clusterings, and the centers of a clustering are randomlysampled data points. We further compress the network size of multilayerbootstrap network by a neural network in a pseudo supervised way forprediction. We report comparison results in data visualization, clustering, anddocument retrieval.
arxiv-7800-126 | Computing With Contextual Numbers | http://arxiv.org/pdf/1408.0889v2.pdf | author:Vahid Moosavi category:cs.CE cs.CV cs.NE published:2014-08-05 summary:Self Organizing Map (SOM) has been applied into several classical modelingtasks including clustering, classification, function approximation andvisualization of high dimensional spaces. The final products of a trained SOMare a set of ordered (low dimensional) indices and their associated highdimensional weight vectors. While in the above-mentioned applications, thefinal high dimensional weight vectors play the primary role in thecomputational steps, from a certain perspective, one can interpret SOM as anonparametric encoder, in which the final low dimensional indices of thetrained SOM are pointer to the high dimensional space. We showed how using aone-dimensional SOM, which is not common in usual applications of SOM, one candevelop a nonparametric mapping from a high dimensional space to a continuousone-dimensional numerical field. These numerical values, called contextualnumbers, are ordered in a way that in a given context, similar numbers refer tosimilar high dimensional states. Further, as these numbers can be treatedsimilarly to usual continuous numbers, they can be replaced with theircorresponding high dimensional states within any data driven modeling problem.As a potential application, we showed how using contextual numbers could beused for the problem of high dimensional spatiotemporal dynamics.
arxiv-7800-127 | Adaptive Learning in Cartesian Product of Reproducing Kernel Hilbert Spaces | http://arxiv.org/pdf/1408.0853v2.pdf | author:Masahiro Yukawa category:cs.LG stat.ML published:2014-08-05 summary:We propose a novel adaptive learning algorithm based on iterative orthogonalprojections in the Cartesian product of multiple reproducing kernel Hilbertspaces (RKHSs). The task is estimating/tracking nonlinear functions which aresupposed to contain multiple components such as (i) linear and nonlinearcomponents, (ii) high- and low- frequency components etc. In this case, the useof multiple RKHSs permits a compact representation of multicomponent functions.The proposed algorithm is where two different methods of the author meet:multikernel adaptive filtering and the algorithm of hyperplane projection alongaffine subspace (HYPASS). In a certain particular case, the sum space of theRKHSs is isomorphic to the product space and hence the proposed algorithm canalso be regarded as an iterative projection method in the sum space. Theefficacy of the proposed algorithm is shown by numerical examples.
arxiv-7800-128 | It is hard to see a needle in a haystack: Modeling contrast masking effect in a numerical observer | http://arxiv.org/pdf/1408.1135v1.pdf | author:Ali R. N. Avanaki, Kathryn S. Espig, Albert Xthona, Tom R. L. Kimpe, Predrag R. Bakic, Andrew D. A. Maidment category:cs.CV published:2014-08-05 summary:Within the framework of a virtual clinical trial for breast imaging, we aimto develop numerical observers that follow the same detection performancetrends as those of a typical human observer. In our prior work, we showed thatby including spatiotemporal contrast sensitivity function (stCSF) of humanvisual system (HVS) in a multi-slice channelized Hotelling observer (msCHO), wecan correctly predict trends of a typical human observer performance with theviewing parameters of browsing speed, viewing distance and contrast. In thiswork we further improve our numerical observer by modeling contrast masking.After stCSF, contrast masking is the second most prominent property of HVS andit refers to the fact that the presence of one signal affects the visibilitythreshold for another signal. Our results indicate that the improved numericalobserver better predicts changes in detection performance with backgroundcomplexity.
arxiv-7800-129 | Determining the Number of Clusters via Iterative Consensus Clustering | http://arxiv.org/pdf/1408.0967v1.pdf | author:Shaina Race, Carl Meyer, Kevin Valakuzhy category:stat.ML cs.CV cs.LG published:2014-08-05 summary:We use a cluster ensemble to determine the number of clusters, k, in a groupof data. A consensus similarity matrix is formed from the ensemble usingmultiple algorithms and several values for k. A random walk is induced on thegraph defined by the consensus matrix and the eigenvalues of the associatedtransition probability matrix are used to determine the number of clusters. Fornoisy or high-dimensional data, an iterative technique is presented to refinethis consensus matrix in way that encourages a block-diagonal form. It is shownthat the resulting consensus matrix is generally superior to existingsimilarity matrices for this type of spectral analysis.
arxiv-7800-130 | Machine learning for many-body physics: The case of the Anderson impurity model | http://arxiv.org/pdf/1408.1143v2.pdf | author:Louis-FranÃ§ois Arsenault, Alejandro Lopez-Bezanilla, O. Anatole von Lilienfeld, Andrew J. Millis category:stat.ML published:2014-08-05 summary:Machine learning methods are applied to finding the Green's function of theAnderson impurity model, a basic model system of quantum many-bodycondensed-matter physics. Different methods of parametrizing the Green'sfunction are investigated; a representation in terms of Legendre polynomials isfound to be superior due to its limited number of coefficients and itsapplicability to state of the art methods of solution. The dependence of theerrors on the size of the training set is determined. The results indicate thata machine learning approach to dynamical mean-field theory may be feasible.
arxiv-7800-131 | A Flexible Iterative Framework for Consensus Clustering | http://arxiv.org/pdf/1408.0972v1.pdf | author:Shaina Race, Carl Meyer category:stat.ML cs.CV cs.LG published:2014-08-05 summary:A novel framework for consensus clustering is presented which has the abilityto determine both the number of clusters and a final solution using multiplealgorithms. A consensus similarity matrix is formed from an ensemble usingmultiple algorithms and several values for k. A variety of dimension reductiontechniques and clustering algorithms are considered for analysis. For noisy orhigh-dimensional data, an iterative technique is presented to refine thisconsensus matrix in way that encourages algorithms to agree upon a commonsolution. We utilize the theory of nearly uncoupled Markov chains to determinethe number, k , of clusters in a dataset by considering a random walk on thegraph defined by the consensus matrix. The eigenvalues of the associatedtransition probability matrix are used to determine the number of clusters.This method succeeds at determining the number of clusters in many datasetswhere previous methods fail. On every considered dataset, our consensus methodprovides a final result with accuracy well above the average of the individualalgorithms.
arxiv-7800-132 | The Case for a Mixed-Initiative Collaborative Neuroevolution Approach | http://arxiv.org/pdf/1408.0998v1.pdf | author:Sebastian Risi, Jinhong Zhang, Rasmus Taarnby, Peter Greve, Jan Piskur, Antonios Liapis, Julian Togelius category:cs.NE published:2014-08-05 summary:It is clear that the current attempts at using algorithms to createartificial neural networks have had mixed success at best when it comes tocreating large networks and/or complex behavior. This should not be unexpected,as creating an artificial brain is essentially a design problem. Human designingenuity still surpasses computational design for most tasks in most domains,including architecture, game design, and authoring literary fiction. This leadsus to ask which the best way is to combine human and machine design capacitieswhen it comes to designing artificial brains. Both of them have their strengthsand weaknesses; for example, humans are much too slow to manually specifythousands of neurons, let alone the billions of neurons that go into a humanbrain, but on the other hand they can rely on a vast repository of common-senseunderstanding and design heuristics that can help them perform a much betterguided search in design space than an algorithm. Therefore, in this paper weargue for a mixed-initiative approach for collaborative online brain buildingand present first results towards this goal.
arxiv-7800-133 | Speech earthquakes: scaling and universality in human voice | http://arxiv.org/pdf/1408.0985v1.pdf | author:Jordi Luque, Bartolo Luque, Lucas Lacasa category:physics.soc-ph cs.CL q-bio.NC published:2014-08-05 summary:Speech is a distinctive complex feature of human capabilities. In order tounderstand the physics underlying speech production, in this work weempirically analyse the statistics of large human speech datasets rangingseveral languages. We first show that during speech the energy is unevenlyreleased and power-law distributed, reporting a universal robustGutenberg-Richter-like law in speech. We further show that such earthquakes inspeech show temporal correlations, as the interevent statistics are againpower-law distributed. Since this feature takes place in the intra-phonemerange, we conjecture that the responsible for this complex phenomenon is notcognitive, but it resides on the physiological speech production mechanism.Moreover, we show that these waiting time distributions are scale invariantunder a renormalisation group transformation, suggesting that the process ofspeech generation is indeed operating close to a critical point. These resultsare put in contrast with current paradigms in speech processing, which pointtowards low dimensional deterministic chaos as the origin of nonlinear traitsin speech fluctuations. As these latter fluctuations are indeed the aspectsthat humanize synthetic speech, these findings may have an impact in futurespeech synthesis technologies. Results are robust and independent of thecommunication language or the number of speakers, pointing towards an universalpattern and yet another hint of complexity in human speech.
arxiv-7800-134 | Volumes of logistic regression models with applications to model selection | http://arxiv.org/pdf/1408.0881v3.pdf | author:James G. Dowty category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH published:2014-08-05 summary:Logistic regression models with $n$ observations and $q$ linearly-independentcovariates are shown to have Fisher information volumes which are bounded belowby $\pi^q$ and above by ${n \choose q} \pi^q$. This is proved with a novelgeneralization of the classical theorems of Pythagoras and de Gua, which is ofindependent interest. The finding that the volume is always finite is new, andit implies that the volume can be directly interpreted as a measure of modelcomplexity. The volume is shown to be a continuous function of the designmatrix $X$ at generic $X$, but to be discontinuous in general. This means thatmodels with sparse design matrices can be significantly less complex thannearby models, so the resulting model-selection criterion prefers sparsemodels. This is analogous to the way that $\ell^1$-regularisation tends toprefer sparse model fits, though in our case this behaviour arisesspontaneously from general principles. Lastly, an unusual topological dualityis shown to exist between the ideal boundaries of the natural and expectationparameter spaces of logistic regression models.
arxiv-7800-135 | Real-Time Traffic Signal Control for Modern Roundabouts by Using Particle Swarm Optimization-Based Fuzzy Controller | http://arxiv.org/pdf/1408.0689v2.pdf | author:Yue-Jiao Gong, Jun Zhang category:cs.NE published:2014-08-04 summary:Due to that the existing traffic facilities can hardly be extended,developing traffic signal control methods is the most important way to improvethe traffic efficiency of modern roundabouts. This paper proposes a noveltraffic signal controller with two fuzzy layers for signalizing the roundabout.The outer layer of the controller computes urgency degrees of all the phasesubsets and then activates the most urgent subset. This mechanism helps toinstantly respond to the current traffic condition of the roundabout so as toimprove real-timeness. The inner layer of the controller computes extensiontime of the current phase. If the extension value is larger than a thresholdvalue, the current phase is maintained; otherwise the next phase in the runningphase subset (selected by the outer layer) is activated. The inner layer adoptswell-designed phase sequences, which helps to smooth the traffic flows and toavoid traffic jam. In general, the proposed traffic signal controller iscapable of improving real-timeness as well as reducing traffic congestion.Moreover, an offline particle swarm optimization (PSO) algorithm is developedto optimize the membership functions adopted in the proposed controller. Byusing optimal membership functions, the performance of the controller can befurther improved. Simulation results demonstrate that the proposed controlleroutperforms previous traffic signal controllers in terms of improving thetraffic efficiency of modern roundabouts.
arxiv-7800-136 | Object Detection Through Exploration With A Foveated Visual Field | http://arxiv.org/pdf/1408.0814v1.pdf | author:Emre Akbas, Miguel P. Eckstein category:cs.CV published:2014-08-04 summary:We present a foveated object detector (FOD) as a biologically-inspiredalternative to the sliding window (SW) approach which is the dominant method ofsearch in computer vision object detection. Similar to the human visual system,the FOD has higher resolution at the fovea and lower resolution at the visualperiphery. Consequently, more computational resources are allocated at thefovea and relatively fewer at the periphery. The FOD processes the entirescene, uses retino-specific object detection classifiers to guide eyemovements, aligns its fovea with regions of interest in the input image andintegrates observations across multiple fixations. Our approach combines modernobject detectors from computer vision with a recent model of peripheral poolingregions found at the V1 layer of the human visual system. We assessed variouseye movement strategies on the PASCAL VOC 2007 dataset and show that the FODperforms on par with the SW detector while bringing significant computationalcost savings.
arxiv-7800-137 | Estimating Maximally Probable Constrained Relations by Mathematical Programming | http://arxiv.org/pdf/1408.0838v1.pdf | author:Lizhen Qu, Bjoern Andres category:cs.LG cs.NA math.OC stat.ML published:2014-08-04 summary:Estimating a constrained relation is a fundamental problem in machinelearning. Special cases are classification (the problem of estimating a mapfrom a set of to-be-classified elements to a set of labels), clustering (theproblem of estimating an equivalence relation on a set) and ranking (theproblem of estimating a linear order on a set). We contribute a family ofprobability measures on the set of all relations between two finite, non-emptysets, which offers a joint abstraction of multi-label classification,correlation clustering and ranking by linear ordering. Estimating (learning) amaximally probable measure, given (a training set of) related and unrelatedpairs, is a convex optimization problem. Estimating (inferring) a maximallyprobable relation, given a measure, is a 01-linear program. It is solved inlinear time for maps. It is NP-hard for equivalence relations and linearorders. Practical solutions for all three cases are shown in experiments withreal data. Finally, estimating a maximally probable measure and relationjointly is posed as a mixed-integer nonlinear program. This formulationsuggests a mathematical programming approach to semi-supervised learning.
arxiv-7800-138 | Targetable Named Entity Recognition in Social Media | http://arxiv.org/pdf/1408.0782v1.pdf | author:Sandeep Ashwini, Jinho D. Choi category:cs.CL published:2014-08-04 summary:We present a novel approach for recognizing what we call targetable namedentities; that is, named entities in a targeted set (e.g, movies, books, TVshows). Unlike many other NER systems that need to retrain their statisticalmodels as new entities arrive, our approach does not require such retraining,which makes it more adaptable for types of entities that are frequentlyupdated. For this preliminary study, we focus on one entity type, movie title,using data collected from Twitter. Our system is tested on two evaluation sets,one including only entities corresponding to movies in our training set, andthe other excluding any of those entities. Our final model shows F1-scores of76.19% and 78.70% on these evaluation sets, which gives strong evidence thatour approach is completely unbiased to any par- ticular set of entities foundduring training.
arxiv-7800-139 | A Multi-Stage Supply Chain Network Optimization Using Genetic Algorithms | http://arxiv.org/pdf/1408.0614v1.pdf | author:Nelson Christopher Dzupire, Yaw Nkansah-Gyekye category:math.OC cs.NE published:2014-08-04 summary:In today's global business market place, individual firms no longer competeas independent entities with unique brand names but as integral part of supplychain links. Key to success of any business is satisfying customer's demands ontime which may result in cost reductions and increase in service level. Insupply chain networks decisions are made with uncertainty about product'sdemands, costs, prices, lead times, quality in a competitive and collaborativeenvironment. If poor decisions are made, they may lead to excess inventoriesthat are costly or to insufficient inventory that cannot meet customer'sdemands. In this work we developed a bi-objective model that minimizes systemwide costs of the supply chain and delays on delivery of products todistribution centers for a three echelon supply chain. Picking a set of Paretofront for multi-objective optimization problems require robust and efficientmethods that can search an entire space. We used evolutionary algorithms tofind the set of Pareto fronts which have proved to be effective in finding theentire set of Pareto fronts.
arxiv-7800-140 | Modulation Classification via Gibbs Sampling Based on a Latent Dirichlet Bayesian Network | http://arxiv.org/pdf/1408.0765v2.pdf | author:Yu Liu, Osvaldo Simeone, Alexander M. Haimovich, Wei Su category:cs.IT cs.CV math.IT published:2014-08-04 summary:A novel Bayesian modulation classification scheme is proposed for asingle-antenna system over frequency-selective fading channels. The method isbased on Gibbs sampling as applied to a latent Dirichlet Bayesian network (BN).The use of the proposed latent Dirichlet BN provides a systematic solution tothe convergence problem encountered by the conventional Gibbs sampling approachfor modulation classification. The method generalizes, and is shown to improveupon, the state of the art.
arxiv-7800-141 | A Pattern Recognition System for Detecting Use of Mobile Phones While Driving | http://arxiv.org/pdf/1408.0680v1.pdf | author:Rafael A. Berri, Alexandre G. Silva, Rafael S. Parpinelli, Elaine Girardi, Rangel Arthur category:cs.CV published:2014-08-04 summary:It is estimated that 80% of crashes and 65% of near collisions involveddrivers inattentive to traffic for three seconds before the event. This paperdevelops an algorithm for extracting characteristics allowing the cell phonesidentification used during driving a vehicle. Experiments were performed onsets of images with 100 positive images (with phone) and the other 100 negativeimages (no phone), containing frontal images of the driver. Support VectorMachine (SVM) with Polynomial kernel is the most advantageous classificationsystem to the features provided by the algorithm, obtaining a success rate of91.57% for the vision system. Tests done on videos show that it is possible touse the image datasets for training classifiers in real situations. Periods of3 seconds were correctly classified at 87.43% of cases.
arxiv-7800-142 | Multithreshold Entropy Linear Classifier | http://arxiv.org/pdf/1408.1054v1.pdf | author:Wojciech Marian Czarnecki, Jacek Tabor category:cs.LG stat.ML published:2014-08-04 summary:Linear classifiers separate the data with a hyperplane. In this paper wefocus on the novel method of construction of multithreshold linear classifier,which separates the data with multiple parallel hyperplanes. Proposed model isbased on the information theory concepts -- namely Renyi's quadratic entropyand Cauchy-Schwarz divergence. We begin with some general properties, including data scale invariance. Thenwe prove that our method is a multithreshold large margin classifier, whichshows the analogy to the SVM, while in the same time works with much broaderclass of hypotheses. What is also interesting, proposed method is aimed at themaximization of the balanced quality measure (such as Matthew's CorrelationCoefficient) as opposed to very common maximization of the accuracy. Thisfeature comes directly from the optimization problem statement and is furtherconfirmed by the experiments on the UCI datasets. It appears, that our Multithreshold Entropy Linear Classifier (MELC) obtainessimilar or higher scores than the ones given by SVM on both synthetic and realdata. We show how proposed approach can be benefitial for the cheminformaticsin the task of ligands activity prediction, where despite better classificationresults, MELC gives some additional insight into the data structure (classes ofunderrepresented chemical compunds).
arxiv-7800-143 | Sample Complexity Analysis for Learning Overcomplete Latent Variable Models through Tensor Methods | http://arxiv.org/pdf/1408.0553v2.pdf | author:Animashree Anandkumar, Rong Ge, Majid Janzamin category:cs.LG math.PR stat.ML published:2014-08-03 summary:We provide guarantees for learning latent variable models emphasizing on theovercomplete regime, where the dimensionality of the latent space can exceedthe observed dimensionality. In particular, we consider multiview mixtures,spherical Gaussian mixtures, ICA, and sparse coding models. We provide tightconcentration bounds for empirical moments through novel covering arguments. Weanalyze parameter recovery through a simple tensor power update algorithm. Inthe semi-supervised setting, we exploit the label or prior information to get arough estimate of the model parameters, and then refine it using the tensormethod on unlabeled samples. We establish that learning is possible when thenumber of components scales as $k=o(d^{p/2})$, where $d$ is the observeddimension, and $p$ is the order of the observed moment employed in the tensormethod. Our concentration bound analysis also leads to minimax samplecomplexity for semi-supervised learning of spherical Gaussian mixtures. In theunsupervised setting, we use a simple initialization algorithm based on SVD ofthe tensor slices, and provide guarantees under the stricter condition that$k\le \beta d$ (where constant $\beta$ can be larger than $1$), where thetensor method recovers the components under a polynomial running time (andexponential in $\beta$). Our analysis establishes that a wide range ofovercomplete latent variable models can be learned efficiently with lowcomputational and sample complexity through tensor decomposition methods.
arxiv-7800-144 | Adaptive Wavelet Based Identification and Extraction of PQRST Combination in Randomly Stretching ECG Sequence | http://arxiv.org/pdf/1408.0453v1.pdf | author:T. R. Gopalakrishnan Nair, A. P. Geetha, M. Asharani category:cs.CV published:2014-08-03 summary:Cardiovascular system study using ECG signals have evolved tremendously inthe domain of electronics and signal processing. However, there are certainfloating challenges unresolved in the analysis and detection of abnormalperformances of cardiovascular system. As the medical field is moving towardsmore automated and intelligent systems, wrong detection or wronginterpretations of ECG waveform of abnormal conditions can be quite fatal.Since the PQRST signals vary their positions randomly, the process of locating,identifying and classifying each feature can be cumbersome and it is prone toerrors. Here we present an automated scheme using adaptive wavelet to detectprominent R-peak with extreme accuracy and algorithmically tag and mark thecoexisting peaks P, Q, S, and T with almost same accuracy. The adaptive waveletapproach used in this scheme is capable of detecting R-peak in ECG with 99.99%accuracy along with the rest of the waveforms.
arxiv-7800-145 | Methodology For Detection of QRS Pattern Using Secondary Wavelets | http://arxiv.org/pdf/1408.0452v1.pdf | author:T. R. Gopalakrishnan Nair, A. P. Geetha, Asharani category:cs.CV published:2014-08-03 summary:Applications of wavelet transform to the field of health care signals havepaved the way for implementing revolutionary approaches in detecting thepresence of certain abnormalities in human health patterns. There wereextensive studies carried out using primary wavelets in various signals likeElectrocardiogram (ECG), sonogram etc. with a certain amount of success. On theother hand analysis using secondary wavelets which inherits the characteristicsof a set of variations available in signals like ECG can be a promise to detectdiseases with ease. Here a method to create a generalized adapted wavelet ispresented which contains the information of QRS pattern collected from ananomaly sample space. The method has been tested and found to be successful inlocating the position of R peak in noise embedded ECG signal.
arxiv-7800-146 | A Bayesian estimation approach to analyze non-Gaussian data-generating processes with latent classes | http://arxiv.org/pdf/1408.0337v1.pdf | author:Naoki Tanaka, Shohei Shimizu, Takashi Washio category:stat.ML published:2014-08-02 summary:A large amount of observational data has been accumulated in various fieldsin recent times, and there is a growing need to estimate the generatingprocesses of these data. A linear non-Gaussian acyclic model (LiNGAM) based onthe non-Gaussianity of external influences has been proposed to estimate thedata-generating processes of variables. However, the results of the estimationcan be biased if there are latent classes. In this paper, we first reviewLiNGAM, its extended model, as well as the estimation procedure for LiNGAM in aBayesian framework. We then propose a new Bayesian estimation procedure thatsolves the problem.
arxiv-7800-147 | Matrix Factorization with Explicit Trust and Distrust Relationships | http://arxiv.org/pdf/1408.0325v1.pdf | author:Rana Forsati, Mehrdad Mahdavi, Mehrnoush Shamsfard, Mohamed Sarwat category:cs.SI cs.IR cs.LG published:2014-08-02 summary:With the advent of online social networks, recommender systems have becamecrucial for the success of many online applications/services due to theirsignificance role in tailoring these applications to user-specific needs orpreferences. Despite their increasing popularity, in general recommendersystems suffer from the data sparsity and the cold-start problems. To alleviatethese issues, in recent years there has been an upsurge of interest inexploiting social information such as trust relations among users along withthe rating data to improve the performance of recommender systems. The mainmotivation for exploiting trust information in recommendation process stemsfrom the observation that the ideas we are exposed to and the choices we makeare significantly influenced by our social context. However, in large usercommunities, in addition to trust relations, the distrust relations also existbetween users. For instance, in Epinions the concepts of personal "web oftrust" and personal "block list" allow users to categorize their friends basedon the quality of reviews into trusted and distrusted friends, respectively. Inthis paper, we propose a matrix factorization based model for recommendation insocial rating networks that properly incorporates both trust and distrustrelationships aiming to improve the quality of recommendations and mitigate thedata sparsity and the cold-start users issues. Through experiments on theEpinions data set, we show that our new algorithm outperforms its standardtrust-enhanced or distrust-enhanced counterparts with respect to accuracy,thereby demonstrating the positive effect that incorporation of explicitdistrust information can have on recommender systems.
arxiv-7800-148 | Estimating Renyi Entropy of Discrete Distributions | http://arxiv.org/pdf/1408.1000v3.pdf | author:Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, Himanshu Tyagi category:cs.IT cs.DS cs.LG math.IT published:2014-08-02 summary:It was recently shown that estimating the Shannon entropy $H({\rm p})$ of adiscrete $k$-symbol distribution ${\rm p}$ requires $\Theta(k/\log k)$ samples,a number that grows near-linearly in the support size. In many applications$H({\rm p})$ can be replaced by the more general R\'enyi entropy of order$\alpha$, $H_\alpha({\rm p})$. We determine the number of samples needed toestimate $H_\alpha({\rm p})$ for all $\alpha$, showing that $\alpha < 1$requires a super-linear, roughly $k^{1/\alpha}$ samples, noninteger $\alpha>1$requires a near-linear $k$ samples, but, perhaps surprisingly, integer$\alpha>1$ requires only $\Theta(k^{1-1/\alpha})$ samples. Furthermore,developing on a recently established connection between polynomialapproximation and estimation of additive functions of the form $\sum_{x} f({\rmp}_x)$, we reduce the sample complexity for noninteger values of $\alpha$ by afactor of $\log k$ compared to the empirical estimator. The estimatorsachieving these bounds are simple and run in time linear in the number ofsamples. Our lower bounds provide explicit constructions of distributions withdifferent R\'enyi entropies that are hard to distinguish.
arxiv-7800-149 | A Framework for learning multi-agent dynamic formation strategy in real-time applications | http://arxiv.org/pdf/1408.0058v1.pdf | author:Mehrab Norouzitallab, Valiallah Monajjemi, Saeed Shiry Ghidary, Mohammad Bagher Menhaj category:cs.RO cs.LG cs.MA published:2014-08-01 summary:Formation strategy is one of the most important parts of many multi-agentsystems with many applications in real world problems. In this paper, aframework for learning this task in a limited domain (restricted environment)is proposed. In this framework, agents learn either directly by observing anexpert behavior or indirectly by observing other agents or objects behavior.First, a group of algorithms for learning formation strategy based on limitedfeatures will be presented. Due to distributed and complex nature of manymulti-agent systems, it is impossible to include all features directly in thelearning process; thus, a modular scheme is proposed in order to reduce thenumber of features. In this method, some important features have indirectinfluence in learning instead of directly involving them as input features.This framework has the ability to dynamically assign a group of positions to agroup of agents to improve system performance. In addition, it can change theformation strategy when the context changes. Finally, this framework is able toautomatically produce many complex and flexible formation strategy algorithmswithout directly involving an expert to present and implement such complexalgorithms.
arxiv-7800-150 | Conditional Restricted Boltzmann Machines for Cold Start Recommendations | http://arxiv.org/pdf/1408.0096v1.pdf | author:Jiankou Li, Wei Zhang category:cs.IR cs.LG stat.ML published:2014-08-01 summary:Restricted Boltzman Machines (RBMs) have been successfully used inrecommender systems. However, as with most of other collaborative filteringtechniques, it cannot solve cold start problems for there is no rating for anew item. In this paper, we first apply conditional RBM (CRBM) which could takeextra information into account and show that CRBM could solve cold startproblem very well, especially for rating prediction task. CRBM naturallycombine the content and collaborative data under a single framework which couldbe fitted effectively. Experiments show that CRBM can be compared favourablywith matrix factorization models, while hidden features learned from the formermodels are more easy to be interpreted.
arxiv-7800-151 | A RobustICA Based Algorithm for Blind Separation of Convolutive Mixtures | http://arxiv.org/pdf/1408.0193v1.pdf | author:Zaid Albataineh, Fathi M. Salem category:cs.LG cs.SD published:2014-08-01 summary:We propose a frequency domain method based on robust independent componentanalysis (RICA) to address the multichannel Blind Source Separation (BSS)problem of convolutive speech mixtures in highly reverberant environments. Weimpose regularization processes to tackle the ill-conditioning problem of thecovariance matrix and to mitigate the performance degradation in the frequencydomain. We apply an algorithm to separate the source signals in adverseconditions, i.e. high reverberation conditions when short observation signalsare available. Furthermore, we study the impact of several parameters on theperformance of separation, e.g. overlapping ratio and window type of thefrequency domain method. We also compare different techniques to solve thefrequency-domain permutation ambiguity. Through simulations and real worldexperiments, we verify the superiority of the presented convolutive algorithmamong other BSS algorithms, including recursive regularized ICA (RR ICA),independent vector analysis (IVA).
arxiv-7800-152 | A Blind Adaptive CDMA Receiver Based on State Space Structures | http://arxiv.org/pdf/1408.0196v2.pdf | author:Zaid Albataineh, Fathi M. Salem category:cs.IT cs.LG math.IT published:2014-08-01 summary:Code Division Multiple Access (CDMA) is a channel access method, based onspread-spectrum technology, used by various radio technologies world-wide. Ingeneral, CDMA is used as an access method in many mobile standards such asCDMA2000 and WCDMA. We address the problem of blind multiuser equalization inthe wideband CDMA system, in the noisy multipath propagation environment.Herein, we propose three new blind receiver schemes, which are based on statespace structures and Independent Component Analysis (ICA). These blindstate-space receivers (BSSR) do not require knowledge of the propagationparameters or spreading code sequences of the users they primarily exploit thenatural assumption of statistical independence among the source signals. Wealso develop three semi blind adaptive detectors by incorporating the newadaptive methods into the standard RAKE receiver structure. Extensivecomparative case study, based on Bit error rate (BER) performance of thesemethods, is carried out for different number of users, symbols per user, andsignal to noise ratio (SNR) in comparison with conventional detectors,including the Blind Multiuser Detectors (BMUD) and Linear Minimum mean squarederror (LMMSE). The results show that the proposed methods outperform the otherdetectors in estimating the symbol signals from the received mixed CDMAsignals. Moreover, the new blind detectors mitigate the multi accessinterference (MAI) in CDMA.
arxiv-7800-153 | Functional Principal Component Analysis and Randomized Sparse Clustering Algorithm for Medical Image Analysis | http://arxiv.org/pdf/1408.0204v1.pdf | author:Nan Lin, Junhai Jiang, Shicheng Guo, Momiao Xiong category:stat.ML cs.AI cs.CV cs.LG published:2014-08-01 summary:Due to advances in sensors, growing large and complex medical image data havethe ability to visualize the pathological change in the cellular or even themolecular level or anatomical changes in tissues and organs. As a consequence,the medical images have the potential to enhance diagnosis of disease,prediction of clinical outcomes, characterization of disease progression,management of health care and development of treatments, but also pose greatmethodological and computational challenges for representation and selection offeatures in image cluster analysis. To address these challenges, we firstextend one dimensional functional principal component analysis to the twodimensional functional principle component analyses (2DFPCA) to fully capturespace variation of image signals. Image signals contain a large number ofredundant and irrelevant features which provide no additional or no usefulinformation for cluster analysis. Widely used methods for removing redundantand irrelevant features are sparse clustering algorithms using a lasso-typepenalty to select the features. However, the accuracy of clustering using alasso-type penalty depends on how to select penalty parameters and a thresholdfor selecting features. In practice, they are difficult to determine. Recently,randomized algorithms have received a great deal of attention in big dataanalysis. This paper presents a randomized algorithm for accurate featureselection in image cluster analysis. The proposed method is applied to ovarianand kidney cancer histology image data from the TCGA database. The resultsdemonstrate that the randomized feature selection method coupled withfunctional principal component analysis substantially outperforms the currentsparse clustering algorithms in image cluster analysis.
arxiv-7800-154 | Memetic Search in Differential Evolution Algorithm | http://arxiv.org/pdf/1408.0101v1.pdf | author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE published:2014-08-01 summary:Differential Evolution (DE) is a renowned optimization stratagem that caneasily solve nonlinear and comprehensive problems. DE is a well known anduncomplicated population based probabilistic approach for comprehensiveoptimization. It has apparently outperformed a number of EvolutionaryAlgorithms and further search heuristics in the vein of Particle SwarmOptimization at what time of testing over both yardstick and actual worldproblems. Nevertheless, DE, like other probabilistic optimization algorithms,from time to time exhibits precipitate convergence and stagnates at suboptimalposition. In order to stay away from stagnation behavior while maintaining anexcellent convergence speed, an innovative search strategy is introduced, namedmemetic search in DE. In the planned strategy, positions update equationcustomized as per a memetic search stratagem. In this strategy a bettersolution participates more times in the position modernize procedure. Theposition update equation is inspired from the memetic search in artificial beecolony algorithm. The proposed strategy is named as Memetic Search inDifferential Evolution (MSDE). To prove efficiency and efficacy of MSDE, it istested over 8 benchmark optimization problems and three real world optimizationproblems. A comparative analysis has also been carried out among proposed MSDEand original DE. Results show that the anticipated algorithm go one better thanthe basic DE and its recent deviations in a good number of the experiments.
arxiv-7800-155 | Text to Multi-level MindMaps: A Novel Method for Hierarchical Visual Abstraction of Natural Language Text | http://arxiv.org/pdf/1408.1031v2.pdf | author:Mohamed Elhoseiny, Ahmed Elgammal category:cs.CL cs.HC published:2014-08-01 summary:MindMapping is a well-known technique used in note taking, which encourageslearning and studying. MindMapping has been manually adopted to help presentknowledge and concepts in a visual form. Unfortunately, there is no reliableautomated approach to generate MindMaps from Natural Language text. This workfirstly introduces MindMap Multilevel Visualization concept which is to jointlyvisualize and summarize textual information. The visualization is achievedpictorially across multiple levels using semantic information (i.e. ontology),while the summarization is achieved by the information in the highest levels asthey represent abstract information in the text. This work also presents thefirst automated approach that takes a text input and generates a MindMapvisualization out of it. The approach could visualize text documents inmultilevel MindMaps, in which a high-level MindMap node could be expanded intochild MindMaps. \ignore{ As far as we know, this is the first work that viewMindMapping as a new approach to jointly summarize and visualize textualinformation.} The proposed method involves understanding of the input text andconverting it into intermediate Detailed Meaning Representation (DMR). The DMRis then visualized with two modes; Single level or Multiple levels, which isconvenient for larger text. The generated MindMaps from both approaches wereevaluated based on Human Subject experiments performed on Amazon MechanicalTurk with various parameter settings.
arxiv-7800-156 | Thurstonian Boltzmann Machines: Learning from Multiple Inequalities | http://arxiv.org/pdf/1408.0055v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG stat.ME published:2014-08-01 summary:We introduce Thurstonian Boltzmann Machines (TBM), a unified architecturethat can naturally incorporate a wide range of data inputs at the same time.Our motivation rests in the Thurstonian view that many discrete data types canbe considered as being generated from a subset of underlying latent continuousvariables, and in the observation that each realisation of a discrete typeimposes certain inequalities on those variables. Thus learning and inference inTBM reduce to making sense of a set of inequalities. Our proposed TBM naturallysupports the following types: Gaussian, intervals, censored, binary,categorical, muticategorical, ordinal, (in)-complete rank with and withoutties. We demonstrate the versatility and capacity of the proposed model onthree applications of very different natures; namely handwritten digitrecognition, collaborative filtering and complex social survey analysis.
arxiv-7800-157 | Randomized Memetic Artificial Bee Colony Algorithm | http://arxiv.org/pdf/1408.0102v1.pdf | author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE published:2014-08-01 summary:Artificial Bee Colony (ABC) optimization algorithm is one of the recentpopulation based probabilistic approach developed for global optimization. ABCis simple and has been showed significant improvement over other NatureInspired Algorithms (NIAs) when tested over some standard benchmark functionsand for some complex real world optimization problems. Memetic Algorithms alsobecome one of the key methodologies to solve the very large and complexreal-world optimization problems. The solution search equation of Memetic ABCis based on Golden Section Search and an arbitrary value which tries to balanceexploration and exploitation of search space. But still there are some chancesto skip the exact solution due to its step size. In order to balance betweendiversification and intensification capability of the Memetic ABC, it israndomized the step size in Memetic ABC. The proposed algorithm is named asRandomized Memetic ABC (RMABC). In RMABC, new solutions are generated nearbythe best so far solution and it helps to increase the exploitation capabilityof Memetic ABC. The experiments on some test problems of different complexitiesand one well known engineering optimization application show that the proposedalgorithm outperforms over Memetic ABC (MeABC) and some other variant of ABCalgorithm(like Gbest guided ABC (GABC),Hooke Jeeves ABC (HJABC), Best-So-FarABC (BSFABC) and Modified ABC (MABC) in case of almost all the problems.
arxiv-7800-158 | Variational Depth from Focus Reconstruction | http://arxiv.org/pdf/1408.0173v2.pdf | author:Michael Moeller, Martin Benning, Carola SchÃ¶nlieb, Daniel Cremers category:cs.CV math.OC published:2014-08-01 summary:This paper deals with the problem of reconstructing a depth map from asequence of differently focused images, also known as depth from focus or shapefrom focus. We propose to state the depth from focus problem as a variationalproblem including a smooth but nonconvex data fidelity term, and a convexnonsmooth regularization, which makes the method robust to noise and leads tomore realistic depth maps. Additionally, we propose to solve the nonconvexminimization problem with a linearized alternating directions method ofmultipliers (ADMM), allowing to minimize the energy very efficiently. Anumerical comparison to classical methods on simulated as well as on real datais presented.
arxiv-7800-159 | A convergence and asymptotic analysis of the generalized symmetric FastICA algorithm | http://arxiv.org/pdf/1408.0145v2.pdf | author:Tianwen Wei category:stat.ML published:2014-08-01 summary:This contribution deals with the generalized symmetric FastICA algorithm inthe domain of Independent Component Analysis (ICA). The generalized symmetricversion of FastICA has been shown to have the potential to achieve theCram\'er-Rao Bound (CRB) by allowing the usage of different nonlinearityfunctions in its parallel implementations of one-unit FastICA. In spite of thisappealing property, a rigorous study of the asymptotic error of the generalizedsymmetric FastICA algorithm is still missing in the community. In fact, all theexisting results exhibit certain limitations, such as ignoring the impact ofdata standardization on the asymptotic statistics or being based on a heuristicapproach. In this work, we aim at filling this blank. The first result of this contribution is the characterization of the limitsof the generalized symmetric FastICA. It is shown that the algorithm optimizesa function that is a sum of the contrast functions used by traditional one-unitFastICA with a correction of the sign. Based on this characterization, wederive a closed-form analytic expression of the asymptotic covariance matrix ofthe generalized symmetric FastICA estimator using the method of estimatingequation and M-estimator.
arxiv-7800-160 | Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms | http://arxiv.org/pdf/1407.8339v6.pdf | author:Wei Chen, Yajun Wang, Yang Yuan, Qinshi Wang category:cs.LG published:2014-07-31 summary:We define a general framework for a large class of combinatorial multi-armedbandit (CMAB) problems, where subsets of base arms with unknown distributionsform super arms. In each round, a super arm is played and the base armscontained in the super arm are played and their outcomes are observed. Wefurther consider the extension in which more based arms could beprobabilistically triggered based on the outcomes of already triggered arms.The reward of the super arm depends on the outcomes of all played arms, and itonly needs to satisfy two mild assumptions, which allow a large class ofnonlinear reward instances. We assume the availability of an offline(\alpha,\beta)-approximation oracle that takes the means of the outcomedistributions of arms and outputs a super arm that with probability {\beta}generates an {\alpha} fraction of the optimal expected reward. The objective ofan online learning algorithm for CMAB is to minimize(\alpha,\beta)-approximation regret, which is the difference between the\alpha{\beta} fraction of the expected reward when always playing the optimalsuper arm, and the expected reward of playing super arms according to thealgorithm. We provide CUCB algorithm that achieves O(log n)distribution-dependent regret, where n is the number of rounds played, and wefurther provide distribution-independent bounds for a large class of rewardfunctions. Our regret analysis is tight in that it matches the bound of UCB1algorithm (up to a constant factor) for the classical MAB problem, and itsignificantly improves the regret bound in a earlier paper on combinatorialbandits with linear rewards. We apply our CMAB framework to two newapplications, probabilistic maximum coverage and social influence maximization,both having nonlinear reward structures. In particular, application to socialinfluence maximization requires our extension on probabilistically triggeredarms.
arxiv-7800-161 | Zipf's law for word frequencies: word forms versus lemmas in long texts | http://arxiv.org/pdf/1407.8322v2.pdf | author:Alvaro Corral, Gemma Boleda, Ramon Ferrer-i-Cancho category:physics.soc-ph cs.CL published:2014-07-31 summary:Zipf's law is a fundamental paradigm in the statistics of written and spokennatural language as well as in other communication systems. We raise thequestion of the elementary units for which Zipf's law should hold in the mostnatural way, studying its validity for plain word forms and for thecorresponding lemma forms. In order to have as homogeneous sources as possible,we analyze some of the longest literary texts ever written, comprising fourdifferent languages, with different levels of morphological complexity. In allcases Zipf's law is fulfilled, in the sense that a power-law distribution ofword or lemma frequencies is valid for several orders of magnitude. Weinvestigate the extent to which the word-lemma transformation preserves twoparameters of Zipf's law: the exponent and the low-frequency cut-off. We arenot able to demonstrate a strict invariance of the tail, as for a few textsboth exponents deviate significantly, but we conclude that the exponents arevery similar, despite the remarkable transformation that going from words tolemmas represents, considerably affecting all ranges of frequencies. Incontrast, the low-frequency cut-offs are less stable.
arxiv-7800-162 | DuSK: A Dual Structure-preserving Kernel for Supervised Tensor Learning with Applications to Neuroimages | http://arxiv.org/pdf/1407.8289v2.pdf | author:Lifang He, Xiangnan Kong, Philip S. Yu, Ann B. Ragin, Zhifeng Hao, Xiaowei Yang category:cs.LG published:2014-07-31 summary:With advances in data collection technologies, tensor data is assumingincreasing prominence in many applications and the problem of supervised tensorlearning has emerged as a topic of critical significance in the data mining andmachine learning community. Conventional methods for supervised tensor learningmainly focus on learning kernels by flattening the tensor into vectors ormatrices, however structural information within the tensors will be lost. Inthis paper, we introduce a new scheme to design structure-preserving kernelsfor supervised tensor learning. Specifically, we demonstrate how to leveragethe naturally available structure within the tensorial representation to encodeprior knowledge in the kernel. We proposed a tensor kernel that can preservetensor structures based upon dual-tensorial mapping. The dual-tensorial mappingfunction can map each tensor instance in the input space to another tensor inthe feature space while preserving the tensorial structure. Theoretically, ourapproach is an extension of the conventional kernels in the vector space totensor space. We applied our novel kernel in conjunction with SVM to real-worldtensor classification problems including brain fMRI classification for threedifferent diseases (i.e., Alzheimer's disease, ADHD and brain damage by HIV).Extensive empirical studies demonstrate that our proposed approach caneffectively boost tensor classification performances, particularly with smallsample sizes.
arxiv-7800-163 | Cumulative Restricted Boltzmann Machines for Ordinal Matrix Data Analysis | http://arxiv.org/pdf/1408.0047v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.IR cs.LG stat.AP stat.ME published:2014-07-31 summary:Ordinal data is omnipresent in almost all multiuser-generated feedback -questionnaires, preferences etc. This paper investigates modelling of ordinaldata with Gaussian restricted Boltzmann machines (RBMs). In particular, wepresent the model architecture, learning and inference procedures for bothvector-variate and matrix-variate ordinal data. We show that our model is ableto capture latent opinion profile of citizens around the world, and iscompetitive against state-of-art collaborative filtering techniques onlarge-scale public datasets. The model thus has the potential to extendapplication of RBMs to diverse domains such as recommendation systems, productreviews and expert assessments.
arxiv-7800-164 | Learning From Ordered Sets and Applications in Collaborative Ranking | http://arxiv.org/pdf/1408.0043v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.LG cs.IR stat.ML published:2014-07-31 summary:Ranking over sets arise when users choose between groups of items. Forexample, a group may be of those movies deemed $5$ stars to them, or acustomized tour package. It turns out, to model this data type properly, weneed to investigate the general combinatorics problem of partitioning a set andordering the subsets. Here we construct a probabilistic log-linear model over aset of ordered subsets. Inference in this combinatorial space is highlychallenging: The space size approaches $(N!/2)6.93145^{N+1}$ as $N$ approachesinfinity. We propose a \texttt{split-and-merge} Metropolis-Hastings procedurethat can explore the state-space efficiently. For discovering hidden aspects inthe data, we enrich the model with latent binary variables so that theposteriors can be efficiently evaluated. Finally, we evaluate the proposedmodel on large-scale collaborative filtering tasks and demonstrate that it iscompetitive against state-of-the-art methods.
arxiv-7800-165 | Learning Nash Equilibria in Congestion Games | http://arxiv.org/pdf/1408.0017v1.pdf | author:Walid Krichene, Benjamin DrighÃ¨s, Alexandre M. Bayen category:cs.LG cs.GT published:2014-07-31 summary:We study the repeated congestion game, in which multiple populations ofplayers share resources, and make, at each iteration, a decentralized decisionon which resources to utilize. We investigate the following question: given amodel of how individual players update their strategies, does the resultingdynamics of strategy profiles converge to the set of Nash equilibria of theone-shot game? We consider in particular a model in which players update theirstrategies using algorithms with sublinear discounted regret. We show that theresulting sequence of strategy profiles converges to the set of Nash equilibriain the sense of Ces\`aro means. However, strong convergence is not guaranteedin general. We show that strong convergence can be guaranteed for a class ofalgorithms with a vanishing upper bound on discounted regret, and which satisfyan additional condition. We call such algorithms AREP algorithms, forApproximate REPlicator, as they can be interpreted as a discrete-timeapproximation of the replicator equation, which models the continuous-timeevolution of population strategies, and which is known to converge for theclass of congestion games. In particular, we show that the discounted Hedgealgorithm belongs to the AREP class, which guarantees its strong convergence.
arxiv-7800-166 | A Bottom-Up Approach for Automatic Pancreas Segmentation in Abdominal CT Scans | http://arxiv.org/pdf/1407.8497v1.pdf | author:Amal Farag, Le Lu, Evrim Turkbey, Jiamin Liu, Ronald M. Summers category:cs.CV published:2014-07-31 summary:Organ segmentation is a prerequisite for a computer-aided diagnosis (CAD)system to detect pathologies and perform quantitative analysis. Foranatomically high-variability abdominal organs such as the pancreas, previoussegmentation works report low accuracies when comparing to organs like theheart or liver. In this paper, a fully-automated bottom-up method is presentedfor pancreas segmentation, using abdominal computed tomography (CT) scans. Themethod is based on a hierarchical two-tiered information propagation byclassifying image patches. It labels superpixels as pancreas or not via poolingpatch-level confidences on 2D CT slices over-segmented by the Simple LinearIterative Clustering approach. A supervised random forest (RF) classifier istrained on the patch level and a two-level cascade of RFs is applied at thesuperpixel level, coupled with multi-channel feature extraction, respectively.On six-fold cross-validation using 80 patient CT volumes, we achieved 68.8%Dice coefficient and 57.2% Jaccard Index, comparable to or slightly better thanpublished state-of-the-art methods.
arxiv-7800-167 | A New Model of Array Grammar for generating Connected Patterns on an Image Neighborhood | http://arxiv.org/pdf/1407.8337v1.pdf | author:G. Vishnu Murthy, Pavan Kumar C., Vakulabharanam Vijaya Kumar category:cs.FL cs.CV published:2014-07-31 summary:Study of patterns on images is recognized as an important step incharacterization and classification of image. The ability to efficientlyanalyze and describe image patterns is thus of fundamental importance. Thestudy of syntactic methods of describing pictures has been of interest forresearchers. Array Grammars can be used to represent and recognize connectedpatterns. In any image the patterns are recognized using connected patterns. Itis very difficult to represent all connected patterns (CP) even on a small 3 x3 neighborhood in a pictorial way. The present paper proposes the model ofarray grammar capable of generating any kind of simple or complex pattern andderivation of connected pattern in an image neighborhood using the proposedgrammar is discussed.
arxiv-7800-168 | Two-pass Discourse Segmentation with Pairing and Global Features | http://arxiv.org/pdf/1407.8215v1.pdf | author:Vanessa Wei Feng, Graeme Hirst category:cs.CL published:2014-07-30 summary:Previous attempts at RST-style discourse segmentation typically adoptfeatures centered on a single token to predict whether to insert a boundarybefore that token. In contrast, we develop a discourse segmenter utilizing aset of pairing features, which are centered on a pair of adjacent tokens in thesentence, by equally taking into account the information from both tokens.Moreover, we propose a novel set of global features, which encodecharacteristics of the segmentation as a whole, once we have an initialsegmentation. We show that both the pairing and global features are useful ontheir own, and their combination achieved an $F_1$ of 92.6% of identifyingin-sentence discourse boundaries, which is a 17.8% error-rate reduction overthe state-of-the-art performance, approaching 95% of human performance. Inaddition, similar improvement is observed across different classificationframeworks.
arxiv-7800-169 | Fast Bayesian Feature Selection for High Dimensional Linear Regression in Genomics via the Ising Approximation | http://arxiv.org/pdf/1407.8187v1.pdf | author:Charles K. Fisher, Pankaj Mehta category:q-bio.QM cs.LG stat.ML published:2014-07-30 summary:Feature selection, identifying a subset of variables that are relevant forpredicting a response, is an important and challenging component of manymethods in statistics and machine learning. Feature selection is especiallydifficult and computationally intensive when the number of variables approachesor exceeds the number of samples, as is often the case for many genomicdatasets. Here, we introduce a new approach -- the Bayesian Ising Approximation(BIA) -- to rapidly calculate posterior probabilities for feature relevance inL2 penalized linear regression. In the regime where the regression problem isstrongly regularized by the prior, we show that computing the marginalposterior probabilities for features is equivalent to computing themagnetizations of an Ising model. Using a mean field approximation, we show itis possible to rapidly compute the feature selection path described by theposterior probabilities as a function of the L2 penalty. We present simulationsand analytical results illustrating the accuracy of the BIA on some simpleregression problems. Finally, we demonstrate the applicability of the BIA tohigh dimensional regression by analyzing a gene expression dataset with nearly30,000 features.
arxiv-7800-170 | Population fluctuation promotes cooperation in networks | http://arxiv.org/pdf/1407.8032v4.pdf | author:Steve Miller, Joshua Knowles category:cs.GT cs.NE physics.soc-ph published:2014-07-30 summary:We consider the problem of explaining the emergence and evolution ofcooperation in dynamic network-structured populations. Building on seminal workby Poncela et al, which shows how cooperation (in one-shot prisoner's dilemma)is supported in growing populations by an evolutionary preferential attachment(EPA) model, we investigate the effect of fluctuations in the population size.We find that the fluctuating model is more robust than Poncela et al's in thatcooperation flourishes for a wide variety of initial conditions. In terms ofboth the temptation to defect, and the types of strategies present in thefounder network, the fluctuating population is found to lead more securely tocooperation. Further, we find that this model will also support the emergenceof cooperation from pre-existing non-cooperative random networks. This model,like Poncela et al's, does not require agents to have memory, recognition ofother agents, or other cognitive abilities, and so may suggest a more generalexplanation of the emergence of cooperation in early evolutionary transitions,than mechanisms such as kin selection, direct and indirect reciprocity.
arxiv-7800-171 | Merging and Shifting of Images with Prominence Coefficient for Predictive Analysis using Combined Image | http://arxiv.org/pdf/1407.8123v1.pdf | author:T. R. Gopalakrishnan Nair, Richa Sharma category:cs.CV published:2014-07-30 summary:Shifting of objects in an image and merging many images after appropriateshifting is being used in several engineering and scientific applications whichrequire complex perception development. A method has been presented here whichcould be used in precision engineering and biological applications where moreprecise prediction is required of a combined phenomenon with varying prominenceof each phenomenon. Accurate merging of intended pixels can be achieved in highquality using frequency domain techniques even though initial properties of theoriginal pixels are lost in this process. This paper introduces a technique toshift and merge various images with varying prominence of each image. Acoefficient named prominence coefficient has been introduced which is capableof making some of the images transparent and highlighting the rest as perrequirement of merging process which can be used as a simple but effectivetechnique for overlapped view of a set of images.
arxiv-7800-172 | Learning Economic Parameters from Revealed Preferences | http://arxiv.org/pdf/1407.7937v1.pdf | author:Maria-Florina Balcan, Amit Daniely, Ruta Mehta, Ruth Urner, Vijay V. Vazirani category:cs.GT cs.LG published:2014-07-30 summary:A recent line of work, starting with Beigman and Vohra (2006) andZadimoghaddam and Roth (2012), has addressed the problem of {\em learning} autility function from revealed preference data. The goal here is to make use ofpast data describing the purchases of a utility maximizing agent when facedwith certain prices and budget constraints in order to produce a hypothesisfunction that can accurately forecast the {\em future} behavior of the agent. In this work we advance this line of work by providing sample complexityguarantees and efficient algorithms for a number of important classes. Bydrawing a connection to recent advances in multi-class learning, we provide acomputationally efficient algorithm with tight sample complexity guarantees($\Theta(d/\epsilon)$ for the case of $d$ goods) for learning linear utilityfunctions under a linear price model. This solves an open question inZadimoghaddam and Roth (2012). Our technique yields numerous generalizationsincluding the ability to learn other well-studied classes of utility functions,to deal with a misspecified model, and with non-linear prices.
arxiv-7800-173 | Accurate merging of images for predictive analysis using combined image | http://arxiv.org/pdf/1407.8176v1.pdf | author:T. R. Gopalakrishnan Nair, Richa Sharma category:cs.CV published:2014-07-30 summary:Several Scientific and engineering applications require merging of sampledimages for complex perception development. In most cases, for suchrequirements, images are merged at intensity level. Even though it gives fairlygood perception of combined scenario of objects and scenes, it is found thatthey are not sufficient enough to analyze certain engineering cases. The mainproblem is incoherent modulation of intensity arising out of phase propertiesbeing lost. In order to compensate these losses, combined phase and amplitudemerge is demanded. We present here a method which could be used in precisionengineering and biological applications where more precise prediction isrequired of a combined phenomenon. When pixels are added, its original propertyis lost but accurate merging of intended pixels can be achieved in high qualityusing frequency domain properties of an image. This paper introduces atechnique to merge various images which can be used as a simple but effectivetechnique for overlapped view of a set of images and producing reduced datasetfor review purposes.
arxiv-7800-174 | Automated Machine Learning on Big Data using Stochastic Algorithm Tuning | http://arxiv.org/pdf/1407.7969v1.pdf | author:Thomas Nickson, Michael A Osborne, Steven Reece, Stephen J Roberts category:stat.ML published:2014-07-30 summary:We introduce a means of automating machine learning (ML) for big data tasks,by performing scalable stochastic Bayesian optimisation of ML algorithmparameters and hyper-parameters. More often than not, the critical tuning of MLalgorithm parameters has relied on domain expertise from experts, along withlaborious hand-tuning, brute search or lengthy sampling runs. Against thisbackground, Bayesian optimisation is finding increasing use in automatingparameter tuning, making ML algorithms accessible even to non-experts. However,the state of the art in Bayesian optimisation is incapable of scaling to thelarge number of evaluations of algorithm performance required to fit realisticmodels to complex, big data. We here describe a stochastic, sparse, Bayesianoptimisation strategy to solve this problem, using many thousands of noisyevaluations of algorithm performance on subsets of data in order to effectivelytrain algorithms for big data. We provide a comprehensive benchmarking ofpossible sparsification strategies for Bayesian optimisation, concluding that aNystrom approximation offers the best scaling and performance for real tasks.Our proposed algorithm demonstrates substantial improvement over the state ofthe art in tuning the parameters of a Gaussian Process time series predictiontask on real, big data.
arxiv-7800-175 | Targeting Optimal Active Learning via Example Quality | http://arxiv.org/pdf/1407.8042v1.pdf | author:Lewis P. G. Evans, Niall M. Adams, Christoforos Anagnostopoulos category:stat.ML cs.LG published:2014-07-30 summary:In many classification problems unlabelled data is abundant and a subset canbe chosen for labelling. This defines the context of active learning (AL),where methods systematically select that subset, to improve a classifier byretraining. Given a classification problem, and a classifier trained on a smallnumber of labelled examples, consider the selection of a single furtherexample. This example will be labelled by the oracle and then used to retrainthe classifier. This example selection raises a central question: given a fullyspecified stochastic description of the classification problem, which exampleis the optimal selection? If optimality is defined in terms of loss, thisdefinition directly produces expected loss reduction (ELR), a central quantitywhose maximum yields the optimal example selection. This work presents a newtheoretical approach to AL, example quality, which defines optimal AL behaviourin terms of ELR. Once optimal AL behaviour is defined mathematically, reasoningabout this abstraction provides insights into AL. In a theoretical context theoptimal selection is compared to existing AL methods, showing that heuristicscan make sub-optimal selections. Algorithms are constructed to estimate examplequality directly. A large-scale experimental study shows these algorithms to becompetitive with standard AL methods.
arxiv-7800-176 | Differentially-Private Logistic Regression for Detecting Multiple-SNP Association in GWAS Databases | http://arxiv.org/pdf/1407.8067v1.pdf | author:Fei Yu, Michal Rybar, Caroline Uhler, Stephen E. Fienberg category:stat.ML cs.LG stat.AP 62P10 published:2014-07-30 summary:Following the publication of an attack on genome-wide association studies(GWAS) data proposed by Homer et al., considerable attention has been given todeveloping methods for releasing GWAS data in a privacy-preserving way. Here,we develop an end-to-end differentially private method for solving regressionproblems with convex penalty functions and selecting the penalty parameters bycross-validation. In particular, we focus on penalized logistic regression withelastic-net regularization, a method widely used to in GWAS analyses toidentify disease-causing genes. We show how a differentially private procedurefor penalized logistic regression with elastic-net regularization can beapplied to the analysis of GWAS data and evaluate our method's performance.
arxiv-7800-177 | The Grow-Shrink strategy for learning Markov network structures constrained by context-specific independences | http://arxiv.org/pdf/1407.8088v1.pdf | author:Alejandro Edera, Yanela Strappa, Facundo Bromberg category:cs.LG cs.DS published:2014-07-30 summary:Markov networks are models for compactly representing complex probabilitydistributions. They are composed by a structure and a set of numerical weights.The structure qualitatively describes independences in the distribution, whichcan be exploited to factorize the distribution into a set of compact functions.A key application for learning structures from data is to automaticallydiscover knowledge. In practice, structure learning algorithms focused on"knowledge discovery" present a limitation: they use a coarse-grainedrepresentation of the structure. As a result, this representation cannotdescribe context-specific independences. Very recently, an algorithm calledCSPC was designed to overcome this limitation, but it has a high computationalcomplexity. This work tries to mitigate this downside presenting CSGS, analgorithm that uses the Grow-Shrink strategy for reducing unnecessarycomputations. On an empirical evaluation, the structures learned by CSGSachieve competitive accuracies and lower computational complexity with respectto those obtained by CSPC.
arxiv-7800-178 | Clustering Approach Towards Image Segmentation: An Analytical Study | http://arxiv.org/pdf/1407.8121v1.pdf | author:Dibya Jyoti Bora, Anil Kumar Gupta category:cs.CV published:2014-07-30 summary:Image processing is an important research area in computer vision. Imagesegmentation plays the vital rule in image processing research. There exist somany methods for image segmentation. Clustering is an unsupervised study.Clustering can also be used for image segmentation. In this paper, an in-depthstudy is done on different clustering techniques that can be used for imagesegmentation with their pros and cons. An experiment for color imagesegmentation based on clustering with K-Means algorithm is performed to observethe accuracy of clustering technique for the segmentation purpose.
arxiv-7800-179 | Stochastic Coordinate Coding and Its Application for Drosophila Gene Expression Pattern Annotation | http://arxiv.org/pdf/1407.8147v2.pdf | author:Binbin Lin, Qingyang Li, Qian Sun, Ming-Jun Lai, Ian Davidson, Wei Fan, Jieping Ye category:cs.LG cs.CE published:2014-07-30 summary:\textit{Drosophila melanogaster} has been established as a model organism forinvestigating the fundamental principles of developmental gene interactions.The gene expression patterns of \textit{Drosophila melanogaster} can bedocumented as digital images, which are annotated with anatomical ontologyterms to facilitate pattern discovery and comparison. The automated annotationof gene expression pattern images has received increasing attention due to therecent expansion of the image database. The effectiveness of gene expressionpattern annotation relies on the quality of feature representation. Previousstudies have demonstrated that sparse coding is effective for extractingfeatures from gene expression images. However, solving sparse coding remains acomputationally challenging problem, especially when dealing with large-scaledata sets and learning large size dictionaries. In this paper, we propose anovel algorithm to solve the sparse coding problem, called StochasticCoordinate Coding (SCC). The proposed algorithm alternatively updates thesparse codes via just a few steps of coordinate descent and updates thedictionary via second order stochastic gradient descent. The computational costis further reduced by focusing on the non-zero components of the sparse codesand the corresponding columns of the dictionary only in the updating procedure.Thus, the proposed algorithm significantly improves the efficiency and thescalability, making sparse coding applicable for large-scale data sets andlarge dictionary sizes. Our experiments on Drosophila gene expression data setsdemonstrate the efficiency and the effectiveness of the proposed algorithm.
arxiv-7800-180 | Bayesian Probabilistic Matrix Factorization: A User Frequency Analysis | http://arxiv.org/pdf/1407.7840v1.pdf | author:Cody Severinski, Ruslan Salakhutdinov category:stat.ML published:2014-07-29 summary:Matrix factorization (MF) has become a common approach to collaborativefiltering, due to ease of implementation and scalability to large data sets.Two existing drawbacks of the basic model is that it does not incorporate sideinformation on either users or items, and assumes a common variance for allusers. We extend the work of constrained probabilistic matrix factorization byderiving the Gibbs updates for the side feature vectors for items(Salakhutdinov and Minh, 2008). We show that this Bayesian treatment to theconstrained PMF model outperforms simple MAP estimation. We also considerextensions to heteroskedastic precision introduced in the literature(Lakshminarayanan, Bouchard, and Archambeau, 2011). We show that this tendsresult in overfitting for deterministic approximation algorithms (ex:Variational inference) when the observed entries in the user / item matrix aredistributed in an non-uniform manner. In light of this, we propose a truncatedprecision model. Our experimental results suggest that this model tends todelay overfitting.
arxiv-7800-181 | Chasing Ghosts: Competing with Stateful Policies | http://arxiv.org/pdf/1407.7635v1.pdf | author:Uriel Feige, Tomer Koren, Moshe Tennenholtz category:cs.LG published:2014-07-29 summary:We consider sequential decision making in a setting where regret is measuredwith respect to a set of stateful reference policies, and feedback is limitedto observing the rewards of the actions performed (the so called "bandit"setting). If either the reference policies are stateless rather than stateful,or the feedback includes the rewards of all actions (the so called "expert"setting), previous work shows that the optimal regret grows like$\Theta(\sqrt{T})$ in terms of the number of decision rounds $T$. The difficulty in our setting is that the decision maker unavoidably losestrack of the internal states of the reference policies, and thus cannotreliably attribute rewards observed in a certain round to any of the referencepolicies. In fact, in this setting it is impossible for the algorithm toestimate which policy gives the highest (or even approximately highest) totalreward. Nevertheless, we design an algorithm that achieves expected regret thatis sublinear in $T$, of the form $O( T/\log^{1/4}{T})$. Our algorithm is basedon a certain local repetition lemma that may be of independent interest. Wealso show that no algorithm can guarantee expected regret better than $O(T/\log^{3/2} T)$.
arxiv-7800-182 | Sure Screening for Gaussian Graphical Models | http://arxiv.org/pdf/1407.7819v1.pdf | author:Shikai Luo, Rui Song, Daniela Witten category:stat.ML cs.LG published:2014-07-29 summary:We propose {graphical sure screening}, or GRASS, a very simple andcomputationally-efficient screening procedure for recovering the structure of aGaussian graphical model in the high-dimensional setting. The GRASS estimate ofthe conditional dependence graph is obtained by thresholding the elements ofthe sample covariance matrix. The proposed approach possesses the surescreening property: with very high probability, the GRASS estimated edge setcontains the true edge set. Furthermore, with high probability, the size of theestimated edge set is controlled. We provide a choice of threshold for GRASSthat can control the expected false positive rate. We illustrate theperformance of GRASS in a simulation study and on a gene expression data set,and show that in practice it performs quite competitively with more complex andcomputationally-demanding techniques for graph estimation.
arxiv-7800-183 | A Hash-based Co-Clustering Algorithm for Categorical Data | http://arxiv.org/pdf/1407.7753v1.pdf | author:Fabricio Olivetti de FranÃ§a category:cs.LG published:2014-07-29 summary:Many real-life data are described by categorical attributes without apre-classification. A common data mining method used to extract informationfrom this type of data is clustering. This method group together the samplesfrom the data that are more similar than all other samples. But, categoricaldata pose a challenge when extracting information because: the calculation oftwo objects similarity is usually done by measuring the number of commonfeatures, but ignore a possible importance weighting; if the data may bedivided differently according to different subsets of the features, thealgorithm may find clusters with different meanings from each other,difficulting the post analysis. Data Co-Clustering of categorical data is thetechnique that tries to find subsets of samples that share a subset of featuresin common. By doing so, not only a sample may belong to more than one clusterbut, the feature selection of each cluster describe its own characteristics. Inthis paper a novel Co-Clustering technique for categorical data is proposed byusing Locality Sensitive Hashing technique in order to preprocess a list ofCo-Clusters seeds based on a previous research. Results indicate this techniqueis capable of finding high quality Co-Clusters in many different categoricaldata sets and scales linearly with the data set size.
arxiv-7800-184 | A CUDA-Based Real Parameter Optimization Benchmark | http://arxiv.org/pdf/1407.7737v1.pdf | author:Ke Ding, Ying Tan category:cs.NE published:2014-07-29 summary:Benchmarking is key for developing and comparing optimization algorithms. Inthis paper, a CUDA-based real parameter optimization benchmark (cuROB) isintroduced. Test functions of diverse properties are included within cuROB andimplemented efficiently with CUDA. Speedup of one order of magnitude can beachieved in comparison with CPU-based benchmark of CEC'14.
arxiv-7800-185 | A Latent Space Analysis of Editor Lifecycles in Wikipedia | http://arxiv.org/pdf/1407.7736v1.pdf | author:Xiangju Qin, Derek Greene, PÃ¡draig Cunningham category:cs.SI cs.CL cs.CY physics.soc-ph published:2014-07-29 summary:Collaborations such as Wikipedia are a key part of the value of the modernInternet. At the same time there is concern that these collaborations arethreatened by high levels of member turnover. In this paper we borrow ideasfrom topic analysis to editor activity on Wikipedia over time into a latentspace that offers an insight into the evolving patterns of editor behavior.This latent space representation reveals a number of different categories ofeditor (e.g. content experts, social networkers) and we show that it doesprovide a signal that predicts an editor's departure from the community. Wealso show that long term editors gradually diversify their participation byshifting edit preference from one or two namespaces to multiple namespaces andexperience relatively soft evolution in their editor profiles, while short termeditors generally distribute their contribution randomly among the namespacesand experience considerably fluctuated evolution in their editor profiles.
arxiv-7800-186 | NMF with Sparse Regularizations in Transformed Domains | http://arxiv.org/pdf/1407.7691v1.pdf | author:JÃ©rÃ©my Rapin, JÃ©rÃ´me Bobin, Anthony Larue, Jean-Luc Starck category:stat.ML cs.LG 94A12 I.5.4 published:2014-07-29 summary:Non-negative blind source separation (non-negative BSS), which is alsoreferred to as non-negative matrix factorization (NMF), is a very active fieldin domains as different as astrophysics, audio processing or biomedical signalprocessing. In this context, the efficient retrieval of the sources requiresthe use of signal priors such as sparsity. If NMF has now been well studiedwith sparse constraints in the direct domain, only very few algorithms canencompass non-negativity together with sparsity in a transformed domain sincesimultaneously dealing with two priors in two different domains is challenging.In this article, we show how a sparse NMF algorithm coined non-negativegeneralized morphological component analysis (nGMCA) can be extended to imposenon-negativity in the direct domain along with sparsity in a transformeddomain, with both analysis and synthesis formulations. To our knowledge, thiswork presents the first comparison of analysis and synthesis priors ---as wellas their reweighted versions--- in the context of blind source separation.Comparisons with state-of-the-art NMF algorithms on realistic data show theefficiency as well as the robustness of the proposed algorithms.
arxiv-7800-187 | A Survey on Two Dimensional Cellular Automata and Its Application in Image Processing | http://arxiv.org/pdf/1407.7626v1.pdf | author:Deepak Ranjan Nayak, Prashanta Kumar Patra, Amitav Mahapatra category:cs.CV published:2014-07-29 summary:Parallel algorithms for solving any image processing task is a highlydemanded approach in the modern world. Cellular Automata (CA) are the mostcommon and simple models of parallel computation. So, CA has been successfullyused in the domain of image processing for the last couple of years. This paperprovides a survey of available literatures of some methodologies employed bydifferent researchers to utilize the cellular automata for solving someimportant problems of image processing. The survey includes some importantimage processing tasks such as rotation, zooming, translation, segmentation,edge detection, compression and noise reduction of images. Finally, theexperimental results of some methodologies are presented.
arxiv-7800-188 | Hyperspectral Imaging and Analysis for Sparse Reconstruction and Recognition | http://arxiv.org/pdf/1407.7686v1.pdf | author:Zohaib Khan category:cs.CV published:2014-07-29 summary:This thesis proposes spatio-spectral techniques for hyperspectral imageanalysis. Adaptive spatio-spectral support and variable exposure hyperspectralimaging is demonstrated to improve spectral reflectance recovery fromhyperspectral images. Novel spectral dimensionality reduction techniques havebeen proposed from the perspective of spectral only and spatio-spectralinformation preservation. It was found that the joint sparse and joint groupsparse hyperspectral image models achieve lower reconstruction error and higherrecognition accuracy using only a small subset of bands. Hyperspectral imagedatabases have been developed and made publicly available for further researchin compressed hyperspectral imaging, forensic document analysis and spectralreflectance recovery.
arxiv-7800-189 | Level-based Analysis of Genetic Algorithms and other Search Processes | http://arxiv.org/pdf/1407.7663v1.pdf | author:Dogan Corus, Duc-Cuong Dang, Anton V. Eremeev, Per Kristian Lehre category:cs.NE q-bio.PE published:2014-07-29 summary:The fitness-level technique is a simple and old way to derive upper boundsfor the expected runtime of simple elitist evolutionary algorithms (EAs).Recently, the technique has been adapted to deduce the runtime of algorithmswith non-elitist populations and unary variation operators. In this paper, weshow that the restriction to unary variation operators can be removed. This gives rise to a much more general analytical tool which is applicable toa wide range of search processes. As introductory examples, we provide simpleruntime analyses of many variants of the Genetic Algorithm on well-knownbenchmark functions, such as OneMax, LeadingOnes, and the sorting problem.
arxiv-7800-190 | OpenML: networked science in machine learning | http://arxiv.org/pdf/1407.7722v2.pdf | author:Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, Luis Torgo category:cs.LG cs.CY published:2014-07-29 summary:Many sciences have made significant breakthroughs by adopting online toolsthat help organize, structure and mine information that is too detailed to beprinted in journals. In this paper, we introduce OpenML, a place for machinelearning researchers to share and organize data in fine detail, so that theycan work more effectively, be more visible, and collaborate with others totackle harder problems. We discuss how OpenML relates to other examples ofnetworked science and what benefits it brings for machine learning research,individual scientists, as well as students and practitioners.
arxiv-7800-191 | Estimating the Accuracies of Multiple Classifiers Without Labeled Data | http://arxiv.org/pdf/1407.7644v2.pdf | author:Ariel Jaffe, Boaz Nadler, Yuval Kluger category:stat.ML cs.LG published:2014-07-29 summary:In various situations one is given only the predictions of multipleclassifiers over a large unlabeled test data. This scenario raises thefollowing questions: Without any labeled data and without any a-prioriknowledge about the reliability of these different classifiers, is it possibleto consistently and computationally efficiently estimate their accuracies?Furthermore, also in a completely unsupervised manner, can one construct a moreaccurate unsupervised ensemble classifier? In this paper, focusing on thebinary case, we present simple, computationally efficient algorithms to solvethese questions. Furthermore, under standard classifier independenceassumptions, we prove our methods are consistent and study their asymptoticerror. Our approach is spectral, based on the fact that the off-diagonalentries of the classifiers' covariance matrix and 3-d tensor are rank-one. Weillustrate the competitive performance of our algorithms via extensiveexperiments on both artificial and real datasets.
arxiv-7800-192 | How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation | http://arxiv.org/pdf/1407.7906v3.pdf | author:Yoshua Bengio category:cs.LG published:2014-07-29 summary:We propose to exploit {\em reconstruction} as a layer-local training signalfor deep learning. Reconstructions can be propagated in a form of targetpropagation playing a role similar to back-propagation but helping to reducethe reliance on derivatives in order to perform credit assignment across manylevels of possibly strong non-linearities (which is difficult forback-propagation). A regularized auto-encoder tends produce a reconstructionthat is a more likely version of its input, i.e., a small move in the directionof higher likelihood. By generalizing gradients, target propagation may alsoallow to train deep networks with discrete hidden units. If the auto-encodertakes both a representation of input and target (or of any side information) ininput, then its reconstruction of input representation provides a targettowards a representation that is more likely, conditioned on all the sideinformation. A deep auto-encoder decoding path generalizes gradient propagationin a learned way that can could thus handle not just infinitesimal changes butlarger, discrete changes, hopefully allowing credit assignment through a longchain of non-linear operations. In addition to each layer being a goodauto-encoder, the encoder also learns to please the upper layers bytransforming the data into a space where it is easier to model by them,flattening manifolds and disentangling factors. The motivations and theoreticaljustifications for this approach are laid down in this paper, along withconjectures that will have to be verified either mathematically orexperimentally, including a hypothesis stating that such auto-encoder mediatedtarget propagation could play in brains the role of credit assignment throughmany non-linear, noisy and discrete transformations.
arxiv-7800-193 | Entropic one-class classifiers | http://arxiv.org/pdf/1407.7556v3.pdf | author:Lorenzo Livi, Alireza Sadeghian, Witold Pedrycz category:cs.CV cs.LG stat.ML I.2.6; K.2.3 published:2014-07-28 summary:The one-class classification problem is a well-known research endeavor inpattern recognition. The problem is also known under different names, such asoutlier and novelty/anomaly detection. The core of the problem consists inmodeling and recognizing patterns belonging only to a so-called target class.All other patterns are termed non-target, and therefore they should berecognized as such. In this paper, we propose a novel one-class classificationsystem that is based on an interplay of different techniques. Primarily, wefollow a dissimilarity representation based approach; we embed the input datainto the dissimilarity space by means of an appropriate parametricdissimilarity measure. This step allows us to process virtually any type ofdata. The dissimilarity vectors are then represented through a weightedEuclidean graphs, which we use to (i) determine the entropy of the datadistribution in the dissimilarity space, and at the same time (ii) deriveeffective decision regions that are modeled as clusters of vertices. Since thedissimilarity measure for the input data is parametric, we optimize itsparameters by means of a global optimization scheme, which considers bothmesoscopic and structural characteristics of the data represented through thegraphs. The proposed one-class classifier is designed to provide both hard(Boolean) and soft decisions about the recognition of test patterns, allowingan accurate description of the classification process. We evaluate theperformance of the system on different benchmarking datasets, containing eitherfeature-based or structured patterns. Experimental results demonstrate theeffectiveness of the proposed technique.
arxiv-7800-194 | Algorithms, Initializations, and Convergence for the Nonnegative Matrix Factorization | http://arxiv.org/pdf/1407.7299v1.pdf | author:Amy N. Langville, Carl D. Meyer, Russell Albright, James Cox, David Duling category:cs.NA cs.LG stat.ML 65F30 published:2014-07-28 summary:It is well known that good initializations can improve the speed and accuracyof the solutions of many nonnegative matrix factorization (NMF) algorithms.Many NMF algorithms are sensitive with respect to the initialization of W or Hor both. This is especially true of algorithms of the alternating least squares(ALS) type, including the two new ALS algorithms that we present in this paper.We compare the results of six initialization procedures (two standard and fournew) on our ALS algorithms. Lastly, we discuss the practical issue of choosingan appropriate convergence criterion.
arxiv-7800-195 | A unified framework for thermal face recognition | http://arxiv.org/pdf/1407.7317v1.pdf | author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV published:2014-07-28 summary:The reduction of the cost of infrared (IR) cameras in recent years has madeIR imaging a highly viable modality for face recognition in practice. Aparticularly attractive advantage of IR-based over conventional, visiblespectrum-based face recognition stems from its invariance to visibleillumination. In this paper we argue that the main limitation of previous workon face recognition using IR lies in its ad hoc approach to treating differentnuisance factors which affect appearance, prohibiting a unified approach thatis capable of handling concurrent changes in multiple (or indeed all) majorextrinsic sources of variability, which is needed in practice. We describe thefirst approach that attempts to achieve this - the framework we proposeachieves outstanding recognition performance in the presence of variable (i)pose, (ii) facial expression, (iii) physiological state, (iv) partial occlusiondue to eye-wear, and (v) quasi-occlusion due to facial hair growth.
arxiv-7800-196 | Discovering Discriminative Cell Attributes for HEp-2 Specimen Image Classification | http://arxiv.org/pdf/1407.7330v1.pdf | author:Arnold Wiliem, Peter Hobson, Brian C. Lovell category:cs.CV cs.CE published:2014-07-28 summary:Recently, there has been a growing interest in developing Computer AidedDiagnostic (CAD) systems for improving the reliability and consistency ofpathology test results. This paper describes a novel CAD system for theAnti-Nuclear Antibody (ANA) test via Indirect Immunofluorescence protocol onHuman Epithelial Type 2 (HEp-2) cells. While prior works have primarily focusedon classifying cell images extracted from ANA specimen images, this work takesa further step by focussing on the specimen image classification problemitself. Our system is able to efficiently classify specimen images as well asproducing meaningful descriptions of ANA pattern class which helps physiciansto understand the differences between various ANA patterns. We achieve thisgoal by designing a specimen-level image descriptor that: (1) is highlydiscriminative; (2) has small descriptor length and (3) is semanticallymeaningful at the cell level. In our work, a specimen image descriptor isrepresented by its overall cell attribute descriptors. As such, we propose twomax-margin based learning schemes to discover cell attributes whilst stillmaintaining the discrimination of the specimen image descriptor. Our learningschemes differ from the existing discriminative attribute learning approachesas they primarily focus on discovering image-level attributes. Comparativeevaluations were undertaken to contrast the proposed approach to variousstate-of-the-art approaches on a novel HEp-2 cell dataset which wasspecifically proposed for the specimen-level classification. Finally, weshowcase the ability of the proposed approach to provide textual descriptionsto explain ANA patterns.
arxiv-7800-197 | Text Classification Using Association Rules, Dependency Pruning and Hyperonymization | http://arxiv.org/pdf/1407.7357v1.pdf | author:Yannis Haralambous, Philippe Lenca category:cs.IR cs.CL published:2014-07-28 summary:We present new methods for pruning and enhancing item- sets for textclassification via association rule mining. Pruning methods are based ondependency syntax and enhancing methods are based on replacing words by theirhyperonyms of various orders. We discuss the impact of these methods, comparedto pruning based on tfidf rank of words.
arxiv-7800-198 | Beyond KernelBoost | http://arxiv.org/pdf/1407.8518v1.pdf | author:Roberto Rigamonti, Vincent Lepetit, Pascal Fua category:cs.CV cs.LG published:2014-07-28 summary:In this Technical Report we propose a set of improvements with respect to theKernelBoost classifier presented in [Becker et al., MICCAI 2013]. We start witha scheme inspired by Auto-Context, but that is suitable in situations where thelack of large training sets poses a potential problem of overfitting. The aimis to capture the interactions between neighboring image pixels to betterregularize the boundaries of segmented regions. As in Auto-Context [Tu et al.,PAMI 2009] the segmentation process is iterative and, at each iteration, thesegmentation results for the previous iterations are taken into account inconjunction with the image itself. However, unlike in [Tu et al., PAMI 2009],we organize our recursion so that the classifiers can progressively focus ondifficult-to-classify locations. This lets us exploit the power of thedecision-tree paradigm while avoiding over-fitting. In the context of thisarchitecture, KernelBoost represents a powerful building block due to itsability to learn on the score maps coming from previous iterations. We firstintroduce two important mechanisms to empower the KernelBoost classifier,namely pooling and the clustering of positive samples based on the appearanceof the corresponding ground-truth. These operations significantly contribute toincrease the effectiveness of the system on biomedical images, where textureplays a major role in the recognition of the different image components. Wethen present some other techniques that can be easily integrated in theKernelBoost framework to further improve the accuracy of the finalsegmentation. We show extensive results on different medical image datasets,including some multi-label tasks, on which our method is shown to outperformstate-of-the-art approaches. The resulting segmentations display high accuracy,neat contours, and reduced noise.
arxiv-7800-199 | Non-parametric Image Registration of Airborne LiDAR, Hyperspectral and Photographic Imagery of Forests | http://arxiv.org/pdf/1410.0226v1.pdf | author:Juheon Lee, Xiaohao Cai, Carola-Bibiane Schonlieb, David Coomes category:cs.CV I.4.8 published:2014-07-28 summary:There is much current interest in using multi-sensor airborne remote sensingto monitor the structure and biodiversity of forests. This paper addresses theapplication of non-parametric image registration techniques to precisely alignimages obtained from multimodal imaging, which is critical for the successfulidentification of individual trees using object recognition approaches.Non-parametric image registration, in particular the technique of optimizingone objective function containing data fidelity and regularization terms,provides flexible algorithms for image registration. Using a survey ofwoodlands in southern Spain as an example, we show that non-parametric imageregistration can be successful at fusing datasets when there is little priorknowledge about how the datasets are interrelated (i.e. in the absence ofground control points). The validity of non-parametric registration methods inairborne remote sensing is demonstrated by a series of experiments. Precisedata fusion is a prerequisite to accurate recognition of objects withinairborne imagery, so non-parametric image registration could make a valuablecontribution to the analysis pipeline.
arxiv-7800-200 | A Numerical Optimization Algorithm Inspired by the Strawberry Plant | http://arxiv.org/pdf/1407.7399v1.pdf | author:F. Merrikh-Bayat category:cs.NE published:2014-07-28 summary:This paper proposes a new numerical optimization algorithm inspired by thestrawberry plant for solving complicated engineering problems. Plants likestrawberry develop both runners and roots for propagation and search for waterresources and minerals. In these plants, runners and roots can be thought of astools for global and local searches, respectively. The proposed algorithm hasthree main differences with the trivial nature-inspired optimizationalgorithms: duplication-elimination of the computational agents at alliterations, subjecting all agents to both small and large movements from thebeginning to end, and the lack of communication (information exchange) betweenagents. Moreover, it has the advantage of using only three parameters to betuned by user. This algorithm is applied to standard test functions and theresults are compared with GA and PSO. The proposed algorithm is also used tosolve an open problem in the field of robust control theory. These simulationsshow that the proposed algorithm can very effectively solve complicatedoptimization problems.
arxiv-7800-201 | 'Almost Sure' Chaotic Properties of Machine Learning Methods | http://arxiv.org/pdf/1407.7417v1.pdf | author:Nabarun Mondal, Partha P. Ghosh category:cs.LG cs.AI published:2014-07-28 summary:It has been demonstrated earlier that universal computation is 'almostsurely' chaotic. Machine learning is a form of computational fixed pointiteration, iterating over the computable function space. We showcase someproperties of this iteration, and establish in general that the iteration is'almost surely' of chaotic nature. This theory explains the observation in thecounter intuitive properties of deep learning methods. This paper demonstratesthat these properties are going to be universal to any learning method.
arxiv-7800-202 | Toward a multilevel representation of protein molecules: comparative approaches to the aggregation/folding propensity problem | http://arxiv.org/pdf/1407.7559v3.pdf | author:Lorenzo Livi, Alessandro Giuliani, Antonello Rizzi category:cs.CE cs.LG q-bio.BM q-bio.MN I.2.6; K.3.2 published:2014-07-28 summary:This paper builds upon the fundamental work of Niwa et al. [34], whichprovides the unique possibility to analyze the relative aggregation/foldingpropensity of the elements of the entire Escherichia coli (E. coli) proteome ina cell-free standardized microenvironment. The hardness of the problem comesfrom the superposition between the driving forces of intra- and inter-moleculeinteractions and it is mirrored by the evidences of shift from folding toaggregation phenotypes by single-point mutations [10]. Here we apply severalstate-of-the-art classification methods coming from the field of structuralpattern recognition, with the aim to compare different representations of thesame proteins gathered from the Niwa et al. data base; such representationsinclude sequences and labeled (contact) graphs enriched with chemico-physicalattributes. By this comparison, we are able to identify also some interestinggeneral properties of proteins. Notably, (i) we suggest a threshold around 250residues discriminating "easily foldable" from "hardly foldable" moleculesconsistent with other independent experiments, and (ii) we highlight therelevance of contact graph spectra for folding behavior discrimination andcharacterization of the E. coli solubility data. The soundness of theexperimental results presented in this paper is proved by the statisticallyrelevant relationships discovered among the chemico-physical description ofproteins and the developed cost matrix of substitution used in the variousdiscrimination systems.
arxiv-7800-203 | A Fast Hierarchical Method for Multi-script and Arbitrary Oriented Scene Text Extraction | http://arxiv.org/pdf/1407.7504v1.pdf | author:Lluis Gomez, Dimosthenis Karatzas category:cs.CV published:2014-07-28 summary:Typography and layout lead to the hierarchical organisation of text in words,text lines, paragraphs. This inherent structure is a key property of text inany script and language, which has nonetheless been minimally leveraged byexisting text detection methods. This paper addresses the problem of textsegmentation in natural scenes from a hierarchical perspective. Contrary toexisting methods, we make explicit use of text structure, aiming directly tothe detection of region groupings corresponding to text within a hierarchyproduced by an agglomerative similarity clustering process over individualregions. We propose an optimal way to construct such an hierarchy introducing afeature space designed to produce text group hypotheses with high recall and anovel stopping rule combining a discriminative classifier and a probabilisticmeasure of group meaningfulness based in perceptual organization. Resultsobtained over four standard datasets, covering text in variable orientationsand different languages, demonstrate that our algorithm, while being trained ina single mixed dataset, outperforms state of the art methods in unconstrainedscenarios.
arxiv-7800-204 | Efficient Regularized Regression for Variable Selection with L0 Penalty | http://arxiv.org/pdf/1407.7508v1.pdf | author:Zhenqiu Liu, Gang Li category:cs.LG stat.ML published:2014-07-28 summary:Variable (feature, gene, model, which we use interchangeably) selections forregression with high-dimensional BIGDATA have found many applications inbioinformatics, computational biology, image processing, and engineering. Oneappealing approach is the L0 regularized regression which penalizes the numberof nonzero features in the model directly. L0 is known as the most essentialsparsity measure and has nice theoretical properties, while the popular L1regularization is only a best convex relaxation of L0. Therefore, it is naturalto expect that L0 regularized regression performs better than LASSO. However,it is well-known that L0 optimization is NP-hard and computationallychallenging. Instead of solving the L0 problems directly, most publications sofar have tried to solve an approximation problem that closely resembles L0regularization. In this paper, we propose an efficient EM algorithm (L0EM) that directlysolves the L0 optimization problem. $L_0$EM is efficient with high dimensionaldata. It also provides a natural solution to all Lp p in [0,2] problems. Theregularized parameter can be either determined through cross-validation or AICand BIC. Theoretical properties of the L0-regularized estimator are given undermild conditions that permit the number of variables to be much larger than thesample size. We demonstrate our methods through simulation and high-dimensionalgenomic data. The results indicate that L0 has better performance than LASSOand L0 with AIC or BIC has similar performance as computationally intensivecross-validation. The proposed algorithms are efficient in identifying thenon-zero variables with less-bias and selecting biologically important genesand pathways with high dimensional BIGDATA.
arxiv-7800-205 | Dependence versus Conditional Dependence in Local Causal Discovery from Gene Expression Data | http://arxiv.org/pdf/1407.7566v1.pdf | author:Eric V. Strobl, Shyam Visweswaran category:q-bio.QM cs.LG stat.ML published:2014-07-28 summary:Motivation: Algorithms that discover variables which are causally related toa target may inform the design of experiments. With observational geneexpression data, many methods discover causal variables by measuring eachvariable's degree of statistical dependence with the target using dependencemeasures (DMs). However, other methods measure each variable's ability toexplain the statistical dependence between the target and the remainingvariables in the data using conditional dependence measures (CDMs), since thisstrategy is guaranteed to find the target's direct causes, direct effects, anddirect causes of the direct effects in the infinite sample limit. In thispaper, we design a new algorithm in order to systematically compare therelative abilities of DMs and CDMs in discovering causal variables from geneexpression data. Results: The proposed algorithm using a CDM is sample efficient, since itconsistently outperforms other state-of-the-art local causal discoveryalgorithms when samples sizes are small. However, the proposed algorithm usinga CDM outperforms the proposed algorithm using a DM only when sample sizes areabove several hundred. These results suggest that accurate causal discoveryfrom gene expression data using current CDM-based algorithms requires datasetswith at least several hundred samples. Availability: The proposed algorithm is freely available athttps://github.com/ericstrobl/DvCD.
arxiv-7800-206 | Dynamic Feature Scaling for Online Learning of Binary Classifiers | http://arxiv.org/pdf/1407.7584v1.pdf | author:Danushka Bollegala category:cs.LG stat.ML published:2014-07-28 summary:Scaling feature values is an important step in numerous machine learningtasks. Different features can have different value ranges and some form of afeature scaling is often required in order to learn an accurate classifier.However, feature scaling is conducted as a preprocessing task prior tolearning. This is problematic in an online setting because of two reasons.First, it might not be possible to accurately determine the value range of afeature at the initial stages of learning when we have observed only a fewnumber of training instances. Second, the distribution of data can change overthe time, which render obsolete any feature scaling that we perform in apre-processing step. We propose a simple but an effective method to dynamicallyscale features at train time, thereby quickly adapting to any changes in thedata stream. We compare the proposed dynamic feature scaling method againstmore complex methods for estimating scaling parameters using several benchmarkdatasets for binary classification. Our proposed feature scaling methodconsistently outperforms more complex methods on all of the benchmark datasetsand improves classification accuracy of a state-of-the-art online binaryclassifier algorithm.
arxiv-7800-207 | Understanding Random Forests: From Theory to Practice | http://arxiv.org/pdf/1407.7502v3.pdf | author:Gilles Louppe category:stat.ML published:2014-07-28 summary:Data analysis and machine learning have become an integrative part of themodern scientific methodology, offering automated procedures for the predictionof a phenomenon based on past observations, unraveling underlying patterns indata and providing insights about the problem. Yet, caution should avoid usingmachine learning as a black-box tool, but rather consider it as a methodology,with a rational thought process that is entirely dependent on the problem understudy. In particular, the use of algorithms should ideally require a reasonableunderstanding of their mechanisms, properties and limitations, in order tobetter apprehend and interpret their results. Accordingly, the goal of this thesis is to provide an in-depth analysis ofrandom forests, consistently calling into question each and every part of thealgorithm, in order to shed new light on its learning capabilities, innerworkings and interpretability. The first part of this work studies theinduction of decision trees and the construction of ensembles of randomizedtrees, motivating their design and purpose whenever possible. Our contributionsfollow with an original complexity analysis of random forests, showing theirgood computational performance and scalability, along with an in-depthdiscussion of their implementation details, as contributed within Scikit-Learn. In the second part of this work, we analyse and discuss the interpretabilityof random forests in the eyes of variable importance measures. The core of ourcontributions rests in the theoretical characterization of the Mean Decrease ofImpurity variable importance measure, from which we prove and derive some ofits properties in the case of multiway totally randomized trees and inasymptotic conditions. In consequence of this work, our analysis demonstratesthat variable importances [...].
arxiv-7800-208 | A discussion on the validation tests employed to compare human action recognition methods using the MSR Action3D dataset | http://arxiv.org/pdf/1407.7390v3.pdf | author:JosÃ© RamÃ³n Padilla-LÃ³pez, Alexandros AndrÃ© Chaaraoui, Francisco FlÃ³rez-Revuelta category:cs.CV published:2014-07-28 summary:This paper aims to determine which is the best human action recognitionmethod based on features extracted from RGB-D devices, such as the MicrosoftKinect. A review of all the papers that make reference to MSR Action3D, themost used dataset that includes depth information acquired from a RGB-D device,has been performed. We found that the validation method used by each workdiffers from the others. So, a direct comparison among works cannot be made.However, almost all the works present their results comparing them withouttaking into account this issue. Therefore, we present different rankingsaccording to the methodology used for the validation in orden to clarify theexisting confusion.
arxiv-7800-209 | An evolutionary solver for linear integer programming | http://arxiv.org/pdf/1407.7211v1.pdf | author:JoÃ£o Pedro Pedroso category:cs.NE cs.AI math.OC 80M50 G.1.6, I.2.8 published:2014-07-27 summary:In this paper we introduce an evolutionary algorithm for the solution oflinear integer programs. The strategy is based on the separation of thevariables into the integer subset and the continuous subset; the integervariables are fixed by the evolutionary system, and the continuous ones aredetermined in function of them, by a linear program solver. We report results obtained for some standard benchmark problems, and comparethem with those obtained by branch-and-bound. The performance of theevolutionary algorithm is promising. Good feasible solutions were generallyobtained, and in some of the difficult benchmark tests it outperformedbranch-and-bound.
arxiv-7800-210 | Online Learning and Profit Maximization from Revealed Preferences | http://arxiv.org/pdf/1407.7294v2.pdf | author:Kareem Amin, Rachel Cummings, Lili Dworkin, Michael Kearns, Aaron Roth category:cs.DS cs.GT cs.LG published:2014-07-27 summary:We consider the problem of learning from revealed preferences in an onlinesetting. In our framework, each period a consumer buys an optimal bundle ofgoods from a merchant according to her (linear) utility function and currentprices, subject to a budget constraint. The merchant observes only thepurchased goods, and seeks to adapt prices to optimize his profits. We give anefficient algorithm for the merchant's problem that consists of a learningphase in which the consumer's utility function is (perhaps partially) inferred,followed by a price optimization step. We also consider an alternative onlinelearning algorithm for the setting where prices are set exogenously, but themerchant would still like to predict the bundle that will be bought by theconsumer for purposes of inventory or supply chain management. In contrast withmost prior work on the revealed preferences problem, we demonstrate that bymaking stronger assumptions on the form of utility functions, efficientalgorithms for both learning and profit maximization are possible, even inadaptive, online settings.
arxiv-7800-211 | Leveraging user profile attributes for improving pedagogical accuracy of learning pathways | http://arxiv.org/pdf/1407.7260v1.pdf | author:Tanmay Sinha, Ankit Banka, Dae Ki Kang category:cs.CY cs.LG published:2014-07-27 summary:In recent years, with the enormous explosion of web based learning resources,personalization has become a critical factor for the success of services thatwish to leverage the power of Web 2.0. However, the relevance, significance andimpact of tailored content delivery in the learning domain is stillquestionable. Apart from considering only interaction based features likeratings and inferring learner preferences from them, if these services were toincorporate innate user profile attributes which affect learning activities,the quality of recommendations produced could be vastly improved. Recognizingthe crucial role of effective guidance in informal educational settings, weprovide a principled way of utilizing multiple sources of information from theuser profile itself for the recommendation task. We explore factors that affectthe choice of learning resources and explain in what way are they helpful toimprove the pedagogical accuracy of learning objects recommended. Through asystematical application of machine learning techniques, we further provide atechnological solution to convert these indirectly mapped learner specificattributes into a direct mapping with the learning resources. This mapping hasa distinct advantage of tagging learning resources to make their metadata moreinformative. The results of our empirical study depict the similarity ofnominal learning attributes with respect to each other. We further succeed incapturing the learner subset, whose preferences are most likely to be anindication of learning resource usage. Our novel system filters learner profileattributes to discover a tag that links them with learning resources.
arxiv-7800-212 | Your click decides your fate: Inferring Information Processing and Attrition Behavior from MOOC Video Clickstream Interactions | http://arxiv.org/pdf/1407.7131v2.pdf | author:Tanmay Sinha, Patrick Jermann, Nan Li, Pierre Dillenbourg category:cs.HC cs.LG published:2014-07-26 summary:In this work, we explore video lecture interaction in Massive Open OnlineCourses (MOOCs), which is central to student learning experience on theseeducational platforms. As a research contribution, we operationalize videolecture clickstreams of students into cognitively plausible higher levelbehaviors, and construct a quantitative information processing index, which canaid instructors to better understand MOOC hurdles and reason aboutunsatisfactory learning outcomes. Our results illustrate how such a metricinspired by cognitive psychology can help answer critical questions regardingstudents' engagement, their future click interactions and participationtrajectories that lead to in-video & course dropouts. Implications for researchand practice are discussed
arxiv-7800-213 | Pushbroom Stereo for High-Speed Navigation in Cluttered Environments | http://arxiv.org/pdf/1407.7091v1.pdf | author:Andrew J. Barry, Russ Tedrake category:cs.RO cs.CV published:2014-07-26 summary:We present a novel stereo vision algorithm that is capable of obstacledetection on a mobile-CPU processor at 120 frames per second. Our systemperforms a subset of standard block-matching stereo processing, searching onlyfor obstacles at a single depth. By using an onboard IMU and state-estimator,we can recover the position of obstacles at all other depths, building andupdating a full depth-map at framerate. Here, we describe both the algorithm and our implementation on a high-speed,small UAV, flying at over 20 MPH (9 m/s) close to obstacles. The systemrequires no external sensing or computation and is, to the best of ourknowledge, the first high-framerate stereo detection system running onboard asmall UAV.
arxiv-7800-214 | Principles and Parameters: a coding theory perspective | http://arxiv.org/pdf/1407.7169v1.pdf | author:Matilde Marcolli category:cs.CL cs.IT math.IT 91F20, 68P30 published:2014-07-26 summary:We propose an approach to Longobardi's parametric comparison method (PCM) viathe theory of error-correcting codes. One associates to a collection oflanguages to be analyzed with the PCM a binary (or ternary) code with one codewords for each language in the family and each word consisting of the binaryvalues of the syntactic parameters of the language, with the ternary caseallowing for an additional parameter state that takes into account phenomena ofentailment of parameters. The code parameters of the resulting code can becompared with some classical bounds in coding theory: the asymptotic bound, theGilbert-Varshamov bound, etc. The position of the code parameters with respectto some of these bounds provides quantitative information on the variability ofsyntactic parameters within and across historical-linguistic families. Whilecomputations carried out for languages belonging to the same family yield codesbelow the GV curve, comparisons across different historical families can giveexamples of isolated codes lying above the asymptotic bound.
arxiv-7800-215 | Pairwise Correlations in Layered Close-Packed Structures | http://arxiv.org/pdf/1407.7159v1.pdf | author:P. M. Riechers, D. P. Varn, J. P. Crutchfield category:cs.LG published:2014-07-26 summary:Given a description of the stacking statistics of layered close-packedstructures in the form of a hidden Markov model, we develop analyticalexpressions for the pairwise correlation functions between the layers. Thesemay be calculated analytically as explicit functions of model parameters or theexpressions may be used as a fast, accurate, and efficient way to obtainnumerical values. We present several examples, finding agreement with previouswork as well as deriving new relations.
arxiv-7800-216 | Crowdsourcing Dialect Characterization through Twitter | http://arxiv.org/pdf/1407.7094v1.pdf | author:Bruno GonÃ§alves, David SÃ¡nchez category:physics.soc-ph cs.CL cs.SI stat.ML published:2014-07-26 summary:We perform a large-scale analysis of language diatopic variation usinggeotagged microblogging datasets. By collecting all Twitter messages written inSpanish over more than two years, we build a corpus from which a carefullyselected list of concepts allows us to characterize Spanish varieties on aglobal scale. A cluster analysis proves the existence of well definedmacroregions sharing common lexical properties. Remarkably enough, we find thatSpanish language is split into two superdialects, namely, an urban speech usedacross major American and Spanish citites and a diverse form that encompassesrural areas and small towns. The latter can be further clustered into smallervarieties with a stronger regional character.
arxiv-7800-217 | Dissimilarity-based Sparse Subset Selection | http://arxiv.org/pdf/1407.6810v2.pdf | author:Ehsan Elhamifar, Guillermo Sapiro, S. Shankar Sastry category:cs.LG stat.ML published:2014-07-25 summary:Finding an informative subset of a large collection of data points or modelsis at the center of many problems in computer vision, recommender systems,bio/health informatics as well as image and natural language processing. Givenpairwise dissimilarities between the elements of a `source set' and a `targetset,' we consider the problem of finding a subset of the source set, calledrepresentatives or exemplars, that can efficiently describe the target set. Weformulate the problem as a row-sparsity regularized trace minimization problem.Since the proposed formulation is, in general, NP-hard, we consider a convexrelaxation. The solution of our optimization finds representatives and theassignment of each element of the target set to each representative, hence,obtaining a clustering. We analyze the solution of our proposed optimization asa function of the regularization parameter. We show that when the two setsjointly partition into multiple groups, our algorithm finds representativesfrom all groups and reveals clustering of the sets. In addition, we show thatthe proposed framework can effectively deal with outliers. Our algorithm workswith arbitrary dissimilarities, which can be asymmetric or violate the triangleinequality. To efficiently implement our algorithm, we consider an AlternatingDirection Method of Multipliers (ADMM) framework, which results in quadraticcomplexity in the problem size. We show that the ADMM implementation allows toparallelize the algorithm, hence further reducing the computational time.Finally, by experiments on real-world datasets, we show that our proposedalgorithm improves the state of the art on the two problems of scenecategorization using representative images and time-series modeling andsegmentation using representative~models.
arxiv-7800-218 | Efficient Bayesian Nonparametric Modelling of Structured Point Processes | http://arxiv.org/pdf/1407.6949v1.pdf | author:Tom Gunter, Chris Lloyd, Michael A. Osborne, Stephen J. Roberts category:stat.ML published:2014-07-25 summary:This paper presents a Bayesian generative model for dependent Cox pointprocesses, alongside an efficient inference scheme which scales as if the pointprocesses were modelled independently. We can handle missing data naturally,infer latent structure, and cope with large numbers of observed processes. Afurther novel contribution enables the model to work effectively in higherdimensional spaces. Using this method, we achieve vastly improved predictiveperformance on both 2D and 1D real data, validating our structured approach.
arxiv-7800-219 | Interpretable Low-Rank Document Representations with Label-Dependent Sparsity Patterns | http://arxiv.org/pdf/1407.6872v1.pdf | author:Ivan Ivek category:cs.CL cs.IR cs.LG published:2014-07-25 summary:In context of document classification, where in a corpus of documents theirlabel tags are readily known, an opportunity lies in utilizing labelinformation to learn document representation spaces with better discriminativeproperties. To this end, in this paper application of a Variational BayesianSupervised Nonnegative Matrix Factorization (supervised vbNMF) withlabel-driven sparsity structure of coefficients is proposed for learning ofdiscriminative nonsubtractive latent semantic components occuring in TF-IDFdocument representations. Constraints are such that the components pursued aremade to be frequently occuring in a small set of labels only, making itpossible to yield document representations with distinctive label-specificsparse activation patterns. A simple measure of quality of this kind ofsparsity structure, dubbed inter-label sparsity, is introduced andexperimentally brought into tight connection with classification performance.Representing a great practical convenience, inter-label sparsity is shown to beeasily controlled in supervised vbNMF by a single parameter.
arxiv-7800-220 | Substitute Based SCODE Word Embeddings in Supervised NLP Tasks | http://arxiv.org/pdf/1407.6853v1.pdf | author:Volkan Cirik, Deniz Yuret category:cs.CL published:2014-07-25 summary:We analyze a word embedding method in supervised tasks. It maps words on asphere such that words co-occurring in similar contexts lie closely. Thesimilarity of contexts is measured by the distribution of substitutes that canfill them. We compared word embeddings, including more recent representations,in Named Entity Recognition (NER), Chunking, and Dependency Parsing. We examineour framework in multilingual dependency parsing as well. The results show thatthe proposed method achieves as good as or better results compared to the otherword embeddings in the tasks we investigate. It achieves state-of-the-artresults in multilingual dependency parsing. Word embeddings in 7 languages areavailable for public use.
arxiv-7800-221 | How the Voynich Manuscript was created | http://arxiv.org/pdf/1407.6639v3.pdf | author:Torsten Timm category:cs.CR cs.CL published:2014-07-24 summary:The Voynich manuscript is a medieval book written in an unknown script. Thispaper studies the relation between similarly spelled words in the Voynichmanuscript. By means of a detailed analysis of similar spelled words it waspossible to reveal the text generation method used for the Voynich manuscript.
arxiv-7800-222 | Recognition of Handwritten Persian/Arabic Numerals Based on Robust Feature Set and K-NN Classifier | http://arxiv.org/pdf/1407.6492v2.pdf | author:Reza Azad, Fatemeh Davami, Hamid Reza Shayegh category:cs.CV published:2014-07-24 summary:This paper has been withdrawn by the author due to a crucial sign error inequation 2 and some mistake in Table 1 information. please let me for changingthis information and updating this paper.
arxiv-7800-223 | Novel and Fast Algorithm for Extracting License Plate Location Based on Edge Analysis | http://arxiv.org/pdf/1407.6496v2.pdf | author:Reza Azad, Mohammad Baghdadi category:cs.CV published:2014-07-24 summary:Nowadays in developing or developed countries, the Intelligent TransportationSystem (ITS) technology has attracted so much attention to itself. LicensePlate Recognition (LPR) systems have many applications in ITSs, such as thepayment of parking fee, controlling the traffic volume, traffic datacollection, etc. This paper presents a new and fast method for license plateextraction based on edge analysis. our proposed method consist of four stage,which are edge detection, non-useable edge and noise removing, edge analysisand morphology-based license plate extraction. In the result part, the proposedalgorithm is applied on vehicle database and the accuracy rate reached 98%.From the experimental results it is shown that the proposed method gives fairlyacceptable level of accuracy for practical license plate recognition system.
arxiv-7800-224 | A Robust and Efficient Method for Improving Accuracy of License Plate Characters Recognition | http://arxiv.org/pdf/1407.6705v2.pdf | author:Reza Azad, Hamid Reza Shayegh, Hamed Amiri category:cs.CV published:2014-07-24 summary:License Plate Recognition (LPR) plays an important role on the trafficmonitoring and parking management. A robust and efficient method for enhancingaccuracy of license plate characters recognition based on K Nearest Neighbours(K-NN) classifier is presented in this paper. The system first prepares acontour form of the extracted character, then the angle and distance featureinformation about the character is extracted and finally K-NN classifier isused to character recognition. Angle and distance features of a character havebeen computed based on distribution of points on the bitmap image of character.In K-NN method, the Euclidean distance between testing point and referencepoints is calculated in order to find the k-nearest neighbours. We evaluatedour method on the available dataset that contain 1200 sample. Using 70% samplesfor training, we tested our method on whole samples and obtained 99% correctrecognition rate.Further, we achieved average 99.41% accuracy usingthree/strategy validation technique on 1200 dataset.
arxiv-7800-225 | Enhancing the Accuracy of Biometric Feature Extraction Fusion Using Gabor Filter and Mahalanobis Distance Algorithm | http://arxiv.org/pdf/1407.6748v1.pdf | author:Ayodeji S. Makinde, Yaw Nkansah-Gyekye, Loserian S. Laizer category:cs.CV published:2014-07-24 summary:Biometric recognition systems have advanced significantly in the last decadeand their use in specific applications will increase in the near future. Theability to conduct meaningful comparisons and assessments will be crucial tosuccessful deployment and increasing biometric adoption. The best modality usedas unimodal biometric systems are unable to fully address the problem of higherrecognition rate. Multimodal biometric systems are able to mitigate some of thelimitations encountered in unimodal biometric systems, such asnon-universality, distinctiveness, non-acceptability, noisy sensor data, spoofattacks, and performance. More reliable recognition accuracy and performanceare achievable as different modalities were being combined together anddifferent algorithms or techniques were being used. The work presented in thispaper focuses on a bimodal biometric system using face and fingerprint. Animage enhancement technique (histogram equalization) is used to enhance theface and fingerprint images. Salient features of the face and fingerprint wereextracted using the Gabor filter technique. A dimensionality reductiontechnique was carried out on both images extracted features using a principalcomponent analysis technique. A feature level fusion algorithm (Mahalanobisdistance technique) is used to combine each unimodal feature together. Theperformance of the proposed approach is validated and is effective.
arxiv-7800-226 | Performance evaluation of wavelet scattering network in image texture classification in various color spaces | http://arxiv.org/pdf/1407.6423v1.pdf | author:Jiasong Wu, Longyu Jiang, Xu Han, Lotfi Senhadji, Huazhong Shu category:cs.CV published:2014-07-24 summary:Texture plays an important role in many image analysis applications. In thispaper, we give a performance evaluation of color texture classification byperforming wavelet scattering network in various color spaces. Experimentalresults on the KTH_TIPS_COL database show that opponent RGB based waveletscattering network outperforms other color spaces. Therefore, when dealing withthe problem of color texture classification, opponent RGB based waveletscattering network is recommended.
arxiv-7800-227 | Learning Structured Outputs from Partial Labels using Forest Ensemble | http://arxiv.org/pdf/1407.6432v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.CV cs.LG published:2014-07-24 summary:Learning structured outputs with general structures is computationallychallenging, except for tree-structured models. Thus we propose an efficientboosting-based algorithm AdaBoost.MRF for this task. The idea is based on therealization that a graph is a superimposition of trees. Different from mostexisting work, our algorithm can handle partial labelling, and thus isparticularly attractive in practice where reliable labels are often sparselyobserved. In addition, our method works exclusively on trees and thus isguaranteed to converge. We apply the AdaBoost.MRF algorithm to an indoor videosurveillance scenario, where activities are modelled at multiple levels.
arxiv-7800-228 | Real-Time and Efficient Method for Accuracy Enhancement of Edge Based License Plate Recognition System | http://arxiv.org/pdf/1407.6498v1.pdf | author:Reza Azad, Babak Azad, Hamid Reza Shayegh category:cs.CV published:2014-07-24 summary:License Plate Recognition plays an important role on the traffic monitoringand parking management. Administration and restriction of those transportationtools for their better service becomes very essential. In this paper, a fastand real time method has an appropriate application to find plates that theplat has tilt and the picture quality is poor. In the proposed method, at thebeginning, the image is converted into binary mode with use of adaptivethreshold. And with use of edge detection and morphology operation, platenumber location has been specified and if the plat has tilt; its tilt isremoved away. Then its characters are distinguished using image processingtechniques. Finally, K Nearest Neighbour (KNN) classifier was used forcharacter recognition. This method has been tested on available data set thathas different images of the background, considering distance, and angel of viewso that the correct extraction rate of plate reached at 98% and characterrecognition rate achieved at 99.12%. Further we tested our characterrecognition stage on Persian vehicle data set and we achieved 99% correctrecognition rate.
arxiv-7800-229 | Novel and Tuneable Method for Skin Detection Based on Hybrid Color Space and Color Statistical Features | http://arxiv.org/pdf/1407.6506v1.pdf | author:Reza Azad, Hamid Reza Shayegh category:cs.CV published:2014-07-24 summary:Skin detection is one of the most important and primary stages in some ofimage processing applications such as face detection and human tracking. Sofar, many approaches are proposed to done this case. Near all of these methodshave tried to find best match intensity distribution with skin pixels based onpopular color spaces such as RGB, CMYK or YCbCr. Results show these methodscannot provide an accurate approach for every kinds of skin. In this paper, anapproach is proposed to solve this problem using statistical featurestechnique. This approach is including two stages. In the first one, from pureskin statistical features were extracted and at the second stage, the skinpixels are detected using HSV and YCbCr color spaces. In the result part, theproposed approach is applied on FEI database and the accuracy rate reached99.25 + 0.2. Further proposed method is applied on complex background databaseand accuracy rate obtained 95.40+0.31%. The proposed approach can be used forall kinds of skin using train stage which is the main advantages of it. Lownoise sensitivity and low computational complexity are some of otheradvantages.
arxiv-7800-230 | Feature Engineering for Knowledge Base Construction | http://arxiv.org/pdf/1407.6439v3.pdf | author:Christopher RÃ©, Amir Abbas Sadeghian, Zifei Shan, Jaeho Shin, Feiran Wang, Sen Wu, Ce Zhang category:cs.DB cs.CL cs.LG published:2014-07-24 summary:Knowledge base construction (KBC) is the process of populating a knowledgebase, i.e., a relational database together with inference rules, withinformation extracted from documents and structured sources. KBC blurs thedistinction between two traditional database problems, information extractionand information integration. For the last several years, our group has beenbuilding knowledge bases with scientific collaborators. Using our approach, wehave built knowledge bases that have comparable and sometimes better qualitythan those constructed by human volunteers. In contrast to these knowledgebases, which took experts a decade or more human years to construct, many ofour projects are constructed by a single graduate student. Our approach to KBC is based on joint probabilistic inference and learning,but we do not see inference as either a panacea or a magic bullet: inference isa tool that allows us to be systematic in how we construct, debug, and improvethe quality of such systems. In addition, inference allows us to constructthese systems in a more loosely coupled way than traditional approaches. Tosupport this idea, we have built the DeepDive system, which has the design goalof letting the user "think about features---not algorithms." We think ofDeepDive as declarative in that one specifies what they want but not how to getit. We describe our approach with a focus on feature engineering, which weargue is an understudied problem relative to its importance to end-to-endquality.
arxiv-7800-231 | New Method for Optimization of License Plate Recognition system with Use of Edge Detection and Connected Component | http://arxiv.org/pdf/1407.6510v1.pdf | author:Reza Azad, Hamid Reza Shayegh category:cs.CV published:2014-07-24 summary:License Plate recognition plays an important role on the traffic monitoringand parking management systems. In this paper, a fast and real time method hasbeen proposed which has an appropriate application to find tilt and poorquality plates. In the proposed method, at the beginning, the image isconverted into binary mode using adaptive threshold. Then, by using some edgedetection and morphology operations, plate number location has been specified.Finally, if the plat has tilt, its tilt is removed away. This method has beentested on another paper data set that has different images of the background,considering distance, and angel of view so that the correct extraction rate ofplate reached at 98.66%.
arxiv-7800-232 | Convolutional Neural Associative Memories: Massive Capacity with Noise Tolerance | http://arxiv.org/pdf/1407.6513v1.pdf | author:Amin Karbasi, Amir Hesam Salavati, Amin Shokrollahi category:cs.NE cs.AI published:2014-07-24 summary:The task of a neural associative memory is to retrieve a set of previouslymemorized patterns from their noisy versions using a network of neurons. Anideal network should have the ability to 1) learn a set of patterns as theyarrive, 2) retrieve the correct patterns from noisy queries, and 3) maximizethe pattern retrieval capacity while maintaining the reliability in respondingto queries. The majority of work on neural associative memories has focused ondesigning networks capable of memorizing any set of randomly chosen patterns atthe expense of limiting the retrieval capacity. In this paper, we show that ifwe target memorizing only those patterns that have inherent redundancy (i.e.,belong to a subspace), we can obtain all the aforementioned properties. This isin sharp contrast with the previous work that could only improve one or twoaspects at the expense of the third. More specifically, we propose frameworkbased on a convolutional neural network along with an iterative algorithm thatlearns the redundancy among the patterns. The resulting network has a retrievalcapacity that is exponential in the size of the network. Moreover, theasymptotic error correction performance of our network is linear in the size ofthe patterns. We then ex- tend our approach to deal with patterns lieapproximately in a subspace. This extension allows us to memorize datasetscontaining natural patterns (e.g., images). Finally, we report experimentalresults on both synthetic and real datasets to support our claims.
arxiv-7800-233 | Trainable and Dynamic Computing: Error Backpropagation through Physical Media | http://arxiv.org/pdf/1407.6637v1.pdf | author:Michiel Hermans, MichaÃ«l Burm, Joni Dambre, Peter Bienstman category:cs.NE published:2014-07-24 summary:Machine learning algorithms, and more in particular neural networks, arguablyexperience a revolution in terms of performance. Currently, the best systems wehave for speech recognition, computer vision and similar problems are based onneural networks, trained using the half-century old backpropagation algorithm.Despite the fact that neural networks are a form of analog computers, they arestill implemented digitally for reasons of convenience and availability. Inthis paper we demonstrate how we can design physical linear dynamic systemswith non-linear feedback as a generic platform for dynamic, neuro-inspiredanalog computing. We show that a crucial advantage of this setup is that theerror backpropagation can be performed physically as well, which greatly speedsup the optimisation process. As we show in this paper, using one experimentallyvalidated and one conceptual example, such systems may be the key to providinga relatively straightforward mechanism for constructing highly scalable, fullydynamic analog computers.
arxiv-7800-234 | Subspace Learning From Bits | http://arxiv.org/pdf/1407.6288v2.pdf | author:Yuejie Chi category:stat.ML cs.IT math.IT published:2014-07-23 summary:This paper proposes a simple sensing and estimation framework to faithfullyrecover the principal subspace of high-dimensional datasets or data streamsfrom a collection of one-bit measurements from distributed sensors based oncomparing accumulated energy projections of their data samples of dimension nover pairs of randomly selected directions. By leveraging low-dimensionalstructures, the top eigenvectors of a properly designed surrogate matrix isshown to recover the principal subspace of rank $r$ as soon as the number ofbit measurements exceeds the order of $nr^2 \log n$, which can be much smallerthan the ambient dimension of the covariance matrix. The sample complexity toobtain reliable comparison outcomes is also obtained. Furthermore, we develop alow-complexity online algorithm to track the principal subspace that allows newbit measurements arrive sequentially. Numerical examples are provided tovalidate the proposed approach.
arxiv-7800-235 | Visual Word Selection without Re-Coding and Re-Pooling | http://arxiv.org/pdf/1407.6174v1.pdf | author:Fatih Cakir, Stan Sclaroff category:cs.CV published:2014-07-23 summary:The Bag-of-Words (BoW) representation is widely used in computer vision. Thesize of the codebook impacts the time and space complexity of the applicationsthat use BoW. Thus, given a training set for a particular computer vision task,a key problem is pruning a large codebook to select only a subset of visualwords. Evaluating possible selections of words to be included in the prunedcodebook can be computationally prohibitive; in a brute-force scheme,evaluating each pruned codebook requires re-coding of all features extractedfrom training images to words in the candidate codebook and then re-pooling thewords to obtain a representation of each image, e.g., histogram of visual wordfrequencies. In this paper, a method is proposed that selects and evaluates asubset of words from an initially large codebook, without the need forre-coding or re-pooling. Formulations are proposed for two commonly-usedschemes: hard and soft (kernel) coding of visual words with average-pooling.The effectiveness of these formulations is evaluated on the 15 Scenes andCaltech 10 benchmarks.
arxiv-7800-236 | Content-Level Selective Offloading in Heterogeneous Networks: Multi-armed Bandit Optimization and Regret Bounds | http://arxiv.org/pdf/1407.6154v1.pdf | author:Pol Blasco, Deniz GÃ¼ndÃ¼z category:cs.IT cs.LG math.IT published:2014-07-23 summary:We consider content-level selective offloading of cellular downlink trafficto a wireless infostation terminal which stores high data-rate content in itscache memory. Cellular users in the vicinity of the infostation can directlydownload the stored content from the infostation through a broadband connection(e.g., WiFi), reducing the latency and load on the cellular network. The goalof the infostation cache controller (CC) is to store the most popular contentin the cache memory such that the maximum amount of traffic is offloaded to theinfostation. In practice, the popularity profile of the files is not known bythe CC, which observes only the instantaneous demands for those contents storedin the cache. Hence, the cache content placement is optimised based on thedemand history and on the cost associated to placing each content in the cache.By refreshing the cache content at regular time intervals, the CC graduallylearns the popularity profile, while at the same time exploiting the limitedcache capacity in the best way possible. This is formulated as a multi-armedbandit (MAB) problem with switching cost. Several algorithms are presented todecide on the cache content over time. The performance is measured in terms ofcache efficiency, defined as the amount of net traffic that is offloaded to theinfostation. In addition to theoretical regret bounds, the proposed algorithmsare analysed through numerical simulations. In particular, the impact of systemparameters, such as the number of files, number of users, cache size, andskewness of the popularity profile, on the performance is studied numerically.It is shown that the proposed algorithms learn the popularity profile quicklyfor a wide range of system parameters.
arxiv-7800-237 | A Fast Synchronization Clustering Algorithm | http://arxiv.org/pdf/1407.7449v1.pdf | author:Xinquan Chen category:cs.LG published:2014-07-23 summary:This paper presents a Fast Synchronization Clustering algorithm (FSynC),which is an improved version of SynC algorithm. In order to decrease the timecomplexity of the original SynC algorithm, we combine grid cell partitioningmethod and Red-Black tree to construct the near neighbor point set of everypoint. By simulated experiments of some artificial data sets and several realdata sets, we observe that FSynC algorithm can often get less time than SynCalgorithm for many kinds of data sets. At last, it gives some researchexpectations to popularize this algorithm.
arxiv-7800-238 | Permutation Models for Collaborative Ranking | http://arxiv.org/pdf/1407.6128v1.pdf | author:Truyen Tran, Svetha Venkatesh category:cs.IR cs.LG stat.ML published:2014-07-23 summary:We study the problem of collaborative filtering where ranking information isavailable. Focusing on the core of the collaborative ranking process, the userand their community, we propose new models for representation of the underlyingpermutations and prediction of ranks. The first approach is based on theassumption that the user makes successive choice of items in a stage-wisemanner. In particular, we extend the Plackett-Luce model in two ways -introducing parameter factoring to account for user-specific contribution, andmodelling the latent community in a generative setting. The second approachrelies on log-linear parameterisation, which relaxes the discrete-choiceassumption, but makes learning and inference much more involved. We proposeMCMC-based learning and inference methods and derive linear-time predictionalgorithms.
arxiv-7800-239 | A Genetic Algorithm for Software Design Migration from Structured to Object Oriented Paradigm | http://arxiv.org/pdf/1407.6116v1.pdf | author:Md. Selim, Saeed Siddik, Alim Ul Gias, M. Abdullah-Al-Wadud, Shah Mostafa Khaled category:cs.SE cs.NE published:2014-07-23 summary:The potential benefit of migrating software design from Structured to ObjectOriented Paradigm is manifolded including modularity, manageability andextendability. This design migration should be automated as it will reduce thetime required in manual process. Our previous work has addressed this issue interms of optimal graph clustering problem formulated by a quadratic IntegerProgram (IP). However, it has been realized that solution to the IP iscomputationally hard and thus heuristic based methods are required to get anear optimal solution. This paper presents a Genetic Algorithm (GA) for optimalclustering with an objective of maximizing intra-cluster edges whereasminimizing the inter-cluster ones. The proposed algorithm relies on fitnessbased parent selection and cross-overing cluster elements to reach an optimalsolution step by step. The scheme was implemented and tested against a set ofreal and synthetic data. The experimental results show that GA outperforms ourprevious works based on Greedy and Monte Carlo approaches by 40% and 49.5%.
arxiv-7800-240 | Autonomous requirements specification processing using natural language processing | http://arxiv.org/pdf/1407.6099v1.pdf | author:S. G. Macdonell, K. Min, A. M. Connor category:cs.CL cs.SE published:2014-07-23 summary:We describe our ongoing research that centres on the application of naturallanguage processing (NLP) to software engineering and systems developmentactivities. In particular, this paper addresses the use of NLP in therequirements analysis and systems design processes. We have developed aprototype toolset that can assist the systems analyst or software engineer toselect and verify terms relevant to a project. In this paper we describe theprocesses employed by the system to extract and classify objects of interestfrom requirements documents. These processes are illustrated using a smallexample.
arxiv-7800-241 | Stabilizing Sparse Cox Model using Clinical Structures in Electronic Medical Records | http://arxiv.org/pdf/1407.6094v1.pdf | author:Shivapratap Gopakumar, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML cs.LG published:2014-07-23 summary:Stability in clinical prediction models is crucial for transferabilitybetween studies, yet has received little attention. The problem is paramount inhigh dimensional data which invites sparse models with feature selectioncapability. We introduce an effective method to stabilize sparse Cox model oftime-to-events using clinical structures inherent in Electronic MedicalRecords. Model estimation is stabilized using a feature graph derived from twotypes of EMR structures: temporal structure of disease and interventionrecurrences, and hierarchical structure of medical knowledge and practices. Wedemonstrate the efficacy of the method in predicting time-to-readmission ofheart failure patients. On two stability measures - the Jaccard index and theConsistency index - the use of clinical structures significantly increasedfeature stability without hurting discriminative power. Our model reported acompetitive AUC of 0.64 (95% CIs: [0.58,0.69]) for 6 months prediction.
arxiv-7800-242 | Learning in games via reinforcement and regularization | http://arxiv.org/pdf/1407.6267v2.pdf | author:Panayotis Mertikopoulos, William H. Sandholm category:math.OC cs.GT cs.LG published:2014-07-23 summary:We investigate a class of reinforcement learning dynamics where playersadjust their strategies based on their actions' cumulative payoffs over time -specifically, by playing mixed strategies that maximize their expectedcumulative payoff minus a regularization term. A widely studied example isexponential reinforcement learning, a process induced by an entropicregularization term which leads mixed strategies to evolve according to thereplicator dynamics. However, in contrast to the class of regularizationfunctions used to define smooth best responses in models of stochasticfictitious play, the functions used in this paper need not be infinitely steepat the boundary of the simplex; in fact, dropping this requirement gives riseto an important dichotomy between steep and nonsteep cases. In this generalframework, we extend several properties of exponential learning, including theelimination of dominated strategies, the asymptotic stability of strict Nashequilibria, and the convergence of time-averaged trajectories in zero-sum gameswith an interior Nash equilibrium.
arxiv-7800-243 | FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation | http://arxiv.org/pdf/1407.6251v2.pdf | author:Philip Lenz, Andreas Geiger, Raquel Urtasun category:cs.CV published:2014-07-23 summary:One of the most popular approaches to multi-target tracking istracking-by-detection. Current min-cost flow algorithms which solve the dataassociation problem optimally have three main drawbacks: they arecomputationally expensive, they assume that the whole video is given as abatch, and they scale badly in memory and computation with the length of thevideo sequence. In this paper, we address each of these issues, resulting in acomputationally and memory-bounded solution. First, we introduce a dynamicversion of the successive shortest-path algorithm which solves the dataassociation problem optimally while reusing computation, resulting insignificantly faster inference than standard solvers. Second, we address theoptimal solution to the data association problem when dealing with an incomingstream of data (i.e., online setting). Finally, we present our maincontribution which is an approximate online solution with bounded memory andcomputation which is capable of handling videos of arbitrarily length whileperforming tracking in real time. We demonstrate the effectiveness of ouralgorithms on the KITTI and PETS2009 benchmarks and show state-of-the-artperformance, while being significantly faster than existing solvers.
arxiv-7800-244 | Novel and Automatic Parking Inventory System Based on Pattern Recognition and Directional Chain Code | http://arxiv.org/pdf/1407.6321v1.pdf | author:Reza Azad, Majid Nazari category:cs.CV published:2014-07-23 summary:The objective of this paper is to design an efficient vehicle license platerecognition System and to implement it for automatic parking inventory system.The system detects the vehicle first and then captures the image of the frontview of the vehicle. Vehicle license plate is localized and characters aresegmented. For finding the place of plate, a novel and real time method isexpressed. A new and robust technique based on directional chain code is usedfor character recognition. The resulting vehicle number is then compared withthe available database of all the vehicles so as to come up with informationabout the vehicle type and to charge entrance cost accordingly. The system isthen allowed to open parking barrier for the vehicle and generate entrance costreceipt. The vehicle information (such as entrance time, date, and cost amount)is also stored in the database to maintain the record. The hardware andsoftware integrated system is implemented and a working prototype model isdeveloped. Under the available database, the average accuracy of locatingvehicle license plate obtained 100%. Using 70% samples of character fortraining, we tested our scheme on whole samples and obtained 100% correctrecognition rate. Further we tested our character recognition stage on Persianvehicle data set and we achieved 99% correct recognition.
arxiv-7800-245 | A robust and adaptable method for face detection based on Color Probabilistic Estimation Technique | http://arxiv.org/pdf/1407.6318v1.pdf | author:Reza Azad, Fatemeh Davami category:cs.CV published:2014-07-23 summary:Human face perception is currently an active research area in the computervision community. Skin detection is one of the most important and primarystages for this purpose. So far, many approaches are proposed to done thiscase. Near all of these methods have tried to find best match intensitydistribution with skin pixels based on popular color spaces such as RGB, HSI orYCBCR. Results show that these methods cannot provide an accurate approach forevery kind of skin. In this paper, an approach is proposed to solve thisproblem using a color probabilistic estimation technique. This approach isincluding two stages. In the first one, the skin intensity distribution isestimated using some train photos of pure skin, and at the second stage, theskin pixels are detected using Gaussian model and optimal threshold tuning.Then from the skin region facial features have been extracted to get the facefrom the skin region. In the results section, the proposed approach is appliedon FEI database and the accuracy rate reached 99.25%. The proposed approach canbe used for all kinds of skin using train stage which is the main advantageamong the other advantages, such as Low noise sensitivity and low computationalcomplexity.
arxiv-7800-246 | Joint Energy-based Detection and Classificationon of Multilingual Text Lines | http://arxiv.org/pdf/1407.6082v1.pdf | author:Igor Milevskiy, Yuri Boykov category:cs.CV published:2014-07-23 summary:This paper proposes a new hierarchical MDL-based model for a joint detectionand classi?cation of multilingual text lines in im- ages taken by hand-heldcameras. The majority of related text detec- tion methods assume alphabet-basedwriting in a single language, e.g. in Latin. They use simple clusteringheuristics speci?c to such texts: prox- imity between letters within one line,larger distance between separate lines, etc. We are interested in asignificantly more ambiguous problem where images combine alphabet andlogographic characters from multiple languages and typographic rules vary a lot(e.g. English, Korean, and Chinese). Complexity of detecting and classifyingtext lines in multiple languages calls for a more principled approach based oninformation- theoretic principles. Our new MDL model includes data costscombining geometric errors with classi?cation likelihoods and a hierarchicalsparsity term based on label costs. This energy model can be e?cientlyminimized by fusion moves. We demonstrate robustness of the proposed algorithmon a large new database of multilingual text images collected in the pub- lictransit system of Seoul.
arxiv-7800-247 | Learning Rank Functionals: An Empirical Study | http://arxiv.org/pdf/1407.6089v2.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.IR cs.LG stat.ML published:2014-07-23 summary:Ranking is a key aspect of many applications, such as information retrieval,question answering, ad placement and recommender systems. Learning to rank hasthe goal of estimating a ranking model automatically from training data. Inpractical settings, the task often reduces to estimating a rank functional ofan object with respect to a query. In this paper, we investigate key issues indesigning an effective learning to rank algorithm. These include datarepresentation, the choice of rank functionals, the design of the loss functionso that it is correlated with the rank metrics used in evaluation. For the lossfunction, we study three techniques: approximating the rank metric by a smoothfunction, decomposition of the loss into a weighted sum of element-wise lossesand into a weighted sum of pairwise losses. We then present derivations ofpiecewise losses using the theory of high-order Markov chains and Markov randomfields. In experiments, we evaluate these design aspects on two tasks: answerranking in a Social Question Answering site, and Web Information Retrieval.
arxiv-7800-248 | Quadratically constrained quadratic programming for classification using particle swarms and applications | http://arxiv.org/pdf/1407.6315v1.pdf | author:Deepak Kumar, A G Ramakrishnan category:cs.AI cs.LG cs.NE math.OC published:2014-07-23 summary:Particle swarm optimization is used in several combinatorial optimizationproblems. In this work, particle swarms are used to solve quadratic programmingproblems with quadratic constraints. The approach of particle swarms is anexample for interior point methods in optimization as an iterative technique.This approach is novel and deals with classification problems without the useof a traditional classifier. Our method determines the optimal hyperplane orclassification boundary for a data set. In a binary classification problem, weconstrain each class as a cluster, which is enclosed by an ellipsoid. Theestimation of the optimal hyperplane between the two clusters is posed as aquadratically constrained quadratic problem. The optimization problem is solvedin distributed format using modified particle swarms. Our method has theadvantage of using the direction towards optimal solution rather than searchingthe entire feasible region. Our results on the Iris, Pima, Wine, and Thyroiddatasets show that the proposed method works better than a neural network andthe performance is close to that of SVM.
arxiv-7800-249 | scikit-image: Image processing in Python | http://arxiv.org/pdf/1407.6245v1.pdf | author:Stefan van der Walt, Johannes L. SchÃ¶nberger, Juan Nunez-Iglesias, FranÃ§ois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, the scikit-image contributors category:cs.MS cs.CV published:2014-07-23 summary:scikit-image is an image processing library that implements algorithms andutilities for use in research, education and industry applications. It isreleased under the liberal "Modified BSD" open source license, provides awell-documented API in the Python programming language, and is developed by anactive, international team of collaborators. In this paper we highlight theadvantages of open source to achieve the goals of the scikit-image library, andwe showcase several real-world image processing applications that usescikit-image.
arxiv-7800-250 | Aggregation of local parametric candidates with exemplar-based occlusion handling for optical flow | http://arxiv.org/pdf/1407.5759v1.pdf | author:Denis Fortun, Patrick Bouthemy, Charles Kervrann category:cs.CV published:2014-07-22 summary:Handling all together large displacements, motion details and occlusionsremains an open issue for reliable computation of optical flow in a videosequence. We propose a two-step aggregation paradigm to address this problem.The idea is to supply local motion candidates at every pixel in a first step,and then to combine them to determine the global optical flow field in a secondstep. We exploit local parametric estimations combined with patchcorrespondences and we experimentally demonstrate that they are sufficient toproduce highly accurate motion candidates. The aggregation step is designed asthe discrete optimization of a global regularized energy. The occlusion map isestimated jointly with the flow field throughout the two steps. We propose ageneric exemplar-based approach for occlusion filling with motion vectors. Weachieve state-of-the-art results in computer vision benchmarks, withparticularly significant improvements in the case of large displacements andocclusions.
arxiv-7800-251 | Artificial Life and the Web: WebAL Comes of Age | http://arxiv.org/pdf/1407.5719v1.pdf | author:Tim Taylor category:cs.NE cs.MA published:2014-07-22 summary:A brief survey is presented of the first 18 years of web-based ArtificialLife ("WebAL") research and applications, covering the period 1995-2013. Thesurvey is followed by a short discussion of common methodologies employed andcurrent technologies relevant to WebAL research. The paper concludes with aquick look at what the future may hold for work in this exciting area.
arxiv-7800-252 | Learning Rich Features from RGB-D Images for Object Detection and Segmentation | http://arxiv.org/pdf/1407.5736v1.pdf | author:Saurabh Gupta, Ross Girshick, Pablo ArbelÃ¡ez, Jitendra Malik category:cs.CV cs.RO published:2014-07-22 summary:In this paper we study the problem of object detection for RGB-D images usingsemantically rich image and depth features. We propose a new geocentricembedding for depth images that encodes height above ground and angle withgravity for each pixel in addition to the horizontal disparity. We demonstratethat this geocentric embedding works better than using raw depth images forlearning feature representations with convolutional neural networks. Our finalobject detection system achieves an average precision of 37.3%, which is a 56%relative improvement over existing methods. We then focus on the task ofinstance segmentation where we label pixels belonging to object instances foundby our detector. For this task, we propose a decision forest approach thatclassifies pixels in the detection window as foreground or background using afamily of unary and binary tests that query shape and geocentric pose features.Finally, we use the output from our object detectors in an existing superpixelclassification framework for semantic scene segmentation and achieve a 24%relative improvement over current state-of-the-art for the object categoriesthat we study. We believe advances such as those represented in this paper willfacilitate the use of perception in fields like robotics.
arxiv-7800-253 | Tree-based iterated local search for Markov random fields with applications in image analysis | http://arxiv.org/pdf/1407.5754v1.pdf | author:Truyen Tran, Dinh Phung, Svetha Venkatesh category:cs.AI cs.CV math.OC published:2014-07-22 summary:The \emph{maximum a posteriori} (MAP) assignment for general structure Markovrandom fields (MRFs) is computationally intractable. In this paper, we exploittree-based methods to efficiently address this problem. Our novel method, namedTree-based Iterated Local Search (T-ILS) takes advantage of the tractability oftree-structures embedded within MRFs to derive strong local search in an ILSframework. The method efficiently explores exponentially large neighborhood anddoes so with limited memory without any requirement on the cost functions. Weevaluate the T-ILS in a simulation of Ising model and two real-world problemsin computer vision: stereo matching, image denoising. Experimental resultsdemonstrate that our methods are competitive against state-of-the-art rivalswith a significant computational gain.
arxiv-7800-254 | Multi-agents adaptive estimation and coverage control using Gaussian regression | http://arxiv.org/pdf/1407.5807v1.pdf | author:Andrea Carron, Marco Todescato, Ruggero Carli, Luca Schenato, Gianluigi Pillonetto category:cs.MA cs.SY stat.ML published:2014-07-22 summary:We consider a scenario where the aim of a group of agents is to perform theoptimal coverage of a region according to a sensory function. In particular,centroidal Voronoi partitions have to be computed. The difficulty of the taskis that the sensory function is unknown and has to be reconstructed on linefrom noisy measurements. Hence, estimation and coverage needs to be performedat the same time. We cast the problem in a Bayesian regression framework, wherethe sensory function is seen as a Gaussian random field. Then, we design a setof control inputs which try to well balance coverage and estimation, alsodiscussing convergence properties of the algorithm. Numerical experiments showthe effectivness of the new approach.
arxiv-7800-255 | Approximate Regularization Path for Nuclear Norm Based H2 Model Reduction | http://arxiv.org/pdf/1407.5820v1.pdf | author:Niclas Blomberg, Cristian R. Rojas, Bo Wahlberg category:cs.SY math.OC stat.ML published:2014-07-22 summary:This paper concerns model reduction of dynamical systems using the nuclearnorm of the Hankel matrix to make a trade-off between model fit and modelcomplexity. This results in a convex optimization problem where this trade-offis determined by one crucial design parameter. The main contribution is amethodology to approximately calculate all solutions up to a certain toleranceto the model reduction problem as a function of the design parameter. This iscalled the regularization path in sparse estimation and is a very importanttool in order to find the appropriate balance between fit and complexity. Weextend this to the more complicated nuclear norm case. The key idea is todetermine when to exactly calculate the optimal solution using an upper boundbased on the so-called duality gap. Hence, by solving a fixed number ofoptimization problems the whole regularization path up to a given tolerance canbe efficiently computed. We illustrate this approach on some numericalexamples.
arxiv-7800-256 | Global optimization using LÃ©vy flights | http://arxiv.org/pdf/1407.5739v1.pdf | author:Truyen Tran, Trung Thanh Nguyen, Hoang Linh Nguyen category:cs.NE published:2014-07-22 summary:This paper studies a class of enhanced diffusion processes in which randomwalkers perform L\'evy flights and apply it for global optimization. L\'evyflights offer controlled balance between exploitation and exploration. Wedevelop four optimization algorithms based on such properties. We compare newalgorithms with the well-known Simulated Annealing on hard test functions andthe results are very promising.
arxiv-7800-257 | Sequential Changepoint Approach for Online Community Detection | http://arxiv.org/pdf/1407.5978v3.pdf | author:David Marangoni-Simonsen, Yao Xie category:stat.ML cs.LG cs.SI math.ST stat.TH published:2014-07-22 summary:We present new algorithms for detecting the emergence of a community in largenetworks from sequential observations. The networks are modeled usingErdos-Renyi random graphs with edges forming between nodes in the communitywith higher probability. Based on statistical changepoint detectionmethodology, we develop three algorithms: the Exhaustive Search (ES), themixture, and the Hierarchical Mixture (H-Mix) methods. Performance of thesemethods is evaluated by the average run length (ARL), which captures thefrequency of false alarms, and the detection delay. Numerical comparisons showthat the ES method performs the best; however, it is exponentially complex. Themixture method is polynomially complex by exploiting the fact that the size ofthe community is typically small in a large network. However, it may react to agroup of active edges that do not form a community. This issue is resolved bythe H-Mix method, which is based on a dendrogram decomposition of the network.We present an asymptotic analytical expression for ARL of the mixture methodwhen the threshold is large. Numerical simulation verifies that ourapproximation is accurate even in the non-asymptotic regime. Hence, it can beused to determine a desired threshold efficiently. Finally, numerical examplesshow that the mixture and the H-Mix methods can both detect a community quicklywith a lower complexity than the ES method.
arxiv-7800-258 | Improved Onlooker Bee Phase in Artificial Bee Colony Algorithm | http://arxiv.org/pdf/1407.5753v1.pdf | author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.NE published:2014-07-22 summary:Artificial Bee Colony (ABC) is a distinguished optimization strategy that canresolve nonlinear and multifaceted problems. It is comparatively astraightforward and modern population based probabilistic approach forcomprehensive optimization. In the vein of the other population basedalgorithms, ABC is moreover computationally classy due to its slow nature ofsearch procedure. The solution exploration equation of ABC is extensivelyinfluenced by a arbitrary quantity which helps in exploration at the cost ofexploitation of the better search space. In the solution exploration equationof ABC due to the outsized step size the chance of skipping the factualsolution is high. Therefore, here this paper improve onlooker bee phase withhelp of a local search strategy inspired by memetic algorithm to balance thediversity and convergence capability of the ABC. The proposed algorithm isnamed as Improved Onlooker Bee Phase in ABC (IoABC). It is tested over 12 wellknown un-biased test problems of diverse complexities and two engineeringoptimization problems; results show that the anticipated algorithm go onebetter than the basic ABC and its recent deviations in a good number of theexperiments.
arxiv-7800-259 | Resolution-limit-free and local Non-negative Matrix Factorization quality functions for graph clustering | http://arxiv.org/pdf/1407.5924v1.pdf | author:Twan van Laarhoven, Elena Marchiori category:stat.ML published:2014-07-22 summary:Many graph clustering quality functions suffer from a resolution limit, theinability to find small clusters in large graphs. So calledresolution-limit-free quality functions do not have this limit. This propertywas previously introduced for hard clustering, that is, graph partitioning. We investigate the resolution-limit-free property in the context ofNon-negative Matrix Factorization (NMF) for hard and soft graph clustering. Touse NMF in the hard clustering setting, a common approach is to assign eachnode to its highest membership cluster. We show that in this case symmetric NMFis not resolution-limit-free, but that it becomes so when hardness constraintsare used as part of the optimization. The resulting function is strongly linkedto the Constant Potts Model. In soft clustering, nodes can belong to more thanone cluster, with varying degrees of membership. In this settingresolution-limit-free turns out to be too strong a property. Therefore weintroduce locality, which roughly states that changing one part of the graphdoes not affect the clustering of other parts of the graph. We argue that thisis a desirable property, provide conditions under which NMF quality functionsare local, and propose a novel class of local probabilistic NMF qualityfunctions for soft graph clustering.
arxiv-7800-260 | Deep Recurrent Neural Networks for Time Series Prediction | http://arxiv.org/pdf/1407.5949v2.pdf | author:Sharat C. Prasad, Piyush Prasad category:cs.NE published:2014-07-22 summary:Ability of deep networks to extract high level features and of recurrentnetworks to perform time-series inference have been studied. In view ofuniversality of one hidden layer network at approximating functions under weakconstraints, the benefit of multiple layers is to enlarge the space ofdynamical systems approximated or, given the space, reduce the number of unitsrequired for a certain error. Traditionally shallow networks with manuallyengineered features are used, back-propagation extent is limited to one andattempt to choose a large number of hidden units to satisfy the Markovcondition is made. In case of Markov models, it has been shown that manysystems need to be modeled as higher order. In the present work, we presentdeep recurrent networks with longer backpropagation through time extent as asolution to modeling systems that are high order and to predicting ahead. Westudy epileptic seizure suppression electro-stimulator. Extraction of manuallyengineered complex features and prediction employing them has not allowed smalllow-power implementations as, to avoid possibility of surgery, extraction ofany features that may be required has to be included. In this solution, arecurrent neural network performs both feature extraction and prediction. Weprove analytically that adding hidden layers or increasing backpropagationextent increases the rate of decrease of approximation error. A DynamicProgramming (DP) training procedure employing matrix operations is derived. DPand use of matrix operations makes the procedure efficient particularly whenusing data-parallel computing. The simulation studies show the geometry of theparameter space, that the network learns the temporal structure, thatparameters converge while model output displays same dynamic behavior as thesystem and greater than .99 Average Detection Rate on all real seizure datatried.
arxiv-7800-261 | The U-curve optimization problem: improvements on the original algorithm and time complexity analysis | http://arxiv.org/pdf/1407.6067v1.pdf | author:Marcelo S. Reis, Carlos E. Ferreira, Junior Barrera category:cs.LG cs.CV 68T10 I.5.2 published:2014-07-22 summary:The U-curve optimization problem is characterized by a decomposable inU-shaped curves cost function over the chains of a Boolean lattice. Thisproblem can be applied to model the classical feature selection problem inMachine Learning. Recently, the U-Curve algorithm was proposed to give optimalsolutions to the U-curve problem. In this article, we point out that theU-Curve algorithm is in fact suboptimal, and introduce the U-Curve-Search (UCS)algorithm, which is actually optimal. We also present the results of optimaland suboptimal experiments, in which UCS is compared with the UBB optimalbranch-and-bound algorithm and the SFFS heuristic, respectively. We show that,in both experiments, $\proc{UCS}$ had a better performance than its competitor.Finally, we analyze the obtained results and point out improvements on UCS thatmight enhance the performance of this algorithm.
arxiv-7800-262 | Detection of Sclerotic Spine Metastases via Random Aggregation of Deep Convolutional Neural Network Classifications | http://arxiv.org/pdf/1407.5976v1.pdf | author:Holger R. Roth, Jianhua Yao, Le Lu, James Stieger, Joseph E. Burns, Ronald M. Summers category:cs.CV published:2014-07-22 summary:Automated detection of sclerotic metastases (bone lesions) in ComputedTomography (CT) images has potential to be an important tool in clinicalpractice and research. State-of-the-art methods show performance of 79%sensitivity or true-positive (TP) rate, at 10 false-positives (FP) per volume.We design a two-tiered coarse-to-fine cascade framework to first operate ahighly sensitive candidate generation system at a maximum sensitivity of ~92%but with high FP level (~50 per patient). Regions of interest (ROI) for lesioncandidates are generated in this step and function as input for the secondtier. In the second tier we generate N 2D views, via scale, randomtranslations, and rotations with respect to each ROI centroid coordinates.These random views are used to train a deep Convolutional Neural Network (CNN)classifier. In testing, the CNN is employed to assign individual probabilitiesfor a new set of N random views that are averaged at each ROI to compute afinal per-candidate classification probability. This second tier behaves as ahighly selective process to reject difficult false positives while preservinghigh sensitivities. We validate the approach on CT images of 59 patients (49with sclerotic metastases and 10 normal controls). The proposed method reducesthe number of FP/vol. from 4 to 1.2, 7 to 3, and 12 to 9.5 when comparing asensitivity rates of 60%, 70%, and 80% respectively in testing. TheArea-Under-the-Curve (AUC) is 0.834. The results show marked improvement uponprevious work.
arxiv-7800-263 | Modeling languages from graph networks | http://arxiv.org/pdf/1407.6027v1.pdf | author:Alberto Besana, Cristina MartÃ­nez category:cs.CL math.CO published:2014-07-22 summary:We model and compute the probability distribution of the letters in randomgenerated words in a language by using the theory of set partitions, Youngtableaux and graph theoretical representation methods. This has been ofinterest for several application areas such as network systems, bioinformatics,internet search, data mining and computacional linguistics.
arxiv-7800-264 | Scalable Kernel Methods via Doubly Stochastic Gradients | http://arxiv.org/pdf/1407.5599v4.pdf | author:Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, Le Song category:cs.LG stat.ML published:2014-07-21 summary:The general perception is that kernel methods are not scalable, and neuralnets are the methods of choice for nonlinear learning problems. Or have wesimply not tried hard enough for kernel methods? Here we propose an approachthat scales up kernel methods using a novel concept called "doubly stochasticfunctional gradients". Our approach relies on the fact that many kernel methodscan be expressed as convex optimization problems, and we solve the problems bymaking two unbiased stochastic approximations to the functional gradient, oneusing random training points and another using random functions associated withthe kernel, and then descending using this noisy functional gradient. We showthat a function produced by this procedure after $t$ iterations converges tothe optimal function in the reproducing kernel Hilbert space in rate $O(1/t)$,and achieves a generalization performance of $O(1/\sqrt{t})$. This doublystochasticity also allows us to avoid keeping the support vectors and toimplement the algorithm in a small memory footprint, which is linear in numberof iterations and independent of data dimension. Our approach can readily scalekernel methods up to the regimes which are dominated by neural nets. We showthat our method can achieve competitive performance to neural nets in datasetssuch as 8 million handwritten digits from MNIST, 2.3 million energy materialsfrom MolecularSpace, and 1 million photos from ImageNet.
arxiv-7800-265 | Predictive support recovery with TV-Elastic Net penalty and logistic regression: an application to structural MRI | http://arxiv.org/pdf/1407.5602v1.pdf | author:Mathieu Dubois, Fouad Hadj-Selem, Tommy Lofstedt, Matthieu Perrot, Clara Fischer, Vincent Frouin, Edouard Duchesnay category:stat.ML published:2014-07-21 summary:The use of machine-learning in neuroimaging offers new perspectives in earlydiagnosis and prognosis of brain diseases. Although such multivariate methodscan capture complex relationships in the data, traditional approaches provideirregular (l2 penalty) or scattered (l1 penalty) predictive pattern with a verylimited relevance. A penalty like Total Variation (TV) that exploits thenatural 3D structure of the images can increase the spatial coherence of theweight map. However, TV penalization leads to non-smooth optimization problemsthat are hard to minimize. We propose an optimization framework that minimizesany combination of l1, l2, and TV penalties while preserving the exact l1penalty. This algorithm uses Nesterov's smoothing technique to approximate theTV penalty with a smooth function such that the loss and the penalties areminimized with an exact accelerated proximal gradient algorithm. We propose anoriginal continuation algorithm that uses successively smaller values of thesmoothing parameter to reach a prescribed precision while achieving the bestpossible convergence rate. This algorithm can be used with other losses orpenalties. The algorithm is applied on a classification problem on the ADNIdataset. We observe that the TV penalty does not necessarily improve theprediction but provides a major breakthrough in terms of support recovery ofthe predictive brain regions.
arxiv-7800-266 | PGMHD: A Scalable Probabilistic Graphical Model for Massive Hierarchical Data Problems | http://arxiv.org/pdf/1407.5656v2.pdf | author:Khalifeh AlJadda, Mohammed Korayem, Camilo Ortiz, Trey Grainger, John A. Miller, William S. York category:cs.AI cs.LG published:2014-07-21 summary:In the big data era, scalability has become a crucial requirement for anyuseful computational model. Probabilistic graphical models are very useful formining and discovering data insights, but they are not scalable enough to besuitable for big data problems. Bayesian Networks particularly demonstrate thislimitation when their data is represented using few random variables while eachrandom variable has a massive set of values. With hierarchical data - data thatis arranged in a treelike structure with several levels - one would expect tosee hundreds of thousands or millions of values distributed over even just asmall number of levels. When modeling this kind of hierarchical data acrosslarge data sets, Bayesian networks become infeasible for representing theprobability distributions for the following reasons: i) Each level represents asingle random variable with hundreds of thousands of values, ii) The number oflevels is usually small, so there are also few random variables, and iii) Thestructure of the network is predefined since the dependency is modeled top-downfrom each parent to each of its child nodes, so the network would contain asingle linear path for the random variables from each parent to each childnode. In this paper we present a scalable probabilistic graphical model toovercome these limitations for massive hierarchical data. We believe theproposed model will lead to an easily-scalable, more readable, and expressiveimplementation for problems that require probabilistic-based solutions formassive amounts of hierarchical data. We successfully applied this model tosolve two different challenging probabilistic-based problems on massivehierarchical data sets for different domains, namely, bioinformatics and latentsemantic discovery over search logs.
arxiv-7800-267 | Certifying the Existence of Epipolar Matrices | http://arxiv.org/pdf/1407.5367v1.pdf | author:Sameer Agarwal, Hon-leung Lee, Bernd Sturmfels, Rekha R. Thomas category:cs.CV math.AG published:2014-07-21 summary:Given a set of point correspondences in two images, the existence of afundamental matrix is a necessary condition for the points to be the images ofa 3-dimensional scene imaged with two pinhole cameras. If the cameracalibration is known then one requires the existence of an essential matrix. We present an efficient algorithm, using exact linear algebra, for testingthe existence of a fundamental matrix. The input is any number of pointcorrespondences. For essential matrices, we characterize the solvability of theDemazure polynomials. In both scenarios, we determine which linear subspacesintersect a fixed set defined by non-linear polynomials. The conditions wederive are polynomials stated purely in terms of image coordinates. Theyrepresent a new class of two-view invariants, free of fundamental(resp.~essential)~matrices.
arxiv-7800-268 | A Novel Hybrid Crossover based Artificial Bee Colony Algorithm for Optimization Problem | http://arxiv.org/pdf/1407.5574v1.pdf | author:Sandeep Kumar, Vivek Kumar Sharma, Rajani Kumari category:cs.AI cs.NE published:2014-07-21 summary:Artificial bee colony (ABC) algorithm has proved its importance in solving anumber of problems including engineering optimization problems. ABC algorithmis one of the most popular and youngest member of the family of populationbased nature inspired meta-heuristic swarm intelligence method. ABC has beenproved its superiority over some other Nature Inspired Algorithms (NIA) whenapplied for both benchmark functions and real world problems. The performanceof search process of ABC depends on a random value which tries to balanceexploration and exploitation phase. In order to increase the performance it isrequired to balance the exploration of search space and exploitation of optimalsolution of the ABC. This paper outlines a new hybrid of ABC algorithm withGenetic Algorithm. The proposed method integrates crossover operation fromGenetic Algorithm (GA) with original ABC algorithm. The proposed method isnamed as Crossover based ABC (CbABC). The CbABC strengthens the exploitationphase of ABC as crossover enhances exploration of search space. The CbABCtested over four standard benchmark functions and a popular continuousoptimization problem.
arxiv-7800-269 | Multichannel Compressive Sensing MRI Using Noiselet Encoding | http://arxiv.org/pdf/1407.5536v2.pdf | author:Kamlesh Pawar, Gary F. Egan, Jingxin Zhang category:physics.med-ph cs.CV published:2014-07-21 summary:The incoherence between measurement and sparsifying transform matrices andthe restricted isometry property (RIP) of measurement matrix are two of the keyfactors in determining the performance of compressive sensing (CS). In CS-MRI,the randomly under-sampled Fourier matrix is used as the measurement matrix andthe wavelet transform is usually used as sparsifying transform matrix. However,the incoherence between the randomly under-sampled Fourier matrix and thewavelet matrix is not optimal, which can deteriorate the performance of CS-MRI.Using the mathematical result that noiselets are maximally incoherent withwavelets, this paper introduces the noiselet unitary bases as the measurementmatrix to improve the incoherence and RIP in CS-MRI, and presents a method todesign the pulse sequence for the noiselet encoding. This novel encoding schemeis combined with the multichannel compressive sensing (MCS) framework to takethe advantage of multichannel data acquisition used in MRI scanners. Anempirical RIP analysis is presented to compare the multichannel noiselet andmultichannel Fourier measurement matrices in MCS. Simulations are presented inthe MCS framework to compare the performance of noiselet encodingreconstructions and Fourier encoding reconstructions at different accelerationfactors. The comparisons indicate that multichannel noiselet measurement matrixhas better RIP than that of its Fourier counterpart, and that noiselet encodedMCS-MRI outperforms Fourier encoded MCS-MRI in preserving image resolution andcan achieve higher acceleration factors. To demonstrate the feasibility of theproposed noiselet encoding scheme, two pulse sequences with tailored spatiallyselective RF excitation pulses was designed and implemented on a 3T scanner toacquire the data in the noiselet domain from a phantom and a human brain.
arxiv-7800-270 | Practical Kernel-Based Reinforcement Learning | http://arxiv.org/pdf/1407.5358v1.pdf | author:AndrÃ© M. S. Barreto, Doina Precup, Joelle Pineau category:cs.LG cs.AI stat.ML published:2014-07-21 summary:Kernel-based reinforcement learning (KBRL) stands out among reinforcementlearning algorithms for its strong theoretical guarantees. By casting thelearning problem as a local kernel approximation, KBRL provides a way ofcomputing a decision policy which is statistically consistent and converges toa unique solution. Unfortunately, the model constructed by KBRL grows with thenumber of sample transitions, resulting in a computational cost that precludesits application to large-scale or on-line domains. In this paper we introducean algorithm that turns KBRL into a practical reinforcement learning tool.Kernel-based stochastic factorization (KBSF) builds on a simple idea: when atransition matrix is represented as the product of two stochastic matrices, onecan swap the factors of the multiplication to obtain another transition matrix,potentially much smaller, which retains some fundamental properties of itsprecursor. KBSF exploits such an insight to compress the information containedin KBRL's model into an approximator of fixed size. This makes it possible tobuild an approximation that takes into account both the difficulty of theproblem and the associated computational cost. KBSF's computational complexityis linear in the number of sample transitions, which is the best one can dowithout discarding data. Moreover, the algorithm's simple mechanics allow for afully incremental implementation that makes the amount of memory usedindependent of the number of sample transitions. The result is a kernel-basedreinforcement learning algorithm that can be applied to large-scale problems inboth off-line and on-line regimes. We derive upper bounds for the distancebetween the value functions computed by KBRL and KBSF using the same data. Wealso illustrate the potential of our algorithm in an extensive empirical studyin which KBSF is applied to difficult tasks based on real-world data.
arxiv-7800-271 | Are There Good Mistakes? A Theoretical Analysis of CEGIS | http://arxiv.org/pdf/1407.5397v1.pdf | author:Susmit Jha, Sanjit A. Seshia category:cs.LO cs.AI cs.LG cs.PL published:2014-07-21 summary:Counterexample-guided inductive synthesis CEGIS is used to synthesizeprograms from a candidate space of programs. The technique is guaranteed toterminate and synthesize the correct program if the space of candidate programsis finite. But the technique may or may not terminate with the correct programif the candidate space of programs is infinite. In this paper, we perform atheoretical analysis of counterexample-guided inductive synthesis technique. Weinvestigate whether the set of candidate spaces for which the correct programcan be synthesized using CEGIS depends on the counterexamples used in inductivesynthesis, that is, whether there are good mistakes which would increase thesynthesis power. We investigate whether the use of minimal counterexamplesinstead of arbitrary counterexamples expands the set of candidate spaces ofprograms for which inductive synthesis can successfully synthesize a correctprogram. We consider two kinds of counterexamples: minimal counterexamples andhistory bounded counterexamples. The history bounded counterexample used in anyiteration of CEGIS is bounded by the examples used in previous iterations ofinductive synthesis. We examine the relative change in power of inductivesynthesis in both cases. We show that the synthesis technique using minimalcounterexamples MinCEGIS has the same synthesis power as CEGIS but thesynthesis technique using history bounded counterexamples HCEGIS has differentpower than that of CEGIS, but none dominates the other.
arxiv-7800-272 | Optimized Method for Iranian Road Signs Detection and recognition system | http://arxiv.org/pdf/1407.5324v1.pdf | author:Reza Azad, Babak Azad, Iman Tavakoli Kazerooni category:cs.CV published:2014-07-20 summary:Road sign recognition is one of the core technologies in IntelligentTransport Systems. In the current study, a robust and real-time method ispresented to identify and detect the roads speed signs in road image indifferent situations. In our proposed method, first, the connected componentsare created in the main image using the edge detection and mathematicalmorphology and the location of the road signs extracted by the geometric andcolor data; then the letters are segmented and recognized by Multiclass SupportVector Machine (SVMs) classifiers. Regarding that the geometric and colorfeatures ate properly used in detection the location of the road signs, so itis not sensitive to the distance and noise and has higher speed and efficiency.In the result part, the proposed approach is applied on Iranian road speed signdatabase and the detection and recognition accuracy rate achieved 98.66% and100% respectively.
arxiv-7800-273 | Object Proposal Generation using Two-Stage Cascade SVMs | http://arxiv.org/pdf/1407.5242v1.pdf | author:Ziming Zhang, Philip H. S. Torr category:cs.CV published:2014-07-20 summary:Object proposal algorithms have shown great promise as a first step forobject recognition and detection. Good object proposal generation algorithmsrequire high object recall rate as well as low computational cost, becausegenerating object proposals is usually utilized as a preprocessing step. Theproblem of how to accelerate the object proposal generation and evaluationprocess without decreasing recall is thus of great interest. In this paper, wepropose a new object proposal generation method using two-stage cascade SVMs,where in the first stage linear filters are learned for predefined quantizedscales/aspect-ratios independently, and in the second stage a global linearclassifier is learned across all the quantized scales/aspect-ratios forcalibration, so that all the proposals can be compared properly. The proposalswith highest scores are our final output. Specifically, we explain ourscale/aspect-ratio quantization scheme, and investigate the effects ofcombinations of $\ell_1$ and $\ell_2$ regularizers in cascade SVMs with/withoutranking constraints in learning. Comprehensive experiments on VOC2007 datasetare conducted, and our results achieve the state-of-the-art performance withhigh object recall rate and high computational efficiency. Besides, our methodhas been demonstrated to be suitable for not only class-specific but alsogeneric object proposal generation.
arxiv-7800-274 | Feature and Region Selection for Visual Learning | http://arxiv.org/pdf/1407.5245v2.pdf | author:Ji Zhao, Liantao Wang, Ricardo Cabral, Fernando De la Torre category:cs.CV cs.LG published:2014-07-20 summary:Visual learning problems such as object classification and action recognitionare typically approached using extensions of the popular bag-of-words (BoW)model. Despite its great success, it is unclear what visual features the BoWmodel is learning: Which regions in the image or video are used to discriminateamong classes? Which are the most discriminative visual words? Answering thesequestions is fundamental for understanding existing BoW models and inspiringbetter models for visual recognition. To answer these questions, this paper presents a method for feature selectionand region selection in the visual BoW model. This allows for an intermediatevisualization of the features and regions that are important for visuallearning. The main idea is to assign latent weights to the features or regions,and jointly optimize these latent variables with the parameters of a classifier(e.g., support vector machine). There are four main benefits of our approach:(1) Our approach accommodates non-linear additive kernels such as the popular$\chi^2$ and intersection kernel; (2) our approach is able to handle bothregions in images and spatio-temporal regions in videos in a unified way; (3)the feature selection problem is convex, and both problems can be solved usinga scalable reduced gradient method; (4) we point out strong connections withmultiple kernel learning and multiple instance learning approaches.Experimental results in the PASCAL VOC 2007, MSR Action Dataset II and YouTubeillustrate the benefits of our approach.
arxiv-7800-275 | A Comparative Analysis for Determining the Optimal Path using PSO and GA | http://arxiv.org/pdf/1407.5327v1.pdf | author:Kavitha Sooda, T. R. Gopalakrishnan Nair category:cs.NI cs.NE published:2014-07-20 summary:Significant research has been carried out recently to find the optimal pathin network routing. Among them, the evolutionary algorithm approach is an areawhere work is carried out extensively. We in this paper have used particleswarm optimization (PSO) and genetic algorithm (GA) for finding the optimalpath and the concept of region based network is introduced along with the useof indirect encoding. We demonstrate the advantage of fitness value and hopcount in both PSO and GA. A comparative study of PSO and genetic algorithm (GA)is carried out, and it was found that PSO converged to arrive at the optimalpath much faster than GA.
arxiv-7800-276 | Context Aware Dynamic Traffic Signal Optimization | http://arxiv.org/pdf/1407.5212v1.pdf | author:Kandarp Khandwala, Rudra Sharma, Snehal Rao category:cs.AI cs.NE published:2014-07-19 summary:Conventional urban traffic control systems have been based on historicaltraffic data. Later advancements made use of detectors, which enabled thegathering of real time traffic data, in order to reorganize and calibratetraffic signalization programs. Further evolvement provided the ability toforecast traffic conditions, in order to develop traffic signalization programsand strategies precomputed and applied at the most appropriate time frame forthe optimal control of the current traffic conditions. We, propose the nextgeneration of traffic control systems based on principles of ArtificialIntelligence and Context Awareness. Most of the existing algorithms use averagewaiting time or length of the queue to assess an algorithms performance.However, a low average waiting time may come at the cost of delaying othervehicles indefinitely. In our algorithm, besides the vehicle queue, we usefairness also as an important performance metric to assess an algorithmsperformance.
arxiv-7800-277 | Exploiting Smoothness in Statistical Learning, Sequential Prediction, and Stochastic Optimization | http://arxiv.org/pdf/1407.5908v1.pdf | author:Mehrdad Mahdavi category:cs.LG published:2014-07-19 summary:In the last several years, the intimate connection between convexoptimization and learning problems, in both statistical and sequentialframeworks, has shifted the focus of algorithmic machine learning to examinethis interplay. In particular, on one hand, this intertwinement brings forwardnew challenges in reassessment of the performance of learning algorithmsincluding generalization and regret bounds under the assumptions imposed byconvexity such as analytical properties of loss functions (e.g., Lipschitzness,strong convexity, and smoothness). On the other hand, emergence of datasets ofan unprecedented size, demands the development of novel and more efficientoptimization algorithms to tackle large-scale learning problems. The overarching goal of this thesis is to reassess the smoothness of lossfunctions in statistical learning, sequential prediction/online learning, andstochastic optimization and explicate its consequences. In particular weexamine how smoothness of loss function could be beneficial or detrimental inthese settings in terms of sample complexity, statistical consistency, regretanalysis, and convergence rate, and investigate how smoothness can be leveragedto devise more efficient learning algorithms.
arxiv-7800-278 | Tight convex relaxations for sparse matrix factorization | http://arxiv.org/pdf/1407.5158v2.pdf | author:Emile Richard, Guillaume Obozinski, Jean-Philippe Vert category:stat.ML cs.LG math.ST stat.TH published:2014-07-19 summary:Based on a new atomic norm, we propose a new convex formulation for sparsematrix factorization problems in which the number of nonzero elements of thefactors is assumed fixed and known. The formulation counts sparse PCA withmultiple factors, subspace clustering and low-rank sparse bilinear regressionas potential applications. We compute slow rates and an upper bound on thestatistical dimension of the suggested norm for rank 1 matrices, showing thatits statistical dimension is an order of magnitude smaller than the usual$\ell\_1$-norm, trace norm and their combinations. Even though our convexformulation is in theory hard and does not lead to provably polynomial timealgorithmic schemes, we propose an active set algorithm leveraging thestructure of the convex problem to solve it and show promising numericalresults.
arxiv-7800-279 | Sparse and spurious: dictionary learning with noise and outliers | http://arxiv.org/pdf/1407.5155v4.pdf | author:RÃ©mi Gribonval, Rodolphe Jenatton, Francis Bach category:cs.LG stat.ML published:2014-07-19 summary:A popular approach within the signal processing and machine learningcommunities consists in modelling signals as sparse linear combinations ofatoms selected from a learned dictionary. While this paradigm has led tonumerous empirical successes in various fields ranging from image to audioprocessing, there have only been a few theoretical arguments supporting theseevidences. In particular, sparse coding, or sparse dictionary learning, relieson a non-convex procedure whose local minima have not been fully analyzed yet.In this paper, we consider a probabilistic model of sparse signals, and showthat, with high probability, sparse coding admits a local minimum around thereference dictionary generating the signals. Our study takes into account thecase of over-complete dictionaries, noisy signals, and possible outliers, thusextending previous work limited to noiseless settings and/or under-completedictionaries. The analysis we conduct is non-asymptotic and makes it possibleto understand how the key quantities of the problem, such as the coherence orthe level of noise, can scale with respect to the dimension of the signals, thenumber of atoms, the sparsity and the number of observations.
arxiv-7800-280 | Statistical Inference of Intractable Generative Models via Classification | http://arxiv.org/pdf/1407.4981v2.pdf | author:Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, Jukka Corander category:stat.CO stat.ME stat.ML published:2014-07-18 summary:Increasingly complex generative models are being used across the disciplinesas they allow for realistic characterization of data, but a common difficultywith them is the prohibitively large computational cost to performlikelihood-based statistical inference. We consider here a likelihood-freeframework where inference is done by identifying parameter values whichgenerate simulated data adequately resembling the observed data. A majordifficulty is how to measure the discrepancy between the simulated and observeddata. Transforming the original problem into a problem of classifying the datainto simulated versus observed, we find that classification accuracy can beused to assess the discrepancy. The complete arsenal of classification methodsbecomes thereby available for inference of intractable generative models.
arxiv-7800-281 | Pixels to Voxels: Modeling Visual Representation in the Human Brain | http://arxiv.org/pdf/1407.5104v1.pdf | author:Pulkit Agrawal, Dustin Stansbury, Jitendra Malik, Jack L. Gallant category:q-bio.NC cs.CV cs.NE published:2014-07-18 summary:The human brain is adept at solving difficult high-level visual processingproblems such as image interpretation and object recognition in natural scenes.Over the past few years neuroscientists have made remarkable progress inunderstanding how the human brain represents categories of objects and actionsin natural scenes. However, all current models of high-level human visionoperate on hand annotated images in which the objects and actions have beenassigned semantic tags by a human operator. No current models can account forhigh-level visual function directly in terms of low-level visual input (i.e.,pixels). To overcome this fundamental limitation we sought to develop a newclass of models that can predict human brain activity directly from low-levelvisual input (i.e., pixels). We explored two classes of models drawn fromcomputer vision and machine learning. The first class of models was based onFisher Vectors (FV) and the second was based on Convolutional Neural Networks(ConvNets). We find that both classes of models accurately predict brainactivity in high-level visual areas, directly from pixels and without the needfor any semantic tags or hand annotation of images. This is the first time thatsuch a mapping has been obtained. The fit models provide a new platform forexploring the functional principles of human vision, and they show that modernmethods of computer vision and machine learning provide important tools forcharacterizing brain function.
arxiv-7800-282 | LSDA: Large Scale Detection Through Adaptation | http://arxiv.org/pdf/1407.5035v3.pdf | author:Judy Hoffman, Sergio Guadarrama, Eric Tzeng, Ronghang Hu, Jeff Donahue, Ross Girshick, Trevor Darrell, Kate Saenko category:cs.CV published:2014-07-18 summary:A major challenge in scaling object detection is the difficulty of obtaininglabeled images for large numbers of categories. Recently, deep convolutionalneural networks (CNNs) have emerged as clear winners on object classificationbenchmarks, in part due to training with 1.2M+ labeled classification images.Unfortunately, only a small fraction of those labels are available for thedetection task. It is much cheaper and easier to collect large quantities ofimage-level labels from search engines than it is to collect detection data andlabel it with precise bounding boxes. In this paper, we propose Large ScaleDetection through Adaptation (LSDA), an algorithm which learns the differencebetween the two tasks and transfers this knowledge to classifiers forcategories without bounding box annotated data, turning them into detectors.Our method has the potential to enable detection for the tens of thousands ofcategories that lack bounding box annotations, yet have plenty ofclassification data. Evaluation on the ImageNet LSVRC-2013 detection challengedemonstrates the efficacy of our approach. This algorithm enables us to producea >7.6K detector by using available classification data from leaf nodes in theImageNet tree. We additionally demonstrate how to modify our architecture toproduce a fast detector (running at 2fps for the 7.6K detector). Models andsoftware are available at
arxiv-7800-283 | Bayesian Nonparametric Crowdsourcing | http://arxiv.org/pdf/1407.5017v1.pdf | author:Pablo G. Moreno, Yee Whye Teh, Fernando Perez-Cruz, Antonio ArtÃ©s-RodrÃ­guez category:stat.ML stat.AP published:2014-07-18 summary:Crowdsourcing has been proven to be an effective and efficient tool toannotate large datasets. User annotations are often noisy, so methods tocombine the annotations to produce reliable estimates of the ground truth arenecessary. We claim that considering the existence of clusters of users in thiscombination step can improve the performance. This is especially important inearly stages of crowdsourcing implementations, where the number of annotationsis low. At this stage there is not enough information to accurately estimatethe bias introduced by each annotator separately, so we have to resort tomodels that consider the statistical links among them. In addition, findingthese clusters is interesting in itself as knowing the behavior of the pool ofannotators allows implementing efficient active learning strategies. Based onthis, we propose in this paper two new fully unsupervised models based on aChinese Restaurant Process (CRP) prior and a hierarchical structure that allowsinferring these groups jointly with the ground truth and the properties of theusers. Efficient inference algorithms based on Gibbs sampling with auxiliaryvariables are proposed. Finally, we perform experiments, both on synthetic andreal databases, to show the advantages of our models over state-of-the-artalgorithms.
arxiv-7800-284 | A Comparative Study of Meta-heuristic Algorithms for Solving Quadratic Assignment Problem | http://arxiv.org/pdf/1407.4863v1.pdf | author:Gamal Abd El-Nasser A. Said, Abeer M. Mahmoud, El-Sayed M. El-Horbaty category:cs.AI cs.NE published:2014-07-18 summary:Quadratic Assignment Problem (QAP) is an NP-hard combinatorial optimizationproblem, therefore, solving the QAP requires applying one or more of themeta-heuristic algorithms. This paper presents a comparative study betweenMeta-heuristic algorithms: Genetic Algorithm, Tabu Search, and Simulatedannealing for solving a real-life (QAP) and analyze their performance in termsof both runtime efficiency and solution quality. The results show that GeneticAlgorithm has a better solution quality while Tabu Search has a fasterexecution time in comparison with other Meta-heuristic algorithms for solvingQAP.
arxiv-7800-285 | Analysis of Gait Pattern to Recognize the Human Activities | http://arxiv.org/pdf/1407.4867v2.pdf | author:Jay Prakash Gupta, Pushkar Dixit, Nishant Singh, Vijay Bhaskar Semwal category:cs.CV published:2014-07-18 summary:Human activity recognition based on the computer vision is the process oflabelling image sequences with action labels. Accurate systems for this problemare applied in areas such as visual surveillance, human computer interactionand video retrieval.
arxiv-7800-286 | Motor Learning Mechanism on the Neuron Scale | http://arxiv.org/pdf/1407.7027v1.pdf | author:Peilei Liu, Ting Wang category:q-bio.NC cs.NE physics.bio-ph published:2014-07-18 summary:Based on existing data, we wish to put forward a biological model of motorsystem on the neuron scale. Then we indicate its implications in statistics andlearning. Specifically, neuron firing frequency and synaptic strength areprobability estimates in essence. And the lateral inhibition also hasstatistical implications. From the standpoint of learning, dendriticcompetition through retrograde messengers is the foundation of conditionalreflex and grandmother cell coding. And they are the kernel mechanisms of motorlearning and sensory motor integration respectively. Finally, we compare motorsystem with sensory system. In short, we would like to bridge the gap betweenmolecule evidences and computational models.
arxiv-7800-287 | Affine Subspace Representation for Feature Description | http://arxiv.org/pdf/1407.4874v1.pdf | author:Zhenhua Wang, Bin Fan, Fuchao Wu category:cs.CV published:2014-07-18 summary:This paper proposes a novel Affine Subspace Representation (ASR) descriptorto deal with affine distortions induced by viewpoint changes. Unlike thetraditional local descriptors such as SIFT, ASR inherently encodes localinformation of multi-view patches, making it robust to affine distortions whilemaintaining a high discriminative ability. To this end, PCA is used torepresent affine-warped patches as PCA-patch vectors for its compactness andefficiency. Then according to the subspace assumption, which implies that thePCA-patch vectors of various affine-warped patches of the same keypoint can berepresented by a low-dimensional linear subspace, the ASR descriptor isobtained by using a simple subspace-to-point mapping. Such a linear subspacerepresentation could accurately capture the underlying information of akeypoint (local structure) under multiple views without sacrificing itsdistinctiveness. To accelerate the computation of ASR descriptor, a fastapproximate algorithm is proposed by moving the most computational part (ie,warp patch under various affine transformations) to an offline training stage.Experimental results show that ASR is not only better than the state-of-the-artdescriptors under various image transformations, but also performs well withouta dedicated affine invariant detector when dealing with viewpoint changes.
arxiv-7800-288 | Classification of Passes in Football Matches using Spatiotemporal Data | http://arxiv.org/pdf/1407.5093v1.pdf | author:Michael Horton, Joachim Gudmundsson, Sanjay Chawla, JoÃ«l Estephan category:cs.LG cs.CG I.5.2 published:2014-07-18 summary:A knowledgeable observer of a game of football (soccer) can make a subjectiveevaluation of the quality of passes made between players during the game. Weinvestigate the problem of producing an automated system to make the sameevaluation of passes. We present a model that constructs numerical predictorvariables from spatiotemporal match data using feature functions based onmethods from computational geometry, and then learns a classification functionfrom labelled examples of the predictor variables. Furthermore, the learnedclassifiers are analysed to determine if there is a relationship between thecomplexity of the algorithm that computed the predictor variable and theimportance of the variable to the classifier. Experimental results show that weare able to produce a classifier with 85.8% accuracy on classifying passes asGood, OK or Bad, and that the predictor variables computed using complexmethods from computational geometry are of moderate importance to the learnedclassifiers. Finally, we show that the inter-rater agreement on passclassification between the machine classifier and a human observer is ofsimilar magnitude to the agreement between two observers.
arxiv-7800-289 | Hand Pointing Detection Using Live Histogram Template of Forehead Skin | http://arxiv.org/pdf/1407.4898v1.pdf | author:Ghassem Tofighi, Nasser Ali Afarin, Kamraan Raahemifar, Anastasios N. Venetsanopoulos category:cs.CV published:2014-07-18 summary:Hand pointing detection has multiple applications in many fields such asvirtual reality and control devices in smart homes. In this paper, we proposeda novel approach to detect pointing vector in 2D space of a room. Afterbackground subtraction, face and forehead is detected. In the second step,forehead skin H-S plane histograms in HSV space is calculated. By using thesehistogram templates of users skin, and back projection method, skin areas aredetected. The contours of hand are extracted using Freeman chain codealgorithm. Next step is finding fingertips. Points in hand contour which arecandidates for the fingertip can be found in convex defects of convex hull andcontour. We introduced a novel method for finding the fingertip based on thespecial points on the contour and their relationships. Our approach detectshand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.
arxiv-7800-290 | Deep Metric Learning for Practical Person Re-Identification | http://arxiv.org/pdf/1407.4979v1.pdf | author:Dong Yi, Zhen Lei, Stan Z. Li category:cs.CV cs.LG cs.NE published:2014-07-18 summary:Various hand-crafted features and metric learning methods prevail in thefield of person re-identification. Compared to these methods, this paperproposes a more general way that can learn a similarity metric from imagepixels directly. By using a "siamese" deep neural network, the proposed methodcan jointly learn the color feature, texture feature and metric in a unifiedframework. The network has a symmetry structure with two sub-networks which areconnected by Cosine function. To deal with the big variations of person images,binomial deviance is used to evaluate the cost between similarities and labels,which is proved to be robust to outliers. Compared to existing researches, a more practical setting is studied in theexperiments that is training and test on different datasets (cross datasetperson re-identification). Both in "intra dataset" and "cross dataset"settings, the superiorities of the proposed method are illustrated on VIPeR andPRID.
arxiv-7800-291 | Extensions of stability selection using subsamples of observations and covariates | http://arxiv.org/pdf/1407.4916v3.pdf | author:Andre Beinrucker, ÃrÃ¼n Dogan, Gilles Blanchard category:stat.ME stat.CO stat.ML published:2014-07-18 summary:We introduce extensions of stability selection, a method to stabilisevariable selection methods introduced by Meinshausen and B\"uhlmann (J R StatSoc 72:417-473, 2010). We propose to apply a base selection method repeatedlyto random observation subsamples and covariate subsets under scrutiny, and toselect covariates based on their selection frequency. We analyse the effectsand benefits of these extensions. Our analysis generalizes the theoreticalresults of Meinshausen and B\"uhlmann (J R Stat Soc 72:417-473, 2010) from thecase of half-samples to subsamples of arbitrary size. We study, in atheoretical manner, the effect of taking random covariate subsets using asimplified score model. Finally we validate these extensions on numericalexperiments on both synthetic and real datasets, and compare the obtainedresults in detail to the original stability selection method.
arxiv-7800-292 | Optimization Under Uncertainty Using the Generalized Inverse Distribution Function | http://arxiv.org/pdf/1407.4636v1.pdf | author:Domenico Quagliarella, Giovanni Petrone, Gianluca Iaccarino category:math.OC cs.NE published:2014-07-17 summary:A framework for robust optimization under uncertainty based on the use of thegeneralized inverse distribution function (GIDF), also called quantilefunction, is here proposed. Compared to more classical approaches that rely onthe usage of statistical moments as deterministic attributes that define theobjectives of the optimization process, the inverse cumulative distributionfunction allows for the use of all the possible information available in theprobabilistic domain. Furthermore, the use of a quantile based approach leadsnaturally to a multi-objective methodology which allows an a-posterioriselection of the candidate design based on risk/opportunity criteria defined bythe designer. Finally, the error on the estimation of the objectives due to theresolution of the GIDF will be proven to be quantifiable
arxiv-7800-293 | A feature construction framework based on outlier detection and discriminative pattern mining | http://arxiv.org/pdf/1407.4668v1.pdf | author:Albrecht Zimmermann category:cs.LG published:2014-07-17 summary:No matter the expressive power and sophistication of supervised learningalgorithms, their effectiveness is restricted by the features describing thedata. This is not a new insight in ML and many methods for feature selection,transformation, and construction have been developed. But while this ison-going for general techniques for feature selection and transformation, i.e.dimensionality reduction, work on feature construction, i.e. enriching thedata, is by now mainly the domain of image, particularly character,recognition, and NLP. In this work, we propose a new general framework for feature construction.The need for feature construction in a data set is indicated by class outliersand discriminative pattern mining used to derive features on theirk-neighborhoods. We instantiate the framework with LOF and C4.5-Rules, andevaluate the usefulness of the derived features on a diverse collection of UCIdata sets. The derived features are more often useful than ones derived byDC-Fringe, and our approach is much less likely to overfit. But while a weaklearner, Naive Bayes, benefits strongly from the feature construction, theeffect is less pronounced for C4.5, and almost vanishes for an SVM leaner. Keywords: feature construction, classification, outlier detection
arxiv-7800-294 | An landcover fuzzy logic classification by maximumlikelihood | http://arxiv.org/pdf/1407.4739v1.pdf | author:T. Sarath, G. Nagalakshmi category:cs.CV cs.LG published:2014-07-17 summary:In present days remote sensing is most used application in many sectors. Thisremote sensing uses different images like multispectral, hyper spectral orultra spectral. The remote sensing image classification is one of thesignificant method to classify image. In this state we classify the maximumlikelihood classification with fuzzy logic. In this we experimenting fuzzylogic like spatial, spectral texture methods in that different sub methods tobe used for image classification.
arxiv-7800-295 | Sparse Partially Linear Additive Models | http://arxiv.org/pdf/1407.4729v1.pdf | author:Yin Lou, Jacob Bien, Rich Caruana, Johannes Gehrke category:stat.ME cs.LG stat.ML published:2014-07-17 summary:The generalized partially linear additive model (GPLAM) is a flexible andinterpretable approach to building predictive models. It combines features inan additive manner, allowing them to have either a linear or nonlinear effecton the response. However, the assignment of features to the linear andnonlinear groups is typically assumed known. Thus, to make a GPLAM a viableapproach in situations in which little is known $apriori$ about the features,one must overcome two primary model selection challenges: deciding whichfeatures to include in the model and determining which features to treatnonlinearly. We introduce sparse partially linear additive models (SPLAMs),which combine model fitting and $both$ of these model selection challenges intoa single convex optimization problem. SPLAM provides a bridge between the Lassoand sparse additive models. Through a statistical oracle inequality andthorough simulation, we demonstrate that SPLAM can outperform other methodsacross a broad spectrum of statistical regimes, including the high-dimensional($p\gg N$) setting. We develop efficient algorithms that are applied to realdata sets with half a million samples and over 45,000 features with excellentpredictive performance.
arxiv-7800-296 | Sparse Quadratic Discriminant Analysis and Community Bayes | http://arxiv.org/pdf/1407.4543v1.pdf | author:Ya Le, Trevor Hastie category:stat.ML stat.CO published:2014-07-17 summary:We develop a class of rules spanning the range between quadratic discriminantanalysis and naive Bayes, through a path of sparse graphical models. A grouplasso penalty is used to introduce shrinkage and encourage a similar pattern ofsparsity across precision matrices. It gives sparse estimates of interactionsand produces interpretable models. Inspired by the connected-componentsstructure of the estimated precision matrices, we propose the community Bayesmodel, which partitions features into several conditional independentcommunities and splits the classification problem into separate smaller ones.The community Bayes idea is quite general and can be applied to non-gaussiandata and likelihood-based classifiers.
arxiv-7800-297 | Toward Selectivity Based Keyword Extraction for Croatian News | http://arxiv.org/pdf/1407.4723v1.pdf | author:Slobodan Beliga, Ana MeÅ¡troviÄ, Sanda MartinÄciÄ-IpÅ¡iÄ category:cs.CL cs.IR cs.SI published:2014-07-17 summary:Preliminary report on network based keyword extraction for Croatian is anunsupervised method for keyword extraction from the complex network. We buildour approach with a new network measure the node selectivity, motivated by theresearch of the graph based centrality approaches. The node selectivity isdefined as the average weight distribution on the links of the single node. Weextract nodes (keyword candidates) based on the selectivity value. Furthermore,we expand extracted nodes to word-tuples ranked with the highest in/outselectivity values. Selectivity based extraction does not require linguisticknowledge while it is purely derived from statistical and structuralinformation en-compassed in the source text which is reflected into thestructure of the network. Obtained sets are evaluated on a manually annotatedkeywords: for the set of extracted keyword candidates average F1 score is24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidatesaverage F1 score is 25,9% and average F2 score is 24,47%.
arxiv-7800-298 | Understanding Zipf's law of word frequencies through sample-space collapse in sentence formation | http://arxiv.org/pdf/1407.4610v2.pdf | author:Stefan Thurner, Rudolf Hanel, Bo Liu, Bernat Corominas-Murtra category:physics.soc-ph cs.CL published:2014-07-17 summary:The formation of sentences is a highly structured and history-dependentprocess. The probability of using a specific word in a sentence stronglydepends on the 'history' of word-usage earlier in that sentence. We study asimple history-dependent model of text generation assuming that thesample-space of word usage reduces along sentence formation, on average. Wefirst show that the model explains the approximate Zipf law found in wordfrequencies as a direct consequence of sample-space reduction. We thenempirically quantify the amount of sample-space reduction in the sentences often famous English books, by analysis of corresponding word-transition tablesthat capture which words can follow any given word in a text. We find a highlynested structure in these transition tables and show that this `nestedness' istightly related to the power law exponents of the observed word frequencydistributions. With the proposed model it is possible to understand that thenestedness of a text can be the origin of the actual scaling exponent, and thatdeviations from the exact Zipf law can be understood by variations of thedegree of nestedness on a book-by-book basis. On a theoretical level we areable to show that in case of weak nesting, Zipf's law breaks down in a fasttransition. Unlike previous attempts to understand Zipf's law in language thesample-space reducing model is not based on assumptions of multiplicative,preferential, or self-organised critical mechanisms behind language formation,but simply used the empirically quantifiable parameter 'nestedness' tounderstand the statistics of word frequencies.
arxiv-7800-299 | Efficient On-the-fly Category Retrieval using ConvNets and GPUs | http://arxiv.org/pdf/1407.4764v3.pdf | author:Ken Chatfield, Karen Simonyan, Andrew Zisserman category:cs.CV cs.LG cs.NE published:2014-07-17 summary:We investigate the gains in precision and speed, that can be obtained byusing Convolutional Networks (ConvNets) for on-the-fly retrieval - whereclassifiers are learnt at run time for a textual query from downloaded images,and used to rank large image or video datasets. We make three contributions: (i) we present an evaluation of state-of-the-artimage representations for object category retrieval over standard benchmarkdatasets containing 1M+ images; (ii) we show that ConvNets can be used toobtain features which are incredibly performant, and yet much lower dimensionalthan previous state-of-the-art image representations, and that theirdimensionality can be reduced further without loss in performance bycompression using product quantization or binarization. Consequently, featureswith the state-of-the-art performance on large-scale datasets of millions ofimages can fit in the memory of even a commodity GPU card; (iii) we show thatan SVM classifier can be learnt within a ConvNet framework on a GPU in parallelwith downloading the new training images, allowing for a continuous refinementof the model as more images become available, and simultaneous training andranking. The outcome is an on-the-fly system that significantly outperforms itspredecessors in terms of: precision of retrieval, memory requirements, andspeed, facilitating accurate on-the-fly learning and ranking in under a secondon a single GPU.
arxiv-7800-300 | Sparse and Low-Rank Covariance Matrices Estimation | http://arxiv.org/pdf/1407.4596v2.pdf | author:Shenglong Zhou, Naihua Xiu, Ziyan Luo, Lingchen Kong category:math.ST math.OC stat.ML stat.TH published:2014-07-17 summary:This paper aims at achieving a simultaneously sparse and low-rank estimatorfrom the semidefinite population covariance matrices. We first benefit from aconvex optimization which develops $l_1$-norm penalty to encourage the sparsityand nuclear norm to favor the low-rank property. For the proposed estimator, wethen prove that with large probability, the Frobenious norm of the estimationrate can be of order $O(\sqrt{s(\log{r})/n})$ under a mild case, where $s$ and$r$ denote the number of sparse entries and the rank of the populationcovariance respectively, $n$ notes the sample capacity. Finally an efficientalternating direction method of multipliers with global convergence is proposedto tackle this problem, and meantime merits of the approach are alsoillustrated by practicing numerical simulations.
