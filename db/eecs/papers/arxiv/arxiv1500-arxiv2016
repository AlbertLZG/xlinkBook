arxiv-1500-1 | Discussion: Latent variable graphical model selection via convex optimization | http://arxiv.org/pdf/1211.0806v1.pdf | author:Steffen Lauritzen, Nicolai Meinshausen category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1500-2 | Discussion: Latent variable graphical model selection via convex optimization | http://arxiv.org/pdf/1211.0801v1.pdf | author:Ming Yuan category:math.ST cs.LG stat.ML stat.TH published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convexoptimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky[arXiv:1008.1290].
arxiv-1500-3 | Efficient Point-to-Subspace Query in $\ell^1$: Theory and Applications in Computer Vision | http://arxiv.org/pdf/1211.0757v1.pdf | author:Ju Sun, Yuqian Zhang, John Wright category:stat.ML cs.CV stat.AP published:2012-11-05 summary:Motivated by vision tasks such as robust face and object recognition, weconsider the following general problem: given a collection of low-dimensionallinear subspaces in a high-dimensional ambient (image) space and a query point(image), efficiently determine the nearest subspace to the query in $\ell^1$distance. We show in theory that Cauchy random embedding of the objects intosignificantly-lower-dimensional spaces helps preserve the identity of thenearest subspace with constant probability. This offers the possibility ofefficiently selecting several candidates for accurate search. We sketchpreliminary experiments on robust face and digit recognition to corroborate ourtheory.
arxiv-1500-4 | Intelligent Algorithm for Optimum Solutions Based on the Principles of Bat Sonar | http://arxiv.org/pdf/1211.0730v1.pdf | author:Mohammed Ali Tawfeeq category:cs.NE published:2012-11-04 summary:This paper presents a new intelligent algorithm that can solve the problemsof finding the optimum solution in the state space among which the desiredsolution resides. The algorithm mimics the principles of bat sonar in findingits targets. The algorithm introduces three search approaches. The first searchapproach considers a single sonar unit (SSU) with a fixed beam length and asingle starting point. In this approach, although the results converge towardthe optimum fitness, it is not guaranteed to find the global optimum solutionespecially for complex problems; it is satisfied with finding 'acceptably good'solutions to these problems. The second approach considers multisonar units(MSU) working in parallel in the same state space. Each unit has its ownstarting point and tries to find the optimum solution. In this approach theprobability that the algorithm converges toward the optimum solution issignificantly increased. It is found that this approach is suitable for complexfunctions and for problems of wide state space. In the third approach, a singlesonar unit with a moment (SSM) is used in order to handle the problem ofconvergence toward a local optimum rather than a global optimum. The momentumterm is added to the length of the transmitted beams. This will give the chanceto find the best fitness in a wider range within the state space. In this papera comparison between the proposed algorithm and genetic algorithm (GA) has beenmade. It showed that both of the algorithms can catch approximately the optimumsolutions for all of the testbed functions except for the function that has alocal minimum, in which the proposed algorithm's result is much better thanthat of the GA algorithm. On the other hand, the comparison showed that therequired execution time to obtain the optimum solution using the proposedalgorithm is much less than that of the GA algorithm.
arxiv-1500-5 | POWERPLAY: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem | http://arxiv.org/pdf/1112.5309v2.pdf | author:Jürgen Schmidhuber category:cs.AI cs.LG published:2011-12-22 summary:Most of computer science focuses on automatically solving given computationalproblems. I focus on automatically inventing or discovering problems in a wayinspired by the playful behavior of animals and humans, to train a more andmore general problem solver from scratch in an unsupervised fashion. Considerthe infinite set of all computable descriptions of tasks with possiblycomputable solutions. The novel algorithmic framework POWERPLAY (2011)continually searches the space of possible pairs of new tasks and modificationsof the current problem solver, until it finds a more powerful problem solverthat provably solves all previously learned tasks plus the new one, while theunmodified predecessor does not. Wow-effects are achieved by continually makingpreviously learned skills more efficient such that they require less time andspace. New skills may (partially) re-use previously learned skills. POWERPLAY'ssearch orders candidate pairs of tasks and solver modifications by theirconditional computational (time & space) complexity, given the storedexperience so far. The new task and its corresponding task-solving skill arethose first found and validated. The computational costs of validating newtasks need not grow with task repertoire size. POWERPLAY's ongoing search fornovelty keeps breaking the generalization abilities of its present solver. Thisis related to Goedel's sequence of increasingly powerful formal theories basedon adding formerly unprovable statements to the axioms without affectingpreviously provable theorems. The continually increasing repertoire of problemsolving procedures can be exploited by a parallel search for solutions toadditional externally posed tasks. POWERPLAY may be viewed as a greedy butpractical implementation of basic principles of creativity. A firstexperimental analysis can be found in separate papers [53,54].
arxiv-1500-6 | Generation of Two-Layer Monotonic Functions | http://arxiv.org/pdf/1211.0660v1.pdf | author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE published:2012-11-04 summary:The problem of implementing a class of functions with particular conditionsby using monotonic multilayer functions is considered. A genetic algorithm isused to create monotonic functions of a certain class, and these areimplemented with two-layer monotonic functions. The existence of a solution tothe given problem suggests that from two monotone functions, a monotonicfunction with the same dimensions can be created. A new algorithm based on thegenetic algorithm is proposed, which easily implemented two-layer monotonicfunctions of a specific class for up to six variables.
arxiv-1500-7 | Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems | http://arxiv.org/pdf/1204.5721v2.pdf | author:Sébastien Bubeck, Nicolò Cesa-Bianchi category:cs.LG stat.ML published:2012-04-25 summary:Multi-armed bandit problems are the most basic examples of sequentialdecision problems with an exploration-exploitation trade-off. This is thebalance between staying with the option that gave highest payoffs in the pastand exploring new options that might give higher payoffs in the future.Although the study of bandit problems dates back to the Thirties,exploration-exploitation trade-offs arise in several modern applications, suchas ad placement, website optimization, and packet routing. Mathematically, amulti-armed bandit is defined by the payoff process associated with eachoption. In this survey, we focus on two extreme cases in which the analysis ofregret is particularly simple and elegant: i.i.d. payoffs and adversarialpayoffs. Besides the basic setting of finitely many actions, we also analyzesome of the most important variants and extensions, such as the contextualbandit model.
arxiv-1500-8 | Segmentation of ultrasound images of thyroid nodule for assisting fine needle aspiration cytology | http://arxiv.org/pdf/1211.0602v1.pdf | author:Jie Zhao, Wei Zheng, Li Zhang, Hua Tian category:cs.CV published:2012-11-03 summary:The incidence of thyroid nodule is very high and generally increases with theage. Thyroid nodule may presage the emergence of thyroid cancer. The thyroidnodule can be completely cured if detected early. Fine needle aspirationcytology is a recognized early diagnosis method of thyroid nodule. There arestill some limitations in the fine needle aspiration cytology, and theultrasound diagnosis of thyroid nodule has become the first choice forauxiliary examination of thyroid nodular disease. If we could combine medicalimaging technology and fine needle aspiration cytology, the diagnostic rate ofthyroid nodule would be improved significantly. The properties of ultrasoundwill degrade the image quality, which makes it difficult to recognize the edgesfor physicians. Image segmentation technique based on graph theory has become aresearch hotspot at present. Normalized cut (Ncut) is a representative one,which is suitable for segmentation of feature parts of medical image. However,how to solve the normalized cut has become a problem, which needs large memorycapacity and heavy calculation of weight matrix. It always generates oversegmentation or less segmentation which leads to inaccurate in thesegmentation. The speckle noise in B ultrasound image of thyroid tumor makesthe quality of the image deteriorate. In the light of this characteristic, wecombine the anisotropic diffusion model with the normalized cut in this paper.After the enhancement of anisotropic diffusion model, it removes the noise inthe B ultrasound image while preserves the important edges and local details.This reduces the amount of computation in constructing the weight matrix of theimproved normalized cut and improves the accuracy of the final segmentationresults. The feasibility of the method is proved by the experimental results.
arxiv-1500-9 | Detecting English Writing Styles For Non-native Speakers | http://arxiv.org/pdf/1211.0498v1.pdf | author:Rami Al-Rfou' category:cs.CL published:2012-11-02 summary:Analyzing writing styles of non-native speakers is a challenging task. Inthis paper, we analyze the comments written in the discussion pages of theEnglish Wikipedia. Using learning algorithms, we are able to detect nativespeakers' writing style with an accuracy of 74%. Given the diversity of theEnglish Wikipedia users and the large number of languages they speak, wemeasure the similarities among their native languages by comparing theinfluence they have on their English writing style. Our results show thatlanguages known to have the same origin and development path have similarfootprint on their speakers' English writing style. To enable further studies,the dataset we extracted from Wikipedia will be made available publicly.
arxiv-1500-10 | Ordinal Rating of Network Performance and Inference by Matrix Completion | http://arxiv.org/pdf/1211.0447v1.pdf | author:Wei Du, Yongjun Liao, and Pierre Geurts, Guy Leduc category:cs.NI cs.LG published:2012-11-02 summary:This paper addresses the large-scale acquisition of end-to-end networkperformance. We made two distinct contributions: ordinal rating of networkperformance and inference by matrix completion. The former reduces measurementcosts and unifies various metrics which eases their processing in applications.The latter enables scalable and accurate inference with no requirement ofstructural information of the network nor geometric constraints. By combiningboth, the acquisition problem bears strong similarities to recommender systems.This paper investigates the applicability of various matrix factorizationmodels used in recommender systems. We found that the simple regularized matrixfactorization is not only practical but also produces accurate results that arebeneficial for peer selection.
arxiv-1500-11 | Learning curves for multi-task Gaussian process regression | http://arxiv.org/pdf/1211.0439v1.pdf | author:Simon R. F. Ashton, Peter Sollich category:cs.LG stat.ML published:2012-11-02 summary:We study the average case performance of multi-task Gaussian process (GP)regression as captured in the learning curve, i.e. the average Bayes error fora chosen task versus the total number of examples $n$ for all tasks. For GPcovariances that are the product of an input-dependent covariance function anda free-form inter-task covariance matrix, we show that accurate approximationsfor the learning curve can be obtained for an arbitrary number of tasks $T$. Weuse these to study the asymptotic learning behaviour for large $n$.Surprisingly, multi-task learning can be asymptotically essentially useless, inthe sense that examples from other tasks help only when the degree ofinter-task correlation, $\rho$, is near its maximal value $\rho=1$. This effectis most extreme for learning of smooth target functions as described by e.g.squared exponential kernels. We also demonstrate that when learning many tasks,the learning curves separate into an initial phase, where the Bayes error oneach task is reduced down to a plateau value by "collective learning" eventhough most tasks have not seen examples, and a final decay that occurs oncethe number of examples is proportional to the number of tasks.
arxiv-1500-12 | Learning classifier systems with memory condition to solve non-Markov problems | http://arxiv.org/pdf/1211.0424v1.pdf | author:Zhaoxiang Zang, Dehua Li, Junying Wang category:cs.NE cs.AI published:2012-11-02 summary:In the family of Learning Classifier Systems, the classifier system XCS hasbeen successfully used for many applications. However, the standard XCS has nomemory mechanism and can only learn optimal policy in Markov environments,where the optimal action is determined solely by the state of current sensoryinput. In practice, most environments are partially observable environments onagent's sensation, which are also known as non-Markov environments. Withinthese environments, XCS either fails, or only develops a suboptimal policy,since it has no memory. In this work, we develop a new classifier system basedon XCS to tackle this problem. It adds an internal message list to XCS as thememory list to record input sensation history, and extends a small number ofclassifiers with memory conditions. The classifier's memory condition, as afoothold to disambiguate non-Markov states, is used to sense a specifiedelement in the memory list. Besides, a detection method is employed torecognize non-Markov states in environments, to avoid these states controllingover classifiers' memory conditions. Furthermore, four sets of differentcomplex maze environments have been tested by the proposed method. Experimentalresults show that our system is one of the best techniques to solve partiallyobservable environments, compared with some well-known classifier systemsproposed for these environments.
arxiv-1500-13 | Verbalizing Ontologies in Controlled Baltic Languages | http://arxiv.org/pdf/1211.0418v1.pdf | author:Normunds Grūzītis, Gunta Nešpore, Baiba Saulīte category:cs.CL cs.AI published:2012-11-02 summary:Controlled natural languages (mostly English-based) recently have emerged asseemingly informal supplementary means for OWL ontology authoring, if comparedto the formal notations that are used by professional knowledge engineers. Inthis paper we present by examples controlled Latvian language that has beendesigned to be compliant with the state of the art Attempto Controlled English.We also discuss relation with controlled Lithuanian language that is beingdesigned in parallel.
arxiv-1500-14 | A Multiscale Framework for Challenging Discrete Optimization | http://arxiv.org/pdf/1210.7070v3.pdf | author:Shai Bagon, Meirav Galun category:cs.CV cs.LG math.OC stat.ML published:2012-10-26 summary:Current state-of-the-art discrete optimization methods struggle behind whenit comes to challenging contrast-enhancing discrete energies (i.e., favoringdifferent labels for neighboring variables). This work suggests a multiscaleapproach for these challenging problems. Deriving an algebraic representationallows us to coarsen any pair-wise energy using any interpolation in aprincipled algebraic manner. Furthermore, we propose an energy-awareinterpolation operator that efficiently exposes the multiscale landscape of theenergy yielding an effective coarse-to-fine optimization scheme. Results onchallenging contrast-enhancing energies show significant improvement overstate-of-the-art methods.
arxiv-1500-15 | Iterative Hard Thresholding Methods for $l_0$ Regularized Convex Cone Programming | http://arxiv.org/pdf/1211.0056v2.pdf | author:Zhaosong Lu category:math.OC cs.LG math.NA stat.CO stat.ML published:2012-10-31 summary:In this paper we consider $l_0$ regularized convex cone programming problems.In particular, we first propose an iterative hard thresholding (IHT) method andits variant for solving $l_0$ regularized box constrained convex programming.We show that the sequence generated by these methods converges to a localminimizer. Also, we establish the iteration complexity of the IHT method forfinding an $\epsilon$-local-optimal solution. We then propose a method forsolving $l_0$ regularized convex cone programming by applying the IHT method toits quadratic penalty relaxation and establish its iteration complexity forfinding an $\epsilon$-approximate local minimizer. Finally, we propose avariant of this method in which the associated penalty parameter is dynamicallyupdated, and show that every accumulation point is a local minimizer of theproblem.
arxiv-1500-16 | Risk estimation for matrix recovery with spectral regularization | http://arxiv.org/pdf/1205.1482v3.pdf | author:Charles-Alban Deledalle, Samuel Vaiter, Gabriel Peyré, Jalal Fadili, Charles Dossal category:math.OC cs.IT cs.LG math.IT math.ST stat.ML stat.TH published:2012-05-07 summary:In this paper, we develop an approach to recursively estimate the quadraticrisk for matrix recovery problems regularized with spectral functions. Towardthis end, in the spirit of the SURE theory, a key step is to compute the (weak)derivative and divergence of a solution with respect to the observations. Assuch a solution is not available in closed form, but rather through a proximalsplitting algorithm, we propose to recursively compute the divergence from thesequence of iterates. A second challenge that we unlocked is the computation ofthe (weak) derivative of the proximity operator of a spectral function. To showthe potential applicability of our approach, we exemplify it on a matrixcompletion problem to objectively and automatically select the regularizationparameter.
arxiv-1500-17 | Extension of TSVM to Multi-Class and Hierarchical Text Classification Problems With General Losses | http://arxiv.org/pdf/1211.0210v1.pdf | author:Sathiya Keerthi Selvaraj, Sundararajan Sellamanickam, Shirish Shevade category:cs.LG published:2012-11-01 summary:Transductive SVM (TSVM) is a well known semi-supervised large margin learningmethod for binary text classification. In this paper we extend this method tomulti-class and hierarchical classification problems. We point out that thedetermination of labels of unlabeled examples with fixed classifier weights isa linear programming problem. We devise an efficient technique for solving it.The method is applicable to general loss functions. We demonstrate the value ofthe new method using large margin loss on a number of multi-class andhierarchical classification datasets. For maxent loss we show empirically thatour method is better than expectation regularization/constraint and posteriorregularization methods, and competitive with the version of entropyregularization method which uses label constraints.
arxiv-1500-18 | Sampling and Reconstruction of Spatial Fields using Mobile Sensors | http://arxiv.org/pdf/1211.0135v1.pdf | author:Jayakrishnan Unnikrishnan, Martin Vetterli category:cs.MM cs.CV cs.IT math.IT published:2012-11-01 summary:Spatial sampling is traditionally studied in a static setting where staticsensors scattered around space take measurements of the spatial field at theirlocations. In this paper we study the emerging paradigm of sampling andreconstructing spatial fields using sensors that move through space. We showthat mobile sensing offers some unique advantages over static sensing insensing time-invariant bandlimited spatial fields. Since a moving sensorencounters such a spatial field along its path as a time-domain signal, atime-domain anti-aliasing filter can be employed prior to sampling the signalreceived at the sensor. Such a filtering procedure, when used by aconfiguration of sensors moving at constant speeds along equispaced parallellines, leads to a complete suppression of spatial aliasing in the direction ofmotion of the sensors. We analytically quantify the advantage of using such asampling scheme over a static sampling scheme by computing the reduction insampling noise due to the filter. We also analyze the effects of non-uniformsensor speeds on the reconstruction accuracy. Using simulation examples wedemonstrate the advantages of mobile sampling over static sampling in practicalproblems. We extend our analysis to sampling and reconstruction schemes for monitoringtime-varying bandlimited fields using mobile sensors. We demonstrate that insome situations we require a lower density of sensors when using a mobilesensing scheme instead of the conventional static sensing scheme. The exactadvantage is quantified for a problem of sampling and reconstructing an audiofield.
arxiv-1500-19 | Transition-Based Dependency Parsing With Pluggable Classifiers | http://arxiv.org/pdf/1211.0074v1.pdf | author:Alex Rudnick category:cs.CL published:2012-11-01 summary:In principle, the design of transition-based dependency parsers makes itpossible to experiment with any general-purpose classifier without otherchanges to the parsing algorithm. In practice, however, it often takessubstantial software engineering to bridge between the differentrepresentations used by two software packages. Here we present extensions toMaltParser that allow the drop-in use of any classifier conforming to theinterface of the Weka machine learning package, a wrapper for the TiMBLmemory-based learner to this interface, and experiments on multilingualdependency parsing with a variety of classifiers. While earlier work hadsuggested that memory-based learners might be a good choice for low-resourceparsing scenarios, we cannot support that hypothesis in this work. We observedthat support-vector machines give better parsing performance than thememory-based learner, regardless of the size of the training set.
arxiv-1500-20 | Dimensionality Reduction and Classification Feature Using Mutual Information Applied to Hyperspectral Images: A Wrapper Strategy Algorithm Based on Minimizing the Error Probability Using the Inequality of Fano | http://arxiv.org/pdf/1211.0055v1.pdf | author:Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV published:2012-10-31 summary:In the feature classification domain, the choice of data affects widely theresults. For the Hyperspectral image, the bands dont all contain theinformation; some bands are irrelevant like those affected by variousatmospheric effects, see Figure.4, and decrease the classification accuracy.And there exist redundant bands to complicate the learning system and productincorrect prediction [14]. Even the bands contain enough information about thescene they may can't predict the classes correctly if the dimension of spaceimages, see Figure.3, is so large that needs many cases to detect therelationship between the bands and the scene (Hughes phenomenon) [10]. We canreduce the dimensionality of hyperspectral images by selecting only therelevant bands (feature selection or subset selection methodology), orextracting, from the original bands, new bands containing the maximalinformation about the classes, using any functions, logical or numerical(feature extraction methodology) [11][9]. Here we focus on the featureselection using mutual information. Hyperspectral images have three advantagesregarding the multispectral images [6],
arxiv-1500-21 | Understanding the Interaction between Interests, Conversations and Friendships in Facebook | http://arxiv.org/pdf/1211.0028v1.pdf | author:Qirong Ho, Rong Yan, Rajat Raina, Eric P. Xing category:cs.SI cs.LG stat.ML published:2012-10-31 summary:In this paper, we explore salient questions about user interests,conversations and friendships in the Facebook social network, using a novellatent space model that integrates several data types. A key challenge ofstudying Facebook's data is the wide range of data modalities such as text,network links, and categorical labels. Our latent space model seamlesslycombines all three data modalities over millions of users, allowing us to studythe interplay between user friendships, interests, and higher-ordernetwork-wide social trends on Facebook. The recovered insights not only answerour initial questions, but also reveal surprising facts about user interests inthe context of Facebook's ecosystem. We also confirm that our results aresignificant with respect to evidential information from the study subjects.
arxiv-1500-22 | Large Scale Language Modeling in Automatic Speech Recognition | http://arxiv.org/pdf/1210.8440v1.pdf | author:Ciprian Chelba, Dan Bikel, Maria Shugrina, Patrick Nguyen, Shankar Kumar category:cs.CL published:2012-10-31 summary:Large language models have been proven quite beneficial for a variety ofautomatic speech recognition tasks in Google. We summarize results on VoiceSearch and a few YouTube speech transcription tasks to highlight the impactthat one can expect from increasing both the amount of training data, and thesize of the language model estimated from such data. Depending on the task,availability and amount of training data used, language model size and amountof work and care put into integrating them in the lattice rescoring step weobserve reductions in word error rate between 6% and 10% relative, for systemson a wide range of operating points between 17% and 52% word error rate.
arxiv-1500-23 | Optimal size, freshness and time-frame for voice search vocabulary | http://arxiv.org/pdf/1210.8436v1.pdf | author:Maryam Kamvar, Ciprian Chelba category:cs.CL cs.IR published:2012-10-31 summary:In this paper, we investigate how to optimize the vocabulary for a voicesearch language model. The metric we optimize over is the out-of-vocabulary(OoV) rate since it is a strong indicator of user experience. In a departurefrom the usual way of measuring OoV rates, web search logs allow us to computethe per-session OoV rate and thus estimate the percentage of users thatexperience a given OoV rate. Under very conservative text normalization, wefind that a voice search vocabulary consisting of 2 to 2.5 million wordsextracted from 1 week of search query data will result in an aggregate OoV rateof 1%; at that size, the same OoV rate will also be experienced by 90% ofusers. The number of words included in the vocabulary is a stable indicator ofthe OoV rate. Altering the freshness of the vocabulary or the duration of thetime window over which the training data is gathered does not significantlychange the OoV rate. Surprisingly, a significantly larger vocabulary(approximately 10 million words) is required to guarantee OoV rates below 1%for 95% of the users.
arxiv-1500-24 | Anomaly Detection in Time Series of Graphs using Fusion of Graph Invariants | http://arxiv.org/pdf/1210.8429v1.pdf | author:Youngser Park, Carey E. Priebe, Abdou Youssef category:stat.ML published:2012-10-31 summary:Given a time series of graphs G(t) = (V, E(t)), t = 1, 2, ..., where thefixed vertex set V represents "actors" and an edge between vertex u and vertexv at time t (uv \in E(t)) represents the existence of a communications eventbetween actors u and v during the tth time period, we wish to detect anomaliesand/or change points. We consider a collection of graph features, orinvariants, and demonstrate that adaptive fusion provides superior inferentialefficacy compared to naive equal weighting for a certain class of anomalydetection problems. Simulation results using a latent process model for timeseries of graphs, as well as illustrative experimental results for a timeseries of graphs derived from the Enron email data, show that a fusionstatistic can provide superior inference compared to individual invariantsalone. These results also demonstrate that an adaptive weighting scheme forfusion of invariants performs better than naive equal weighting.
arxiv-1500-25 | First Experiments with PowerPlay | http://arxiv.org/pdf/1210.8385v1.pdf | author:Rupesh Kumar Srivastava, Bas R. Steunebrink, Jürgen Schmidhuber category:cs.AI cs.LG published:2012-10-31 summary:Like a scientist or a playing child, PowerPlay not only learns new skills tosolve given problems, but also invents new interesting problems by itself. Bydesign, it continually comes up with the fastest to find, initially novel, buteventually solvable tasks. It also continually simplifies or compresses orspeeds up solutions to previous tasks. Here we describe first experiments withPowerPlay. A self-delimiting recurrent neural network SLIM RNN is used as ageneral computational problem solving architecture. Its connection weights canencode arbitrary, self-delimiting, halting or non-halting programs affectingboth environment (through effectors) and internal states encoding abstractionsof event sequences. Our PowerPlay-driven SLIM RNN learns to become anincreasingly general solver of self-invented problems, continually adding newproblem solving procedures to its growing skill repertoire. Extending a recentconference paper, we identify interesting, emerging, developmental stages ofour open-ended system. We also show how it automatically self-modularizes,frequently re-using code for previously invented skills, always trying toinvent novel tasks that can be quickly validated because they do not requiretoo many weight changes affecting too many previous tasks.
arxiv-1500-26 | Learning Attitudes and Attributes from Multi-Aspect Reviews | http://arxiv.org/pdf/1210.3926v2.pdf | author:Julian McAuley, Jure Leskovec, Dan Jurafsky category:cs.CL cs.IR cs.LG published:2012-10-15 summary:The majority of online reviews consist of plain-text feedback together with asingle numeric score. However, there are multiple dimensions to products andopinions, and understanding the `aspects' that contribute to users' ratings mayhelp us to better understand their individual preferences. For example, auser's impression of an audiobook presumably depends on aspects such as thestory and the narrator, and knowing their opinions on these aspects may help usto recommend better products. In this paper, we build models for rating systemsin which such dimensions are explicit, in the sense that users leave separateratings for each aspect of a product. By introducing new corpora consisting offive million reviews, rated with between three and six aspects, we evaluate ourmodels on three prediction tasks: First, we use our model to uncover whichparts of a review discuss which of the rated aspects. Second, we use our modelto summarize reviews, which for us means finding the sentences that bestexplain a user's rating. Finally, since aspect ratings are optional in many ofthe datasets we consider, we use our model to recover those ratings that aremissing from a user's evaluation. Our model matches state-of-the-art approacheson existing small-scale datasets, while scaling to the real-world datasets weintroduce. Moreover, our model is able to `disentangle' content and sentimentwords: we automatically learn content words that are indicative of a particularaspect as well as the aspect-specific sentiment words that are indicative of aparticular rating.
arxiv-1500-27 | Temporal Autoencoding Restricted Boltzmann Machine | http://arxiv.org/pdf/1210.8353v1.pdf | author:Chris Häusler, Alex Susemihl category:stat.ML cs.AI cs.LG published:2012-10-31 summary:Much work has been done refining and characterizing the receptive fieldslearned by deep learning algorithms. A lot of this work has focused on thedevelopment of Gabor-like filters learned when enforcing sparsity constraintson a natural image dataset. Little work however has investigated how thesefilters might expand to the temporal domain, namely through training on naturalmovies. Here we investigate exactly this problem in established temporal deeplearning algorithms as well as a new learning paradigm suggested here, theTemporal Autoencoding Restricted Boltzmann Machine (TARBM).
arxiv-1500-28 | Mugshot Identification from Manipulated Facial Images | http://arxiv.org/pdf/1210.8318v1.pdf | author:H. R. Chennamma, Lalitha Rangarajan category:cs.CV cs.MM published:2012-10-31 summary:Editing on digital images is ubiquitous. Identification of deliberatelymodified facial images is a new challenge for face identification system. Inthis paper, we address the problem of identification of a face or person fromheavily altered facial images. In this face identification problem, the inputto the system is a manipulated or transformed face image and the system reportsback the determined identity from a database of known individuals. Such asystem can be useful in mugshot identification in which mugshot databasecontains two views (frontal and profile) of each criminal. We considered onlyfrontal view from the available database for face identification and the queryimage is a manipulated face generated by face transformation software toolavailable online. We propose SIFT features for efficient face identification inthis scenario. Further comparative analysis has been given with well knowneigenface approach. Experiments have been conducted with real case images toevaluate the performance of both methods.
arxiv-1500-29 | Learning in the Model Space for Fault Diagnosis | http://arxiv.org/pdf/1210.8291v1.pdf | author:Huanhuan Chen, Peter Tino, Xin Yao, Ali Rodan category:cs.LG cs.AI published:2012-10-31 summary:The emergence of large scaled sensor networks facilitates the collection oflarge amounts of real-time data to monitor and control complex engineeringsystems. However, in many cases the collected data may be incomplete orinconsistent, while the underlying environment may be time-varying orun-formulated. In this paper, we have developed an innovative cognitive faultdiagnosis framework that tackles the above challenges. This frameworkinvestigates fault diagnosis in the model space instead of in the signal space.Learning in the model space is implemented by fitting a series of models usinga series of signal segments selected with a rolling window. By investigatingthe learning techniques in the fitted model space, faulty models can bediscriminated from healthy models using one-class learning algorithm. Theframework enables us to construct fault library when unknown faults occur,which can be regarded as cognitive fault isolation. This paper alsotheoretically investigates how to measure the pairwise distance between twomodels in the model space and incorporates the model distance into the learningalgorithm in the model space. The results on three benchmark applications andone simulated model for the Barcelona water distribution network have confirmedthe effectiveness of the proposed framework.
arxiv-1500-30 | On the Relation Between the Common Labelling and the Median Graph | http://arxiv.org/pdf/1210.8262v1.pdf | author:Nicola Rebagliati, Albert Solé-Ribalta, Marcello Pelillo, Francesc Serratosa category:cs.CV published:2012-10-31 summary:In structural pattern recognition, given a set of graphs, the computation ofa Generalized Median Graph is a well known problem. Some methods approach theproblem by assuming a relation between the Generalized Median Graph and theCommon Labelling problem. However, this relation has still not been formallyproved. In this paper, we analyse such relation between both problems. The mainresult proves that the cost of the common labelling upper-bounds the cost ofthe median with respect to the given set. In addition, we show that the twoproblems are equivalent in some cases.
arxiv-1500-31 | Hierarchical Learning Algorithm for the Beta Basis Function Neural Network | http://arxiv.org/pdf/1210.8124v1.pdf | author:Habib Dhahri, Mohamed Adel Alimi category:cs.NE cs.AI published:2012-10-30 summary:The paper presents a two-level learning method for the design of the BetaBasis Function Neural Network BBFNN. A Genetic Algorithm is employed at theupper level to construct BBFNN, while the key learning parameters :the width,the centers and the Beta form are optimised using the gradient algorithm at thelower level. In order to demonstrate the effectiveness of this hierarchicallearning algorithm HLABBFNN, we need to validate our algorithm for theapproximation of non-linear function.
arxiv-1500-32 | Min Max Generalization for Two-stage Deterministic Batch Mode Reinforcement Learning: Relaxation Schemes | http://arxiv.org/pdf/1202.5298v2.pdf | author:Raphael Fonteneau, Damien Ernst, Bernard Boigelot, Quentin Louveaux category:cs.SY cs.LG published:2012-02-23 summary:We study the minmax optimization problem introduced in [22] for computingpolicies for batch mode reinforcement learning in a deterministic setting.First, we show that this problem is NP-hard. In the two-stage case, we providetwo relaxation schemes. The first relaxation scheme works by dropping someconstraints in order to obtain a problem that is solvable in polynomial time.The second relaxation scheme, based on a Lagrangian relaxation where allconstraints are dualized, leads to a conic quadratic programming problem. Wealso theoretically prove and empirically illustrate that both relaxationschemes provide better results than those given in [22].
arxiv-1500-33 | Learning Smooth Pattern Transformation Manifolds | http://arxiv.org/pdf/1112.5640v5.pdf | author:Elif Vural, Pascal Frossard category:cs.CV published:2011-12-23 summary:Manifold models provide low-dimensional representations that are useful forprocessing and analyzing data in a transformation-invariant way. In this paper,we study the problem of learning smooth pattern transformation manifolds fromimage sets that represent observations of geometrically transformed signals. Inorder to construct a manifold, we build a representative pattern whosetransformations accurately fit various input images. We examine two objectivesof the manifold building problem, namely, approximation and classification. Forthe approximation problem, we propose a greedy method that constructs arepresentative pattern by selecting analytic atoms from a continuous dictionarymanifold. We present a DC (Difference-of-Convex) optimization scheme that isapplicable to a wide range of transformation and dictionary models, anddemonstrate its application to transformation manifolds generated by rotation,translation and anisotropic scaling of a reference pattern. Then, we generalizethis approach to a setting with multiple transformation manifolds, where eachmanifold represents a different class of signals. We present an iterativemultiple manifold building algorithm such that the classification accuracy ispromoted in the learning of the representative patterns. Experimental resultssuggest that the proposed methods yield high accuracy in the approximation andclassification of data compared to some reference methods, while the invarianceto geometric transformations is achieved due to the transformation manifoldmodel.
arxiv-1500-34 | Implementation of a Vision System for a Landmine Detecting Robot Using Artificial Neural Network | http://arxiv.org/pdf/1210.7956v1.pdf | author:Roger Achkar, Michel Owayjan category:cs.NE cs.CV 68T45 I.2.6; I.5.1 published:2012-10-30 summary:Landmines, specifically anti-tank mines, cluster bombs, and unexplodedordnance form a serious problem in many countries. Several landmine sweepingtechniques are used for minesweeping. This paper presents the design and theimplementation of the vision system of an autonomous robot for landmineslocalization. The proposed work develops state-of-the-art techniques in digitalimage processing for pre-processing captured images of the contaminated area.After enhancement, Artificial Neural Network (ANN) is used in order toidentify, recognize and classify the landmines' make and model. TheBack-Propagation algorithm is used for training the network. The proposed workproved to be able to identify and classify different types of landmines undervarious conditions (rotated landmine, partially covered landmine) with asuccess rate of up to 90%.
arxiv-1500-35 | The Model of Semantic Concepts Lattice For Data Mining Of Microblogs | http://arxiv.org/pdf/1210.7917v1.pdf | author:Bohdan Pavlyshenko category:cs.CL cs.IR published:2012-10-30 summary:The model of semantic concept lattice for data mining of microblogs has beenproposed in this work. It is shown that the use of this model is effective forthe semantic relations analysis and for the detection of associative rules ofkey words.
arxiv-1500-36 | Learning Onto-Relational Rules with Inductive Logic Programming | http://arxiv.org/pdf/1210.2984v2.pdf | author:Francesca A. Lisi category:cs.AI cs.DB cs.LG cs.LO published:2012-10-10 summary:Rules complement and extend ontologies on the Semantic Web. We refer to theserules as onto-relational since they combine DL-based ontology languages andKnowledge Representation formalisms supporting the relational data model withinthe tradition of Logic Programming and Deductive Databases. Rule authoring is avery demanding Knowledge Engineering task which can be automated thoughpartially by applying Machine Learning algorithms. In this chapter we show howInductive Logic Programming (ILP), born at the intersection of Machine Learningand Logic Programming and considered as a major approach to RelationalLearning, can be adapted to Onto-Relational Learning. For the sake ofillustration, we provide details of a specific Onto-Relational Learningsolution to the problem of learning rule-based definitions of DL concepts androles with ILP.
arxiv-1500-37 | Performance Evaluation of Different Techniques for texture Classification | http://arxiv.org/pdf/1210.7669v1.pdf | author:Pooja Maknikar category:cs.CV published:2012-10-29 summary:Texture is the term used to characterize the surface of a given object orphenomenon and is an important feature used in image processing and patternrecognition. Our aim is to compare various Texture analyzing methods andcompare the results based on time complexity and accuracy of classification.The project describes texture classification using Wavelet Transform and Cooccurrence Matrix. Comparison of features of a sample texture with database ofdifferent textures is performed. In wavelet transform we use the Haar, Symletsand Daubechies wavelets. We find that, thee Haar wavelet proves to be the mostefficient method in terms of performance assessment parameters mentioned above.Comparison of Haar wavelet and Co-occurrence matrix method of classificationalso goes in the favor of Haar. Though the time requirement is high in thelater method, it gives excellent results for classification accuracy except ifthe image is rotated.
arxiv-1500-38 | Text Classification with Compression Algorithms | http://arxiv.org/pdf/1210.7657v1.pdf | author:Antonio Giuliano Zippo category:cs.LG published:2012-10-29 summary:This work concerns a comparison of SVM kernel methods in text categorizationtasks. In particular I define a kernel function that estimates the similaritybetween two objects computing by their compressed lengths. In fact, compressionalgorithms can detect arbitrarily long dependencies within the text strings.Data text vectorization looses information in feature extractions and is highlysensitive by textual language. Furthermore, these methods are languageindependent and require no text preprocessing. Moreover, the accuracy computedon the datasets (Web-KB, 20ng and Reuters-21578), in some case, is greater thanGaussian, linear and polynomial kernels. The method limits are represented bycomputational time complexity of the Gram matrix and by very poor performanceon non-textual datasets.
arxiv-1500-39 | The fortresses of Ejin: an example of outlining a site from satellite images | http://arxiv.org/pdf/1210.7631v1.pdf | author:Amelia Carolina Sparavigna category:cs.CV published:2012-10-29 summary:From 1960's to 1970's, the Chinese Army built some fortified artificialhills. Some of them are located in the Inner Mongolia, Western China. Theselarge fortresses are surrounded by moats. For some of them it is still possibleto see earthworks, trenches and ditches, the planning of which could have asymbolic meaning. We can argue this result form their digital outlining,obtained after an image processing of satellite images, based on edgedetection.
arxiv-1500-40 | Empirical Normalization for Quadratic Discriminant Analysis and Classifying Cancer Subtypes | http://arxiv.org/pdf/1203.6345v2.pdf | author:Mark A. Kon, Nikolay Nikolaev category:stat.ML published:2012-03-28 summary:We introduce a new discriminant analysis method (Empirical DiscriminantAnalysis or EDA) for binary classification in machine learning. Given a datasetof feature vectors, this method defines an empirical feature map transformingthe training and test data into new data with components having Gaussianempirical distributions. This map is an empirical version of the Gaussiancopula used in probability and mathematical finance. The purpose is to form afeature mapped dataset as close as possible to Gaussian, after which standardquadratic discriminants can be used for classification. We discuss this methodin general, and apply it to some datasets in computational biology.
arxiv-1500-41 | Recognizing Static Signs from the Brazilian Sign Language: Comparing Large-Margin Decision Directed Acyclic Graphs, Voting Support Vector Machines and Artificial Neural Networks | http://arxiv.org/pdf/1210.7461v1.pdf | author:César Roberto de Souza, Ednaldo Brigante Pizzolato, Mauro dos Santos Anjo category:cs.CV cs.LG stat.ML published:2012-10-28 summary:In this paper, we explore and detail our experiments in ahigh-dimensionality, multi-class image classification problem often found inthe automatic recognition of Sign Languages. Here, our efforts are directedtowards comparing the characteristics, advantages and drawbacks of creating andtraining Support Vector Machines disposed in a Directed Acyclic Graph andArtificial Neural Networks to classify signs from the Brazilian Sign Language(LIBRAS). We explore how the different heuristics, hyperparameters andmulti-class decision schemes affect the performance, efficiency and ease of usefor each classifier. We provide hyperparameter surface maps capturing accuracyand efficiency, comparisons between DDAGs and 1-vs-1 SVMs, and effects ofheuristics when training ANNs with Resilient Backpropagation. We reportstatistically significant results using Cohen's Kappa statistic for contingencytables.
arxiv-1500-42 | Learning mixtures of spherical Gaussians: moment methods and spectral decompositions | http://arxiv.org/pdf/1206.5766v4.pdf | author:Daniel Hsu, Sham M. Kakade category:cs.LG stat.ML published:2012-06-25 summary:This work provides a computationally efficient and statistically consistentmoment-based estimator for mixtures of spherical Gaussians. Under the conditionthat component means are in general position, a simple spectral decompositiontechnique yields consistent parameter estimates from low-order observablemoments, without additional minimum separation assumptions needed by previouscomputationally efficient estimation procedures. Thus computational andinformation-theoretic barriers to efficient estimation in mixture models areprecluded when the mixture components have means in general position andspherical covariances. Some connections are made to estimation problems relatedto independent component analysis.
arxiv-1500-43 | Resolution Enhancement of Range Images via Color-Image Segmentation | http://arxiv.org/pdf/1210.7403v1.pdf | author:Arnav Bhavsar category:cs.CV published:2012-10-28 summary:We report a method for super-resolution of range images. Our approachleverages the interpretation of LR image as sparse samples on the HR grid.Based on this interpretation, we demonstrate that our recently reportedapproach, which reconstructs dense range images from sparse range data byexploiting a registered colour image, can be applied for the task of resolutionenhancement of range images. Our method only uses a single colour image inaddition to the range observation in the super-resolution process. Using theproposed approach, we demonstrate super-resolution results for large factors(e.g. 4) with good localization accuracy.
arxiv-1500-44 | The Bayesian Bridge | http://arxiv.org/pdf/1109.2279v2.pdf | author:Nicholas G. Polson, James G. Scott, Jesse Windle category:stat.ME stat.CO stat.ML published:2011-09-11 summary:We propose the Bayesian bridge estimator for regularized regression andclassification. Two key mixture representations for the Bayesian bridge modelare developed: (1) a scale mixture of normals with respect to an alpha-stablerandom variable; and (2) a mixture of Bartlett--Fejer kernels (or triangledensities) with respect to a two-component mixture of gamma random variables.Both lead to MCMC methods for posterior simulation, and these methods turn outto have complementary domains of maximum efficiency. The first representationis a well known result due to West (1987), and is the better choice forcollinear design matrices. The second representation is new, and is moreefficient for orthogonal problems, largely because it avoids the need to dealwith exponentially tilted stable random variables. It also provides insightinto the multimodality of the joint posterior distribution, a feature of thebridge model that is notably absent under ridge or lasso-type priors. We provea theorem that extends this representation to a wider class of densitiesrepresentable as scale mixtures of betas, and provide an explicit inversionformula for the mixing distribution. The connections with slice sampling andscale mixtures of normals are explored. On the practical side, we find that theBayesian bridge model outperforms its classical cousin in estimation andprediction across a variety of data sets, both simulated and real. We also showthat the MCMC for fitting the bridge model exhibits excellent mixingproperties, particularly for the global scale parameter. This makes for afavorable contrast with analogous MCMC algorithms for other sparse Bayesianmodels. All methods described in this paper are implemented in the R packageBayesBridge. An extensive set of simulation results are provided in twosupplemental files.
arxiv-1500-45 | The Hangulphabet: A Descriptive Alphabet | http://arxiv.org/pdf/1210.7282v1.pdf | author:Robert Bishop, Ruggero Micheletto category:cs.CL published:2012-10-27 summary:This paper describes the Hangulphabet, a new writing system that should proveuseful in a number of contexts. Using the Hangulphabet, a user can instantlysee voicing, manner and place of articulation of any phoneme found in humanlanguage. The Hangulphabet places consonant graphemes on a grid with the x-axisrepresenting the place of articulation and the y-axis representing manner ofarticulation. Each individual grapheme contains radicals from both axes wherethe points intersect. The top radical represents manner of articulation wherethe bottom represents place of articulation. A horizontal line running throughthe middle of the bottom radical represents voicing. For vowels, place ofarticulation is located on a grid that represents the position of the tongue inthe mouth. This grid is similar to that of the IPA vowel chart (InternationalPhonetic Association, 1999). The difference with the Hangulphabet being thetrapezoid representing the vocal apparatus is on a slight tilt. Place ofarticulation for a vowel is represented by a breakout figure from the grid.This system can be used as an alternative to the International PhoneticAlphabet (IPA) or as a complement to it. Beginning students of linguistics mayfind it particularly useful. A Hangulphabet font has been created to facilitateswitching between the Hangulphabet and the IPA.
arxiv-1500-46 | Dynamic Pricing under Finite Space Demand Uncertainty: A Multi-Armed Bandit with Dependent Arms | http://arxiv.org/pdf/1206.5345v4.pdf | author:Pouya Tehrani, Yixuan Zhai, Qing Zhao category:cs.LG published:2012-06-23 summary:We consider a dynamic pricing problem under unknown demand models. In thisproblem a seller offers prices to a stream of customers and observes eithersuccess or failure in each sale attempt. The underlying demand model is unknownto the seller and can take one of N possible forms. In this paper, we show thatthis problem can be formulated as a multi-armed bandit with dependent arms. Wepropose a dynamic pricing policy based on the likelihood ratio test. We showthat the proposed policy achieves complete learning, i.e., it offers a boundedregret where regret is defined as the revenue loss with respect to the casewith a known demand model. This is in sharp contrast with the logarithmicgrowing regret in multi-armed bandit with independent arms.
arxiv-1500-47 | Fast Exact Max-Kernel Search | http://arxiv.org/pdf/1210.6287v2.pdf | author:Ryan R. Curtin, Parikshit Ram, Alexander G. Gray category:cs.DS cs.IR cs.LG published:2012-10-23 summary:The wide applicability of kernels makes the problem of max-kernel searchubiquitous and more general than the usual similarity search in metric spaces.We focus on solving this problem efficiently. We begin by characterizing theinherent hardness of the max-kernel search problem with a novel notion ofdirectional concentration. Following that, we present a method to use an $O(n\log n)$ algorithm to index any set of objects (points in $\Real^\dims$ orabstract objects) directly in the Hilbert space without any explicit featurerepresentations of the objects in this space. We present the first provably$O(\log n)$ algorithm for exact max-kernel search using this index. Empiricalresults for a variety of data sets as well as abstract objects demonstrate upto 4 orders of magnitude speedup in some cases. Extensions for approximatemax-kernel search are also presented.
arxiv-1500-48 | Alberti's letter counts | http://arxiv.org/pdf/1210.7137v1.pdf | author:Bernard Ycart category:math.HO cs.CL published:2012-10-26 summary:Four centuries before modern statistical linguistics was born, Leon BattistaAlberti (1404--1472) compared the frequency of vowels in Latin poems andorations, making the first quantified observation of a stylistic differenceever. Using a corpus of 20 Latin texts (over 5 million letters), Alberti'sobservations are statistically assessed. Letter counts prove that poets usedsignificantly more a's, e's, and y's, whereas orators used more of the othervowels. The sample sizes needed to justify the assertions are studied, andproved to be within reach for Alberti's scholarship.
arxiv-1500-49 | 3D Face Recognition using Significant Point based SULD Descriptor | http://arxiv.org/pdf/1210.7102v1.pdf | author:B. H. Shekar, N. Harivinod, M. Sharmila Kumari, K. Raghurama Holla category:cs.CV published:2012-10-26 summary:In this work, we present a new 3D face recognition method based on Speeded-UpLocal Descriptor (SULD) of significant points extracted from the range imagesof faces. The proposed model consists of a method for extracting distinctiveinvariant features from range images of faces that can be used to performreliable matching between different poses of range images of faces. For a given3D face scan, range images are computed and the potential interest points areidentified by searching at all scales. Based on the stability of the interestpoint, significant points are extracted. For each significant point we computethe SULD descriptor which consists of vector made of values from the convolvedHaar wavelet responses located on concentric circles centred on the significantpoint, and where the amount of Gaussian smoothing is proportional to the radiiof the circles. Experimental results show that the newly proposed methodprovides higher recognition rate compared to other existing contemporary modelsdeveloped for 3D face recognition.
arxiv-1500-50 | Selective Transfer Learning for Cross Domain Recommendation | http://arxiv.org/pdf/1210.7056v1.pdf | author:Zhongqi Lu, Erheng Zhong, Lili Zhao, Wei Xiang, Weike Pan, Qiang Yang category:cs.LG cs.IR stat.ML published:2012-10-26 summary:Collaborative filtering (CF) aims to predict users' ratings on itemsaccording to historical user-item preference data. In many real-worldapplications, preference data are usually sparse, which would make modelsoverfit and fail to give accurate predictions. Recently, several research worksshow that by transferring knowledge from some manually selected source domains,the data sparseness problem could be mitigated. However for most cases, partsof source domain data are not consistent with the observations in the targetdomain, which may misguide the target domain model building. In this paper, wepropose a novel criterion based on empirical prediction error and its varianceto better capture the consistency across domains in CF settings. Consequently,we embed this criterion into a boosting framework to perform selectiveknowledge transfer. Comparing to several state-of-the-art methods, we show thatour proposed selective transfer learning framework can significantly improvethe accuracy of rating prediction tasks on several real-world recommendationtasks.
arxiv-1500-51 | Large-Scale Sparse Principal Component Analysis with Application to Text Data | http://arxiv.org/pdf/1210.7054v1.pdf | author:Youwei Zhang, Laurent El Ghaoui category:stat.ML cs.LG math.OC published:2012-10-26 summary:Sparse PCA provides a linear combination of small number of features thatmaximizes variance across data. Although Sparse PCA has apparent advantagescompared to PCA, such as better interpretability, it is generally thought to becomputationally much more expensive. In this paper, we demonstrate thesurprising fact that sparse PCA can be easier than PCA in practice, and that itcan be reliably applied to very large data sets. This comes from a rigorousfeature elimination pre-processing result, coupled with the favorable fact thatfeatures in real-life data typically have exponentially decreasing variances,which allows for many features to be eliminated. We introduce a fast blockcoordinate ascent algorithm with much better computational complexity than theexisting first-order ones. We provide experimental results obtained on textcorpora involving millions of documents and hundreds of thousands of features.These results illustrate how Sparse PCA can help organize a large corpus oftext data in a user-interpretable way, providing an attractive alternativeapproach to topic models.
arxiv-1500-52 | User-level Weibo Recommendation incorporating Social Influence based on Semi-Supervised Algorithm | http://arxiv.org/pdf/1210.7047v1.pdf | author:Daifeng Li, Zhipeng Luo, Golden Guo-zheng Sun, Jie Tang, Jingwei Zhang category:cs.SI cs.CY cs.LG published:2012-10-26 summary:Tencent Weibo, as one of the most popular micro-blogging services in China,has attracted millions of users, producing 30-60 millions of weibo (similar astweet in Twitter) daily. With the overload problem of user generate content,Tencent users find it is more and more hard to browse and find valuableinformation at the first time. In this paper, we propose a Factor Graph basedweibo recommendation algorithm TSI-WR (Topic-Level Social Influence based WeiboRecommendation), which could help Tencent users to find most suitableinformation. The main innovation is that we consider both direct and indirectsocial influence from topic level based on social balance theory. The mainadvantages of adopting this strategy are that it could first build a moreaccurate description of latent relationship between two users with weakconnections, which could help to solve the data sparsity problem; secondprovide a more accurate recommendation for a certain user from a wider range.Other meaningful contextual information is also combined into our model, whichinclude: Users profile, Users influence, Content of weibos, Topic informationof weibos and etc. We also design a semi-supervised algorithm to further reducethe influence of data sparisty. The experiments show that all the selectedvariables are important and the proposed model outperforms several baselinemethods.
arxiv-1500-53 | Full Object Boundary Detection by Applying Scale Invariant Features in a Region Merging Segmentation Algorithm | http://arxiv.org/pdf/1210.7038v1.pdf | author:Reza Oji, Farshad Tajeripour category:cs.CV cs.AI published:2012-10-26 summary:Object detection is a fundamental task in computer vision and has manyapplications in image processing. This paper proposes a new approach for objectdetection by applying scale invariant feature transform (SIFT) in an automaticsegmentation algorithm. SIFT is an invariant algorithm respect to scale,translation and rotation. The features are very distinct and provide stablekeypoints that can be used for matching an object in different images. Atfirst, an object is trained with different aspects for finding best keypoints.The object can be recognized in the other images by using achieved keypoints.Then, a robust segmentation algorithm is used to detect the object with fullboundary based on SIFT keypoints. In segmentation algorithm, a merging role isdefined to merge the regions in image with the assistance of keypoints. Theresults show that the proposed approach is reliable for object detection andcan extract object boundary well.
arxiv-1500-54 | An Exponential Lower Bound on the Complexity of Regularization Paths | http://arxiv.org/pdf/0903.4817v3.pdf | author:Bernd Gärtner, Martin Jaggi, Clément Maria category:cs.LG cs.CG cs.CV math.OC stat.ML 90C20 F.2.2; I.5.1 published:2009-03-27 summary:For a variety of regularized optimization problems in machine learning,algorithms computing the entire solution path have been developed recently.Most of these methods are quadratic programs that are parameterized by a singleparameter, as for example the Support Vector Machine (SVM). Solution pathalgorithms do not only compute the solution for one particular value of theregularization parameter but the entire path of solutions, making the selectionof an optimal parameter much easier. It has been assumed that these piecewise linear solution paths have onlylinear complexity, i.e. linearly many bends. We prove that for the supportvector machine this complexity can be exponential in the number of trainingpoints in the worst case. More strongly, we construct a single instance of ninput points in d dimensions for an SVM such that at least \Theta(2^{n/2}) =\Theta(2^d) many distinct subsets of support vectors occur as theregularization parameter changes.
arxiv-1500-55 | Performance Evaluation of Random Set Based Pedestrian Tracking Algorithms | http://arxiv.org/pdf/1211.0191v1.pdf | author:Branko Ristic, Jamie Sherrah, Ángel F. García-Fernández category:cs.CV published:2012-10-25 summary:The paper evaluates the error performance of three random finite set basedmulti-object trackers in the context of pedestrian video tracking. Theevaluation is carried out using a publicly available video dataset of 4500frames (town centre street) for which the ground truth is available. The inputto all pedestrian tracking algorithms is an identical set of head and bodydetections, obtained using the Histogram of Oriented Gradients (HOG) detector.The tracking error is measured using the recently proposed OSPA metric fortracks, adopted as the only known mathematically rigorous metric for measuringthe distance between two sets of tracks. A comparative analysis is presentedunder various conditions.
arxiv-1500-56 | Enhancing the functional content of protein interaction networks | http://arxiv.org/pdf/1210.6912v1.pdf | author:Gaurav Pandey, Sahil Manocha, Gowtham Atluri, Vipin Kumar category:q-bio.MN cs.CE cs.LG q-bio.GN stat.ML published:2012-10-25 summary:Protein interaction networks are a promising type of data for studyingcomplex biological systems. However, despite the rich information embedded inthese networks, they face important data quality challenges of noise andincompleteness that adversely affect the results obtained from their analysis.Here, we explore the use of the concept of common neighborhood similarity(CNS), which is a form of local structure in networks, to address these issues.Although several CNS measures have been proposed in the literature, anunderstanding of their relative efficacies for the analysis of interactionnetworks has been lacking. We follow the framework of graph transformation toconvert the given interaction network into a transformed network correspondingto a variety of CNS measures evaluated. The effectiveness of each measure isthen estimated by comparing the quality of protein function predictionsobtained from its corresponding transformed network with those from theoriginal network. Using a large set of S. cerevisiae interactions, and a set of136 GO terms, we find that several of the transformed networks produce moreaccurate predictions than those obtained from the original network. Inparticular, the $HC.cont$ measure proposed here performs particularly well forthis task. Further investigation reveals that the two major factorscontributing to this improvement are the abilities of CNS measures, especially$HC.cont$, to prune out noisy edges and introduce new links betweenfunctionally related proteins.
arxiv-1500-57 | Ancestor Sampling for Particle Gibbs | http://arxiv.org/pdf/1210.6911v1.pdf | author:Fredrik Lindsten, Michael I. Jordan, Thomas B. Schön category:stat.CO stat.ML published:2012-10-25 summary:We present a novel method in the family of particle MCMC methods that werefer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to theexisting PG with backward simulation (PG-BS) procedure, we use backwardsampling to (considerably) improve the mixing of the PG kernel. Instead ofusing separate forward and backward sweeps as in PG-BS, however, we achieve thesame effect in a single forward sweep. We apply the PG-AS framework to thechallenging class of non-Markovian state-space models. We develop a truncationstrategy of these models that is applicable in principle to anybackward-simulation-based method, but which is particularly well suited to thePG-AS framework. In particular, as we show in a simulation study, PG-AS canyield an order-of-magnitude improved accuracy relative to PG-BS due to itsrobustness to the truncation error. Several application examples are discussed,including Rao-Blackwellized particle smoothing and inference in degeneratestate-space models.
arxiv-1500-58 | A Self-Organized Neural Comparator | http://arxiv.org/pdf/1210.6230v2.pdf | author:Guillermo A. Ludueña, Claudius Gros category:q-bio.NC cs.NE published:2012-10-23 summary:Learning algorithms need generally the possibility to compare several streamsof information. Neural learning architectures hence need a unit, a comparator,able to compare several inputs encoding either internal or externalinformation, like for instance predictions and sensory readings. Without thepossibility of comparing the values of prediction to actual sensory inputs,reward evaluation and supervised learning would not be possible. Comparators are usually not implemented explicitly, necessary comparisons arecommonly performed by directly comparing one-to-one the respective activities.This implies that the characteristics of the two input streams (like size andencoding) must be provided at the time of designing the system. It is however plausible that biological comparators emerge fromself-organizing, genetically encoded principles, which allow the system toadapt to the changes in the input and in the organism. We propose an unsupervised neural circuitry, where the function of inputcomparison emerges via self-organization only from the interaction of thesystem with the respective inputs, without external influence or supervision. The proposed neural comparator adapts, unsupervised, according to thecorrelations present in the input streams. The system consists of a multilayerfeed-forward neural network which follows a local output minimization(anti-Hebbian) rule for adaptation of the synaptic weights. The local output minimization allows the circuit to autonomously acquire thecapability of comparing the neural activities received from different neuralpopulations, which may differ in the size of the population and in the neuralencoding used. The comparator is able to compare objects never encounteredbefore in the sensory input streams and to evaluate a measure of theirsimilarity, even when differently encoded.
arxiv-1500-59 | Extended object reconstruction in adaptive-optics imaging: the multiresolution approach | http://arxiv.org/pdf/1210.6649v1.pdf | author:Roberto Baena Gallé, Jorge Núñez, Szymon Gladysz category:astro-ph.IM cs.CV math.NA published:2012-10-25 summary:We propose the application of multiresolution transforms, such as wavelets(WT) and curvelets (CT), to the reconstruction of images of extended objectsthat have been acquired with adaptive optics (AO) systems. Such multichannelapproaches normally make use of probabilistic tools in order to distinguishsignificant structures from noise and reconstruction residuals. Furthermore, weaim to check the historical assumption that image-reconstruction algorithmsusing static PSFs are not suitable for AO imaging. We convolve an image ofSaturn taken with the Hubble Space Telescope (HST) with AO PSFs from the 5-mHale telescope at the Palomar Observatory and add both shot and readout noise.Subsequently, we apply different approaches to the blurred and noisy data inorder to recover the original object. The approaches include multi-frame blinddeconvolution (with the algorithm IDAC), myopic deconvolution withregularization (with MISTRAL) and wavelets- or curvelets-based static PSFdeconvolution (AWMLE and ACMLE algorithms). We used the mean squared error(MSE) and the structural similarity index (SSIM) to compare the results. Wediscuss the strengths and weaknesses of the two metrics. We found that CTproduces better results than WT, as measured in terms of MSE and SSIM.Multichannel deconvolution with a static PSF produces results which aregenerally better than the results obtained with the myopic/blind approaches(for the images we tested) thus showing that the ability of a method tosuppress the noise and to track the underlying iterative process is just ascritical as the capability of the myopic/blind approaches to update the PSF.
arxiv-1500-60 | Structured Sparsity Models for Multiparty Speech Recovery from Reverberant Recordings | http://arxiv.org/pdf/1210.6766v1.pdf | author:Afsaneh Asaei, Mohammad Golbabaee, Hervé Bourlard, Volkan Cevher category:cs.LG cs.SD published:2012-10-25 summary:We tackle the multi-party speech recovery problem through modeling theacoustic of the reverberant chambers. Our approach exploits structured sparsitymodels to perform room modeling and speech recovery. We propose a scheme forcharacterizing the room acoustic from the unknown competing speech sourcesrelying on localization of the early images of the speakers by sparseapproximation of the spatial spectra of the virtual sources in a free-spacemodel. The images are then clustered exploiting the low-rank structure of thespectro-temporal components belonging to each source. This enables us toidentify the early support of the room impulse response function and its uniquemap to the room geometry. To further tackle the ambiguity of the reflectionratios, we propose a novel formulation of the reverberation model and estimatethe absorption coefficients through a convex optimization exploiting jointsparsity model formulated upon spatio-spectral sparsity of concurrent speechrepresentation. The acoustic parameters are then incorporated for separatingindividual speech signals through either structured sparse recovery or inversefiltering the acoustic channels. The experiments conducted on real datarecordings demonstrate the effectiveness of the proposed approach formulti-party speech recovery and recognition.
arxiv-1500-61 | $QD$-Learning: A Collaborative Distributed Strategy for Multi-Agent Reinforcement Learning Through Consensus + Innovations | http://arxiv.org/pdf/1205.0047v2.pdf | author:Soummya Kar, Jose' M. F. Moura, H. Vincent Poor category:stat.ML cs.LG cs.MA math.OC math.PR published:2012-04-30 summary:The paper considers a class of multi-agent Markov decision processes (MDPs),in which the network agents respond differently (as manifested by theinstantaneous one-stage random costs) to a global controlled state and thecontrol actions of a remote controller. The paper investigates a distributedreinforcement learning setup with no prior information on the global statetransition and local agent cost statistics. Specifically, with the agents'objective consisting of minimizing a network-averaged infinite horizondiscounted cost, the paper proposes a distributed version of $Q$-learning,$\mathcal{QD}$-learning, in which the network agents collaborate by means oflocal processing and mutual information exchange over a sparse (possiblystochastic) communication network to achieve the network goal. Under theassumption that each agent is only aware of its local online cost data and theinter-agent communication network is \emph{weakly} connected, the proposeddistributed scheme is almost surely (a.s.) shown to yield asymptotically thedesired value function and the optimal stationary control policy at eachnetwork agent. The analytical techniques developed in the paper to address themixed time-scale stochastic dynamics of the \emph{consensus + innovations}form, which arise as a result of the proposed interactive distributed scheme,are of independent interest.
arxiv-1500-62 | Clustering hidden Markov models with variational HEM | http://arxiv.org/pdf/1210.6707v1.pdf | author:Emanuele Coviello, Antoni B. Chan, Gert R. G. Lanckriet category:cs.LG cs.CV stat.ML published:2012-10-24 summary:The hidden Markov model (HMM) is a widely-used generative model that copeswith sequential data, assuming that each observation is conditioned on thestate of a hidden Markov chain. In this paper, we derive a novel algorithm tocluster HMMs based on the hierarchical EM (HEM) algorithm. The proposedalgorithm i) clusters a given collection of HMMs into groups of HMMs that aresimilar, in terms of the distributions they represent, and ii) characterizeseach group by a "cluster center", i.e., a novel HMM that is representative forthe group, in a manner that is consistent with the underlying generative modelof the HMM. To cope with intractable inference in the E-step, the HEM algorithmis formulated as a variational optimization problem, and efficiently solved forthe HMM case by leveraging an appropriate variational approximation. Thebenefits of the proposed algorithm, which we call variational HEM (VHEM), aredemonstrated on several tasks involving time-series data, such as hierarchicalclustering of motion capture sequences, and automatic annotation and retrievalof music and of online hand-writing data, showing improvements over currentmethods. In particular, our variational HEM algorithm effectively leverageslarge amounts of data when learning annotation models by using an efficienthierarchical estimation procedure, which reduces learning times and memoryrequirements, while improving model robustness through better regularization.
arxiv-1500-63 | Neural Networks for Complex Data | http://arxiv.org/pdf/1210.6511v1.pdf | author:Marie Cottrell, Madalina Olteanu, Fabrice Rossi, Joseph Rynkiewicz, Nathalie Villa-Vialaneix category:cs.NE cs.LG stat.ML published:2012-10-24 summary:Artificial neural networks are simple and efficient machine learning tools.Defined originally in the traditional setting of simple vector data, neuralnetwork models have evolved to address more and more difficulties of complexreal world problems, ranging from time evolving data to sophisticated datastructures such as graphs and functions. This paper summarizes advances onthose themes from the last decade, with a focus on results obtained by membersof the SAMM team of Universit\'e Paris 1
arxiv-1500-64 | Topic-Level Opinion Influence Model(TOIM): An Investigation Using Tencent Micro-Blogging | http://arxiv.org/pdf/1210.6497v1.pdf | author:Daifeng Li, Ying Ding, Xin Shuai, Golden Guo-zheng Sun, Jie Tang, Zhipeng Luo, Jingwei Zhang, Guo Zhang category:cs.SI cs.CY cs.LG published:2012-10-24 summary:Mining user opinion from Micro-Blogging has been extensively studied on themost popular social networking sites such as Twitter and Facebook in the U.S.,but few studies have been done on Micro-Blogging websites in other countries(e.g. China). In this paper, we analyze the social opinion influence onTencent, one of the largest Micro-Blogging websites in China, endeavoring tounveil the behavior patterns of Chinese Micro-Blogging users. This paperproposes a Topic-Level Opinion Influence Model (TOIM) that simultaneouslyincorporates topic factor and social direct influence in a unifiedprobabilistic framework. Based on TOIM, two topic level opinion influencepropagation and aggregation algorithms are developed to consider the indirectinfluence: CP (Conservative Propagation) and NCP (None ConservativePropagation). Users' historical social interaction records are leveraged byTOIM to construct their progressive opinions and neighbors' opinion influencethrough a statistical learning process, which can be further utilized topredict users' future opinions on some specific topics. To evaluate and testthis proposed model, an experiment was designed and a sub-dataset from TencentMicro-Blogging was used. The experimental results show that TOIM outperformsbaseline methods on predicting users' opinion. The applications of CP and NCPhave no significant differences and could significantly improve recall andF1-measure of TOIM.
arxiv-1500-65 | Black-Box Complexity: Breaking the $O(n \log n)$ Barrier of LeadingOnes | http://arxiv.org/pdf/1210.6465v1.pdf | author:Benjamin Doerr, Carola Winzen category:cs.DS cs.NE published:2012-10-24 summary:We show that the unrestricted black-box complexity of the $n$-dimensionalXOR- and permutation-invariant LeadingOnes function class is $O(n \log (n) /\log \log n)$. This shows that the recent natural looking $O(n\log n)$ bound isnot tight. The black-box optimization algorithm leading to this bound can be implementedin a way that only 3-ary unbiased variation operators are used. Hence our boundis also valid for the unbiased black-box complexity recently introduced byLehre and Witt (GECCO 2010). The bound also remains valid if we impose theadditional restriction that the black-box algorithm does not have access to theobjective values but only to their relative order (ranking-based black-boxcomplexity).
arxiv-1500-66 | Predicting Near-Future Churners and Win-Backs in the Telecommunications Industry | http://arxiv.org/pdf/1210.6891v1.pdf | author:Clifton Phua, Hong Cao, João Bártolo Gomes, Minh Nhut Nguyen category:cs.CE cs.LG published:2012-10-24 summary:In this work, we presented the strategies and techniques that we havedeveloped for predicting the near-future churners and win-backs for a telecomcompany. On a large-scale and real-world database containing customer profilesand some transaction data from a telecom company, we first analyzed the dataschema, developed feature computation strategies and then extracted a large setof relevant features that can be associated with the customer churning andreturning behaviors. Our features include both the original driver factors aswell as some derived features. We evaluated our features on the imbalancecorrected dataset, i.e. under-sampled dataset and compare a large number ofexisting machine learning tools, especially decision tree-based classifiers,for predicting the churners and win-backs. In general, we find RandomForest andSimpleCart learning algorithms generally perform well and tend to provide uswith highly competitive prediction performance. Among the top-15 driver factorsthat signal the churn behavior, we find that the service utilization, e.g. lasttwo months' download and upload volume, last three months' average upload anddownload, and the payment related factors are the most indicative features forpredicting if churn will happen soon. Such features can collectively telldiscrepancies between the service plans, payments and the dynamically changingutilization needs of the customers. Our proposed features and theircomputational strategy exhibit reasonable precision performance to predictchurn behavior in near future.
arxiv-1500-67 | On the geometric structure of fMRI searchlight-based information maps | http://arxiv.org/pdf/1210.6317v1.pdf | author:Shivakumar Viswanathan, Matthew Cieslak, Scott T. Grafton category:q-bio.NC q-bio.QM stat.AP stat.ML published:2012-10-23 summary:Information mapping is a popular application of Multivoxel Pattern Analysis(MVPA) to fMRI. Information maps are constructed using the so calledsearchlight method, where the spherical multivoxel neighborhood of every voxel(i.e., a searchlight) in the brain is evaluated for the presence oftask-relevant response patterns. Despite their widespread use, information mapspresent several challenges for interpretation. One such challenge has to dowith inferring the size and shape of a multivoxel pattern from its signature onthe information map. To address this issue, we formally examined the geometricbasis of this mapping relationship. Based on geometric considerations, we showhow and why small patterns (i.e., having smaller spatial extents) can produce alarger signature on the information map as compared to large patterns,independent of the size of the searchlight radius. Furthermore, we show thatthe number of informative searchlights over the brain increase as a function ofsearchlight radius, even in the complete absence of any multivariate responsepatterns. These properties are unrelated to the statistical capabilities of thepattern-analysis algorithms used but are obligatory geometric propertiesarising from using the searchlight procedure.
arxiv-1500-68 | MLPACK: A Scalable C++ Machine Learning Library | http://arxiv.org/pdf/1210.6293v1.pdf | author:Ryan R. Curtin, James R. Cline, N. P. Slagle, William B. March, Parikshit Ram, Nishant A. Mehta, Alexander G. Gray category:cs.MS cs.CV cs.LG published:2012-10-23 summary:MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learninglibrary released in late 2011 offering both a simple, consistent API accessibleto novice users and high performance and flexibility to expert users byleveraging modern features of C++. MLPACK provides cutting-edge algorithmswhose benchmarks exhibit far better performance than other leading machinelearning libraries. MLPACK version 1.0.3, licensed under the LGPL, is availableat http://www.mlpack.org.
arxiv-1500-69 | Textural Approach to Palmprint Identification | http://arxiv.org/pdf/1210.6192v1.pdf | author:Rachita Misra, Kasturika B ray category:cs.CV cs.CR cs.GR published:2012-10-23 summary:Biometrics which use of human physiological characteristics for identifyingan individual is now a widespread method of identification and authentication.Biometric identification is a technology which uses several image processingtechniques and describes the general procedure for identification andverification using feature extraction, storage and matching from the digitizedimage of biometric characters such as Finger Print, Face, Iris or Palm Print.The current paper uses palm print biometrics. Here we have presented anidentification approach using textural properties of palm print images. Theelegance of the method is that the conventional edge detection technique isextended to suitably describe the texture features. In this technique all thecharacteristics of the palm such as principal lines, edges and wrinkles areconsidered with equal importance.
arxiv-1500-70 | Further properties of Gaussian Reproducing Kernel Hilbert Spaces | http://arxiv.org/pdf/1210.6170v1.pdf | author:Minh Ha Quang category:stat.ML math.FA 68T05, 68P30 published:2012-10-23 summary:We generalize the orthonormal basis for the Gaussian RKHS described in\cite{MinhGaussian2010} to an infinite, continuously parametrized, family oforthonormal bases, along with some implications. The proofs are directgeneralizations of those in \cite{MinhGaussian2010}.
arxiv-1500-71 | Novel Architecture for 3D model in virtual communities from detected face | http://arxiv.org/pdf/1210.6157v1.pdf | author:Vibekananda Dutta, Dr Nishtha Kesswani, Deepti Gahalot category:cs.CV published:2012-10-23 summary:In this research paper we suggest how to extract a face from an image, modifyit, characterize it in terms of high-level properties, and apply it to thecreation of a personalized avatar. In this research work we tested, weimplemented the algorithm on several hundred facial images, including manytaken under uncontrolled acquisition conditions, and found to exhibitsatisfactory performance for immediate practical use.
arxiv-1500-72 | A Bayesian method for the analysis of deterministic and stochastic time series | http://arxiv.org/pdf/1209.3730v2.pdf | author:C. A. L. Bailer-Jones category:astro-ph.IM astro-ph.SR stat.ML published:2012-09-17 summary:I introduce a general, Bayesian method for modelling univariate time seriesdata assumed to be drawn from a continuous, stochastic process. The methodaccommodates arbitrary temporal sampling, and takes into account measurementuncertainties for arbitrary error models (not just Gaussian) on both the timeand signal variables. Any model for the deterministic component of thevariation of the signal with time is supported, as is any model of thestochastic component on the signal and time variables. Models illustrated hereare constant and sinusoidal models for the signal mean combined with a Gaussianstochastic component, as well as a purely stochastic model, theOrnstein-Uhlenbeck process. The posterior probability distribution over modelparameters is determined via Monte Carlo sampling. Models are compared usingthe "cross-validation likelihood", in which the posterior-averaged likelihoodfor different partitions of the data are combined. In principle this is morerobust to changes in the prior than is the evidence (the prior-averagedlikelihood). The method is demonstrated by applying it to the light curves of11 ultra cool dwarf stars, claimed by a previous study to show statisticallysignificant variability. This is reassessed here by calculating thecross-validation likelihood for various time series models, including a nullhypothesis of no variability beyond the error bars. 10 of 11 light curves areconfirmed as being significantly variable, and one of these seems to beperiodic, with two plausible periods identified. Another object is bestdescribed by the Ornstein-Uhlenbeck process, a conclusion which is obviouslylimited to the set of models actually tested.
arxiv-1500-73 | Time After Time: Notes on Delays In Spiking Neural P Systems | http://arxiv.org/pdf/1210.6119v1.pdf | author:Francis George C. Cabarle, Kelvin C. Buño, Henry N. Adorna category:cs.NE cs.DC cs.ET 97P20 F.1 published:2012-10-23 summary:Spiking Neural P systems, SNP systems for short, are biologically inspiredcomputing devices based on how neurons perform computations. SNP systems useonly one type of symbol, the spike, in the computations. Information is encodedin the time differences of spikes or the multiplicity of spikes produced atcertain times. SNP systems with delays (associated with rules) and thosewithout delays are two of several Turing complete SNP system variants inliterature. In this work we investigate how restricted forms of SNP systemswith delays can be simulated by SNP systems without delays. We show thesimulations for the following spike routing constructs: sequential, iteration,join, and split.
arxiv-1500-74 | Interplay: Dispersed Activation in Neural Networks | http://arxiv.org/pdf/1210.6082v1.pdf | author:Richard L. Churchill category:cs.NE q-bio.NC published:2012-10-22 summary:This paper presents a multi-point stimulation of a Hebbian neural networkwith investigation of the interplay between the stimulus waves through theneurons of the network. Equilibrium of the resulting memory is achieved forrecall of specific memory data at a rate faster than single point stimulus. Theinterplay of the intersecting stimuli appears to parallel the clarificationprocess of recall in biological systems.
arxiv-1500-75 | Classification Analysis Of Authorship Fiction Texts in The Space Of Semantic Fields | http://arxiv.org/pdf/1210.5965v1.pdf | author:Bohdan Pavlyshenko category:cs.CL published:2012-10-22 summary:The use of naive Bayesian classifier (NB) and the classifier by the k nearestneighbors (kNN) in classification semantic analysis of authors' texts ofEnglish fiction has been analysed. The authors' works are considered in thevector space the basis of which is formed by the frequency characteristics ofsemantic fields of nouns and verbs. Highly precise classification of authors'texts in the vector space of semantic fields indicates about the presence ofparticular spheres of author's idiolect in this space which characterizes theindividual author's style.
arxiv-1500-76 | Some Chances and Challenges in Applying Language Technologies to Historical Studies in Chinese | http://arxiv.org/pdf/1210.5898v1.pdf | author:Chao-Lin Liu, Guantao Jin, Qingfeng Liu, Wei-Yun Chiu, Yih-Soong Yu category:cs.CL cs.DL cs.IR published:2012-10-22 summary:We report applications of language technology to analyzing historicaldocuments in the Database for the Study of Modern Chinese Thoughts andLiterature (DSMCTL). We studied two historical issues with the reportedtechniques: the conceptualization of "huaren" (Chinese people) and the attemptto institute constitutional monarchy in the late Qing dynasty. We also discussresearch challenges for supporting sophisticated issues using our experiencewith DSMCTL, the Database of Government Officials of the Republic of China, andthe Dream of the Red Chamber. Advanced techniques and tools for lexical,syntactic, semantic, and pragmatic processing of language information, alongwith more thorough data collection, are needed to strengthen the collaborationbetween historians and computer scientists.
arxiv-1500-77 | Initialization of Self-Organizing Maps: Principal Components Versus Random Initialization. A Case Study | http://arxiv.org/pdf/1210.5873v1.pdf | author:A. A. Akinduko, E. M. Mirkes category:stat.ML cs.LG published:2012-10-22 summary:The performance of the Self-Organizing Map (SOM) algorithm is dependent onthe initial weights of the map. The different initialization methods canbroadly be classified into random and data analysis based initializationapproach. In this paper, the performance of random initialization (RI) approachis compared to that of principal component initialization (PCI) in which theinitial map weights are chosen from the space of the principal component.Performance is evaluated by the fraction of variance unexplained (FVU).Datasets were classified into quasi-linear and non-linear and it was observedthat RI performed better for non-linear datasets; however the performance ofPCI approach remains inconclusive for quasi-linear datasets.
arxiv-1500-78 | Supervised Learning with Similarity Functions | http://arxiv.org/pdf/1210.5840v1.pdf | author:Purushottam Kar, Prateek Jain category:cs.LG stat.ML published:2012-10-22 summary:We address the problem of general supervised learning when data can only beaccessed through an (indefinite) similarity function between data points.Existing work on learning with indefinite kernels has concentrated solely onbinary/multi-class classification problems. We propose a model that is genericenough to handle any supervised learning task and also subsumes the modelpreviously proposed for classification. We give a "goodness" criterion forsimilarity functions w.r.t. a given supervised learning task and then adapt awell-known landmarking technique to provide efficient algorithms for supervisedlearning using "good" similarity functions. We demonstrate the effectiveness ofour model on three important super-vised learning problems: a) real-valuedregression, b) ordinal regression and c) ranking where we show that our methodguarantees bounded generalization error. Furthermore, for the case ofreal-valued regression, we give a natural goodness definition that, when usedin conjunction with a recent result in sparse vector recovery, guarantees asparse predictor with bounded generalization error. Finally, we report resultsof our learning algorithms on regression and ordinal regression tasks usingnon-PSD similarity functions and demonstrate the effectiveness of ouralgorithms, especially that of the sparse landmark selection algorithm thatachieves significantly higher accuracies than the baseline methods whileoffering reduced computational costs.
arxiv-1500-79 | Multi-Stage Multi-Task Feature Learning | http://arxiv.org/pdf/1210.5806v1.pdf | author:Pinghua Gong, Jieping Ye, Changshui Zhang category:stat.ML published:2012-10-22 summary:Multi-task sparse feature learning aims to improve the generalizationperformance by exploiting the shared features among tasks. It has beensuccessfully applied to many applications including computer vision andbiomedical informatics. Most of the existing multi-task sparse feature learningalgorithms are formulated as a convex sparse regularization problem, which isusually suboptimal, due to its looseness for approximating an $\ell_0$-typeregularizer. In this paper, we propose a non-convex formulation for multi-tasksparse feature learning based on a novel non-convex regularizer. To solve thenon-convex optimization problem, we propose a Multi-Stage Multi-Task FeatureLearning (MSMTFL) algorithm; we also provide intuitive interpretations,detailed convergence and reproducibility analysis for the proposed algorithm.Moreover, we present a detailed theoretical analysis showing that MSMTFLachieves a better parameter estimation error bound than the convex formulation.Empirical studies on both synthetic and real-world data sets demonstrate theeffectiveness of MSMTFL in comparison with the state of the art multi-tasksparse feature learning algorithms.
arxiv-1500-80 | Extraction of domain-specific bilingual lexicon from comparable corpora: compositional translation and ranking | http://arxiv.org/pdf/1210.5751v1.pdf | author:Estelle Delpech, Béatrice Daille, Emmanuel Morin, Claire Lemaire category:cs.CL published:2012-10-21 summary:This paper proposes a method for extracting translations of morphologicallyconstructed terms from comparable corpora. The method is based on compositionaltranslation and exploits translation equivalences at the morpheme-level, whichallows for the generation of "fertile" translations (translation pairs in whichthe target term has more words than the source term). Ranking methods relyingon corpus-based and translation-based features are used to select the bestcandidate translation. We obtain an average precision of 91% on the Top1candidate translation. The method was tested on two language pairs(English-French and English-German) and with a small specialized comparablecorpora (400k words per language).
arxiv-1500-81 | Developing ICC Profile Using Gray Level Control In Offset Printing Process | http://arxiv.org/pdf/1210.5732v1.pdf | author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV published:2012-10-21 summary:In prepress department RGB image has to be converted to CMYK image. Tocontrol that amount of black, cyan, magenta and yellow has to be controlled byusing color separation method. Graycolor separation method is selected tocontrol the amounts of these colors because it increase the quality of printingalso. A single printer used for printing the same image on different paper alsoresults in different printed images. To remove this problem a different ICCprofile based on gray level control is developedand a sheet offset printer iscalibrated using that profile and a subjective evaluation shows satisfactoryresults for different quality papers.
arxiv-1500-82 | Optimal Computational Trade-Off of Inexact Proximal Methods | http://arxiv.org/pdf/1210.5034v2.pdf | author:Pierre Machart, Sandrine Anthoine, Luca Baldassarre category:cs.LG cs.CV cs.NA published:2012-10-18 summary:In this paper, we investigate the trade-off between convergence rate andcomputational cost when minimizing a composite functional withproximal-gradient methods, which are popular optimisation tools in machinelearning. We consider the case when the proximity operator is computed via aniterative procedure, which provides an approximation of the exact proximityoperator. In that case, we obtain algorithms with two nested loops. We showthat the strategy that minimizes the computational cost to reach a solutionwith a desired accuracy in finite time is to set the number of inner iterationsto a constant, which differs from the strategy indicated by a convergence rateanalysis. In the process, we also present a new procedure called SIP (that isSpeedy Inexact Proximal-gradient algorithm) that is both computationallyefficient and easy to implement. Our numerical experiments confirm thetheoretical findings and suggest that SIP can be a very competitive alternativeto the standard procedure.
arxiv-1500-83 | Identifications of concealed weapon in a Human Body | http://arxiv.org/pdf/1210.5653v1.pdf | author:Prof. Samir K. Bandyopadhyay, Biswajita Datta, Sudipta Roy category:cs.CV published:2012-10-20 summary:The detection of weapons concealed underneath a person cloths is very muchimportant to the improvement of the security of the public as well as thesafety of public assets like airports, buildings and railway stations etc.
arxiv-1500-84 | Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials | http://arxiv.org/pdf/1210.5644v1.pdf | author:Philipp Krähenbühl, Vladlen Koltun category:cs.CV cs.AI cs.LG published:2012-10-20 summary:Most state-of-the-art techniques for multi-class image segmentation andlabeling use conditional random fields defined over pixels or image regions.While region-level models often feature dense pairwise connectivity,pixel-level models are considerably larger and have only permitted sparse graphstructures. In this paper, we consider fully connected CRF models defined onthe complete set of pixels in an image. The resulting graphs have billions ofedges, making traditional inference algorithms impractical. Our maincontribution is a highly efficient approximate inference algorithm for fullyconnected CRF models in which the pairwise edge potentials are defined by alinear combination of Gaussian kernels. Our experiments demonstrate that denseconnectivity at the pixel level substantially improves segmentation andlabeling accuracy.
arxiv-1500-85 | Hidden Trends in 90 Years of Harvard Business Review | http://arxiv.org/pdf/1210.5581v1.pdf | author:Chia-Chi Tsai, Chao-Lin Liu, Wei-Jie Huang, Man-Kwan Shan category:cs.CL cs.DL cs.IR published:2012-10-20 summary:In this paper, we demonstrate and discuss results of our mining the abstractsof the publications in Harvard Business Review between 1922 and 2012.Techniques for computing n-grams, collocations, basic sentiment analysis, andnamed-entity recognition were employed to uncover trends hidden in theabstracts. We present findings about international relationships, sentiment inHBR's abstracts, important international companies, influential technologicalinventions, renown researchers in management theories, US presidents viachronological analyses.
arxiv-1500-86 | Online Learning in Decentralized Multiuser Resource Sharing Problems | http://arxiv.org/pdf/1210.5544v1.pdf | author:Cem Tekin, Mingyan Liu category:cs.LG published:2012-10-19 summary:In this paper, we consider the general scenario of resource sharing in adecentralized system when the resource rewards/qualities are time-varying andunknown to the users, and using the same resource by multiple users leads toreduced quality due to resource sharing. Firstly, we consider auser-independent reward model with no communication between the users, where auser gets feedback about the congestion level in the resource it uses.Secondly, we consider user-specific rewards and allow costly communicationbetween the users. The users have a cooperative goal of achieving the highestsystem utility. There are multiple obstacles in achieving this goal such as thedecentralized nature of the system, unknown resource qualities, communication,computation and switching costs. We propose distributed learning algorithmswith logarithmic regret with respect to the optimal allocation. Our logarithmicregret result holds under both i.i.d. and Markovian reward models, as well asunder communication, computation and switching costs.
arxiv-1500-87 | Modeling with Copulas and Vines in Estimation of Distribution Algorithms | http://arxiv.org/pdf/1210.5500v1.pdf | author:Marta Soto, Yasser González-Fernández, Alberto Ochoa category:cs.NE stat.ME published:2012-10-19 summary:The aim of this work is studying the use of copulas and vines in theoptimization with Estimation of Distribution Algorithms (EDAs). Two EDAs arebuilt around the multivariate product and normal copulas, and other two arebased on pair-copula decomposition of vine models. Empirically we study theeffect of both marginal distributions and dependence structure separately, andshow that both aspects play a crucial role in the success of the optimization.The results show that the use of copulas and vines opens new opportunities to amore appropriate modeling of search distributions in EDAs.
arxiv-1500-88 | Design of English-Hindi Translation Memory for Efficient Translation | http://arxiv.org/pdf/1210.5517v1.pdf | author:Nisheeth Joshi, Iti Mathur category:cs.CL published:2012-10-19 summary:Developing parallel corpora is an important and a difficult activity forMachine Translation. This requires manual annotation by Human Translators.Translating same text again is a useless activity. There are tools available toimplement this for European Languages, but no such tool is available for IndianLanguages. In this paper we present a tool for Indian Languages which not onlyprovides automatic translations of the previously available translation butalso provides multiple translations, in cases where a sentence has multipletranslations, in ranked list of suggestive translations for a sentence.Moreover this tool also lets translators have global and local saving optionsof their work, so that they may share it with others, which further lightensthe task.
arxiv-1500-89 | Disentangling Factors of Variation via Generative Entangling | http://arxiv.org/pdf/1210.5474v1.pdf | author:Guillaume Desjardins, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG cs.NE published:2012-10-19 summary:Here we propose a novel model family with the objective of learning todisentangle the factors of variation in data. Our approach is based on thespike-and-slab restricted Boltzmann machine which we generalize to includehigher-order interactions among multiple latent variables. Seen from agenerative perspective, the multiplicative interactions emulates the entanglingof factors of variation. Inference in the model can be seen as disentanglingthese generative factors. Unlike previous attempts at disentangling latentfactors, the proposed model is trained using no supervised informationregarding the latent factors. We apply our model to the task of facialexpression classification.
arxiv-1500-90 | Collaborative Ensemble Learning: Combining Collaborative and Content-Based Information Filtering via Hierarchical Bayes | http://arxiv.org/pdf/1212.2508v1.pdf | author:Kai Yu, Anton Schwaighofer, Volker Tresp, Wei-Ying Ma, HongJiang Zhang category:cs.LG cs.IR stat.ML published:2012-10-19 summary:Collaborative filtering (CF) and content-based filtering (CBF) have widelybeen used in information filtering applications. Both approaches have theirstrengths and weaknesses which is why researchers have developed hybridsystems. This paper proposes a novel approach to unify CF and CBF in aprobabilistic framework, named collaborative ensemble learning. It usesprobabilistic SVMs to model each user's profile (as CBF does).At the predictionphase, it combines a society OF users profiles, represented by their respectiveSVM models, to predict an active users preferences(the CF idea).The combinationscheme is embedded in a probabilistic framework and retains an intuitiveexplanation.Moreover, collaborative ensemble learning does not require a globaltraining stage and thus can incrementally incorporate new data.We reportresults based on two data sets. For the Reuters-21578 text data set, wesimulate user ratings under the assumption that each user is interested in onlyone category. In the second experiment, we use users' opinions on a set of 642art images that were collected through a web-based survey. For both data sets,collaborative ensemble achieved excellent performance in terms ofrecommendation accuracy.
arxiv-1500-91 | Markov Random Walk Representations with Continuous Distributions | http://arxiv.org/pdf/1212.2510v1.pdf | author:Chen-Hsiang Yeang, Martin Szummer category:cs.LG stat.ML published:2012-10-19 summary:Representations based on random walks can exploit discrete data distributionsfor clustering and classification. We extend such representations from discreteto continuous distributions. Transition probabilities are now calculated usinga diffusion equation with a diffusion coefficient that inversely depends on thedata density. We relate this diffusion equation to a path integral and derivethe corresponding path probability measure. The framework is useful forincorporating continuous data densities and prior knowledge.
arxiv-1500-92 | Stochastic complexity of Bayesian networks | http://arxiv.org/pdf/1212.2511v1.pdf | author:Keisuke Yamazaki, Sumio Watanbe category:cs.LG stat.ML published:2012-10-19 summary:Bayesian networks are now being used in enormous fields, for example,diagnosis of a system, data mining, clustering and so on. In spite of theirwide range of applications, the statistical properties have not yet beenclarified, because the models are nonidentifiable and non-regular. In aBayesian network, the set of its parameter for a smaller model is an analyticset with singularities in the space of large ones. Because of thesesingularities, the Fisher information matrices are not positive definite. Inother words, the mathematical foundation for learning was not constructed. Inrecent years, however, we have developed a method to analyze non-regular modelsusing algebraic geometry. This method revealed the relation between the modelssingularities and its statistical properties. In this paper, applying thismethod to Bayesian networks with latent variables, we clarify the order of thestochastic complexities.Our result claims that the upper bound of those issmaller than the dimension of the parameter space. This means that the Bayesiangeneralization error is also far smaller than that of regular model, and thatSchwarzs model selection criterion BIC needs to be improved for Bayesiannetworks.
arxiv-1500-93 | A Generalized Mean Field Algorithm for Variational Inference in Exponential Families | http://arxiv.org/pdf/1212.2512v1.pdf | author:Eric P. Xing, Michael I. Jordan, Stuart Russell category:cs.LG stat.ML published:2012-10-19 summary:The mean field methods, which entail approximating intractable probabilitydistributions variationally with distributions from a tractable family, enjoyhigh efficiency, guaranteed convergence, and provide lower bounds on the truelikelihood. But due to requirement for model-specific derivation of theoptimization equations and unclear inference quality in various models, it isnot widely used as a generic approximate inference algorithm. In this paper, wediscuss a generalized mean field theory on variational approximation to a broadclass of intractable distributions using a rich set of tractable distributionsvia constrained optimization over distribution spaces. We present a class ofgeneralized mean field (GMF) algorithms for approximate inference in complexexponential family models, which entails limiting the optimization over theclass of cluster-factorizable distributions. GMF is a generic method requiringno model-specific derivations. It factors a complex model into a set ofdisjoint variable clusters, and uses a set of canonical fix-point equations toiteratively update the cluster distributions, and converge to locally optimalcluster marginals that preserve the original dependency structure within eachcluster, hence, fully decomposed the overall inference problem. We empiricallyanalyzed the effect of different tractable family (clusters of differentgranularity) on inference quality, and compared GMF with BP on severalcanonical models. Possible extension to higher-order MF approximation is alsodiscussed.
arxiv-1500-94 | Efficient Parametric Projection Pursuit Density Estimation | http://arxiv.org/pdf/1212.2513v1.pdf | author:Max Welling, Richard S. Zemel, Geoffrey E. Hinton category:cs.LG stat.ML published:2012-10-19 summary:Product models of low dimensional experts are a powerful way to avoid thecurse of dimensionality. We present the ``under-complete product of experts'(UPoE), where each expert models a one dimensional projection of the data. TheUPoE is fully tractable and may be interpreted as a parametric probabilisticmodel for projection pursuit. Its ML learning rules are identical to theapproximate learning rules proposed before for under-complete ICA. We alsoderive an efficient sequential learning algorithm and discuss its relationshipto projection pursuit density estimation and feature induction algorithms foradditive random field models.
arxiv-1500-95 | Boltzmann Machine Learning with the Latent Maximum Entropy Principle | http://arxiv.org/pdf/1212.2514v1.pdf | author:Shaojun Wang, Dale Schuurmans, Fuchun Peng, Yunxin Zhao category:cs.LG stat.ML published:2012-10-19 summary:We present a new statistical learning paradigm for Boltzmann machines basedon a new inference principle we have proposed: the latent maximum entropyprinciple (LME). LME is different both from Jaynes maximum entropy principleand from standard maximum likelihood estimation.We demonstrate the LMEprinciple BY deriving new algorithms for Boltzmann machine parameterestimation, and show how robust and fast new variant of the EM algorithm can bedeveloped.Our experiments show that estimation based on LME generally yieldsbetter results than maximum likelihood estimation, particularly when inferringhidden units from small amounts of data.
arxiv-1500-96 | Learning Measurement Models for Unobserved Variables | http://arxiv.org/pdf/1212.2516v1.pdf | author:Ricardo Silva, Richard Scheines, Clark Glymour, Peter L. Spirtes category:cs.LG stat.ML published:2012-10-19 summary:Observed associations in a database may be due in whole or part to variationsin unrecorded (latent) variables. Identifying such variables and their causalrelationships with one another is a principal goal in many scientific andpractical domains. Previous work shows that, given a partition of observedvariables such that members of a class share only a single latent common cause,standard search algorithms for causal Bayes nets can infer structural relationsbetween latent variables. We introduce an algorithm for discovering suchpartitions when they exist. Uniquely among available procedures, the algorithmis (asymptotically) correct under standard assumptions in causal Bayes netsearch algorithms, requires no prior knowledge of the number of latentvariables, and does not depend on the mathematical form of the relationshipsamong the latent variables. We evaluate the algorithm on a variety of simulateddata sets.
arxiv-1500-97 | Learning Module Networks | http://arxiv.org/pdf/1212.2517v1.pdf | author:Eran Segal, Dana Pe'er, Aviv Regev, Daphne Koller, Nir Friedman category:cs.LG cs.CE stat.ML published:2012-10-19 summary:Methods for learning Bayesian network structure can discover dependencystructure between observed variables, and have been shown to be useful in manyapplications. However, in domains that involve a large number of variables, thespace of possible network structures is enormous, making it difficult, for bothcomputational and statistical reasons, to identify a good model. In this paper,we consider a solution to this problem, suitable for domains where manyvariables have similar behavior. Our method is based on a new class of models,which we call module networks. A module network explicitly represents thenotion of a module - a set of variables that have the same parents in thenetwork and share the same conditional probability distribution. We define thesemantics of module networks, and describe an algorithm that learns a modulenetwork from data. The algorithm learns both the partitioning of the variablesinto modules and the dependency structure between the variables. We evaluateour algorithm on synthetic data, and on real data in the domains of geneexpression and the stock market. Our results show that module networksgeneralize better than Bayesian networks, and that the learned module networkstructure reveals regularities that are obscured in learned Bayesian networks.
arxiv-1500-98 | On the Convergence of Bound Optimization Algorithms | http://arxiv.org/pdf/1212.2490v1.pdf | author:Ruslan R Salakhutdinov, Sam T Roweis, Zoubin Ghahramani category:cs.LG stat.ML published:2012-10-19 summary:Many practitioners who use the EM algorithm complain that it is sometimesslow. When does this happen, and what can be done about it? In this paper, westudy the general class of bound optimization algorithms - includingExpectation-Maximization, Iterative Scaling and CCCP - and their relationshipto direct optimization algorithms such as gradient-based methods for parameterlearning. We derive a general relationship between the updates performed bybound optimization methods and those of gradient and second-order methods andidentify analytic conditions under which bound optimization algorithms exhibitquasi-Newton behavior, and conditions under which they possess poor,first-order convergence. Based on this analysis, we consider several specificalgorithms, interpret and analyze their convergence properties and provide somerecipes for preprocessing input to these algorithms to yield faster convergencebehavior. We report empirical results supporting our analysis and showing thatsimple data preprocessing can result in dramatically improved performance ofbound optimizers in practice.
arxiv-1500-99 | Automated Analytic Asymptotic Evaluation of the Marginal Likelihood for Latent Models | http://arxiv.org/pdf/1212.2491v1.pdf | author:Dmitry Rusakov, Dan Geiger category:cs.LG stat.ML published:2012-10-19 summary:We present and implement two algorithms for analytic asymptotic evaluation ofthe marginal likelihood of data given a Bayesian network with hidden nodes. Asshown by previous work, this evaluation is particularly hard for latentBayesian network models, namely networks that include hidden variables, whereasymptotic approximation deviates from the standard BIC score. Our algorithmssolve two central difficulties in asymptotic evaluation of marginal likelihoodintegrals, namely, evaluation of regular dimensionality drop for latentBayesian network models and computation of non-standard approximation formulasfor singular statistics for these models. The presented algorithms areimplemented in Matlab and Maple and their usage is demonstrated for marginallikelihood approximations for Bayesian networks with hidden variables.
arxiv-1500-100 | Learning Generative Models of Similarity Matrices | http://arxiv.org/pdf/1212.2494v1.pdf | author:Romer Rosales, Brendan J. Frey category:cs.LG stat.ML published:2012-10-19 summary:We describe a probabilistic (generative) view of affinity matrices along withinference algorithms for a subclass of problems associated with dataclustering. This probabilistic view is helpful in understanding differentmodels and algorithms that are based on affinity functions OF the data. INparticular, we show how(greedy) inference FOR a specific probabilistic model ISequivalent TO the spectral clustering algorithm.It also provides a frameworkFOR developing new algorithms AND extended models. AS one CASE, we present newgenerative data clustering models that allow us TO infer the underlyingdistance measure suitable for the clustering problem at hand. These models seemto perform well in a larger class of problems for which other clusteringalgorithms (including spectral clustering) usually fail. Experimentalevaluation was performed in a variety point data sets, showing excellentperformance.
arxiv-1500-101 | Learning Continuous Time Bayesian Networks | http://arxiv.org/pdf/1212.2498v1.pdf | author:Uri Nodelman, Christian R. Shelton, Daphne Koller category:cs.LG stat.ML published:2012-10-19 summary:Continuous time Bayesian networks (CTBNs) describe structured stochasticprocesses with finitely many states that evolve over continuous time. A CTBN isa directed (possibly cyclic) dependency graph over a set of variables, each ofwhich represents a finite state continuous time Markov process whose transitionmodel is a function of its parents. We address the problem of learningparameters and structure of a CTBN from fully observed data. We define aconjugate prior for CTBNs, and show how it can be used both for Bayesianparameter estimation and as the basis of a Bayesian score for structurelearning. Because acyclicity is not a constraint in CTBNs, we can show that thestructure learning problem is significantly easier, both in theory and inpractice, than structure learning for dynamic Bayesian networks (DBNs).Furthermore, as CTBNs can tailor the parameters and dependency structure to thedifferent time granularities of the evolution of different variables, they canprovide a better fit to continuous-time processes than DBNs with a fixed timegranularity.
arxiv-1500-102 | On Local Optima in Learning Bayesian Networks | http://arxiv.org/pdf/1212.2500v1.pdf | author:Jens D. Nielsen, Tomas Kocka, Jose M. Pena category:cs.LG cs.AI stat.ML published:2012-10-19 summary:This paper proposes and evaluates the k-greedy equivalence search algorithm(KES) for learning Bayesian networks (BNs) from complete data. The maincharacteristic of KES is that it allows a trade-off between greediness andrandomness, thus exploring different good local optima. When greediness is setat maximum, KES corresponds to the greedy equivalence search algorithm (GES).When greediness is kept at minimum, we prove that under mild assumptions KESasymptotically returns any inclusion optimal BN with nonzero probability.Experimental results for both synthetic and real data are reported showing thatKES often finds a better local optima than GES. Moreover, we use KES toexperimentally confirm that the number of different local optima is often huge.
arxiv-1500-103 | Practically Perfect | http://arxiv.org/pdf/1212.2503v1.pdf | author:Christopher Meek, David Maxwell Chickering category:cs.AI stat.ML published:2012-10-19 summary:The property of perfectness plays an important role in the theory of Bayesiannetworks. First, the existence of perfect distributions for arbitrary sets ofvariables and directed acyclic graphs implies that various methods for readingindependence from the structure of the graph (e.g., Pearl, 1988; Lauritzen,Dawid, Larsen & Leimer, 1990) are complete. Second, the asymptotic reliabilityof various search methods is guaranteed under the assumption that thegenerating distribution is perfect (e.g., Spirtes, Glymour & Scheines, 2000;Chickering & Meek, 2002). We provide a lower-bound on the probability ofsampling a non-perfect distribution when using a fixed number of bits torepresent the parameters of the Bayesian network. This bound approaches zeroexponentially fast as one increases the number of bits used to represent theparameters. This result implies that perfect distributions with fixed-lengthrepresentations exist. We also provide a lower-bound on the number of bitsneeded to guarantee that a distribution sampled from a uniform Dirichletdistribution is perfect with probability greater than 1/2. This result isuseful for constructing randomized reductions for hardness proofs.
arxiv-1500-104 | Efficiently Inducing Features of Conditional Random Fields | http://arxiv.org/pdf/1212.2504v1.pdf | author:Andrew McCallum category:cs.LG stat.ML published:2012-10-19 summary:Conditional Random Fields (CRFs) are undirected graphical models, a specialcase of which correspond to conditionally-trained finite state machines. A keyadvantage of these models is their great flexibility to include a wide array ofoverlapping, multi-granularity, non-independent features of the input. In faceof this freedom, an important question that remains is, what features should beused? This paper presents a feature induction method for CRFs. Founded on theprinciple of constructing only those feature conjunctions that significantlyincrease log-likelihood, the approach is based on that of Della Pietra et al[1997], but altered to work with conditional rather than joint probabilities,and with additional modifications for providing tractability specifically for asequence model. In comparison with traditional approaches, automated featureinduction offers both improved accuracy and more than an order of magnitudereduction in feature count; it enables the use of richer, higher-order Markovmodels, and offers more freedom to liberally guess about which atomic featuresmay be relevant to a task. The induction method applies to linear-chain CRFs,as well as to more arbitrary CRF structures, also known as Relational MarkovNetworks [Taskar & Koller, 2002]. We present experimental results on a namedentity extraction task.
arxiv-1500-105 | Monte Carlo Matrix Inversion Policy Evaluation | http://arxiv.org/pdf/1212.2471v1.pdf | author:Fletcher Lu, Dale Schuurmans category:cs.LG cs.AI cs.NA published:2012-10-19 summary:In 1950, Forsythe and Leibler (1950) introduced a statistical technique forfinding the inverse of a matrix by characterizing the elements of the matrixinverse as expected values of a sequence of random walks. Barto and Duff (1994)subsequently showed relations between this technique and standard dynamicprogramming and temporal differencing methods. The advantage of the Monte Carlomatrix inversion (MCMI) approach is that it scales better with respect tostate-space size than alternative techniques. In this paper, we introduce analgorithm for performing reinforcement learning policy evaluation using MCMI.We demonstrate that MCMI improves on runtime over a maximum likelihoodmodel-based policy evaluation approach and on both runtime and accuracy overthe temporal differencing (TD) policy evaluation approach. We further improveon MCMI policy evaluation by adding an importance sampling technique to ouralgorithm to reduce the variance of our estimator. Lastly, we illustratetechniques for scaling up MCMI to large state spaces in order to perform policyimprovement.
arxiv-1500-106 | Budgeted Learning of Naive-Bayes Classifiers | http://arxiv.org/pdf/1212.2472v1.pdf | author:Daniel J. Lizotte, Omid Madani, Russell Greiner category:cs.LG stat.ML published:2012-10-19 summary:Frequently, acquiring training data has an associated cost. We consider thesituation where the learner may purchase data during training, subject TO abudget. IN particular, we examine the CASE WHERE each feature label has anassociated cost, AND the total cost OF ALL feature labels acquired duringtraining must NOT exceed the budget.This paper compares methods FOR choosingwhich feature label TO purchase next, given the budget AND the CURRENT beliefstate OF naive Bayes model parameters.Whereas active learning has traditionallyfocused ON myopic(greedy) strategies FOR query selection, this paper presents atractable method FOR incorporating knowledge OF the budget INTO the decisionmaking process, which improves performance.
arxiv-1500-107 | Learning Riemannian Metrics | http://arxiv.org/pdf/1212.2474v1.pdf | author:Guy Lebanon category:cs.LG stat.ML published:2012-10-19 summary:We propose a solution to the problem of estimating a Riemannian metricassociated with a given differentiable manifold. The metric learning problem isbased on minimizing the relative volume of a given set of points. We derive thedetails for a family of metrics on the multinomial simplex. The resultingmetric has applications in text classification and bears some similarity toTFIDF representation of text documents.
arxiv-1500-108 | Efficient Gradient Estimation for Motor Control Learning | http://arxiv.org/pdf/1212.2475v1.pdf | author:Gregory Lawrence, Noah Cowan, Stuart Russell category:cs.LG cs.SY published:2012-10-19 summary:The task of estimating the gradient of a function in the presence of noise iscentral to several forms of reinforcement learning, including policy searchmethods. We present two techniques for reducing gradient estimation errors inthe presence of observable input noise applied to the control signal. The firstmethod extends the idea of a reinforcement baseline by fitting a local linearmodel to the function whose gradient is being estimated; we show how to findthe linear model that minimizes the variance of the gradient estimate, and howto estimate the model from data. The second method improves this further bydiscounting components of the gradient vector that have high variance. Thesemethods are applied to the problem of motor control learning, where actuatornoise has a significant influence on behavior. In particular, we apply thetechniques to learn locally optimal controllers for a dart-throwing task usinga simulated three-link arm; we demonstrate that proposed methods significantlyimprove the reward function gradient estimate and, consequently, the learningcurve, over existing methods.
arxiv-1500-109 | 1 Billion Pages = 1 Million Dollars? Mining the Web to Play "Who Wants to be a Millionaire?" | http://arxiv.org/pdf/1212.2477v1.pdf | author:Shyong, K. Lam, David M Pennock, Dan Cosley, Steve Lawrence category:cs.IR cs.CL published:2012-10-19 summary:We exploit the redundancy and volume of information on the web to build acomputerized player for the ABC TV game show 'Who Wants To Be A Millionaire?'The player consists of a question-answering module and a decision-makingmodule. The question-answering module utilizes question transformationtechniques, natural language parsing, multiple information retrievalalgorithms, and multiple search engines; results are combined in the spirit ofensemble learning using an adaptive weighting scheme. Empirically, the systemcorrectly answers about 75% of questions from the Millionaire CD-ROM, 3rdedition - general-interest trivia questions often about popular culture andcommon knowledge. The decision-making module chooses from allowable actions inthe game in order to maximize expected risk-adjusted winnings, where theestimated probability of answering correctly is a function of past performanceand confidence in in correctly answering the current question. When given a sixquestion head start (i.e., when starting from the $2,000 level), we find thatthe system performs about as well on average as humans starting at thebeginning. Our system demonstrates the potential of simple but well-chosentechniques for mining answers from unstructured information such as the web.
arxiv-1500-110 | Approximate Inference and Constrained Optimization | http://arxiv.org/pdf/1212.2480v1.pdf | author:Tom Heskes, Kees Albers, Hilbert Kappen category:cs.LG cs.AI stat.ML published:2012-10-19 summary:Loopy and generalized belief propagation are popular algorithms forapproximate inference in Markov random fields and Bayesian networks. Fixedpoints of these algorithms correspond to extrema of the Bethe and Kikuchi freeenergy. However, belief propagation does not always converge, which explainsthe need for approaches that explicitly minimize the Kikuchi/Bethe free energy,such as CCCP and UPS. Here we describe a class of algorithms that solves thistypically nonconvex constrained minimization of the Kikuchi free energy througha sequence of convex constrained minimizations of upper bounds on the Kikuchifree energy. Intuitively one would expect tighter bounds to lead to fasteralgorithms, which is indeed convincingly demonstrated in our simulations.Several ideas are applied to obtain tight convex bounds that yield dramaticspeed-ups over CCCP.
arxiv-1500-111 | Sufficient Dimensionality Reduction with Irrelevant Statistics | http://arxiv.org/pdf/1212.2483v1.pdf | author:Amir Globerson, Gal Chechik, Naftali Tishby category:cs.LG stat.ML published:2012-10-19 summary:The problem of finding a reduced dimensionality representation of categoricalvariables while preserving their most relevant characteristics is fundamentalfor the analysis of complex data. Specifically, given a co-occurrence matrix oftwo variables, one often seeks a compact representation of one variable whichpreserves information about the other variable. We have recently introduced``Sufficient Dimensionality Reduction' [GT-2003], a method that extractscontinuous reduced dimensional features whose measurements (i.e., expectationvalues) capture maximal mutual information among the variables. However, suchmeasurements often capture information that is irrelevant for a given task.Widely known examples are illumination conditions, which are irrelevant asfeatures for face recognition, writing style which is irrelevant as a featurefor content classification, and intonation which is irrelevant as a feature forspeech recognition. Such irrelevance cannot be deduced apriori, since itdepends on the details of the task, and is thus inherently ill defined in thepurely unsupervised case. Separating relevant from irrelevant features can beachieved using additional side data that contains such irrelevant structures.This approach was taken in [CT-2002], extending the information bottleneckmethod, which uses clustering to compress the data. Here we use thisside-information framework to identify features whose measurements aremaximally informative for the original data set, but carry as littleinformation as possible on a side data set. In statistical terms this can beunderstood as extracting statistics which are maximally sufficient for theoriginal dataset, while simultaneously maximally ancillary for the sidedataset. We formulate this tradeoff as a constrained optimization problem andcharacterize its solutions. We then derive a gradient descent algorithm forthis problem, which is based on the Generalized Iterative Scaling method forfinding maximum entropy distributions. The method is demonstrated on syntheticdata, as well as on real face recognition datasets, and is shown to outperformstandard methods such as oriented PCA.
arxiv-1500-112 | Locally Weighted Naive Bayes | http://arxiv.org/pdf/1212.2487v1.pdf | author:Eibe Frank, Mark Hall, Bernhard Pfahringer category:cs.LG stat.ML published:2012-10-19 summary:Despite its simplicity, the naive Bayes classifier has surprised machinelearning researchers by exhibiting good performance on a variety of learningproblems. Encouraged by these results, researchers have looked to overcomenaive Bayes primary weakness - attribute independence - and improve theperformance of the algorithm. This paper presents a locally weighted version ofnaive Bayes that relaxes the independence assumption by learning local modelsat prediction time. Experimental results show that locally weighted naive Bayesrarely degrades accuracy compared to standard naive Bayes and, in many cases,improves accuracy dramatically. The main advantage of this method compared toother techniques for enhancing naive Bayes is its conceptual and computationalsimplicity.
arxiv-1500-113 | A Distance-Based Branch and Bound Feature Selection Algorithm | http://arxiv.org/pdf/1212.2488v1.pdf | author:Ari Frank, Dan Geiger, Zohar Yakhini category:cs.LG stat.ML published:2012-10-19 summary:There is no known efficient method for selecting k Gaussian features from nwhich achieve the lowest Bayesian classification error. We show an example ofhow greedy algorithms faced with this task are led to give results that are notoptimal. This motivates us to propose a more robust approach. We present aBranch and Bound algorithm for finding a subset of k independent Gaussianfeatures which minimizes the naive Bayesian classification error. Our algorithmuses additive monotonic distance measures to produce bounds for the Bayesianclassification error in order to exclude many feature subsets from evaluation,while still returning an optimal solution. We test our method on synthetic dataas well as data obtained from gene expression profiling.
arxiv-1500-114 | The Information Bottleneck EM Algorithm | http://arxiv.org/pdf/1212.2460v1.pdf | author:Gal Elidan, Nir Friedman category:cs.LG stat.ML published:2012-10-19 summary:Learning with hidden variables is a central challenge in probabilisticgraphical models that has important implications for many real-life problems.The classical approach is using the Expectation Maximization (EM) algorithm.This algorithm, however, can get trapped in local maxima. In this paper weexplore a new approach that is based on the Information Bottleneck principle.In this approach, we view the learning problem as a tradeoff between twoinformation theoretic objectives. The first is to make the hidden variablesuninformative about the identity of specific instances. The second is to makethe hidden variables informative about the observed attributes. By exploringdifferent tradeoffs between these two objectives, we can gradually converge ona high-scoring solution. As we show, the resulting, Information BottleneckExpectation Maximization (IB-EM) algorithm, manages to find solutions that aresuperior to standard EM methods.
arxiv-1500-115 | A New Algorithm for Maximum Likelihood Estimation in Gaussian Graphical Models for Marginal Independence | http://arxiv.org/pdf/1212.2462v1.pdf | author:Mathias Drton, Thomas S. Richardson category:stat.ME cs.LG stat.ML published:2012-10-19 summary:Graphical models with bi-directed edges (<->) represent marginalindependence: the absence of an edge between two vertices indicates that thecorresponding variables are marginally independent. In this paper, we considermaximum likelihood estimation in the case of continuous variables with aGaussian joint distribution, sometimes termed a covariance graph model. Wepresent a new fitting algorithm which exploits standard regression techniquesand establish its convergence properties. Moreover, we contrast our procedureto existing estimation methods.
arxiv-1500-116 | A Robust Independence Test for Constraint-Based Learning of Causal Structure | http://arxiv.org/pdf/1212.2464v1.pdf | author:Denver Dash, Marek J. Druzdzel category:cs.AI cs.LG stat.ML published:2012-10-19 summary:Constraint-based (CB) learning is a formalism for learning a causal networkwith a database D by performing a series of conditional-independence tests toinfer structural information. This paper considers a new test of independencethat combines ideas from Bayesian learning, Bayesian network inference, andclassical hypothesis testing to produce a more reliable and robust test. Thenew test can be calculated in the same asymptotic time and space required forthe standard tests such as the chi-squared test, but it allows thespecification of a prior distribution over parameters and can be used when thedatabase is incomplete. We prove that the test is correct, and we demonstrateempirically that, when used with a CB causal discovery algorithm withnoninformative priors, it recovers structural features more reliably and itproduces networks with smaller KL-Divergence, especially as the number of nodesincreases or the number of records decreases. Another benefit is the dramaticreduction in the probability that a CB algorithm will stall during the search,providing a remedy for an annoying problem plaguing CB learning when thedatabase is small.
arxiv-1500-117 | On Information Regularization | http://arxiv.org/pdf/1212.2466v1.pdf | author:Adrian Corduneanu, Tommi S. Jaakkola category:cs.LG stat.ML published:2012-10-19 summary:We formulate a principle for classification with the knowledge of themarginal distribution over the data points (unlabeled data). The principle iscast in terms of Tikhonov style regularization where the regularization penaltyarticulates the way in which the marginal density should constrain otherwiseunrestricted conditional distributions. Specifically, the regularizationpenalty penalizes any information introduced between the examples and labelsbeyond what is provided by the available labeled examples. The work extendsSzummer and Jaakkola's information regularization (NIPS 2002) to multipledimensions, providing a regularizer independent of the covering of the spaceused in the derivation. We show in addition how the information regularizer canbe used as a measure of complexity of the classification task with unlabeleddata and prove a relevant sample-complexity bound. We illustrate theregularization principle in practice by restricting the class of conditionaldistributions to be logistic regression models and constructing theregularization penalty from a finite set of unlabeled examples.
arxiv-1500-118 | Large-Sample Learning of Bayesian Networks is NP-Hard | http://arxiv.org/pdf/1212.2468v1.pdf | author:David Maxwell Chickering, Christopher Meek, David Heckerman category:cs.LG cs.AI stat.ML published:2012-10-19 summary:In this paper, we provide new complexity results for algorithms that learndiscrete-variable Bayesian networks from data. Our results apply whenever thelearning algorithm uses a scoring criterion that favors the simplest model ableto represent the generative distribution exactly. Our results therefore holdwhenever the learning algorithm uses a consistent scoring criterion and isapplied to a sufficiently large dataset. We show that identifying high-scoringstructures is hard, even when we are given an independence oracle, an inferenceoracle, and/or an information oracle. Our negative results also apply to thelearning of discrete-variable Bayesian networks in which each node has at mostk parents, for all k > 3.
arxiv-1500-119 | Reasoning about Bayesian Network Classifiers | http://arxiv.org/pdf/1212.2470v1.pdf | author:Hei Chan, Adnan Darwiche category:cs.LG cs.AI stat.ML published:2012-10-19 summary:Bayesian network classifiers are used in many fields, and one common class ofclassifiers are naive Bayes classifiers. In this paper, we introduce anapproach for reasoning about Bayesian network classifiers in which weexplicitly convert them into Ordered Decision Diagrams (ODDs), which are thenused to reason about the properties of these classifiers. Specifically, wepresent an algorithm for converting any naive Bayes classifier into an ODD, andwe show theoretically and experimentally that this algorithm can give us an ODDthat is tractable in size even given an intractable number of instances. SinceODDs are tractable representations of classifiers, our algorithm allows us toefficiently test the equivalence of two naive Bayes classifiers andcharacterize discrepancies between them. We also show a number of additionalresults including a count of distinct classifiers that can be induced bychanging some CPT in a naive Bayes classifier, and the range of allowablechanges to a CPT which keeps the current classifier unchanged.
arxiv-1500-120 | Active Collaborative Filtering | http://arxiv.org/pdf/1212.2442v1.pdf | author:Craig Boutilier, Richard S. Zemel, Benjamin Marlin category:cs.IR cs.LG stat.ML published:2012-10-19 summary:Collaborative filtering (CF) allows the preferences of multiple users to bepooled to make recommendations regarding unseen products. We consider in thispaper the problem of online and interactive CF: given the current ratingsassociated with a user, what queries (new ratings) would most improve thequality of the recommendations made? We cast this terms of expected value ofinformation (EVOI); but the online computational cost of computing optimalqueries is prohibitive. We show how offline prototyping and computation ofbounds on EVOI can be used to dramatically reduce the required onlinecomputation. The framework we develop is general, but we focus on derivationsand empirical study in the specific case of the multiple-cause vectorquantization model.
arxiv-1500-121 | Bayesian Hierarchical Mixtures of Experts | http://arxiv.org/pdf/1212.2447v1.pdf | author:Christopher M. Bishop, Markus Svensen category:cs.LG stat.ML published:2012-10-19 summary:The Hierarchical Mixture of Experts (HME) is a well-known tree-based modelfor regression and classification, based on soft probabilistic splits. In itsoriginal formulation it was trained by maximum likelihood, and is thereforeprone to over-fitting. Furthermore the maximum likelihood framework offers nonatural metric for optimizing the complexity and structure of the tree.Previous attempts to provide a Bayesian treatment of the HME model have reliedeither on ad-hoc local Gaussian approximations or have dealt with relatedmodels representing the joint distribution of both input and output variables.In this paper we describe a fully Bayesian treatment of the HME model based onvariational inference. By combining local and global variational methods weobtain a rigourous lower bound on the marginal probability of the data underthe model. This bound is optimized during the training phase, and its resultingvalue can be used for model order selection. We present results using thisapproach for a data set describing robot arm kinematics.
arxiv-1500-122 | Web-Based Question Answering: A Decision-Making Perspective | http://arxiv.org/pdf/1212.2453v1.pdf | author:David Azari, Eric J. Horvitz, Susan Dumais, Eric Brill category:cs.IR cs.CL published:2012-10-19 summary:We describe an investigation of the use of probabilistic models andcost-benefit analyses to guide resource-intensive procedures used by aWeb-based question answering system. We first provide an overview of researchon question-answering systems. Then, we present details on AskMSR, a prototypeweb-based question answering system. We discuss Bayesian analyses of thequality of answers generated by the system and show how we can endow the systemwith the ability to make decisions about the number of queries issued to asearch engine, given the cost of queries and the expected value of queryresults in refining an ultimate answer. Finally, we review the results of a setof experiments.
arxiv-1500-123 | Bayesian Estimation for Continuous-Time Sparse Stochastic Processes | http://arxiv.org/pdf/1210.5394v1.pdf | author:Arash Amini, Ulugbek S. Kamilov, Emrah Bostan, Michael Unser category:cs.LG published:2012-10-19 summary:We consider continuous-time sparse stochastic processes from which we haveonly a finite number of noisy/noiseless samples. Our goal is to estimate thenoiseless samples (denoising) and the signal in-between (interpolationproblem). By relying on tools from the theory of splines, we derive the joint a prioridistribution of the samples and show how this probability density function canbe factorized. The factorization enables us to tractably implement the maximuma posteriori and minimum mean-square error (MMSE) criteria as two statisticalapproaches for estimating the unknowns. We compare the derived statisticalmethods with well-known techniques for the recovery of sparse signals, such asthe $\ell_1$ norm and Log ($\ell_1$-$\ell_0$ relaxation) regularizationmethods. The simulation results show that, under certain conditions, theperformance of the regularization techniques can be very close to that of theMMSE estimator.
arxiv-1500-124 | Adaptive Stratified Sampling for Monte-Carlo integration of Differentiable functions | http://arxiv.org/pdf/1210.5345v1.pdf | author:Alexandra Carpentier, Rémi Munos category:stat.ML published:2012-10-19 summary:We consider the problem of adaptive stratified sampling for Monte Carlointegration of a differentiable function given a finite number of evaluationsto the function. We construct a sampling scheme that samples more often inregions where the function oscillates more, while allocating the samples suchthat they are well spread on the domain (this notion shares similitude with lowdiscrepancy). We prove that the estimate returned by the algorithm is almostsimilarly accurate as the estimate that an optimal oracle strategy (that wouldknow the variations of the function everywhere) would return, and provide afinite-sample analysis.
arxiv-1500-125 | The origin of Mayan languages from Formosan language group of Austronesian | http://arxiv.org/pdf/1210.5321v1.pdf | author:Koji Ohnishi category:cs.CL q-bio.PE published:2012-10-19 summary:Basic body-part names (BBPNs) were defined as body-part names in Swadeshbasic 200 words. Non-Mayan cognates of Mayan (MY) BBPNs were extensivelysearched for, by comparing with non-MY vocabulary, including ca.1300 basicwords of 82 AN languages listed by Tryon (1985), etc. Thus found cognates (CGs)in non-MY are listed in Table 1, as classified by language groups to which mostsimilar cognates (MSCs) of MY BBPNs belong. CGs of MY are classified to 23mutually unrelated CG-items, of which 17.5 CG-items have their MSCs inAustronesian (AN), giving its closest similarity score (CSS), CSS(AN) = 17.5,which consists of 10.33 MSCs in Formosan, 1.83 MSCs in WesternMalayo-Polynesian (W.MP), 0.33 in Central MP, 0.0 in SHWNG, and 5.0 in Oceanic[i.e., CSS(FORM)= 10.33, CSS(W.MP) = 1.88, ..., CSS(OC)= 5.0]. These CSSs forlanguage (sub)groups are also listed in the underline portion of every sectionof (Section1 - Section 6) in Table 1. Chi-squar test (degree of freedom = 1)using [Eq 1] and [Eqs.2] revealed that MSCs of MY BBPNs are distributed inFormosan in significantly higher frequency (P < 0.001) than in other subgroupsof AN, as well as than in non-AN languages. MY is thus concluded to have beenderived from Formosan of AN. Eskimo shows some BBPN similarities to FORM andMY.
arxiv-1500-126 | Least Absolute Gradient Selector: Statistical Regression via Pseudo-Hard Thresholding | http://arxiv.org/pdf/1204.2353v4.pdf | author:Kun Yang category:stat.ML stat.AP stat.ME published:2012-04-11 summary:Variable selection in linear models plays a pivotal role in modernstatistics. Hard-thresholding methods such as $l_0$ regularization aretheoretically ideal but computationally infeasible. In this paper, we propose anew approach, called the LAGS, short for "least absulute gradient selector", tothis challenging yet interesting problem by mimicking the discrete selectionprocess of $l_0$ regularization. To estimate $\beta$ under the influence ofnoise, we consider, nevertheless, the following convex program [\hat{\beta} =\textrm{arg min}\frac{1}{n}\X^{T}(y - X\beta)\_1 + \lambda_n\sum_{i =1}^pw_i(y;X;n)\beta_i] $\lambda_n > 0$ controls the sparsity and $w_i > 0$ dependent on $y, X$ and$n$ is the weights on different $\beta_i$; $n$ is the sample size.Surprisingly, we shall show in the paper, both geometrically and analytically,that LAGS enjoys two attractive properties: (1) LAGS demonstrates discreteselection behavior and hard thresholding property as $l_0$ regularization bystrategically chosen $w_i$, we call this property "pseudo-hard thresholding";(2) Asymptotically, LAGS is consistent and capable of discovering the truemodel; nonasymptotically, LAGS is capable of identifying the sparsity in themodel and the prediction error of the coefficients is bounded at the noiselevel up to a logarithmic factor---$\log p$, where $p$ is the number ofpredictors. Computationally, LAGS can be solved efficiently by convex program routinesfor its convexity or by simplex algorithm after recasting it into a linearprogram. The numeric simulation shows that LAGS is superior compared tosoft-thresholding methods in terms of mean squared error and parsimony of themodel.
arxiv-1500-127 | Matrix reconstruction with the local max norm | http://arxiv.org/pdf/1210.5196v1.pdf | author:Rina Foygel, Nathan Srebro, Ruslan Salakhutdinov category:stat.ML cs.LG published:2012-10-18 summary:We introduce a new family of matrix norms, the "local max" norms,generalizing existing methods such as the max norm, the trace norm (nuclearnorm), and the weighted or smoothed weighted trace norms, which have beenextensively used in the literature as regularizers for matrix reconstructionproblems. We show that this new family can be used to interpolate between the(weighted or unweighted) trace norm and the more conservative max norm. We testthis interpolation on simulated data and on the large-scale Netflix andMovieLens ratings data, and find improved accuracy relative to the existingmatrix norms. We also provide theoretical results showing learning guaranteesfor some of the new norms.
arxiv-1500-128 | Modeling transition dynamics in MDPs with RKHS embeddings of conditional distributions | http://arxiv.org/pdf/1112.4722v2.pdf | author:Steffen Grünewälder, Luca Baldassarre, Massimiliano Pontil, Arthur Gretton, Guy Lever category:cs.LG published:2011-12-20 summary:We propose a new, nonparametric approach to estimating the value function inreinforcement learning. This approach makes use of a recently developedrepresentation of conditional distributions as functions in a reproducingkernel Hilbert space. Such representations bypass the need for estimatingtransition probabilities, and apply to any domain on which kernels can bedefined. Our approach avoids the need to approximate intractable integralssince expectations are represented as RKHS inner products whose computation haslinear complexity in the sample size. Thus, we can efficiently perform valuefunction estimation in a wide variety of settings, including finite statespaces, continuous states spaces, and partially observable tasks where onlysensor measurements are available. A second advantage of the approach is thatwe learn the conditional distribution representation from a training sample,and do not require an exhaustive exploration of the state space. We proveconvergence of our approach either to the optimal policy, or to the closestprojection of the optimal policy in our model class, under reasonableassumptions. In experiments, we demonstrate the performance of our algorithm ona learning task in a continuous state space (the under-actuated pendulum), andon a navigation problem where only images from a sensor are observed. Wecompare with least-squares policy iteration where a Gaussian process is usedfor value function estimation. Our algorithm achieves better performance inboth tasks.
arxiv-1500-129 | LSBN: A Large-Scale Bayesian Structure Learning Framework for Model Averaging | http://arxiv.org/pdf/1210.5135v1.pdf | author:Yang Lu, Mengying Wang, Menglu Li, Qili Zhu, Bo Yuan category:cs.LG stat.ML published:2012-10-18 summary:The motivation for this paper is to apply Bayesian structure learning usingModel Averaging in large-scale networks. Currently, Bayesian model averagingalgorithm is applicable to networks with only tens of variables, restrained byits super-exponential complexity. We present a novel framework, calledLSBN(Large-Scale Bayesian Network), making it possible to handle networks withinfinite size by following the principle of divide-and-conquer. The method ofLSBN comprises three steps. In general, LSBN first performs the partition byusing a second-order partition strategy, which achieves more robust results.LSBN conducts sampling and structure learning within each overlapping communityafter the community is isolated from other variables by Markov Blanket. FinallyLSBN employs an efficient algorithm, to merge structures of overlappingcommunities into a whole. In comparison with other four state-of-artlarge-scale network structure learning algorithms such as ARACNE, PC, GreedySearch and MMHC, LSBN shows comparable results in five common benchmarkdatasets, evaluated by precision, recall and f-score. What's more, LSBN makesit possible to learn large-scale Bayesian structure by Model Averaging whichused to be intractable. In summary, LSBN provides an scalable and parallelframework for the reconstruction of network structures. Besides, the completeinformation of overlapping communities serves as the byproduct, which could beused to mine meaningful clusters in biological networks, such asprotein-protein-interaction network or gene regulatory network, as well as insocial network.
arxiv-1500-130 | A Novel Learning Algorithm for Bayesian Network and Its Efficient Implementation on GPU | http://arxiv.org/pdf/1210.5128v1.pdf | author:Yu Wang, Weikang Qian, Shuchang Zhang, Bo Yuan category:cs.DC cs.LG published:2012-10-18 summary:Computational inference of causal relationships underlying complex networks,such as gene-regulatory pathways, is NP-complete due to its combinatorialnature when permuting all possible interactions. Markov chain Monte Carlo(MCMC) has been introduced to sample only part of the combinations while stillguaranteeing convergence and traversability, which therefore becomes widelyused. However, MCMC is not able to perform efficiently enough for networks thathave more than 15~20 nodes because of the computational complexity. In thispaper, we use general purpose processor (GPP) and general purpose graphicsprocessing unit (GPGPU) to implement and accelerate a novel Bayesian networklearning algorithm. With a hash-table-based memory-saving strategy and a noveltask assigning strategy, we achieve a 10-fold acceleration per iteration thanusing a serial GPP. Specially, we use a greedy method to search for the bestgraph from a given order. We incorporate a prior component in the currentscoring function, which further facilitates the searching. Overall, we are ableto apply this system to networks with more than 60 nodes, allowing inferencesand modeling of bigger and more complex networks than current methods.
arxiv-1500-131 | Cramer Rao-Type Bounds for Sparse Bayesian Learning | http://arxiv.org/pdf/1202.1119v2.pdf | author:Ranjitha Prasad, Chandra R. Murthy category:cs.LG stat.ML published:2012-02-06 summary:In this paper, we derive Hybrid, Bayesian and Marginalized Cram\'{e}r-Raolower bounds (HCRB, BCRB and MCRB) for the single and multiple measurementvector Sparse Bayesian Learning (SBL) problem of estimating compressiblevectors and their prior distribution parameters. We assume the unknown vectorto be drawn from a compressible Student-t prior distribution. We derive CRBsthat encompass the deterministic or random nature of the unknown parameters ofthe prior distribution and the regression noise variance. We extend the MCRB tothe case where the compressible vector is distributed according to a generalcompressible prior distribution, of which the generalized Pareto distributionis a special case. We use the derived bounds to uncover the relationshipbetween the compressibility and Mean Square Error (MSE) in the estimates.Further, we illustrate the tightness and utility of the bounds throughsimulations, by comparing them with the MSE performance of two popularSBL-based estimators. It is found that the MCRB is generally the tightest amongthe bounds derived and that the MSE performance of the Expectation-Maximization(EM) algorithm coincides with the MCRB for the compressible vector. Throughsimulations, we demonstrate the dependence of the MSE performance of SBL basedestimators on the compressibility of the vector for several values of thenumber of observations and at different signal powers.
arxiv-1500-132 | Justice blocks and predictability of US Supreme Court votes | http://arxiv.org/pdf/1210.4768v1.pdf | author:Roger Guimera, Marta Sales-Pardo category:physics.soc-ph stat.ML published:2012-10-17 summary:Successful attempts to predict judges' votes shed light into how legaldecisions are made and, ultimately, into the behavior and evolution of thejudiciary. Here, we investigate to what extent it is possible to makepredictions of a justice's vote based on the other justices' votes in the samecase. For our predictions, we use models and methods that have been developedto uncover hidden associations between actors in complex social networks. Weshow that these methods are more accurate at predicting justice's votes thanforecasts made by legal experts and by algorithms that take into considerationthe content of the cases. We argue that, within our framework, highpredictability is a quantitative proxy for stable justice (and case) blocks,which probably reflect stable a priori attitudes toward the law. We find thatU. S. Supreme Court justice votes are more predictable than one would expectfrom an ideal court composed of perfectly independent justices. Deviations fromideal behavior are most apparent in divided 5-4 decisions, where justice blocksseem to be most stable. Moreover, we find evidence that justice predictabilitydecreased during the 50-year period spanning from the Warren Court to theRehnquist Court, and that aggregate court predictability has been significantlylower during Democratic presidencies. More broadly, our results show that it ispossible to use methods developed for the analysis of complex social networksto quantitatively investigate historical questions related to politicaldecision-making.
arxiv-1500-133 | Mixture model for designs in high dimensional regression and the LASSO | http://arxiv.org/pdf/1210.4762v1.pdf | author:Stéphane Chrétien category:math.ST stat.ML stat.TH published:2012-10-17 summary:The LASSO is a recent technique for variable selection in the regressionmodel \bean y & = & X\beta +\epsilon, \eean where $X\in \R^{n\times p}$ and$\epsilon$ is a centered gaussian i.i.d. noise vector $\mathcalN(0,\sigma^2I)$. The LASSO has been proved to perform exact support recoveryfor regression vectors when the design matrix satisfies certain algebraicconditions and $\beta$ is sufficiently sparse. Estimation of the vector$X\beta$ has also extensively been studied for the purpose of prediction underthe same algebraic conditions on $X$ and under sufficient sparsity of $\beta$.Among many other, the coherence is an index which can be used to study thesenice properties of the LASSO. More precisely, a small coherence implies thatmost sparse vectors, with less nonzero components than the order $n/\log(p)$,can be recovered with high probability if its nonzero components are largerthan the order $\sigma \sqrt{\log(p)}$. However, many matrices occuring inpractice do not have a small coherence and thus, most results which haveappeared in the litterature cannot be applied. The goal of this paper is tostudy a model for which precise results can be obtained. In the proposed model,the columns of the design matrix are drawn from a Gaussian mixture model andthe coherence condition is imposed on the much smaller matrix whose columns arethe mixture's centers, instead of on $X$ itself. Our main theorem states that$X\beta$ is as well estimated as in the case of small coherence up to acorrection parametrized by the maximal variance in the mixture model.
arxiv-1500-134 | Regulating the information in spikes: a useful bias | http://arxiv.org/pdf/1210.4695v1.pdf | author:David Balduzzi category:q-bio.NC cs.IT cs.LG math.IT published:2012-10-17 summary:The bias/variance tradeoff is fundamental to learning: increasing a model'scomplexity can improve its fit on training data, but potentially worsensperformance on future samples. Remarkably, however, the human braineffortlessly handles a wide-range of complex pattern recognition tasks. On thebasis of these conflicting observations, it has been argued that useful biasesin the form of "generic mechanisms for representation" must be hardwired intocortex (Geman et al). This note describes a useful bias that encourages cooperative learning whichis both biologically plausible and rigorously justified.
arxiv-1500-135 | Learning Dictionaries with Bounded Self-Coherence | http://arxiv.org/pdf/1205.6210v2.pdf | author:Christian D. Sigg, Tomas Dikk, Joachim M. Buhmann category:stat.ML cs.LG published:2012-05-28 summary:Sparse coding in learned dictionaries has been established as a successfulapproach for signal denoising, source separation and solving inverse problemsin general. A dictionary learning method adapts an initial dictionary to aparticular signal class by iteratively computing an approximate factorizationof a training data matrix into a dictionary and a sparse coding matrix. Thelearned dictionary is characterized by two properties: the coherence of thedictionary to observations of the signal class, and the self-coherence of thedictionary atoms. A high coherence to the signal class enables the sparsecoding of signal observations with a small approximation error, while a lowself-coherence of the atoms guarantees atom recovery and a more rapid residualerror decay rate for the sparse coding algorithm. The two goals of high signalcoherence and low self-coherence are typically in conflict, therefore one seeksa trade-off between them, depending on the application. We present a dictionarylearning method with an effective control over the self-coherence of thetrained dictionary, enabling a trade-off between maximizing the sparsity ofcodings and approximating an equiangular tight frame.
arxiv-1500-136 | Mean-Field Learning: a Survey | http://arxiv.org/pdf/1210.4657v1.pdf | author:Hamidou Tembine, Raul Tempone, Pedro Vilanova category:cs.LG cs.GT cs.MA math.DS stat.ML published:2012-10-17 summary:In this paper we study iterative procedures for stationary equilibria ingames with large number of players. Most of learning algorithms for games withcontinuous action spaces are limited to strict contraction best reply maps inwhich the Banach-Picard iteration converges with geometrical convergence rate.When the best reply map is not a contraction, Ishikawa-based learning isproposed. The algorithm is shown to behave well for Lipschitz continuous andpseudo-contractive maps. However, the convergence rate is still unsatisfactory.Several acceleration techniques are presented. We explain how cognitive userscan improve the convergence rate based only on few number of measurements. Themethodology provides nice properties in mean field games where the payofffunction depends only on own-action and the mean of the mean-field (firstmoment mean-field games). A learning framework that exploits the structure ofsuch games, called, mean-field learning, is proposed. The proposed mean-fieldlearning framework is suitable not only for games but also for non-convexglobal optimization problems. Then, we introduce mean-field learning withoutfeedback and examine the convergence to equilibria in beauty contest games,which have interesting applications in financial markets. Finally, we provide afully distributed mean-field learning and its speedup versions for satisfactorysolution in wireless networks. We illustrate the convergence rate improvementwith numerical examples.
arxiv-1500-137 | Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization | http://arxiv.org/pdf/1003.3967v4.pdf | author:Daniel Golovin, Andreas Krause category:cs.LG cs.AI cs.DS published:2010-03-21 summary:Solving stochastic optimization problems under partial observability, whereone needs to adaptively make decisions with uncertain outcomes, is afundamental but notoriously difficult challenge. In this paper, we introducethe concept of adaptive submodularity, generalizing submodular set functions toadaptive policies. We prove that if a problem satisfies this property, a simpleadaptive greedy algorithm is guaranteed to be competitive with the optimalpolicy. In addition to providing performance guarantees for both stochasticmaximization and coverage, adaptive submodularity can be exploited todrastically speed up the greedy algorithm by using lazy evaluations. Weillustrate the usefulness of the concept by giving several examples of adaptivesubmodular objectives arising in diverse applications including sensorplacement, viral marketing and active learning. Proving adaptive submodularityfor these problems allows us to recover existing results in these applicationsas special cases, improve approximation guarantees and handle naturalgeneralizations.
arxiv-1500-138 | A Direct Approach to Multi-class Boosting and Extensions | http://arxiv.org/pdf/1210.4601v1.pdf | author:Chunhua Shen, Sakrapee Paisitkriangkrai, Anton van den Hengel category:cs.LG published:2012-10-17 summary:Boosting methods combine a set of moderately accurate weaklearners to form ahighly accurate predictor. Despite the practical importance of multi-classboosting, it has received far less attention than its binary counterpart. Inthis work, we propose a fully-corrective multi-class boosting formulation whichdirectly solves the multi-class problem without dividing it into multiplebinary classification problems. In contrast, most previous multi-class boostingalgorithms decompose a multi-boost problem into multiple binary boostingproblems. By explicitly deriving the Lagrange dual of the primal optimizationproblem, we are able to construct a column generation-based fully-correctiveapproach to boosting which directly optimizes multi-class classificationperformance. The new approach not only updates all weak learners' coefficientsat every iteration, but does so in a manner flexible enough to accommodatevarious loss functions and regularizations. For example, it enables us tointroduce structural sparsity through mixed-norm regularization to promotegroup sparsity and feature sharing. Boosting with shared features isparticularly beneficial in complex prediction problems where features can beexpensive to compute. Our experiments on various data sets demonstrate that ourdirect multi-class boosting generalizes as well as, or better than, a range ofcompeting multi-class boosting methods. The end result is a highly effectiveand compact ensemble classifier which can be trained in a distributed fashion.
arxiv-1500-139 | Efficient algorithms for training the parameters of hidden Markov models using stochastic expectation maximization EM training and Viterbi training | http://arxiv.org/pdf/0909.0737v2.pdf | author:Tin Yin Lam, Irmtraud M. Meyer category:q-bio.QM cs.LG q-bio.GN published:2009-09-03 summary:Background: Hidden Markov models are widely employed by numerousbioinformatics programs used today. Applications range widely from comparativegene prediction to time-series analyses of micro-array data. The parameters ofthe underlying models need to be adjusted for specific data sets, for examplethe genome of a particular species, in order to maximize the predictionaccuracy. Computationally efficient algorithms for parameter training are thuskey to maximizing the usability of a wide range of bioinformatics applications. Results: We introduce two computationally efficient training algorithms, onefor Viterbi training and one for stochastic expectation maximization (EM)training, which render the memory requirements independent of the sequencelength. Unlike the existing algorithms for Viterbi and stochastic EM trainingwhich require a two-step procedure, our two new algorithms require only onestep and scan the input sequence in only one direction. We also implement thesetwo new algorithms and the already published linear-memory algorithm for EMtraining into the hidden Markov model compiler HMM-Converter and examine theirrespective practical merits for three small example models. Conclusions: Bioinformatics applications employing hidden Markov models canuse the two algorithms in order to make Viterbi training and stochastic EMtraining more computationally efficient. Using these algorithms, parametertraining can thus be attempted for more complex models and longer trainingsequences. The two new algorithms have the added advantage of being easier toimplement than the corresponding default algorithms for Viterbi training andstochastic EM training.
arxiv-1500-140 | Factorized Multi-Modal Topic Model | http://arxiv.org/pdf/1210.4920v1.pdf | author:Seppo Virtanen, Yangqing Jia, Arto Klami, Trevor Darrell category:cs.LG cs.IR stat.ML published:2012-10-16 summary:Multi-modal data collections, such as corpora of paired images and textsnippets, require analysis methods beyond single-view component and topicmodels. For continuous observations the current dominant approach is based onextensions of canonical correlation analysis, factorizing the variation intocomponents shared by the different modalities and those private to each ofthem. For count data, multiple variants of topic models attempting to tie themodalities together have been presented. All of these, however, lack theability to learn components private to one modality, and consequently will tryto force dependencies even between minimally correlating modalities. In thiswork we combine the two approaches by presenting a novel HDP-based topic modelthat automatically learns both shared and private topics. The model is shown tobe especially useful for querying the contents of one domain given samples ofthe other.
arxiv-1500-141 | Latent Dirichlet Allocation Uncovers Spectral Characteristics of Drought Stressed Plants | http://arxiv.org/pdf/1210.4919v1.pdf | author:Mirwaes Wahabzada, Kristian Kersting, Christian Bauckhage, Christoph Roemer, Agim Ballvora, Francisco Pinto, Uwe Rascher, Jens Leon, Lutz Ploemer category:cs.LG cs.CE stat.ML published:2012-10-16 summary:Understanding the adaptation process of plants to drought stress is essentialin improving management practices, breeding strategies as well as engineeringviable crops for a sustainable agriculture in the coming decades.Hyper-spectral imaging provides a particularly promising approach to gain suchunderstanding since it allows to discover non-destructively spectralcharacteristics of plants governed primarily by scattering and absorptioncharacteristics of the leaf internal structure and biochemical constituents.Several drought stress indices have been derived using hyper-spectral imaging.However, they are typically based on few hyper-spectral images only, rely oninterpretations of experts, and consider few wavelengths only. In this study,we present the first data-driven approach to discovering spectral droughtstress indices, treating it as an unsupervised labeling problem at massivescale. To make use of short range dependencies of spectral wavelengths, wedevelop an online variational Bayes algorithm for latent Dirichlet allocationwith convolved Dirichlet regularizer. This approach scales to massive datasetsand, hence, provides a more objective complement to plant physiologicalpractices. The spectral topics found conform to plant physiological knowledgeand can be computed in a fraction of the time compared to existing LDAapproaches.
arxiv-1500-142 | Dynamic Teaching in Sequential Decision Making Environments | http://arxiv.org/pdf/1210.4918v1.pdf | author:Thomas J. Walsh, Sergiu Goschin category:cs.LG cs.AI stat.ML published:2012-10-16 summary:We describe theoretical bounds and a practical algorithm for teaching a modelby demonstration in a sequential decision making environment. Unlike previousefforts that have optimized learners that watch a teacher demonstrate a staticpolicy, we focus on the teacher as a decision maker who can dynamically choosedifferent policies to teach different parts of the environment. We developseveral teaching frameworks based on previously defined supervised protocols,such as Teaching Dimension, extending them to handle noise and sequences ofinputs encountered in an MDP.We provide theoretical bounds on the learnabilityof several important model classes in this setting and suggest a practicalalgorithm for dynamic teaching.
arxiv-1500-143 | Fast Graph Construction Using Auction Algorithm | http://arxiv.org/pdf/1210.4917v1.pdf | author:Jun Wang, Yinglong Xia category:cs.LG stat.ML published:2012-10-16 summary:In practical machine learning systems, graph based data representation hasbeen widely used in various learning paradigms, ranging from unsupervisedclustering to supervised classification. Besides those applications withnatural graph or network structure data, such as social network analysis andrelational learning, many other applications often involve a critical step inconverting data vectors to an adjacency graph. In particular, a sparse subgraphextracted from the original graph is often required due to both theoretic andpractical needs. Previous study clearly shows that the performance of differentlearning algorithms, e.g., clustering and classification, benefits from suchsparse subgraphs with balanced node connectivity. However, the existing graphconstruction methods are either computationally expensive or withunsatisfactory performance. In this paper, we utilize a scalable method calledauction algorithm and its parallel extension to recover a sparse yet nearlybalanced subgraph with significantly reduced computational cost. Empiricalstudy and comparison with the state-ofart approaches clearly demonstrate thesuperiority of the proposed method in both efficiency and accuracy.
arxiv-1500-144 | Latent Structured Ranking | http://arxiv.org/pdf/1210.4914v1.pdf | author:Jason Weston, John Blitzer category:cs.LG cs.IR stat.ML published:2012-10-16 summary:Many latent (factorized) models have been proposed for recommendation taskslike collaborative filtering and for ranking tasks like document or imageretrieval and annotation. Common to all those methods is that during inferencethe items are scored independently by their similarity to the query in thelatent embedding space. The structure of the ranked list (i.e. considering theset of items returned as a whole) is not taken into account. This can be aproblem because the set of top predictions can be either too diverse (containresults that contradict each other) or are not diverse enough. In this paper weintroduce a method for learning latent structured rankings that improves overexisting methods by providing the right blend of predictions at the top of theranked list. Particular emphasis is put on making this method scalable.Empirical results on large scale image annotation and music recommendationtasks show improvements over existing approaches.
arxiv-1500-145 | An Improved Admissible Heuristic for Learning Optimal Bayesian Networks | http://arxiv.org/pdf/1210.4913v1.pdf | author:Changhe Yuan, Brandon Malone category:cs.AI cs.LG stat.ML published:2012-10-16 summary:Recently two search algorithms, A* and breadth-first branch and bound(BFBnB), were developed based on a simple admissible heuristic for learningBayesian network structures that optimize a scoring function. The heuristicrepresents a relaxation of the learning problem such that each variable choosesoptimal parents independently. As a result, the heuristic may contain manydirected cycles and result in a loose bound. This paper introduces an improvedadmissible heuristic that tries to avoid directed cycles within small groups ofvariables. A sparse representation is also introduced to store only the uniqueoptimal parent choices. Empirical results show that the new techniquessignificantly improved the efficiency and scalability of A* and BFBnB on mostof datasets tested in this paper.
arxiv-1500-146 | New Advances and Theoretical Insights into EDML | http://arxiv.org/pdf/1210.4910v1.pdf | author:Khaled S. Refaat, Arthur Choi, Adnan Darwiche category:cs.AI cs.LG stat.ML published:2012-10-16 summary:EDML is a recently proposed algorithm for learning MAP parameters in Bayesiannetworks. In this paper, we present a number of new advances and insights onthe EDML algorithm. First, we provide the multivalued extension of EDML,originally proposed for Bayesian networks over binary variables. Next, weidentify a simplified characterization of EDML that further implies a simplefixed-point algorithm for the convex optimization problem that underlies it.This characterization further reveals a connection between EDML and EM: a fixedpoint of EDML is a fixed point of EM, and vice versa. We thus identify also anew characterization of EM fixed points, but in the semantics of EDML. Finally,we propose a hybrid EDML/EM algorithm that takes advantage of the improvedempirical convergence behavior of EDML, while maintaining the monotonicimprovement property of EM.
arxiv-1500-147 | Active Learning with Distributional Estimates | http://arxiv.org/pdf/1210.4909v1.pdf | author:Jens Roeder, Boaz Nadler, Kevin Kunzmann, Fred A. Hamprecht category:cs.LG stat.ML published:2012-10-16 summary:Active Learning (AL) is increasingly important in a broad range ofapplications. Two main AL principles to obtain accurate classification with fewlabeled data are refinement of the current decision boundary and exploration ofpoorly sampled regions. In this paper we derive a novel AL scheme that balancesthese two principles in a natural way. In contrast to many AL strategies, whichare based on an estimated class conditional probability ^p(yx), a keycomponent of our approach is to view this quantity as a random variable, henceexplicitly considering the uncertainty in its estimated value. Our maincontribution is a novel mathematical framework for uncertainty-based AL, and acorresponding AL scheme, where the uncertainty in ^p(yx) is modeled by asecond-order distribution. On the practical side, we show how to approximatesuch second-order distributions for kernel density classification. Finally, wefind that over a large number of UCI, USPS and Caltech4 datasets, our AL schemeachieves significantly better learning curves than popular AL methods such asuncertainty sampling and error reduction sampling, when all use the same kerneldensity classifier.
arxiv-1500-148 | Latent Composite Likelihood Learning for the Structured Canonical Correlation Model | http://arxiv.org/pdf/1210.4905v1.pdf | author:Ricardo Silva category:stat.ML cs.LG published:2012-10-16 summary:Latent variable models are used to estimate variables of interest quantitieswhich are observable only up to some measurement error. In many studies, suchvariables are known but not precisely quantifiable (such as "job satisfaction"in social sciences and marketing, "analytical ability" in educational testing,or "inflation" in economics). This leads to the development of measurementinstruments to record noisy indirect evidence for such unobserved variablessuch as surveys, tests and price indexes. In such problems, there arepostulated latent variables and a given measurement model. At the same time,other unantecipated latent variables can add further unmeasured confounding tothe observed variables. The problem is how to deal with unantecipated latentsvariables. In this paper, we provide a method loosely inspired by canonicalcorrelation that makes use of background information concerning the "known"latent variables. Given a partially specified structure, it provides astructure learning approach to detect "unknown unknowns," the confoundingeffect of potentially infinitely many other latent variables. This is donewithout explicitly modeling such extra latent factors. Because of the specialstructure of the problem, we are able to exploit a new variation of compositelikelihood fitting to efficiently learn this structure. Validation is providedwith experiments in synthetic data and the analysis of a large survey done witha sample of over 100,000 staff members of the National Health Service of theUnited Kingdom.
arxiv-1500-149 | Efficiently Searching for Frustrated Cycles in MAP Inference | http://arxiv.org/pdf/1210.4902v1.pdf | author:David Sontag, Do Kook Choe, Yitao Li category:cs.DS cs.LG stat.ML published:2012-10-16 summary:Dual decomposition provides a tractable framework for designing algorithmsfor finding the most probable (MAP) configuration in graphical models. However,for many real-world inference problems, the typical decomposition has a largeintegrality gap, due to frustrated cycles. One way to tighten the relaxation isto introduce additional constraints that explicitly enforce cycle consistency.Earlier work showed that cluster-pursuit algorithms, which iterativelyintroduce cycle and other higherorder consistency constraints, allows one toexactly solve many hard inference problems. However, these algorithmsexplicitly enumerate a candidate set of clusters, limiting them to triplets orother short cycles. We solve the search problem for cycle constraints, giving anearly linear time algorithm for finding the most frustrated cycle of arbitrarylength. We show how to use this search algorithm together with the dualdecomposition framework and clusterpursuit. The new algorithm exactly solvesMAP inference problems arising from relational classification and stereovision.
arxiv-1500-150 | Fast Exact Inference for Recursive Cardinality Models | http://arxiv.org/pdf/1210.4899v1.pdf | author:Daniel Tarlow, Kevin Swersky, Richard S. Zemel, Ryan Prescott Adams, Brendan J. Frey category:cs.LG stat.ML published:2012-10-16 summary:Cardinality potentials are a generally useful class of high order potentialthat affect probabilities based on how many of D binary variables are active.Maximum a posteriori (MAP) inference for cardinality potential models iswell-understood, with efficient computations taking O(DlogD) time. Yetefficient marginalization and sampling have not been addressed as thoroughly inthe machine learning community. We show that there exists a simple algorithmfor computing marginal probabilities and drawing exact joint samples that runsin O(Dlog2 D) time, and we show how to frame the algorithm as efficient beliefpropagation in a low order tree-structured model that includes additionalauxiliary variables. We then develop a new, more general class of models,termed Recursive Cardinality models, which take advantage of this efficiency.Finally, we show how to do efficient exact inference in models composed of atree structure and a cardinality potential. We explore the expressive power ofRecursive Cardinality models and empirically demonstrate their utility.
arxiv-1500-151 | Value Function Approximation in Noisy Environments Using Locally Smoothed Regularized Approximate Linear Programs | http://arxiv.org/pdf/1210.4898v1.pdf | author:Gavin Taylor, Ron Parr category:cs.LG stat.ML published:2012-10-16 summary:Recently, Petrik et al. demonstrated that L1Regularized Approximate LinearProgramming (RALP) could produce value functions and policies which comparedfavorably to established linear value function approximation techniques likeLSPI. RALP's success primarily stems from the ability to solve the featureselection and value function approximation steps simultaneously. RALP'sperformance guarantees become looser if sampled next states are used. For verynoisy domains, RALP requires an accurate model rather than samples, which canbe unrealistic in some practical scenarios. In this paper, we demonstrate thisweakness, and then introduce Locally Smoothed L1-Regularized Approximate LinearProgramming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuraciesstemming from noise even without an accurate model. We show that, given somesmoothness assumptions, as the number of samples increases, error from noiseapproaches zero, and provide experimental examples of LS-RALP's success oncommon reinforcement learning benchmark problems.
arxiv-1500-152 | Closed-Form Learning of Markov Networks from Dependency Networks | http://arxiv.org/pdf/1210.4896v1.pdf | author:Daniel Lowd category:cs.LG cs.AI stat.ML published:2012-10-16 summary:Markov networks (MNs) are a powerful way to compactly represent a jointprobability distribution, but most MN structure learning methods are very slow,due to the high cost of evaluating candidates structures. Dependency networks(DNs) represent a probability distribution as a set of conditional probabilitydistributions. DNs are very fast to learn, but the conditional distributionsmay be inconsistent with each other and few inference algorithms support DNs.In this paper, we present a closed-form method for converting a DN into an MN,allowing us to enjoy both the efficiency of DN learning and the convenience ofthe MN representation. When the DN is consistent, this conversion is exact. Forinconsistent DNs, we present averaging methods that significantly improve theapproximation. In experiments on 12 standard datasets, our methods are ordersof magnitude faster than and often more accurate than combining conditionaldistributions using weight learning.
arxiv-1500-153 | Sparse Q-learning with Mirror Descent | http://arxiv.org/pdf/1210.4893v1.pdf | author:Sridhar Mahadevan, Bo Liu category:cs.LG stat.ML published:2012-10-16 summary:This paper explores a new framework for reinforcement learning based ononline convex optimization, in particular mirror descent and relatedalgorithms. Mirror descent can be viewed as an enhanced gradient method,particularly suited to minimization of convex functions in highdimensionalspaces. Unlike traditional gradient methods, mirror descent undertakes gradientupdates of weights in both the dual space and primal space, which are linkedtogether using a Legendre transform. Mirror descent can be viewed as a proximalalgorithm where the distance generating function used is a Bregman divergence.A new class of proximal-gradient based temporal-difference (TD) methods arepresented based on different Bregman divergences, which are more powerful thanregular TD learning. Examples of Bregman divergences that are studied includep-norm functions, and Mahalanobis distance based on the covariance of samplegradients. A new family of sparse mirror-descent reinforcement learning methodsare proposed, which are able to find sparse fixed points of an l1-regularizedBellman equation at significantly less computational cost than previous methodsbased on second-order matrix methods. An experimental study of mirror-descentreinforcement learning is presented using discrete and continuous Markovdecision processes.
arxiv-1500-154 | Unsupervised Joint Alignment and Clustering using Bayesian Nonparametrics | http://arxiv.org/pdf/1210.4892v1.pdf | author:Marwan A. Mattar, Allen R. Hanson, Erik G. Learned-Miller category:cs.LG stat.ML published:2012-10-16 summary:Joint alignment of a collection of functions is the process of independentlytransforming the functions so that they appear more similar to each other.Typically, such unsupervised alignment algorithms fail when presented withcomplex data sets arising from multiple modalities or make restrictiveassumptions about the form of the functions or transformations, limiting theirgenerality. We present a transformed Bayesian infinite mixture model that cansimultaneously align and cluster a data set. Our model and associated learningscheme offer two key advantages: the optimal number of clusters is determinedin a data-driven fashion through the use of a Dirichlet process prior, and itcan accommodate any transformation function parameterized by a continuousparameter vector. As a result, it is applicable to a wide range of data types,and transformation functions. We present positive results on synthetictwo-dimensional data, on a set of one-dimensional curves, and on various imagedata sets, showing large improvements over previous work. We discuss severalvariations of the model and conclude with directions for future work.
arxiv-1500-155 | Learning STRIPS Operators from Noisy and Incomplete Observations | http://arxiv.org/pdf/1210.4889v1.pdf | author:Kira Mourao, Luke S. Zettlemoyer, Ronald P. A. Petrick, Mark Steedman category:cs.LG cs.AI stat.ML published:2012-10-16 summary:Agents learning to act autonomously in real-world domains must acquire amodel of the dynamics of the domain in which they operate. Learning domaindynamics can be challenging, especially where an agent only has partial accessto the world state, and/or noisy external sensors. Even in standard STRIPSdomains, existing approaches cannot learn from noisy, incomplete observationstypical of real-world domains. We propose a method which learns STRIPS actionmodels in such domains, by decomposing the problem into first learning atransition function between states in the form of a set of classifiers, andthen deriving explicit STRIPS rules from the classifiers' parameters. Weevaluate our approach on simulated standard planning domains from theInternational Planning Competition, and show that it learns useful domaindescriptions from noisy, incomplete observations.
arxiv-1500-156 | Local Structure Discovery in Bayesian Networks | http://arxiv.org/pdf/1210.4888v1.pdf | author:Teppo Niinimaki, Pekka Parviainen category:cs.LG cs.AI stat.ML published:2012-10-16 summary:Learning a Bayesian network structure from data is an NP-hard problem andthus exact algorithms are feasible only for small data sets. Therefore, networkstructures for larger networks are usually learned with various heuristics.Another approach to scaling up the structure learning is local learning. Inlocal learning, the modeler has one or more target variables that are ofspecial interest; he wants to learn the structure near the target variables andis not interested in the rest of the variables. In this paper, we present ascore-based local learning algorithm called SLL. We conjecture that ouralgorithm is theoretically sound in the sense that it is optimal in the limitof large sample size. Empirical results suggest that SLL is competitive whencompared to the constraint-based HITON algorithm. We also study the prospectsof constructing the network structure for the whole node set based on localresults by presenting two algorithms and comparing them to several heuristics.
arxiv-1500-157 | Hilbert Space Embeddings of POMDPs | http://arxiv.org/pdf/1210.4887v1.pdf | author:Yu Nishiyama, Abdeslam Boularias, Arthur Gretton, Kenji Fukumizu category:cs.LG cs.AI stat.ML published:2012-10-16 summary:A nonparametric approach for policy learning for POMDPs is proposed. Theapproach represents distributions over the states, observations, and actions asembeddings in feature spaces, which are reproducing kernel Hilbert spaces.Distributions over states given the observations are obtained by applying thekernel Bayes' rule to these distribution embeddings. Policies and valuefunctions are defined on the feature space over states, which leads to afeature space expression for the Bellman equation. Value iteration may then beused to estimate the optimal value function and associated policy. Experimentalresults confirm that the correct policy is learned using the feature spacerepresentation.
arxiv-1500-158 | A Spectral Algorithm for Latent Junction Trees | http://arxiv.org/pdf/1210.4884v1.pdf | author:Ankur P. Parikh, Le Song, Mariya Ishteva, Gabi Teodoru, Eric P. Xing category:cs.LG stat.ML published:2012-10-16 summary:Latent variable models are an elegant framework for capturing richprobabilistic dependencies in many applications. However, current approachestypically parametrize these models using conditional probability tables, andlearning relies predominantly on local search heuristics such as ExpectationMaximization. Using tensor algebra, we propose an alternative parameterizationof latent variable models (where the model structures are junction trees) thatstill allows for computation of marginals among observed variables. While thisnovel representation leads to a moderate increase in the number of parametersfor junction trees of low treewidth, it lets us design a local-minimum-freealgorithm for learning this parameterization. The main computation of thealgorithm involves only tensor operations and SVDs which can be orders ofmagnitude faster than EM algorithms for large datasets. To our knowledge, thisis the first provably consistent parameter learning technique for a large classof low-treewidth latent graphical models beyond trees. We demonstrate theadvantages of our method on synthetic and real datasets.
arxiv-1500-159 | A Model-Based Approach to Rounding in Spectral Clustering | http://arxiv.org/pdf/1210.4883v1.pdf | author:Leonard K. M. Poon, April H. Liu, Tengfei Liu, Nevin Lianwen Zhang category:cs.LG cs.NA stat.ML published:2012-10-16 summary:In spectral clustering, one defines a similarity matrix for a collection ofdata points, transforms the matrix to get the Laplacian matrix, finds theeigenvectors of the Laplacian matrix, and obtains a partition of the data usingthe leading eigenvectors. The last step is sometimes referred to as rounding,where one needs to decide how many leading eigenvectors to use, to determinethe number of clusters, and to partition the data points. In this paper, wepropose a novel method for rounding. The method differs from previous methodsin three ways. First, we relax the assumption that the number of clustersequals the number of eigenvectors used. Second, when deciding the number ofleading eigenvectors to use, we not only rely on information contained in theleading eigenvectors themselves, but also use subsequent eigenvectors. Third,our method is model-based and solves all the three subproblems of roundingusing a class of graphical models called latent tree models. We evaluate ourmethod on both synthetic and real-world data. The results show that our methodworks correctly in the ideal case where between-clusters similarity is 0, anddegrades gracefully as one moves away from the ideal case.
arxiv-1500-160 | Tightening Fractional Covering Upper Bounds on the Partition Function for High-Order Region Graphs | http://arxiv.org/pdf/1210.4881v1.pdf | author:Tamir Hazan, Jian Peng, Amnon Shashua category:cs.LG stat.ML published:2012-10-16 summary:In this paper we present a new approach for tightening upper bounds on thepartition function. Our upper bounds are based on fractional covering bounds onthe entropy function, and result in a concave program to compute these boundsand a convex program to tighten them. To solve these programs effectively forgeneral region graphs we utilize the entropy barrier method, thus decomposingthe original programs by their dual programs and solve them with dual blockoptimization scheme. The entropy barrier method provides an elegant frameworkto generalize the message-passing scheme to high-order region graph, as well asto solve the block dual steps in closed-form. This is a key for computationalrelevancy for large problems with thousands of regions.
arxiv-1500-161 | Inferring Strategies from Limited Reconnaissance in Real-time Strategy Games | http://arxiv.org/pdf/1210.4880v1.pdf | author:Jesse Hostetler, Ethan W. Dereszynski, Thomas G. Dietterich, Alan Fern category:cs.AI cs.GT cs.LG published:2012-10-16 summary:In typical real-time strategy (RTS) games, enemy units are visible only whenthey are within sight range of a friendly unit. Knowledge of an opponent'sdisposition is limited to what can be observed through scouting. Information iscostly, since units dedicated to scouting are unavailable for other purposes,and the enemy will resist scouting attempts. It is important to infer as muchas possible about the opponent's current and future strategy from the availableobservations. We present a dynamic Bayes net model of strategies in the RTSgame Starcraft that combines a generative model of how strategies relate toobservable quantities with a principled framework for incorporating evidencegained via scouting. We demonstrate the model's ability to infer unobservedaspects of the game from realistic observations.
arxiv-1500-162 | Causal Discovery of Linear Cyclic Models from Multiple Experimental Data Sets with Overlapping Variables | http://arxiv.org/pdf/1210.4879v1.pdf | author:Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer category:stat.ME cs.AI stat.ML published:2012-10-16 summary:Much of scientific data is collected as randomized experiments intervening onsome and observing other variables of interest. Quite often, a given phenomenonis investigated in several studies, and different sets of variables areinvolved in each study. In this article we consider the problem of integratingsuch knowledge, inferring as much as possible concerning the underlying causalstructure with respect to the union of observed variables from suchexperimental or passive observational overlapping data sets. We do not assumeacyclicity or joint causal sufficiency of the underlying data generating model,but we do restrict the causal relationships to be linear and use only secondorder statistics of the data. We derive conditions for full modelidentifiability in the most generic case, and provide novel techniques forincorporating an assumption of faithfulness to aid in inference. In each casewe seek to establish what is and what is not determined by the data at hand.
arxiv-1500-163 | Active Imitation Learning via Reduction to I.I.D. Active Learning | http://arxiv.org/pdf/1210.4876v1.pdf | author:Kshitij Judah, Alan Fern, Thomas G. Dietterich category:cs.LG stat.ML published:2012-10-16 summary:In standard passive imitation learning, the goal is to learn a target policyby passively observing full execution trajectories of it. Unfortunately,generating such trajectories can require substantial expert effort and beimpractical in some cases. In this paper, we consider active imitation learningwith the goal of reducing this effort by querying the expert about the desiredaction at individual states, which are selected based on answers to pastqueries and the learner's interactions with an environment simulator. Weintroduce a new approach based on reducing active imitation learning to i.i.d.active learning, which can leverage progress in the i.i.d. setting. Our firstcontribution, is to analyze reductions for both non-stationary and stationarypolicies, showing that the label complexity (number of queries) of activeimitation learning can be substantially less than passive learning. Our secondcontribution, is to introduce a practical algorithm inspired by the reductions,which is shown to be highly effective in four test domains compared to a numberof alternatives.
arxiv-1500-164 | Nested Dictionary Learning for Hierarchical Organization of Imagery and Text | http://arxiv.org/pdf/1210.4872v1.pdf | author:Lingbo Li, XianXing Zhang, Mingyuan Zhou, Lawrence Carin category:cs.LG cs.CV stat.ML published:2012-10-16 summary:A tree-based dictionary learning model is developed for joint analysis ofimagery and associated text. The dictionary learning may be applied directly tothe imagery from patches, or to general feature vectors extracted from patchesor superpixels (using any existing method for image feature extraction). Eachimage is associated with a path through the tree (from root to a leaf), andeach of the multiple patches in a given image is associated with one node inthat path. Nodes near the tree root are shared between multiple paths,representing image characteristics that are common among different types ofimages. Moving toward the leaves, nodes become specialized, representingdetails in image classes. If available, words (text) are also jointly modeled,with a path-dependent probability over words. The tree structure is inferredvia a nested Dirichlet process, and a retrospective stick-breaking sampler isused to infer the tree depth and width.
arxiv-1500-165 | Learning Mixtures of Submodular Shells with Application to Document Summarization | http://arxiv.org/pdf/1210.4871v1.pdf | author:Hui Lin, Jeff A. Bilmes category:cs.LG cs.CL cs.IR stat.ML published:2012-10-16 summary:We introduce a method to learn a mixture of submodular "shells" in alarge-margin setting. A submodular shell is an abstract submodular functionthat can be instantiated with a ground set and a set of parameters to produce asubmodular function. A mixture of such shells can then also be so instantiatedto produce a more complex submodular function. What our algorithm learns arethe mixture weights over such shells. We provide a risk bound guarantee whenlearning in a large-margin structured-prediction setting using a projectedsubgradient method when only approximate submodular optimization is possible(such as with submodular function maximization). We apply this method to theproblem of multi-document summarization and produce the best results reportedso far on the widely used NIST DUC-05 through DUC-07 document summarizationcorpora.
arxiv-1500-166 | Crowdsourcing Control: Moving Beyond Multiple Choice | http://arxiv.org/pdf/1210.4870v1.pdf | author:Christopher H. Lin, Mausam, Daniel Weld category:cs.AI cs.LG published:2012-10-16 summary:To ensure quality results from crowdsourced tasks, requesters often aggregateworker responses and use one of a plethora of strategies to infer the correctanswer from the set of noisy responses. However, all current models assumeprior knowledge of all possible outcomes of the task. While not an unreasonableassumption for tasks that can be posited as multiple-choice questions (e.g.n-ary classification), we observe that many tasks do not naturally fit thisparadigm, but instead demand a free-response formulation where the outcomespace is of infinite size (e.g. audio transcription). We model such tasks witha novel probabilistic graphical model, and design and implement LazySusan, adecision-theoretic controller that dynamically requests responses as necessaryin order to infer answers to these tasks. We also design an EM algorithm tojointly learn the parameters of our model while inferring the correct answersto multiple tasks at a time. Live experiments on Amazon Mechanical Turkdemonstrate the superiority of LazySusan at solving SAT Math questions,eliminating 83.2% of the error and achieving greater net utility compared tothe state-ofthe-art strategy, majority-voting. We also show in live experimentsthat our EM algorithm outperforms majority-voting on a visualization task thatwe design.
arxiv-1500-167 | Response Aware Model-Based Collaborative Filtering | http://arxiv.org/pdf/1210.4869v1.pdf | author:Guang Ling, Haiqin Yang, Michael R. Lyu, Irwin King category:cs.LG cs.IR stat.ML published:2012-10-16 summary:Previous work on recommender systems mainly focus on fitting the ratingsprovided by users. However, the response patterns, i.e., some items are ratedwhile others not, are generally ignored. We argue that failing to observe suchresponse patterns can lead to biased parameter estimation and sub-optimal modelperformance. Although several pieces of work have tried to model users'response patterns, they miss the effectiveness and interpretability of thesuccessful matrix factorization collaborative filtering approaches. To bridgethe gap, in this paper, we unify explicit response models and PMF to establishthe Response Aware Probabilistic Matrix Factorization (RAPMF) framework. Weshow that RAPMF subsumes PMF as a special case. Empirically we demonstrate themerits of RAPMF from various aspects.
arxiv-1500-168 | Lifted Relational Variational Inference | http://arxiv.org/pdf/1210.4867v1.pdf | author:Jaesik Choi, Eyal Amir category:cs.LG stat.ML published:2012-10-16 summary:Hybrid continuous-discrete models naturally represent many real-worldapplications in robotics, finance, and environmental engineering. Inferencewith large-scale models is challenging because relational structuresdeteriorate rapidly during inference with observations. The main contributionof this paper is an efficient relational variational inference algorithm thatfactors largescale probability models into simpler variational models, composedof mixtures of iid (Bernoulli) random variables. The algorithm takesprobability relational models of largescale hybrid systems and converts them toa close-to-optimal variational models. Then, it efficiently calculates marginalprobabilities on the variational models by using a latent (or lifted) variableelimination or a lifted stochastic sampling. This inference is unique becauseit maintains the relational structure upon individual observations and duringinference steps.
arxiv-1500-169 | DBN-Based Combinatorial Resampling for Articulated Object Tracking | http://arxiv.org/pdf/1210.4863v1.pdf | author:Severine Dubuisson, Christophe Gonzales, Xuan Son NGuyen category:cs.CV published:2012-10-16 summary:Particle Filter is an effective solution to track objects in video sequencesin complex situations. Its key idea is to estimate the density over thepossible states of the object using a weighted sample whose elements are calledparticles. One of its crucial step is a resampling step in which particles areresampled to avoid some degeneracy problem. In this paper, we introduce a newresampling method called Combinatorial Resampling that exploits some featuresof articulated objects to resample over an implicitly created sample of anexponential size better representing the density to estimate. We prove that itis sound and, through experimentations both on challenging synthetic and realvideo sequences, we show that it outperforms all classical resampling methodsboth in terms of the quality of its results and in terms of response times.
arxiv-1500-170 | Sample-efficient Nonstationary Policy Evaluation for Contextual Bandits | http://arxiv.org/pdf/1210.4862v1.pdf | author:Miroslav Dudik, Dumitru Erhan, John Langford, Lihong Li category:cs.LG stat.ML published:2012-10-16 summary:We present and prove properties of a new offline policy evaluator for anexploration learning setting which is superior to previous evaluators. Inparticular, it simultaneously and correctly incorporates techniques fromimportance weighting, doubly robust evaluation, and nonstationary policyevaluation approaches. In addition, our approach allows generating longerhistories by careful control of a bias-variance tradeoff, and further decreasesvariance by incorporating information about randomness of the target policy.Empirical evidence from synthetic and realworld exploration learning problemsshows the new evaluator successfully unifies previous approaches and usesinformation an order of magnitude more efficiently.
arxiv-1500-171 | Spectral Estimation of Conditional Random Graph Models for Large-Scale Network Data | http://arxiv.org/pdf/1210.4860v1.pdf | author:Antonino Freno, Mikaela Keller, Gemma C. Garriga, Marc Tommasi category:cs.SI cs.LG physics.soc-ph stat.ML published:2012-10-16 summary:Generative models for graphs have been typically committed to strong priorassumptions concerning the form of the modeled distributions. Moreover, thevast majority of currently available models are either only suitable forcharacterizing some particular network properties (such as degree distributionor clustering coefficient), or they are aimed at estimating joint probabilitydistributions, which is often intractable in large-scale networks. In thispaper, we first propose a novel network statistic, based on the Laplacianspectrum of graphs, which allows to dispense with any parametric assumptionconcerning the modeled network properties. Second, we use the defined statisticto develop the Fiedler random graph model, switching the focus from theestimation of joint probability distributions to a more tractable conditionalestimation setting. After analyzing the dependence structure characterizingFiedler random graphs, we evaluate them experimentally in edge prediction overseveral real-world networks, showing that they allow to reach a much higherprediction accuracy than various alternative statistical models.
arxiv-1500-172 | Mechanism Design for Cost Optimal PAC Learning in the Presence of Strategic Noisy Annotators | http://arxiv.org/pdf/1210.4859v1.pdf | author:Dinesh Garg, Sourangshu Bhattacharya, S. Sundararajan, Shirish Shevade category:cs.LG cs.GT stat.ML published:2012-10-16 summary:We consider the problem of Probably Approximate Correct (PAC) learning of abinary classifier from noisy labeled examples acquired from multiple annotators(each characterized by a respective classification noise rate). First, weconsider the complete information scenario, where the learner knows the noiserates of all the annotators. For this scenario, we derive sample complexitybound for the Minimum Disagreement Algorithm (MDA) on the number of labeledexamples to be obtained from each annotator. Next, we consider the incompleteinformation scenario, where each annotator is strategic and holds therespective noise rate as a private information. For this scenario, we design acost optimal procurement auction mechanism along the lines of Myerson's optimalauction design framework in a non-trivial manner. This mechanism satisfiesincentive compatibility property, thereby facilitating the learner to elicittrue noise rates of all the annotators.
arxiv-1500-173 | Exploiting compositionality to explore a large space of model structures | http://arxiv.org/pdf/1210.4856v1.pdf | author:Roger Grosse, Ruslan R Salakhutdinov, William T. Freeman, Joshua B. Tenenbaum category:cs.LG stat.ML published:2012-10-16 summary:The recent proliferation of richly structured probabilistic models raises thequestion of how to automatically determine an appropriate model for a dataset.We investigate this question for a space of matrix decomposition models whichcan express a variety of widely used models from unsupervised learning. Toenable model selection, we organize these models into a context-free grammarwhich generates a wide variety of structures through the compositionalapplication of a few simple rules. We use our grammar to generically andefficiently infer latent components and estimate predictive likelihood fornearly 2500 structures using a small toolbox of reusable algorithms. Using agreedy search over our grammar, we automatically choose the decompositionstructure from raw data by evaluating only a small fraction of all models. Theproposed method typically finds the correct structure for synthetic data andbacks off gracefully to simpler models under heavy noise. It learns sensiblestructures for datasets as diverse as image patches, motion capture, 20Questions, and U.S. Senate votes, all using exactly the same code.
arxiv-1500-174 | A Slice Sampler for Restricted Hierarchical Beta Process with Applications to Shared Subspace Learning | http://arxiv.org/pdf/1210.4855v1.pdf | author:Sunil Kumar Gupta, Dinh Q. Phung, Svetha Venkatesh category:cs.LG cs.CV stat.ML published:2012-10-16 summary:Hierarchical beta process has found interesting applications in recent years.In this paper we present a modified hierarchical beta process prior withapplications to hierarchical modeling of multiple data sources. The novel useof the prior over a hierarchical factor model allows factors to be sharedacross different sources. We derive a slice sampler for this model, enablingtractable inference even when the likelihood and the prior over parameters arenon-conjugate. This allows the application of the model in much wider contextswithout restrictions. We present two different data generative models a linearGaussianGaussian model for real valued data and a linear Poisson-gamma modelfor count data. Encouraging transfer learning results are shown for two realworld applications text modeling and content based image retrieval.
arxiv-1500-175 | Semantic Understanding of Professional Soccer Commentaries | http://arxiv.org/pdf/1210.4854v1.pdf | author:Hannaneh Hajishirzi, Mohammad Rastegari, Ali Farhadi, Jessica K. Hodgins category:cs.CL cs.AI published:2012-10-16 summary:This paper presents a novel approach to the problem of semantic parsing vialearning the correspondences between complex sentences and rich sets of events.Our main intuition is that correct correspondences tend to occur morefrequently. Our model benefits from a discriminative notion of similarity tolearn the correspondence between sentence and an event and a ranking machinerythat scores the popularity of each correspondence. Our method can discover agroup of events (called macro-events) that best describes a sentence. Weevaluate our method on our novel dataset of professional soccer commentaries.The empirical results show that our method significantly outperforms thestate-of-theart.
arxiv-1500-176 | Learning to Rank With Bregman Divergences and Monotone Retargeting | http://arxiv.org/pdf/1210.4851v1.pdf | author:Sreangsu Acharyya, Oluwasanmi Koyejo, Joydeep Ghosh category:cs.LG stat.ML published:2012-10-16 summary:This paper introduces a novel approach for learning to rank (LETOR) based onthe notion of monotone retargeting. It involves minimizing a divergence betweenall monotonic increasing transformations of the training scores and aparameterized prediction function. The minimization is both over thetransformations as well as over the parameters. It is applied to Bregmandivergences, a large class of "distance like" functions that were recentlyshown to be the unique class that is statistically consistent with thenormalized discounted gain (NDCG) criterion [19]. The algorithm usesalternating projection style updates, in which one set of simultaneousprojections can be computed independent of the Bregman divergence and the otherreduces to parameter estimation of a generalized linear model. This results ineasily implemented, efficiently parallelizable algorithm for the LETOR taskthat enjoys global optimum guarantees under mild conditions. We presentempirical results on benchmark datasets showing that this approach canoutperform the state of the art NDCG consistent techniques.
arxiv-1500-177 | Markov Determinantal Point Processes | http://arxiv.org/pdf/1210.4850v1.pdf | author:Raja Hafiz Affandi, Alex Kulesza, Emily B. Fox category:cs.LG cs.IR stat.ML published:2012-10-16 summary:A determinantal point process (DPP) is a random process useful for modelingthe combinatorial problem of subset selection. In particular, DPPs encourage arandom subset Y to contain a diverse set of items selected from a base set Y.For example, we might use a DPP to display a set of news headlines that arerelevant to a user's interests while covering a variety of topics. Suppose,however, that we are asked to sequentially select multiple diverse sets ofitems, for example, displaying new headlines day-by-day. We might want thesesets to be diverse not just individually but also through time, offeringheadlines today that are unlike the ones shown yesterday. In this paper, weconstruct a Markov DPP (M-DPP) that models a sequence of random sets {Yt}. Theproposed M-DPP defines a stationary process that maintains DPP margins.Crucially, the induced union process Zt = Yt u Yt-1 is also marginallyDPP-distributed. Jointly, these properties imply that the sequence of randomsets are encouraged to be diverse both at a given time step as well as acrosstime steps. We describe an exact, efficient sampling procedure, and a methodfor incrementally learning a quality measure over items in the base set Y basedon external preferences. We apply the M-DPP to the task of sequentiallydisplaying diverse and relevant news articles to a user with topic preferences.
arxiv-1500-178 | Variational Dual-Tree Framework for Large-Scale Transition Matrix Approximation | http://arxiv.org/pdf/1210.4846v1.pdf | author:Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht category:cs.LG stat.ML published:2012-10-16 summary:In recent years, non-parametric methods utilizing random walks on graphs havebeen used to solve a wide range of machine learning problems, but in theirsimplest form they do not scale well due to the quadratic complexity. In thispaper, a new dual-tree based variational approach for approximating thetransition matrix and efficiently performing the random walk is proposed. Theapproach exploits a connection between kernel density estimation, mixturemodeling, and random walk on graphs in an optimization of the transition matrixfor the data graph that ties together edge transitions probabilities that aresimilar. Compared to the de facto standard approximation method based onk-nearestneighbors, we demonstrate order of magnitudes speedup withoutsacrificing accuracy for Label Propagation tasks on benchmark data sets insemi-supervised learning.
arxiv-1500-179 | Deterministic MDPs with Adversarial Rewards and Bandit Feedback | http://arxiv.org/pdf/1210.4843v1.pdf | author:Raman Arora, Ofer Dekel, Ambuj Tewari category:cs.GT cs.LG published:2012-10-16 summary:We consider a Markov decision process with deterministic state transitiondynamics, adversarially generated rewards that change arbitrarily from round toround, and a bandit feedback model in which the decision maker only observesthe rewards it receives. In this setting, we present a novel and efficientonline decision making algorithm named MarcoPolo. Under mild assumptions on thestructure of the transition dynamics, we prove that MarcoPolo enjoys a regretof O(T^(3/4)sqrt(log(T))) against the best deterministic policy in hindsight.Specifically, our analysis does not rely on the stringent unichain assumption,which dominates much of the previous work on this topic.
arxiv-1500-180 | An Efficient Message-Passing Algorithm for the M-Best MAP Problem | http://arxiv.org/pdf/1210.4841v1.pdf | author:Dhruv Batra category:cs.AI cs.LG stat.ML published:2012-10-16 summary:Much effort has been directed at algorithms for obtaining the highestprobability configuration in a probabilistic random field model known as themaximum a posteriori (MAP) inference problem. In many situations, one couldbenefit from having not just a single solution, but the top M most probablesolutions known as the M-Best MAP problem. In this paper, we propose anefficient message-passing based algorithm for solving the M-Best MAP problem.Specifically, our algorithm solves the recently proposed Linear Programming(LP) formulation of M-Best MAP [7], while being orders of magnitude faster thana generic LP-solver. Our approach relies on studying a particular partialLagrangian relaxation of the M-Best MAP LP which exposes a naturalcombinatorial structure of the problem that we exploit.
arxiv-1500-181 | Leveraging Side Observations in Stochastic Bandits | http://arxiv.org/pdf/1210.4839v1.pdf | author:Stephane Caron, Branislav Kveton, Marc Lelarge, Smriti Bhagat category:cs.LG stat.ML published:2012-10-16 summary:This paper considers stochastic bandits with side observations, a model thataccounts for both the exploration/exploitation dilemma and relationshipsbetween arms. In this setting, after pulling an arm i, the decision maker alsoobserves the rewards for some other actions related to i. We will see that thismodel is suited to content recommendation in social networks, where users'reactions may be endorsed or not by their friends. We provide efficientalgorithms based on upper confidence bounds (UCBs) to leverage this additionalinformation and derive new bounds improving on standard regret guarantees. Wealso evaluate these policies in the context of movie recommendation in socialnetworks: experiments on real datasets show substantial learning rate speedupsranging from 2.2x to 14x on dense networks.
arxiv-1500-182 | Hilbert Space Embedding for Dirichlet Process Mixtures | http://arxiv.org/pdf/1210.4347v1.pdf | author:Krikamol Muandet category:stat.ML cs.LG published:2012-10-16 summary:This paper proposes a Hilbert space embedding for Dirichlet Process mixturemodels via a stick-breaking construction of Sethuraman. Although Bayesiannonparametrics offers a powerful approach to construct a prior that avoids theneed to specify the model size/complexity explicitly, an exact inference isoften intractable. On the other hand, frequentist approaches such as kernelmachines, which suffer from the model selection/comparison problems, oftenbenefit from efficient learning algorithms. This paper discusses thepossibility to combine the best of both worlds by using the Dirichlet Processmixture model as a case study.
arxiv-1500-183 | Implementation of Radon Transformation for Electrical Impedance Tomography (EIT) | http://arxiv.org/pdf/1211.1252v1.pdf | author:Md. Ali Hossain, Ahsan-Ul-Ambia, Md. Aktaruzzaman, Md. Ahaduzzaman Khan category:cs.CV published:2012-10-16 summary:Radon Transformation is generally used to construct optical image (like CTimage) from the projection data in biomedical imaging. In this paper, theconcept of Radon Transformation is implemented to reconstruct ElectricalImpedance Topographic Image (conductivity or resistivity distribution) of acircular subject. A parallel resistance model of a subject is proposed forElectrical Impedance Topography(EIT) or Magnetic Induction Tomography(MIT). Acircular subject with embedded circular objects is segmented into equal widthslices from different angles. For each angle, Conductance and Conductivity ofeach slice is calculated and stored in an array. A back projection method isused to generate a two-dimensional image from one-dimensional projections. As aback projection method, Inverse Radon Transformation is applied on thecalculated conductance and conductivity to reconstruct two dimensional images.These images are compared to the target image. In the time of imagereconstruction, different filters are used and these images are compared witheach other and target image.
arxiv-1500-184 | Semi-Supervised Classification Through the Bag-of-Paths Group Betweenness | http://arxiv.org/pdf/1210.4276v1.pdf | author:Bertrand Lebichot, Ilkka Kivimäki, Kevin Françoisse, Marco Saerens category:stat.ML cs.LG published:2012-10-16 summary:This paper introduces a novel, well-founded, betweenness measure, called theBag-of-Paths (BoP) betweenness, as well as its extension, the BoP groupbetweenness, to tackle semisupervised classification problems on weighteddirected graphs. The objective of semi-supervised classification is to assign alabel to unlabeled nodes using the whole topology of the graph and the labelednodes at our disposal. The BoP betweenness relies on a bag-of-paths frameworkassigning a Boltzmann distribution on the set of all possible paths through thenetwork such that long (high-cost) paths have a low probability of being pickedfrom the bag, while short (low-cost) paths have a high probability of beingpicked. Within that context, the BoP betweenness of node j is defined as thesum of the a posteriori probabilities that node j lies in-between two arbitrarynodes i, k, when picking a path starting in i and ending in k. Intuitively, anode typically receives a high betweenness if it has a large probability ofappearing on paths connecting two arbitrary nodes of the network. This quantitycan be computed in closed form by inverting a n x n matrix where n is thenumber of nodes. For the group betweenness, the paths are constrained to startand end in nodes within the same class, therefore defining a group betweennessfor each class. Unlabeled nodes are then classified according to the classshowing the highest group betweenness. Experiments on various real-world datasets show that BoP group betweenness outperforms all the tested stateof-the-art methods. The benefit of the BoP betweenness is particularlynoticeable when only a few labeled nodes are available.
arxiv-1500-185 | A polygon-based interpolation operator for super-resolution imaging | http://arxiv.org/pdf/1210.3404v2.pdf | author:Stéfan J. van der Walt, B. M. Herbst category:cs.CV published:2012-10-12 summary:We outline the super-resolution reconstruction problem posed as amaximization of probability. We then introduce an interpolation method based onpolygonal pixel overlap, express it as a linear operator, and use it to improvereconstruction. Polygon interpolation outperforms the simpler bilinearinterpolation operator and, unlike Gaussian modeling of pixels, requires noparameter estimation. A free software implementation that reproduces theresults shown is provided.
arxiv-1500-186 | The Kernel Pitman-Yor Process | http://arxiv.org/pdf/1210.4184v1.pdf | author:Sotirios P. Chatzis, Dimitrios Korkinof, Yiannis Demiris category:cs.LG cs.AI stat.ML published:2012-10-15 summary:In this work, we propose the kernel Pitman-Yor process (KPYP) fornonparametric clustering of data with general spatial or temporalinterdependencies. The KPYP is constructed by first introducing an infinitesequence of random locations. Then, based on the stick-breaking construction ofthe Pitman-Yor process, we define a predictor-dependent random probabilitymeasure by considering that the discount hyperparameters of theBeta-distributed random weights (stick variables) of the process are notuniform among the weights, but controlled by a kernel function expressing theproximity between the location assigned to each weight and the givenpredictors.
arxiv-1500-187 | A Biologically Realistic Model of Saccadic Eye Control with Probabilistic Population Codes | http://arxiv.org/pdf/1210.4145v1.pdf | author:Sacha Sokoloski category:cs.NE q-bio.NC published:2012-10-15 summary:The posterior parietal cortex is believed to direct eye movements, especiallyin regards to target tracking tasks, and a number of debates exist over theprecise nature of the computations performed by the parietal cortex, with eachside supported by different sets of biological evidence. In this paper I willpresent my model which navigates a course between some of these debates,towards the end of presenting a model which can explain some of the competinginterpretations among the data sets. In particular, rather than assuming thatproprioception or efference copies form the key source of information forcomputing eye position information, I use a biological plausible implementationof a Kalman filter to optimally combine the two signals, and a simple gaincontrol mechanism in order to accommodate the latency of the proprioceptivesignal. Fitting within the Bayesian brain hypothesis, the result is a Bayesoptimal solution to the eye control problem, with a range of data supportingclaims of biological plausibility.
arxiv-1500-188 | Getting Feasible Variable Estimates From Infeasible Ones: MRF Local Polytope Study | http://arxiv.org/pdf/1210.4081v1.pdf | author:Bogdan Savchynskyy, Stefan Schmidt category:cs.NA cs.CV cs.DS cs.LG math.OC published:2012-10-15 summary:This paper proposes a method for construction of approximate feasible primalsolutions from dual ones for large-scale optimization problems possessingcertain separability properties. Whereas infeasible primal estimates cantypically be produced from (sub-)gradients of the dual function, it is oftennot easy to project them to the primal feasible set, since the projectionitself has a complexity comparable to the complexity of the initial problem. Wepropose an alternative efficient method to obtain feasibility and show that itsproperties influencing the convergence to the optimum are similar to theproperties of the Euclidean projection. We apply our method to the localpolytope relaxation of inference problems for Markov Random Fields anddemonstrate its superiority over existing methods.
arxiv-1500-189 | Local Optima Networks, Landscape Autocorrelation and Heuristic Search Performance | http://arxiv.org/pdf/1210.4021v1.pdf | author:Francisco Chicano, Fabio Daolio, Gabriela Ochoa, Sébastien Verel, Marco Tomassini, Enrique Alba category:cs.AI cs.NE published:2012-10-15 summary:Recent developments in fitness landscape analysis include the study of LocalOptima Networks (LON) and applications of the Elementary Landscapes theory.This paper represents a first step at combining these two tools to exploretheir ability to forecast the performance of search algorithms. We base ouranalysis on the Quadratic Assignment Problem (QAP) and conduct a largestatistical study over 600 generated instances of different types. Our resultsreveal interesting links between the network measures, the autocorrelationmeasures and the performance of heuristic search algorithms.
arxiv-1500-190 | The Perturbed Variation | http://arxiv.org/pdf/1210.4006v1.pdf | author:Maayan Harel, Shie Mannor category:cs.LG stat.ML published:2012-10-15 summary:We introduce a new discrepancy score between two distributions that gives anindication on their similarity. While much research has been done to determineif two samples come from exactly the same distribution, much less researchconsidered the problem of determining if two finite samples come from similardistributions. The new score gives an intuitive interpretation of similarity;it optimally perturbs the distributions so that they best fit each other. Thescore is defined between distributions, and can be efficiently estimated fromsamples. We provide convergence bounds of the estimated score, and develophypothesis testing procedures that test if two data sets come from similardistributions. The statistical power of this procedures is presented insimulations. We also compare the score's capacity to detect similarity withthat of other known measures on real data.
arxiv-1500-191 | Opinion Mining for Relating Subjective Expressions and Annual Earnings in US Financial Statements | http://arxiv.org/pdf/1210.3865v1.pdf | author:Chien-Liang Chen, Chao-Lin Liu, Yuan-Chen Chang, Hsiang-Ping Tsai category:cs.CL cs.AI cs.IR q-fin.GN published:2012-10-15 summary:Financial statements contain quantitative information and manager'ssubjective evaluation of firm's financial status. Using information released inU.S. 10-K filings. Both qualitative and quantitative appraisals are crucial forquality financial decisions. To extract such opinioned statements from thereports, we built tagging models based on the conditional random field (CRF)techniques, considering a variety of combinations of linguistic factorsincluding morphology, orthography, predicate-argument structure, syntax, andsimple semantics. Our results show that the CRF models are reasonably effectiveto find opinion holders in experiments when we adopted the popular MPQA corpusfor training and testing. The contribution of our paper is to identify opinionpatterns in multiword expressions (MWEs) forms rather than in single wordforms. We find that the managers of corporations attempt to use more optimisticwords to obfuscate negative financial performance and to accentuate thepositive financial performance. Our results also show that decreasing earningswere often accompanied by ambiguous and mild statements in the reporting yearand that increasing earnings were stated in assertive and positive way.
arxiv-1500-192 | Image Processing using Smooth Ordering of its Patches | http://arxiv.org/pdf/1210.3832v1.pdf | author:Idan Ram, Michael Elad, Israel Cohen category:cs.CV published:2012-10-14 summary:We propose an image processing scheme based on reordering of its patches. Fora given corrupted image, we extract all patches with overlaps, refer to theseas coordinates in high-dimensional space, and order them such that they arechained in the "shortest possible path", essentially solving the travelingsalesman problem. The obtained ordering applied to the corrupted image, impliesa permutation of the image pixels to what should be a regular signal. Thisenables us to obtain good recovery of the clean image by applying relativelysimple 1D smoothing operations (such as filtering or interpolation) to thereordered set of pixels. We explore the use of the proposed approach to imagedenoising and inpainting, and show promising results in both cases.
arxiv-1500-193 | Online computation of sparse representations of time varying stimuli using a biologically motivated neural network | http://arxiv.org/pdf/1210.3741v1.pdf | author:Tao Hu, Dmitri B. Chklovskii category:q-bio.NC cs.NE published:2012-10-13 summary:Natural stimuli are highly redundant, possessing significant spatial andtemporal correlations. While sparse coding has been proposed as an efficientstrategy employed by neural systems to encode sensory stimuli, the underlyingmechanisms are still not well understood. Most previous approaches model theneural dynamics by the sparse representation dictionary itself and compute therepresentation coefficients offline. In reality, faced with the challenge ofconstantly changing stimuli, neurons must compute the sparse representationsdynamically in an online fashion. Here, we describe a leaky linearized Bregmaniteration (LLBI) algorithm which computes the time varying sparserepresentations using a biologically motivated network of leaky rectifyingneurons. Compared to previous attempt of dynamic sparse coding, LLBI exploitsthe temporal correlation of stimuli and demonstrate better performance both inrepresentation error and the smoothness of temporal evolution of sparsecoefficients.
arxiv-1500-194 | Inference of Fine-grained Attributes of Bengali Corpus for Stylometry Detection | http://arxiv.org/pdf/1210.3729v1.pdf | author:Tanmoy Chakraborty, Sivaji Bandyopadhyay category:cs.CL cs.CV published:2012-10-13 summary:Stylometry, the science of inferring characteristics of the author from thecharacteristics of documents written by that author, is a problem with a longhistory and belongs to the core task of Text categorization that involvesauthorship identification, plagiarism detection, forensic investigation,computer security, copyright and estate disputes etc. In this work, we presenta strategy for stylometry detection of documents written in Bengali. We adopt aset of fine-grained attribute features with a set of lexical markers for theanalysis of the text and use three semi-supervised measures for makingdecisions. Finally, a majority voting approach has been taken for finalclassification. The system is fully automatic and language-independent.Evaluation results of our attempt for Bengali author's stylometry detectionshow reasonably promising accuracy in comparison to the baseline model.
arxiv-1500-195 | On the Role of Contrast and Regularity in Perceptual Boundary Saliency | http://arxiv.org/pdf/1210.3718v1.pdf | author:Mariano Tepper, Pablo Musé, Andrés Almansa category:cs.CV stat.AP published:2012-10-13 summary:Mathematical Morphology proposes to extract shapes from images as connectedcomponents of level sets. These methods prove very suitable for shaperecognition and analysis. We present a method to select the perceptuallysignificant (i.e., contrasted) level lines (boundaries of level sets), usingthe Helmholtz principle as first proposed by Desolneux et al. Contrarily to theclassical formulation by Desolneux et al. where level lines must be entirelysalient, the proposed method allows to detect partially salient level lines,thus resulting in more robust and more stable detections. We then tackle theproblem of combining two gestalts as a measure of saliency and propose a methodthat reinforces detections. Results in natural images show the good performanceof the proposed methods.
arxiv-1500-196 | Noise Tolerance under Risk Minimization | http://arxiv.org/pdf/1109.5231v4.pdf | author:Naresh Manwani, P. S. Sastry category:cs.LG published:2011-09-24 summary:In this paper we explore noise tolerant learning of classifiers. We formulatethe problem as follows. We assume that there is an ${\bf unobservable}$training set which is noise-free. The actual training set given to the learningalgorithm is obtained from this ideal data set by corrupting the class label ofeach example. The probability that the class label of an example is corruptedis a function of the feature vector of the example. This would account for mostkinds of noisy data one encounters in practice. We say that a learning methodis noise tolerant if the classifiers learnt with the ideal noise-free data andwith noisy data, both have the same classification accuracy on the noise-freedata. In this paper we analyze the noise tolerance properties of riskminimization (under different loss functions), which is a generic method forlearning classifiers. We show that risk minimization under 0-1 loss functionhas impressive noise tolerance properties and that under squared error loss istolerant only to uniform noise; risk minimization under other loss functions isnot noise tolerant. We conclude the paper with some discussion on implicationsof these theoretical results.
arxiv-1500-197 | Geometric Decision Tree | http://arxiv.org/pdf/1009.3604v5.pdf | author:Naresh Manwani, P. S. Sastry category:cs.LG published:2010-09-19 summary:In this paper we present a new algorithm for learning oblique decision trees.Most of the current decision tree algorithms rely on impurity measures toassess the goodness of hyperplanes at each node while learning a decision treein a top-down fashion. These impurity measures do not properly capture thegeometric structures in the data. Motivated by this, our algorithm uses astrategy to assess the hyperplanes in such a way that the geometric structurein the data is taken into account. At each node of the decision tree, we findthe clustering hyperplanes for both the classes and use their angle bisectorsas the split rule at that node. We show through empirical studies that thisidea leads to small decision trees and better performance. We also present someanalysis to show that the angle bisectors of clustering hyperplanes that we useas the split rules at each node, are solutions of an interesting optimizationproblem and hence argue that this is a principled method of learning a decisiontree.
arxiv-1500-198 | Semi-supervised logistic discrimination via labeled data and unlabeled data from different sampling distributions | http://arxiv.org/pdf/1108.5244v3.pdf | author:Shuichi Kawano category:stat.ML stat.ME published:2011-08-26 summary:This article addresses the problem of classification method based on bothlabeled and unlabeled data, where we assume that a density function for labeleddata is different from that for unlabeled data. We propose a semi-supervisedlogistic regression model for classification problem along with the techniqueof covariate shift adaptation. Unknown parameters involved in proposed modelsare estimated by regularization with EM algorithm. A crucial issue in themodeling process is the choices of tuning parameters in our semi-supervisedlogistic models. In order to select the parameters, a model selection criterionis derived from an information-theoretic approach. Some numerical studies showthat our modeling procedure performs well in various cases.
arxiv-1500-199 | A Flexible Mixed Integer Programming framework for Nurse Scheduling | http://arxiv.org/pdf/1210.3652v1.pdf | author:Murphy Choy, Michelle Cheong category:cs.DS cs.NE published:2012-10-12 summary:In this paper, a nurse-scheduling model is developed using mixed integerprogramming model. It is deployed to a general care ward to replace andautomate the current manual approach for scheduling. The developed modeldiffers from other similar studies in that it optimizes both hospitalsrequirement as well as nurse preferences by allowing flexibility in thetransfer of nurses from different duties. The model also incorporatedadditional policies which are part of the hospitals requirement but not part ofthe legislations. Hospitals key primary mission is to ensure continuous wardcare service with appropriate number of nursing staffs and the right mix ofnursing skills. The planning and scheduling is done to avoid additional nonessential cost for hospital. Nurses preferences are taken into considerationssuch as the number of night shift and consecutive rest days. We will alsoreformulate problems from another paper which considers the penalty objectiveusing the model but without the flexible components. The models are built usingAIMMS which solves the problem in very short amount of time.
arxiv-1500-200 | Quick Summary | http://arxiv.org/pdf/1210.3634v1.pdf | author:Robert Wahlstedt category:cs.CL cs.AI published:2012-10-12 summary:Quick Summary is an innovate implementation of an automatic documentsummarizer that inputs a document in the English language and evaluates eachsentence. The scanner or evaluator determines criteria based on its grammaticalstructure and place in the paragraph. The program then asks the user to specifythe number of sentences the person wishes to highlight. For example should theuser ask to have three of the most important sentences, it would highlight thefirst and most important sentence in green. Commonly this is the sentencecontaining the conclusion. Then Quick Summary finds the second most importantsentence usually called a satellite and highlights it in yellow. This isusually the topic sentence. Then the program finds the third most importantsentence and highlights it in red. The implementations of this technology areuseful in a society of information overload when a person typically receives 42emails a day (Microsoft). The paper also is a candid look at difficulty thatmachine learning has in textural translating. However, it speaks on how toovercome the obstacles that historically prevented progress. This paperproposes mathematical meta-data criteria that justify the place of importanceof a sentence. Just as tools for the study of relational symmetry inbio-informatics, this tool seeks to classify words with greater clarity."Survey Finds Workers Average Only Three Productive Days per Week." MicrosoftNews Center. Microsoft. Web. 31 Mar. 2012.
arxiv-1500-201 | Notes on image annotation | http://arxiv.org/pdf/1210.3448v1.pdf | author:Adela Barriuso, Antonio Torralba category:cs.CV cs.HC published:2012-10-12 summary:We are under the illusion that seeing is effortless, but frequently thevisual system is lazy and makes us believe that we understand something when infact we don't. Labeling a picture forces us to become aware of the difficultiesunderlying scene understanding. Suddenly, the act of seeing is not effortlessanymore. We have to make an effort in order to understand parts of the picturethat we neglected at first glance. In this report, an expert image annotator relates her experience onsegmenting and labeling tens of thousands of images. During this process, thenotes she took try to highlight the difficulties encountered, the solutionsadopted, and the decisions made in order to get a consistent set ofannotations. Those annotations constitute the SUN database.
arxiv-1500-202 | Enhanced Compressed Sensing Recovery with Level Set Normals | http://arxiv.org/pdf/1210.3350v1.pdf | author:Virginia Estellers, Jean-Philippe Thiran, Xavier Bresson category:cs.CV published:2012-10-11 summary:We propose a compressive sensing algorithm that exploits geometric propertiesof images to recover images of high quality from few measurements. The imagereconstruction is done by iterating the two following steps: 1) estimation ofnormal vectors of the image level curves and 2) reconstruction of an imagefitting the normal vectors, the compressed sensing measurements and thesparsity constraint. The proposed technique can naturally extend to non localoperators and graphs to exploit the repetitive nature of textured images inorder to recover fine detail structures. In both cases, the problem is reducedto a series of convex minimization problems that can be efficiently solved witha combination of variable splitting and augmented Lagrangian methods, leadingto fast and easy-to-code algorithms. Extended experiments show a clearimprovement over related state-of-the-art algorithms in the quality of thereconstructed images and the robustness of the proposed method to noise,different kind of images and reduced measurements.
arxiv-1500-203 | Three dimensional tracking of gold nanoparticles using digital holographic microscopy | http://arxiv.org/pdf/1210.3326v1.pdf | author:Frédéric Verpillat, Fadwa Joud, Pierre Desbiolles, Michel Gross category:physics.optics cs.CV published:2012-10-11 summary:In this paper we present a digital holographic microscope to track goldcolloids in three dimensions. We report observations of 100nm gold particles inmotion in water. The expected signal and the chosen method of reconstructionare described. We also discuss about how to implement the numerical calculationto reach real-time 3D tracking.
arxiv-1500-204 | Artex is AnotheR TEXt summarizer | http://arxiv.org/pdf/1210.3312v1.pdf | author:Juan-Manuel Torres-Moreno category:cs.IR cs.AI cs.CL published:2012-10-11 summary:This paper describes Artex, another algorithm for Automatic TextSummarization. In order to rank sentences, a simple inner product is calculatedbetween each sentence, a document vector (text topic) and a lexical vector(vocabulary used by a sentence). Summaries are then generated by assembling thehighest ranked sentences. No ruled-based linguistic post-processing isnecessary in order to obtain summaries. Tests over several datasets (comingfrom Document Understanding Conferences (DUC), Text Analysis Conferences (TAC),evaluation campaigns, etc.) in French, English and Spanish have shown thatsummarizer achieves interesting results.
arxiv-1500-205 | Unsupervised Detection and Tracking of Arbitrary Objects with Dependent Dirichlet Process Mixtures | http://arxiv.org/pdf/1210.3288v1.pdf | author:Willie Neiswanger, Frank Wood category:stat.ML cs.CV cs.LG published:2012-10-11 summary:This paper proposes a technique for the unsupervised detection and trackingof arbitrary objects in videos. It is intended to reduce the need for detectionand localization methods tailored to specific object types and serve as ageneral framework applicable to videos with varied objects, backgrounds, andimage qualities. The technique uses a dependent Dirichlet process mixture(DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel datathat can be easily and efficiently extracted from the regions in a video thatrepresent objects. This paper describes a specific implementation of the modelusing spatial and color pixel data extracted via frame differencing and givestwo algorithms for performing inference in the model to accomplish detectionand tracking. This technique is demonstrated on multiple synthetic andbenchmark video datasets that illustrate its ability to, without modification,detect and track objects with diverse physical characteristics moving overnon-uniform backgrounds and through occlusion.
arxiv-1500-206 | The Robustness and Super-Robustness of L^p Estimation, when p < 1 | http://arxiv.org/pdf/1206.5057v5.pdf | author:Qinghuai Gao category:cs.LG math.ST stat.TH published:2012-06-22 summary:In robust statistics, the breakdown point of an estimator is the percentageof outliers with which an estimator still generates reliable estimation. Theupper bound of breakdown point is 50%, which means it is not possible togenerate reliable estimation with more than half outliers. In this paper, it is shown that for majority of experiences, when theoutliers exceed 50%, but if they are distributed randomly enough, it is stillpossible to generate a reliable estimation from minority good observations. Thephenomenal of that the breakdown point is larger than 50% is named as superrobustness. And, in this paper, a robust estimator is called strict robust ifit generates a perfect estimation when all the good observations are perfect. More specifically, the super robustness of the maximum likelihood estimatorof the exponential power distribution, or L^p estimation, where p<1, isinvestigated. This paper starts with proving that L^p (p<1) is a strict robustlocation estimator. Further, it is proved that L^p (p < 1)has the property ofstrict super-robustness on translation, rotation, scaling transformation androbustness on Euclidean transform.
arxiv-1500-207 | Adaptive sequential Monte Carlo by means of mixture of experts | http://arxiv.org/pdf/1108.2836v2.pdf | author:J. Cornebise, E. Moulines, J. Olsson category:stat.ME stat.CO stat.ML published:2011-08-14 summary:Appropriately designing the proposal kernel of particle filters is an issueof significant importance, since a bad choice may lead to deterioration of theparticle sample and, consequently, waste of computational power. In this paperwe introduce a novel algorithm adaptively approximating the so-called optimalproposal kernel by a mixture of integrated curved exponential distributionswith logistic weights. This family of distributions, referred to as mixtures ofexperts, is broad enough to be used in the presence of multi-modality orstrongly skewed distributions. The mixtures are fitted, via online-EM methods,to the optimal kernel through minimisation of the Kullback-Leibler divergencebetween the auxiliary target and instrumental distributions of the particlefilter. At each iteration of the particle filter, the algorithm is required tosolve only a single optimisation problem for the whole particle sample,yielding an algorithm with only linear complexity. In addition, we illustratein a simulation study how the method can be successfully applied to optimalfiltering in nonlinear state-space models.
arxiv-1500-208 | Computationally Efficient Implementation of Convolution-based Locally Adaptive Binarization Techniques | http://arxiv.org/pdf/1210.3165v1.pdf | author:Ayatullah Faruk Mollah, Subhadip Basu, Mita Nasipuri category:cs.CV published:2012-10-11 summary:One of the most important steps of document image processing is binarization.The computational requirements of locally adaptive binarization techniques makethem unsuitable for devices with limited computing facilities. In this paper,we have presented a computationally efficient implementation of convolutionbased locally adaptive binarization techniques keeping the performancecomparable to the original implementation. The computational complexity hasbeen reduced from O(W2N2) to O(WN2) where WxW is the window size and NxN is theimage size. Experiments over benchmark datasets show that the computation timehas been reduced by 5 to 15 times depending on the window size while memoryconsumption remains the same with respect to the state-of-the-art algorithmicimplementation.
arxiv-1500-209 | A Benchmark to Select Data Mining Based Classification Algorithms For Business Intelligence And Decision Support Systems | http://arxiv.org/pdf/1210.3139v1.pdf | author:Pardeep Kumar, Nitin, Vivek Kumar Sehgal, Durg Singh Chauhan category:cs.DB cs.LG published:2012-10-11 summary:DSS serve the management, operations, and planning levels of an organizationand help to make decisions, which may be rapidly changing and not easilyspecified in advance. Data mining has a vital role to extract importantinformation to help in decision making of a decision support system.Integration of data mining and decision support systems (DSS) can lead to theimproved performance and can enable the tackling of new types of problems.Artificial Intelligence methods are improving the quality of decision support,and have become embedded in many applications ranges from ant lockingautomobile brakes to these days interactive search engines. It provides variousmachine learning techniques to support data mining. The classification is oneof the main and valuable tasks of data mining. Several types of classificationalgorithms have been suggested, tested and compared to determine the futuretrends based on unseen data. There has been no single algorithm found to besuperior over all others for all data sets. The objective of this paper is tocompare various classification algorithms that have been frequently used indata mining for decision support systems. Three decision trees basedalgorithms, one artificial neural network, one statistical, one support vectormachines with and without ada boost and one clustering algorithm are tested andcompared on four data sets from different domains in terms of predictiveaccuracy, error rate, classification index, comprehensibility and trainingtime. Experimental results demonstrate that Genetic Algorithm (GA) and supportvector machines based algorithms are better in terms of predictive accuracy.SVM without adaboost shall be the first choice in context of speed andpredictive accuracy. Adaboost improves the accuracy of SVM but on the cost oflarge training time.
arxiv-1500-210 | Locality-Sensitive Hashing with Margin Based Feature Selection | http://arxiv.org/pdf/1209.5833v2.pdf | author:Makiko Konoshima, Yui Noma category:cs.LG cs.IR published:2012-09-26 summary:We propose a learning method with feature selection for Locality-SensitiveHashing. Locality-Sensitive Hashing converts feature vectors into bit arrays.These bit arrays can be used to perform similarity searches and personalauthentication. The proposed method uses bit arrays longer than those used inthe end for similarity and other searches and by learning selects the bits thatwill be used. We demonstrated this method can effectively perform optimizationfor cases such as fingerprint images with a large number of labels andextremely few data that share the same labels, as well as verifying that it isalso effective for natural images, handwritten digits, and speech features.
arxiv-1500-211 | On the Sensitivity of Shape Fitting Problems | http://arxiv.org/pdf/1209.4893v2.pdf | author:Kasturi Varadarajan, Xin Xiao category:cs.CG cs.LG published:2012-09-21 summary:In this article, we study shape fitting problems, $\epsilon$-coresets, andtotal sensitivity. We focus on the $(j,k)$-projective clustering problems,including $k$-median/$k$-means, $k$-line clustering, $j$-subspaceapproximation, and the integer $(j,k)$-projective clustering problem. We deriveupper bounds of total sensitivities for these problems, and obtain$\epsilon$-coresets using these upper bounds. Using a dimension-reduction typeargument, we are able to greatly simplify earlier results on total sensitivityfor the $k$-median/$k$-means clustering problems, and obtainpositively-weighted $\epsilon$-coresets for several variants of the$(j,k)$-projective clustering problem. We also extend an earlier result on$\epsilon$-coresets for the integer $(j,k)$-projective clustering problem infixed dimension to the case of high dimension.
arxiv-1500-212 | Sequential Convex Programming Methods for A Class of Structured Nonlinear Programming | http://arxiv.org/pdf/1210.3039v1.pdf | author:Zhaosong Lu category:math.OC cs.NA stat.CO stat.ML published:2012-10-10 summary:In this paper we study a broad class of structured nonlinear programming(SNLP) problems. In particular, we first establish the first-order optimalityconditions for them. Then we propose sequential convex programming (SCP)methods for solving them in which each iteration is obtained by solving aconvex programming problem exactly or inexactly. Under some suitableassumptions, we establish that any accumulation point of the sequence generatedby the methods is a KKT point of the SNLP problems. In addition, we propose avariant of the exact SCP method for SNLP in which nonmonotone scheme and"local" Lipschitz constants of the associated functions are used. And a similarconvergence result as mentioned above is established.
arxiv-1500-213 | Efficient Solution to the 3D Problem of Automatic Wall Paintings Reassembly | http://arxiv.org/pdf/1210.2877v1.pdf | author:Constantin Papaodysseus, Dimitris Arabadjis, Michalis Exarhos, Panayiotis Rousopoulos, Solomon Zannos, Michail Panagopoulos, Lena Papazoglou-Manioudaki category:cs.CV math.DG published:2012-10-10 summary:This paper introduces a new approach for the automated reconstruction -reassembly of fragmented objects having one surface near to plane, on the basisof the 3D representation of their constituent fragments. The whole processstarts by 3D scanning of the available fragments. The obtained representationsare properly processed so that they can be tested for possible matches. Next,four novel criteria are introduced, that lead to the determination of pairs ofmatching fragments. These criteria have been chosen so as the whole processimitates the instinctive reassembling method dedicated scholars apply. Thefirst criterion exploits the volume of the gap between two properly placedfragments. The second one considers the fragments' overlapping in each possiblematching position. Criteria 3,4 employ principles from calculus of variationsto obtain bounds for the area and the mean curvature of the contact surfacesand the length of contact curves, which must hold if the two fragments match.The method has been applied, with great success, both in the reconstruction ofobjects artificially broken by the authors and, most importantly, in thevirtual reassembling of parts of wall paintings belonging to the Mycenaiccivilization (c. 1300 B.C.), excavated in a highly fragmented condition inTyrins, Greece.
arxiv-1500-214 | Comparing several heuristics for a packing problem | http://arxiv.org/pdf/1210.4502v1.pdf | author:Camelia-M. Pintea, Cristian Pascan, Mara Hajdu-Macelaru category:cs.NE published:2012-10-10 summary:Packing problems are in general NP-hard, even for simple cases. Since nowthere are no highly efficient algorithms available for solving packingproblems. The two-dimensional bin packing problem is about packing all givenrectangular items, into a minimum size rectangular bin, without overlapping.The restriction is that the items cannot be rotated. The current paper iscomparing a greedy algorithm with a hybrid genetic algorithm in order to seewhich technique is better for the given problem. The algorithms are tested ondifferent sizes data.
arxiv-1500-215 | Kinects and Human Kinetics: A New Approach for Studying Crowd Behavior | http://arxiv.org/pdf/1210.2838v1.pdf | author:Stefan Seer, Norbert Brändle, Carlo Ratti category:cs.CV physics.soc-ph published:2012-10-10 summary:Modeling crowd behavior relies on accurate data of pedestrian movements at ahigh level of detail. Imaging sensors such as cameras provide a good basis forcapturing such detailed pedestrian motion data. However, currently availablecomputer vision technologies, when applied to conventional video footage, stillcannot automatically unveil accurate motions of groups of people or crowds fromthe image sequences. We present a novel data collection approach for studyingcrowd behavior which uses the increasingly popular low-cost sensor MicrosoftKinect. The Kinect captures both standard camera data and a three-dimensionaldepth map. Our human detection and tracking algorithm is based on agglomerativeclustering of depth data captured from an elevated view - in contrast to thelateral view used for gesture recognition in Kinect gaming applications. Ourapproach transforms local Kinect 3D data to a common world coordinate system inorder to stitch together human trajectories from multiple Kinects, which allowsfor a scalable and flexible capturing area. At a testbed with real-worldpedestrian traffic we demonstrate that our approach can provide accuratetrajectories from three Kinects with a Pedestrian Detection Rate of up to 94%and a Multiple Object Tracking Precision of 4 cm. Using a comprehensive datasetof 2240 captured human trajectories we calibrate three variations of the SocialForce model. The results of our model validations indicate their particularability to reproduce the observed crowd behavior in microscopic simulations.
arxiv-1500-216 | An anisotropy preserving metric for DTI processing | http://arxiv.org/pdf/1210.2826v1.pdf | author:Anne Collard, Silvère Bonnabel, Christophe Phillips, Rodolphe Sepulchre category:cs.CV math.DG published:2012-10-10 summary:Statistical analysis of Diffusion Tensor Imaging (DTI) data requires acomputational framework that is both numerically tractable (to account for thehigh dimensional nature of the data) and geometric (to account for thenonlinear nature of diffusion tensors). Building upon earlier studies that haveshown that a Riemannian framework is appropriate to address these challenges,the present paper proposes a novel metric and an accompanying computationalframework for DTI data processing. The proposed metric retains the geometry andthe computational tractability of earlier methods grounded in the affineinvariant metric. In addition, and in contrast to earlier methods, it providesan interpolation method which preserves anisotropy, a central informationcarried by diffusion tensor data.
arxiv-1500-217 | Rank/Norm Regularization with Closed-Form Solutions: Application to Subspace Clustering | http://arxiv.org/pdf/1202.3772v2.pdf | author:Yao-Liang Yu, Dale Schuurmans category:cs.LG cs.NA stat.ML published:2012-02-14 summary:When data is sampled from an unknown subspace, principal component analysis(PCA) provides an effective way to estimate the subspace and hence reduce thedimension of the data. At the heart of PCA is the Eckart-Young-Mirsky theorem,which characterizes the best rank k approximation of a matrix. In this paper,we prove a generalization of the Eckart-Young-Mirsky theorem under allunitarily invariant norms. Using this result, we obtain closed-form solutionsfor a set of rank/norm regularized problems, and derive closed-form solutionsfor a general class of subspace clustering problems (where data is modelled byunions of unknown subspaces). From these results we obtain new theoreticalinsights and promising experimental results.
arxiv-1500-218 | A General Methodology for the Determination of 2D Bodies Elastic Deformation Invariants. Application to the Automatic Identification of Parasites | http://arxiv.org/pdf/1210.2646v1.pdf | author:Dimitris Arabadjis, Panayiotis Rousopoulos, Constantin Papaodysseus, Michalis Panagopoulos, Panayiota Loumou, Georgios Theodoropoulos category:cs.CV cs.AI published:2012-10-09 summary:A novel methodology is introduced here that exploits 2D images of arbitraryelastic body deformation instances, so as to quantify mechano-elasticcharacteristics that are deformation invariant. Determination of suchcharacteristics allows for developing methods offering an image of theundeformed body. General assumptions about the mechano-elastic properties ofthe bodies are stated, which lead to two different approaches for obtainingbodies' deformation invariants. One was developed to spot deformed body'sneutral line and its cross sections, while the other solves deformation PDEs byperforming a set of equivalent image operations on the deformed body images.Both these processes may furnish a body undeformed version from its deformedimage. This was confirmed by obtaining the undeformed shape of deformedparasites, cells (protozoa), fibers and human lips. In addition, the method hasbeen applied to the important problem of parasite automatic classification fromtheir microscopic images. To achieve this, we first apply the previous methodto straighten the highly deformed parasites and then we apply a dedicated curveclassification method to the straightened parasite contours. It is demonstratedthat essentially different deformations of the same parasite give rise topractically the same undeformed shape, thus confirming the consistency of theintroduced methodology. Finally, the developed pattern recognition methodclassifies the unwrapped parasites into 6 families, with an accuracy rate of97.6 %.
arxiv-1500-219 | Multi-view constrained clustering with an incomplete mapping between views | http://arxiv.org/pdf/1210.2640v1.pdf | author:Eric Eaton, Marie desJardins, Sara Jacob category:cs.LG cs.AI published:2012-10-09 summary:Multi-view learning algorithms typically assume a complete bipartite mappingbetween the different views in order to exchange information during thelearning process. However, many applications provide only a partial mappingbetween the views, creating a challenge for current methods. To address thisproblem, we propose a multi-view algorithm based on constrained clustering thatcan operate with an incomplete mapping. Given a set of pairwise constraints ineach view, our approach propagates these constraints using a local similaritymeasure to those instances that can be mapped to the other views, allowing thepropagated constraints to be transferred across views via the partial mapping.It uses co-EM to iteratively estimate the propagation within each view based onthe current clustering model, transfer the constraints across views, and thenupdate the clustering model. By alternating the learning process between views,this approach produces a unified clustering model that is consistent with allviews. We show that this approach significantly improves clustering performanceover several other methods for transferring constraints and allows multi-viewclustering to be reliably applied when given a limited mapping between theviews. Our evaluation reveals that the propagated constraints have highprecision with respect to the true clusters in the data, explaining theirbenefit to clustering performance in both single- and multi-view learningscenarios.
arxiv-1500-220 | Optimization in Differentiable Manifolds in Order to Determine the Method of Construction of Prehistoric Wall-Paintings | http://arxiv.org/pdf/1210.2629v1.pdf | author:Dimitris Arabadjis, Panayiotis Rousopoulos, Constantin Papaodysseus, Michalis Exarhos, Michalis Panagopoulos, Lena Papazoglou-Manioudaki category:cs.CV cs.AI cs.CG published:2012-10-09 summary:In this paper a general methodology is introduced for the determination ofpotential prototype curves used for the drawing of prehistoric wall-paintings.The approach includes a) preprocessing of the wall-paintings contours toproperly partition them, according to their curvature, b) choice of prototypecurves families, c) analysis and optimization in 4-manifold for a firstestimation of the form of these prototypes, d) clustering of the contour partsand the prototypes, to determine a minimal number of potential guides, e)further optimization in 4-manifold, applied to each cluster separately, inorder to determine the exact functional form of the potential guides, togetherwith the corresponding drawn contour parts. The introduced methodologysimultaneously deals with two problems: a) the arbitrariness in data-pointsorientation and b) the determination of one proper form for a prototype curvethat optimally fits the corresponding contour data. Arbitrariness inorientation has been dealt with a novel curvature based error, while the properforms of curve prototypes have been exhaustively determined by embeddingcurvature deformations of the prototypes into 4-manifolds. Application of thismethodology to celebrated wall-paintings excavated at Tyrins, Greece and theGreek island of Thera, manifests it is highly probable that thesewall-paintings had been drawn by means of geometric guides that correspond tolinear spirals and hyperbolae. These geometric forms fit the drawings' lineswith an exceptionally low average error, less than 0.39mm. Hence, the approachsuggests the existence of accurate realizations of complicated geometricentities, more than 1000 years before their axiomatic formulation in ClassicalAges.
arxiv-1500-221 | Gaussian process modelling of multiple short time series | http://arxiv.org/pdf/1210.2503v1.pdf | author:Hande Topa, Antti Honkela category:stat.ML q-bio.QM stat.ME published:2012-10-09 summary:We present techniques for effective Gaussian process (GP) modelling ofmultiple short time series. These problems are common when applying GP modelsindependently to each gene in a gene expression time series data set. Such setstypically contain very few time points. Naive application of common GPmodelling techniques can lead to severe over-fitting or under-fitting in asignificant fraction of the fitted models, depending on the details of the dataset. We propose avoiding over-fitting by constraining the GP length-scale tovalues that focus most of the energy spectrum to frequencies below the Nyquistfrequency corresponding to the sampling frequency in the data set.Under-fitting can be avoided by more informative priors on observation noise.Combining these methods allows applying GP methods reliably automatically tolarge numbers of independent instances of short time series. This isillustrated with experiments with both synthetic data and real gene expressiondata.
arxiv-1500-222 | Parameterized Runtime Analyses of Evolutionary Algorithms for the Euclidean Traveling Salesperson Problem | http://arxiv.org/pdf/1207.0578v2.pdf | author:Andrew M. Sutton, Frank Neumann category:cs.NE cs.DS published:2012-07-03 summary:Parameterized runtime analysis seeks to understand the influence of problemstructure on algorithmic runtime. In this paper, we contribute to thetheoretical understanding of evolutionary algorithms and carry out aparameterized analysis of evolutionary algorithms for the Euclidean travelingsalesperson problem (Euclidean TSP). We investigate the structural properties in TSP instances that influence theoptimization process of evolutionary algorithms and use this information tobound the runtime of simple evolutionary algorithms. Our analysis studies theruntime in dependence of the number of inner points $k$ and shows that $(\mu +\lambda)$ evolutionary algorithms solve the Euclidean TSP in expected time$O((\mu/\lambda) \cdot n^3\gamma(\epsilon) + n\gamma(\epsilon) + (\mu/\lambda)\cdot n^{4k}(2k-1)!)$ where $\gamma$ is a function of the minimum angle$\epsilon$ between any three points. Finally, our analysis provides insights into designing a mutation operatorthat improves the upper bound on expected runtime. We show that a mixedmutation strategy that incorporates both 2-opt moves and permutation jumpsresults in an upper bound of $O((\mu/\lambda) \cdot n^3\gamma(\epsilon) +n\gamma(\epsilon) + (\mu/\lambda) \cdot n^{2k}(k-1)!)$ for the $(\mu+\lambda)$EA.
arxiv-1500-223 | Level Set Estimation from Compressive Measurements using Box Constrained Total Variation Regularization | http://arxiv.org/pdf/1210.2474v1.pdf | author:Akshay Soni, Jarvis Haupt category:cs.CV stat.AP stat.ML published:2012-10-09 summary:Estimating the level set of a signal from measurements is a task that arisesin a variety of fields, including medical imaging, astronomy, and digitalelevation mapping. Motivated by scenarios where accurate and completemeasurements of the signal may not available, we examine here a simpleprocedure for estimating the level set of a signal from highly incompletemeasurements, which may additionally be corrupted by additive noise. Theproposed procedure is based on box-constrained Total Variation (TV)regularization. We demonstrate the performance of our approach, relative toexisting state-of-the-art techniques for level set estimation from compressivemeasurements, via several simulation examples.
arxiv-1500-224 | Group Model Selection Using Marginal Correlations: The Good, the Bad and the Ugly | http://arxiv.org/pdf/1210.2440v1.pdf | author:Waheed U. Bajwa, Dustin G. Mixon category:math.ST cs.IT math.IT stat.ML stat.TH published:2012-10-08 summary:Group model selection is the problem of determining a small subset of groupsof predictors (e.g., the expression data of genes) that are responsible formajority of the variation in a response variable (e.g., the malignancy of atumor). This paper focuses on group model selection in high-dimensional linearmodels, in which the number of predictors far exceeds the number of samples ofthe response variable. Existing works on high-dimensional group model selectioneither require the number of samples of the response variable to besignificantly larger than the total number of predictors contributing to theresponse or impose restrictive statistical priors on the predictors and/ornonzero regression coefficients. This paper provides comprehensiveunderstanding of a low-complexity approach to group model selection that avoidssome of these limitations. The proposed approach, termed Group Thresholding(GroTh), is based on thresholding of marginal correlations of groups ofpredictors with the response variable and is reminiscent of existingthresholding-based approaches in the literature. The most importantcontribution of the paper in this regard is relating the performance of GroThto a polynomial-time verifiable property of the predictors for the general caseof arbitrary (random or deterministic) predictors and arbitrary nonzeroregression coefficients.
arxiv-1500-225 | Mining Permission Request Patterns from Android and Facebook Applications (extended author version) | http://arxiv.org/pdf/1210.2429v1.pdf | author:Mario Frank, Ben Dong, Adrienne Porter Felt, Dawn Song category:cs.CR cs.AI stat.ML published:2012-10-08 summary:Android and Facebook provide third-party applications with access to users'private data and the ability to perform potentially sensitive operations (e.g.,post to a user's wall or place phone calls). As a security measure, theseplatforms restrict applications' privileges with permission systems: users mustapprove the permissions requested by applications before the applications canmake privacy- or security-relevant API calls. However, recent studies haveshown that users often do not understand permission requests and lack a notionof typicality of requests. As a first step towards simplifying permissionsystems, we cluster a corpus of 188,389 Android applications and 27,029Facebook applications to find patterns in permission requests. Using a methodfor Boolean matrix factorization for finding overlapping clusters, we find thatFacebook permission requests follow a clear structure that exhibits highstability when fitted with only five clusters, whereas Android applicationsdemonstrate more complex permission requests. We also find that low-reputationapplications often deviate from the permission request patterns that weidentified for high-reputation applications suggesting that permission requestpatterns are indicative for user satisfaction or application quality.
arxiv-1500-226 | Touchalytics: On the Applicability of Touchscreen Input as a Behavioral Biometric for Continuous Authentication | http://arxiv.org/pdf/1207.6231v2.pdf | author:Mario Frank, Ralf Biedert, Eugene Ma, Ivan Martinovic, Dawn Song category:cs.CR cs.LG published:2012-07-26 summary:We investigate whether a classifier can continuously authenticate users basedon the way they interact with the touchscreen of a smart phone. We propose aset of 30 behavioral touch features that can be extracted from raw touchscreenlogs and demonstrate that different users populate distinct subspaces of thisfeature space. In a systematic experiment designed to test how this behavioralpattern exhibits consistency over time, we collected touch data from usersinteracting with a smart phone using basic navigation maneuvers, i.e., up-downand left-right scrolling. We propose a classification framework that learns thetouch behavior of a user during an enrollment phase and is able to accept orreject the current user by monitoring interaction with the touch screen. Theclassifier achieves a median equal error rate of 0% for intra-sessionauthentication, 2%-3% for inter-session authentication and below 4% when theauthentication test was carried out one week after the enrollment phase. Whileour experimental findings disqualify this method as a standalone authenticationmechanism for long-term authentication, it could be implemented as a means toextend screen-lock time or as a part of a multi-modal biometric authenticationsystem.
arxiv-1500-227 | Video De-fencing | http://arxiv.org/pdf/1210.2388v1.pdf | author:Yadong Mu, Wei Liu, Shuicheng Yan category:cs.CV cs.MM published:2012-10-08 summary:This paper describes and provides an initial solution to a novel videoediting task, i.e., video de-fencing. It targets automatic restoration of thevideo clips that are corrupted by fence-like occlusions during capture. Our keyobservation lies in the visual parallax between fences and background scenes,which is caused by the fact that the former are typically closer to the camera.Unlike in traditional image inpainting, fence-occluded pixels in the videostend to appear later in the temporal dimension and are therefore recoverablevia optimized pixel selection from relevant frames. To eventually producefence-free videos, major challenges include cross-frame sub-pixel imagealignment under diverse scene depth, and "correct" pixel selection that isrobust to dominating fence pixels. Several novel tools are developed in thispaper, including soft fence detection, weighted truncated optical flow methodand robust temporal median filter. The proposed algorithm is validated onseveral real-world video clips with fences.
arxiv-1500-228 | The Power of Linear Reconstruction Attacks | http://arxiv.org/pdf/1210.2381v1.pdf | author:Shiva Prasad Kasiviswanathan, Mark Rudelson, Adam Smith category:cs.DS cs.CR cs.LG math.PR published:2012-10-08 summary:We consider the power of linear reconstruction attacks in statistical dataprivacy, showing that they can be applied to a much wider range of settingsthan previously understood. Linear attacks have been studied before (Dinur andNissim PODS'03, Dwork, McSherry and Talwar STOC'07, Kasiviswanathan, Rudelson,Smith and Ullman STOC'10, De TCC'12, Muthukrishnan and Nikolov STOC'12) buthave so far been applied only in settings with releases that are obviouslylinear. Consider a database curator who manages a database of sensitive informationbut wants to release statistics about how a sensitive attribute (say, disease)in the database relates to some nonsensitive attributes (e.g., postal code,age, gender, etc). We show one can mount linear reconstruction attacks based onany release that gives: a) the fraction of records that satisfy a givennon-degenerate boolean function. Such releases include contingency tables(previously studied by Kasiviswanathan et al., STOC'10) as well as more complexoutputs like the error rate of classifiers such as decision trees; b) any oneof a large class of M-estimators (that is, the output of empirical riskminimization algorithms), including the standard estimators for linear andlogistic regression. We make two contributions: first, we show how these types of releases can betransformed into a linear format, making them amenable to existingpolynomial-time reconstruction algorithms. This is already perhaps surprising,since many of the above releases (like M-estimators) are obtained by solvinghighly nonlinear formulations. Second, we show how to analyze the resultingattacks under various distributional assumptions on the data. Specifically, weconsider a setting in which the same statistic (either a) or b) above) isreleased about how the sensitive attribute relates to all subsets of size k(out of a total of d) nonsensitive boolean attributes.
arxiv-1500-229 | A Connectionist Network Approach to Find Numerical Solutions of Diophantine Equations | http://arxiv.org/pdf/1206.1971v2.pdf | author:Siby Abraham, Sugata Sanyal, Mukund Sanglikar category:cs.NE published:2012-06-09 summary:The paper introduces a connectionist network approach to find numericalsolutions of Diophantine equations as an attempt to address the famousHilbert's tenth problem. The proposed methodology uses a three layer feedforward neural network with back propagation as sequential learning procedureto find numerical solutions of a class of Diophantine equations. It uses adynamically constructed network architecture where number of nodes in the inputlayer is chosen based on the number of variables in the equation. The powers ofthe given Diophantine equation are taken as input to the input layer. Thetraining of the network starts with initial random integral weights. Theweights are updated based on the back propagation of the error values at theoutput layer. The optimization of weights is augmented by adding a momentumfactor into the network. The optimized weights of the connection between theinput layer and the hidden layer are taken as numerical solution of the givenDiophantine equation. The procedure is validated using different DiophantineEquations of different number of variables and different powers.
arxiv-1500-230 | Modeling Weather Conditions Consequences on Road Trafficking Behaviors | http://arxiv.org/pdf/1210.2294v1.pdf | author:Guillaume Allain, Fabrice Gamboa, Philippe Goudal, Jean-Noël Kien, Jean-Michel Loubes category:stat.ME stat.AP stat.ML published:2012-10-08 summary:We provide a model to understand how adverse weather conditions modifytraffic flow dynamic. We first prove that the microscopic Free Flow Speed ofthe vehicles is changed and then provide a rule to model this change. For this,we consider a thresholded linear model, corresponding to an application of aMARS model to road trafficking. This model adapts itself locally to the wholeroad network and provides accurate unbiased forecasted speed using live orshort term forecasted weather data information.
arxiv-1500-231 | A Fast Distributed Proximal-Gradient Method | http://arxiv.org/pdf/1210.2289v1.pdf | author:Annie I. Chen, Asuman Ozdaglar category:cs.DC cs.LG stat.ML published:2012-10-08 summary:We present a distributed proximal-gradient method for optimizing the averageof convex functions, each of which is the private local objective of an agentin a network with time-varying topology. The local objectives have distinctdifferentiable components, but they share a common nondifferentiable component,which has a favorable structure suitable for effective computation of theproximal operator. In our method, each agent iteratively updates its estimateof the global minimum by optimizing its local objective function, andexchanging estimates with others via communication in the network. UsingNesterov-type acceleration techniques and multiple communication steps periteration, we show that this method converges at the rate 1/k (where k is thenumber of communication rounds between the agents), which is faster than theconvergence rate of the existing distributed methods for solving this problem.The superior convergence rate of our method is also verified by numericalexperiments.
arxiv-1500-232 | Intra-Retinal Layer Segmentation of 3D Optical Coherence Tomography Using Coarse Grained Diffusion Map | http://arxiv.org/pdf/1210.0310v2.pdf | author:Raheleh Kafieh, Hossein Rabbani, Michael D. Abramoff, Milan Sonka category:cs.CV published:2012-10-01 summary:Optical coherence tomography (OCT) is a powerful and noninvasive method forretinal imaging. In this paper, we introduce a fast segmentation method basedon a new variant of spectral graph theory named diffusion maps. The research isperformed on spectral domain (SD) OCT images depicting macular and optic nervehead appearance. The presented approach does not require edge-based imageinformation and relies on regional image texture. Consequently, the proposedmethod demonstrates robustness in situations of low image contrast or poorlayer-to-layer image gradients. Diffusion mapping is applied to 2D and 3D OCTdatasets composed of two steps, one for partitioning the data into importantand less important sections, and another one for localization of internallayers.In the first step, the pixels/voxels are grouped in rectangular/cubicsets to form a graph node.The weights of a graph are calculated based ongeometric distances between pixels/voxels and differences of their meanintensity.The first diffusion map clusters the data into three parts, thesecond of which is the area of interest. The other two sections are eliminatedfrom the remaining calculations. In the second step, the remaining area issubjected to another diffusion map assessment and the internal layers arelocalized based on their textural similarities.The proposed method was testedon 23 datasets from two patient groups (glaucoma and normals). The meanunsigned border positioning errors(mean - SD) was 8.52 - 3.13 and 7.56 - 2.95micrometer for the 2D and 3D methods, respectively.
arxiv-1500-233 | Semisupervised Classifier Evaluation and Recalibration | http://arxiv.org/pdf/1210.2162v1.pdf | author:Peter Welinder, Max Welling, Pietro Perona category:cs.LG cs.CV published:2012-10-08 summary:How many labeled examples are needed to estimate a classifier's performanceon a new dataset? We study the case where data is plentiful, but labels areexpensive. We show that by making a few reasonable assumptions on the structureof the data, it is possible to estimate performance curves, with confidencebounds, using a small number of ground truth labels. Our approach, which wecall Semisupervised Performance Evaluation (SPE), is based on a generativemodel for the classifier's confidence scores. In addition to estimating theperformance of classifiers on new datasets, SPE can be used to recalibrate aclassifier by re-estimating the class-conditional confidence distributions.
arxiv-1500-234 | Epitome for Automatic Image Colorization | http://arxiv.org/pdf/1210.4481v1.pdf | author:Yingzhen Yang, Xinqi Chu, Tian-Tsong Ng, Alex Yong-Sang Chia, Shuicheng Yan, Thomas S. Huang category:cs.CV cs.LG cs.MM published:2012-10-08 summary:Image colorization adds color to grayscale images. It not only increases thevisual appeal of grayscale images, but also enriches the information containedin scientific images that lack color information. Most existing methods ofcolorization require laborious user interaction for scribbles or imagesegmentation. To eliminate the need for human labor, we develop an automaticimage colorization method using epitome. Built upon a generative graphicalmodel, epitome is a condensed image appearance and shape model which alsoproves to be an effective summary of color information for the colorizationtask. We train the epitome from the reference images and perform inference inthe epitome to colorize grayscale images, rendering better colorization resultsthan previous method in our experiments.
arxiv-1500-235 | On the Sample Complexity of Predictive Sparse Coding | http://arxiv.org/pdf/1202.4050v2.pdf | author:Nishant A. Mehta, Alexander G. Gray category:cs.LG stat.ML published:2012-02-18 summary:The goal of predictive sparse coding is to learn a representation of examplesas sparse linear combinations of elements from a dictionary, such that alearned hypothesis linear in the new representation performs well on apredictive task. Predictive sparse coding algorithms recently have demonstratedimpressive performance on a variety of supervised tasks, but theirgeneralization properties have not been studied. We establish the firstgeneralization error bounds for predictive sparse coding, covering twosettings: 1) the overcomplete setting, where the number of features k exceedsthe original dimensionality d; and 2) the high or infinite-dimensional setting,where only dimension-free bounds are useful. Both learning bounds intimatelydepend on stability properties of the learned sparse encoder, as measured onthe training sample. Consequently, we first present a fundamental stabilityresult for the LASSO, a result characterizing the stability of the sparse codeswith respect to perturbations to the dictionary. In the overcomplete setting,we present an estimation error bound that decays as \tilde{O}(sqrt(d k/m)) withrespect to d and k. In the high or infinite-dimensional setting, we show adimension-free bound that is \tilde{O}(sqrt(k^2 s / m)) with respect to k ands, where s is an upper bound on the number of non-zeros in the sparse code forany training data point.
arxiv-1500-236 | Sparsity by Worst-Case Quadratic Penalties | http://arxiv.org/pdf/1210.2077v1.pdf | author:Yves Grandvalet, Julien Chiquet, Christophe Ambroise category:stat.ML stat.CO published:2012-10-07 summary:This paper proposes a new robust regression interpretation of sparsepenalties such as the elastic net and the group-lasso. Beyond providing a newviewpoint on these penalization schemes, our approach results in a unifiedoptimization strategy. Our evaluation experiments demonstrate that thisstrategy, implemented on the elastic net, is computationally extremelyefficient for small to medium size problems. Our accompanying software solvesproblems at machine precision in the time required to get a rough estimate withcompeting state-of-the-art algorithms.
arxiv-1500-237 | Reply to Comments on Neuroelectrodynamics: Where are the Real Conceptual Pitfalls? | http://arxiv.org/pdf/1210.1983v1.pdf | author:Dorian Aur category:cs.NE nlin.AO physics.bio-ph q-bio.NC published:2012-10-06 summary:The fundamental, powerful process of computation in the brain has been widelymisunderstood. The paper [1] associates the general failure to buildintelligent thinking machines with current reductionist principles of temporalcoding and advocates for a change in paradigm regarding the brain analogy.Since fragments of information are stored in proteins which can shift betweenseveral structures to perform their function, the biological substrate isactively involved in physical computation. The intrinsic nonlinear dynamics ofaction potentials and synaptic activities maintain physical interactions withinand between neurons in the brain. During these events the required informationis exchanged between molecular structures (proteins) which store fragments ofinformation and the generated electric flux which carries and integratesinformation in the brain. The entire process of physical interaction explainshow the brain actively creates or experiences meaning. This process ofinteraction during an action potential generation can be simply seen as themoment when the neuron solves a many-body problem. A neuroelectrodynamic theoryshows that the neuron solves equations rather than exclusively computesfunctions. With the main focus on temporal patterns, the spike timing dogma(STD) has neglected important forms of computation which do occur insideneurons. In addition, artificial neural models have missed the most importantpart since the real super-computing power of the brain has its origins incomputations that occur within neurons.
arxiv-1500-238 | Feature Selection via L1-Penalized Squared-Loss Mutual Information | http://arxiv.org/pdf/1210.1960v1.pdf | author:Wittawat Jitkrittum, Hirotaka Hachiya, Masashi Sugiyama category:stat.ML cs.LG published:2012-10-06 summary:Feature selection is a technique to screen out less important features. Manyexisting supervised feature selection algorithms use redundancy and relevancyas the main criteria to select features. However, feature interaction,potentially a key characteristic in real-world problems, has not received muchattention. As an attempt to take feature interaction into account, we proposeL1-LSMI, an L1-regularization based algorithm that maximizes a squared-lossvariant of mutual information between selected features and outputs. Numericalresults show that L1-LSMI performs well in handling redundancy, detectingnon-linear dependency, and considering feature interaction.
arxiv-1500-239 | A comparative study on face recognition techniques and neural network | http://arxiv.org/pdf/1210.1916v1.pdf | author:Meftah Ur Rahman category:cs.CV published:2012-10-06 summary:In modern times, face recognition has become one of the key aspects ofcomputer vision. There are at least two reasons for this trend; the first isthe commercial and law enforcement applications, and the second is theavailability of feasible technologies after years of research. Due to the verynature of the problem, computer scientists, neuro-scientists and psychologistsall share a keen interest in this field. In plain words, it is a computerapplication for automatically identifying a person from a still image or videoframe. One of the ways to accomplish this is by comparing selected featuresfrom the image and a facial database. There are hundreds if not thousandfactors associated with this. In this paper some of the most common techniquesavailable including applications of neural network in facial recognition arestudied and compared with respect to their performance.
arxiv-1500-240 | Designing various component analysis at will | http://arxiv.org/pdf/1207.3554v2.pdf | author:Akisato Kimura, Masashi Sugiyama, Sakano Hitoshi, Hirokazu Kameoka category:cs.CV cs.NA stat.ME stat.ML published:2012-07-16 summary:This paper provides a generic framework of component analysis (CA) methodsintroducing a new expression for scatter matrices and Gram matrices, calledGeneralized Pairwise Expression (GPE). This expression is quite compact buthighly powerful: The framework includes not only (1) the standard CA methodsbut also (2) several regularization techniques, (3) weighted extensions, (4)some clustering methods, and (5) their semi-supervised extensions. This paperalso presents quite a simple methodology for designing a desired CA method fromthe proposed framework: Adopting the known GPEs as templates, and generating anew method by combining these templates appropriately.
arxiv-1500-241 | Automatic Relevance Determination in Nonnegative Matrix Factorization with the β-Divergence | http://arxiv.org/pdf/1111.6085v3.pdf | author:Vincent Y. F. Tan, Cédric Févotte category:stat.ML stat.ME published:2011-11-25 summary:This paper addresses the estimation of the latent dimensionality innonnegative matrix factorization (NMF) with the \beta-divergence. The\beta-divergence is a family of cost functions that includes the squaredEuclidean distance, Kullback-Leibler and Itakura-Saito divergences as specialcases. Learning the model order is important as it is necessary to strike theright balance between data fidelity and overfitting. We propose a Bayesianmodel based on automatic relevance determination in which the columns of thedictionary matrix and the rows of the activation matrix are tied togetherthrough a common scale parameter in their prior. A family ofmajorization-minimization algorithms is proposed for maximum a posteriori (MAP)estimation. A subset of scale parameters is driven to a small lower bound inthe course of inference, with the effect of pruning the corresponding spuriouscomponents. We demonstrate the efficacy and robustness of our algorithms byperforming extensive experiments on synthetic data, the swimmer dataset, amusic decomposition example and a stock price prediction task.
arxiv-1500-242 | Modularity-Based Clustering for Network-Constrained Trajectories | http://arxiv.org/pdf/1205.2172v2.pdf | author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:stat.ML cs.LG published:2012-05-10 summary:We present a novel clustering approach for moving object trajectories thatare constrained by an underlying road network. The approach builds a similaritygraph based on these trajectories then uses modularity-optimization hiearchicalgraph clustering to regroup trajectories with similar profiles. Ourexperimental study shows the superiority of the proposed approach over classichierarchical clustering and gives a brief insight to visualization of theclustering results.
arxiv-1500-243 | A network of spiking neurons for computing sparse representations in an energy efficient way | http://arxiv.org/pdf/1210.1530v1.pdf | author:Tao Hu, Alexander Genkin, Dmitri B. Chklovskii category:cs.NE q-bio.NC published:2012-10-04 summary:Computing sparse redundant representations is an important problem both inapplied mathematics and neuroscience. In many applications, this problem mustbe solved in an energy efficient way. Here, we propose a hybrid distributedalgorithm (HDA), which solves this problem on a network of simple nodescommunicating via low-bandwidth channels. HDA nodes perform bothgradient-descent-like steps on analog internal variables andcoordinate-descent-like steps via quantized external variables communicated toeach other. Interestingly, such operation is equivalent to a network ofintegrate-and-fire neurons, suggesting that HDA may serve as a model of neuralcomputation. We show that the numerical performance of HDA is on par withexisting algorithms. In the asymptotic regime the representation error of HDAdecays with time, t, as 1/t. HDA is stable against time-varying noise,specifically, the representation error decays as 1/sqrt(t) for Gaussian whitenoise.
arxiv-1500-244 | A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and Tighter Bound | http://arxiv.org/pdf/1210.1461v1.pdf | author:Shusen Wang, Zhihua Zhang, Jian Li category:cs.LG cs.DM stat.ML published:2012-10-04 summary:The CUR matrix decomposition is an important extension of Nystr\"{o}mapproximation to a general matrix. It approximates any data matrix in terms ofa small number of its columns and rows. In this paper we propose a novelrandomized CUR algorithm with an expected relative-error bound. The proposedalgorithm has the advantages over the existing relative-error CUR algorithmsthat it possesses tighter theoretical bound and lower time complexity, and thatit can avoid maintaining the whole data matrix in main memory. Finally,experiments on several real-world datasets demonstrate significant improvementover the existing relative-error algorithms.
arxiv-1500-245 | On the Computational Complexity of Stochastic Controller Optimization in POMDPs | http://arxiv.org/pdf/1107.3090v2.pdf | author:Nikos Vlassis, Michael L. Littman, David Barber category:cs.CC cs.LG cs.SY math.OC F.2.1 published:2011-07-15 summary:We show that the problem of finding an optimal stochastic 'blind' controllerin a Markov decision process is an NP-hard problem. The corresponding decisionproblem is NP-hard, in PSPACE, and SQRT-SUM-hard, hence placing it in NP wouldimply breakthroughs in long-standing open problems in computer science. Ourresult establishes that the more general problem of stochastic controlleroptimization in POMDPs is also NP-hard. Nonetheless, we outline a special casethat is convex and admits efficient global solutions.
arxiv-1500-246 | Learning Heterogeneous Similarity Measures for Hybrid-Recommendations in Meta-Mining | http://arxiv.org/pdf/1210.1317v1.pdf | author:Phong Nguyen, Jun Wang, Melanie Hilario, Alexandros Kalousis category:cs.LG cs.AI published:2012-10-04 summary:The notion of meta-mining has appeared recently and extends the traditionalmeta-learning in two ways. First it does not learn meta-models that providesupport only for the learning algorithm selection task but ones that supportthe whole data-mining process. In addition it abandons the so called black-boxapproach to algorithm description followed in meta-learning. Now in addition tothe datasets, algorithms also have descriptors, workflows as well. For thelatter two these descriptions are semantic, describing properties of thealgorithms. With the availability of descriptors both for datasets and datamining workflows the traditional modelling techniques followed inmeta-learning, typically based on classification and regression algorithms, areno longer appropriate. Instead we are faced with a problem the nature of whichis much more similar to the problems that appear in recommendation systems. Themost important meta-mining requirements are that suggestions should use onlydatasets and workflows descriptors and the cold-start problem, e.g. providingworkflow suggestions for new datasets. In this paper we take a different view on the meta-mining modelling problemand treat it as a recommender problem. In order to account for the meta-miningspecificities we derive a novel metric-based-learning recommender approach. Ourmethod learns two homogeneous metrics, one in the dataset and one in theworkflow space, and a heterogeneous one in the dataset-workflow space. Alllearned metrics reflect similarities established from the dataset-workflowpreference matrix. We demonstrate our method on meta-mining over biological(microarray datasets) problems. The application of our method is not limited tothe meta-mining problem, its formulations is general enough so that it can beapplied on problems with similar requirements.
arxiv-1500-247 | Efficient Constrained Regret Minimization | http://arxiv.org/pdf/1205.2265v2.pdf | author:Mehrdad Mahdavi, Tianbao Yang, Rong Jin category:cs.LG published:2012-05-08 summary:Online learning constitutes a mathematical and compelling framework toanalyze sequential decision making problems in adversarial environments. Thelearner repeatedly chooses an action, the environment responds with an outcome,and then the learner receives a reward for the played action. The goal of thelearner is to maximize his total reward. However, there are situations inwhich, in addition to maximizing the cumulative reward, there are someadditional constraints on the sequence of decisions that must be satisfied onaverage by the learner. In this paper we study an extension to the onlinelearning where the learner aims to maximize the total reward given that someadditional constraints need to be satisfied. By leveraging on the theory ofLagrangian method in constrained optimization, we propose Lagrangianexponentially weighted average (LEWA) algorithm, which is a primal-dual variantof the well known exponentially weighted average algorithm, to efficientlysolve constrained online decision making problems. Using novel theoreticalanalysis, we establish the regret and the violation of the constraint bounds infull information and bandit feedback models.
arxiv-1500-248 | Unfolding Latent Tree Structures using 4th Order Tensors | http://arxiv.org/pdf/1210.1258v1.pdf | author:Mariya Ishteva, Haesun Park, Le Song category:cs.LG stat.ML published:2012-10-03 summary:Discovering the latent structure from many observed variables is an importantyet challenging learning task. Existing approaches for discovering latentstructures often require the unknown number of hidden states as an input. Inthis paper, we propose a quartet based approach which is \emph{agnostic} tothis number. The key contribution is a novel rank characterization of thetensor associated with the marginal distribution of a quartet. Thischaracterization allows us to design a \emph{nuclear norm} based test forresolving quartet relations. We then use the quartet test as a subroutine in adivide-and-conquer algorithm for recovering the latent tree structure. Undermild conditions, the algorithm is consistent and its error probability decaysexponentially with increasing sample size. We demonstrate that the proposedapproach compares favorably to alternatives. In a real world stock dataset, italso discovers meaningful groupings of variables, and produces a model thatfits the data better.
arxiv-1500-249 | Evaluating Discussion Boards on BlackBoard as a Collaborative Learning Tool A Students Survey and Reflections | http://arxiv.org/pdf/1210.1230v1.pdf | author:AbdelHameed A. Badawy, Michelle M. Hugue category:cs.CV cs.CY published:2012-10-03 summary:In this paper, we investigate how the students think of their experience in ajunior level course that has a blackboard course presence where the studentsuse the discussion boards extensively. A survey is set up through blackboard asa voluntary quiz and the student who participated were given a freebie point.The results and the participation were very interesting in terms of thefeedback we got via open comments from the students as well as the statisticswe gathered from the answers to the questions. The students have shownunderstanding and willingness to participate in pedagogy-enhancing endeavors.
arxiv-1500-250 | Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization | http://arxiv.org/pdf/1210.1190v1.pdf | author:Abhishek Kumar, Vikas Sindhwani, Prabhanjan Kambadur category:stat.ML cs.LG published:2012-10-03 summary:The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012)turns non-negative matrix factorization (NMF) into a tractable problem.Recently, a new class of provably-correct NMF algorithms have emerged underthis assumption. In this paper, we reformulate the separable NMF problem asthat of finding the extreme rays of the conical hull of a finite set ofvectors. From this geometric perspective, we derive new separable NMFalgorithms that are highly scalable and empirically noise robust, and haveseveral other favorable properties in relation to existing methods. A parallelimplementation of our algorithm demonstrates high scalability on shared- anddistributed-memory machines.
arxiv-1500-251 | Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations | http://arxiv.org/pdf/1210.1121v1.pdf | author:Krishnakumar Balasubramanian, Kai Yu, Guy Lebanon category:stat.ML cs.LG published:2012-10-03 summary:We propose and analyze a novel framework for learning sparse representations,based on two statistical techniques: kernel smoothing and marginal regression.The proposed approach provides a flexible framework for incorporating featuresimilarity or temporal information present in data sets, via non-parametrickernel smoothing. We provide generalization bounds for dictionary learningusing smooth sparse coding and show how the sample complexity depends on the L1norm of kernel function used. Furthermore, we propose using marginal regressionfor obtaining sparse codes, which significantly improves the speed and allowsone to scale to large dictionary sizes easily. We demonstrate the advantages ofthe proposed approach, both in terms of accuracy and speed by extensiveexperimentation on several real data sets. In addition, we demonstrate how theproposed approach could be used for improving semi-supervised sparse coding.
arxiv-1500-252 | Sensory Anticipation of Optical Flow in Mobile Robotics | http://arxiv.org/pdf/1210.1104v1.pdf | author:Arturo Ribes, Jesús Cerquides, Yiannis Demiris, Ramón López de Mántaras category:cs.RO cs.LG published:2012-10-03 summary:In order to anticipate dangerous events, like a collision, an agent needs tomake long-term predictions. However, those are challenging due to uncertaintiesin internal and external variables and environment dynamics. A sensorimotormodel is acquired online by the mobile robot using a state-of-the-art methodthat learns the optical flow distribution in images, both in space and time.The learnt model is used to anticipate the optical flow up to a given timehorizon and to predict an imminent collision by using reinforcement learning.We demonstrate that multi-modal predictions reduce to simpler distributionsonce actions are taken into account.
arxiv-1500-253 | Predicting human preferences using the block structure of complex social networks | http://arxiv.org/pdf/1210.1048v1.pdf | author:Roger Guimera, Alejandro Llorente, Esteban Moro, Marta Sales-Pardo category:physics.soc-ph cs.SI stat.ML published:2012-10-03 summary:With ever-increasing available data, predicting individuals' preferences andhelping them locate the most relevant information has become a pressing need.Understanding and predicting preferences is also important from a fundamentalpoint of view, as part of what has been called a "new" computational socialscience. Here, we propose a novel approach based on stochastic block models,which have been developed by sociologists as plausible models of complexnetworks of social interactions. Our model is in the spirit of predictingindividuals' preferences based on the preferences of others but, rather thanfitting a particular model, we rely on a Bayesian approach that samples overthe ensemble of all possible models. We show that our approach is considerablymore accurate than leading recommender algorithms, with major relativeimprovements between 38% and 99% over industry-level algorithms. Besides, ourapproach sheds light on decision-making processes by identifying groups ofindividuals that have consistently similar preferences, and enabling theanalysis of the characteristics of those groups.
arxiv-1500-254 | Robust Degraded Face Recognition Using Enhanced Local Frequency Descriptor and Multi-scale Competition | http://arxiv.org/pdf/1210.1033v1.pdf | author:Guangling Sun, Guoqing Li, Xinpeng Zhang category:cs.CV published:2012-10-03 summary:Recognizing degraded faces from low resolution and blurred images are commonyet challenging task. Local Frequency Descriptor (LFD) has been proved to beeffective for this task yet it is extracted from a spatial neighborhood of apixel of a frequency plane independently regardless of correlations betweenfrequencies. In addition, it uses a fixed window size named single scale ofshort-term Frequency transform (STFT). To explore the frequency correlationsand preserve low resolution and blur insensitive simultaneously, we proposeEnhanced LFD in which information in space and frequency is jointly utilized soas to be more descriptive and discriminative than LFD. The multi-scalecompetition strategy that extracts multiple descriptors corresponding tomultiple window sizes of STFT and take one corresponding to maximum confidenceas the final recognition result. The experiments conducted on Yale and FERETdatabases demonstrate that promising results have been achieved by the proposedEnhanced LFD and multi-scale competition strategy.
arxiv-1500-255 | Blurred Image Classification based on Adaptive Dictionary | http://arxiv.org/pdf/1210.1029v1.pdf | author:Guangling Sun, Guoqing Li, Jie Yin category:cs.CV published:2012-10-03 summary:Two types of framework for blurred image classification based on adaptivedictionary are proposed. Given a blurred image, instead of image deblurring,the semantic category of the image is determined by blur insensitive sparsecoefficients calculated depending on an adaptive dictionary. The dictionary isadaptive to the Point Spread Function (PSF) estimated from input blurred image.The PSF is assumed to be space invariant and inferred separately in oneframework or updated combining with sparse coefficients calculation in analternative and iterative algorithm in the other framework. The experiment hasevaluated three types of blur, naming defocus blur, simple motion blur andcamera shake blur. The experiment results confirm the effectiveness of theproposed frameworks.
arxiv-1500-256 | Logical segmentation for article extraction in digitized old newspapers | http://arxiv.org/pdf/1210.0999v1.pdf | author:Thomas Palfray, David Hébert, Stéphane Nicolas, Pierrick Tranouez, Thierry Paquet category:cs.IR cs.CV cs.DL published:2012-10-03 summary:Newspapers are documents made of news item and informative articles. They arenot meant to be red iteratively: the reader can pick his items in any order hefancies. Ignoring this structural property, most digitized newspaper archivesonly offer access by issue or at best by page to their content. We have built adigitization workflow that automatically extracts newspaper articles fromimages, which allows indexing and retrieval of information at the articlelevel. Our back-end system extracts the logical structure of the page toproduce the informative units: the articles. Each image is labelled at thepixel level, through a machine learning based method, then the page logicalstructure is constructed up from there by the detection of structuring entitiessuch as horizontal and vertical separators, titles and text lines. This logicalstructure is stored in a METS wrapper associated to the ALTO file produced bythe system including the OCRed text. Our front-end system provides a web highdefinition visualisation of images, textual indexing and retrieval facilities,searching and reading at the article level. Articles transcriptions can becollaboratively corrected, which as a consequence allows for better indexing.We are currently testing our system on the archives of the Journal de Rouen,one of France eldest local newspaper. These 250 years of publication amount to300 000 pages of very variable image quality and layout complexity. Test year1808 can be consulted at plair.univ-rouen.fr.
arxiv-1500-257 | Combined Descriptors in Spatial Pyramid Domain for Image Classification | http://arxiv.org/pdf/1210.0386v3.pdf | author:Junlin Hu, Ping Guo category:cs.CV I.4.9; I.5.4 published:2012-10-01 summary:Recently spatial pyramid matching (SPM) with scale invariant featuretransform (SIFT) descriptor has been successfully used in image classification.Unfortunately, the codebook generation and feature quantization proceduresusing SIFT feature have the high complexity both in time and space. To addressthis problem, in this paper, we propose an approach which combines local binarypatterns (LBP) and three-patch local binary patterns (TPLBP) in spatial pyramiddomain. The proposed method does not need to learn the codebook and featurequantization processing, hence it becomes very efficient. Experiments on twopopular benchmark datasets demonstrate that the proposed method alwayssignificantly outperforms the very popular SPM based SIFT descriptor methodboth in time and classification accuracy.
arxiv-1500-258 | Learning from Collective Intelligence in Groups | http://arxiv.org/pdf/1210.0954v1.pdf | author:Guo-Jun Qi, Charu Aggarwal, Pierre Moulin, Thomas Huang category:cs.SI cs.LG published:2012-10-03 summary:Collective intelligence, which aggregates the shared information from largecrowds, is often negatively impacted by unreliable information sources with thelow quality data. This becomes a barrier to the effective use of collectiveintelligence in a variety of applications. In order to address this issue, wepropose a probabilistic model to jointly assess the reliability of sources andfind the true data. We observe that different sources are often not independentof each other. Instead, sources are prone to be mutually influenced, whichmakes them dependent when sharing information with each other. High dependencybetween sources makes collective intelligence vulnerable to the overuse ofredundant (and possibly incorrect) information from the dependent sources.Thus, we reveal the latent group structure among dependent sources, andaggregate the information at the group level rather than from individualsources directly. This can prevent the collective intelligence from beinginappropriately dominated by dependent sources. We will also explicitly revealthe reliability of groups, and minimize the negative impacts of unreliablegroups. Experimental results on real-world data sets show the effectiveness ofthe proposed approach with respect to existing algorithms.
arxiv-1500-259 | Schrödinger Diffusion for Shape Analysis with Texture | http://arxiv.org/pdf/1210.0880v1.pdf | author:Jose A. Iglesias, Ron Kimmel category:cs.CV cs.CG cs.GR math.AP 68U05, 35K08 published:2012-10-02 summary:In recent years, quantities derived from the heat equation have becomepopular in shape processing and analysis of triangulated surfaces. Suchmeasures are often robust with respect to different kinds of perturbations,including near-isometries, topological noise and partialities. Here, we proposeto exploit the semigroup of a Schr\"{o}dinger operator in order to deal withtexture data, while maintaining the desirable properties of the heat kernel. Wedefine a family of Schr\"{o}dinger diffusion distances analogous to the onesassociated to the heat kernels, and show that they are continuous underperturbations of the data. As an application, we introduce a method forretrieval of textured shapes through comparison of Schr\"{o}dinger diffusiondistance histograms with the earth's mover distance, and present some numericalexperiments showing superior performance compared to an analogous method thatignores the texture.
arxiv-1500-260 | Classification of Hepatic Lesions using the Matching Metric | http://arxiv.org/pdf/1210.0866v1.pdf | author:Aaron Adcock, Daniel Rubin, Gunnar Carlsson category:cs.CV cs.CG math.AT published:2012-10-02 summary:In this paper we present a methodology of classifying hepatic (liver) lesionsusing multidimensional persistent homology, the matching metric (also calledthe bottleneck distance), and a support vector machine. We present ourclassification results on a dataset of 132 lesions that have been outlined andannotated by radiologists. We find that topological features are useful in theclassification of hepatic lesions. We also find that two-dimensional persistenthomology outperforms one-dimensional persistent homology in this application.
arxiv-1500-261 | Learning mixtures of structured distributions over discrete domains | http://arxiv.org/pdf/1210.0864v1.pdf | author:Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS math.ST stat.TH published:2012-10-02 summary:Let $\mathfrak{C}$ be a class of probability distributions over the discretedomain $[n] = \{1,...,n\}.$ We show that if $\mathfrak{C}$ satisfies a rathergeneral condition -- essentially, that each distribution in $\mathfrak{C}$ canbe well-approximated by a variable-width histogram with few bins -- then thereis a highly efficient (both in terms of running time and sample complexity)algorithm that can learn any mixture of $k$ unknown distributions from$\mathfrak{C}.$ We analyze several natural types of distributions over $[n]$, includinglog-concave, monotone hazard rate and unimodal distributions, and show thatthey have the required structural property of being well-approximated by ahistogram with few bins. Applying our general algorithm, we obtainnear-optimally efficient algorithms for all these mixture learning problems.
arxiv-1500-262 | Detecting multiword phrases in mathematical text corpora | http://arxiv.org/pdf/1210.0852v1.pdf | author:Winfried Gödert category:cs.CL cs.IR 68P20 H.3.1; I.7.3 published:2012-10-02 summary:We present an approach for detecting multiword phrases in mathematical textcorpora. The method used is based on characteristic features of mathematicalterminology. It makes use of a software tool named Lingo which allows toidentify words by means of previously defined dictionaries for specific wordclasses as adjectives, personal names or nouns. The detection of multiwordgroups is done algorithmically. Possible advantages of the method for indexingand information retrieval and conclusions for applying dictionary-based methodsof automatic indexing instead of stemming procedures are discussed.
arxiv-1500-263 | Enhancing Twitter Data Analysis with Simple Semantic Filtering: Example in Tracking Influenza-Like Illnesses | http://arxiv.org/pdf/1210.0848v1.pdf | author:Son Doan, Lucila Ohno-Machado, Nigel Collier category:cs.SI cs.CL physics.soc-ph published:2012-10-02 summary:Systems that exploit publicly available user generated content such asTwitter messages have been successful in tracking seasonal influenza. Wedeveloped a novel filtering method for Influenza-Like-Illnesses (ILI)-relatedmessages using 587 million messages from Twitter micro-blogs. We first filteredmessages based on syndrome keywords from the BioCaster Ontology, an extantknowledge model of laymen's terms. We then filtered the messages according tosemantic features such as negation, hashtags, emoticons, humor and geography.The data covered 36 weeks for the US 2009 influenza season from 30th August2009 to 8th May 2010. Results showed that our system achieved the highestPearson correlation coefficient of 98.46% (p-value<2.2e-16), an improvement of3.98% over the previous state-of-the-art method. The results indicate thatsimple NLP-based enhancements to existing approaches to mine Twitter data canincrease the value of this inexpensive resource.
arxiv-1500-264 | A Survey of Multibiometric Systems | http://arxiv.org/pdf/1210.0829v1.pdf | author:Harbi AlMahafzah, Maen Zaid AlRwashdeh category:cs.CV published:2012-10-02 summary:Most biometric systems deployed in real-world applications are unimodal.Using unimodal biometric systems have to contend with a variety of problemssuch as: Noise in sensed data; Intra-class variations; Inter-classsimilarities; Non-universality; Spoof attacks. These problems have addressed byusing multibiometric systems, which expected to be more reliable due to thepresence of multiple, independent pieces of evidence.
arxiv-1500-265 | Distributed High Dimensional Information Theoretical Image Registration via Random Projections | http://arxiv.org/pdf/1210.0824v1.pdf | author:Zoltan Szabo, Andras Lorincz category:cs.IT cs.LG math.IT stat.ML published:2012-10-02 summary:Information theoretical measures, such as entropy, mutual information, andvarious divergences, exhibit robust characteristics in image registrationapplications. However, the estimation of these quantities is computationallyintensive in high dimensions. On the other hand, consistent estimation frompairwise distances of the sample points is possible, which suits randomprojection (RP) based low dimensional embeddings. We adapt the RP technique tothis task by means of a simple ensemble method. To the best of our knowledge,this is the first distributed, RP based information theoretical imageregistration approach. The efficiency of the method is demonstrated throughnumerical examples.
arxiv-1500-266 | Discrete geodesic calculus in the space of viscous fluidic objects | http://arxiv.org/pdf/1210.0822v1.pdf | author:Martin Rumpf, Benedikt Wirth category:math.NA cs.CV published:2012-10-02 summary:Based on a local approximation of the Riemannian distance on a manifold by acomputationally cheap dissimilarity measure, a time discrete geodesic calculusis developed, and applications to shape space are explored. The dissimilaritymeasure is derived from a deformation energy whose Hessian reproduces theunderlying Riemannian metric, and it is used to define length and energy ofdiscrete paths in shape space. The notion of discrete geodesics defined asenergy minimizing paths gives rise to a discrete logarithmic map, a variationaldefinition of a discrete exponential map, and a time discrete paralleltransport. This new concept is applied to a shape space in which shapes areconsidered as boundary contours of physical objects consisting of viscousmaterial. The flexibility and computational efficiency of the approach isdemonstrated for topology preserving shape morphing, the representation ofpaths in shape space via local shape variations as path generators, shapeextrapolation via discrete geodesic flow, and the transfer of geometricfeatures.
arxiv-1500-267 | Multibiometric: Feature Level Fusion Using FKP Multi-Instance biometric | http://arxiv.org/pdf/1210.0818v1.pdf | author:Harbi AlMahafzah, Mohammad Imran, H. S. Sheshadri category:cs.CV published:2012-10-02 summary:This paper proposed the use of multi-instance feature level fusion as a meansto improve the performance of Finger Knuckle Print (FKP) verification. Alog-Gabor filter has been used to extract the image local orientationinformation, and represent the FKP features. Experiments are performed usingthe FKP database, which consists of 7,920 images. Results indicate that themulti-instance verification approach outperforms higher performance than usingany single instance. The influence on biometric performance using feature levelfusion under different fusion rules have been demonstrated in this paper.
arxiv-1500-268 | A Semantic Approach for Automatic Structuring and Analysis of Software Process Patterns | http://arxiv.org/pdf/1210.0794v1.pdf | author:Nahla Jlaiel, Khouloud Madhbouh, Mohamed Ben Ahmed category:cs.AI cs.CL published:2012-10-02 summary:The main contribution of this paper, is to propose a novel semantic approachbased on a Natural Language Processing technique in order to ensure a semanticunification of unstructured process patterns which are expressed not only indifferent formats but also, in different forms. This approach is implementedusing the GATE text engineering framework and then evaluated leading up tohigh-quality results motivating us to continue in this direction.
arxiv-1500-269 | Graph-Based Approaches to Clustering Network-Constrained Trajectory Data | http://arxiv.org/pdf/1210.0762v1.pdf | author:Mohamed Khalil El Mahrsi, Fabrice Rossi category:cs.LG stat.ML published:2012-10-02 summary:Even though clustering trajectory data attracted considerable attention inthe last few years, most of prior work assumed that moving objects can movefreely in an euclidean space and did not consider the eventual presence of anunderlying road network and its influence on evaluating the similarity betweentrajectories. In this paper, we present two approaches to clusteringnetwork-constrained trajectory data. The first approach discovers clusters oftrajectories that traveled along the same parts of the road network. The secondapproach is segment-oriented and aims to group together road segments based ontrajectories that they have in common. Both approaches use a graph model todepict the interactions between observations w.r.t. their similarity andcluster this similarity graph using a community detection algorithm. We alsopresent experimental results obtained on synthetic data to showcase ourpropositions.
arxiv-1500-270 | A fast compression-based similarity measure with applications to content-based image retrieval | http://arxiv.org/pdf/1210.0758v1.pdf | author:Daniele Cerra, Mihai Datcu category:stat.ML cs.IR cs.LG published:2012-10-02 summary:Compression-based similarity measures are effectively employed inapplications on diverse data types with a basically parameter-free approach.Nevertheless, there are problems in applying these techniques tomedium-to-large datasets which have been seldom addressed. This paper proposesa similarity measure based on compression with dictionaries, the FastCompression Distance (FCD), which reduces the complexity of these methods,without degradations in performance. On its basis a content-based color imageretrieval system is defined, which can be compared to state-of-the-art methodsbased on invariant color features. Through the FCD a better understanding ofcompression-based techniques is achieved, by performing experiments on datasetswhich are larger than the ones analyzed so far in literature.
arxiv-1500-271 | Invariance of visual operations at the level of receptive fields | http://arxiv.org/pdf/1210.0754v1.pdf | author:Tony Lindeberg category:q-bio.NC cs.CV published:2012-10-02 summary:Receptive field profiles registered by cell recordings have shown thatmammalian vision has developed receptive fields tuned to different sizes andorientations in the image domain as well as to different image velocities inspace-time. This article presents a theoretical model by which families ofidealized receptive field profiles can be derived mathematically from a smallset of basic assumptions that correspond to structural properties of theenvironment. The article also presents a theory for how basic invarianceproperties to variations in scale, viewing direction and relative motion can beobtained from the output of such receptive fields, using complementaryselection mechanisms that operate over the output of families of receptivefields tuned to different parameters. Thereby, the theory shows how basicinvariance properties of a visual system can be obtained already at the levelof receptive fields, and we can explain the different shapes of receptive fieldprofiles found in biological vision from a requirement that the visual systemshould be invariant to the natural types of image transformations that occur inits environment.
arxiv-1500-272 | Evaluation of linear classifiers on articles containing pharmacokinetic evidence of drug-drug interactions | http://arxiv.org/pdf/1210.0734v1.pdf | author:Artemy Kolchinsky, Anália Lourenço, Lang Li, Luis M. Rocha category:stat.ML cs.LG q-bio.QM published:2012-10-02 summary:Background. Drug-drug interaction (DDI) is a major cause of morbidity andmortality. [...] Biomedical literature mining can aid DDI research byextracting relevant DDI signals from either the published literature or largeclinical databases. However, though drug interaction is an ideal area fortranslational research, the inclusion of literature mining methodologies in DDIworkflows is still very preliminary. One area that can benefit from literaturemining is the automatic identification of a large number of potential DDIs,whose pharmacological mechanisms and clinical significance can then be studiedvia in vitro pharmacology and in populo pharmaco-epidemiology. Experiments. Weimplemented a set of classifiers for identifying published articles relevant toexperimental pharmacokinetic DDI evidence. These documents are important foridentifying causal mechanisms behind putative drug-drug interactions, animportant step in the extraction of large numbers of potential DDIs. Weevaluate performance of several linear classifiers on PubMed abstracts, underdifferent feature transformation and dimensionality reduction methods. Inaddition, we investigate the performance benefits of including variouspublicly-available named entity recognition features, as well as a set ofinternally-developed pharmacokinetic dictionaries. Results. We found thatseveral classifiers performed well in distinguishing relevant and irrelevantabstracts. We found that the combination of unigram and bigram textual featuresgave better performance than unigram features alone, and also thatnormalization transforms that adjusted for feature frequency and documentlength improved classification. For some classifiers, such as lineardiscriminant analysis (LDA), proper dimensionality reduction had a large impacton performance. Finally, the inclusion of NER features and dictionaries wasfound not to help classification.
arxiv-1500-273 | TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data Classification | http://arxiv.org/pdf/1210.0699v1.pdf | author:Xavier Bresson, Ruiliang Zhang category:cs.LG published:2012-10-02 summary:We introduce semi-supervised data classification algorithms based on totalvariation (TV), Reproducing Kernel Hilbert Space (RKHS), support vector machine(SVM), Cheeger cut, labeled and unlabeled data points. We design binary andmulti-class semi-supervised classification algorithms. We compare the TV-basedclassification algorithms with the related Laplacian-based algorithms, and showthat TV classification perform significantly better when the number of labeleddata is small.
arxiv-1500-274 | Local stability and robustness of sparse dictionary learning in the presence of noise | http://arxiv.org/pdf/1210.0685v1.pdf | author:Rodolphe Jenatton, Rémi Gribonval, Francis Bach category:stat.ML cs.LG published:2012-10-02 summary:A popular approach within the signal processing and machine learningcommunities consists in modelling signals as sparse linear combinations ofatoms selected from a learned dictionary. While this paradigm has led tonumerous empirical successes in various fields ranging from image to audioprocessing, there have only been a few theoretical arguments supporting theseevidences. In particular, sparse coding, or sparse dictionary learning, relieson a non-convex procedure whose local minima have not been fully analyzed yet.In this paper, we consider a probabilistic model of sparse signals, and showthat, with high probability, sparse coding admits a local minimum around thereference dictionary generating the signals. Our study takes into account thecase of over-complete dictionaries and noisy signals, thus extending previouswork limited to noiseless settings and/or under-complete dictionaries. Theanalysis we conduct is non-asymptotic and makes it possible to understand howthe key quantities of the problem, such as the coherence or the level of noise,can scale with respect to the dimension of the signals, the number of atoms,the sparsity and the number of observations.
arxiv-1500-275 | Super-resolution using Sparse Representations over Learned Dictionaries: Reconstruction of Brain Structure using Electron Microscopy | http://arxiv.org/pdf/1210.0564v1.pdf | author:Tao Hu, Juan Nunez-Iglesias, Shiv Vitaladevuni, Lou Scheffer, Shan Xu, Mehdi Bolorizadeh, Harald Hess, Richard Fetter, Dmitri Chklovskii category:cs.CV q-bio.NC stat.ML published:2012-10-01 summary:A central problem in neuroscience is reconstructing neuronal circuits on thesynapse level. Due to a wide range of scales in brain architecture suchreconstruction requires imaging that is both high-resolution andhigh-throughput. Existing electron microscopy (EM) techniques possess requiredresolution in the lateral plane and either high-throughput or high depthresolution but not both. Here, we exploit recent advances in unsupervisedlearning and signal processing to obtain high depth-resolution EM imagescomputationally without sacrificing throughput. First, we show that the braintissue can be represented as a sparse linear combination of localized basisfunctions that are learned using high-resolution datasets. We then developcompressive sensing-inspired techniques that can reconstruct the brain tissuefrom very few (typically 5) tomographic views of each section. This enablestracing of neuronal processes and, hence, high throughput reconstruction ofneural circuits on the level of individual synapses.
arxiv-1500-276 | Sparse LMS via Online Linearized Bregman Iteration | http://arxiv.org/pdf/1210.0563v1.pdf | author:Tao Hu, Dmitri B. Chklovskii category:cs.IT cs.LG math.IT stat.ML published:2012-10-01 summary:We propose a version of least-mean-square (LMS) algorithm for sparse systemidentification. Our algorithm called online linearized Bregman iteration (OLBI)is derived from minimizing the cumulative prediction error squared along withan l1-l2 norm regularizer. By systematically treating the non-differentiableregularizer we arrive at a simple two-step iteration. We demonstrate that OLBIis bias free and compare its operation with existing sparse LMS algorithms byrederiving them in the online convex optimization framework. We performconvergence analysis of OLBI for white input signals and derive theoreticalexpressions for both the steady state and instantaneous mean square deviations(MSD). We demonstrate numerically that OLBI improves the performance of LMStype algorithms for signals generated from sparse tap weights.
arxiv-1500-277 | Think Locally, Act Globally: Perfectly Balanced Graph Partitioning | http://arxiv.org/pdf/1210.0477v1.pdf | author:Peter Sanders, Christian Schulz category:cs.DS cs.DC cs.NE published:2012-10-01 summary:We present a novel local improvement scheme for the perfectly balanced graphpartitioning problem. This scheme encodes local searches that are notrestricted to a balance constraint into a model allowing us to findcombinations of these searches maintaining balance by applying a negative cycledetection algorithm. We combine this technique with an algorithm to balanceunbalanced solutions and integrate it into a parallel multi-level evolutionaryalgorithm, KaFFPaE, to tackle the problem. Overall, we obtain a system that isfast on the one hand and on the other hand is able to improve or reproduce mostof the best known perfectly balanced partitioning results ever reported in theliterature.
arxiv-1500-278 | Memory Constraint Online Multitask Classification | http://arxiv.org/pdf/1210.0473v1.pdf | author:Giovanni Cavallanti, Nicolò Cesa-Bianchi category:cs.LG published:2012-10-01 summary:We investigate online kernel algorithms which simultaneously process multipleclassification tasks while a fixed constraint is imposed on the size of theiractive sets. We focus in particular on the design of algorithms that canefficiently deal with problems where the number of tasks is extremely high andthe task data are large scale. Two new projection-based algorithms areintroduced to efficiently tackle those issues while presenting different tradeoffs on how the available memory is managed with respect to the priorinformation about the learning tasks. Theoretically sound budget algorithms aredevised by coupling the Randomized Budget Perceptron and the Forgetronalgorithms with the multitask kernel. We show how the two seemingly contrastingproperties of learning from multiple tasks and keeping a constant memoryfootprint can be balanced, and how the sharing of the available space amongdifferent tasks is automatically taken care of. We propose and discuss newinsights on the multitask kernel. Experiments show that online kernel multitaskalgorithms running on a budget can efficiently tackle real world learningproblems involving multiple tasks.
arxiv-1500-279 | Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure | http://arxiv.org/pdf/1205.4481v4.pdf | author:Hua Ouyang, Alexander Gray category:cs.LG stat.CO stat.ML published:2012-05-21 summary:In this work we consider the stochastic minimization of nonsmooth convex lossfunctions, a central problem in machine learning. We propose a novel algorithmcalled Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), whichexploits the structure of common nonsmooth loss functions to achieve optimalconvergence rates for a class of problems including SVMs. It is the firststochastic algorithm that can achieve the optimal O(1/t) rate for minimizingnonsmooth loss functions (with strong convexity). The fast rates are confirmedby empirical comparisons, in which ANSGD significantly outperforms previoussubgradient descent algorithms including SGD.
arxiv-1500-280 | Enhanced Techniques for PDF Image Segmentation and Text Extraction | http://arxiv.org/pdf/1210.0347v1.pdf | author:D. Sasirekha, E. Chandra category:cs.CV published:2012-10-01 summary:Extracting text objects from the PDF images is a challenging problem. Thetext data present in the PDF images contain certain useful information forautomatic annotation, indexing etc. However variations of the text due todifferences in text style, font, size, orientation, alignment as well ascomplex structure make the problem of automatic text extraction extremelydifficult and challenging job. This paper presents two techniques underblock-based classification. After a brief introduction of the classificationmethods, two methods were enhanced and results were evaluated. The performancemetrics for segmentation and time consumption are tested for both the models.
arxiv-1500-281 | Statistical methods for tissue array images - algorithmic scoring and co-training | http://arxiv.org/pdf/1102.0059v2.pdf | author:Donghui Yan, Pei Wang, Michael Linden, Beatrice Knudsen, Timothy Randolph category:stat.ME cs.CE cs.CV cs.LG q-bio.QM published:2011-02-01 summary:Recent advances in tissue microarray technology have allowedimmunohistochemistry to become a powerful medium-to-high throughput analysistool, particularly for the validation of diagnostic and prognostic biomarkers.However, as study size grows, the manual evaluation of these assays becomes aprohibitive limitation; it vastly reduces throughput and greatly increasesvariability and expense. We propose an algorithm - Tissue Array Co-OccurrenceMatrix Analysis (TACOMA) - for quantifying cellular phenotypes based ontextural regularity summarized by local inter-pixel relationships. Thealgorithm can be easily trained for any staining pattern, is absent ofsensitive tuning parameters and has the ability to report salient pixels in animage that contribute to its score. Pathologists' input via informativetraining patches is an important aspect of the algorithm that allows thetraining for any specific marker or cell type. With co-training, the error rateof TACOMA can be reduced substantially for a very small training sample (e.g.,with size 30). We give theoretical insights into the success of co-training viathinning of the feature set in a high-dimensional setting when there is"sufficient" redundancy among the features. TACOMA is flexible, transparent andprovides a scoring process that can be evaluated with clarity and confidence.In a study based on an estrogen receptor (ER) marker, we show that TACOMA iscomparable to, or outperforms, pathologists' performance in terms of accuracyand repeatability.
arxiv-1500-282 | Learning to rank from medical imaging data | http://arxiv.org/pdf/1207.3598v2.pdf | author:Fabian Pedregosa, Alexandre Gramfort, Gaël Varoquaux, Elodie Cauvet, Christophe Pallier, Bertrand Thirion category:cs.LG cs.CV published:2012-07-16 summary:Medical images can be used to predict a clinical score coding for theseverity of a disease, a pain level or the complexity of a cognitive task. Inall these cases, the predicted variable has a natural order. While a standardclassifier discards this information, we would like to take it into account inorder to improve prediction performance. A standard linear regression doesmodel such information, however the linearity assumption is likely not besatisfied when predicting from pixel intensities in an image. In this paper weaddress these modeling challenges with a supervised learning procedure wherethe model aims to order or rank images. We use a linear model for itsrobustness in high dimension and its possible interpretation. We show onsimulations and two fMRI datasets that this approach is able to predict thecorrect ordering on pairs of images, yielding higher prediction accuracy thanstandard regression and multiclass classification techniques.
arxiv-1500-283 | On The Convergence of a Nash Seeking Algorithm with Stochastic State Dependent Payoff | http://arxiv.org/pdf/1210.0193v1.pdf | author:A. F. Hanif, H. Tembine, M. Assaad, D. Zeghlache category:math.OC math.DS math.NA stat.ML published:2012-09-30 summary:Distributed strategic learning has been getting attention in recent years. Assystems become distributed finding Nash equilibria in a distributed fashion isbecoming more important for various applications. In this paper, we develop adistributed strategic learning framework for seeking Nash equilibria understochastic state-dependent payoff functions. We extend the work of Krsticet.al. in [1] to the case of stochastic state dependent payoff functions. Wedevelop an iterative distributed algorithm for Nash seeking and examine itsconvergence to a limiting trajectory defined by an Ordinary DifferentialEquation (ODE). We show convergence of our proposed algorithm for vanishingstep size and provide an error bound for fixed step size. Finally, we conduct astability analysis and apply the proposed scheme in a generic wirelessnetworks. We also present numerical results which corroborate our claim.
arxiv-1500-284 | Robust Parametric Classification and Variable Selection by a Minimum Distance Criterion | http://arxiv.org/pdf/1109.6090v3.pdf | author:Eric C. Chi, David W. Scott category:stat.ME stat.CO stat.ML published:2011-09-28 summary:We investigate a robust penalized logistic regression algorithm based on aminimum distance criterion. Influential outliers are often associated with theexplosion of parameter vector estimates, but in the context of standardlogistic regression, the bias due to outliers always causes the parametervector to implode, that is shrink towards the zero vector. Thus, usingLASSO-like penalties to perform variable selection in the presence of outlierscan result in missed detections of relevant covariates. We show that bychoosing a minimum distance criterion together with an Elastic Net penalty, wecan simultaneously find a parsimonious model and avoid estimation implosioneven in the presence of many outliers in the important small $n$ large $p$situation. Implementation using an MM algorithm is described and performanceevaluated.
arxiv-1500-285 | A Low Cost Vision Based Hybrid Fiducial Mark Tracking Technique for Mobile Industrial Robots | http://arxiv.org/pdf/1210.0153v1.pdf | author:Mohammed Y Aalsalem, Wazir Zada Khan, Quratul Ain Arshad category:cs.CV cs.RO published:2012-09-29 summary:The field of robotic vision is developing rapidly. Robots can reactintelligently and provide assistance to user activities through sentientcomputing. Since industrial applications pose complex requirements that cannotbe handled by humans, an efficient low cost and robust technique is requiredfor the tracking of mobile industrial robots. The existing sensor basedtechniques for mobile robot tracking are expensive and complex to deploy,configure and maintain. Also some of them demand dedicated and often expensivehardware. This paper presents a low cost vision based technique called HybridFiducial Mark Tracking (HFMT) technique for tracking mobile industrial robot.HFMT technique requires off-the-shelf hardware (CCD cameras) and printable 2-Dcircular marks used as fiducials for tracking a mobile industrial robot on apre-defined path. This proposed technique allows the robot to track on apredefined path by using fiducials for the detection of Right and Left turns onthe path and White Strip for tracking the path. The HFMT technique isimplemented and tested on an indoor mobile robot at our laboratory.Experimental results from robot navigating in real environments have confirmedthat our approach is simple and robust and can be adopted in any hostileindustrial environment where humans are unable to work.
arxiv-1500-286 | Self-Delimiting Neural Networks | http://arxiv.org/pdf/1210.0118v1.pdf | author:Juergen Schmidhuber category:cs.NE published:2012-09-29 summary:Self-delimiting (SLIM) programs are a central concept of theoretical computerscience, particularly algorithmic information & probability theory, andasymptotically optimal program search (AOPS). To apply AOPS to (possiblyrecurrent) neural networks (NNs), I introduce SLIM NNs. Neurons of a typicalSLIM NN have threshold activation functions. During a computational episode,activations are spreading from input neurons through the SLIM NN until thecomputation activates a special halt neuron. Weights of the NN's usedconnections define its program. Halting programs form a prefix code. The resetof the initial NN state does not cost more than the latest program execution.Since prefixes of SLIM programs influence their suffixes (weight changesoccurring early in an episode influence which weights are considered later),SLIM NN learning algorithms (LAs) should execute weight changes online duringactivation spreading. This can be achieved by applying AOPS to growing SLIMNNs. To efficiently teach a SLIM NN to solve many tasks, such as correctlyclassifying many different patterns, or solving many different robot controltasks, each connection keeps a list of tasks it is used for. The lists may beefficiently updated during training. To evaluate the overall effect ofcurrently tested weight changes, a SLIM NN LA needs to re-test performance onlyon the efficiently computable union of tasks potentially affected by thecurrent weight changes. Future SLIM NNs will be implemented on 3-dimensionalbrain-like multi-processor hardware. Their LAs will minimize task-specifictotal wire length of used connections, to encourage efficient solutions ofsubtasks by subsets of neurons that are physically close. The novel class ofSLIM NN LAs is currently being probed in ongoing experiments to be reported inseparate papers.
arxiv-1500-287 | The law of brevity in macaque vocal communication is not an artifact of analyzing mean call durations | http://arxiv.org/pdf/1207.3169v2.pdf | author:Stuart Semple, Minna J. Hsu, Govindasamy Agoramoorthy, Ramon Ferrer-i-Cancho category:q-bio.NC cs.CL published:2012-07-13 summary:Words follow the law of brevity, i.e. more frequent words tend to be shorter.From a statistical point of view, this qualitative definition of the law statesthat word length and word frequency are negatively correlated. Here the recentfinding of patterning consistent with the law of brevity in Formosan macaquevocal communication (Semple et al., 2010) is revisited. It is shown that thenegative correlation between mean duration and frequency of use in thevocalizations of Formosan macaques is not an artifact of the use of a meanduration for each call type instead of the customary 'word' length of studiesof the law in human language. The key point demonstrated is that the totalduration of calls of a particular type increases with the number of calls ofthat type. The finding of the law of brevity in the vocalizations of thesemacaques therefore defies a trivial explanation.
arxiv-1500-288 | Optimistic Agents are Asymptotically Optimal | http://arxiv.org/pdf/1210.0077v1.pdf | author:Peter Sunehag, Marcus Hutter category:cs.AI cs.LG published:2012-09-29 summary:We use optimism to introduce generic asymptotically optimal reinforcementlearning agents. They achieve, with an arbitrary finite or compact class ofenvironments, asymptotically optimal behavior. Furthermore, in the finitedeterministic case we provide finite error bounds.
arxiv-1500-289 | Iterative Reweighted Minimization Methods for $l_p$ Regularized Unconstrained Nonlinear Programming | http://arxiv.org/pdf/1210.0066v1.pdf | author:Zhaosong Lu category:math.OC cs.LG stat.CO stat.ML published:2012-09-29 summary:In this paper we study general $l_p$ regularized unconstrained minimizationproblems. In particular, we derive lower bounds for nonzero entries of first-and second-order stationary points, and hence also of local minimizers of the$l_p$ minimization problems. We extend some existing iterative reweighted $l_1$(IRL1) and $l_2$ (IRL2) minimization methods to solve these problems andproposed new variants for them in which each subproblem has a closed formsolution. Also, we provide a unified convergence analysis for these methods. Inaddition, we propose a novel Lipschitz continuous $\epsilon$-approximation to$\x\^p_p$. Using this result, we develop new IRL1 methods for the $l_p$minimization problems and showed that any accumulation point of the sequencegenerated by these methods is a first-order stationary point, provided that theapproximation parameter $\epsilon$ is below a computable threshold value. Thisis a remarkable result since all existing iterative reweighted minimizationmethods require that $\epsilon$ be dynamically updated and approach zero. Ourcomputational results demonstrate that the new IRL1 method is generally morestable than the existing IRL1 methods [21,18] in terms of objective functionvalue and CPU time.
arxiv-1500-290 | Band Selection and Classification of Hyperspectral Images using Mutual Information: An algorithm based on minimizing the error probability using the inequality of Fano | http://arxiv.org/pdf/1210.0528v1.pdf | author:Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV 68U10, 68R05 published:2012-09-28 summary:Hyperspectral image is a substitution of more than a hundred images, calledbands, of the same region. They are taken at juxtaposed frequencies. Thereference image of the region is called Ground Truth map (GT). the problematicis how to find the good bands to classify the pixels of regions; because thebands can be not only redundant, but a source of confusion, and decreasing sothe accuracy of classification. Some methods use Mutual Information (MI) andthreshold, to select relevant bands. Recently there's an algorithm selectionbased on mutual information, using bandwidth rejection and a threshold tocontrol and eliminate redundancy. The band top ranking the MI is selected, andif its neighbors have sensibly the same MI with the GT, they will be consideredredundant and so discarded. This is the most inconvenient of this method,because this avoids the advantage of hyperspectral images: some preciousinformation can be discarded. In this paper we'll make difference betweenuseful and useless redundancy. A band contains useful redundancy if itcontributes to decreasing error probability. According to this scheme, weintroduce new algorithm using also mutual information, but it retains only thebands minimizing the error probability of classification. To controlredundancy, we introduce a complementary threshold. So the good band candidatemust contribute to decrease the last error probability augmented by thethreshold. This process is a wrapper strategy; it gets high performance ofclassification accuracy but it is expensive than filter strategy.
arxiv-1500-291 | Dimensionality Reduction and Classification feature using Mutual Information applied to Hyperspectral Images : A Filter strategy based algorithm | http://arxiv.org/pdf/1210.0052v1.pdf | author:ELkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV 68U10, 68R05 published:2012-09-28 summary:Hyperspectral images (HIS) classification is a high technical remote sensingtool. The goal is to reproduce a thematic map that will be compared with areference ground truth map (GT), constructed by expecting the region. The HIScontains more than a hundred bidirectional measures, called bands (or simplyimages), of the same region. They are taken at juxtaposed frequencies.Unfortunately, some bands contain redundant information, others are affected bythe noise, and the high dimensionality of features made the accuracy ofclassification lower. The problematic is how to find the good bands to classifythe pixels of regions. Some methods use Mutual Information (MI) and threshold,to select relevant bands, without treatment of redundancy. Others control andeliminate redundancy by selecting the band top ranking the MI, and if itsneighbors have sensibly the same MI with the GT, they will be consideredredundant and so discarded. This is the most inconvenient of this method,because this avoids the advantage of hyperspectral images: some preciousinformation can be discarded. In this paper we'll accept the useful redundancy.A band contains useful redundancy if it contributes to produce an estimatedreference map that has higher MI with the GT.nTo control redundancy, weintroduce a complementary threshold added to last value of MI. This process isa Filter strategy; it gets a better performance of classification accuracy andnot expensive, but less preferment than Wrapper strategy.
arxiv-1500-292 | Coupled quasi-harmonic bases | http://arxiv.org/pdf/1210.0026v1.pdf | author:A. Kovnatsky, M. M. Bronstein, A. M. Bronstein, K. Glashoff, R. Kimmel category:cs.CV cs.GR published:2012-09-28 summary:The use of Laplacian eigenbases has been shown to be fruitful in manycomputer graphics applications. Today, state-of-the-art approaches to shapeanalysis, synthesis, and correspondence rely on these natural harmonic basesthat allow using classical tools from harmonic analysis on manifolds. However,many applications involving multiple shapes are obstacled by the fact thatLaplacian eigenbases computed independently on different shapes are oftenincompatible with each other. In this paper, we propose the construction ofcommon approximate eigenbases for multiple shapes using approximate jointdiagonalization algorithms. We illustrate the benefits of the proposed approachon tasks from shape editing, pose transfer, correspondence, and similarity.
arxiv-1500-293 | The failure of the law of brevity in two New World primates. Statistical caveats | http://arxiv.org/pdf/1204.3198v2.pdf | author:Ramon Ferrer-i-Cancho, Antoni Hernández-Fernández category:q-bio.NC cs.CL published:2012-04-14 summary:Parallels of Zipf's law of brevity, the tendency of more frequent words to beshorter, have been found in bottlenose dolphins and Formosan macaques. Althoughthese findings suggest that behavioral repertoires are shaped by a generalprinciple of compression, common marmosets and golden-backed uakaris do notexhibit the law. However, we argue that the law may be impossible or difficultto detect statistically in a given species if the repertoire is too small, aproblem that could be affecting golden backed uakaris, and show that the law ispresent in a subset of the repertoire of common marmosets. We suggest that thevisibility of the law will depend on the subset of the repertoire underconsideration or the repertoire size.
arxiv-1500-294 | Sparse Modeling of Intrinsic Correspondences | http://arxiv.org/pdf/1209.6560v1.pdf | author:J. Pokrass, A. M. Bronstein, M. M. Bronstein, P. Sprechmann, G. Sapiro category:cs.GR cs.CG cs.CV published:2012-09-28 summary:We present a novel sparse modeling approach to non-rigid shape matching usingonly the ability to detect repeatable regions. As the input to our algorithm,we are given only two sets of regions in two shapes; no descriptors areprovided so the correspondence between the regions is not know, nor we know howmany regions correspond in the two shapes. We show that even with such scarceinformation, it is possible to establish very accurate correspondence betweenthe shapes by using methods from the field of sparse modeling, being this, thefirst non-trivial use of sparse models in shape correspondence. We formulatethe problem of permuted sparse coding, in which we solve simultaneously for anunknown permutation ordering the regions on two shapes and for an unknowncorrespondence in functional representation. We also propose a robust variantcapable of handling incomplete matches. Numerically, the problem is solvedefficiently by alternating the solution of a linear assignment and a sparsecoding problem. The proposed methods are evaluated qualitatively andquantitatively on standard benchmarks containing both synthetic and scannedobjects.
arxiv-1500-295 | Improving accuracy and power with transfer learning using a meta-analytic database | http://arxiv.org/pdf/1209.5375v2.pdf | author:Yannick Schwartz, Gaël Varoquaux, Christophe Pallier, Philippe Pinel, Jean-Baptiste Poline, Bertrand Thirion category:stat.ML published:2012-09-24 summary:Typical cohorts in brain imaging studies are not large enough for systematictesting of all the information contained in the images. To build testableworking hypotheses, investigators thus rely on analysis of previous work,sometimes formalized in a so-called meta-analysis. In brain imaging, thisapproach underlies the specification of regions of interest (ROIs) that areusually selected on the basis of the coordinates of previously detectedeffects. In this paper, we propose to use a database of images, rather thancoordinates, and frame the problem as transfer learning: learning adiscriminant model on a reference task to apply it to a different but relatednew task. To facilitate statistical analysis of small cohorts, we use a sparsediscriminant model that selects predictive voxels on the reference task andthus provides a principled procedure to define ROIs. The benefits of ourapproach are twofold. First it uses the reference database for prediction, i.e.to provide potential biomarkers in a clinical setting. Second it increasesstatistical power on the new task. We demonstrate on a set of 18 pairs offunctional MRI experimental conditions that our approach gives good prediction.In addition, on a specific transfer situation involving different scanners atdifferent locations, we show that voxel selection based on transfer learningleads to higher detection power on small cohorts.
arxiv-1500-296 | A Complete System for Candidate Polyps Detection in Virtual Colonoscopy | http://arxiv.org/pdf/1209.6525v1.pdf | author:Marcelo Fiori, Pablo Musé, Guillermo Sapiro category:cs.CV cs.LG published:2012-09-28 summary:Computer tomographic colonography, combined with computer-aided detection, isa promising emerging technique for colonic polyp analysis. We present acomplete pipeline for polyp detection, starting with a simple colonsegmentation technique that enhances polyps, followed by an adaptive-scalecandidate polyp delineation and classification based on new texture andgeometric features that consider both the information in the candidate polyplocation and its immediate surrounding area. The proposed system is tested withground truth data, including flat and small polyps which are hard to detecteven with optical colonoscopy. For polyps larger than 6mm in size we achieve100% sensitivity with just 0.9 false positives per case, and for polyps largerthan 3mm in size we achieve 93% sensitivity with 2.8 false positives per case.
arxiv-1500-297 | Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eQTL mapping | http://arxiv.org/pdf/0909.1373v3.pdf | author:Seyoung Kim, Eric P. Xing category:stat.ML q-bio.GN q-bio.QM stat.AP stat.ME published:2009-09-08 summary:We consider the problem of estimating a sparse multi-response regressionfunction, with an application to expression quantitative trait locus (eQTL)mapping, where the goal is to discover genetic variations that influencegene-expression levels. In particular, we investigate a shrinkage techniquecapable of capturing a given hierarchical structure over the responses, such asa hierarchical clustering tree with leaf nodes for responses and internal nodesfor clusters of related responses at multiple granularity, and we seek toleverage this structure to recover covariates relevant to eachhierarchically-defined cluster of responses. We propose a tree-guided grouplasso, or tree lasso, for estimating such structured sparsity undermulti-response regression by employing a novel penalty function constructedfrom the tree. We describe a systematic weighting scheme for the overlappinggroups in the tree-penalty such that each regression coefficient is penalizedin a balanced manner despite the inhomogeneous multiplicity of groupmemberships of the regression coefficients due to overlaps among groups. Forefficient optimization, we employ a smoothing proximal gradient method that wasoriginally developed for a general class of structured-sparsity-inducingpenalties. Using simulated and yeast data sets, we demonstrate that our methodshows a superior performance in terms of both prediction errors and recovery oftrue sparsity patterns, compared to other methods for learning amultivariate-response regression.
arxiv-1500-298 | Predicate Generation for Learning-Based Quantifier-Free Loop Invariant Inference | http://arxiv.org/pdf/1207.7167v2.pdf | author:Wonchan Lee, Yungbum Jung, Bow-yaw Wang, Kwangkuen Yi category:cs.LO cs.LG F.3.1 published:2012-07-31 summary:We address the predicate generation problem in the context of loop invariantinference. Motivated by the interpolation-based abstraction refinementtechnique, we apply the interpolation theorem to synthesize predicatesimplicitly implied by program texts. Our technique is able to improve theeffectiveness and efficiency of the learning-based loop invariant inferencealgorithm in [14]. We report experiment results of examples from Linux,SPEC2000, and Tar utility.
arxiv-1500-299 | Partial Gaussian Graphical Model Estimation | http://arxiv.org/pdf/1209.6419v1.pdf | author:Xiao-Tong Yuan, Tong Zhang category:cs.LG cs.IT math.IT stat.ML published:2012-09-28 summary:This paper studies the partial estimation of Gaussian graphical models fromhigh-dimensional empirical observations. We derive a convex formulation forthis problem using $\ell_1$-regularized maximum-likelihood estimation, whichcan be solved via a block coordinate descent algorithm. Statistical estimationperformance can be established for our method. The proposed approach hascompetitive empirical performance compared to existing methods, as demonstratedby various experiments on synthetic and real datasets.
arxiv-1500-300 | A Deterministic Analysis of an Online Convex Mixture of Expert Algorithms | http://arxiv.org/pdf/1209.6409v1.pdf | author:Mehmet A. Donmez, Sait Tunc, Suleyman S. Kozat category:cs.LG published:2012-09-28 summary:We analyze an online learning algorithm that adaptively combines outputs oftwo constituent algorithms (or the experts) running in parallel to model anunknown desired signal. This online learning algorithm is shown to achieve (andin some cases outperform) the mean-square error (MSE) performance of the bestconstituent algorithm in the mixture in the steady-state. However, the MSEanalysis of this algorithm in the literature uses approximations and relies onstatistical models on the underlying signals and systems. Hence, such ananalysis may not be useful or valid for signals generated by various real lifesystems that show high degrees of nonstationarity, limit cycles and, in manycases, that are even chaotic. In this paper, we produce results in anindividual sequence manner. In particular, we relate the time-accumulatedsquared estimation error of this online algorithm at any time over any intervalto the time accumulated squared estimation error of the optimal convex mixtureof the constituent algorithms directly tuned to the underlying signal in adeterministic sense without any statistical assumptions. In this sense, ouranalysis provides the transient, steady-state and tracking behavior of thisalgorithm in a strong sense without any approximations in the derivations orstatistical assumptions on the underlying signals such that our results areguaranteed to hold. We illustrate the introduced results through examples.
