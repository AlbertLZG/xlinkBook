arxiv-1500-1 | The Segmentation Fusion Method On10 Multi-Sensors | http://arxiv.org/pdf/1208.4842v1.pdf | author:Firouz Abdullah Al-Wassai, N. V. Kalyankar category:cs.CV published:2012-08-23 summary:The most significant problem may be undesirable effects for the spectralsignatures of fused images as well as the benefits of using fused images mostlycompared to their source images were acquired at the same time by one sensor.They may or may not be suitable for the fusion of other images. It becomestherefore increasingly important to investigate techniques that allowmulti-sensor, multi-date image fusion to make final conclusions can be drawn onthe most suitable method of fusion. So, In this study we present a new methodSegmentation Fusion method (SF) for remotely sensed images is presented byconsidering the physical characteristics of sensors, which uses a feature levelprocessing paradigm. In a particularly, attempts to test the proposed methodperformance on 10 multi-sensor images and comparing it with different fusiontechniques for estimating the quality and degree of information improvementquantitatively by using various spatial and spectral metrics.
arxiv-1500-2 | Introduction of the weight edition errors in the Levenshtein distance | http://arxiv.org/pdf/1208.4503v1.pdf | author:Gueddah Hicham category:cs.CL published:2012-08-22 summary:In this paper, we present a new approach dedicated to correcting the spellingerrors of the Arabic language. This approach corrects typographical errors likeinserting, deleting, and permutation. Our method is inspired from theLevenshtein algorithm, and allows a finer and better scheduling thanLevenshtein. The results obtained are very satisfactory and encouraging, whichshows the interest of our new approach.
arxiv-1500-3 | A non-parametric mixture model for topic modeling over time | http://arxiv.org/pdf/1208.4411v1.pdf | author:Avinava Dubey, Ahmed Hefny, Sinead Williamson, Eric P. Xing category:stat.ML published:2012-08-22 summary:A single, stationary topic model such as latent Dirichlet allocation isinappropriate for modeling corpora that span long time periods, as thepopularity of topics is likely to change over time. A number of models thatincorporate time have been proposed, but in general they either exhibit limitedforms of temporal variation, or require computationally expensive inferencemethods. In this paper we propose non-parametric Topics over Time (npTOT), amodel for time-varying topics that allows an unbounded number of topics andexible distribution over the temporal variations in those topics' popularity.We develop a collapsed Gibbs sampler for the proposed model and compare againstexisting models on synthetic and real document sets.
arxiv-1500-4 | Generating ordered list of Recommended Items: a Hybrid Recommender System of Microblog | http://arxiv.org/pdf/1208.4147v3.pdf | author:Yingzhen Li, Ye Zhang category:cs.IR cs.LG cs.SI published:2012-08-21 summary:Precise recommendation of followers helps in improving the user experienceand maintaining the prosperity of twitter and microblog platforms. In thispaper, we design a hybrid recommender system of microblog as a solution of KDDCup 2012, track 1 task, which requires predicting users a user might follow inTencent Microblog. We describe the background of the problem and present thealgorithm consisting of keyword analysis, user taxonomy, (potential)interestsextraction and item recommendation. Experimental result shows the highperformance of our algorithm. Some possible improvements are discussed, whichleads to further study.
arxiv-1500-5 | Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev Descent | http://arxiv.org/pdf/1208.4391v2.pdf | author:Yanchao Yang, Ganesh Sundaramoorthi category:cs.CV cs.SY published:2012-08-21 summary:We present a method to track the precise shape of an object in video based onnew modeling and optimization on a new Riemannian manifold of parameterizedregions. Joint dynamic shape and appearance models, in which a template of the objectis propagated to match the object shape and radiance in the next frame, areadvantageous over methods employing global image statistics in cases of complexobject radiance and cluttered background. In cases of 3D object motion andviewpoint change, self-occlusions and dis-occlusions of the object areprominent, and current methods employing joint shape and appearance models areunable to adapt to new shape and appearance information, leading to inaccurateshape detection. In this work, we model self-occlusions and dis-occlusions in ajoint shape and appearance tracking framework. Self-occlusions and the warp to propagate the template are coupled, thus ajoint problem is formulated. We derive a coarse-to-fine optimization scheme,advantageous in object tracking, that initially perturbs the template by coarseperturbations before transitioning to finer-scale perturbations, traversing allscales, seamlessly and automatically. The scheme is a gradient descent on anovel infinite-dimensional Riemannian manifold that we introduce. The manifoldconsists of planar parameterized regions, and the metric that we introduce is anovel Sobolev-type metric defined on infinitesimal vector fields on regions.The metric has the property of resulting in a gradient descent thatautomatically favors coarse-scale deformations (when they reduce the energy)before moving to finer-scale deformations. Experiments on video exhibiting occlusion/dis-occlusion, complex radiance andbackground show that occlusion/dis-occlusion modeling leads to superior shapeaccuracy compared to recent methods employing joint shape/appearance models oremploying global statistics.
arxiv-1500-6 | An Online Character Recognition System to Convert Grantha Script to Malayalam | http://arxiv.org/pdf/1208.4316v1.pdf | author:Sreeraj. M, Sumam Mary Idicula category:cs.CV published:2012-08-21 summary:This paper presents a novel approach to recognize Grantha, an ancient scriptin South India and converting it to Malayalam, a prevalent language in SouthIndia using online character recognition mechanism. The motivation behind thiswork owes its credit to (i) developing a mechanism to recognize Grantha scriptin this modern world and (ii) affirming the strong connection among Grantha andMalayalam. A framework for the recognition of Grantha script using onlinecharacter recognition is designed and implemented. The features extracted fromthe Grantha script comprises mainly of time-domain features based on writingdirection and curvature. The recognized characters are mapped to correspondingMalayalam characters. The framework was tested on a bed of medium lengthmanuscripts containing 9-12 sample lines and printed pages of a book titledSoundarya Lahari writtenin Grantha by Sri Adi Shankara to recognize the wordsand sentences. The manuscript recognition rates with the system are for Granthaas 92.11%, Old Malayalam 90.82% and for new Malayalam script 89.56%. Therecognition rates of pages of the printed book are for Grantha as 96.16%, OldMalayalam script 95.22% and new Malayalam script as 92.32% respectively. Theseresults show the efficiency of the developed system.
arxiv-1500-7 | Minerva and minepy: a C engine for the MINE suite and its R, Python and MATLAB wrappers | http://arxiv.org/pdf/1208.4271v2.pdf | author:Davide Albanese, Michele Filosi, Roberto Visintainer, Samantha Riccadonna, Giuseppe Jurman, Cesare Furlanello category:stat.ML q-bio.QM published:2012-08-21 summary:We introduce a novel implementation in ANSI C of the MINE family ofalgorithms for computing maximal information-based measures of dependencebetween two variables in large datasets, with the aim of a low memory footprintand ease of integration within bioinformatics pipelines. We provide thelibraries minerva (with the R interface) and minepy for Python, MATLAB, Octaveand C++. The C solution reduces the large memory requirement of the originalJava implementation, has good upscaling properties, and offers a nativeparallelization for the R interface. Low memory requirements are demonstratedon the MINE benchmarks as well as on large (n=1340) microarray and IlluminaGAII RNA-seq transcriptomics datasets. Availability and Implementation: Source code and binaries are freelyavailable for download under GPL3 licence at http://minepy.sourceforge.net forminepy and through the CRAN repository http://cran.r-project.org for the Rpackage minerva. All software is multiplatform (MS Windows, Linux and OSX).
arxiv-1500-8 | Iterative graph cuts for image segmentation with a nonlinear statistical shape prior | http://arxiv.org/pdf/1208.4384v2.pdf | author:Joshua C. Chang, Tom Chou category:cs.CV math.OC q-bio.QM stat.AP published:2012-08-21 summary:Shape-based regularization has proven to be a useful method for delineatingobjects within noisy images where one has prior knowledge of the shape of thetargeted object. When a collection of possible shapes is available, thespecification of a shape prior using kernel density estimation is a naturaltechnique. Unfortunately, energy functionals arising from kernel densityestimation are of a form that makes them impossible to directly minimize usingefficient optimization algorithms such as graph cuts. Our main contribution isto show how one may recast the energy functional into a form that isminimizable iteratively and efficiently using graph cuts.
arxiv-1500-9 | Learning LiNGAM based on data with more variables than observations | http://arxiv.org/pdf/1208.4183v1.pdf | author:Shohei Shimizu category:stat.ML published:2012-08-21 summary:A very important topic in systems biology is developing statistical methodsthat automatically find causal relations in gene regulatory networks with noprior knowledge of causal connectivity. Many methods have been developed fortime series data. However, discovery methods based on steady-state data areoften necessary and preferable since obtaining time series data can be moreexpensive and/or infeasible for many biological systems. A conventionalapproach is causal Bayesian networks. However, estimation of Bayesian networksis ill-posed. In many cases it cannot uniquely identify the underlying causalnetwork and only gives a large class of equivalent causal networks that cannotbe distinguished between based on the data distribution. We propose a newdiscovery algorithm for uniquely identifying the underlying causal network ofgenes. To the best of our knowledge, the proposed method is the first algorithmfor learning gene networks based on a fully identifiable causal model calledLiNGAM. We here compare our algorithm with competing algorithms usingartificially-generated data, although it is definitely better to test it basedon real microarray gene expression data.
arxiv-1500-10 | A Learning Theoretic Approach to Energy Harvesting Communication System Optimization | http://arxiv.org/pdf/1208.4290v2.pdf | author:Pol Blasco, Deniz Gündüz, Mischa Dohler category:cs.LG cs.NI published:2012-08-21 summary:A point-to-point wireless communication system in which the transmitter isequipped with an energy harvesting device and a rechargeable battery, isstudied. Both the energy and the data arrivals at the transmitter are modeledas Markov processes. Delay-limited communication is considered assuming thatthe underlying channel is block fading with memory, and the instantaneouschannel state information is available at both the transmitter and thereceiver. The expected total transmitted data during the transmitter'sactivation time is maximized under three different sets of assumptionsregarding the information available at the transmitter about the underlyingstochastic processes. A learning theoretic approach is introduced, which doesnot assume any a priori information on the Markov processes governing thecommunication system. In addition, online and offline optimization problems arestudied for the same setting. Full statistical knowledge and causal informationon the realizations of the underlying stochastic processes are assumed in theonline optimization problem, while the offline optimization problem assumesnon-causal knowledge of the realizations in advance. Comparing the optimalsolutions in all three frameworks, the performance loss due to the lack of thetransmitter's information regarding the behaviors of the underlying Markovprocesses is quantified.
arxiv-1500-11 | A Unified Approach for Modeling and Recognition of Individual Actions and Group Activities | http://arxiv.org/pdf/1208.4398v1.pdf | author:Qiang Qiu, Rama Chellappa category:cs.CV stat.ML published:2012-08-21 summary:Recognizing group activities is challenging due to the difficulties inisolating individual entities, finding the respective roles played by theindividuals and representing the complex interactions among the participants.Individual actions and group activities in videos can be represented in acommon framework as they share the following common feature: both are composedof a set of low-level features describing motions, e.g., optical flow for eachpixel or a trajectory for each feature point, according to a set of compositionconstraints in both temporal and spatial dimensions. In this paper, we presenta unified model to assess the similarity between two given individual or groupactivities. Our approach avoids explicit extraction of individual actors,identifying and representing the inter-person interactions. With the proposedapproach, retrieval from a video database can be performed throughQuery-by-Example; and activities can be recognized by querying videoscontaining known activities. The suggested video matching process can beperformed in an unsupervised manner. We demonstrate the performance of ourapproach by recognizing a set of human actions and football plays.
arxiv-1500-12 | Performance Tuning Of J48 Algorithm For Prediction Of Soil Fertility | http://arxiv.org/pdf/1208.3943v1.pdf | author:Jay Gholap category:cs.LG cs.DB cs.PF stat.ML published:2012-08-20 summary:Data mining involves the systematic analysis of large data sets, and datamining in agricultural soil datasets is exciting and modern research area. Theproductive capacity of a soil depends on soil fertility. Achieving andmaintaining appropriate levels of soil fertility, is of utmost importance ifagricultural land is to remain capable of nourishing crop production. In thisresearch, Steps for building a predictive model of soil fertility have beenexplained. This paper aims at predicting soil fertility class using decision treealgorithms in data mining . Further, it focuses on performance tuning of J48decision tree algorithm with the help of meta-techniques such as attributeselection and boosting.
arxiv-1500-13 | Semi-supervised Clustering Ensemble by Voting | http://arxiv.org/pdf/1208.4138v1.pdf | author:Ashraf Mohammed Iqbal, Abidalrahman Moh'd, Zahoor Khan category:cs.LG stat.ML published:2012-08-20 summary:Clustering ensemble is one of the most recent advances in unsupervisedlearning. It aims to combine the clustering results obtained using differentalgorithms or from different runs of the same clustering algorithm for the samedata set, this is accomplished using on a consensus function, the efficiencyand accuracy of this method has been proven in many works in literature. In thefirst part of this paper we make a comparison among current approaches toclustering ensemble in literature. All of these approaches consist of two mainsteps: the ensemble generation and consensus function. In the second part ofthe paper, we suggest engaging supervision in the clustering ensemble procedureto get more enhancements on the clustering results. Supervision can be appliedin two places: either by using semi-supervised algorithms in the clusteringensemble generation step or in the form of a feedback used by the consensusfunction stage. Also, we introduce a flexible two parameter weightingmechanism, the first parameter describes the compatibility between the datasetsunder study and the semi-supervised clustering algorithms used to generate thebase partitions, the second parameter is used to provide the user feedback onthe these partitions. The two parameters are engaged in a "relabeling andvoting" based consensus function to produce the final clustering.
arxiv-1500-14 | Learning sparse messages in networks of neural cliques | http://arxiv.org/pdf/1208.4009v1.pdf | author:Behrooz Kamary Aliabadi, Claude Berrou, Vincent Gripon, Xiaoran Jiang category:cs.NE published:2012-08-20 summary:An extension to a recently introduced binary neural network is proposed inorder to allow the learning of sparse messages, in large numbers and with highmemory efficiency. This new network is justified both in biological andinformational terms. The learning and retrieval rules are detailed andillustrated by various simulation results.
arxiv-1500-15 | Recent Technological Advances in Natural Language Processing and Artificial Intelligence | http://arxiv.org/pdf/1208.4079v1.pdf | author:Nishal Pradeepkumar Shah category:cs.CL I.2.7 published:2012-08-20 summary:A recent advance in computer technology has permitted scientists to implementand test algorithms that were known from quite some time (or not) but whichwere computationally expensive. Two such projects are IBM's Jeopardy as a partof its DeepQA project [1] and Wolfram's Wolframalpha[2]. Both these methodsimplement natural language processing (another goal of AI scientists) and tryto answer questions as asked by the user. Though the goal of the two projectsis similar, both of them have a different procedure at it's core. In thefollowing sections, the mechanism and history of IBM's Jeopardy and Wolframalpha has been explained followed by the implications of these projects inrealizing Ray Kurzweil's [3] dream of passing the Turing test by 2029. A recipeof taking the above projects to a new level is also explained.
arxiv-1500-16 | Joint-ViVo: Selecting and Weighting Visual Words Jointly for Bag-of-Features based Tissue Classification in Medical Images | http://arxiv.org/pdf/1208.3822v2.pdf | author:Jingyan Wang category:cs.CV stat.ML published:2012-08-19 summary:Automatically classifying the tissues types of Region of Interest (ROI) inmedical imaging has been an important application in Computer-Aided Diagnosis(CAD), such as classification of breast parenchymal tissue in the mammogram,classify lung disease patterns in High-Resolution Computed Tomography (HRCT)etc. Recently, bag-of-features method has shown its power in this field,treating each ROI as a set of local features. In this paper, we investigateusing the bag-of-features strategy to classify the tissue types in medicalimaging applications. Two important issues are considered here: the visualvocabulary learning and weighting. Although there are already plenty ofalgorithms to deal with them, all of them treat them independently, namely, thevocabulary learned first and then the histogram weighted. Inspired byAuto-Context who learns the features and classifier jointly, we try to developa novel algorithm that learns the vocabulary and weights jointly. The newalgorithm, called Joint-ViVo, works in an iterative way. In each iteration, wefirst learn the weights for each visual word by maximizing the margin of ROItriplets, and then select the most discriminate visual words based on thelearned weights for the next iteration. We test our algorithm on three tissueclassification tasks: identifying brain tissue type in magnetic resonanceimaging (MRI), classifying lung tissue in HRCT images, and classifying breasttissue density in mammograms. The results show that Joint-ViVo can performeffectively for classifying tissues.
arxiv-1500-17 | Discriminative Sparse Coding on Multi-Manifold for Data Representation and Classification | http://arxiv.org/pdf/1208.3839v2.pdf | author:Jing-Yan Wang category:cs.CV cs.LG stat.ML published:2012-08-19 summary:Sparse coding has been popularly used as an effective data representationmethod in various applications, such as computer vision, medical imaging andbioinformatics, etc. However, the conventional sparse coding algorithms and itsmanifold regularized variants (graph sparse coding and Laplacian sparsecoding), learn the codebook and codes in a unsupervised manner and neglect theclass information available in the training set. To address this problem, inthis paper we propose a novel discriminative sparse coding method based onmulti-manifold, by learning discriminative class-conditional codebooks andsparse codes from both data feature space and class labels. First, the entiretraining set is partitioned into multiple manifolds according to the classlabels. Then, we formulate the sparse coding as a manifold-manifold matchingproblem and learn class-conditional codebooks and codes to maximize themanifold margins of different classes. Lastly, we present a data point-manifoldmatching error based strategy to classify the unlabeled data point.Experimental results on somatic mutations identification and breast tumorsclassification in ultrasonic images tasks demonstrate the efficacy of theproposed data representation-classification approach.
arxiv-1500-18 | Adaptive Graph via Multiple Kernel Learning for Nonnegative Matrix Factorization | http://arxiv.org/pdf/1208.3845v3.pdf | author:Jing-Yan Wang, Mustafa AbdulJabbar category:cs.LG cs.CV stat.ML published:2012-08-19 summary:Nonnegative Matrix Factorization (NMF) has been continuously evolving inseveral areas like pattern recognition and information retrieval methods. Itfactorizes a matrix into a product of 2 low-rank non-negative matrices thatwill define parts-based, and linear representation of nonnegative data.Recently, Graph regularized NMF (GrNMF) is proposed to find a compactrepresentation,which uncovers the hidden semantics and simultaneously respectsthe intrinsic geometric structure. In GNMF, an affinity graph is constructedfrom the original data space to encode the geometrical information. In thispaper, we propose a novel idea which engages a Multiple Kernel Learningapproach into refining the graph structure that reflects the factorization ofthe matrix and the new data space. The GrNMF is improved by utilizing the graphrefined by the kernel learning, and then a novel kernel learning method isintroduced under the GrNMF framework. Our approach shows encouraging results ofthe proposed algorithm in comparison to the state-of-the-art clusteringalgorithms like NMF, GrNMF, SVD etc.
arxiv-1500-19 | Evaluation of Computational Grammar Formalisms for Indian Languages | http://arxiv.org/pdf/1209.1301v1.pdf | author:Nisheeth Joshi, Iti Mathur category:cs.CL published:2012-08-19 summary:Natural Language Parsing has been the most prominent research area since thegenesis of Natural Language Processing. Probabilistic Parsers are beingdeveloped to make the process of parser development much easier, accurate andfast. In Indian context, identification of which Computational GrammarFormalism is to be used is still a question which needs to be answered. In thispaper we focus on this problem and try to analyze different formalisms forIndian languages.
arxiv-1500-20 | Input Scheme for Hindi Using Phonetic Mapping | http://arxiv.org/pdf/1209.1300v1.pdf | author:Nisheeth Joshi, Iti Mathur category:cs.CL published:2012-08-19 summary:Written Communication on Computers requires knowledge of writing text for thedesired language using Computer. Mostly people do not use any other languagebesides English. This creates a barrier. To resolve this issue we havedeveloped a scheme to input text in Hindi using phonetic mapping scheme. Usingthis scheme we generate intermediate code strings and match them withpronunciations of input text. Our system show significant success over otherinput systems available.
arxiv-1500-21 | Trace transform based method for color image domain identification | http://arxiv.org/pdf/1208.3901v2.pdf | author:Igor G. Olaizola, Marco Quartulli, Julian Florez, Basilio Sierra category:cs.CV published:2012-08-19 summary:Context categorization is a fundamental pre-requisite for multi-domainmultimedia content analysis applications in order to manage contextualinformation in an efficient manner. In this paper, we introduce a new colorimage context categorization method (DITEC) based on the trace transform. Theproblem of dimensionality reduction of the obtained trace transform signal isaddressed through statistical descriptors that keep the underlying information.These extracted features offer a highly discriminant behavior for contentcategorization. The theoretical properties of the method are analyzed andvalidated experimentally through two different datasets.
arxiv-1500-22 | Online Learning with Predictable Sequences | http://arxiv.org/pdf/1208.3728v2.pdf | author:Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.LG published:2012-08-18 summary:We present methods for online linear optimization that take advantage ofbenign (as opposed to worst-case) sequences. Specifically if the sequenceencountered by the learner is described well by a known "predictable process",the algorithms presented enjoy tighter bounds as compared to the typical worstcase bounds. Additionally, the methods achieve the usual worst-case regretbounds if the sequence is not benign. Our approach can be seen as a way ofadding prior knowledge about the sequence within the paradigm of onlinelearning. The setting is shown to encompass partial and side information.Variance and path-length bounds can be seen as particular examples of onlinelearning with simple predictable sequences. We further extend our methods and results to include competing with a set ofpossible predictable processes (models), that is "learning" the predictableprocess itself concurrently with using it to obtain better regret guarantees.We show that such model selection is possible under various assumptions on theavailable feedback. Our results suggest a promising direction of furtherresearch with potential applications to stock market and time seriesprediction.
arxiv-1500-23 | Improved Total Variation based Image Compressive Sensing Recovery by Nonlocal Regularization | http://arxiv.org/pdf/1208.3716v2.pdf | author:Jian Zhang, Shaohui Liu, Debin Zhao, Ruiqin Xiong, Siwei Ma category:cs.CV published:2012-08-18 summary:Recently, total variation (TV) based minimization algorithms have achievedgreat success in compressive sensing (CS) recovery for natural images due toits virtue of preserving edges. However, the use of TV is not able to recoverthe fine details and textures, and often suffers from undesirable staircaseartifact. To reduce these effects, this letter presents an improved TV basedimage CS recovery algorithm by introducing a new nonlocal regularizationconstraint into CS optimization problem. The nonlocal regularization is builton the well known nonlocal means (NLM) filtering and takes advantage ofself-similarity in images, which helps to suppress the staircase effect andrestore the fine details. Furthermore, an efficient augmented Lagrangian basedalgorithm is developed to solve the above combined TV and nonlocalregularization constrained problem. Experimental results demonstrate that theproposed algorithm achieves significant performance improvements over thestate-of-the-art TV based algorithm in both PSNR and visual perception.
arxiv-1500-24 | Multiple graph regularized protein domain ranking | http://arxiv.org/pdf/1208.3779v3.pdf | author:Jim Jing-Yan Wang, Halima Bensmail, Xin Gao category:cs.LG cs.CE cs.IR q-bio.QM published:2012-08-18 summary:Background Protein domain ranking is a fundamental task in structuralbiology. Most protein domain ranking methods rely on the pairwise comparison ofprotein domains while neglecting the global manifold structure of the proteindomain database. Recently, graph regularized ranking that exploits the globalstructure of the graph defined by the pairwise similarities has been proposed.However, the existing graph regularized ranking methods are very sensitive tothe choice of the graph model and parameters, and this remains a difficultproblem for most of the protein domain ranking methods. Results To tackle this problem, we have developed the Multiple Graphregularized Ranking algorithm, MultiG- Rank. Instead of using a single graph toregularize the ranking scores, MultiG-Rank approximates the intrinsic manifoldof protein domain distribution by combining multiple initial graphs for theregularization. Graph weights are learned with ranking scores jointly andautomatically, by alternately minimizing an ob- jective function in aniterative algorithm. Experimental results on a subset of the ASTRAL SCOPprotein domain database demonstrate that MultiG-Rank achieves a better rankingperformance than single graph regularized ranking methods and pairwisesimilarity based ranking methods. Conclusion The problem of graph model and parameter selection in graphregularized protein domain ranking can be solved effectively by combiningmultiple graphs. This aspect of generalization introduces a new frontier inapplying multiple graphs to solving protein domain ranking applications.
arxiv-1500-25 | Image Super-Resolution via Dual-Dictionary Learning And Sparse Representation | http://arxiv.org/pdf/1208.3723v1.pdf | author:Jian Zhang, Chen Zhao, Ruiqin Xiong, Siwei Ma, Debin Zhao category:cs.CV published:2012-08-18 summary:Learning-based image super-resolution aims to reconstruct high-frequency (HF)details from the prior model trained by a set of high- and low-resolution imagepatches. In this paper, HF to be estimated is considered as a combination oftwo components: main high-frequency (MHF) and residual high-frequency (RHF),and we propose a novel image super-resolution method via dual-dictionarylearning and sparse representation, which consists of the main dictionarylearning and the residual dictionary learning, to recover MHF and RHFrespectively. Extensive experimental results on test images validate that byemploying the proposed two-layer progressive scheme, more image details can berecovered and much better results can be achieved than the state-of-the-artalgorithms in terms of both PSNR and visual perception.
arxiv-1500-26 | Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms | http://arxiv.org/pdf/1208.3719v2.pdf | author:Chris Thornton, Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown category:cs.LG published:2012-08-18 summary:Many different machine learning algorithms exist; taking into account eachalgorithm's hyperparameters, there is a staggeringly large number of possiblealternatives overall. We consider the problem of simultaneously selecting alearning algorithm and setting its hyperparameters, going beyond previous workthat addresses these issues in isolation. We show that this problem can beaddressed by a fully automated approach, leveraging recent innovations inBayesian optimization. Specifically, we consider a wide range of featureselection techniques (combining 3 search and 8 evaluator methods) and allclassification approaches implemented in WEKA, spanning 2 ensemble methods, 10meta-methods, 27 base classifiers, and hyperparameter settings for eachclassifier. On each of 21 popular datasets from the UCI repository, the KDD Cup09, variants of the MNIST dataset and CIFAR-10, we show classificationperformance often much better than using standard selection/hyperparameteroptimization methods. We hope that our approach will help non-expert users tomore effectively identify machine learning algorithms and hyperparametersettings appropriate to their applications, and hence to achieve improvedperformance.
arxiv-1500-27 | An Evaluation of Popular Copy-Move Forgery Detection Approaches | http://arxiv.org/pdf/1208.3665v2.pdf | author:Vincent Christlein, Christian Riess, Johannes Jordan, Corinna Riess, Elli Angelopoulou category:cs.CV I.4.9 published:2012-08-17 summary:A copy-move forgery is created by copying and pasting content within the sameimage, and potentially post-processing it. In recent years, the detection ofcopy-move forgeries has become one of the most actively researched topics inblind image forensics. A considerable number of different algorithms have beenproposed focusing on different types of postprocessed copies. In this paper, weaim to answer which copy-move forgery detection algorithms and processing steps(e.g., matching, filtering, outlier detection, affine transformationestimation) perform best in various postprocessing scenarios. The focus of ouranalysis is to evaluate the performance of previously proposed feature sets. Weachieve this by casting existing algorithms in a common pipeline. In thispaper, we examined the 15 most prominent feature sets. We analyzed thedetection performance on a per-image basis and on a per-pixel basis. We createda challenging real-world copy-move dataset, and a software framework forsystematic image manipulation. Experiments show, that the keypoint-basedfeatures SIFT and SURF, as well as the block-based DCT, DWT, KPCA, PCA andZernike features perform very well. These feature sets exhibit the bestrobustness against various noise sources and downsampling, while reliablyidentifying the copied regions.
arxiv-1500-28 | Efficient Active Learning of Halfspaces: an Aggressive Approach | http://arxiv.org/pdf/1208.3561v3.pdf | author:Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz category:cs.LG published:2012-08-17 summary:We study pool-based active learning of half-spaces. We revisit the aggressiveapproach for active learning in the realizable case, and show that it can bemade efficient and practical, while also having theoretical guarantees underreasonable assumptions. We further show, both theoretically and experimentally,that it can be preferable to mellow approaches. Our efficient aggressive activelearner of half-spaces has formal approximation guarantees that hold when thepool is separable with a margin. While our analysis is focused on therealizable setting, we show that a simple heuristic allows using the samealgorithm successfully for pools with low error as well. We further compare theaggressive approach to the mellow approach, and prove that there are cases inwhich the aggressive approach results in significantly better label complexitycompared to the mellow approach. We demonstrate experimentally that substantialimprovements in label complexity can be achieved using the aggressive approach,for both realizable and low-error settings.
arxiv-1500-29 | Modeling and Control of CSTR using Model based Neural Network Predictive Control | http://arxiv.org/pdf/1208.3600v1.pdf | author:Piyush Shrivastava category:cs.AI cs.NE nlin.AO published:2012-08-17 summary:This paper presents a predictive control strategy based on neural networkmodel of the plant is applied to Continuous Stirred Tank Reactor (CSTR). Thissystem is a highly nonlinear process; therefore, a nonlinear predictive method,e.g., neural network predictive control, can be a better match to govern thesystem dynamics. In the paper, the NN model and the way in which it can be usedto predict the behavior of the CSTR process over a certain prediction horizonare described, and some comments about the optimization procedure are made.Predictive control algorithm is applied to control the concentration in acontinuous stirred tank reactor (CSTR), whose parameters are optimallydetermined by solving quadratic performance index using the optimizationalgorithm. An efficient control of the product concentration in cstr can beachieved only through accurate model. Here an attempt is made to alleviate themodeling difficulties using Artificial Intelligent technique such as NeuralNetwork. Simulation results demonstrate the feasibility and effectiveness ofthe NNMPC technique.
arxiv-1500-30 | Information-theoretic Dictionary Learning for Image Classification | http://arxiv.org/pdf/1208.3687v1.pdf | author:Qiang Qiu, Vishal M. Patel, Rama Chellappa category:cs.CV cs.IT math.IT stat.ML published:2012-08-17 summary:We present a two-stage approach for learning dictionaries for objectclassification tasks based on the principle of information maximization. Theproposed method seeks a dictionary that is compact, discriminative, andgenerative. In the first stage, dictionary atoms are selected from an initialdictionary by maximizing the mutual information measure on dictionarycompactness, discrimination and reconstruction. In the second stage, theselected dictionary atoms are updated for improved reconstructive anddiscriminative power using a simple gradient ascent algorithm on mutualinformation. Experiments using real datasets demonstrate the effectiveness ofour approach for image classification tasks.
arxiv-1500-31 | An improvement direction for filter selection techniques using information theory measures and quadratic optimization | http://arxiv.org/pdf/1208.3689v1.pdf | author:Waad Bouaguel, Ghazi Bel Mufti category:cs.LG cs.IT math.IT published:2012-08-17 summary:Filter selection techniques are known for their simplicity and efficiency.However this kind of methods doesn't take into consideration the featuresinter-redundancy. Consequently the un-removed redundant features remain in thefinal classification model, giving lower generalization performance. In thispaper we propose to use a mathematical optimization method that reducesinter-features redundancy and maximize relevance between each feature and thetarget variable.
arxiv-1500-32 | Leveraging Subjective Human Annotation for Clustering Historic Newspaper Articles | http://arxiv.org/pdf/1208.3530v1.pdf | author:Haimonti Dutta, William Chan, Deepak Shankargouda, Manoj Pooleery, Axinia Radeva, Kyle Rego, Boyi Xie, Rebecca Passonneau, Austin Lee, Barbara Taranto category:cs.IR cs.CL cs.DL published:2012-08-17 summary:The New York Public Library is participating in the Chronicling Americainitiative to develop an online searchable database of historically significantnewspaper articles. Microfilm copies of the newspapers are scanned and highresolution Optical Character Recognition (OCR) software is run on them. Thetext from the OCR provides a wealth of data and opinion for researchers andhistorians. However, categorization of articles provided by the OCR engine isrudimentary and a large number of the articles are labeled editorial withoutfurther grouping. Manually sorting articles into fine-grained categories istime consuming if not impossible given the size of the corpus. This paperstudies techniques for automatic categorization of newspaper articles so as toenhance search and retrieval on the archive. We explore unsupervised (e.g.KMeans) and semi-supervised (e.g. constrained clustering) learning algorithmsto develop article categorization schemes geared towards the needs ofend-users. A pilot study was designed to understand whether there was unanimousagreement amongst patrons regarding how articles can be categorized. It wasfound that the task was very subjective and consequently automated algorithmsthat could deal with subjective labels were used. While the small scale pilotstudy was extremely helpful in designing machine learning algorithms, a muchlarger system needs to be developed to collect annotations from users of thearchive. The "BODHI" system currently being developed is a step in thatdirection, allowing users to correct wrongly scanned OCR and providing keywordsand tags for newspaper articles used frequently. On successful implementationof the beta version of this system, we hope that it can be integrated withexisting software being developed for the Chronicling America project.
arxiv-1500-33 | Distance Metric Learning for Kernel Machines | http://arxiv.org/pdf/1208.3422v2.pdf | author:Zhixiang Xu, Kilian Q. Weinberger, Olivier Chapelle category:stat.ML cs.LG published:2012-08-16 summary:Recent work in metric learning has significantly improved thestate-of-the-art in k-nearest neighbor classification. Support vector machines(SVM), particularly with RBF kernels, are amongst the most popularclassification algorithms that uses distance metrics to compare examples. Thispaper provides an empirical analysis of the efficacy of three of the mostpopular Mahalanobis metric learning algorithms as pre-processing for SVMtraining. We show that none of these algorithms generate metrics that lead toparticularly satisfying improvements for SVM-RBF classification. As a remedy weintroduce support vector metric learning (SVML), a novel algorithm thatseamlessly combines the learning of a Mahalanobis metric with the training ofthe RBF-SVM parameters. We demonstrate the capabilities of SVML on ninebenchmark data sets of varying sizes and difficulties. In our study, SVMLoutperforms all alternative state-of-the-art metric learning algorithms interms of accuracy and establishes itself as a serious alternative to thestandard Euclidean metric with model selection by cross validation.
arxiv-1500-34 | Contour Completion Around a Fixation Point | http://arxiv.org/pdf/1208.3512v1.pdf | author:Toshiro Kubota category:cs.CV published:2012-08-16 summary:The paper presents two edge grouping algorithms for finding a closed contourstarting from a particular edge point and enclosing a fixation point. Bothalgorithms search a shortest simple cycle in \textit{an angularly orderedgraph} derived from an edge image where a vertex is an end point of a contourfragment and an undirected arc is drawn between a pair of end-points whosevisual angle from the fixation point is less than a threshold value, which isset to $\pi/2$ in our experiments. The first algorithm restricts the searchspace by disregarding arcs that cross the line extending from the fixationpoint to the starting point. The second algorithm improves the solution of thefirst algorithm in a greedy manner. The algorithms were tested with a largenumber of natural images with manually placed fixation and starting points. Theresults are promising.
arxiv-1500-35 | Automated Marble Plate Classification System Based On Different Neural Network Input Training Sets and PLC Implementation | http://arxiv.org/pdf/1208.6310v1.pdf | author:Irina Topalova category:cs.NE cs.LG published:2012-08-16 summary:The process of sorting marble plates according to their surface texture is animportant task in the automated marble plate production. Nowadays someinspection systems in marble industry that automate the classification tasksare too expensive and are compatible only with specific technological equipmentin the plant. In this paper a new approach to the design of an Automated MarblePlate Classification System (AMPCS),based on different neural network inputtraining sets is proposed, aiming at high classification accuracy using simpleprocessing and application of only standard devices. It is based on training aclassification MLP neural network with three different input training sets:extracted texture histograms, Discrete Cosine and Wavelet Transform over thehistograms. The algorithm is implemented in a PLC for real-time operation. Theperformance of the system is assessed with each one of the input training sets.The experimental test results regarding classification accuracy and quickoperation are represented and discussed.
arxiv-1500-36 | Consistent selection of tuning parameters via variable selection stability | http://arxiv.org/pdf/1208.3380v2.pdf | author:Wei Sun, Junhui Wang, Yixin Fang category:stat.ML stat.ME published:2012-08-16 summary:Penalized regression models are popularly used in high-dimensional dataanalysis to conduct variable selection and model fitting simultaneously.Whereas success has been widely reported in literature, their performanceslargely depend on the tuning parameters that balance the trade-off betweenmodel fitting and model sparsity. Existing tuning criteria mainly follow theroute of minimizing the estimated prediction error or maximizing the posteriormodel probability, such as cross-validation, AIC and BIC. This articleintroduces a general tuning parameter selection criterion based on a novelconcept of variable selection stability. The key idea is to select the tuningparameters so that the resultant penalized regression model is stable invariable selection. The asymptotic selection consistency is established forboth fixed and diverging dimensions. The effectiveness of the proposedcriterion is also demonstrated in a variety of simulated examples as well as anapplication to the prostate cancer data.
arxiv-1500-37 | Efficient Algorithm for Extremely Large Multi-task Regression with Massive Structured Sparsity | http://arxiv.org/pdf/1208.3014v1.pdf | author:Seunghak Lee, Eric P. Xing category:stat.ML q-bio.QM published:2012-08-15 summary:We develop a highly scalable optimization method called "hierarchicalgroup-thresholding" for solving a multi-task regression model with complexstructured sparsity constraints on both input and output spaces. Despite therecent emergence of several efficient optimization algorithms for tacklingcomplex sparsity-inducing regularizers, true scalability in practicalhigh-dimensional problems where a huge amount (e.g., millions) of sparsitypatterns need to be enforced remains an open challenge, because all existingalgorithms must deal with ALL such patterns exhaustively in every iteration,which is computationally prohibitive. Our proposed algorithm addresses thescalability problem by screening out multiple groups of coefficientssimultaneously and systematically. We employ a hierarchical tree representationof group constraints to accelerate the process of removing irrelevantconstraints by taking advantage of the inclusion relationships between groupsparsities, thereby avoiding dealing with all constraints in every optimizationstep, and necessitating optimization operation only on a small number ofoutstanding coefficients. In our experiments, we demonstrate the efficiency ofour method on simulation datasets, and in an application of detecting geneticvariants associated with gene expression traits.
arxiv-1500-38 | More than Word Frequencies: Authorship Attribution via Natural Frequency Zoned Word Distribution Analysis | http://arxiv.org/pdf/1208.3001v1.pdf | author:Zhili Chen, Liusheng Huang, Wei Yang, Peng Meng, Haibo Miao category:cs.CL published:2012-08-15 summary:With such increasing popularity and availability of digital text data,authorships of digital texts can not be taken for granted due to the ease ofcopying and parsing. This paper presents a new text style analysis callednatural frequency zoned word distribution analysis (NFZ-WDA), and then a basicauthorship attribution scheme and an open authorship attribution scheme fordigital texts based on the analysis. NFZ-WDA is based on the observation thatall authors leave distinct intrinsic word usage traces on texts written by themand these intrinsic styles can be identified and employed to analyze theauthorship. The intrinsic word usage styles can be estimated through theanalysis of word distribution within a text, which is more than normal wordfrequency analysis and can be expressed as: which groups of words are used inthe text; how frequently does each group of words occur; how are theoccurrences of each group of words distributed in the text. Next, the basicauthorship attribution scheme and the open authorship attribution schemeprovide solutions for both closed and open authorship attribution problems.Through analysis and extensive experimental studies, this paper demonstratesthe efficiency of the proposed method for authorship attribution.
arxiv-1500-39 | Color Image Compression Algorithm Based on the DCT Blocks | http://arxiv.org/pdf/1208.3133v1.pdf | author:Walaa M. Abd-Elhafiez, Wajeb Gharibi category:cs.CV published:2012-08-15 summary:This paper presents the performance of different blockbased discrete cosinetransform (DCT) algorithms for compressing color image. In this RGB componentof color image are converted to YCbCr before DCT transform is applied. Y isluminance component;Cb and Cr are chrominance components of the image. Themodification of the image data is done based on the classification of imageblocks to edge blocks and non-edge blocks, then the edge block of the image iscompressed with low compression and the nonedge blocks is compressed with highcompression. The analysis results have indicated that the performance of thesuggested method is much better, where the constructed images are lessdistorted and compressed with higher factor.
arxiv-1500-40 | Performance Analysis Of Neuro Genetic Algorithm Applied On Detecting Proportion Of Components In Manhole Gas Mixture | http://arxiv.org/pdf/1209.1048v1.pdf | author:Varun Kumar Ojha, Paramartha Dutta, Hiranmay Saha category:cs.NE cs.CV published:2012-08-15 summary:The article presents performance analysis of a real valued neuro geneticalgorithm applied for the detection of proportion of the gases found in manholegas mixture. The neural network (NN) trained using genetic algorithm (GA) leadsto concept of neuro genetic algorithm, which is used for implementing anintelligent sensory system for the detection of component gases present inmanhole gas mixture Usually a manhole gas mixture contains several toxic gaseslike Hydrogen Sulfide, Ammonia, Methane, Carbon Dioxide, Nitrogen Oxide, andCarbon Monoxide. A semiconductor based gas sensor array used for sensingmanhole gas components is an integral part of the proposed intelligent system.It consists of many sensor elements, where each sensor element is responsiblefor sensing particular gas component. Multiple sensors of different gases usedfor detecting gas mixture of multiple gases, results in cross-sensitivity. Thecross-sensitivity is a major issue and the problem is viewed as patternrecognition problem. The objective of this article is to present performanceanalysis of the real valued neuro genetic algorithm which is applied formultiple gas detection.
arxiv-1500-41 | Asymptotic Generalization Bound of Fisher's Linear Discriminant Analysis | http://arxiv.org/pdf/1208.3030v2.pdf | author:Wei Bian, Dacheng Tao category:stat.ML cs.LG published:2012-08-15 summary:Fisher's linear discriminant analysis (FLDA) is an important dimensionreduction method in statistical pattern recognition. It has been shown thatFLDA is asymptotically Bayes optimal under the homoscedastic Gaussianassumption. However, this classical result has the following two majorlimitations: 1) it holds only for a fixed dimensionality $D$, and thus does notapply when $D$ and the training sample size $N$ are proportionally large; 2) itdoes not provide a quantitative description on how the generalization abilityof FLDA is affected by $D$ and $N$. In this paper, we present an asymptoticgeneralization analysis of FLDA based on random matrix theory, in a settingwhere both $D$ and $N$ increase and $D/N\longrightarrow\gamma\in[0,1)$. Theobtained lower bound of the generalization discrimination power overcomes bothlimitations of the classical result, i.e., it is applicable when $D$ and $N$are proportionally large and provides a quantitative description of thegeneralization ability of FLDA in terms of the ratio $\gamma=D/N$ and thepopulation discrimination power. Besides, the discrimination power bound alsoleads to an upper bound on the generalization error of binary-classificationwith FLDA.
arxiv-1500-42 | Parallelization of Maximum Entropy POS Tagging for Bahasa Indonesia with MapReduce | http://arxiv.org/pdf/1208.3047v1.pdf | author:Arif Nurwidyantoro, Edi Winarko category:cs.DC cs.CL published:2012-08-15 summary:In this paper, MapReduce programming model is used to parallelize trainingand tagging proceess in Maximum Entropy part of speech tagging for BahasaIndonesia. In training process, MapReduce model is implemented dictionary,tagtoken, and feature creation. In tagging process, MapReduce is implemented totag lines of document in parallel. The training experiments showed that totaltraining time using MapReduce is faster, but its result reading time inside theprocess slow down the total training time. The tagging experiments usingdifferent number of map and reduce process showed that MapReduce implementationcould speedup the tagging process. The fastest tagging result is showed bytagging process using 1,000,000 word corpus and 30 map process.
arxiv-1500-43 | A Method for Selecting Noun Sense using Co-occurrence Relation in English-Korean Translation | http://arxiv.org/pdf/1208.2777v1.pdf | author:Hyonil Kim, Changil Choe category:cs.CL published:2012-08-14 summary:The sense analysis is still critical problem in machine translation system,especially such as English-Korean translation which the syntactical differentbetween source and target languages is very great. We suggest a method forselecting the noun sense using contextual feature in English-KoreanTranslation.
arxiv-1500-44 | Analysis of a Statistical Hypothesis Based Learning Mechanism for Faster crawling | http://arxiv.org/pdf/1208.2808v1.pdf | author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.LG cs.IR published:2012-08-14 summary:The growth of world-wide-web (WWW) spreads its wings from an intangiblequantities of web-pages to a gigantic hub of web information which graduallyincreases the complexity of crawling process in a search engine. A searchengine handles a lot of queries from various parts of this world, and theanswers of it solely depend on the knowledge that it gathers by means ofcrawling. The information sharing becomes a most common habit of the society,and it is done by means of publishing structured, semi-structured andunstructured resources on the web. This social practice leads to an exponentialgrowth of web-resource, and hence it became essential to crawl for continuousupdating of web-knowledge and modification of several existing resources in anysituation. In this paper one statistical hypothesis based learning mechanism isincorporated for learning the behavior of crawling speed in differentenvironment of network, and for intelligently control of the speed of crawler.The scaling technique is used to compare the performance proposed method withthe standard crawler. The high speed performance is observed after scaling, andthe retrieval of relevant web-resource in such a high speed is analyzed.
arxiv-1500-45 | Using Program Synthesis for Social Recommendations | http://arxiv.org/pdf/1208.2925v1.pdf | author:Alvin Cheung, Armando Solar-Lezama, Samuel Madden category:cs.LG cs.DB cs.PL cs.SI physics.soc-ph published:2012-08-14 summary:This paper presents a new approach to select events of interest to a user ina social media setting where events are generated by the activities of theuser's friends through their mobile devices. We argue that given the uniquerequirements of the social media setting, the problem is best viewed as aninductive learning problem, where the goal is to first generalize from theusers' expressed "likes" and "dislikes" of specific events, then to produce aprogram that can be manipulated by the system and distributed to the collectiondevices to collect only data of interest. The key contribution of this paper isa new algorithm that combines existing machine learning techniques with newprogram synthesis technology to learn users' preferences. We show that whencompared with the more standard approaches, our new algorithm provides up toorder-of-magnitude reductions in model training time, and significantly higherprediction accuracies for our target application. The approach also improves onstandard machine learning techniques in that it produces clear programs thatcan be manipulated to optimize data collection and filtering.
arxiv-1500-46 | Metric distances derived from cosine similarity and Pearson and Spearman correlations | http://arxiv.org/pdf/1208.3145v1.pdf | author:Stijn van Dongen, Anton J. Enright category:stat.ME cs.LG published:2012-08-14 summary:We investigate two classes of transformations of cosine similarity andPearson and Spearman correlations into metric distances, utilising the simpletool of metric-preserving functions. The first class puts anti-correlatedobjects maximally far apart. Previously known transforms fall within thisclass. The second class collates correlated and anti-correlated objects. Anexample of such a transformation that yields a metric distance is the sinefunction when applied to centered data.
arxiv-1500-47 | Design of Low Noise Amplifiers Using Particle Swarm Optimization | http://arxiv.org/pdf/1208.6028v1.pdf | author:Sadik Ulker category:cs.NE published:2012-08-13 summary:This short paper presents a work on the design of low noise microwaveamplifiers using particle swarm optimization (PSO) technique. Particle SwarmOptimization is used as a method that is applied to a single stage amplifiercircuit to meet two criteria: desired gain and desired low noise. The aim is toget the best optimized design using the predefined constraints for gain and lownoise values. The code is written to apply the algorithm to meet the desiredgoals and the obtained results are verified using different simulators. Theresults obtained show that PSO can be applied very efficiently for this kind ofdesign problems with multiple constraints.
arxiv-1500-48 | Nonparametric sparsity and regularization | http://arxiv.org/pdf/1208.2572v1.pdf | author:Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro verri category:stat.ML cs.LG math.OC published:2012-08-13 summary:In this work we are interested in the problems of supervised learning andvariable selection when the input-output dependence is described by a nonlinearfunction depending on a few variables. Our goal is to consider a sparsenonparametric model, hence avoiding linear or additive models. The key idea isto measure the importance of each variable in the model by making use ofpartial derivatives. Based on this intuition we propose a new notion ofnonparametric sparsity and a corresponding least squares regularization scheme.Using concepts and results from the theory of reproducing kernel Hilbert spacesand proximal methods, we show that the proposed learning algorithm correspondsto a minimization problem which can be provably solved by an iterativeprocedure. The consistency properties of the obtained estimator are studiedboth in terms of prediction and selection performance. An extensive empiricalanalysis shows that the proposed method performs favorably with respect to thestate-of-the-art methods.
arxiv-1500-49 | A Plea for Neutral Comparison Studies in Computational Sciences | http://arxiv.org/pdf/1208.2651v1.pdf | author:Anne-Laure Boulesteix, Manuel J. A. Eugster category:stat.CO cs.CV stat.ME stat.ML published:2012-08-13 summary:In a context where most published articles are devoted to the development of"new methods", comparison studies are generally appreciated by readers butsurprisingly given poor consideration by many scientific journals. Inconnection with recent articles on over-optimism and epistemology published inBioinformatics, this letter stresses the importance of neutral comparisonstudies for the objective evaluation of existing methods and the establishmentof standards by drawing parallels with clinical research.
arxiv-1500-50 | Path Integral Control by Reproducing Kernel Hilbert Space Embedding | http://arxiv.org/pdf/1208.2523v1.pdf | author:Konrad Rawlik, Marc Toussaint, Sethu Vijayakumar category:cs.LG stat.ML published:2012-08-13 summary:We present an embedding of stochastic optimal control problems, of the socalled path integral form, into reproducing kernel Hilbert spaces. Usingconsistent, sample based estimates of the embedding leads to a model free,non-parametric approach for calculation of an approximate solution to thecontrol problem. This formulation admits a decomposition of the problem into aninvariant and task dependent component. Consequently, we make much moreefficient use of the sample data compared to previous sample based approachesin this domain, e.g., by allowing sample re-use across tasks. Numericalexamples on test problems, which illustrate the sample efficiency, areprovided.
arxiv-1500-51 | Stable Segmentation of Digital Image | http://arxiv.org/pdf/1208.2655v1.pdf | author:M. Kharinov category:cs.CV published:2012-08-13 summary:In the paper the optimal image segmentation by means of piecewise constantapproximations is considered. The optimality is defined by a minimum value ofthe total squared error or by equivalent value of standard deviation of theapproximation from the image. The optimal approximations are definedindependently on the method of their obtaining and might be generated indifferent algorithms. We investigate the computation of the optimalapproximation on the grounds of stability with respect to a given set ofmodifications. To obtain the optimal approximation the Mumford-Shuh model isgeneralized and developed, which in the computational part is combined with theOtsu method in multi-thresholding version. The proposed solution is provedanalytically and experimentally on the example of the standard image.
arxiv-1500-52 | Detecting Events and Patterns in Large-Scale User Generated Textual Streams with Statistical Learning Methods | http://arxiv.org/pdf/1208.2873v1.pdf | author:Vasileios Lampos category:cs.LG cs.CL cs.IR cs.SI stat.AP stat.ML published:2012-08-13 summary:A vast amount of textual web streams is influenced by events or phenomenaemerging in the real world. The social web forms an excellent modern paradigm,where unstructured user generated content is published on a regular basis andin most occasions is freely distributed. The present Ph.D. Thesis deals withthe problem of inferring information - or patterns in general - about eventsemerging in real life based on the contents of this textual stream. We showthat it is possible to extract valuable information about social phenomena,such as an epidemic or even rainfall rates, by automatic analysis of thecontent published in Social Media, and in particular Twitter, using StatisticalMachine Learning methods. An important intermediate task regards the formationand identification of features which characterise a target event; we select anduse those textual features in several linear, non-linear and hybrid inferenceapproaches achieving a significantly good performance in terms of the appliedloss function. By examining further this rich data set, we also propose methodsfor extracting various types of mood signals revealing how affective norms - atleast within the social web's population - evolve during the day and howsignificant events emerging in the real world are influencing them. Lastly, wepresent some preliminary findings showing several spatiotemporalcharacteristics of this textual information as well as the potential of usingit to tackle tasks such as the prediction of voting intentions.
arxiv-1500-53 | An Efficient Genetic Programming System with Geometric Semantic Operators and its Application to Human Oral Bioavailability Prediction | http://arxiv.org/pdf/1208.2437v1.pdf | author:Mauro Castelli, Luca Manzoni, Leonardo Vanneschi category:cs.NE published:2012-08-12 summary:Very recently new genetic operators, called geometric semantic operators,have been defined for genetic programming. Contrarily to standard geneticoperators, which are uniquely based on the syntax of the individuals, these newoperators are based on their semantics, meaning with it the set of input-outputpairs on training data. Furthermore, these operators present the interestingproperty of inducing a unimodal fitness landscape for every problem thatconsists in finding a match between given input and output data (for instanceregression and classification). Nevertheless, the current definition of theseoperators has a serious limitation: they impose an exponential growth in thesize of the individuals in the population, so their use is impossible inpractice. This paper is intended to overcome this limitation, presenting a newgenetic programming system that implements geometric semantic operators in anextremely efficient way. To demonstrate the power of the proposed system, weuse it to solve a complex real-life application in the field ofpharmacokinetic: the prediction of the human oral bioavailability of potentialnew drugs. Besides the excellent performances on training data, which wereexpected because the fitness landscape is unimodal, we also report an excellentgeneralization ability of the proposed system, at least for the studiedapplication. In fact, it outperforms standard genetic programming and a wideset of other well-known machine learning methods.
arxiv-1500-54 | How to sample if you must: on optimal functional sampling | http://arxiv.org/pdf/1208.2417v1.pdf | author:Assaf Hallak, Shie Mannor category:stat.ML cs.LG published:2012-08-12 summary:We examine a fundamental problem that models various active sampling setups,such as network tomography. We analyze sampling of a multivariate normaldistribution with an unknown expectation that needs to be estimated: in oursetup it is possible to sample the distribution from a given set of linearfunctionals, and the difficulty addressed is how to optimally select thecombinations to achieve low estimation error. Although this problem is in theheart of the field of optimal design, no efficient solutions for the case withmany functionals exist. We present some bounds and an efficient sub-optimalsolution for this problem for more structured sets such as binary functionalsthat are induced by graph walks.
arxiv-1500-55 | Feasibility of Genetic Algorithm for Textile Defect Classification Using Neural Network | http://arxiv.org/pdf/1208.6025v1.pdf | author:Md. Tarek Habib, Rahat Hossain Faisal, M. Rokonuzzaman category:cs.NE published:2012-08-11 summary:The global market for textile industry is highly competitive nowadays.Quality control in production process in textile industry has been a key factorfor retaining existence in such competitive market. Automated textileinspection systems are very useful in this respect, because manual inspectionis time consuming and not accurate enough. Hence, automated textile inspectionsystems have been drawing plenty of attention of the researchers of differentcountries in order to replace manual inspection. Defect detection and defectclassification are the two major problems that are posed by the research ofautomated textile inspection systems. In this paper, we perform an extensiveinvestigation on the applicability of genetic algorithm (GA) in the context oftextile defect classification using neural network (NN). We observe the effectof tuning different network parameters and explain the reasons. We empiricallyfind a suitable NN model in the context of textile defect classification. Wecompare the performance of this model with that of the classification modelsimplemented by others.
arxiv-1500-56 | A Large Population Size Can Be Unhelpful in Evolutionary Algorithms | http://arxiv.org/pdf/1208.2345v1.pdf | author:Tianshi Chen, Ke Tang, Guoliang Chen, Xin Yao category:cs.NE published:2012-08-11 summary:The utilization of populations is one of the most important features ofevolutionary algorithms (EAs). There have been many studies analyzing theimpact of different population sizes on the performance of EAs. However, mostof such studies are based computational experiments, except for a few cases.The common wisdom so far appears to be that a large population would increasethe population diversity and thus help an EA. Indeed, increasing the populationsize has been a commonly used strategy in tuning an EA when it did not performas well as expected for a given problem. He and Yao (2002) showed theoreticallythat for some problem instance classes, a population can help to reduce theruntime of an EA from exponential to polynomial time. This paper analyzes therole of population further in EAs and shows rigorously that large populationsmay not always be useful. Conditions, under which large populations can beharmful, are discussed in this paper. Although the theoretical analysis wascarried out on one multi-modal problem using a specific type of EAs, it hasmuch wider implications. The analysis has revealed certain problemcharacteristics, which can be either the problem considered here or otherproblems, that lead to the disadvantages of large population sizes. Theanalytical approach developed in this paper can also be applied to analyzingEAs on other problems.
arxiv-1500-57 | Energy Efficient Wireless Communication using Genetic Algorithm Guided Faster Light Weight Digital Signature Algorithm (GADSA) | http://arxiv.org/pdf/1208.2333v1.pdf | author:Arindam Sarkar, J. K. Mandal category:cs.CR cs.NE published:2012-08-11 summary:In this paper GA based light weight faster version of Digital SignatureAlgorithm (GADSA) in wireless communication has been proposed. Various geneticoperators like crossover and mutation are used to optimizing amount of modularmultiplication. Roulette Wheel selection mechanism helps to select bestchromosome which in turn helps in faster computation and minimizes the timerequirements for DSA. Minimization of number of modular multiplication itself aNP-hard problem that means there is no polynomial time deterministic algorithmfor this purpose. This paper deals with this problem using GA basedoptimization algorithm for minimization of the modular multiplication. ProposedGADSA initiates with an initial population comprises of set of valid andcomplete set of individuals. Some operators are used to generate feasible validoffspring from the existing one. Among several exponents the best solutionreached by GADSA is compared with some of the existing techniques. Extensivesimulations shows competitive results for the proposed GADSA.
arxiv-1500-58 | Brain tumor MRI image classification with feature selection and extraction using linear discriminant analysis | http://arxiv.org/pdf/1208.2128v1.pdf | author:V. P. Gladis Pushpa Rathi, S. Palani category:cs.CV cs.LG published:2012-08-10 summary:Feature extraction is a method of capturing visual content of an image. Thefeature extraction is the process to represent raw image in its reduced form tofacilitate decision making such as pattern classification. We have tried toaddress the problem of classification MRI brain images by creating a robust andmore accurate classifier which can act as an expert assistant to medicalpractitioners. The objective of this paper is to present a novel method offeature selection and extraction. This approach combines the Intensity,Texture, shape based features and classifies the tumor as white matter, Graymatter, CSF, abnormal and normal area. The experiment is performed on 140 tumorcontained brain MR images from the Internet Brain Segmentation Repository. Theproposed technique has been carried out over a larger database as compare toany previous work and is more robust and effective. PCA and Linear DiscriminantAnalysis (LDA) were applied on the training sets. The Support Vector Machine(SVM) classifier served as a comparison of nonlinear techniques Vs linear ones.PCA and LDA methods are used to reduce the number of features used. The featureselection using the proposed technique is more beneficial as it analyses thedata according to grouping class variable and gives reduced feature set withhigh classification accuracy.
arxiv-1500-59 | Curved Space Optimization: A Random Search based on General Relativity Theory | http://arxiv.org/pdf/1208.2214v1.pdf | author:Fereydoun Farrahi Moghaddam, Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.NE published:2012-08-10 summary:Designing a fast and efficient optimization method with local optimaavoidance capability on a variety of optimization problems is still an openproblem for many researchers. In this work, the concept of a new globaloptimization method with an open implementation area is introduced as a CurvedSpace Optimization (CSO) method, which is a simple probabilistic optimizationmethod enhanced by concepts of general relativity theory. To address globaloptimization challenges such as performance and convergence, this new method isdesigned based on transformation of a random search space into a new searchspace based on concepts of space-time curvature in general relativity theory.In order to evaluate the performance of our proposed method, an implementationof CSO is deployed and its results are compared on benchmark functions withstate-of-the art optimization methods. The results show that the performance ofCSO is promising on unimodal and multimodal benchmark functions with differentsearch space dimension sizes.
arxiv-1500-60 | A study on non-destructive method for detecting Toxin in pepper using Neural networks | http://arxiv.org/pdf/1208.2092v1.pdf | author:M. Rajalakshmi, P. Subashini category:cs.NE cs.CV published:2012-08-10 summary:Mycotoxin contamination in certain agricultural systems have been a seriousconcern for human and animal health. Mycotoxins are toxic substances producedmostly as secondary metabolites by fungi that grow on seeds and feed in thefield, or in storage. The food-borne Mycotoxins likely to be of greatestsignificance for human health in tropical developing countries are Aflatoxinsand Fumonisins. Chili pepper is also prone to Aflatoxin contamination duringharvesting, production and storage periods.Various methods used for detectionof Mycotoxins give accurate results, but they are slow, expensive anddestructive. Destructive method is testing a material that degrades the sampleunder investigation. Whereas, non-destructive testing will, after testing,allow the part to be used for its intended purpose. Ultrasonic methods,Multispectral image processing methods, Terahertz methods, X-ray andThermography have been very popular in nondestructive testing andcharacterization of materials and health monitoring. Image processing methodsare used to improve the visual quality of the pictures and to extract usefulinformation from them. In this proposed work, the chili pepper samples will becollected, and the X-ray, multispectral images of the samples will be processedusing image processing methods. The term "Computational Intelligence" referredas simulation of human intelligence on computers. It is also called as"Artificial Intelligence" (AI) approach. The techniques used in AI approach areNeural network, Fuzzy logic and evolutionary computation. Finally, thecomputational intelligence method will be used in addition to image processingto provide best, high performance and accurate results for detecting theMycotoxin level in the samples collected.
arxiv-1500-61 | Learning pseudo-Boolean k-DNF and Submodular Functions | http://arxiv.org/pdf/1208.2294v1.pdf | author:Sofya Raskhodnikova, Grigory Yaroslavtsev category:cs.LG cs.DM cs.DS published:2012-08-10 summary:We prove that any submodular function f: {0,1}^n -> {0,1,...,k} can berepresented as a pseudo-Boolean 2k-DNF formula. Pseudo-Boolean DNFs are anatural generalization of DNF representation for functions with integer range.Each term in such a formula has an associated integral constant. We show thatan analog of Hastad's switching lemma holds for pseudo-Boolean k-DNFs if allconstants associated with the terms of the formula are bounded. This allows us to generalize Mansour's PAC-learning algorithm for k-DNFs topseudo-Boolean k-DNFs, and hence gives a PAC-learning algorithm with membershipqueries under the uniform distribution for submodular functions of the formf:{0,1}^n -> {0,1,...,k}. Our algorithm runs in time polynomial in n, k^{O(k\log k / \epsilon)}, 1/\epsilon and log(1/\delta) and works even in theagnostic setting. The line of previous work on learning submodular functions[Balcan, Harvey (STOC '11), Gupta, Hardt, Roth, Ullman (STOC '11), Cheraghchi,Klivans, Kothari, Lee (SODA '12)] implies only n^{O(k)} query complexity forlearning submodular functions in this setting, for fixed epsilon and delta. Our learning algorithm implies a property tester for submodularity offunctions f:{0,1}^n -> {0, ..., k} with query complexity polynomial in n fork=O((\log n/ \loglog n)^{1/2}) and constant proximity parameter \epsilon.
arxiv-1500-62 | Inverse Reinforcement Learning with Gaussian Process | http://arxiv.org/pdf/1208.2112v2.pdf | author:Qifeng Qiao, Peter A. Beling category:cs.LG published:2012-08-10 summary:We present new algorithms for inverse reinforcement learning (IRL, or inverseoptimal control) in convex optimization settings. We argue that finite-spaceIRL can be posed as a convex quadratic program under a Bayesian inferenceframework with the objective of maximum a posterior estimation. To deal withproblems in large or even infinite state space, we propose a Gaussian processmodel and use preference graphs to represent observations of decisiontrajectories. Our method is distinguished from other approaches to IRL in thatit makes no assumptions about the form of the reward function and yet itretains the promise of computationally manageable implementations for potentialreal-world applications. In comparison with an establish algorithm onsmall-scale numerical problems, our method demonstrated better accuracy inapprenticeship learning and a more robust dependence on the number ofobservations.
arxiv-1500-63 | Balancing Lifetime and Classification Accuracy of Wireless Sensor Networks | http://arxiv.org/pdf/1208.2278v1.pdf | author:Kush R. Varshney, Peter M. van de Ven category:cs.NI stat.ML published:2012-08-10 summary:Wireless sensor networks are composed of distributed sensors that can be usedfor signal detection or classification. The likelihood functions of thehypotheses are often not known in advance, and decision rules have to belearned via supervised learning. A specific such algorithm is Fisherdiscriminant analysis (FDA), the classification accuracy of which has beenpreviously studied in the context of wireless sensor networks. Previous work,however, does not take into account the communication protocol or batterylifetime of the sensor networks; in this paper we extend the existing studiesby proposing a model that captures the relationship between battery lifetimeand classification accuracy. In order to do so we combine the FDA with a modelthat captures the dynamics of the Carrier-Sense Multiple-Access (CSMA)algorithm, the random-access algorithm used to regulate communications insensor networks. This allows us to study the interaction between theclassification accuracy, battery lifetime and effort put towards learning, aswell as the impact of the back-off rates of CSMA on the accuracy. Wecharacterize the tradeoff between the length of the training stage andaccuracy, and show that accuracy is non-monotone in the back-off rate due tochanges in the training sample size and overfitting.
arxiv-1500-64 | Metric Learning across Heterogeneous Domains by Respectively Aligning Both Priors and Posteriors | http://arxiv.org/pdf/1208.1829v1.pdf | author:Qiang Qian, Songcan Chen category:cs.LG published:2012-08-09 summary:In this paper, we attempts to learn a single metric across two heterogeneousdomains where source domain is fully labeled and has many samples while targetdomain has only a few labeled samples but abundant unlabeled samples. To thebest of our knowledge, this task is seldom touched. The proposed learning modelhas a simple underlying motivation: all the samples in both the source and thetarget domains are mapped into a common space, where both their priorsP(sample)s and their posteriors P(labelsample)s are forced to be respectivelyaligned as much as possible. We show that the two mappings, from both thesource domain and the target domain to the common space, can be reparameterizedinto a single positive semi-definite(PSD) matrix. Then we develop an efficientBregman Projection algorithm to optimize the PDS matrix over which a LogDetfunction is used to regularize. Furthermore, we also show that this model canbe easily kernelized and verify its effectiveness in crosslanguage retrievaltask and cross-domain object recognition task.
arxiv-1500-65 | Margin Distribution Controlled Boosting | http://arxiv.org/pdf/1208.1846v1.pdf | author:Guangxu Guo, Songcan Chen category:cs.LG published:2012-08-09 summary:Schapire's margin theory provides a theoretical explanation to the success ofboosting-type methods and manifests that a good margin distribution (MD) oftraining samples is essential for generalization. However the statement that aMD is good is vague, consequently, many recently developed algorithms try togenerate a MD in their goodness senses for boosting generalization. Unliketheir indirect control over MD, in this paper, we propose an alternativeboosting algorithm termed Margin distribution Controlled Boosting (MCBoost)which directly controls the MD by introducing and optimizing a key adjustablemargin parameter. MCBoost's optimization implementation adopts the columngeneration technique to ensure fast convergence and small number of weakclassifiers involved in the final MCBooster. We empirically demonstrate: 1)AdaBoost is actually also a MD controlled algorithm and its iteration numberacts as a parameter controlling the distribution and 2) the generalizationperformance of MCBoost evaluated on UCI benchmark datasets is validated betterthan those of AdaBoost, L2Boost, LPBoost, AdaBoost-CG and MDBoost.
arxiv-1500-66 | Scaling Multiple-Source Entity Resolution using Statistically Efficient Transfer Learning | http://arxiv.org/pdf/1208.1860v1.pdf | author:Sahand Negahban, Benjamin I. P. Rubinstein, Jim Gemmell category:cs.DB cs.LG published:2012-08-09 summary:We consider a serious, previously-unexplored challenge facing almost allapproaches to scaling up entity resolution (ER) to multiple data sources: theprohibitive cost of labeling training data for supervised learning ofsimilarity scores for each pair of sources. While there exists a richliterature describing almost all aspects of pairwise ER, this new challenge isarising now due to the unprecedented ability to acquire and store data fromonline sources, features driven by ER such as enriched search verticals, andthe uniqueness of noisy and missing data characteristics for each source. Weshow on real-world and synthetic data that for state-of-the-art techniques, thereality of heterogeneous sources means that the number of labeled training datamust scale quadratically in the number of sources, just to maintain constantprecision/recall. We address this challenge with a brand new transfer learningalgorithm which requires far less training data (or equivalently, achievessuperior accuracy with the same data) and is trained using fast convexoptimization. The intuition behind our approach is to adaptively sharestructure learned about one scoring problem with all other scoring problemssharing a data source in common. We demonstrate that our theoreticallymotivated approach incurs no runtime cost while it can maintain constantprecision/recall with the cost of labeling increasing only linearly with thenumber of sources.
arxiv-1500-67 | Self-Organizing Time Map: An Abstraction of Temporal Multivariate Patterns | http://arxiv.org/pdf/1208.1819v1.pdf | author:Peter Sarlin category:cs.LG cs.DS published:2012-08-09 summary:This paper adopts and adapts Kohonen's standard Self-Organizing Map (SOM) forexploratory temporal structure analysis. The Self-Organizing Time Map (SOTM)implements SOM-type learning to one-dimensional arrays for individual timeunits, preserves the orientation with short-term memory and arranges the arraysin an ascending order of time. The two-dimensional representation of the SOTMattempts thus twofold topology preservation, where the horizontal directionpreserves time topology and the vertical direction data topology. This enablesdiscovering the occurrence and exploring the properties of temporal structuralchanges in data. For representing qualities and properties of SOTMs, we adaptmeasures and visualizations from the standard SOM paradigm, as well asintroduce a measure of temporal structural changes. The functioning of theSOTM, and its visualizations and quality and property measures, are illustratedon artificial toy data. The usefulness of the SOTM in a real-world setting isshown on poverty, welfare and development indicators.
arxiv-1500-68 | Sharp analysis of low-rank kernel matrix approximations | http://arxiv.org/pdf/1208.2015v3.pdf | author:Francis Bach category:cs.LG math.ST stat.TH published:2012-08-09 summary:We consider supervised learning problems within the positive-definite kernelframework, such as kernel ridge regression, kernel logistic regression or thesupport vector machine. With kernels leading to infinite-dimensional featurespaces, a common practical limiting difficulty is the necessity of computingthe kernel matrix, which most frequently leads to algorithms with running timeat least quadratic in the number of observations n, i.e., O(n^2). Low-rankapproximations of the kernel matrix are often considered as they allow thereduction of running time complexities to O(p^2 n), where p is the rank of theapproximation. The practicality of such methods thus depends on the requiredrank p. In this paper, we show that in the context of kernel ridge regression,for approximations based on a random subset of columns of the original kernelmatrix, the rank p may be chosen to be linear in the degrees of freedomassociated with the problem, a quantity which is classically used in thestatistical analysis of such methods, and is often seen as the implicit numberof parameters of non-parametric estimators. This result enables simplealgorithms that have sub-quadratic running time complexity, but provablyexhibit the same predictive performance than existing algorithms, for any givenproblem instance, and not only for worst-case situations.
arxiv-1500-69 | High-Dimensional Screening Using Multiple Grouping of Variables | http://arxiv.org/pdf/1208.2043v3.pdf | author:Divyanshu Vats category:stat.ML cs.IT math.IT published:2012-08-09 summary:Screening is the problem of finding a superset of the set of non-zero entriesin an unknown p-dimensional vector \beta* given n noisy observations.Naturally, we want this superset to be as small as possible. We propose a novelframework for screening, which we refer to as Multiple Grouping (MuG), thatgroups variables, performs variable selection over the groups, and repeats thisprocess multiple number of times to estimate a sequence of sets that containsthe non-zero entries in \beta*. Screening is done by taking an intersection ofall these estimated sets. The MuG framework can be used in conjunction with anygroup based variable selection algorithm. In the high-dimensional setting,where p >> n, we show that when MuG is used with the group Lasso estimator,screening can be consistently performed without using any tuning parameter. Ournumerical simulations clearly show the merits of using the MuG framework inpractice.
arxiv-1500-70 | Stereo Acoustic Perception based on Real Time Video Acquisition for Navigational Assistance | http://arxiv.org/pdf/1208.1880v1.pdf | author:Supreeth K. Rao, Arpitha Prasad B., Anushree R. Shetty, Chinmai, R. Bhakthavathsalam, Rajeshwari Hegde category:cs.CV cs.MM cs.SD published:2012-08-09 summary:A smart navigation system (an Electronic Travel Aid) based on an objectdetection mechanism has been designed to detect the presence of obstacles thatimmediately impede the path, by means of real time video processing. Thealgorithm can be used for any general purpose navigational aid. This paper isdiscussed, keeping in mind the navigation of the visually impaired, and is notlimited to the same. A video camera feeds images of the surroundings to a Da-Vinci Digital Media Processor, DM642, which works on the video, frame by frame.The processor carries out image processing techniques whose result containsinformation about the object in terms of image pixels. The algorithm aims toselect the object which, among all others, poses maximum threat to thenavigation. A database containing a total of three sounds is constructed.Hence, each image translates to a beep, where every beep informs the navigatorof the obstacles directly in front of him. This paper implements an algorithmthat is more efficient as compared to its predecessors.
arxiv-1500-71 | A Survey of Recent View-based 3D Model Retrieval Methods | http://arxiv.org/pdf/1208.3670v1.pdf | author:Qiong Liu category:cs.CV published:2012-08-08 summary:Extensive research efforts have been dedicated to 3D model retrieval inrecent decades. Recently, view-based methods have attracted much researchattention due to the high discriminative property of multi-views for 3D objectrepresentation. In this report, we summarize the view-based 3D model methodsand provide the further research trends. This paper focuses on the scheme formatching between multiple views of 3D models and the application ofbag-of-visual-words method in 3D model retrieval. For matching between multipleviews, the many-to-many matching, probabilistic matching and semisupervisedlearning methods are introduced. For bag-of-visual-words application in 3Dmodel retrieval, we first briefly review the bag-of-visual-words works onmultimedia and computer vision tasks, where the visual dictionary has beendetailed introduced. Then a series of 3D model retrieval methods by usingbag-of-visual-words description are surveyed in this paper. At last, wesummarize the further research content in view-based 3D model retrieval.
arxiv-1500-72 | Performance Measurement and Method Analysis (PMMA) for Fingerprint Reconstruction | http://arxiv.org/pdf/1208.1670v1.pdf | author:Josphineleela Ramakrishnan, Ramakrishnan Malaisamy category:cs.CV published:2012-08-08 summary:Fingerprint reconstruction is one of the most well-known and publicizedbiometrics. Because of their uniqueness and consistency over time, fingerprintshave been used for identification over a century, more recently becomingautomated due to advancements in computed capabilities. Fingerprintreconstruction is popular because of the inherent ease of acquisition, thenumerous sources (e.g. ten fingers) available for collection, and theirestablished use and collections by law enforcement and immigration.Fingerprints have always been the most practical and positive means ofidentification. Offenders, being well aware of this, have been coming up withways to escape identification by that means. Erasing left over fingerprints,using gloves, fingerprint forgery; are certain examples of methods tried bythem, over the years. Failing to prevent themselves, they moved to an extent ofmutilating their finger skin pattern, to remain unidentified. This article isbased upon obliteration of finger ridge patterns and discusses some known casesin relation to the same, in chronological order; highlighting the reasons whyoffenders go to an extent of performing such act. The paper gives an overviewof different methods and performance measurement of the fingerprintreconstruction.
arxiv-1500-73 | An Efficient Automatic Attendance System Using Fingerprint Reconstruction Technique | http://arxiv.org/pdf/1208.1672v1.pdf | author:Josphineleela Ramakrishnan, M. Ramakrishnan category:cs.CV published:2012-08-08 summary:Biometric time and attendance system is one of the most successfulapplications of biometric technology. One of the main advantage of a biometrictime and attendance system is it avoids "buddy-punching". Buddy punching was amajor loophole which will be exploiting in the traditional time attendancesystems. Fingerprint recognition is an established field today, but stillidentifying individual from a set of enrolled fingerprints is a time takingprocess. Most fingerprint-based biometric systems store the minutiae templateof a user in the database. It has been traditionally assumed that the minutiaetemplate of a user does not reveal any information about the originalfingerprint. This belief has now been shown to be false; several algorithmshave been proposed that can reconstruct fingerprint images from minutiaetemplates. In this paper, a novel fingerprint reconstruction algorithm isproposed to reconstruct the phase image, which is then converted into thegrayscale image. The proposed reconstruction algorithm reconstructs the phaseimage from minutiae. The proposed reconstruction algorithm is used to automatethe whole process of taking attendance, manually which is a laborious andtroublesome work and waste a lot of time, with its managing and maintaining therecords for a period of time is also a burdensome task. The proposedreconstruction algorithm has been evaluated with respect to the success ratesof type-I attack (match the reconstructed fingerprint against the originalfingerprint) and type-II attack (match the reconstructed fingerprint againstdifferent impressions of the original fingerprint) using a commercialfingerprint recognition system. Given the reconstructed image from ouralgorithm, we show that both types of attacks can be effectively launchedagainst a fingerprint recognition system.
arxiv-1500-74 | Color Assessment and Transfer for Web Pages | http://arxiv.org/pdf/1208.1679v1.pdf | author:Ou Wu category:cs.HC cs.CV cs.GR H.4.m; H.2.8 published:2012-08-07 summary:Colors play a particularly important role in both designing and accessing Webpages. A well-designed color scheme improves Web pages' visual aesthetic andfacilitates user interactions. As far as we know, existing color assessmentstudies focus on images; studies on color assessment and editing for Web pagesare rare. This paper investigates color assessment for Web pages based onexisting online color theme-rating data sets and applies this assessment to Webcolor edit. This study consists of three parts. First, we study the extractionof a Web page's color theme. Second, we construct color assessment models thatscore the color compatibility of a Web page by leveraging machine learningtechniques. Third, we incorporate the learned color assessment model into a newapplication, namely, color transfer for Web pages. Our study combinestechniques from computer graphics, Web mining, computer vision, and machinelearning. Experimental results suggest that our constructed color assessmentmodels are effective, and useful in the color transfer for Web pages, which hasreceived little attention in both Web mining and computer graphics communities.
arxiv-1500-75 | Data Selection for Semi-Supervised Learning | http://arxiv.org/pdf/1208.1315v1.pdf | author:Shafigh Parsazad, Ehsan Saboori, Amin Allahyar category:cs.LG published:2012-08-07 summary:The real challenge in pattern recognition task and machine learning processis to train a discriminator using labeled data and use it to distinguishbetween future data as accurate as possible. However, most of the problems inthe real world have numerous data, which labeling them is a cumbersome or evenan impossible matter. Semi-supervised learning is one approach to overcomethese types of problems. It uses only a small set of labeled with the companyof huge remain and unlabeled data to train the discriminator. Insemi-supervised learning, it is very essential that which data is labeled anddepend on position of data it effectiveness changes. In this paper, we proposedan evolutionary approach called Artificial Immune System (AIS) to determinewhich data is better to be labeled to get the high quality data. Theexperimental results represent the effectiveness of this algorithm in findingthese data points.
arxiv-1500-76 | Guess Who Rated This Movie: Identifying Users Through Subspace Clustering | http://arxiv.org/pdf/1208.1544v1.pdf | author:Amy Zhang, Nadia Fawaz, Stratis Ioannidis, Andrea Montanari category:cs.LG published:2012-08-07 summary:It is often the case that, within an online recommender system, multipleusers share a common account. Can such shared accounts be identified solely onthe basis of the user- provided ratings? Once a shared account is identified,can the different users sharing it be identified as well? Whenever such useridentification is feasible, it opens the way to possible improvements inpersonalized recommendations, but also raises privacy concerns. We develop amodel for composite accounts based on unions of linear subspaces, and usesubspace clustering for carrying out the identification task. We show that asignificant fraction of such accounts is identifiable in a reliable manner, andillustrate potential uses for personalized recommendation.
arxiv-1500-77 | Fast and Robust Recursive Algorithms for Separable Nonnegative Matrix Factorization | http://arxiv.org/pdf/1208.1237v3.pdf | author:Nicolas Gillis, Stephen A. Vavasis category:stat.ML cs.LG math.OC published:2012-08-06 summary:In this paper, we study the nonnegative matrix factorization problem underthe separability assumption (that is, there exists a cone spanned by a smallsubset of the columns of the input nonnegative data matrix containing allcolumns), which is equivalent to the hyperspectral unmixing problem under thelinear mixing model and the pure-pixel assumption. We present a family of fastrecursive algorithms, and prove they are robust under any small perturbationsof the input data matrix. This family generalizes several existinghyperspectral unmixing algorithms and hence provides for the first time atheoretical justification of their better practical performance.
arxiv-1500-78 | Structured Prediction Cascades | http://arxiv.org/pdf/1208.3279v1.pdf | author:David Weiss, Benjamin Sapp, Ben Taskar category:stat.ML cs.LG published:2012-08-06 summary:Structured prediction tasks pose a fundamental trade-off between the need formodel complexity to increase predictive power and the limited computationalresources for inference in the exponentially-sized output spaces such modelsrequire. We formulate and develop the Structured Prediction Cascadearchitecture: a sequence of increasingly complex models that progressivelyfilter the space of possible outputs. The key principle of our approach is thateach model in the cascade is optimized to accurately filter and refine thestructured output state space of the next model, speeding up both learning andinference in the next layer of the cascade. We learn cascades by optimizing anovel convex loss function that controls the trade-off between the filteringefficiency and the accuracy of the cascade, and provide generalization boundsfor both accuracy and efficiency. We also extend our approach to intractablemodels using tree-decomposition ensembles, and provide algorithms and theoryfor this setting. We evaluate our approach on several large-scale problems,achieving state-of-the-art performance in handwriting recognition and humanpose recognition. We find that structured prediction cascades allow tremendousspeedups and the use of previously intractable features and models in bothsettings.
arxiv-1500-79 | One Permutation Hashing for Efficient Search and Learning | http://arxiv.org/pdf/1208.1259v1.pdf | author:Ping Li, Art Owen, Cun-Hui Zhang category:cs.LG cs.IR cs.IT math.IT stat.CO stat.ML published:2012-08-06 summary:Recently, the method of b-bit minwise hashing has been applied to large-scalelinear learning and sublinear time near-neighbor search. The major drawback ofminwise hashing is the expensive preprocessing cost, as the method requiresapplying (e.g.,) k=200 to 500 permutations on the data. The testing time canalso be expensive if a new data point (e.g., a new document or image) has notbeen processed, which might be a significant issue in user-facing applications. We develop a very simple solution based on one permutation hashing.Conceptually, given a massive binary data matrix, we permute the columns onlyonce and divide the permuted columns evenly into k bins; and we simply store,for each data vector, the smallest nonzero location in each bin. Theinteresting probability analysis (which is validated by experiments) revealsthat our one permutation scheme should perform very similarly to the original(k-permutation) minwise hashing. In fact, the one permutation scheme can beeven slightly more accurate, due to the "sample-without-replacement" effect. Our experiments with training linear SVM and logistic regression on thewebspam dataset demonstrate that this one permutation hashing scheme canachieve the same (or even slightly better) accuracies compared to the originalk-permutation scheme. To test the robustness of our method, we also experimentwith the small news20 dataset which is very sparse and has merely on average500 nonzeros in each data vector. Interestingly, our one permutation schemenoticeably outperforms the k-permutation scheme when k is not too small on thenews20 dataset. In summary, our method can achieve at least the same accuracyas the original k-permutation scheme, at merely 1/k of the originalpreprocessing cost.
arxiv-1500-80 | Sequential Estimation Methods from Inclusion Principle | http://arxiv.org/pdf/1208.1056v1.pdf | author:Xinjia Chen category:math.ST cs.LG math.PR stat.TH published:2012-08-05 summary:In this paper, we propose new sequential estimation methods based oninclusion principle. The main idea is to reformulate the estimation problems asconstructing sequential random intervals and use confidence sequences tocontrol the associated coverage probabilities. In contrast to existingasymptotic sequential methods, our estimation procedures rigorously guaranteethe pre-specified levels of confidence.
arxiv-1500-81 | APRIL: Active Preference-learning based Reinforcement Learning | http://arxiv.org/pdf/1208.0984v1.pdf | author:Riad Akrour, Marc Schoenauer, Michèle Sebag category:cs.LG published:2012-08-05 summary:This paper focuses on reinforcement learning (RL) with limited priorknowledge. In the domain of swarm robotics for instance, the expert can hardlydesign a reward function or demonstrate the target behavior, forbidding the useof both standard RL and inverse reinforcement learning. Although with a limitedexpertise, the human expert is still often able to emit preferences and rankthe agent demonstrations. Earlier work has presented an iterativepreference-based RL framework: expert preferences are exploited to learn anapproximate policy return, thus enabling the agent to achieve direct policysearch. Iteratively, the agent selects a new candidate policy and demonstratesit; the expert ranks the new demonstration comparatively to the previous bestone; the expert's ranking feedback enables the agent to refine the approximatepolicy return, and the process is iterated. In this paper, preference-basedreinforcement learning is combined with active ranking in order to decrease thenumber of ranking queries to the expert needed to yield a satisfactory policy.Experiments on the mountain car and the cancer treatment testbeds witness thata couple of dozen rankings enable to learn a competent policy.
arxiv-1500-82 | Human Activity Learning using Object Affordances from RGB-D Videos | http://arxiv.org/pdf/1208.0967v1.pdf | author:Hema Swetha Koppula, Rudhir Gupta, Ashutosh Saxena category:cs.CV published:2012-08-04 summary:Human activities comprise several sub-activities performed in a sequence andinvolve interactions with various objects. This makes reasoning about theobject affordances a central task for activity recognition. In this work, weconsider the problem of jointly labeling the object affordances and humanactivities from RGB-D videos. We frame the problem as a Markov Random Fieldwhere the nodes represent objects and sub-activities, and the edges representthe relationships between object affordances, their relations withsub-activities, and their evolution over time. We formulate the learningproblem using a structural SVM approach, where labeling over various alternatetemporal segmentations are considered as latent variables. We tested our methodon a dataset comprising 120 activity videos collected from four subjects, andobtained an end-to-end precision of 81.8% and recall of 80.0% for labeling theactivities.
arxiv-1500-83 | Recklessly Approximate Sparse Coding | http://arxiv.org/pdf/1208.0959v2.pdf | author:Misha Denil, Nando de Freitas category:cs.LG cs.CV stat.ML published:2012-08-04 summary:It has recently been observed that certain extremely simple feature encodingtechniques are able to achieve state of the art performance on several standardimage classification benchmarks including deep belief networks, convolutionalnets, factored RBMs, mcRBMs, convolutional RBMs, sparse autoencoders andseveral others. Moreover, these "triangle" or "soft threshold" encodings areex- tremely efficient to compute. Several intuitive arguments have been putforward to explain this remarkable performance, yet no mathematicaljustification has been offered. The main result of this report is to show that these features are realized asan approximate solution to the a non-negative sparse coding problem. Using thisconnection we describe several variants of the soft threshold features anddemonstrate their effectiveness on two image classification benchmark tasks.
arxiv-1500-84 | A Novel Approach of Color Image Hiding using RGB Color planes and DWT | http://arxiv.org/pdf/1208.0803v1.pdf | author:Nilanjan Dey, Anamitra Bardhan Roy, Sayantan Dey category:cs.CR cs.CV published:2012-08-03 summary:This work proposes a wavelet based Steganographic technique for the colorimage. The true color cover image and the true color secret image both aredecomposed into three separate color planes namely R, G and B. Each plane ofthe images is decomposed into four sub bands using DWT. Each color plane of thesecret image is hidden by alpha blending technique in the corresponding subbands of the respective color planes of the original image. During embedding,secret image is dispersed within the original image depending upon the alphavalue. Extraction of the secret image varies according to the alpha value. Inthis approach the stego image generated is of acceptable level ofimperceptibility and distortion compared to the cover image and the overallsecurity is high.
arxiv-1500-85 | Cross-conformal predictors | http://arxiv.org/pdf/1208.0806v1.pdf | author:Vladimir Vovk category:stat.ML cs.LG 62G15 published:2012-08-03 summary:This note introduces the method of cross-conformal prediction, which is ahybrid of the methods of inductive conformal prediction and cross-validation,and studies its validity and predictive efficiency empirically.
arxiv-1500-86 | On the Consistency of AUC Pairwise Optimization | http://arxiv.org/pdf/1208.0645v4.pdf | author:Wei Gao, Zhi-Hua Zhou category:cs.LG stat.ML published:2012-08-03 summary:AUC (area under ROC curve) is an important evaluation criterion, which hasbeen popularly used in many learning tasks such as class-imbalance learning,cost-sensitive learning, learning to rank, etc. Many learning approaches try tooptimize AUC, while owing to the non-convexity and discontinuousness of AUC,almost all approaches work with surrogate loss functions. Thus, the consistencyof AUC is crucial; however, it has been almost untouched before. In this paper,we provide a sufficient condition for the asymptotic consistency of learningapproaches based on surrogate loss functions. Based on this result, we provethat exponential loss and logistic loss are consistent with AUC, but hinge lossis inconsistent. Then, we derive the $q$-norm hinge loss and general hinge lossthat are consistent with AUC. We also derive the consistent bounds forexponential loss and logistic loss, and obtain the consistent bounds for manysurrogate loss functions under the non-noise setting. Further, we disclose anequivalence between the exponential surrogate loss of AUC and exponentialsurrogate loss of accuracy, and one straightforward consequence of such findingis that AdaBoost and RankBoost are equivalent.
arxiv-1500-87 | Learning Theory Approach to Minimum Error Entropy Criterion | http://arxiv.org/pdf/1208.0848v2.pdf | author:Ting Hu, Jun Fan, Qiang Wu, Ding-Xuan Zhou category:cs.LG stat.ML published:2012-08-03 summary:We consider the minimum error entropy (MEE) criterion and an empirical riskminimization learning algorithm in a regression setting. A learning theoryapproach is presented for this MEE algorithm and explicit error bounds areprovided in terms of the approximation ability and capacity of the involvedhypothesis space when the MEE scaling parameter is large. Novel asymptoticanalysis is conducted for the generalization error associated with Renyi'sentropy and a Parzen window function, to overcome technical difficulties arisenfrom the essential differences between the classical least squares problems andthe MEE setting. A semi-norm and the involved symmetrized least squares errorare introduced, which is related to some ranking algorithms.
arxiv-1500-88 | Wisdom of the Crowd: Incorporating Social Influence in Recommendation Models | http://arxiv.org/pdf/1208.0782v2.pdf | author:Shang Shang, Pan Hui, Sanjeev R. Kulkarni, Paul W. Cuff category:cs.IR cs.LG cs.SI physics.soc-ph published:2012-08-03 summary:Recommendation systems have received considerable attention recently.However, most research has been focused on improving the performance ofcollaborative filtering (CF) techniques. Social networks, indispensably,provide us extra information on people's preferences, and should be consideredand deployed to improve the quality of recommendations. In this paper, wepropose two recommendation models, for individuals and for groups respectively,based on social contagion and social influence network theory. In therecommendation model for individuals, we improve the result of collaborativefiltering prediction with social contagion outcome, which simulates the resultof information cascade in the decision-making process. In the recommendationmodel for groups, we apply social influence network theory to takeinterpersonal influence into account to form a settled pattern of disagreement,and then aggregate opinions of group members. By introducing the concept ofsusceptibility and interpersonal influence, the settled rating results areflexible, and inclined to members whose ratings are "essential".
arxiv-1500-89 | Statistical Results on Filtering and Epi-convergence for Learning-Based Model Predictive Control | http://arxiv.org/pdf/1208.0864v1.pdf | author:Anil Aswani, Humberto Gonzalez, S. Shankar Sastry, Claire Tomlin category:math.OC cs.LG cs.SY published:2012-08-03 summary:Learning-based model predictive control (LBMPC) is a technique that providesdeterministic guarantees on robustness, while statistical identification toolsare used to identify richer models of the system in order to improveperformance. This technical note provides proofs that elucidate the reasons forour choice of measurement model, as well as giving proofs concerning thestochastic convergence of LBMPC. The first part of this note discussessimultaneous state estimation and statistical identification (or learning) ofunmodeled dynamics, for dynamical systems that can be described by ordinarydifferential equations (ODE's). The second part provides proofs concerning theepi-convergence of different statistical estimators that can be used with thelearning-based model predictive control (LBMPC) technique. In particular, weprove results on the statistical properties of a nonparametric estimator thatwe have designed to have the correct deterministic and stochastic propertiesfor numerical implementation when used in conjunction with LBMPC.
arxiv-1500-90 | Fast and Accurate Algorithms for Re-Weighted L1-Norm Minimization | http://arxiv.org/pdf/1208.0651v1.pdf | author:M. Salman Asif, Justin Romberg category:stat.CO cs.IT math.IT stat.ML published:2012-08-03 summary:To recover a sparse signal from an underdetermined system, we often solve aconstrained L1-norm minimization problem. In many cases, the signal sparsityand the recovery performance can be further improved by replacing the L1 normwith a "weighted" L1 norm. Without any prior information about nonzero elementsof the signal, the procedure for selecting weights is iterative in nature.Common approaches update the weights at every iteration using the solution of aweighted L1 problem from the previous iteration. In this paper, we present two homotopy-based algorithms that efficientlysolve reweighted L1 problems. First, we present an algorithm that quicklyupdates the solution of a weighted L1 problem as the weights change. Since thesolution changes only slightly with small changes in the weights, we develop ahomotopy algorithm that replaces the old weights with the new ones in a smallnumber of computationally inexpensive steps. Second, we propose an algorithmthat solves a weighted L1 problem by adaptively selecting the weights whileestimating the signal. This algorithm integrates the reweighting into everystep along the homotopy path by changing the weights according to the changesin the solution and its support, allowing us to achieve a high quality signalreconstruction by solving a single homotopy problem. We compare the performanceof both algorithms, in terms of reconstruction accuracy and computationalcomplexity, against state-of-the-art solvers and show that our methods havesmaller computational cost. In addition, we will show that the adaptiveselection of the weights inside the homotopy often yields reconstructions ofhigher quality.
arxiv-1500-91 | A Random Walk Based Model Incorporating Social Information for Recommendations | http://arxiv.org/pdf/1208.0787v2.pdf | author:Shang Shang, Sanjeev R. Kulkarni, Paul W. Cuff, Pan Hui category:cs.IR cs.LG published:2012-08-03 summary:Collaborative filtering (CF) is one of the most popular approaches to build arecommendation system. In this paper, we propose a hybrid collaborativefiltering model based on a Makovian random walk to address the data sparsityand cold start problems in recommendation systems. More precisely, we constructa directed graph whose nodes consist of items and users, together with itemcontent, user profile and social network information. We incorporate user'sratings into edge settings in the graph model. The model provides personalizedrecommendations and predictions to individuals and groups. The proposedalgorithms are evaluated on MovieLens and Epinions datasets. Experimentalresults show that the proposed methods perform well compared with othergraph-based methods, especially in the cold start case.
arxiv-1500-92 | Fast Planar Correlation Clustering for Image Segmentation | http://arxiv.org/pdf/1208.0378v1.pdf | author:Julian Yarkony, Alexander T. Ihler, Charless C. Fowlkes category:cs.CV cs.DS cs.LG stat.ML published:2012-08-02 summary:We describe a new optimization scheme for finding high-quality correlationclusterings in planar graphs that uses weighted perfect matching as asubroutine. Our method provides lower-bounds on the energy of the optimalcorrelation clustering that are typically fast to compute and tight inpractice. We demonstrate our algorithm on the problem of image segmentationwhere this approach outperforms existing global optimization techniques inminimizing the objective and is competitive with the state of the art inproducing high-quality segmentations.
arxiv-1500-93 | Optimization hardness as transient chaos in an analog approach to constraint satisfaction | http://arxiv.org/pdf/1208.0526v1.pdf | author:Maria Ercsey-Ravasz, Zoltan Toroczkai category:cs.CC cs.NE math.DS nlin.CD F.2.3; F.1.0 published:2012-08-02 summary:Boolean satisfiability [1] (k-SAT) is one of the most studied optimizationproblems, as an efficient (that is, polynomial-time) solution to k-SAT (for$k\geq 3$) implies efficient solutions to a large number of hard optimizationproblems [2,3]. Here we propose a mapping of k-SAT into a deterministiccontinuous-time dynamical system with a unique correspondence between itsattractors and the k-SAT solution clusters. We show that beyond a constraintdensity threshold, the analog trajectories become transiently chaotic [4-7],and the boundaries between the basins of attraction [8] of the solutionclusters become fractal [7-9], signaling the appearance of optimizationhardness [10]. Analytical arguments and simulations indicate that the systemalways finds solutions for satisfiable formulae even in the frozen regimes ofrandom 3-SAT [11] and of locked occupation problems [12] (considered among thehardest algorithmic benchmarks); a property partly due to the system'shyperbolic [4,13] character. The system finds solutions in polynomialcontinuous-time, however, at the expense of exponential fluctuations in itsenergy function.
arxiv-1500-94 | Ancestral Inference from Functional Data: Statistical Methods and Numerical Examples | http://arxiv.org/pdf/1208.0628v1.pdf | author:Pantelis Z. Hadjipantelis, Nick S. Jones, John Moriarty, David Springate, Christopher G. Knight category:stat.ML q-bio.PE q-bio.QM published:2012-08-02 summary:Many biological characteristics of evolutionary interest are not scalarvariables but continuous functions. Here we use phylogenetic Gaussian processregression to model the evolution of simulated function-valued traits. Givenfunction-valued data only from the tips of an evolutionary tree and utilisingindependent principal component analysis (IPCA) as a method for dimensionreduction, we construct distributional estimates of ancestral function-valuedtraits, and estimate parameters describing their evolutionary dynamics.
arxiv-1500-95 | Efficient Point-to-Subspace Query in $\ell^1$ with Application to Robust Object Instance Recognition | http://arxiv.org/pdf/1208.0432v3.pdf | author:Ju Sun, Yuqian Zhang, John Wright category:cs.CV cs.LG stat.ML published:2012-08-02 summary:Motivated by vision tasks such as robust face and object recognition, weconsider the following general problem: given a collection of low-dimensionallinear subspaces in a high-dimensional ambient (image) space, and a query point(image), efficiently determine the nearest subspace to the query in $\ell^1$distance. In contrast to the naive exhaustive search which entails large-scalelinear programs, we show that the computational burden can be cut downsignificantly by a simple two-stage algorithm: (1) projecting the query anddata-base subspaces into lower-dimensional space by random Cauchy matrix, andsolving small-scale distance evaluations (linear programs) in the projectionspace to locate candidate nearest; (2) with few candidates upon independentrepetition of (1), getting back to the high-dimensional space and performingexhaustive search. To preserve the identity of the nearest subspace withnontrivial probability, the projection dimension typically is low-orderpolynomial of the subspace dimension multiplied by logarithm of number of thesubspaces (Theorem 2.1). The reduced dimensionality and hence complexityrenders the proposed algorithm particularly relevant to vision application suchas robust face and object instance recognition that we investigate empirically.
arxiv-1500-96 | A hybrid artificial immune system and Self Organising Map for network intrusion detection | http://arxiv.org/pdf/1208.0541v1.pdf | author:Simon T. Powers, Jun He category:cs.NE cs.CR published:2012-08-02 summary:Network intrusion detection is the problem of detecting unauthorised use of,or access to, computer systems over a network. Two broad approaches exist totackle this problem: anomaly detection and misuse detection. An anomalydetection system is trained only on examples of normal connections, and thushas the potential to detect novel attacks. However, many anomaly detectionsystems simply report the anomalous activity, rather than analysing it furtherin order to report higher-level information that is of more use to a securityofficer. On the other hand, misuse detection systems recognise known attackpatterns, thereby allowing them to provide more detailed information about anintrusion. However, such systems cannot detect novel attacks. A hybrid system is presented in this paper with the aim of combining theadvantages of both approaches. Specifically, anomalous network connections areinitially detected using an artificial immune system. Connections that areflagged as anomalous are then categorised using a Kohonen Self Organising Map,allowing higher-level information, in the form of cluster membership, to beextracted. Experimental results on the KDD 1999 Cup dataset show a low falsepositive rate and a detection and classification rate for Denial-of-Service andUser-to-Root attacks that is higher than those in a sample of other works.
arxiv-1500-97 | Multidimensional Membership Mixture Models | http://arxiv.org/pdf/1208.0402v1.pdf | author:Yun Jiang, Marcus Lim, Ashutosh Saxena category:cs.LG stat.ML published:2012-08-02 summary:We present the multidimensional membership mixture (M3) models where everydimension of the membership represents an independent mixture model and eachdata point is generated from the selected mixture components jointly. This ishelpful when the data has a certain shared structure. For example, three uniquemeans and three unique variances can effectively form a Gaussian mixture modelwith nine components, while requiring only six parameters to fully describe it.In this paper, we present three instantiations of M3 models (together with thelearning and inference algorithms): infinite, finite, and hybrid, depending onwhether the number of mixtures is fixed or not. They are built upon Dirichletprocess mixture models, latent Dirichlet allocation, and a combinationrespectively. We then consider two applications: topic modeling and learning 3Dobject arrangements. Our experiments show that our M3 models achieve betterperformance using fewer topics than many classic topic models. We also observethat topics from the different dimensions of M3 models are meaningful andorthogonal to each other.
arxiv-1500-98 | A phase-sensitive method for filtering on the sphere | http://arxiv.org/pdf/1208.0385v2.pdf | author:Ramakrishna Kakarala, Philip Ogunbona category:math.RT cs.CV published:2012-08-02 summary:This paper examines filtering on a sphere, by first examining the roles ofspherical harmonic magnitude and phase. We show that phase is more importantthan magnitude in determining the structure of a spherical function. We examinethe properties of linear phase shifts in the spherical harmonic domain, whichsuggest a mechanism for constructing finite-impulse-response (FIR) filters. Weshow that those filters have desirable properties, such as being associative,mapping spherical functions to spherical functions, allowing directionalfiltering, and being defined by relatively simple equations. We provideexamples of the filters for both spherical and manifold data.
arxiv-1500-99 | The bistable brain: a neuronal model with symbiotic interactions | http://arxiv.org/pdf/1208.0223v1.pdf | author:Ricardo Lopez-Ruiz, Daniele Fournier-Prunaret category:nlin.CD cs.NE math.DS published:2012-08-01 summary:In general, the behavior of large and complex aggregates of elementarycomponents can not be understood nor extrapolated from the properties of a fewcomponents. The brain is a good example of this type of networked systems wheresome patterns of behavior are observed independently of the topology and of thenumber of coupled units. Following this insight, we have studied the dynamicsof different aggregates of logistic maps according to a particular {\itsymbiotic} coupling scheme that imitates the neuronal excitation coupling. Allthese aggregates show some common dynamical properties, concretely a bistablebehavior that is reported here with a certain detail. Thus, the qualitativerelationship with neural systems is suggested through a naive model of many ofsuch networked logistic maps whose behavior mimics the waking-sleepingbistability displayed by brain systems. Due to its relevance, some regions ofmultistability are determined and sketched for all these logistic models.
arxiv-1500-100 | Artificial Neural Network Based Prediction of Optimal Pseudo-Damping and Meta-Damping in Oscillatory Fractional Order Dynamical Systems | http://arxiv.org/pdf/1208.0318v1.pdf | author:Saptarshi Das, Indranil Pan, Khrist Sur, Shantanu Das category:cs.SY cs.NE published:2012-08-01 summary:This paper investigates typical behaviors like damped oscillations infractional order (FO) dynamical systems. Such response occurs due to thepresence of, what is conceived as, pseudo-damping and meta-damping in somespecial class of FO systems. Here, approximation of such damped oscillation inFO systems with the conventional notion of integer order damping and timeconstant has been carried out using Genetic Algorithm (GA). Next, a multilayerfeed-forward Artificial Neural Network (ANN) has been trained using the GAbased results to predict the optimal pseudo and meta-damping from knowledge ofthe maximum order or number of terms in the FO dynamical system.
arxiv-1500-101 | Adaptation of pedagogical resources description standard (LOM) with the specificity of Arabic language | http://arxiv.org/pdf/1208.0200v1.pdf | author:Asma Boudhief, Mohsen Maraoui, Mounir Zrigui category:cs.CL published:2012-08-01 summary:In this article we focus firstly on the principle of pedagogical indexing andcharacteristics of Arabic language and secondly on the possibility of adaptingthe standard for describing learning resources used (the LOM and itsApplication Profiles) with learning conditions such as the educational levelsof students and their levels of understanding,... the educational context withtaking into account the representative elements of text, text length, ... inparticular, we put in relief the specificity of the Arabic language which is acomplex language, characterized by its flexion, its voyellation andagglutination.
arxiv-1500-102 | Oracle inequalities for computationally adaptive model selection | http://arxiv.org/pdf/1208.0129v1.pdf | author:Alekh Agarwal, Peter L. Bartlett, John C. Duchi category:stat.ML cs.LG published:2012-08-01 summary:We analyze general model selection procedures using penalized empirical lossminimization under computational constraints. While classical model selectionapproaches do not consider computational aspects of performing model selection,we argue that any practical model selection procedure must not only trade offestimation and approximation error, but also the computational effort requiredto compute empirical minimizers for different function classes. We provide aframework for analyzing such problems, and we give algorithms for modelselection under a computational budget. These algorithms satisfy oracleinequalities that show that the risk of the selected model is not much worsethan if we had devoted all of our omputational budget to the optimal functionclass.
arxiv-1500-103 | Initial Version of State Transition Algorithm | http://arxiv.org/pdf/1208.0228v2.pdf | author:Xiaojun Zhou, Chunhua Yang, Weihua Gui category:math.OC cs.NE published:2012-08-01 summary:In terms of the concepts of state and state transition, a new algorithm-StateTransition Algorithm (STA) is proposed in order to probe into classical andintelligent optimization algorithms. On the basis of state and statetransition, it becomes much simpler and easier to understand. As for continuousfunction optimization problems, three special operators named rotation,translation and expansion are presented. While for discrete functionoptimization problems, an operator called general elementary transformation isintroduced. Finally, with 4 common benchmark continuous functions and adiscrete problem used to test the performance of STA, the experiment shows thatSTA is a promising algorithm due to its good search capability.
arxiv-1500-104 | Learning a peptide-protein binding affinity predictor with kernel ridge regression | http://arxiv.org/pdf/1207.7253v1.pdf | author:Sébastien Giguère, Mario Marchand, François Laviolette, Alexandre Drouin, Jacques Corbeil category:q-bio.QM cs.LG q-bio.BM stat.ML 92B05 published:2012-07-31 summary:We propose a specialized string kernel for small bio-molecules, peptides andpseudo-sequences of binding interfaces. The kernel incorporatesphysico-chemical properties of amino acids and elegantly generalize eightkernels, such as the Oligo, the Weighted Degree, the Blended Spectrum, and theRadial Basis Function. We provide a low complexity dynamic programmingalgorithm for the exact computation of the kernel and a linear time algorithmfor it's approximation. Combined with kernel ridge regression and SupCK, anovel binding pocket kernel, the proposed kernel yields biologically relevantand good prediction accuracy on the PepX database. For the first time, amachine learning predictor is capable of accurately predicting the bindingaffinity of any peptide to any protein. The method was also applied to bothsingle-target and pan-specific Major Histocompatibility Complex class IIbenchmark datasets and three Quantitative Structure Affinity Model benchmarkdatasets. On all benchmarks, our method significantly (p-value < 0.057) outperforms thecurrent state-of-the-art methods at predicting peptide-protein bindingaffinities. The proposed approach is flexible and can be applied to predict anyquantitative biological activity. The method should be of value to a largesegment of the research community with the potential to acceleratepeptide-based drug and vaccine development.
arxiv-1500-105 | Predicate Generation for Learning-Based Quantifier-Free Loop Invariant Inference | http://arxiv.org/pdf/1207.7167v2.pdf | author:Wonchan Lee, Yungbum Jung, Bow-yaw Wang, Kwangkuen Yi category:cs.LO cs.LG F.3.1 published:2012-07-31 summary:We address the predicate generation problem in the context of loop invariantinference. Motivated by the interpolation-based abstraction refinementtechnique, we apply the interpolation theorem to synthesize predicatesimplicitly implied by program texts. Our technique is able to improve theeffectiveness and efficiency of the learning-based loop invariant inferencealgorithm in [14]. We report experiment results of examples from Linux,SPEC2000, and Tar utility.
arxiv-1500-106 | Gaussian process regression as a predictive model for Quality-of-Service in Web service systems | http://arxiv.org/pdf/1207.6910v2.pdf | author:Jakub M. Tomczak, Jerzy Swiatek, Krzysztof Latawiec category:cs.NI cs.LG published:2012-07-30 summary:In this paper, we present the Gaussian process regression as the predictivemodel for Quality-of-Service (QoS) attributes in Web service systems. The goalis to predict performance of the execution system expressed as QoS attributesgiven existing execution system, service repository, and inputs, e.g., streamsof requests. In order to evaluate the performance of Gaussian processregression the simulation environment was developed. Two quality indexes wereused, namely, Mean Absolute Error and Mean Squared Error. The results obtainedwithin the experiment show that the Gaussian process performed the best withlinear kernel and statistically significantly better comparing toClassification and Regression Trees (CART) method.
arxiv-1500-107 | A Survey Of Activity Recognition And Understanding The Behavior In Video Survelliance | http://arxiv.org/pdf/1207.6774v1.pdf | author:A. R. Revathi, Dhananjay Kumar category:cs.CV published:2012-07-29 summary:This paper presents a review of human activity recognition and behaviourunderstanding in video sequence. The key objective of this paper is to providea general review on the overall process of a surveillance system used in thecurrent trend. Visual surveillance system is directed on automaticidentification of events of interest, especially on tracking and classificationof moving objects. The processing step of the video surveillance systemincludes the following stages: Surrounding model, object representation, objecttracking, activity recognition and behaviour understanding. It describestechniques that use to define a general set of activities that are applicableto a wide range of scenes and environments in video sequence.
arxiv-1500-108 | Universally Consistent Latent Position Estimation and Vertex Classification for Random Dot Product Graphs | http://arxiv.org/pdf/1207.6745v1.pdf | author:Daniel L. Sussman, Minh Tang, Carey E. Priebe category:stat.ML math.ST stat.TH published:2012-07-29 summary:In this work we show that, using the eigen-decomposition of the adjacencymatrix, we can consistently estimate latent positions for random dot productgraphs provided the latent positions are i.i.d. from some distribution. Ifclass labels are observed for a number of vertices tending to infinity, then weshow that the remaining vertices can be classified with error converging toBayes optimal using the $k$-nearest-neighbors classification rule. We evaluatethe proposed methods on simulated data and a graph derived from Wikipedia.
arxiv-1500-109 | Exploring Promising Stepping Stones by Combining Novelty Search with Interactive Evolution | http://arxiv.org/pdf/1207.6682v1.pdf | author:Brian G. Woolley, Kenneth O. Stanley category:cs.NE I.2.6 published:2012-07-28 summary:The field of evolutionary computation is inspired by the achievements ofnatural evolution, in which there is no final objective. Yet the pursuit ofobjectives is ubiquitous in simulated evolution. A significant problem is thatobjective approaches assume that intermediate stepping stones will increasinglyresemble the final objective when in fact they often do not. The consequence isthat while solutions may exist, searching for such objectives may not discoverthem. This paper highlights the importance of leveraging human insight duringsearch as an alternative to articulating explicit objectives. In particular, anew approach called novelty-assisted interactive evolutionary computation(NA-IEC) combines human intuition with novelty search for the first time tofacilitate the serendipitous discovery of agent behaviors. In this approach,the human user directs evolution by selecting what is interesting from theon-screen population of behaviors. However, unlike in typical IEC, the user cannow request that the next generation be filled with novel descendants. Theexperimental results demonstrate that combining human insight with noveltysearch finds solutions significantly faster and at lower genomic complexitiesthan fully-automated processes, including pure novelty search, suggesting animportant role for human users in the search for solutions.
arxiv-1500-110 | Group Iterative Spectrum Thresholding for Super-Resolution Sparse Spectral Selection | http://arxiv.org/pdf/1207.6684v2.pdf | author:Yiyuan She, Huanghuang Li, Jiangping Wang, Dapeng Wu category:stat.ML published:2012-07-28 summary:Recently, sparsity-based algorithms are proposed for super-resolutionspectrum estimation. However, to achieve adequately high resolution inreal-world signal analysis, the dictionary atoms have to be close to each otherin frequency, thereby resulting in a coherent design. The popular convexcompressed sensing methods break down in presence of high coherence and largenoise. We propose a new regularization approach to handle model collinearityand obtain parsimonious frequency selection simultaneously. It takes advantageof the pairing structure of sine and cosine atoms in the frequency dictionary.A probabilistic spectrum screening is also developed for fast computation inhigh dimensions. A data-resampling version of high-dimensional BayesianInformation Criterion is used to determine the regularization parameters.Experiments show the efficacy and efficiency of the proposed algorithms inchallenging situations with small sample size, high frequency resolution, andlow signal-to-noise ratio.
arxiv-1500-111 | Detection of Deviations in Mobile Applications Network Behavior | http://arxiv.org/pdf/1208.0564v2.pdf | author:L. Chekina, D. Mimran, L. Rokach, Y. Elovici, B. Shapira category:cs.CR cs.LG published:2012-07-27 summary:In this paper a novel system for detecting meaningful deviations in a mobileapplication's network behavior is proposed. The main goal of the proposedsystem is to protect mobile device users and cellular infrastructure companiesfrom malicious applications. The new system is capable of: (1) identifyingmalicious attacks or masquerading applications installed on a mobile device,and (2) identifying republishing of popular applications injected with amalicious code. The detection is performed based on the application's networktraffic patterns only. For each application two types of models are learned.The first model, local, represents the personal traffic pattern for each userusing an application and is learned on the device. The second model,collaborative, represents traffic patterns of numerous users using anapplication and is learned on the system server. Machine-learning methods areused for learning and detection purposes. This paper focuses on methodsutilized for local (i.e., on mobile device) learning and detection ofdeviations from the normal application's behavior. These methods wereimplemented and evaluated on Android devices. The evaluation experimentsdemonstrate that: (1) various applications have specific network trafficpatterns and certain application categories can be distinguishable by theirnetwork patterns, (2) different levels of deviations from normal behavior canbe detected accurately, and (3) local learning is feasible and has a lowperformance overhead on mobile devices.
arxiv-1500-112 | Measuring the Complexity of Ultra-Large-Scale Adaptive Systems | http://arxiv.org/pdf/1207.6656v2.pdf | author:Michele Amoretti, Carlos Gershenson category:cs.NE cs.NI nlin.AO published:2012-07-27 summary:Ultra-large scale (ULS) systems are becoming pervasive. They are inherentlycomplex, which makes their design and control a challenge for traditionalmethods. Here we propose the design and analysis of ULS systems using measuresof complexity, emergence, self-organization, and homeostasis based oninformation theory. These measures allow the evaluation of ULS systems and thuscan be used to guide their design. We evaluate the proposal with a ULScomputing system provided with adaptation mechanisms. We show the evolution ofthe system with stable and also changing workload, using different fitnessfunctions. When the adaptive plan forces the system to converge to a predefinedperformance level, the nodes may result in highly unstable configurations, thatcorrespond to a high variance in time of the measured complexity. Conversely,if the adaptive plan is less "aggressive", the system may be more stable, butthe optimal performance may not be achieved.
arxiv-1500-113 | Supervised Laplacian Eigenmaps with Applications in Clinical Diagnostics for Pediatric Cardiology | http://arxiv.org/pdf/1207.7035v1.pdf | author:Thomas Perry, Hongyuan Zha, Patricio Frias, Dadan Zeng, Mark Braunstein category:cs.LG published:2012-07-27 summary:Electronic health records contain rich textual data which possess criticalpredictive information for machine-learning based diagnostic aids. However manytraditional machine learning methods fail to simultaneously integrate bothvector space data and text. We present a supervised method using Laplacianeigenmaps to augment existing machine-learning methods with low-dimensionalrepresentations of textual predictors which preserve the local similarities.The proposed implementation performs alternating optimization using gradientdescent. For the evaluation we applied our method to over 2,000 patient recordsfrom a large single-center pediatric cardiology practice to predict if patientswere diagnosed with cardiac disease. Our method was compared with latentsemantic indexing, latent Dirichlet allocation, and local Fisher discriminantanalysis. The results were assessed using AUC, MCC, specificity, andsensitivity. Results indicate supervised Laplacian eigenmaps was the highestperforming method in our study, achieving 0.782 and 0.374 for AUC and MCCrespectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over thebaseline which excluded textual data and a 2.69% and 5.35% increase in AUC andMCC respectively over unsupervised Laplacian eigenmaps. This method allows manyexisting machine learning predictors to effectively and efficiently utilize thepotential of textual predictors.
arxiv-1500-114 | Optimal Data Collection For Informative Rankings Expose Well-Connected Graphs | http://arxiv.org/pdf/1207.6430v2.pdf | author:Braxton Osting, Christoph Brune, Stanley J. Osher category:stat.ML cs.LG stat.AP published:2012-07-26 summary:Given a graph where vertices represent alternatives and arcs representpairwise comparison data, the statistical ranking problem is to find apotential function, defined on the vertices, such that the gradient of thepotential function agrees with the pairwise comparisons. Our goal in this paperis to develop a method for collecting data for which the least squaresestimator for the ranking problem has maximal Fisher information. Our approach,based on experimental design, is to view data collection as a bi-leveloptimization problem where the inner problem is the ranking problem and theouter problem is to identify data which maximizes the informativeness of theranking. Under certain assumptions, the data collection problem decouples,reducing to a problem of finding multigraphs with large algebraic connectivity.This reduction of the data collection problem to graph-theoretic questions isone of the primary contributions of this work. As an application, we study theYahoo! Movie user rating dataset and demonstrate that the addition of a smallnumber of well-chosen pairwise comparisons can significantly increase theFisher informativeness of the ranking. As another application, we study the2011-12 NCAA football schedule and propose schedules with the same number ofgames which are significantly more informative. Using spectral clusteringmethods to identify highly-connected communities within the division, we arguethat the NCAA could improve its notoriously poor rankings by simply schedulingmore out-of-conference games.
arxiv-1500-115 | On When and How to use SAT to Mine Frequent Itemsets | http://arxiv.org/pdf/1207.6253v1.pdf | author:Rui Henriques, Inês Lynce, Vasco Manquinho category:cs.AI cs.DB cs.LG published:2012-07-26 summary:A new stream of research was born in the last decade with the goal of miningitemsets of interest using Constraint Programming (CP). This has promoted anatural way to combine complex constraints in a highly flexible manner.Although CP state-of-the-art solutions formulate the task using Booleanvariables, the few attempts to adopt propositional Satisfiability (SAT)provided an unsatisfactory performance. This work deepens the study on when andhow to use SAT for the frequent itemset mining (FIM) problem by definingdifferent encodings with multiple task-driven enumeration options and searchstrategies. Although for the majority of the scenarios SAT-based solutionsappear to be non-competitive with CP peers, results show a variety ofinteresting cases where SAT encodings are the best option.
arxiv-1500-116 | A novel Hopfield neural network approach for minimizing total weighted tardiness of jobs scheduled on identical machines | http://arxiv.org/pdf/1208.4583v1.pdf | author:N. Fogarasi, K. Tornai, J. Levendovszky category:cs.NE 90C27 G.1.6 published:2012-07-26 summary:This paper explores fast, polynomial time heuristic approximate solutions tothe NP-hard problem of scheduling jobs on N identical machines. The jobs areindependent and are allowed to be stopped and restarted on another machine at alater time. They have well-de?ned deadlines, and relative priorities quantifiedby non-negative real weights. The objective is to find schedules which minimizethe total weighted tardiness (TWT) of all jobs. We show how this problem can bemapped into quadratic form and present a polynomial time heuristic solutionbased on the Hop?eld Neural Network (HNN) approach. It is demonstrated, throughthe results of extensive numerical simulations, that this solution outperformsother popular heuristic methods. The proposed heuristic is both theoreticallyand empirically shown to be scalable to large problem sizes (over 100 jobs tobe scheduled), which makes it applicable to grid computing scheduling, arisingin fields such as computational biology, chemistry and finance.
arxiv-1500-117 | Identifying Users From Their Rating Patterns | http://arxiv.org/pdf/1207.6379v1.pdf | author:José Bento, Nadia Fawaz, Andrea Montanari, Stratis Ioannidis category:cs.IR cs.LG stat.ML published:2012-07-26 summary:This paper reports on our analysis of the 2011 CAMRa Challenge dataset (Track2) for context-aware movie recommendation systems. The train dataset comprises4,536,891 ratings provided by 171,670 users on 23,974$ movies, as well as thehousehold groupings of a subset of the users. The test dataset comprises 5,450ratings for which the user label is missing, but the household label isprovided. The challenge required to identify the user labels for the ratings inthe test set. Our main finding is that temporal information (time labels of theratings) is significantly more useful for achieving this objective than theuser preferences (the actual ratings). Using a model that leverages on thisfact, we are able to identify users within a known household with an accuracyof approximately 96% (i.e. misclassification rate around 4%).
arxiv-1500-118 | Touchalytics: On the Applicability of Touchscreen Input as a Behavioral Biometric for Continuous Authentication | http://arxiv.org/pdf/1207.6231v2.pdf | author:Mario Frank, Ralf Biedert, Eugene Ma, Ivan Martinovic, Dawn Song category:cs.CR cs.LG published:2012-07-26 summary:We investigate whether a classifier can continuously authenticate users basedon the way they interact with the touchscreen of a smart phone. We propose aset of 30 behavioral touch features that can be extracted from raw touchscreenlogs and demonstrate that different users populate distinct subspaces of thisfeature space. In a systematic experiment designed to test how this behavioralpattern exhibits consistency over time, we collected touch data from usersinteracting with a smart phone using basic navigation maneuvers, i.e., up-downand left-right scrolling. We propose a classification framework that learns thetouch behavior of a user during an enrollment phase and is able to accept orreject the current user by monitoring interaction with the touch screen. Theclassifier achieves a median equal error rate of 0% for intra-sessionauthentication, 2%-3% for inter-session authentication and below 4% when theauthentication test was carried out one week after the enrollment phase. Whileour experimental findings disqualify this method as a standalone authenticationmechanism for long-term authentication, it could be implemented as a means toextend screen-lock time or as a part of a multi-modal biometric authenticationsystem.
arxiv-1500-119 | Determinantal point processes for machine learning | http://arxiv.org/pdf/1207.6083v4.pdf | author:Alex Kulesza, Ben Taskar category:stat.ML cs.IR cs.LG published:2012-07-25 summary:Determinantal point processes (DPPs) are elegant probabilistic models ofrepulsion that arise in quantum physics and random matrix theory. In contrastto traditional structured models like Markov random fields, which becomeintractable and hard to approximate in the presence of negative correlations,DPPs offer efficient and exact algorithms for sampling, marginalization,conditioning, and other inference tasks. We provide a gentle introduction toDPPs, focusing on the intuitions, algorithms, and extensions that are mostrelevant to the machine learning community, and show how DPPs can be applied toreal-world applications like finding diverse sets of high-quality searchresults, building informative summaries by selecting diverse sentences fromdocuments, modeling non-overlapping human poses in images or video, andautomatically building timelines of important news stories.
arxiv-1500-120 | The expected performance of stellar parametrization with Gaia spectrophotometry | http://arxiv.org/pdf/1207.6005v2.pdf | author:C. Liu, C. A. L. Bailer-Jones, R. Sordo, A. Vallenari, R. Borrachero, X. Luri, P. Sartoretti category:astro-ph.IM astro-ph.GA stat.ML published:2012-07-25 summary:Gaia will obtain astrometry and spectrophotometry for essentially all sourcesin the sky down to a broad band magnitude limit of G=20, an expected yield of10^9 stars. Its main scientific objective is to reveal the formation andevolution of our Galaxy through chemo-dynamical analysis. In addition toinferring positions, parallaxes and proper motions from the astrometry, we mustalso infer the astrophysical parameters of the stars from thespectrophotometry, the BP/RP spectrum. Here we investigate the performance ofthree different algorithms (SVM, ILIUM, Aeneas) for estimating the effectivetemperature, line-of-sight interstellar extinction, metallicity and surfacegravity of A-M stars over a wide range of these parameters and over the fullmagnitude range Gaia will observe (G=6-20mag). One of the algorithms, Aeneas,infers the posterior probability density function over all parameters, and canoptionally take into account the parallax and the Hertzsprung-Russell diagramto improve the estimates. For all algorithms the accuracy of estimation dependson G and on the value of the parameters themselves, so a broad summary ofperformance is only approximate. For stars at G=15 with less than twomagnitudes extinction, we expect to be able to estimate Teff to within 1%, loggto 0.1-0.2dex, and [Fe/H] (for FGKM stars) to 0.1-0.2dex, just using the BP/RPspectrum (mean absolute error statistics are quoted). Performance degrades atlarger extinctions, but not always by a large amount. Extinction can beestimated to an accuracy of 0.05-0.2mag for stars across the full parameterrange with a priori unknown extinction between 0 and 10mag. Performancedegrades at fainter magnitudes, but even at G=19 we can estimate logg to betterthan 0.2dex for all spectral types, and [Fe/H] to within 0.35dex for FGKMstars, for extinctions below 1mag.
arxiv-1500-121 | Optimal Sampling Points in Reproducing Kernel Hilbert Spaces | http://arxiv.org/pdf/1207.5871v1.pdf | author:Rui Wang, Haizhang Zhang category:cs.IT math.IT stat.ML published:2012-07-25 summary:The recent developments of basis pursuit and compressed sensing seek toextract information from as few samples as possible. In such applications,since the number of samples is restricted, one should deploy the samplingpoints wisely. We are motivated to study the optimal distribution of finitesampling points. Formulation under the framework of optimal reconstructionyields a minimization problem. In the discrete case, we estimate the distancebetween the optimal subspace resulting from a general Karhunen-Loeve transformand the kernel space to obtain another algorithm that is computationallyfavorable. Numerical experiments are then presented to illustrate theperformance of the algorithms for the searching of optimal sampling points.
arxiv-1500-122 | Equivalence of distance-based and RKHS-based statistics in hypothesis testing | http://arxiv.org/pdf/1207.6076v3.pdf | author:Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu category:stat.ME cs.LG math.ST stat.ML stat.TH published:2012-07-25 summary:We provide a unifying framework linking two classes of statistics used intwo-sample and independence testing: on the one hand, the energy distances anddistance covariances from the statistics literature; on the other, maximum meandiscrepancies (MMD), that is, distances between embeddings of distributions toreproducing kernel Hilbert spaces (RKHS), as established in machine learning.In the case where the energy distance is computed with a semimetric of negativetype, a positive definite kernel, termed distance kernel, may be defined suchthat the MMD corresponds exactly to the energy distance. Conversely, for anypositive definite kernel, we can interpret the MMD as energy distance withrespect to some negative-type semimetric. This equivalence readily extends todistance covariance using kernels on the product space. We determine the classof probability distributions for which the test statistics are consistentagainst all alternatives. Finally, we investigate the performance of the familyof distance kernels in two-sample and independence tests: we show in particularthat the energy distance most commonly employed in statistics is just onemember of a parametric family of kernels, and that other choices from thisfamily can yield more powerful tests.
arxiv-1500-123 | VOI-aware MCTS | http://arxiv.org/pdf/1207.5589v1.pdf | author:David Tolpin, Solomon Eyal Shimony category:cs.AI cs.LG published:2012-07-24 summary:UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in gamesand Markov decision processes, is based on UCB1, a sampling policy for theMulti-armed Bandit problem (MAB) that minimizes the cumulative regret. However,search differs from MAB in that in MCTS it is usually only the final "arm pull"(the actual move selection) that collects a reward, rather than all "armpulls". In this paper, an MCTS sampling policy based on Value of Information(VOI) estimates of rollouts is suggested. Empirical evaluation of the policyand comparison to UCB1 and UCT is performed on random MAB instances as well ason Computer Go.
arxiv-1500-124 | FST Based Morphological Analyzer for Hindi Language | http://arxiv.org/pdf/1207.5409v1.pdf | author:Deepak Kumar, Manjeet Singh, Seema Shukla category:cs.CL cs.IR published:2012-07-23 summary:Hindi being a highly inflectional language, FST (Finite State Transducer)based approach is most efficient for developing a morphological analyzer forthis language. The work presented in this paper uses the SFST (Stuttgart FiniteState Transducer) tool for generating the FST. A lexicon of root words iscreated. Rules are then added for generating inflectional and derivationalwords from these root words. The Morph Analyzer developed was used in a Part OfSpeech (POS) Tagger based on Stanford POS Tagger. The system was first trainedusing a manually tagged corpus and MAXENT (Maximum Entropy) approach ofStanford POS tagger was then used for tagging input sentences. Themorphological analyzer gives approximately 97% correct results. POS taggergives an accuracy of approximately 87% for the sentences that have the wordsknown to the trained model file, and 80% accuracy for the sentences that havethe words unknown to the trained model file.
arxiv-1500-125 | Bellman Error Based Feature Generation using Random Projections on Sparse Spaces | http://arxiv.org/pdf/1207.5554v3.pdf | author:Mahdi Milani Fard, Yuri Grinberg, Amir-massoud Farahmand, Joelle Pineau, Doina Precup category:cs.LG stat.ML published:2012-07-23 summary:We address the problem of automatic generation of features for value functionapproximation. Bellman Error Basis Functions (BEBFs) have been shown to improvethe error of policy evaluation with function approximation, with a convergencerate similar to that of value iteration. We propose a simple, fast and robustalgorithm based on random projections to generate BEBFs for sparse featurespaces. We provide a finite sample analysis of the proposed method, and provethat projections logarithmic in the dimension of the original space are enoughto guarantee contraction in the error. Empirical results demonstrate thestrength of this method.
arxiv-1500-126 | Towards a theory of statistical tree-shape analysis | http://arxiv.org/pdf/1207.5371v1.pdf | author:Aasa Feragen, Pechin Lo, Marleen de Bruijne, Mads Nielsen, Francois Lauze category:stat.ME cs.CV math.MG published:2012-07-23 summary:In order to develop statistical methods for shapes with a tree-structure, weconstruct a shape space framework for tree-like shapes and study metrics on theshape space. This shape space has singularities, corresponding to topologicaltransitions in the represented trees. We study two closely related metrics onthe shape space, TED and QED. QED is a quotient Euclidean distance arisingnaturally from the shape space formulation, while TED is the classical treeedit distance. Using Gromov's metric geometry we gain new insight into thegeometries defined by TED and QED. We show that the new metric QED has nicegeometric properties which facilitate statistical analysis, such as existenceand local uniqueness of geodesics and averages. TED, on the other hand, doesnot share the geometric advantages of QED, but has nice algorithmic properties.We provide a theoretical framework and experimental results on synthetic datatrees as well as airway trees from pulmonary CT scans. This way, we effectivelyillustrate that our framework has both the theoretical and qualitativeproperties necessary to build a theory of statistical tree-shape analysis.
arxiv-1500-127 | A Robust Signal Classification Scheme for Cognitive Radio | http://arxiv.org/pdf/1207.5342v1.pdf | author:Hanwen Cao, Jürgen Peissig category:cs.IT cs.LG cs.NI math.IT C.2.1 published:2012-07-23 summary:This paper presents a robust signal classification scheme for achievingcomprehensive spectrum sensing of multiple coexisting wireless systems. It isbuilt upon a group of feature-based signal detection algorithms enhanced by theproposed dimension cancelation (DIC) method for mitigating the noiseuncertainty problem. The classification scheme is implemented on our testbedconsisting real-world wireless devices. The simulation and experimentalperformances agree with each other well and shows the e?ectiveness androbustness of the proposed scheme.
arxiv-1500-128 | Guarantees of Augmented Trace Norm Models in Tensor Recovery | http://arxiv.org/pdf/1207.5326v1.pdf | author:Ziqiang Shi, Jiqing Han, Tieran Zheng, Shiwen Deng, Ji Li category:cs.IT cs.CV math.IT published:2012-07-23 summary:This paper studies the recovery guarantees of the models of minimizing$\\mathcal{X}\_*+\frac{1}{2\alpha}\\mathcal{X}\_F^2$ where $\mathcal{X}$ isa tensor and $\\mathcal{X}\_*$ and $\\mathcal{X}\_F$ are the trace andFrobenius norm of respectively. We show that they can efficiently recoverlow-rank tensors. In particular, they enjoy exact guarantees similar to thoseknown for minimizing $\\mathcal{X}\_*$ under the conditions on the sensingoperator such as its null-space property, restricted isometry property, orspherical section property. To recover a low-rank tensor $\mathcal{X}^0$,minimizing $\\mathcal{X}\_*+\frac{1}{2\alpha}\\mathcal{X}\_F^2$ returns thesame solution as minimizing $\\mathcal{X}\_*$ almost whenever$\alpha\geq10\mathop {\max}\limits_{i}\X^0_{(i)}\_2$.
arxiv-1500-129 | A prototype for projecting HPSG syntactic lexica towards LMF | http://arxiv.org/pdf/1207.5328v3.pdf | author:Kais Haddar, Héla Fehri, Laurent Romary category:cs.CL published:2012-07-23 summary:The comparative evaluation of Arabic HPSG grammar lexica requires a deepstudy of their linguistic coverage. The complexity of this task results mainlyfrom the heterogeneity of the descriptive components within those lexica(underlying linguistic resources and different data categories, for example).It is therefore essential to define more homogeneous representations, which inturn will enable us to compare them and eventually merge them. In this context,we present a method for comparing HPSG lexica based on a rule system. Thismethod is implemented within a prototype for the projection from Arabic HPSG toa normalised pivot language compliant with LMF (ISO 24613 - Lexical MarkupFramework) and serialised using a TEI (Text Encoding Initiative) basedrepresentation. The design of this system is based on an initial study of theHPSG formalism looking at its adequacy for the representation of Arabic, andfrom this, we identify the appropriate feature structures corresponding to eachArabic lexical category and their possible LMF counterparts.
arxiv-1500-130 | Generalization Bounds for Metric and Similarity Learning | http://arxiv.org/pdf/1207.5437v2.pdf | author:Qiong Cao, Zheng-Chu Guo, Yiming Ying category:cs.LG stat.ML published:2012-07-23 summary:Recently, metric learning and similarity learning have attracted a largeamount of interest. Many models and optimisation algorithms have been proposed.However, there is relatively little work on the generalization analysis of suchmethods. In this paper, we derive novel generalization bounds of metric andsimilarity learning. In particular, we first show that the generalizationanalysis reduces to the estimation of the Rademacher average over"sums-of-i.i.d." sample-blocks related to the specific matrix norm. Then, wederive generalization bounds for metric/similarity learning with differentmatrix-norm regularisers by estimating their specific Rademacher complexities.Our analysis indicates that sparse metric/similarity learning with $L^1$-normregularisation could lead to significantly better bounds than those withFrobenius-norm regularisation. Our novel generalization analysis develops andrefines the techniques of U-statistics and Rademacher complexity analysis.
arxiv-1500-131 | Evolving Musical Counterpoint: The Chronopoint Musical Evolution System | http://arxiv.org/pdf/1207.5560v1.pdf | author:Jeffrey Power Jacobs, James Reggia category:cs.SD cs.AI cs.NE published:2012-07-23 summary:Musical counterpoint, a musical technique in which two or more independentmelodies are played simultaneously with the goal of creating harmony, has beenaround since the baroque era. However, to our knowledge computationalgeneration of aesthetically pleasing linear counterpoint based on subjectivefitness assessment has not been explored by the evolutionary computationcommunity (although generation using objective fitness has been attempted inquite a few cases). The independence of contrapuntal melodies and thesubjective nature of musical aesthetics provide an excellent platform for theapplication of genetic algorithms. In this paper, a genetic algorithm approachto generating contrapuntal melodies is explained, with a description of thevarious musical heuristics used and of how variable-length chromosome stringsare used to avoid generating "jerky" rhythms and melodic phrases, as well ashow subjectivity is incorporated into the algorithm's fitness measures. Next,results from empirical testing of the algorithm are presented, with a focus onhow a user's musical sophistication influences their experience. Lastly,further musical and compositional applications of the algorithm are discussedalong with planned future work on the algorithm.
arxiv-1500-132 | MCTS Based on Simple Regret | http://arxiv.org/pdf/1207.5536v1.pdf | author:David Tolpin, Solomon Eyal Shimony category:cs.AI cs.LG published:2012-07-23 summary:UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in gamesand Markov decision processes, is based on UCB, a sampling policy for theMulti-armed Bandit problem (MAB) that minimizes the cumulative regret. However,search differs from MAB in that in MCTS it is usually only the final "arm pull"(the actual move selection) that collects a reward, rather than all "armpulls". Therefore, it makes more sense to minimize the simple regret, asopposed to the cumulative regret. We begin by introducing policies formulti-armed bandits with lower finite-time and asymptotic simple regret thanUCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperformsUCT empirically. Optimizing the sampling process is itself a metareasoning problem, a solutionof which can use value of information (VOI) techniques. Although the theory ofVOI for search exists, applying it to MCTS is non-trivial, as typical myopicassumptions fail. Lacking a complete working VOI theory for MCTS, wenevertheless propose a sampling scheme that is "aware" of VOI, achieving analgorithm that in empirical evaluation outperforms both UCT and the otherproposed algorithms.
arxiv-1500-133 | Nonlinear spectral unmixing of hyperspectral images using Gaussian processes | http://arxiv.org/pdf/1207.5451v1.pdf | author:Yoann Altmann, Nicolas Dobigeon, Steve McLaughlin, Jean-Yves Tourneret category:stat.ML stat.AP published:2012-07-23 summary:This paper presents an unsupervised algorithm for nonlinear unmixing ofhyperspectral images. The proposed model assumes that the pixel reflectancesresult from a nonlinear function of the abundance vectors associated with thepure spectral components. We assume that the spectral signatures of the purecomponents and the nonlinear function are unknown. The first step of theproposed method consists of the Bayesian estimation of the abundance vectorsfor all the image pixels and the nonlinear function relating the abundancevectors to the observations. The endmembers are subsequently estimated usingGaussian process regression. The performance of the unmixing strategy isevaluated with simulations conducted on synthetic and real data.
arxiv-1500-134 | Meta-Learning of Exploration/Exploitation Strategies: The Multi-Armed Bandit Case | http://arxiv.org/pdf/1207.5208v1.pdf | author:Francis Maes, Damien Ernst, Louis Wehenkel category:cs.AI cs.LG stat.ML published:2012-07-22 summary:The exploration/exploitation (E/E) dilemma arises naturally in many subfieldsof Science. Multi-armed bandit problems formalize this dilemma in its canonicalform. Most current research in this field focuses on generic solutions that canbe applied to a wide range of problems. However, in practice, it is often thecase that a form of prior information is available about the specific class oftarget problems. Prior knowledge is rarely used in current solutions due to thelack of a systematic approach to incorporate it into the E/E strategy. To address a specific class of E/E problems, we propose to proceed in threesteps: (i) model prior knowledge in the form of a probability distribution overthe target class of E/E problems; (ii) choose a large hypothesis space ofcandidate E/E strategies; and (iii), solve an optimization problem to find acandidate E/E strategy of maximal average performance over a sample of problemsdrawn from the prior distribution. We illustrate this meta-learning approach with two different hypothesisspaces: one where E/E strategies are numerically parameterized and anotherwhere E/E strategies are represented as small symbolic formulas. We proposeappropriate optimization algorithms for both cases. Our experiments, withtwo-armed Bernoulli bandit problems and various playing budgets, show that themeta-learnt E/E strategies outperform generic strategies of the literature(UCB1, UCB1-Tuned, UCB-v, KL-UCB and epsilon greedy); they also evaluate therobustness of the learnt E/E strategies, by tests carried out on arms whoserewards follow a truncated Gaussian distribution.
arxiv-1500-135 | Optimal discovery with probabilistic expert advice: finite time analysis and macroscopic optimality | http://arxiv.org/pdf/1207.5259v3.pdf | author:Sebastien Bubeck, Damien Ernst, Aurelien Garivier category:cs.LG stat.ML published:2012-07-22 summary:We consider an original problem that arises from the issue of securityanalysis of a power system and that we name optimal discovery withprobabilistic expert advice. We address it with an algorithm based on theoptimistic paradigm and on the Good-Turing missing mass estimator. We prove twodifferent regret bounds on the performance of this algorithm under weakassumptions on the probabilistic experts. Under more restrictive hypotheses, wealso prove a macroscopic optimality result, comparing the algorithm both withan oracle strategy and with uniform sampling. Finally, we provide numericalexperiments illustrating these theoretical findings.
arxiv-1500-136 | A New Training Algorithm for Kanerva's Sparse Distributed Memory | http://arxiv.org/pdf/1207.5774v3.pdf | author:Lou Marvin Caraig category:cs.CV cs.LG cs.NE published:2012-07-22 summary:The Sparse Distributed Memory proposed by Pentii Kanerva (SDM in short) wasthought to be a model of human long term memory. The architecture of the SDMpermits to store binary patterns and to retrieve them using partially matchingpatterns. However Kanerva's model is especially efficient only in handlingrandom data. The purpose of this article is to introduce a new approach oftraining Kanerva's SDM that can handle efficiently non-random data, and toprovide it the capability to recognize inverted patterns. This approach uses asignal model which is different from the one proposed for different purposes byHely, Willshaw and Hayes in [4]. This article additionally suggests a differentway of creating hard locations in the memory despite the Kanerva's staticmodel.
arxiv-1500-137 | Learning Probabilistic Systems from Tree Samples | http://arxiv.org/pdf/1207.5091v1.pdf | author:Anvesh Komuravelli, Corina S. Pasareanu, Edmund M. Clarke category:cs.LO cs.LG published:2012-07-21 summary:We consider the problem of learning a non-deterministic probabilistic systemconsistent with a given finite set of positive and negative tree samples.Consistency is defined with respect to strong simulation conformance. Wepropose learning algorithms that use traditional and a new "stochastic"state-space partitioning, the latter resulting in the minimum number of states.We then use them to solve the problem of "active learning", that uses aknowledgeable teacher to generate samples as counterexamples to simulationequivalence queries. We show that the problem is undecidable in general, butthat it becomes decidable under a suitable condition on the teacher which comesnaturally from the way samples are generated from failed simulation checks. Thelatter problem is shown to be undecidable if we impose an additional conditionon the learner to always conjecture a "minimum state" hypothesis. We thereforepropose a semi-algorithm using stochastic partitions. Finally, we apply theproposed (semi-) algorithms to infer intermediate assumptions in an automatedassume-guarantee verification framework for probabilistic systems.
arxiv-1500-138 | Causal Inference on Time Series using Structural Equation Models | http://arxiv.org/pdf/1207.5136v1.pdf | author:Jonas Peters, Dominik Janzing, Bernhard Schölkopf category:stat.ML cs.LG stat.ME published:2012-07-21 summary:Causal inference uses observations to infer the causal structure of the datagenerating system. We study a class of functional models that we call TimeSeries Models with Independent Noise (TiMINo). These models require independentresidual time series, whereas traditional methods like Granger causalityexploit the variance of residuals. There are two main contributions: (1)Theoretical: By restricting the model class (e.g. to additive noise) we canprovide a more general identifiability result than existing ones. This resultincorporates lagged and instantaneous effects that can be nonlinear and do notneed to be faithful, and non-instantaneous feedbacks between the time series.(2) Practical: If there are no feedback loops between time series, we proposean algorithm based on non-linear independence tests of time series. When thedata are causally insufficient, or the data generating process does not satisfythe model assumptions, this algorithm may still give partial results, butmostly avoids incorrect answers. An extension to (non-instantaneous) feedbacksis possible, but not discussed. It outperforms existing methods on artificialand real data. Code can be provided upon request.
arxiv-1500-139 | Piecewise Linear Patch Reconstruction for Segmentation and Description of Non-smooth Image Structures | http://arxiv.org/pdf/1207.5113v1.pdf | author:Junyan Wang, Kap Luk Chan category:cs.CV published:2012-07-21 summary:In this paper, we propose a unified energy minimization model for thesegmentation of non-smooth image structures. The energy of piecewise linearpatch reconstruction is considered as an objective measure of the quality ofthe segmentation of non-smooth structures. The segmentation is achieved byminimizing the single energy without any separate process of featureextraction. We also prove that the error of segmentation is bounded by theproposed energy functional, meaning that minimizing the proposed energy leadsto reducing the error of segmentation. As a by-product, our method produces adictionary of optimized orthonormal descriptors for each segmented region. Theunique feature of our method is that it achieves the simultaneous segmentationand description for non-smooth image structures under the same optimizationframework. The experiments validate our theoretical claims and show the clearsuperior performance of our methods over other related methods for segmentationof various image textures. We show that our model can be coupled with thepiecewise smooth model to handle both smooth and non-smooth structures, and wedemonstrate that the proposed model is capable of coping with multipledifferent regions through the one-against-all strategy.
arxiv-1500-140 | Fast nonparametric classification based on data depth | http://arxiv.org/pdf/1207.4992v2.pdf | author:Tatjana Lange, Karl Mosler, Pavlo Mozharovskyi category:stat.ML cs.LG 62H30 published:2012-07-20 summary:A new procedure, called DDa-procedure, is developed to solve the problem ofclassifying d-dimensional objects into q >= 2 classes. The procedure iscompletely nonparametric; it uses q-dimensional depth plots and a veryefficient algorithm for discrimination analysis in the depth space [0,1]^q.Specifically, the depth is the zonoid depth, and the algorithm is thealpha-procedure. In case of more than two classes several binaryclassifications are performed and a majority rule is applied. Specialtreatments are discussed for 'outsiders', that is, data having zero depthvector. The DDa-classifier is applied to simulated as well as real data, andthe results are compared with those of similar procedures that have beenrecently proposed. In most cases the new procedure has comparable error rates,but is much faster than other classification approaches, including the SVM.
arxiv-1500-141 | Multisegmentation through wavelets: Comparing the efficacy of Daubechies vs Coiflets | http://arxiv.org/pdf/1207.5007v1.pdf | author:Madhur Srivastava, Yashwant Yashu, Satish K. Singh, Prasanta K. Panigrahi category:cs.CV published:2012-07-20 summary:In this paper, we carry out a comparative study of the efficacy of waveletsbelonging to Daubechies and Coiflet family in achieving image segmentationthrough a fast statistical algorithm.The fact that wavelets belonging toDaubechies family optimally capture the polynomial trends and those of Coifletfamily satisfy mini-max condition, makes this comparison interesting. In thecontext of the present algorithm, it is found that the performance of Coifletwavelets is better, as compared to Daubechies wavelet.
arxiv-1500-142 | Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural Network | http://arxiv.org/pdf/1207.4931v1.pdf | author:G. N. Tripathi, V. Rihani category:cs.RO cs.AI cs.LG cs.NE published:2012-07-20 summary:The paper presents the electronic design and motion planning of a robot basedon decision making regarding its straight motion and precise turn usingArtificial Neural Network (ANN). The ANN helps in learning of robot so that itperforms motion autonomously. The weights calculated are implemented inmicrocontroller. The performance has been tested to be excellent.
arxiv-1500-143 | A Novel Metric Approach Evaluation For The Spatial Enhancement Of Pan-Sharpened Images | http://arxiv.org/pdf/1207.5064v1.pdf | author:Firouz Abdullah Al-Wassai, Dr. N. V. Kalyankar category:cs.CV published:2012-07-20 summary:Various and different methods can be used to produce high-resolutionmultispectral images from high-resolution panchromatic image (PAN) andlow-resolution multispectral images (MS), mostly on the pixel level. TheQuality of image fusion is an essential determinant of the value of processingimages fusion for many applications. Spatial and spectral qualities are the twoimportant indexes that used to evaluate the quality of any fused image.However, the jury is still out of fused image's benefits if it compared withits original images. In addition, there is a lack of measures for assessing theobjective quality of the spatial resolution for the fusion methods. So, anobjective quality of the spatial resolution assessment for fusion images isrequired. Therefore, this paper describes a new approach proposed to estimatethe spatial resolution improve by High Past Division Index (HPDI) uponcalculating the spatial-frequency of the edge regions of the image and it dealswith a comparison of various analytical techniques for evaluating the Spatialquality, and estimating the colour distortion added by image fusion including:MG, SG, FCC, SD, En, SNR, CC and NRMSE. In addition, this paper devotes toconcentrate on the comparison of various image fusion techniques based on pixeland feature fusion technique.
arxiv-1500-144 | Parameter and Structure Learning in Nested Markov Models | http://arxiv.org/pdf/1207.5058v1.pdf | author:Ilya Shpitser, Thomas S. Richardson, James M. Robins, Robin Evans category:stat.ML math.ST stat.TH published:2012-07-20 summary:The constraints arising from DAG models with latent variables can benaturally represented by means of acyclic directed mixed graphs (ADMGs). Suchgraphs contain directed and bidirected arrows, and contain no directed cycles.DAGs with latent variables imply independence constraints in the distributionresulting from a 'fixing' operation, in which a joint distribution is dividedby a conditional. This operation generalizes marginalizing and conditioning.Some of these constraints correspond to identifiable 'dormant' independenceconstraints, with the well known 'Verma constraint' as one example. Recently,models defined by a set of the constraints arising after fixing from a DAG withlatents, were characterized via a recursive factorization and a nested Markovproperty. In addition, a parameterization was given in the discrete case. Inthis paper we use this parameterization to describe a parameter fittingalgorithm, and a search and score structure learning algorithm for these nestedMarkov models. We apply our algorithms to a variety of datasets.
arxiv-1500-145 | The Fast Cauchy Transform and Faster Robust Linear Regression | http://arxiv.org/pdf/1207.4684v3.pdf | author:Kenneth L. Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, Xiangrui Meng, David P. Woodruff category:cs.DS stat.ML published:2012-07-19 summary:We provide fast algorithms for overconstrained $\ell_p$ regression andrelated problems: for an $n\times d$ input matrix $A$ and vector$b\in\mathbb{R}^n$, in $O(nd\log n)$ time we reduce the problem$\min_{x\in\mathbb{R}^d} \Ax-b\_p$ to the same problem with input matrix$\tilde A$ of dimension $s \times d$ and corresponding $\tilde b$ of dimension$s\times 1$. Here, $\tilde A$ and $\tilde b$ are a coreset for the problem,consisting of sampled and rescaled rows of $A$ and $b$; and $s$ is independentof $n$ and polynomial in $d$. Our results improve on the best previousalgorithms when $n\gg d$, for all $p\in[1,\infty)$ except $p=2$. We alsoprovide a suite of improved results for finding well-conditioned bases viaellipsoidal rounding, illustrating tradeoffs between running time andconditioning quality, including a one-pass conditioning algorithm for general$\ell_p$ problems. We also provide an empirical evaluation of implementations of our algorithmsfor $p=1$, comparing them with related algorithms. Our empirical results showthat, in the asymptotic regime, the theory is a very good guide to thepractical performance of these algorithms. Our algorithms use our fasterconstructions of well-conditioned bases for $\ell_p$ spaces and, for $p=1$, afast subspace embedding of independent interest that we call the Fast CauchyTransform: a distribution over matrices $\Pi:\mathbb{R}^n\mapsto\mathbb{R}^{O(d\log d)}$, found obliviously to $A$, that approximatelypreserves the $\ell_1$ norms: that is, with large probability, simultaneouslyfor all $x$, $\Ax\_1 \approx \\Pi Ax\_1$, with distortion $O(d^{2+\eta})$,for an arbitrarily small constant $\eta>0$; and, moreover, $\Pi A$ can becomputed in $O(nd\log d)$ time. The techniques underlying our Fast CauchyTransform include fast Johnson-Lindenstrauss transforms, low-coherencematrices, and rescaling by Cauchy random variables.
arxiv-1500-146 | Proceedings of the 29th International Conference on Machine Learning (ICML-12) | http://arxiv.org/pdf/1207.4676v2.pdf | author:John Langford, Joelle Pineau category:cs.LG stat.ML published:2012-07-19 summary:This is an index to the papers that appear in the Proceedings of the 29thInternational Conference on Machine Learning (ICML-12). The conference was heldin Edinburgh, Scotland, June 27th - July 3rd, 2012.
arxiv-1500-147 | Quick HyperVolume | http://arxiv.org/pdf/1207.4598v2.pdf | author:Luís M. S. Russo, Alexandre P. Francisco category:cs.DS cs.DM cs.NE published:2012-07-19 summary:We present a new algorithm to calculate exact hypervolumes. Given a set of$d$-dimensional points, it computes the hypervolume of the dominated space.Determining this value is an important subroutine of MultiobjectiveEvolutionary Algorithms (MOEAs). We analyze the "Quick Hypervolume" (QHV)algorithm theoretically and experimentally. The theoretical results are asignificant contribution to the current state of the art. Moreover theexperimental performance is also very competitive, compared with existing exacthypervolume algorithms. A full description of the algorithm is currently submitted to IEEETransactions on Evolutionary Computation.
arxiv-1500-148 | Models of Disease Spectra | http://arxiv.org/pdf/1207.4674v1.pdf | author:Iead Rezek, Christian Beckmann category:stat.ML published:2012-07-19 summary:Case vs control comparisons have been the classical approach to the study ofneurological diseases. However, most patients will not fall cleanly into eithergroup. Instead, clinicians will typically find patients that cannot beclassified as having clearly progressed into the disease state. For thosesubjects, very little can be said about their brain function on the basis ofanalyses of group differences. To describe the intermediate brain functionrequires models that interpolate between the disease states. We have chosenGaussian Processes (GP) regression to obtain a continuous spectrum of brainactivation and to extract the unknown disease progression profile. Our modelsincorporate spatial distribution of measures of activation, e.g. thecorrelation of an fMRI trace with an input stimulus, and so constituteultra-high multi-variate GP regressors. We applied GPs to model fMRI imagephenotypes across Alzheimer's Disease (AD) behavioural measures, e.g. MMSE, ACEetc. scores, and obtained predictions at non-observed MMSE/ACE values. Theoverall model confirmed the known reduction in the spatial extent of activityin response to reading versus false-font stimulation. The predictiveuncertainty indicated the worsening confidence intervals at behavioural scoresdistance from those used for GP training. Thus, the model indicated the type ofpatient (what behavioural score) that would need to included in the trainingdata to improve models predictions.
arxiv-1500-149 | Clustering of Local Optima in Combinatorial Fitness Landscapes | http://arxiv.org/pdf/1207.4632v1.pdf | author:Gabriela Ochoa, Sébastien Verel, Fabio Daolio, Marco Tomassini category:cs.NE cs.AI published:2012-07-19 summary:Using the recently proposed model of combinatorial landscapes: local optimanetworks, we study the distribution of local optima in two classes of instancesof the quadratic assignment problem. Our results indicate that the two probleminstance classes give rise to very different configuration spaces. For theso-called real-like class, the optima networks possess a clear modularstructure, while the networks belonging to the class of random uniforminstances are less well partitionable into clusters. We briefly discuss theconsequences of the findings for heuristically searching the correspondingproblem spaces.
arxiv-1500-150 | Analyzing the Effect of Objective Correlation on the Efficient Set of MNK-Landscapes | http://arxiv.org/pdf/1207.4631v1.pdf | author:Sébastien Verel, Arnaud Liefooghe, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI published:2012-07-19 summary:In multiobjective combinatorial optimization, there exists two main classesof metaheuristics, based either on multiple aggregations, or on a dominancerelation. As in the single objective case, the structure of the search spacecan explain the difficulty for multiobjective metaheuristics, and guide thedesign of such methods. In this work we analyze the properties ofmultiobjective combinatorial search spaces. In particular, we focus on thefeatures related the efficient set, and we pay a particular attention to thecorrelation between objectives. Few benchmark takes such objective correlationinto account. Here, we define a general method to design multiobjectiveproblems with correlation. As an example, we extend the well-knownmultiobjective NK-landscapes. By measuring different properties of the searchspace, we show the importance of considering the objective correlation on thedesign of metaheuristics.
arxiv-1500-151 | Hierarchical Clustering using Randomly Selected Similarities | http://arxiv.org/pdf/1207.4748v1.pdf | author:Brian Eriksson category:stat.ML cs.IT cs.LG math.IT published:2012-07-19 summary:The problem of hierarchical clustering items from pairwise similarities isfound across various scientific disciplines, from biology to networking. Often,applications of clustering techniques are limited by the cost of obtainingsimilarities between pairs of items. While prior work has been developed toreconstruct clustering using a significantly reduced set of pairwisesimilarities via adaptive measurements, these techniques are only applicablewhen choice of similarities are available to the user. In this paper, weexamine reconstructing hierarchical clustering under similarity observationsat-random. We derive precise bounds which show that a significant fraction ofthe hierarchical clustering can be recovered using fewer than all the pairwisesimilarities. We find that the correct hierarchical clustering down to aconstant fraction of the total number of items (i.e., clusters sized O(N)) canbe found using only O(N log N) randomly selected pairwise similarities inexpectation.
arxiv-1500-152 | On the Neutrality of Flowshop Scheduling Fitness Landscapes | http://arxiv.org/pdf/1207.4629v1.pdf | author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE cs.AI published:2012-07-19 summary:Solving efficiently complex problems using metaheuristics, and in particularlocal searches, requires incorporating knowledge about the problem to solve. Inthis paper, the permutation flowshop problem is studied. It is well known thatin such problems, several solutions may have the same fitness value. As thisneutrality property is an important one, it should be taken into account duringthe design of optimization methods. Then in the context of the permutationflowshop, a deep landscape analysis focused on the neutrality property isdriven and propositions on the way to use this neutrality to guide efficientlythe search are given.
arxiv-1500-153 | Block-Coordinate Frank-Wolfe Optimization for Structural SVMs | http://arxiv.org/pdf/1207.4747v4.pdf | author:Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, Patrick Pletscher category:cs.LG math.OC stat.ML G.1.6; I.2.6 published:2012-07-19 summary:We propose a randomized block-coordinate variant of the classic Frank-Wolfealgorithm for convex optimization with block-separable constraints. Despite itslower iteration cost, we show that it achieves a similar convergence rate induality gap as the full Frank-Wolfe algorithm. We also show that, when appliedto the dual structural support vector machine (SVM) objective, this yields anonline algorithm that has the same low iteration complexity as primalstochastic subgradient methods. However, unlike stochastic subgradient methods,the block-coordinate Frank-Wolfe algorithm allows us to compute the optimalstep-size and yields a computable duality gap guarantee. Our experimentsindicate that this simple algorithm outperforms competing structural SVMsolvers.
arxiv-1500-154 | Automorphism Groups of Graphical Models and Lifted Variational Inference | http://arxiv.org/pdf/1207.4814v1.pdf | author:Hung Hai Bui, Tuyen N. Huynh, Sebastian Riedel category:cs.AI cs.LG math.CO stat.CO stat.ML published:2012-07-19 summary:Using the theory of group action, we first introduce the concept of theautomorphism group of an exponential family or a graphical model, thusformalizing the general notion of symmetry of a probabilistic model. Thisautomorphism group provides a precise mathematical framework for liftedinference in the general exponential family. Its group action partitions theset of random variables and feature functions into equivalent classes (calledorbits) having identical marginals and expectations. Then the inference problemis effectively reduced to that of computing marginals or expectations for eachclass, thus avoiding the need to deal with each individual variable or feature.We demonstrate the usefulness of this general framework in lifting two classesof variational approximation for MAP inference: local LP relaxation and localLP relaxation with cycle constraints; the latter yields the first liftedinference that operate on a bound tighter than local constraints. Initialexperimental results demonstrate that lifted MAP inference with cycleconstraints achieved the state of the art performance, obtaining much betterobjective function values than local approximation while remaining relativelyefficient.
arxiv-1500-155 | On the Effect of Connectedness for Biobjective Multiple and Long Path Problems | http://arxiv.org/pdf/1207.4628v1.pdf | author:Sébastien Verel, Arnaud Liefooghe, Jérémie Humeau, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI published:2012-07-19 summary:Recently, the property of connectedness has been claimed to give a strongmotivation on the design of local search techniques for multiobjectivecombinatorial optimization (MOCO). Indeed, when connectedness holds, a basicPareto local search, initialized with at least one non-dominated solution,allows to identify the efficient set exhaustively. However, this becomesquickly infeasible in practice as the number of efficient solutions typicallygrows exponentially with the instance size. As a consequence, we generally haveto deal with a limited-size approximation, where a good sample set has to befound. In this paper, we propose the biobjective multiple and long pathproblems to show experimentally that, on the first problems, even if theefficient set is connected, a local search may be outperformed by a simpleevolutionary algorithm in the sampling of the efficient set. At the opposite,on the second problems, a local search algorithm may successfully approximate adisconnected efficient set. Then, we argue that connectedness is not the singleproperty to study for the design of local search heuristics for MOCO. This workopens new discussions on a proper definition of the multiobjective fitnesslandscape.
arxiv-1500-156 | The Road to VEGAS: Guiding the Search over Neutral Networks | http://arxiv.org/pdf/1207.4626v1.pdf | author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE published:2012-07-19 summary:VEGAS (Varying Evolvability-Guided Adaptive Search) is a new methodologyproposed to deal with the neutrality property of some optimization problems. tsmain feature is to consider the whole neutral network rather than an arbitrarysolution. Moreover, VEGAS is designed to escape from plateaus based on theevolvability of solution and a multi-armed bandit. Experiments are conducted onNK-landscapes with neutrality. Results show the importance of considering thewhole neutral network and of guiding the search cleverly. The impact of thelevel of neutrality and of the exploration-exploitation trade-off are deeplyanalyzed.
arxiv-1500-157 | Local stability of Belief Propagation algorithm with multiple fixed points | http://arxiv.org/pdf/1207.4597v1.pdf | author:Victorin Martin, Jean-Marc Lasgouttes, Cyril Furtlehner category:stat.ML cs.LG published:2012-07-19 summary:A number of problems in statistical physics and computer science can beexpressed as the computation of marginal probabilities over a Markov randomfield. Belief propagation, an iterative message-passing algorithm, computesexactly such marginals when the underlying graph is a tree. But it has gainedits popularity as an efficient way to approximate them in the more generalcase, even if it can exhibits multiple fixed points and is not guaranteed toconverge. In this paper, we express a new sufficient condition for localstability of a belief propagation fixed point in terms of the graph structureand the beliefs values at the fixed point. This gives credence to the usualunderstanding that Belief Propagation performs better on sparse graphs.
arxiv-1500-158 | Appropriate Nouns with Obligatory Modifiers | http://arxiv.org/pdf/1207.4625v1.pdf | author:E. Laporte category:cs.CL published:2012-07-19 summary:The notion of appropriate sequence as introduced by Z. Harris provides apowerful syntactic way of analysing the detailed meaning of various sentences,including ambiguous ones. In an adjectival sentence like 'The leather wasyellow', the introduction of an appropriate noun, here 'colour', specifieswhich quality the adjective describes. In some other adjectival sentences withan appropriate noun, that noun plays the same part as 'colour' and seems to berelevant to the description of the adjective. These appropriate nouns canusually be used in elementary sentences like 'The leather had some colour', butin many cases they have a more or less obligatory modifier. For example, youcan hardly mention that an object has a colour without qualifying that colourat all. About 300 French nouns are appropriate in at least one adjectivalsentence and have an obligatory modifier. They enter in a number of sentencestructures related by several syntactic transformations. The appropriateness ofthe noun and the fact that the modifier is obligatory are reflected in thesetransformations. The description of these syntactic phenomena provides a basisfor a classification of these nouns. It also concerns the lexical properties ofthousands of predicative adjectives, and in particular the relations betweenthe sentence without the noun : 'The leather was yellow' and the adjectivalsentence with the noun : 'The colour of the leather was yellow'.
arxiv-1500-159 | Empirical review of standard benchmark functions using evolutionary global optimization | http://arxiv.org/pdf/1207.4318v1.pdf | author:Johannes M. Dieterich, Bernd Hartke category:cs.NE published:2012-07-18 summary:We have employed a recent implementation of genetic algorithms to study arange of standard benchmark functions for global optimization. It turns outthat some of them are not very useful as challenging test functions, since theyneither allow for a discrimination between different variants of geneticoperators nor exhibit a dimensionality scaling resembling that of real-worldproblems, for example that of global structure optimization of atomic andmolecular clusters. The latter properties seem to be simulated better by twoother types of benchmark functions. One type is designed to be deceptive,exemplified here by Lunacek's function. The other type offers additionaladvantages of markedly increased complexity and of broad tunability in searchspace characteristics. For the latter type, we use an implementation based onrandomly distributed Gaussians. We advocate the use of the latter types of testfunctions for algorithm development and benchmarking.
arxiv-1500-160 | Protein Function Prediction Based on Kernel Logistic Regression with 2-order Graphic Neighbor Information | http://arxiv.org/pdf/1207.4463v1.pdf | author:Jingwei Liu category:q-bio.QM cs.LG q-bio.MN published:2012-07-18 summary:To enhance the accuracy of protein-protein interaction function prediction, a2-order graphic neighbor information feature extraction method based onundirected simple graph is proposed in this paper, which extends the 1-ordergraphic neighbor featureextraction method. And the chi-square test statisticalmethod is also involved in feature combination. To demonstrate theeffectiveness of our 2-order graphic neighbor feature, four logistic regressionmodels (logistic regression (abbrev. LR), diffusion kernel logistic regression(abbrev. DKLR), polynomial kernel logistic regression (abbrev. PKLR), andradial basis function (RBF) based kernel logistic regression (abbrev. RBF KLR))are investigated on the two feature sets. The experimental results of proteinfunction prediction of Yeast Proteome Database (YPD) using the theprotein-protein interaction data of Munich Information Center for ProteinSequences (MIPS) show that 2-order graphic neighbor information of proteins cansignificantly improve the average overall percentage of protein functionprediction especially with RBF KLR. And, with a new 5-top chi-square featurecombination method, RBF KLR can achieve 99.05% average overall percentage on2-order neighbor feature combination set.
arxiv-1500-161 | Better Mixing via Deep Representations | http://arxiv.org/pdf/1207.4404v1.pdf | author:Yoshua Bengio, Grégoire Mesnil, Yann Dauphin, Salah Rifai category:cs.LG published:2012-07-18 summary:It has previously been hypothesized, and supported with some experimentalevidence, that deeper representations, when well trained, tend to do a betterjob at disentangling the underlying factors of variation. We study thefollowing related conjecture: better representations, in the sense of betterdisentangling, can be exploited to produce faster-mixing Markov chains.Consequently, mixing would be more efficient at higher levels ofrepresentation. To better understand why and how this is happening, we proposea secondary conjecture: the higher-level samples fill more uniformly the spacethey occupy and the high-density manifolds tend to unfold when represented athigher levels. The paper discusses these hypotheses and tests themexperimentally through visualization and measurements of mixing andinterpolating between samples.
arxiv-1500-162 | First-improvement vs. Best-improvement Local Optima Networks of NK Landscapes | http://arxiv.org/pdf/1207.4455v1.pdf | author:Gabriela Ochoa, Sébastien Verel, Marco Tomassini category:cs.NE cs.AI published:2012-07-18 summary:This paper extends a recently proposed model for combinatorial landscapes:Local Optima Networks (LON), to incorporate a first-improvement (greedy-ascent)hill-climbing algorithm, instead of a best-improvement (steepest-ascent) one,for the definition and extraction of the basins of attraction of the landscapeoptima. A statistical analysis comparing best and first improvement networkmodels for a set of NK landscapes, is presented and discussed. Our resultssuggest structural differences between the two models with respect to both thenetwork connectivity, and the nature of the basins of attraction. The impact ofthese differences in the behavior of search heuristics based on first and bestimprovement local search is thoroughly discussed.
arxiv-1500-163 | Assessment of SAR Image Filtering using Adaptive Stack Filters | http://arxiv.org/pdf/1207.4308v1.pdf | author:Maria E. Buemi, Marta Mejail, Julio Jacobo, Alejandro C. Frery, Heitor S. Ramos category:cs.CV published:2012-07-18 summary:Stack filters are a special case of non-linear filters. They have a goodperformance for filtering images with different types of noise while preservingedges and details. A stack filter decomposes an input image into several binaryimages according to a set of thresholds. Each binary image is then filtered bya Boolean function, which characterizes the filter. Adaptive stack filters canbe designed to be optimal; they are computed from a pair of images consistingof an ideal noiseless image and its noisy version. In this work we study theperformance of adaptive stack filters when they are applied to SyntheticAperture Radar (SAR) images. This is done by evaluating the quality of thefiltered images through the use of suitable image quality indexes and bymeasuring the classification accuracy of the resulting images.
arxiv-1500-164 | Frame Interpretation and Validation in a Open Domain Dialogue System | http://arxiv.org/pdf/1207.4307v1.pdf | author:Artur Ventura, Nuno Diegues, David Martins de Matos category:cs.CL cs.RO I.2.7; I.2.9 published:2012-07-18 summary:Our goal in this paper is to establish a means for a dialogue platform to beable to cope with open domains considering the possible interaction between theembodied agent and humans. To this end we present an algorithm capable ofprocessing natural language utterances and validate them against knowledgestructures of an intelligent agent's mind. Our algorithm leverages dialoguetechniques in order to solve ambiguities and acquire knowledge about unknownentities.
arxiv-1500-165 | Pareto Local Optima of Multiobjective NK-Landscapes with Correlated Objectives | http://arxiv.org/pdf/1207.4452v1.pdf | author:Sébastien Verel, Arnaud Liefooghe, Laetitia Jourdan, Clarisse Dhaenens category:cs.NE cs.AI published:2012-07-18 summary:In this paper, we conduct a fitness landscape analysis for multiobjectivecombinatorial optimization, based on the local optima of multiobjectiveNK-landscapes with objective correlation. In single-objective optimization, ithas become clear that local optima have a strong impact on the performance ofmetaheuristics. Here, we propose an extension to the multiobjective case, basedon the Pareto dominance. We study the co-influence of the problem dimension,the degree of non-linearity, the number of objectives and the correlationdegree between objective functions on the number of Pareto local optima.
arxiv-1500-166 | Set-based Multiobjective Fitness Landscapes: A Preliminary Study | http://arxiv.org/pdf/1207.4451v1.pdf | author:Sébastien Verel, Arnaud Liefooghe, Clarisse Dhaenens category:cs.NE cs.AI published:2012-07-18 summary:Fitness landscape analysis aims to understand the geometry of a givenoptimization problem in order to design more efficient search algorithms.However, there is a very little knowledge on the landscape of multiobjectiveproblems. In this work, following a recent proposal by Zitzler et al. (2010),we consider multiobjective optimization as a set problem. Then, we give ageneral definition of set-based multiobjective fitness landscapes. Anexperimental set-based fitness landscape analysis is conducted on themultiobjective NK-landscapes with objective correlation. The aim is to adaptand to enhance the comprehensive design of set-based multiobjective searchapproaches, motivated by an a priori analysis of the corresponding set problemproperties.
arxiv-1500-167 | NILS: a Neutrality-based Iterated Local Search and its application to Flowshop Scheduling | http://arxiv.org/pdf/1207.4450v1.pdf | author:Marie-Eleonore Marmion, Clarisse Dhaenens, Laetitia Jourdan, Arnaud Liefooghe, Sébastien Verel category:cs.NE cs.AI published:2012-07-18 summary:This paper presents a new methodology that exploits specific characteristicsfrom the fitness landscape. In particular, we are interested in the property ofneutrality, that deals with the fact that the same fitness value is assigned tonumerous solutions from the search space. Many combinatorial optimizationproblems share this property, that is generally very inhibiting for localsearch algorithms. A neutrality-based iterated local search, that allowsneutral walks to move on the plateaus, is proposed and experimented on apermutation flowshop scheduling problem with the aim of minimizing themakespan. Our experiments show that the proposed approach is able to findimproving solutions compared with a classical iterated local search. Moreover,the tradeoff between the exploitation of neutrality and the exploration of newparts of the search space is deeply analyzed.
arxiv-1500-168 | DAMS: Distributed Adaptive Metaheuristic Selection | http://arxiv.org/pdf/1207.4448v1.pdf | author:Bilel Derbel, Sébastien Verel category:cs.NE cs.AI published:2012-07-18 summary:We present a distributed generic algorithm called DAMS dedicated to adaptiveoptimization in distributed environments. Given a set of metaheuristic, thegoal of DAMS is to coordinate their local execution on distributed nodes inorder to optimize the global performance of the distributed system. DAMS isbased on three-layer architecture allowing node to decide distributively whatlocal information to communicate, and what metaheuristic to apply while theoptimization process is in progress. The adaptive features of DAMS are firstaddressed in a very general setting. A specific DAMS called SBM is thendescribed and analyzed from both a parallel and an adaptive point of view. SBMis a simple, yet efficient, adaptive distributed algorithm using anexploitation component allowing nodes to select the metaheuristic with the bestlocally observed performance, and an exploration component allowing nodes todetect the metaheuristic with the actual best performance. The efficiency ofBSM-DAMS is demonstrated through experimentations and comparisons with otheradaptive strategies (sequential and distributed).
arxiv-1500-169 | Communities of Minima in Local Optima Networks of Combinatorial Spaces | http://arxiv.org/pdf/1207.4445v1.pdf | author:Fabio Daolio, Marco Tomassini, Sébastien Verel, Gabriela Ochoa category:cs.NE cs.AI published:2012-07-18 summary:In this work we present a new methodology to study the structure of theconfiguration spaces of hard combinatorial problems. It consists in buildingthe network that has as nodes the locally optimal configurations and as edgesthe weighted oriented transitions between their basins of attraction. We applythe approach to the detection of communities in the optima networks produced bytwo different classes of instances of a hard combinatorial optimizationproblem: the quadratic assignment problem (QAP). We provide evidence indicatingthat the two problem instance classes give rise to very different configurationspaces. For the so-called real-like class, the networks possess a clear modularstructure, while the optima networks belonging to the class of random uniforminstances are less well partitionable into clusters. This is convincinglysupported by using several statistical tests. Finally, we shortly discuss theconsequences of the findings for heuristically searching the correspondingproblem spaces.
arxiv-1500-170 | Complex-network analysis of combinatorial spaces: The NK landscape case | http://arxiv.org/pdf/1207.4442v1.pdf | author:Marco Tomassini, Sébastien Verel, Gabriela Ochoa category:cs.NE nlin.AO published:2012-07-18 summary:We propose a network characterization of combinatorial fitness landscapes byadapting the notion of inherent networks proposed for energy surfaces. We usethe well-known family of NK landscapes as an example. In our case the inherentnetwork is the graph whose vertices represent the local maxima in thelandscape, and the edges account for the transition probabilities between theircorresponding basins of attraction. We exhaustively extracted such networks onrepresentative NK landscape instances, and performed a statisticalcharacterization of their properties. We found that most of these networkproperties are related to the search difficulty on the underlying NK landscapeswith varying values of K.
arxiv-1500-171 | Content Based Multimedia Information Retrieval to Support Digital Libraries | http://arxiv.org/pdf/1207.4259v1.pdf | author:Mohammad Nabil Almunawar category:cs.IR cs.CV published:2012-07-18 summary:Content-based multimedia information retrieval is an interesting researcharea since it allows retrieval based on inherent characteristic of multimediaobjects. For example retrieval based on visual characteristics such as colour,shapes or textures of objects in images or retrieval based on spatialrelationships among objects in the media (images or video clips). This paperreviews some work done in image and video retrieval and then proposes anintegrated model that can handle images and video clips uniformly. Using thismodel retrieval on images or video clips can be done based on the sameframework.
arxiv-1500-172 | Stochastic optimization and sparse statistical recovery: An optimal algorithm for high dimensions | http://arxiv.org/pdf/1207.4421v1.pdf | author:Alekh Agarwal, Sahand Negahban, Martin J. Wainwright category:stat.ML cs.LG math.OC published:2012-07-18 summary:We develop and analyze stochastic optimization algorithms for problems inwhich the expected loss is strongly convex, and the optimum is (approximately)sparse. Previous approaches are able to exploit only one of these twostructures, yielding an $\order(\pdim/T)$ convergence rate for strongly convexobjectives in $\pdim$ dimensions, and an $\order(\sqrt{(\spindex \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Ouralgorithm is based on successively solving a series of $\ell_1$-regularizedoptimization problems using Nesterov's dual averaging algorithm. We establishthat the error of our solution after $T$ iterations is at most$\order((\spindex \log\pdim)/T)$, with natural extensions to approximatesparsity. Our results apply to locally Lipschitz losses including the logistic,exponential, hinge and least-squares losses. By recourse to statistical minimaxresults, we show that our convergence rates are optimal up to multiplicativeconstant factors. The effectiveness of our approach is also confirmed innumerical simulations, in which we compare to several baselines on aleast-squares regression problem.
arxiv-1500-173 | Penalty Constraints and Kernelization of M-Estimation Based Fuzzy C-Means | http://arxiv.org/pdf/1207.4417v2.pdf | author:Jingwei Liu, Meizhi Xu category:cs.CV stat.CO published:2012-07-18 summary:A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithmis proposed with iterative reweighted least squares (IRLS) algorithm, andpenalty constraint and kernelization extensions of MFCM algorithms are alsodeveloped. Introducing penalty information to the object functions of MFCMalgorithms, the spatially constrained fuzzy C-means (SFCM) is extended topenalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclideandistance with kernel method, the MFCM and pMFCM algorithms are extended tokernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms.The performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated inthree tasks: pattern recognition on 10 standard data sets from UCI MachineLearning databases, noise image segmentation performances on a synthetic image,a magnetic resonance brain image (MRI), and image segmentation of a standardimages from Berkeley Segmentation Dataset and Benchmark. The experimentalresults demonstrate the effectiveness of our proposed algorithms in patternrecognition and image segmentation.
arxiv-1500-174 | On the Statistical Efficiency of $\ell_{1,p}$ Multi-Task Learning of Gaussian Graphical Models | http://arxiv.org/pdf/1207.4255v2.pdf | author:Jean Honorio, Tommi Jaakkola, Dimitris Samaras category:cs.LG stat.ML published:2012-07-18 summary:In this paper, we present $\ell_{1,p}$ multi-task structure learning forGaussian graphical models. We analyze the sufficient number of samples for thecorrect recovery of the support union and edge signs. We also analyze thenecessary number of samples for any conceivable method by providinginformation-theoretic lower bounds. We compare the statistical efficiency ofmulti-task learning versus that of single-task learning. For experiments, weuse a block coordinate descent method that is provably convergent and generatesa sequence of positive definite solutions. We provide experimental validationon synthetic data as well as on two publicly available real-world data sets,including functional magnetic resonance imaging and gene expression data.
arxiv-1500-175 | Computation of the Hausdorff distance between sets of line segments in parallel | http://arxiv.org/pdf/1207.3962v1.pdf | author:Helmut Alt, Ludmila Scharf category:cs.CG cs.CV cs.DC published:2012-07-17 summary:We show that the Hausdorff distance for two sets of non-intersecting linesegments can be computed in parallel in $O(\log^2 n)$ time using O(n)processors in a CREW-PRAM computation model. We discuss how some parts of thesequential algorithm can be performed in parallel using previously knownparallel algorithms; and identify the so-far unsolved part of the problem forthe parallel computation, which is the following: Given two sets of$x$-monotone curve segments, red and blue, for each red segment find itsextremal intersection points with the blue set, i.e. points with the minimaland maximal $x$-coordinate. Each segment set is assumed to be intersectionfree. For this intersection problem we describe a parallel algorithm whichcompletes the Hausdorff distance computation within the stated time andprocessor bounds.
arxiv-1500-176 | Polarimetric SAR Image Segmentation with B-Splines and a New Statistical Model | http://arxiv.org/pdf/1207.3944v1.pdf | author:Alejandro C. Frery, Julio Jacobo-Berlles, Juliana Gambini, Marta Mejail category:cs.CV stat.ML published:2012-07-17 summary:We present an approach for polarimetric Synthetic Aperture Radar (SAR) imageregion boundary detection based on the use of B-Spline active contours and anew model for polarimetric SAR data: the GHP distribution. In order to detectthe boundary of a region, initial B-Spline curves are specified, eitherautomatically or manually, and the proposed algorithm uses a deformablecontours technique to find the boundary. In doing this, the parameters of thepolarimetric GHP model for the data are estimated, in order to find thetransition points between the region being segmented and the surrounding area.This is a local algorithm since it works only on the region to be segmented.Results of its performance are presented.
arxiv-1500-177 | Automatic Segmentation of Manipuri (Meiteilon) Word into Syllabic Units | http://arxiv.org/pdf/1207.3932v1.pdf | author:Kishorjit Nongmeikapam, Vidya Raj RK, Oinam Imocha Singh, Sivaji Bandyopadhyay category:cs.CL I.2.7 published:2012-07-17 summary:The work of automatic segmentation of a Manipuri language (or Meiteilon) wordinto syllabic units is demonstrated in this paper. This language is a scheduledIndian language of Tibeto-Burman origin, which is also a very highlyagglutinative language. This language usages two script: a Bengali script andMeitei Mayek (Script). The present work is based on the second script. Analgorithm is designed so as to identify mainly the syllables of Manipuri originword. The result of the algorithm shows a Recall of 74.77, Precision of 91.21and F-Score of 82.18 which is a reasonable score with the first attempt of suchkind for this language.
arxiv-1500-178 | A Two-Stage Combined Classifier in Scale Space Texture Classification | http://arxiv.org/pdf/1207.4089v1.pdf | author:Mehrdad J. Gangeh, Robert P. W. Duin, Bart M. ter Haar Romeny, Mohamed S. Kamel category:cs.CV cs.LG published:2012-07-17 summary:Textures often show multiscale properties and hence multiscale techniques areconsidered useful for texture analysis. Scale-space theory as a biologicallymotivated approach may be used to construct multiscale textures. In this papervarious ways are studied to combine features on different scales for textureclassification of small image patches. We use the N-jet of derivatives up tothe second order at different scales to generate distinct patternrepresentations (DPR) of feature subsets. Each feature subset in the DPR isgiven to a base classifier (BC) of a two-stage combined classifier. Thedecisions made by these BCs are combined in two stages over scales andderivatives. Various combining systems and their significances and differencesare discussed. The learning curves are used to evaluate the performances. Wefound for small sample sizes combining classifiers performs significantlybetter than combining feature spaces (CFS). It is also shown that combiningclassifiers performs better than the support vector machine on CFS inmultiscale texture classification.
arxiv-1500-179 | Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning | http://arxiv.org/pdf/1207.3859v3.pdf | author:Ulugbek S. Kamilov, Sundeep Rangan, Alyson K. Fletcher, Michael Unser category:cs.IT cs.LG math.IT published:2012-07-17 summary:We consider the estimation of an i.i.d. (possibly non-Gaussian) vector $\xbf\in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade modelconsisting of a known linear transform followed by a probabilisticcomponentwise (possibly nonlinear) measurement channel. A novel method, calledadaptive generalized approximate message passing (Adaptive GAMP), that enablesjoint learning of the statistics of the prior and measurement channel alongwith estimation of the unknown vector $\xbf$ is presented. The proposedalgorithm is a generalization of a recently-developed EM-GAMP that usesexpectation-maximization (EM) iterations where the posteriors in the E-stepsare computed via approximate message passing. The methodology can be applied toa large class of learning problems including the learning of sparse priors incompressed sensing or identification of linear-nonlinear cascade models indynamical systems and neural spiking processes. We prove that for large i.i.d.Gaussian transform matrices the asymptotic componentwise behavior of theadaptive GAMP algorithm is predicted by a simple set of scalar state evolutionequations. In addition, we show that when a certain maximum-likelihoodestimation can be performed in each step, the adaptive GAMP method can yieldasymptotically consistent parameter estimates, which implies that the algorithmachieves a reconstruction quality equivalent to the oracle algorithm that knowsthe correct parameter values. Remarkably, this result applies to essentiallyarbitrary parametrizations of the unknown distributions, including ones thatare nonlinear and non-Gaussian. The adaptive GAMP methodology thus provides asystematic, general and computationally efficient method applicable to a largerange of complex linear-nonlinear models with provable guarantees.
arxiv-1500-180 | Ensemble Clustering with Logic Rules | http://arxiv.org/pdf/1207.3961v3.pdf | author:Deniz Akdemir category:stat.ML cs.LG published:2012-07-17 summary:In this article, the logic rule ensembles approach to supervised learning isapplied to the unsupervised or semi-supervised clustering. Logic rules whichwere obtained by combining simple conjunctive rules are used to partition theinput space and an ensemble of these rules is used to define a similaritymatrix. Similarity partitioning is used to partition the data in anhierarchical manner. We have used internal and external measures of clustervalidity to evaluate the quality of clusterings or to identify the number ofclusters.
arxiv-1500-181 | Model Selection for Degree-corrected Block Models | http://arxiv.org/pdf/1207.3994v2.pdf | author:Xiaoran Yan, Cosma Rohilla Shalizi, Jacob E. Jensen, Florent Krzakala, Cristopher Moore, Lenka Zdeborova, Pan Zhang, Yaojia Zhu category:cs.SI math.ST physics.soc-ph stat.ML stat.TH published:2012-07-17 summary:The proliferation of models for networks raises challenging problems of modelselection: the data are sparse and globally dependent, and models are typicallyhigh-dimensional and have large numbers of latent variables. Together, theseissues mean that the usual model-selection criteria do not work properly fornetworks. We illustrate these challenges, and show one way to resolve them, byconsidering the key network-analysis problem of dividing a graph intocommunities or blocks of nodes with homogeneous patterns of links to the restof the network. The standard tool for doing this is the stochastic block model,under which the probability of a link between two nodes is a function solely ofthe blocks to which they belong. This imposes a homogeneous degree distributionwithin each block; this can be unrealistic, so degree-corrected block modelsadd a parameter for each node, modulating its over-all degree. The choicebetween ordinary and degree-corrected block models matters because they makevery different inferences about communities. We present the first principledand tractable approach to model selection between standard and degree-correctedblock models, based on new large-graph asymptotics for the distribution oflog-likelihood ratios under the stochastic block model, finding substantialdepartures from classical results for sparse graphs. We also developlinear-time approximations for log-likelihoods under both the stochastic blockmodel and the degree-corrected model, using belief propagation. Applications tosimulated and real networks show excellent agreement with our approximations.Our results thus both solve the practical problem of deciding on degreecorrection, and point to a general approach to model selection in networkanalysis.
arxiv-1500-182 | Fusing image representations for classification using support vector machines | http://arxiv.org/pdf/1207.3607v1.pdf | author:Can Demirkesen, Hocine Cherifi category:cs.CV cs.LG published:2012-07-16 summary:In order to improve classification accuracy different image representationsare usually combined. This can be done by using two different fusing schemes.In feature level fusion schemes, image representations are combined before theclassification process. In classifier fusion, the decisions taken separatelybased on individual representations are fused to make a decision. In this paperthe main methods derived for both strategies are evaluated. Our experimentalresults show that classifier fusion performs better. Specifically Bayes beliefintegration is the best performing strategy for image classification task.
arxiv-1500-183 | Accuracy Measures for the Comparison of Classifiers | http://arxiv.org/pdf/1207.3790v1.pdf | author:Vincent Labatut, Hocine Cherifi category:cs.LG published:2012-07-16 summary:The selection of the best classification algorithm for a given dataset is avery widespread problem. It is also a complex one, in the sense it requires tomake several important methodological choices. Among them, in this work wefocus on the measure used to assess the classification performance and rank thealgorithms. We present the most popular measures and discuss their properties.Despite the numerous measures proposed over the years, many of them turn out tobe equivalent in this specific case, to have interpretation problems, or to beunsuitable for our purpose. Consequently, classic overall success rate ormarginal rates should be preferred for this specific task.
arxiv-1500-184 | Hierarchical Approach for Total Variation Digital Image Inpainting | http://arxiv.org/pdf/1207.3576v2.pdf | author:S. Padmavathi, N. Archana, K. P. Soman category:cs.CV published:2012-07-16 summary:The art of recovering an image from damage in an undetectable form is knownas inpainting. The manual work of inpainting is most often a very timeconsuming process. Due to digitalization of this technique, it is automatic andfaster. In this paper, after the user selects the regions to be reconstructed,the algorithm automatically reconstruct the lost regions with the help of theinformation surrounding them. The existing methods perform very well when theregion to be reconstructed is very small, but fails in proper reconstruction asthe area increases. This paper describes a Hierarchical method by which thearea to be inpainted is reduced in multiple levels and Total Variation(TV)method is used to inpaint in each level. This algorithm gives betterperformance when compared with other existing algorithms such as nearestneighbor interpolation, Inpainting through Blurring and Sobolev Inpainting.
arxiv-1500-185 | Qualitative Comparison of Community Detection Algorithms | http://arxiv.org/pdf/1207.3603v1.pdf | author:Günce Orman, Vincent Labatut, Hocine Cherifi category:cs.SI cs.CV physics.soc-ph published:2012-07-16 summary:Community detection is a very active field in complex networks analysis,consisting in identifying groups of nodes more densely interconnectedrelatively to the rest of the network. The existing algorithms are usuallytested and compared on real-world and artificial networks, their performancebeing assessed through some partition similarity measure. However, artificialnetworks realism can be questioned, and the appropriateness of those measuresis not obvious. In this study, we take advantage of recent advances concerningthe characterization of community structures to tackle these questions. Wefirst generate networks thanks to the most realistic model available to date.Their analysis reveals they display only some of the properties observed inreal-world community structures. We then apply five community detectionalgorithms on these networks and find out the performance assessedquantitatively does not necessarily agree with a qualitative analysis of theidentified communities. It therefore seems both approaches should be applied toperform a relevant comparison of the algorithms.
arxiv-1500-186 | Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood | http://arxiv.org/pdf/1207.3649v1.pdf | author:Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari category:stat.ML published:2012-07-16 summary:We consider probabilistic multinomial probit classification using Gaussianprocess (GP) priors. The challenges with the multiclass GP classification arethe integration over the non-Gaussian posterior distribution, and the increaseof the number of unknown latent variables as the number of target classesgrows. Expectation propagation (EP) has proven to be a very accurate method forapproximate inference but the existing EP approaches for the multinomial probitGP classification rely on numerical quadratures or independence assumptionsbetween the latent values from different classes to facilitate thecomputations. In this paper, we propose a novel nested EP approach which doesnot require numerical quadratures, and approximates accurately allbetween-class posterior dependencies of the latent values, but still scaleslinearly in the number of classes. The predictive accuracy of the nested EPapproach is compared to Laplace, variational Bayes, and Markov chain MonteCarlo (MCMC) approximations with various benchmark data sets. In theexperiments nested EP was the most consistent method with respect to MCMCsampling, but the differences between the compared methods were small if onlythe classification accuracy is concerned.
arxiv-1500-187 | Surrogate Losses in Passive and Active Learning | http://arxiv.org/pdf/1207.3772v3.pdf | author:Steve Hanneke, Liu Yang category:math.ST cs.LG stat.ML stat.TH published:2012-07-16 summary:Active learning is a type of sequential design for supervised machinelearning, in which the learning algorithm sequentially requests the labels ofselected instances from a large pool of unlabeled data points. The objective isto produce a classifier of relatively low risk, as measured under the 0-1 loss,ideally using fewer label requests than the number of random labeled datapoints sufficient to achieve the same. This work investigates the potentialuses of surrogate loss functions in the context of active learning.Specifically, it presents an active learning algorithm based on an arbitraryclassification-calibrated surrogate loss function, along with an analysis ofthe number of label requests sufficient for the classifier returned by thealgorithm to achieve a given risk under the 0-1 loss. Interestingly, theseresults cannot be obtained by simply optimizing the surrogate risk via activelearning to an extent sufficient to provide a guarantee on the 0-1 loss, as iscommon practice in the analysis of surrogate losses for passive learning. Someof the results have additional implications for the use of surrogate losses inpassive learning.
arxiv-1500-188 | Autofocus Correction of Azimuth Phase Error and Residual Range Cell Migration in Spotlight SAR Polar Format Imagery | http://arxiv.org/pdf/1207.7245v1.pdf | author:Xinhua Mao, Daiyin Zhu, Zhaoda Zhu category:astro-ph.IM cs.CV published:2012-07-16 summary:Synthetic aperture radar (SAR) images are often blurred by phaseperturbations induced by uncompensated sensor motion and /or unknownpropagation effects caused by turbulent media. To get refocused images,autofocus proves to be useful post-processing technique applied to estimate andcompensate the unknown phase errors. However, a severe drawback of theconventional autofocus algorithms is that they are only capable of removingone-dimensional azimuth phase errors (APE). As the resolution becomes finer,residual range cell migration (RCM), which makes the defocus inherentlytwo-dimensional, becomes a new challenge. In this paper, correction of APE andresidual RCM are presented in the framework of polar format algorithm (PFA).First, an insight into the underlying mathematical mechanism of polarreformatting is presented. Then based on this new formulation, the effect ofpolar reformatting on the uncompensated APE and residual RCM is investigated indetail. By using the derived analytical relationship between APE and residualRCM, an efficient two-dimensional (2-D) autofocus method is proposed.Experimental results indicate the effectiveness of the proposed method.
arxiv-1500-189 | Diagnosing client faults using SVM-based intelligent inference from TCP packet traces | http://arxiv.org/pdf/1207.3560v1.pdf | author:Chathuranga Widanapathirana, Y. Ahmet Sekercioglu, Paul G. Fitzpatrick, Milosh V. Ivanovich, Jonathan C. Li category:cs.NI cs.AI cs.LG published:2012-07-16 summary:We present the Intelligent Automated Client Diagnostic (IACD) system, whichonly relies on inference from Transmission Control Protocol (TCP) packet tracesfor rapid diagnosis of client device problems that cause network performanceissues. Using soft-margin Support Vector Machine (SVM) classifiers, the system(i) distinguishes link problems from client problems, and (ii) identifiescharacteristics unique to client faults to report the root cause of the clientdevice problem. Experimental evaluation demonstrated the capability of the IACDsystem to distinguish between faulty and healthy links and to diagnose theclient faults with 98% accuracy in healthy links. The system can perform faultdiagnosis independent of the client's specific TCP implementation, enablingdiagnosis capability on diverse range of client computers.
arxiv-1500-190 | Image Labeling on a Network: Using Social-Network Metadata for Image Classification | http://arxiv.org/pdf/1207.3809v1.pdf | author:Julian McAuley, Jure Leskovec category:cs.CV cs.SI physics.soc-ph published:2012-07-16 summary:Large-scale image retrieval benchmarks invariably consist of images from theWeb. Many of these benchmarks are derived from online photo sharing networks,like Flickr, which in addition to hosting images also provide a highlyinteractive social community. Such communities generate rich metadata that cannaturally be harnessed for image classification and retrieval. Here we studyfour popular benchmark datasets, extending them with social-network metadata,such as the groups to which each image belongs, the comment thread associatedwith the image, who uploaded it, their location, and their network of friends.Since these types of data are inherently relational, we propose a model thatexplicitly accounts for the interdependencies between images sharing commonproperties. We model the task as a binary labeling problem on a network, anduse structured learning techniques to learn model parameters. We find thatsocial-network metadata are useful in a variety of classification tasks, inmany cases outperforming methods based on image content.
arxiv-1500-191 | MARFCAT: Transitioning to Binary and Larger Data Sets of SATE IV | http://arxiv.org/pdf/1207.3718v2.pdf | author:Serguei A. Mokhov, Joey Paquet, Mourad Debbabi, Yankui Sun category:cs.CR cs.PL cs.SE stat.ML K.6.5; D.3 published:2012-07-16 summary:We present a second iteration of a machine learning approach to static codeanalysis and fingerprinting for weaknesses related to security, softwareengineering, and others using the open-source MARF framework and the MARFCATapplication based on it for the NIST's SATE IV static analysis tool expositionworkshop's data sets that include additional test cases, including new largesynthetic cases. To aid detection of weak or vulnerable code, including sourceor binary on different platforms the machine learning approach proved to befast and accurate to for such tasks where other tools are either much slower orhave much smaller recall of known vulnerabilities. We use signal and NLPprocessing techniques in our approach to accomplish the identification andclassification tasks. MARFCAT's design from the beginning in 2010 made isindependent of the language being analyzed, source code, bytecode, or binary.In this follow up work with explore some preliminary results in this area. Weevaluated also additional algorithms that were used to process the data.
arxiv-1500-192 | Designing various component analysis at will | http://arxiv.org/pdf/1207.3554v2.pdf | author:Akisato Kimura, Masashi Sugiyama, Sakano Hitoshi, Hirokazu Kameoka category:cs.CV cs.NA stat.ME stat.ML published:2012-07-16 summary:This paper provides a generic framework of component analysis (CA) methodsintroducing a new expression for scatter matrices and Gram matrices, calledGeneralized Pairwise Expression (GPE). This expression is quite compact buthighly powerful: The framework includes not only (1) the standard CA methodsbut also (2) several regularization techniques, (3) weighted extensions, (4)some clustering methods, and (5) their semi-supervised extensions. This paperalso presents quite a simple methodology for designing a desired CA method fromthe proposed framework: Adopting the known GPEs as templates, and generating anew method by combining these templates appropriately.
arxiv-1500-193 | Towards a Self-Organized Agent-Based Simulation Model for Exploration of Human Synaptic Connections | http://arxiv.org/pdf/1207.3760v1.pdf | author:Önder Gürcan, Carole Bernon, Kemal S. Türker category:cs.NE cs.AI cs.LG nlin.AO published:2012-07-16 summary:In this paper, the early design of our self-organized agent-based simulationmodel for exploration of synaptic connections that faithfully generates what isobserved in natural situation is given. While we take inspiration fromneuroscience, our intent is not to create a veridical model of processes inneurodevelopmental biology, nor to represent a real biological system. Instead,our goal is to design a simulation model that learns acting in the same way ofhuman nervous system by using findings on human subjects using reflexmethodologies in order to estimate unknown connections.
arxiv-1500-194 | Learning to rank from medical imaging data | http://arxiv.org/pdf/1207.3598v2.pdf | author:Fabian Pedregosa, Alexandre Gramfort, Gaël Varoquaux, Elodie Cauvet, Christophe Pallier, Bertrand Thirion category:cs.LG cs.CV published:2012-07-16 summary:Medical images can be used to predict a clinical score coding for theseverity of a disease, a pain level or the complexity of a cognitive task. Inall these cases, the predicted variable has a natural order. While a standardclassifier discards this information, we would like to take it into account inorder to improve prediction performance. A standard linear regression doesmodel such information, however the linearity assumption is likely not besatisfied when predicting from pixel intensities in an image. In this paper weaddress these modeling challenges with a supervised learning procedure wherethe model aims to order or rank images. We use a linear model for itsrobustness in high dimension and its possible interpretation. We show onsimulations and two fMRI datasets that this approach is able to predict thecorrect ordering on pairs of images, yielding higher prediction accuracy thanstandard regression and multiclass classification techniques.
arxiv-1500-195 | Improved brain pattern recovery through ranking approaches | http://arxiv.org/pdf/1207.3520v1.pdf | author:Fabian Pedregosa, Alexandre Gramfort, Gaël Varoquaux, Bertrand Thirion, Christophe Pallier, Elodie Cauvet category:cs.LG stat.ML published:2012-07-15 summary:Inferring the functional specificity of brain regions from functionalMagnetic Resonance Images (fMRI) data is a challenging statistical problem.While the General Linear Model (GLM) remains the standard approach for brainmapping, supervised learning techniques (a.k.a.} decoding) have proven to beuseful to capture multivariate statistical effects distributed across voxelsand brain regions. Up to now, much effort has been made to improve decoding byincorporating prior knowledge in the form of a particular regularization term.In this paper we demonstrate that further improvement can be made by accountingfor non-linearities using a ranking approach rather than the commonly usedleast-square regression. Through simulation, we compare the recovery propertiesof our approach to linear models commonly used in fMRI based decoding. Wedemonstrate the superiority of ranking with a real fMRI dataset.
arxiv-1500-196 | Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models | http://arxiv.org/pdf/1207.3538v3.pdf | author:Quan Wang category:cs.CV published:2012-07-15 summary:Principal component analysis (PCA) is a popular tool for lineardimensionality reduction and feature extraction. Kernel PCA is the nonlinearform of PCA, which better exploits the complicated spatial structure ofhigh-dimensional features. In this paper, we first review the basic ideas ofPCA and kernel PCA. Then we focus on the reconstruction of pre-images forkernel PCA. We also give an introduction on how PCA is used in active shapemodels (ASMs), and discuss how kernel PCA can be applied to improve traditionalASMs. Then we show some experimental results to compare the performance ofkernel PCA and standard PCA for classification problems. We also implement thekernel PCA-based ASMs, and use it to construct human face models.
arxiv-1500-197 | HMRF-EM-image: Implementation of the Hidden Markov Random Field Model and its Expectation-Maximization Algorithm | http://arxiv.org/pdf/1207.3510v2.pdf | author:Quan Wang category:cs.CV published:2012-07-15 summary:In this project, we study the hidden Markov random field (HMRF) model and itsexpectation-maximization (EM) algorithm. We implement a MATLAB toolbox namedHMRF-EM-image for 2D image segmentation using the HMRF-EM framework. Thistoolbox also implements edge-prior-preserving image segmentation, and can beeasily reconfigured for other problems, such as 3D image segmentation.
arxiv-1500-198 | Approximated Computation of Belief Functions for Robust Design Optimization | http://arxiv.org/pdf/1207.3442v1.pdf | author:Massimiliano Vasile, Edmondo Minisci, Quirien Wijnands category:cs.CE cs.NE cs.SY math.OC math.PR published:2012-07-14 summary:This paper presents some ideas to reduce the computational cost ofevidence-based robust design optimization. Evidence Theory crystallizes boththe aleatory and epistemic uncertainties in the design parameters, providingtwo quantitative measures, Belief and Plausibility, of the credibility of thecomputed value of the design budgets. The paper proposes some techniques tocompute an approximation of Belief and Plausibility at a cost that is afraction of the one required for an accurate calculation of the two values.Some simple test cases will show how the proposed techniques scale with thedimension of the problem. Finally a simple example of spacecraft system designis presented.
arxiv-1500-199 | Incremental Learning of 3D-DCT Compact Representations for Robust Visual Tracking | http://arxiv.org/pdf/1207.3389v2.pdf | author:Xi Li, Anthony Dick, Chunhua Shen, Anton van den Hengel, Hanzi Wang category:cs.CV cs.LG published:2012-07-14 summary:Visual tracking usually requires an object appearance model that is robust tochanging illumination, pose and other factors encountered in video. In thispaper, we construct an appearance model using the 3D discrete cosine transform(3D-DCT). The 3D-DCT is based on a set of cosine basis functions, which aredetermined by the dimensions of the 3D signal and thus independent of the inputvideo data. In addition, the 3D-DCT can generate a compact energy spectrumwhose high-frequency coefficients are sparse if the appearance samples aresimilar. By discarding these high-frequency coefficients, we simultaneouslyobtain a compact 3D-DCT based object representation and a signalreconstruction-based similarity measure (reflecting the information loss fromsignal reconstruction). To efficiently update the object representation, wepropose an incremental 3D-DCT algorithm, which decomposes the 3D-DCT intosuccessive operations of the 2D discrete cosine transform (2D-DCT) and 1Ddiscrete cosine transform (1D-DCT) on the input video data.
arxiv-1500-200 | MahNMF: Manhattan Non-negative Matrix Factorization | http://arxiv.org/pdf/1207.3438v1.pdf | author:Naiyang Guan, Dacheng Tao, Zhigang Luo, John Shawe-Taylor category:stat.ML cs.LG cs.NA 65K10 published:2012-07-14 summary:Non-negative matrix factorization (NMF) approximates a non-negative matrix$X$ by a product of two non-negative low-rank factor matrices $W$ and $H$. NMFand its extensions minimize either the Kullback-Leibler divergence or theEuclidean distance between $X$ and $W^T H$ to model the Poisson noise or theGaussian noise. In practice, when the noise distribution is heavy tailed, theycannot perform well. This paper presents Manhattan NMF (MahNMF) which minimizesthe Manhattan distance between $X$ and $W^T H$ for modeling the heavy tailedLaplacian noise. Similar to sparse and low-rank matrix decompositions, MahNMFrobustly estimates the low-rank part and the sparse part of a non-negativematrix and thus performs effectively when data are contaminated by outliers. Weextend MahNMF for various practical applications by developing box-constrainedMahNMF, manifold regularized MahNMF, group sparse MahNMF, elastic net inducingMahNMF, and symmetric MahNMF. The major contribution of this paper lies in twofast optimization algorithms for MahNMF and its extensions: the rank-oneresidual iteration (RRI) method and Nesterov's smoothing method. In particular,by approximating the residual matrix by the outer product of one row of W andone row of $H$ in MahNMF, we develop an RRI method to iteratively update eachvariable of $W$ and $H$ in a closed form solution. Although RRI is efficientfor small scale MahNMF and some of its extensions, it is neither scalable tolarge scale matrices nor flexible enough to optimize all MahNMF extensions.Since the objective functions of MahNMF and its extensions are neither convexnor smooth, we apply Nesterov's smoothing method to recursively optimize onefactor matrix with another matrix fixed. By setting the smoothing parameterinversely proportional to the iteration number, we improve the approximationaccuracy iteratively for both MahNMF and its extensions.
arxiv-1500-201 | Robust Mission Design Through Evidence Theory and Multi-Agent Collaborative Search | http://arxiv.org/pdf/1207.3437v1.pdf | author:Massimiliano Vasile category:cs.CE cs.NE cs.SY math.OC math.PR published:2012-07-14 summary:In this paper, the preliminary design of a space mission is approachedintroducing uncertainties on the design parameters and formulating theresulting reliable design problem as a multiobjective optimization problem.Uncertainties are modelled through evidence theory and the belief, orcredibility, in the successful achievement of mission goals is maximised alongwith the reliability of constraint satisfaction. The multiobjectiveoptimisation problem is solved through a novel algorithm based on thecollaboration of a population of agents in search for the set of highlyreliable solutions. Two typical problems in mission analysis are used toillustrate the proposed methodology.
arxiv-1500-202 | Scaling of Model Approximation Errors and Expected Entropy Distances | http://arxiv.org/pdf/1207.3399v2.pdf | author:Guido F. Montufar, Johannes Rauh category:stat.ML 94A17, 62B15 published:2012-07-14 summary:We compute the expected value of the Kullback-Leibler divergence to variousfundamental statistical models with respect to canonical priors on theprobability simplex. We obtain closed formulas for the expected modelapproximation errors, depending on the dimension of the models and thecardinalities of their sample spaces. For the uniform prior, the expecteddivergence from any model containing the uniform distribution is bounded by aconstant $1-\gamma$, and for the models that we consider, this bound isapproached if the state space is very large and the models' dimension does notgrow too fast. For Dirichlet priors the expected divergence is bounded in asimilar way, if the concentration parameters take reasonable values. Theseresults serve as reference values for more complicated statistical models.
arxiv-1500-203 | Dimension Reduction by Mutual Information Feature Extraction | http://arxiv.org/pdf/1207.3394v1.pdf | author:Ali Shadvar category:cs.LG cs.CV published:2012-07-14 summary:During the past decades, to study high-dimensional data in a large variety ofproblems, researchers have proposed many Feature Extraction algorithms. One ofthe most effective approaches for optimal feature extraction is based on mutualinformation (MI). However it is not always easy to get an accurate estimationfor high dimensional MI. In terms of MI, the optimal feature extraction iscreating a feature set from the data which jointly have the largest dependencyon the target class and minimum redundancy. In this paper, acomponent-by-component gradient ascent method is proposed for featureextraction which is based on one-dimensional MI estimates. We will refer tothis algorithm as Mutual Information Feature Extraction (MIFX). The performanceof this proposed method is evaluated using UCI databases. The results indicatethat MIFX provides a robust performance over different data sets which arealmost always the best or comparable to the best ones.
arxiv-1500-204 | Tracking Tetrahymena Pyriformis Cells using Decision Trees | http://arxiv.org/pdf/1207.3127v1.pdf | author:Quan Wang, Yan Ou, A. Agung Julius, Kim L. Boyer, Min Jun Kim category:cs.CV published:2012-07-13 summary:Matching cells over time has long been the most difficult step in celltracking. In this paper, we approach this problem by recasting it as aclassification problem. We construct a feature set for each cell, and compute afeature difference vector between a cell in the current frame and a cell in aprevious frame. Then we determine whether the two cells represent the same cellover time by training decision trees as our binary classifiers. With the outputof decision trees, we are able to formulate an assignment problem for our cellassociation task and solve it using a modified version of the Hungarianalgorithm.
arxiv-1500-205 | Learning the Pseudoinverse Solution to Network Weights | http://arxiv.org/pdf/1207.3368v1.pdf | author:Jonathan Tapson, Andre van Schaik category:cs.NE published:2012-07-13 summary:The last decade has seen the parallel emergence in computational neuroscienceand machine learning of neural network structures which spread the input signalrandomly to a higher dimensional space; perform a nonlinear activation; andthen solve for a regression or classification output by means of a mathematicalpseudoinverse operation. In the field of neuromorphic engineering, thesemethods are increasingly popular for synthesizing biologically plausible neuralnetworks, but the "learning method" - computation of the pseudoinverse bysingular value decomposition - is problematic both for biological plausibilityand because it is not an online or an adaptive method. We present an online orincremental method of computing the pseudoinverse, which we argue isbiologically plausible as a learning method, and which can be made adaptablefor non-stationary data streams. The method is significantly morememory-efficient than the conventional computation of pseudoinverses bysingular value decomposition.
arxiv-1500-206 | Color Constancy based on Image Similarity via Bilayer Sparse Coding | http://arxiv.org/pdf/1207.3142v2.pdf | author:Bing Li, Weihua Xiong, Weiming Hu category:cs.CV published:2012-07-13 summary:Computational color constancy is a very important topic in computer visionand has attracted many researchers' attention. Recently, lots of research hasshown the effects of high level visual content information for illuminationestimation. However, all of these existing methods are essentiallycombinational strategies in which image's content analysis is only used toguide the combination or selection from a variety of individual illuminationestimation methods. In this paper, we propose a novel bilayer sparse codingmodel for illumination estimation that considers image similarity in terms ofboth low level color distribution and high level image scene contentsimultaneously. For the purpose, the image's scene content information isintegrated with its color distribution to obtain optimal illuminationestimation model. The experimental results on two real-world image sets showthat our algorithm is superior to other prevailing illumination estimationmethods, even better than combinational methods.
arxiv-1500-207 | The Price of Privacy in Untrusted Recommendation Engines | http://arxiv.org/pdf/1207.3269v2.pdf | author:Siddhartha Banerjee, Nidhi Hegde, Laurent Massoulié category:cs.LG cs.IT math.IT published:2012-07-13 summary:Recent increase in online privacy concerns prompts the following question:can a recommender system be accurate if users do not entrust it with theirprivate data? To answer this, we study the problem of learning item-clustersunder local differential privacy, a powerful, formal notion of data privacy. Wedevelop bounds on the sample-complexity of learning item-clusters fromprivatized user inputs. Significantly, our results identify a sample-complexityseparation between learning in an information-rich and an information-scarceregime, thereby highlighting the interaction between privacy and the amount ofinformation (ratings) available to each user. In the information-rich regime, where each user rates at least a constantfraction of items, a spectral clustering approach is shown to achieve asample-complexity lower bound derived from a simple information-theoreticargument based on Fano's inequality. However, the information-scarce regime,where each user rates only a vanishing fraction of items, is found to require afundamentally different approach both for lower bounds and algorithms. To thisend, we develop new techniques for bounding mutual information under a notionof channel-mismatch, and also propose a new algorithm, MaxSense, and show thatit achieves optimal sample-complexity in this setting. The techniques we develop for bounding mutual information may be of broaderinterest. To illustrate this, we show their applicability to $(i)$ learningbased on 1-bit sketches, and $(ii)$ adaptive learning, where queries can beadapted based on answers to past queries.
arxiv-1500-208 | Deconvolution of vibroacoustic images using a simulation model based on a three dimensional point spread function | http://arxiv.org/pdf/1207.3370v1.pdf | author:Talita Perciano, Matthew Urban, Nelson D. A. Mascarenhas, Mostafa Fatemi, Alejandro C. Frery, Glauber T. Silva category:cs.CV published:2012-07-13 summary:Vibro-acoustography (VA) is a medical imaging method based on thedifference-frequency generation produced by the mixture of two focusedultrasound beams. VA has been applied to different problems in medical imagingsuch as imaging bones, microcalcifications in the breast, mass lesions, andcalcified arteries. The obtained images may have a resolution of 0.7--0.8 mm.Current VA systems based on confocal or linear array transducers generateC-scan images at the beam focal plane. Images on the axial plane are alsopossible, however the system resolution along depth worsens when compared tothe lateral one. Typical axial resolution is about 1.0 cm. Furthermore, theelevation resolution of linear array systems is larger than that in lateraldirection. This asymmetry degrades C-scan images obtained using linear arrays.The purpose of this article is to study VA image restoration based on a 3Dpoint spread function (PSF) using classical deconvolution algorithms: Wiener,constrained least-squares (CLSs), and geometric mean filters. To assess thefilters' performance, we use an image quality index that accounts forcorrelation loss, luminance and contrast distortion. Results for simulated VAimages show that the quality index achieved with the Wiener filter is 0.9 (1indicates perfect restoration). This filter yielded the best result incomparison with the other ones. Moreover, the deconvolution algorithms wereapplied to an experimental VA image of a phantom composed of three stretched0.5 mm wires. Experiments were performed using transducer driven at twofrequencies, 3075 kHz and 3125 kHz, which resulted in the difference-frequencyof 50 kHz. Restorations with the theoretical line spread function (LSF) did notrecover sufficient information to identify the wires in the images. However,using an estimated LSF the obtained results displayed enough information tospot the wires in the images.
arxiv-1500-209 | The law of brevity in macaque vocal communication is not an artifact of analyzing mean call durations | http://arxiv.org/pdf/1207.3169v2.pdf | author:Stuart Semple, Minna J. Hsu, Govindasamy Agoramoorthy, Ramon Ferrer-i-Cancho category:q-bio.NC cs.CL published:2012-07-13 summary:Words follow the law of brevity, i.e. more frequent words tend to be shorter.From a statistical point of view, this qualitative definition of the law statesthat word length and word frequency are negatively correlated. Here the recentfinding of patterning consistent with the law of brevity in Formosan macaquevocal communication (Semple et al., 2010) is revisited. It is shown that thenegative correlation between mean duration and frequency of use in thevocalizations of Formosan macaques is not an artifact of the use of a meanduration for each call type instead of the customary 'word' length of studiesof the law in human language. The key point demonstrated is that the totalduration of calls of a particular type increases with the number of calls ofthat type. The finding of the law of brevity in the vocalizations of thesemacaques therefore defies a trivial explanation.
arxiv-1500-210 | Hypothesis Testing in Speckled Data with Stochastic Distances | http://arxiv.org/pdf/1207.2959v1.pdf | author:Abraão D. C. Nascimento, Renato J. Cintra, Alejandro C. Frery category:stat.ML cs.GR published:2012-07-12 summary:Images obtained with coherent illumination, as is the case of sonar,ultrasound-B, laser and Synthetic Aperture Radar -- SAR, are affected byspeckle noise which reduces the ability to extract information from the data.Specialized techniques are required to deal with such imagery, which has beenmodeled by the G0 distribution and under which regions with different degreesof roughness and mean brightness can be characterized by two parameters; athird parameter, the number of looks, is related to the overall signal-to-noiseratio. Assessing distances between samples is an important step in imageanalysis; they provide grounds of the separability and, therefore, of theperformance of classification procedures. This work derives and compares eightstochastic distances and assesses the performance of hypothesis tests thatemploy them and maximum likelihood estimation. We conclude that tests based onthe triangular distance have the closest empirical size to the theoretical one,while those based on the arithmetic-geometric distances have the best power.Since the power of tests based on the triangular distance is close to optimum,we conclude that the safest choice is using this distance for hypothesistesting, even when compared with classical distances as Kullback-Leibler andBhattacharyya.
arxiv-1500-211 | Biogeography-Based Informative Gene Selection and Cancer Classification Using SVM and Random Forests | http://arxiv.org/pdf/1207.3285v1.pdf | author:Sarvesh Nikumbh, Shameek Ghosh, Valadi Jayaraman category:cs.NE stat.ML published:2012-07-12 summary:Microarray cancer gene expression data comprise of very high dimensions.Reducing the dimensions helps in improving the overall analysis andclassification performance. We propose two hybrid techniques, Biogeography -based Optimization - Random Forests (BBO - RF) and BBO - SVM (Support VectorMachines) with gene ranking as a heuristic, for microarray gene expressionanalysis. This heuristic is obtained from information gain filter rankingprocedure. The BBO algorithm generates a population of candidate subset ofgenes, as part of an ecosystem of habitats, and employs the migration andmutation processes across multiple generations of the population to improve theclassification accuracy. The fitness of each gene subset is assessed by theclassifiers - SVM and Random Forests. The performances of these hybridtechniques are evaluated on three cancer gene expression datasets retrievedfrom the Kent Ridge Biomedical datasets collection and the libSVM datarepository. Our results demonstrate that genes selected by the proposedtechniques yield classification accuracies comparable to previously reportedalgorithms.
arxiv-1500-212 | Probabilistic index maps for modeling natural signals | http://arxiv.org/pdf/1207.4179v1.pdf | author:Nebojsa Jojic, Yaron Caspi, Manuel Reyes-Gomez category:cs.CV published:2012-07-12 summary:One of the major problems in modeling natural signals is that signals withvery similar structure may locally have completely different measurements,e.g., images taken under different illumination conditions, or the speechsignal captured in different environments. While there have been manysuccessful attempts to address these problems in application-specific settings,we believe that underlying a large set of problems in signal representation isa representational deficiency of intensity-derived local measurements that arethe basis of most efficient models. We argue that interesting structure insignals is better captured when the signal is de- fined as a matrix whoseentries are discrete indices to a separate palette of possible measurements. Inorder to model the variability in signal structure, we define a signal classnot by a single index map, but by a probability distribution over the indexmaps, which can be estimated from the data, and which we call probabilisticindex maps. The existing algorithm can be adapted to work with thisrepresentation. Furthermore, the probabilistic index map representation leadsto algorithms with computational costs proportional to either the size of thepalette or the log of the size of the palette, making the cost of significantlyincreased invariance to non-structural changes quite bearable. We illustratethe benefits of the probabilistic index map representation in severalapplications in computer vision and speech processing.
arxiv-1500-213 | A Hierarchical Graphical Model for Record Linkage | http://arxiv.org/pdf/1207.4180v1.pdf | author:Pradeep Ravikumar, William Cohen category:cs.LG cs.IR stat.ML published:2012-07-12 summary:The task of matching co-referent records is known among other names as rocordlinkage. For large record-linkage problems, often there is little or no labeleddata available, but unlabeled data shows a reasonable clear structure. For suchproblems, unsupervised or semi-supervised methods are preferable to supervisedmethods. In this paper, we describe a hierarchical graphical model frameworkfor the linakge-problem in an unsupervised setting. In addition to proposingnew methods, we also cast existing unsupervised probabilistic record-linkagemethods in this framework. Some of the techniques we propose to minimizeoverfitting in the above model are of interest in the general graphical modelsetting. We describe a method for incorporating monotinicity constraints in agraphical model. We also outline a bootstrapping approach of using"single-field" classifiers to noisily label latent variables in a hierarchicalmodel. Experimental results show that our proposed unsupervised methods performquite competitively even with fully supervised record-linkage methods.
arxiv-1500-214 | Distributed Strongly Convex Optimization | http://arxiv.org/pdf/1207.3031v2.pdf | author:Konstantinos I. Tsianos, Michael G. Rabbat category:cs.DC cs.LG stat.ML published:2012-07-12 summary:A lot of effort has been invested into characterizing the convergence ratesof gradient based algorithms for non-linear convex optimization. Recently,motivated by large datasets and problems in machine learning, the interest hasshifted towards distributed optimization. In this work we present a distributedalgorithm for strongly convex constrained optimization. Each node in a networkof n computers converges to the optimum of a strongly convex, L-Lipchitzcontinuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is thenumber of iterations. This rate is achieved in the online setting where thedata is revealed one at a time to the nodes, and in the batch setting whereeach node has access to its full local dataset from the start. The sameconvergence rate is achieved in expectation when the subgradients used at eachnode are corrupted with additive zero-mean noise.
arxiv-1500-215 | Expectation Propagation in Gaussian Process Dynamical Systems: Extended Version | http://arxiv.org/pdf/1207.2940v4.pdf | author:Marc Peter Deisenroth, Shakir Mohamed category:stat.ML cs.LG cs.SY published:2012-07-12 summary:Rich and complex time-series data, such as those generated from engineeringsystems, financial markets, videos or neural recordings, are now a commonfeature of modern data analysis. Explaining the phenomena underlying thesediverse data sets requires flexible and accurate models. In this paper, wepromote Gaussian process dynamical systems (GPDS) as a rich model class that isappropriate for such analysis. In particular, we present a message passingalgorithm for approximate inference in GPDSs based on expectation propagation.By posing inference as a general message passing problem, we iterateforward-backward smoothing. Thus, we obtain more accurate posteriordistributions over latent structures, resulting in improved predictiveperformance compared to state-of-the-art GPDS smoothers, which are specialcases of our general message passing algorithm. Hence, we provide a unifyingapproach within which to contextualize message passing in GPDSs.
arxiv-1500-216 | ROI Segmentation for Feature Extraction from Human Facial Images | http://arxiv.org/pdf/1207.2922v1.pdf | author:Surbhi, Vishal Arora category:cs.CV cs.HC published:2012-07-12 summary:Human Computer Interaction (HCI) is the biggest goal of computer visionresearchers. Features form the different facial images are able to provide avery deep knowledge about the activities performed by the different facialmovements. In this paper we presented a technique for feature extraction fromvarious regions of interest with the help of Skin color segmentation technique,Thresholding, knowledge based technique for face recognition.
arxiv-1500-217 | Non-Local Euclidean Medians | http://arxiv.org/pdf/1207.3056v2.pdf | author:Kunal N. Chaudhury, Amit Singer category:cs.CV cs.DS published:2012-07-12 summary:In this letter, we note that the denoising performance of Non-Local Means(NLM) at large noise levels can be improved by replacing the mean by theEuclidean median. We call this new denoising algorithm the Non-Local EuclideanMedians (NLEM). At the heart of NLEM is the observation that the median is morerobust to outliers than the mean. In particular, we provide a simple geometricinsight that explains why NLEM performs better than NLM in the vicinity ofedges, particularly at large noise levels. NLEM can be efficiently implementedusing iteratively reweighted least squares, and its computational complexity iscomparable to that of NLM. We provide some preliminary results to study theproposed algorithm and to compare it with NLM.
arxiv-1500-218 | Near-Optimal Algorithms for Differentially-Private Principal Components | http://arxiv.org/pdf/1207.2812v3.pdf | author:Kamalika Chaudhuri, Anand D. Sarwate, Kaushik Sinha category:stat.ML cs.CR cs.LG published:2012-07-12 summary:Principal components analysis (PCA) is a standard tool for identifying goodlow-dimensional approximations to data in high dimension. Many data sets ofinterest contain private or sensitive information about individuals. Algorithmswhich operate on such data should be sensitive to the privacy risks inpublishing their outputs. Differential privacy is a framework for developingtradeoffs between privacy and the utility of these outputs. In this paper weinvestigate the theory and empirical performance of differentially privateapproximations to PCA and propose a new method which explicitly optimizes theutility of the output. We show that the sample complexity of the proposedmethod differs from the existing procedure in the scaling with the datadimension, and that our method is nearly optimal in terms of this scaling. Wefurthermore illustrate our results, showing that on real data there is a largeperformance gap between the existing method and our method.
arxiv-1500-219 | Optimal rates for first-order stochastic convex optimization under Tsybakov noise condition | http://arxiv.org/pdf/1207.3012v2.pdf | author:Aaditya Ramdas, Aarti Singh category:cs.LG stat.ML published:2012-07-12 summary:We focus on the problem of minimizing a convex function $f$ over a convex set$S$ given $T$ queries to a stochastic first order oracle. We argue that thecomplexity of convex minimization is only determined by the rate of growth ofthe function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-likenoise condition. Specifically, we prove that if $f$ grows at least as fast as$\x-x^*_{f,S}\^\kappa$ around its minimum, for some $\kappa > 1$, then theoptimal rate of learning $f(x^*_{f,S})$ is$\Theta(T^{-\frac{\kappa}{2\kappa-2}})$. The classic rate $\Theta(1/\sqrt T)$for convex functions and $\Theta(1/T)$ for strongly convex functions arespecial cases of our result for $\kappa \rightarrow \infty$ and $\kappa=2$, andeven faster rates are attained for $\kappa <2$. We also derive tight bounds forthe complexity of learning $x_{f,S}^*$, where the optimal rate is$\Theta(T^{-\frac{1}{2\kappa-2}})$. Interestingly, these precise rates forconvex optimization also characterize the complexity of active learning and ourresults further strengthen the connections between the two fields, both ofwhich rely on feedback-driven queries.
arxiv-1500-220 | Supervised Texture Classification Using a Novel Compression-Based Similarity Measure | http://arxiv.org/pdf/1207.3071v2.pdf | author:Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel category:cs.CV cs.LG published:2012-07-12 summary:Supervised pixel-based texture classification is usually performed in thefeature space. We propose to perform this task in (dis)similarity space byintroducing a new compression-based (dis)similarity measure. The proposedmeasure utilizes two dimensional MPEG-1 encoder, which takes into considerationthe spatial locality and connectivity of pixels in the images. The proposedformulation has been carefully designed based on MPEG encoder functionality. Tothis end, by design, it solely uses P-frame coding to find the (dis)similarityamong patches/images. We show that the proposed measure works properly on bothsmall and large patch sizes. Experimental results show that the proposedapproach significantly improves the performance of supervised pixel-basedtexture classification on Brodatz and outdoor images compared to othercompression-based dissimilarity measures as well as approaches performed infeature space. It also improves the computation speed by about 40% compared toits rivals.
arxiv-1500-221 | Camera identification by grouping images from database, based on shared noise patterns | http://arxiv.org/pdf/1207.2641v2.pdf | author:Teun Baar, Wiger van Houten, Zeno Geradts category:cs.CV published:2012-07-11 summary:Previous research showed that camera specific noise patterns, so-calledPRNU-patterns, are extracted from images and related images could be found. Inthis particular research the focus is on grouping images from a database, basedon a shared noise pattern as an identification method for cameras. Using themethod as described in this article, groups of images, created using the samecamera, could be linked from a large database of images. Using MATLABprogramming, relevant image noise patterns are extracted from images muchquicker than common methods by the use of faster noise extraction filters andimprovements to reduce the calculation costs. Relating noise patterns, with acorrelation above a certain threshold value, can quickly be matched. Hereby,from a database of images, groups of relating images could be linked and themethod could be used to scan a large number of images for suspect noisepatterns.
arxiv-1500-222 | Clustering based approach extracting collocations | http://arxiv.org/pdf/1207.2714v1.pdf | author:Mohamed Achraf Ben Mohamed, Mounir Zrigui, Mohsen Maraoui category:cs.CL published:2012-07-11 summary:The following study presents a collocation extraction approach based onclustering technique. This study uses a combination of several classicalmeasures which cover all aspects of a given corpus then it suggests separatingbigrams found in the corpus in several disjoint groups according to theprobability of presence of collocations. This will allow excluding groups wherethe presence of collocations is very unlikely and thus reducing in a meaningfulway the search space.
arxiv-1500-223 | Genetic agent approach for improving on-the-fly web map generalization | http://arxiv.org/pdf/1207.2697v1.pdf | author:Brahim lejdel, Okba kazar category:cs.MA cs.CG cs.NE published:2012-07-11 summary:The utilization of web mapping becomes increasingly important in the domainof cartography. Users want access to spatial data on the web specific to theirneeds. For this reason, different approaches were appeared for generatingon-the-fly the maps demanded by users, but those not suffice for guide aflexible and efficient process. Thus, new approach must be developed forimproving this process according to the user needs. This work focuses ondefining a new strategy which improves on-the-fly map generalization processand resolves the spatial conflicts. This approach uses the multiplerepresentation and cartographic generalization. The map generalization processis based on the implementation of multi- agent system where each agent wasequipped with a genetic patrimony.
arxiv-1500-224 | Variational Chernoff Bounds for Graphical Models | http://arxiv.org/pdf/1207.4172v1.pdf | author:Pradeep Ravikumar, John Lafferty category:cs.LG stat.ML published:2012-07-11 summary:Recent research has made significant progress on the problem of bounding logpartition functions for exponential family graphical models. Such bounds haveassociated dual parameters that are often used as heuristic estimates of themarginal probabilities required in inference and learning. However thesevariational estimates do not give rigorous bounds on marginal probabilities,nor do they give estimates for probabilities of more general events than simplemarginals. In this paper we build on this recent work by deriving rigorousupper and lower bounds on event probabilities for graphical models. Ourapproach is based on the use of generalized Chernoff bounds to express boundson event probabilities in terms of convex optimization problems; theseoptimization problems, in turn, require estimates of generalized log partitionfunctions. Simulations indicate that this technique can result in useful,rigorous bounds to complement the heuristic variational estimates, withcomparable computational cost.
arxiv-1500-225 | The Author-Topic Model for Authors and Documents | http://arxiv.org/pdf/1207.4169v1.pdf | author:Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, Padhraic Smyth category:cs.IR cs.LG stat.ML published:2012-07-11 summary:We introduce the author-topic model, a generative model for documents thatextends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to includeauthorship information. Each author is associated with a multinomialdistribution over topics and each topic is associated with a multinomialdistribution over words. A document with multiple authors is modeled as adistribution over topics that is a mixture of the distributions associated withthe authors. We apply the model to a collection of 1,700 NIPS conference papersand 160,000 CiteSeer abstracts. Exact inference is intractable for thesedatasets and we use Gibbs sampling to estimate the topic and authordistributions. We compare the performance with two other generative models fordocuments, which are special cases of the author-topic model: LDA (a topicmodel) and a simple author model in which each author is associated with adistribution over words rather than a distribution over topics. We show topicsrecovered by the author-topic model, and demonstrate applications to computingsimilarity between authors and entropy of author output.
arxiv-1500-226 | Predictive State Representations: A New Theory for Modeling Dynamical Systems | http://arxiv.org/pdf/1207.4167v1.pdf | author:Satinder Singh, Michael James, Matthew Rudary category:cs.AI cs.LG published:2012-07-11 summary:Modeling dynamical systems, both for control purposes and to make predictionsabout their behavior, is ubiquitous in science and engineering. Predictivestate representations (PSRs) are a recently introduced class of models fordiscrete-time dynamical systems. The key idea behind PSRs and the closelyrelated OOMs (Jaeger's observable operator models) is to represent the state ofthe system as a set of predictions of observable outcomes of experiments onecan do in the system. This makes PSRs rather different from history-basedmodels such as nth-order Markov models and hidden-state-based models such asHMMs and POMDPs. We introduce an interesting construct, the systemdynamicsmatrix, and show how PSRs can be derived simply from it. We also use thisconstruct to show formally that PSRs are more general than both nth-orderMarkov models and HMMs/POMDPs. Finally, we discuss the main difference betweenPSRs and OOMs and conclude with directions for future work.
arxiv-1500-227 | Factored Latent Analysis for far-field tracking data | http://arxiv.org/pdf/1207.4164v1.pdf | author:Chris Stauffer category:cs.LG stat.ML published:2012-07-11 summary:This paper uses Factored Latent Analysis (FLA) to learn a factorized,segmental representation for observations of tracked objects over time.Factored Latent Analysis is latent class analysis in which the observationspace is subdivided and each aspect of the original space is represented by aseparate latent class model. One could simply treat these factors as completelyindependent and ignore their interdependencies or one could concatenate themtogether and attempt to learn latent class structure for the completeobservation space. Alternatively, FLA allows the interdependencies to beexploited in estimating an effective model, which is also capable ofrepresenting a factored latent state. In this paper, FLA is used to learn a setof factored latent classes to represent different modalities of observations oftracked objects. Different characteristics of the state of tracked objects areeach represented by separate latent class models, including normalized size,normalized speed, normalized direction, and position. This model also enableseffective temporal segmentation of these sequences. This method is data-driven,unsupervised using only pairwise observation statistics. This data-driven andunsupervised activity classi- fication technique exhibits good performance inmultiple challenging environments.
arxiv-1500-228 | The evolutionary origins of modularity | http://arxiv.org/pdf/1207.2743v2.pdf | author:Jeff Clune, Jean-Baptiste Mouret, Hod Lipson category:q-bio.PE cs.NE q-bio.MN q-bio.NC published:2012-07-11 summary:A central biological question is how natural organisms are so evolvable(capable of quickly adapting to new environments). A key driver of evolvabilityis the widespread modularity of biological networks--their organization asfunctional, sparsely connected subunits--but there is no consensus regardingwhy modularity itself evolved. While most hypotheses assume indirect selectionfor evolvability, here we demonstrate that the ubiquitous, direct selectionpressure to reduce the cost of connections between network nodes causes theemergence of modular networks. Experiments with selection pressures to maximizenetwork performance and minimize connection costs yield networks that aresignificantly more modular and more evolvable than control experiments thatonly select for performance. These results will catalyze research in numerousdisciplines, including neuroscience, genetics and harnessing evolution forengineering purposes.
arxiv-1500-229 | On the Choice of Regions for Generalized Belief Propagation | http://arxiv.org/pdf/1207.4158v1.pdf | author:Max Welling category:cs.AI cs.LG published:2012-07-11 summary:Generalized belief propagation (GBP) has proven to be a promising techniquefor approximate inference tasks in AI and machine learning. However, the choiceof a good set of clusters to be used in GBP has remained more of an art then ascience until this day. This paper proposes a sequential approach to adding newclusters of nodes and their interactions (i.e. "regions") to the approximation.We first review and analyze the recently introduced region graphs and find thatthree kinds of operations ("split", "merge" and "death") leave the free energyand (under some conditions) the fixed points of GBP invariant. This leads tothe notion of "weakly irreducible" regions as the natural candidates to beadded to the approximation. Computational complexity of the GBP algorithm iscontrolled by restricting attention to regions with small "region-width".Combining the above with an efficient (i.e. local in the graph) measure topredict the improved accuracy of GBP leads to the sequential "region pursuit"algorithm for adding new regions bottom-up to the region graph. Experimentsshow that this algorithm can indeed perform close to optimally.
arxiv-1500-230 | An Integrated, Conditional Model of Information Extraction and Coreference with Applications to Citation Matching | http://arxiv.org/pdf/1207.4157v1.pdf | author:Ben Wellner, Andrew McCallum, Fuchun Peng, Michael Hay category:cs.LG cs.DL cs.IR stat.ML published:2012-07-11 summary:Although information extraction and coreference resolution appear together inmany applications, most current systems perform them as ndependent steps. Thispaper describes an approach to integrated inference for extraction andcoreference based on conditionally-trained undirected graphical models. Wediscuss the advantages of conditional probability training, and of acoreference model structure based on graph partitioning. On a data set ofresearch paper citations, we show significant reduction in error by usingextraction uncertainty to improve coreference citation matching accuracy, andusing coreference to improve the accuracy of the extracted fields.
arxiv-1500-231 | Graph partition strategies for generalized mean field inference | http://arxiv.org/pdf/1207.4156v1.pdf | author:Eric P. Xing, Michael I. Jordan, Stuart Russell category:cs.LG stat.ML published:2012-07-11 summary:An autonomous variational inference algorithm for arbitrary graphical modelsrequires the ability to optimize variational approximations over the space ofmodel parameters as well as over the choice of tractable families used for thevariational approximation. In this paper, we present a novel combination ofgraph partitioning algorithms with a generalized mean field (GMF) inferencealgorithm. This combination optimizes over disjoint clustering of variables andperforms inference using those clusters. We provide a formal analysis of therelationship between the graph cut and the GMF approximation, and exploreseveral graph partition strategies empirically. Our empirical results providerather clear support for a weighted version of MinCut as a useful clusteringalgorithm for GMF inference, which is consistent with the implications from theformal analysis.
arxiv-1500-232 | ARMA Time-Series Modeling with Graphical Models | http://arxiv.org/pdf/1207.4162v2.pdf | author:Bo Thiesson, David Maxwell Chickering, David Heckerman, Christopher Meek category:stat.AP cs.LG stat.ME published:2012-07-11 summary:We express the classic ARMA time-series model as a directed graphical model.In doing so, we find that the deterministic relationships in the model make iteffectively impossible to use the EM algorithm for learning model parameters.To remedy this problem, we replace the deterministic relationships withGaussian distributions having a small variance, yielding the stochastic ARMA(ARMA) model. This modification allows us to use the EM algorithm to learnparmeters and to forecast,even in situations where some data is missing. Thismodification, in conjunction with the graphicalmodel approach, also allows usto include cross predictors in situations where there are multiple times seriesand/or additional nontemporal covariates. More surprising,experiments suggestthat the move to stochastic ARMA yields improved accuracy through bettersmoothing. We demonstrate improvements afforded by cross prediction and bettersmoothing on real data.
arxiv-1500-233 | PAC-learning bounded tree-width Graphical Models | http://arxiv.org/pdf/1207.4151v1.pdf | author:Mukund Narasimhan, Jeff A. Bilmes category:cs.LG cs.DS stat.ML published:2012-07-11 summary:We show that the class of strongly connected graphical models with treewidthat most k can be properly efficiently PAC-learnt with respect to theKullback-Leibler Divergence. Previous approaches to this problem, such as thoseof Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable byreducing it to a combinatorial optimization problem. However, for k > 1, thisproblem is NP-complete ([15]), and so unless P=NP, these approaches will takeexponential amounts of time. Our approach differs significantly from these, inthat it first attempts to find approximate conditional independencies bysolving (polynomially many) submodular optimization problems, and then using adynamic programming formulation to combine the approximate conditionalindependence information to derive a graphical model with underlying graph ofthe tree-width specified. This gives us an efficient (polynomial time in thenumber of random variables) PAC-learning algorithm which requires onlypolynomial number of samples of the true distribution, and only polynomialrunning time.
arxiv-1500-234 | Similarity-Driven Cluster Merging Method for Unsupervised Fuzzy Clustering | http://arxiv.org/pdf/1207.4155v1.pdf | author:Xuejian Xiong, Kap Chan, Kian Lee Tan category:cs.LG stat.ML published:2012-07-11 summary:In this paper, a similarity-driven cluster merging method is proposed forunsuper-vised fuzzy clustering. The cluster merging method is used to resolvethe problem of cluster validation. Starting with an overspecified number ofclusters in the data, pairs of similar clusters are merged based on theproposed similarity-driven cluster merging criterion. The similarity betweenclusters is calculated by a fuzzy cluster similarity matrix, while an adaptivethreshold is used for merging. In addition, a modified generalized ob- jectivefunction is used for prototype-based fuzzy clustering. The function includesthe p-norm distance measure as well as principal components of the clusters.The number of the principal components is determined automatically from thedata being clustered. The properties of this unsupervised fuzzy clusteringalgorithm are illustrated by several experiments.
arxiv-1500-235 | Maximum Entropy for Collaborative Filtering | http://arxiv.org/pdf/1207.4152v1.pdf | author:Lawrence Zitnick, Takeo Kanade category:cs.IR cs.LG published:2012-07-11 summary:Within the task of collaborative filtering two challenges for computingconditional probabilities exist. First, the amount of training data availableis typically sparse with respect to the size of the domain. Thus, support forhigher-order interactions is generally not present. Second, the variables thatwe are conditioning upon vary for each query. That is, users label differentvariables during each query. For this reason, there is no consistent input tooutput mapping. To address these problems we purpose a maximum entropy approachusing a non-standard measure of entropy. This approach can be simplified tosolving a set of linear equations that can be efficiently solved.
arxiv-1500-236 | From Fields to Trees | http://arxiv.org/pdf/1207.4149v1.pdf | author:Firas Hamze, Nando de Freitas category:stat.CO cs.LG published:2012-07-11 summary:We present new MCMC algorithms for computing the posterior distributions andexpectations of the unknown variables in undirected graphical models withregular structure. For demonstration purposes, we focus on Markov Random Fields(MRFs). By partitioning the MRFs into non-overlapping trees, it is possible tocompute the posterior distribution of a particular tree exactly by conditioningon the remaining tree. These exact solutions allow us to construct efficientblocked and Rao-Blackwellised MCMC algorithms. We show empirically that treesampling is considerably more efficient than other partitioned sampling schemesand the naive Gibbs sampler, even in cases where loopy belief propagation failsto converge. We prove that tree sampling exhibits lower variance than the naiveGibbs sampler and other naive partitioning schemes using the theoreticalmeasure of maximal correlation. We also construct new information theory toolsfor comparing different MCMC schemes and show that, under these, tree samplingis more efficient.
arxiv-1500-237 | Dynamical Systems Trees | http://arxiv.org/pdf/1207.4148v1.pdf | author:Andrew Howard, Tony S. Jebara category:cs.LG stat.ML published:2012-07-11 summary:We propose dynamical systems trees (DSTs) as a flexible class of models fordescribing multiple processes that interact via a hierarchy of aggregatingparent chains. DSTs extend Kalman filters, hidden Markov models and nonlineardynamical systems to an interactive group scenario. Various individualprocesses interact as communities and sub-communities in a tree structure thatis unrolled in time. To accommodate nonlinear temporal activity, eachindividual leaf process is modeled as a dynamical system containing discreteand/or continuous hidden states with discrete and/or Gaussian emissions.Subsequent higher level parent processes act like hidden Markov models andmediate the interaction between leaf processes or between other parentprocesses in the hierarchy. Aggregator chains are parents of child processesthat they combine and mediate, yielding a compact overall parameterization. Weprovide tractable inference and learning algorithms for arbitrary DSTtopologies via an efficient structured mean-field algorithm. The diverseapplicability of DSTs is demonstrated by experiments on gene expression dataand by modeling group behavior in the setting of an American football game.
arxiv-1500-238 | A Bayesian Approach toward Active Learning for Collaborative Filtering | http://arxiv.org/pdf/1207.4146v1.pdf | author:Rong Jin, Luo Si category:cs.LG cs.IR stat.ML published:2012-07-11 summary:Collaborative filtering is a useful technique for exploiting the preferencepatterns of a group of users to predict the utility of items for the activeuser. In general, the performance of collaborative filtering depends on thenumber of rated examples given by the active user. The more the number of ratedexamples given by the active user, the more accurate the predicted ratings willbe. Active learning provides an effective way to acquire the most informativerated examples from active users. Previous work on active learning forcollaborative filtering only considers the expected loss function based on theestimated model, which can be misleading when the estimated model isinaccurate. This paper takes one step further by taking into account of theposterior distribution of the estimated model, which results in more robustactive learning algorithm. Empirical studies with datasets of movie ratingsshow that when the number of ratings from the active user is restricted to besmall, active learning methods only based on the estimated model don't performwell while the active learning method using the model distribution achievessubstantially better performance.
arxiv-1500-239 | A Generative Bayesian Model for Aggregating Experts' Probabilities | http://arxiv.org/pdf/1207.4144v1.pdf | author:Joseph Kahn category:cs.LG stat.ML published:2012-07-11 summary:In order to improve forecasts, a decisionmaker often combines probabilitiesgiven by various sources, such as human experts and machine learningclassifiers. When few training data are available, aggregation can be improvedby incorporating prior knowledge about the event being forecasted and aboutsalient properties of the experts. To this end, we develop a generativeBayesian aggregation model for probabilistic classi cation. The model includesan event-specific prior, measures of individual experts' bias, calibration,accuracy, and a measure of dependence betweeen experts. Rather than requireabsolute measures, we show that aggregation may be expressed in terms ofrelative accuracy between experts. The model results in a weighted logarithmicopinion pool (LogOps) that satis es consistency criteria such as the externalBayesian property. We derive analytic solutions for independent and forexchangeable experts. Empirical tests demonstrate the model's use, comparingits accuracy with other aggregation methods.
arxiv-1500-240 | Conditional Chow-Liu Tree Structures for Modeling Discrete-Valued Vector Time Series | http://arxiv.org/pdf/1207.4142v1.pdf | author:Sergey Kirshner, Padhraic Smyth, Andrew Robertson category:cs.LG stat.ML published:2012-07-11 summary:We consider the problem of modeling discrete-valued vector time series datausing extensions of Chow-Liu tree models to capture both dependencies acrosstime and dependencies across variables. Conditional Chow-Liu tree models areintroduced, as an extension to standard Chow-Liu trees, for modelingconditional rather than joint densities. We describe learning algorithms forsuch models and show how they can be used to learn parsimonious representationsfor the output distributions in hidden Markov models. These models are appliedto the important problem of simulating and forecasting daily precipitationoccurrence for networks of rain stations. To demonstrate the effectiveness ofthe models, we compare their performance versus a number of alternatives usinghistorical precipitation data from Southwestern Australia and the WesternUnited States. We illustrate how the structure and parameters of the models canbe used to provide an improved meteorological interpretation of such data.
arxiv-1500-241 | An Extended Cencov-Campbell Characterization of Conditional Information Geometry | http://arxiv.org/pdf/1207.4139v1.pdf | author:Guy Lebanon category:cs.LG stat.ML published:2012-07-11 summary:We formulate and prove an axiomatic characterization of conditionalinformation geometry, for both the normalized and the nonnormalized cases. Thischaracterization extends the axiomatic derivation of the Fisher geometry byCencov and Campbell to the cone of positive conditional models, and as aspecial case to the manifold of conditional distributions. Due to the closeconnection between the conditional I-divergence and the product Fisherinformation metric the characterization provides a new axiomatic interpretationof the primal problems underlying logistic regression and AdaBoost.
arxiv-1500-242 | Active Model Selection | http://arxiv.org/pdf/1207.4138v1.pdf | author:Omid Madani, Daniel J. Lizotte, Russell Greiner category:cs.LG stat.ML published:2012-07-11 summary:Classical learning assumes the learner is given a labeled data sample, fromwhich it learns a model. The field of Active Learning deals with the situationwhere the learner begins not with a training sample, but instead with resourcesthat it can use to obtain information to help identify the optimal model. Tobetter understand this task, this paper presents and analyses the simplified"(budgeted) active model selection" version, which captures the pureexploration aspect of many active learning problems in a clean and simpleproblem formulation. Here the learner can use a fixed budget of "model probes"(where each probe evaluates the specified model on a random indistinguishableinstance) to identify which of a given set of possible models has the highestexpected accuracy. Our goal is a policy that sequentially determines whichmodel to probe next, based on the information observed so far. We present aformal description of this task, and show that it is NPhard in general. We theninvestigate a number of algorithms for this task, including several existingones (eg, "Round-Robin", "Interval Estimation", "Gittins") as well as somenovel ones (e.g., "Biased-Robin"), describing first their approximationproperties and then their empirical performance on various problem instances.We observe empirically that the simple biased-robin algorithm significantlyoutperforms the other algorithms in the case of identical costs and priors.
arxiv-1500-243 | Face Recognition Algorithms based on Transformed Shape Features | http://arxiv.org/pdf/1207.2537v1.pdf | author:Sambhunath Biswas, Amrita Biswas category:cs.CV published:2012-07-11 summary:Human face recognition is, indeed, a challenging task, especially under theillumination and pose variations. We examine in the present paper effectivenessof two simple algorithms using coiflet packet and Radon transforms to recognizehuman faces from some databases of still gray level images, under theenvironment of illumination and pose variations. Both the algorithms convert2-D gray level training face images into their respective depth maps orphysical shape which are subsequently transformed by Coiflet packet and Radontransforms to compute energy for feature extraction. Experiments show that suchtransformed shape features are robust to illumination and pose variations. Withthe features extracted, training classes are optimally separated through lineardiscriminant analysis (LDA), while classification for test face images is madethrough a k-NN classifier, based on L1 norm and Mahalanobis distance measures.Proposed algorithms are then tested on face images that differ inillumination,expression or pose separately, obtained from threedatabases,namely, ORL, Yale and Essex-Grimace databases. Results, so obtained,are compared with two different existing algorithms.Performance usingDaubechies wavelets is also examined. It is seen that the proposed Coifletpacket and Radon transform based algorithms have significant performance,especially under different illumination conditions and pose variation.Comparison shows the proposed algorithms are superior.
arxiv-1500-244 | Automated Training and Maintenance through Kinect | http://arxiv.org/pdf/1207.2597v1.pdf | author:Saket Warade, Jagannath Aghav, Petitpierre Claude, Sandeep Udayagiri category:cs.CV cs.ET cs.GR cs.HC published:2012-07-11 summary:In this paper, we have worked on reducing burden on mechanic involvingcomplex automobile maintenance activities that are performed in centralisedworkshops. We have presented a system prototype that combines Augmented Realitywith Kinect. With the use of Kinect, very high quality sensors are available atconsiderably low costs, thus reducing overall expenditure for system design.The system can be operated either in Speech mode or in Gesture mode. The systemcan be controlled by various audio commands if user opts for Speech mode. Thesame controlling can also be done by using a set of Gestures in Gesture mode. Gesture recognition is the task performed by Kinect system. This system,bundled with RGB and Depth camera, processes the skeletal data by keeping trackof 20 different body joints. Recognizing Gestures is done by verifying usermovements and checking them against predefined condition. Augmented Realitymodule captures real-time image data streams from high resolution camera. Thismodule then generates 3D model that is superimposed on real time data.
arxiv-1500-245 | Efficient Prediction of DNA-Binding Proteins Using Machine Learning | http://arxiv.org/pdf/1207.2600v1.pdf | author:Sokyna Qatawneh, Afaf Alneaimi, Thamer Rawashdeh, Mmohammad Muhairat, Rami Qahwaji, Stan Ipson category:cs.CV q-bio.QM published:2012-07-11 summary:DNA-binding proteins are a class of proteins which have a specific or generalaffinity to DNA and include three important components: transcription factors;nucleases, and histones. DNA-binding proteins also perform important roles inmany types of cellular activities. In this paper we describe machine learningsystems for the prediction of DNA- binding proteins where a Support VectorMachine and a Cascade Correlation Neural Network are optimized and thencompared to determine the learning algorithm that achieves the best predictionperformance. The information used for classification is derived fromcharacteristics that include overall charge, patch size and amino acidscomposition. In total 121 DNA- binding proteins and 238 non-binding proteinsare used to build and evaluate the system. For SVM using the ANOVA Kernel withJack-knife evaluation, an accuracy of 86.7% has been achieved with 91.1% forsensitivity and 85.3% for specificity. For CCNN optimized over the entiredataset with Jack knife evaluation we report an accuracy of 75.4%, while thevalues of specificity and sensitivity achieved were 72.3% and 82.6%,respectively.
arxiv-1500-246 | A Novel Approach Coloured Object Tracker with Adaptive Model and Bandwidth using Mean Shift Algorithm | http://arxiv.org/pdf/1207.2602v1.pdf | author:Seyed Amir Mohammadi, Mohammad Reza Mahzoun category:cs.CV published:2012-07-11 summary:The traditional color-based mean-shift tracking algorithm is popular amongtracking methods due to its simple and efficient procedure, however, the lackof dynamism in its target model makes it unsuitable for tracking objects whichhave changes in their sizes and shapes. In this paper, we propose a fast novelthreephase colored object tracker algorithm based on mean shift idea whileutilizing adaptive model. The proposed method can improve the mentionedweaknesses of the original mean-shift algorithm. The experimental results showthat the new method is feasible, robust and has acceptable speed in comparisonwith other algorithms.15 page,
arxiv-1500-247 | Nugget Discovery with a Multi-objective Cultural Algorithm | http://arxiv.org/pdf/1207.2630v1.pdf | author:Sujatha Srinivasan, Sivakumar Ramakrishnan category:cs.NE I.5.2; I.2.0 published:2012-07-11 summary:Partial classification popularly known as nugget discovery comes underdescriptive knowledge discovery. It involves mining rules for a target class ofinterest. Classification "If-Then" rules are the most sought out by decisionmakers since they are the most comprehensible form of knowledge mined by datamining techniques. The rules have certain properties namely the rule metricswhich are used to evaluate them. Mining rules with user specified propertiescan be considered as a multi-objective optimization problem since the ruleshave to satisfy more than one property to be used by the user. Culturalalgorithm (CA) with its knowledge sources have been used in solving manyoptimization problems. However research gap exists in using cultural algorithmfor multi-objective optimization of rules. In the current study amulti-objective cultural algorithm is proposed for partial classification.Results of experiments on benchmark data sets reveal good performance.
arxiv-1500-248 | The Minimum Information Principle for Discriminative Learning | http://arxiv.org/pdf/1207.4110v1.pdf | author:Amir Globerson, Naftali Tishby category:cs.LG stat.ML published:2012-07-11 summary:Exponential models of distributions are widely used in machine learning forclassiffication and modelling. It is well known that they can be interpreted asmaximum entropy models under empirical expectation constraints. In this work,we argue that for classiffication tasks, mutual information is a more suitableinformation theoretic measure to be optimized. We show how the principle ofminimum mutual information generalizes that of maximum entropy, and provides acomprehensive framework for building discriminative classiffiers. A gametheoretic interpretation of our approach is then given, and severalgeneralization bounds provided. We present iterative algorithms for solving theminimum information problem and its convex dual, and demonstrate theirperformance on various classiffication tasks. The results show that minimuminformation classiffiers outperform the corresponding maximum entropy models.
arxiv-1500-249 | Algebraic Statistics in Model Selection | http://arxiv.org/pdf/1207.4112v1.pdf | author:Luis David Garcia category:cs.LG stat.ML published:2012-07-11 summary:We develop the necessary theory in computational algebraic geometry to placeBayesian networks into the realm of algebraic statistics. We present analgebra{statistics dictionary focused on statistical modeling. In particular,we link the notion of effiective dimension of a Bayesian network with thenotion of algebraic dimension of a variety. We also obtain the independence andnon{independence constraints on the distributions over the observable variablesimplied by a Bayesian network with hidden variables, via a generating set of anideal of polynomials associated to the network. These results extend previouswork on the subject. Finally, the relevance of these results for modelselection is discussed.
arxiv-1500-250 | On-line Prediction with Kernels and the Complexity Approximation Principle | http://arxiv.org/pdf/1207.4113v1.pdf | author:Alex Gammerman, Yuri Kalnishkan, Vladimir Vovk category:cs.LG stat.ML published:2012-07-11 summary:The paper describes an application of Aggregating Algorithm to the problem ofregression. It generalizes earlier results concerned with plain linearregression to kernel techniques and presents an on-line algorithm whichperforms nearly as well as any oblivious kernel predictor. The paper containsthe derivation of an estimate on the performance of this algorithm. Theestimate is then used to derive an application of the Complexity ApproximationPrinciple to kernel methods.
arxiv-1500-251 | Iterative Conditional Fitting for Gaussian Ancestral Graph Models | http://arxiv.org/pdf/1207.4118v1.pdf | author:Mathias Drton, Thomas S. Richardson category:stat.ME cs.LG stat.ML published:2012-07-11 summary:Ancestral graph models, introduced by Richardson and Spirtes (2002),generalize both Markov random fields and Bayesian networks to a class of graphswith a global Markov property that is closed under conditioning andmarginalization. By design, ancestral graphs encode precisely the conditionalindependence structures that can arise from Bayesian networks with selectionand unobserved (hidden/latent) variables. Thus, ancestral graph models providea potentially very useful framework for exploratory model selection whenunobserved variables might be involved in the data-generating process but noparticular hidden structure can be specified. In this paper, we present theIterative Conditional Fitting (ICF) algorithm for maximum likelihood estimationin Gaussian ancestral graph models. The name reflects that in each step of theprocedure a conditional distribution is estimated, subject to constraints,while a marginal distribution is held fixed. This approach is in duality to thewell-known Iterative Proportional Fitting algorithm, in which marginaldistributions are fitted while conditional distributions are held fixed.
arxiv-1500-252 | Applying Discrete PCA in Data Analysis | http://arxiv.org/pdf/1207.4125v1.pdf | author:Wray L. Buntine, Aleks Jakulin category:cs.LG stat.ML published:2012-07-11 summary:Methods for analysis of principal components in discrete data have existedfor some time under various names such as grade of membership modelling,probabilistic latent semantic analysis, and genotype inference with admixture.In this paper we explore a number of extensions to the common theory, andpresent some application of these methods to some common statistical tasks. Weshow that these methods can be interpreted as a discrete version of ICA. Wedevelop a hierarchical version yielding components at different levels ofdetail, and additional techniques for Gibbs sampling. We compare the algorithmson a text prediction task using support vector machines, and to informationretrieval.
arxiv-1500-253 | Recovering Articulated Object Models from 3D Range Data | http://arxiv.org/pdf/1207.4129v1.pdf | author:Dragomir Anguelov, Daphne Koller, Hoi-Cheung Pang, Praveen Srinivasan, Sebastian Thrun category:cs.CV published:2012-07-11 summary:We address the problem of unsupervised learning of complex articulated objectmodels from 3D range data. We describe an algorithm whose input is a set ofmeshes corresponding to different configurations of an articulated object. Thealgorithm automatically recovers a decomposition of the object intoapproximately rigid parts, the location of the parts in the different objectinstances, and the articulated object skeleton linking the parts. Our algorithmfirst registers allthe meshes using an unsupervised non-rigid techniquedescribed in a companion paper. It then segments the meshes using a graphicalmodel that captures the spatial contiguity of parts. The segmentation is doneusing the EM algorithm, iterating between finding a decomposition of the objectinto rigid parts, and finding the location of the parts in the objectinstances. Although the graphical model is densely connected, the objectdecomposition step can be performed optimally and efficiently, allowing us toidentify a large number of object parts while avoiding local maxima. Wedemonstrate the algorithm on real world datasets, recovering a 15-partarticulated model of a human puppet from just 7 different puppetconfigurations, as well as a 4 part model of a fiexing arm where significantnon-rigid deformation was present.
arxiv-1500-254 | Exponential Families for Conditional Random Fields | http://arxiv.org/pdf/1207.4131v1.pdf | author:Yasemin Altun, Alex Smola, Thomas Hofmann category:cs.LG stat.ML published:2012-07-11 summary:In this paper we de ne conditional random elds in reproducing kernel Hilbertspaces and show connections to Gaussian Process classi cation. More specically, we prove decomposition results for undirected graphical models and wegive constructions for kernels. Finally we present e cient means of solving theoptimization problem using reduced rank decompositions and we show howstationarity can be exploited e ciently in the optimization process.
arxiv-1500-255 | MOB-ESP and other Improvements in Probability Estimation | http://arxiv.org/pdf/1207.4132v1.pdf | author:Rodney Nielsen category:cs.LG cs.AI stat.ML published:2012-07-11 summary:A key prerequisite to optimal reasoning under uncertainty in intelligentsystems is to start with good class probability estimates. This paper improveson the current best probability estimation trees (Bagged-PETs) and alsopresents a new ensemble-based algorithm (MOB-ESP). Comparisons are made usingseveral benchmark datasets and multiple metrics. These experiments show thatMOB-ESP outputs significantly more accurate class probabilities than either thebaseline BPETs algorithm or the enhanced version presented here (EB-PETs).These results are based on metrics closely associated with the average accuracyof the predictions. MOB-ESP also provides much better probability rankings thanB-PETs. The paper further suggests how these estimation techniques can beapplied in concert with a broader category of classifiers.
arxiv-1500-256 | "Ideal Parent" Structure Learning for Continuous Variable Networks | http://arxiv.org/pdf/1207.4133v1.pdf | author:Iftach Nachman, Gal Elidan, Nir Friedman category:cs.LG stat.ML published:2012-07-11 summary:In recent years, there is a growing interest in learning Bayesian networkswith continuous variables. Learning the structure of such networks is acomputationally expensive procedure, which limits most applications toparameter learning. This problem is even more acute when learning networks withhidden variables. We present a general method for significantly speeding thestructure search algorithm for continuous variable networks with commonparametric distributions. Importantly, our method facilitates the addition ofnew hidden variables into the network structure efficiently. We demonstrate themethod on several data sets, both for learning structure on fully observabledata, and for introducing new hidden variables during structure search.
arxiv-1500-257 | Bayesian Learning in Undirected Graphical Models: Approximate MCMC algorithms | http://arxiv.org/pdf/1207.4134v1.pdf | author:Iain Murray, Zoubin Ghahramani category:cs.LG stat.ML published:2012-07-11 summary:Bayesian learning in undirected graphical modelscomputing posteriordistributions over parameters and predictive quantities is exceptionallydifficult. We conjecture that for general undirected models, there are notractable MCMC (Markov Chain Monte Carlo) schemes giving the correctequilibrium distribution over parameters. While this intractability, due to thepartition function, is familiar to those performing parameter optimisation,Bayesian learning of posterior distributions over undirected model parametershas been unexplored and poses novel challenges. we propose several approximateMCMC schemes and test on fully observed binary models (Boltzmann machines) fora small coronary heart disease data set and larger artificial systems. Whileapproximations must perform well on the model, their interaction with thesampling scheme is also important. Samplers based on variational mean- fieldapproximations generally performed poorly, more advanced methods using loopypropagation, brief sampling and stochastic dynamics lead to acceptableparameter posteriors. Finally, we demonstrate these techniques on a Markovrandom field with hidden variables.
arxiv-1500-258 | Cups Products in Z2-Cohomology of 3D Polyhedral Complexes | http://arxiv.org/pdf/1207.2346v3.pdf | author:Rocio Gonalez-Diaz, Javier Lamar, Ronald Umble category:cs.CV 55-XX published:2012-07-10 summary:Let $I=(\mathbb{Z}^3,26,6,B)$ be a 3D digital image, let $Q(I)$ be theassociated cubical complex and let $\partial Q(I)$ be the subcomplex of $Q(I)$whose maximal cells are the quadrangles of $Q(I)$ shared by a voxel of $B$ inthe foreground -- the object under study -- and by a voxel of$\mathbb{Z}^3\smallsetminus B$ in the background -- the ambient space. We showhow to simplify the combinatorial structure of $\partial Q(I)$ and obtain a 3Dpolyhedral complex $P(I)$ homeomorphic to $\partial Q(I)$ but with fewer cells.We introduce an algorithm that computes cup products on$H^*(P(I);\mathbb{Z}_2)$ directly from the combinatorics. The computationalmethod introduced here can be effectively applied to any polyhedral complexembedded in $\mathbb{R}^3$.
arxiv-1500-259 | A Spectral Learning Approach to Range-Only SLAM | http://arxiv.org/pdf/1207.2491v1.pdf | author:Byron Boots, Geoffrey J. Gordon category:cs.LG cs.RO stat.ML published:2012-07-10 summary:We present a novel spectral learning algorithm for simultaneous localizationand mapping (SLAM) from range data with known correspondences. This algorithmis an instance of a general spectral system identification framework, fromwhich it inherits several desirable properties, including statisticalconsistency and no local optima. Compared with popular batch optimization ormultiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectralapproach offers guaranteed low computational requirements and good trackingperformance. Compared with popular extended Kalman filter (EKF) or extendedinformation filter (EIF) approaches, and many MHT ones, our approach does notneed to linearize a transition or measurement model; such linearizations cancause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularlyfor the highly non-Gaussian posteriors encountered in range-only SLAM. Weprovide a theoretical analysis of our method, including finite-sample errorbounds. Finally, we demonstrate on a real-world robotic SLAM problem that ouralgorithm is not only theoretically justified, but works well in practice: in acomparison of multiple methods, the lowest errors come from a combination ofour algorithm with batch optimization, but our method alone produces nearly asgood a result at far lower computational cost.
arxiv-1500-260 | Non-Convex Rank Minimization via an Empirical Bayesian Approach | http://arxiv.org/pdf/1207.2440v1.pdf | author:David Wipf category:stat.ML cs.CV cs.IT math.IT published:2012-07-10 summary:In many applications that require matrix solutions of minimal rank, theunderlying cost function is non-convex leading to an intractable, NP-hardoptimization problem. Consequently, the convex nuclear norm is frequently usedas a surrogate penalty term for matrix rank. The problem is that in manypractical scenarios there is no longer any guarantee that we can correctlyestimate generative low-rank matrices of interest, theoretical special casesnotwithstanding. Consequently, this paper proposes an alternative empiricalBayesian procedure build upon a variational approximation that, unlike thenuclear norm, retains the same globally minimizing point estimate as the rankfunction under many useful constraints. However, locally minimizing solutionsare largely smoothed away via marginalization, allowing the algorithm tosucceed when standard convex relaxations completely fail. While the proposedmethodology is generally applicable to a wide range of low-rank applications,we focus our attention on the robust principal component analysis problem(RPCA), which involves estimating an unknown low-rank matrix with unknownsparse corruptions. Theoretical and empirical evidence are presented to showthat our method is potentially superior to related MAP-based approaches, forwhich the convex principle component pursuit (PCP) algorithm (Candes et al.,2011) can be viewed as a special case.
arxiv-1500-261 | A Multi-Agents Architecture to Learn Vision Operators and their Parameters | http://arxiv.org/pdf/1207.2426v1.pdf | author:Issam Qaffou, Mohammed Sadgal, Abdelaziz Elfazziki category:cs.CV published:2012-07-10 summary:In a vision system, every task needs that the operators to apply should be{\guillemotleft} well chosen {\guillemotright} and their parameters should bealso {\guillemotleft} well adjusted {\guillemotright}. The diversity ofoperators and the multitude of their parameters constitute a big challenge forusers. As it is very difficult to make the {\guillemotleft} right{\guillemotright} choice, lack of a specific rule, many disadvantages appearand affect the computation time and especially the quality of results. In thispaper we present a multi-agent architecture to learn the best operators toapply and their best parameters for a class of images. Our architectureconsists of three types of agents: User Agent, Operator Agent and ParameterAgent. The User Agent determines the phases of treatment, a library ofoperators and the possible values of their parameters. The Operator Agentconstructs all possible combinations of operators and the Parameter Agent, thecore of the architecture, adjusts the parameters of each combination bytreating a large number of images. Through the reinforcement learningmechanism, our architecture does not consider only the system opportunities butalso the user preferences.
arxiv-1500-262 | Dual-Space Analysis of the Sparse Linear Model | http://arxiv.org/pdf/1207.2422v1.pdf | author:David Wipf, Yi Wu category:stat.ML cs.CV cs.IT math.IT published:2012-07-10 summary:Sparse linear (or generalized linear) models combine a standard likelihoodfunction with a sparse prior on the unknown coefficients. These priors canconveniently be expressed as a maximization over zero-mean Gaussians withdifferent variance hyperparameters. Standard MAP estimation (Type I) involvesmaximizing over both the hyperparameters and coefficients, while an empiricalBayesian alternative (Type II) first marginalizes the coefficients and thenmaximizes over the hyperparameters, leading to a tractable posteriorapproximation. The underlying cost functions can be related via a dual-spaceframework from Wipf et al. (2011), which allows both the Type I or Type IIobjectives to be expressed in either coefficient or hyperparmeter space. Thisperspective is useful because some analyses or extensions are more conducive todevelopment in one space or the other. Herein we consider the estimation of atrade-off parameter balancing sparsity and data fit. As this parameter iseffectively a variance, natural estimators exist by assessing the problem inhyperparameter (variance) space, transitioning natural ideas from Type II tosolve what is much less intuitive for Type I. In contrast, for analyses ofupdate rules and sparsity properties of local and global solutions, as well asextensions to more general likelihood models, we can leverage coefficient-spacetechniques developed for Type I and apply them to Type II. For example, thisallows us to prove that Type II-inspired techniques can be successfulrecovering sparse coefficients when unfavorable restricted isometry properties(RIP) lead to failure of popular L1 reconstructions. It also facilitates theanalysis of Type II when non-Gaussian likelihood models lead to intractableintegrations.
arxiv-1500-263 | Improvement of ISOM by using filter | http://arxiv.org/pdf/1207.2268v1.pdf | author:Imen Chaabouni, Wiem Fourati, Med Salim Bouhlel category:cs.MM cs.CV published:2012-07-10 summary:Image compression helps in storing the transmitted data in proficient way bydecreasing its redundancy. This technique helps in transferring more digital ormultimedia data over internet as it increases the storage space. It isimportant to maintain the image quality even if it is compressed to certainextent. Depend upon this the image compression is classified into twocategories : lossy and lossless image compression. There are many lossy digitalimage compression techniques exists. Among this Incremental Self Organizing Mapis a familiar one. The good pictures quality can be retrieved if imagedenoising technique is used for compression and also provides bettercompression ratio. Image denoising is an important pre-processing step for manyimage analysis and computer vision system. It refers to the task of recoveringa good estimate of the true image from a degraded observation without alteringand changing useful structure in the image such as discontinuities and edges.Many approaches have been proposed to remove the noise effectively whilepreserving the original image details and features as much as possible. Thispaper proposes a technique for image compression using Incremental SelfOrganizing Map (ISOM) with Discret Wavelet Transform (DWT) by applyingfiltering techniques which play a crucial role in enhancing the quality of areconstructed image. The experimental result shows that the proposed techniqueobtained better compression ratio value.
arxiv-1500-264 | Challenges for Distributional Compositional Semantics | http://arxiv.org/pdf/1207.2265v1.pdf | author:Daoud Clarke category:cs.CL cs.AI published:2012-07-10 summary:This paper summarises the current state-of-the art in the study ofcompositionality in distributional semantics, and major challenges for thisarea. We single out generalised quantifiers and intensional semantics as areason which to focus attention for the development of the theory. Once suitabletheories have been developed, algorithms will be needed to apply the theory totasks. Evaluation is a major problem; we single out application to recognisingtextual entailment and machine translation for this purpose.
arxiv-1500-265 | A Genetic Algorithm Approach for Solving a Flexible Job Shop Scheduling Problem | http://arxiv.org/pdf/1207.2253v1.pdf | author:Sayedmohammadreza Vaghefinezhad, Kuan Yew Wong category:math.OC cs.NE published:2012-07-10 summary:Flexible job shop scheduling has been noticed as an effective manufacturingsystem to cope with rapid development in today's competitive environment.Flexible job shop scheduling problem (FJSSP) is known as a NP-hard problem inthe field of optimization. Considering the dynamic state of the real worldmakes this problem more and more complicated. Most studies in the field ofFJSSP have only focused on minimizing the total makespan. In this paper, amathematical model for FJSSP has been developed. The objective function ismaximizing the total profit while meeting some constraints. Time-varying rawmaterial costs and selling prices and dissimilar demands for each period, havebeen considered to decrease gaps between reality and the model. A manufacturerthat produces various parts of gas valves has been used as a case study. Itsscheduling problem for multi-part, multi-period, and multi-operation withparallel machines has been solved by using genetic algorithm (GA). The bestobtained answer determines the economic amount of production by differentmachines that belong to predefined operations for each part to satisfy customerdemand in each period.
arxiv-1500-266 | Kernelized Supervised Dictionary Learning | http://arxiv.org/pdf/1207.2488v4.pdf | author:Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel category:cs.CV cs.LG published:2012-07-10 summary:In this paper, we propose supervised dictionary learning (SDL) byincorporating information on class labels into the learning of the dictionary.To this end, we propose to learn the dictionary in a space where the dependencybetween the signals and their corresponding labels is maximized. To maximizethis dependency, the recently introduced Hilbert Schmidt independence criterion(HSIC) is used. One of the main advantages of this novel approach for SDL isthat it can be easily kernelized by incorporating a kernel, particularly adata-derived kernel such as normalized compression distance, into theformulation. The learned dictionary is compact and the proposed approach isfast. We show that it outperforms other unsupervised and supervised dictionarylearning approaches in the literature, using real-world data.
arxiv-1500-267 | Pseudo-likelihood methods for community detection in large sparse networks | http://arxiv.org/pdf/1207.2340v3.pdf | author:Arash A. Amini, Aiyou Chen, Peter J. Bickel, Elizaveta Levina category:cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH published:2012-07-10 summary:Many algorithms have been proposed for fitting network models withcommunities, but most of them do not scale well to large networks, and oftenfail on sparse networks. Here we propose a new fast pseudo-likelihood methodfor fitting the stochastic block model for networks, as well as a variant thatallows for an arbitrary degree distribution by conditioning on degrees. We showthat the algorithms perform well under a range of settings, including on verysparse networks, and illustrate on the example of a network of political blogs.We also propose spectral clustering with perturbations, a method of independentinterest, which works well on sparse networks where regular spectral clusteringfails, and use it to provide an initial value for pseudo-likelihood. We provethat pseudo-likelihood provides consistent estimates of the communities under amild condition on the starting value, for the case of a block model with twocommunities.
arxiv-1500-268 | Comparative Study for Inference of Hidden Classes in Stochastic Block Models | http://arxiv.org/pdf/1207.2328v2.pdf | author:Pan Zhang, Florent Krzakala, Jörg Reichardt, Lenka Zdeborová category:cs.LG stat.ML published:2012-07-10 summary:Inference of hidden classes in stochastic block model is a classical problemwith important applications. Most commonly used methods for this probleminvolve na\"{\i}ve mean field approaches or heuristic spectral methods.Recently, belief propagation was proposed for this problem. In thiscontribution we perform a comparative study between the three methods onsynthetically created networks. We show that belief propagation shows muchbetter performance when compared to na\"{\i}ve mean field and spectralapproaches. This applies to accuracy, computational efficiency and the tendencyto overfit the data.
arxiv-1500-269 | Distinct word length frequencies: distributions and symbol entropies | http://arxiv.org/pdf/1207.2334v2.pdf | author:Reginald D. Smith category:cs.CL published:2012-07-10 summary:The distribution of frequency counts of distinct words by length in alanguage's vocabulary will be analyzed using two methods. The first, will lookat the empirical distributions of several languages and derive a distributionthat reasonably explains the number of distinct words as a function of length.We will be able to derive the frequency count, mean word length, and varianceof word length based on the marginal probability of letters and spaces. Thesecond, based on information theory, will demonstrate that the conditionalentropies can also be used to estimate the frequency of distinct words of agiven length in a language. In addition, it will be shown how these techniquescan also be applied to estimate higher order entropies using vocabulary wordlength.
arxiv-1500-270 | Forecasting electricity consumption by aggregating specialized experts | http://arxiv.org/pdf/1207.1965v1.pdf | author:Marie Devaine, Pierre Gaillard, Yannig Goude, Gilles Stoltz category:stat.ML cs.LG stat.AP published:2012-07-09 summary:We consider the setting of sequential prediction of arbitrary sequences basedon specialized experts. We first provide a review of the relevant literatureand present two theoretical contributions: a general analysis of the specialistaggregation rule of Freund et al. (1997) and an adaptation of fixed-share rulesof Herbster and Warmuth (1998) in this setting. We then apply these rules tothe sequential short-term (one-day-ahead) forecasting of electricityconsumption; to do so, we consider two data sets, a Slovakian one and a Frenchone, respectively concerned with hourly and half-hourly predictions. We followa general methodology to perform the stated empirical studies and detail inparticular tuning issues of the learning parameters. The introduced aggregationrules demonstrate an improved accuracy on the data sets at hand; theimprovements lie in a reduced mean squared error but also in a more robustbehavior with respect to large occasional errors.
arxiv-1500-271 | Estimating a Causal Order among Groups of Variables in Linear Models | http://arxiv.org/pdf/1207.1977v1.pdf | author:Doris Entner, Patrik O. Hoyer category:stat.ML cs.LG stat.ME published:2012-07-09 summary:The machine learning community has recently devoted much attention to theproblem of inferring causal relationships from statistical data. Most of thiswork has focused on uncovering connections among scalar random variables. Wegeneralize existing methods to apply to collections of multi-dimensional randomvectors, focusing on techniques applicable to linear models. The performance ofthe resulting algorithms is evaluated and compared in simulations, which showthat our methods can, in many cases, provide useful information on causalrelationships even for relatively small sample sizes.
arxiv-1500-272 | Finding Structure in Text, Genome and Other Symbolic Sequences | http://arxiv.org/pdf/1207.1847v1.pdf | author:Ted Dunning category:cs.CL cs.IR published:2012-07-08 summary:The statistical methods derived and described in this thesis provide new waysto elucidate the structural properties of text and other symbolic sequences.Generically, these methods allow detection of a difference in the frequency ofa single feature, the detection of a difference between the frequencies of anensemble of features and the attribution of the source of a text. These threeabstract tasks suffice to solve problems in a wide variety of settings.Furthermore, the techniques described in this thesis can be extended to providea wide range of additional tests beyond the ones described here. A variety of applications for these methods are examined in detail. Theseapplications are drawn from the area of text analysis and genetic sequenceanalysis. The textually oriented tasks include finding interesting collocationsand cooccurent phrases, language identification, and information retrieval. Thebiologically oriented tasks include species identification and the discovery ofpreviously unreported long range structure in genes. In the applicationsreported here where direct comparison is possible, the performance of these newmethods substantially exceeds the state of the art. Overall, the methods described here provide new and effective ways to analysetext and other symbolic sequences. Their particular strength is that they dealwell with situations where relatively little data are available. Since thesemethods are abstract in nature, they can be applied in novel situations withrelative ease.
arxiv-1500-273 | Keeping greed good: sparse regression under design uncertainty with application to biomass characterization | http://arxiv.org/pdf/1207.1888v1.pdf | author:David J. Biagioni, Ryan Elmore, Wesley Jones category:stat.AP stat.CO stat.ME stat.ML published:2012-07-08 summary:In this paper, we consider the classic measurement error regression scenarioin which our independent, or design, variables are observed with severalsources of additive noise. We will show that our motivating example'sreplicated measurements on both the design and dependent variables may beleveraged to enhance a sparse regression algorithm. Specifically, we estimatethe variance and use it to scale our design variables. We demonstrate theefficacy of scaling from several points of view and validate it empiricallywith a biomass characterization data set using two of the most widely usedsparse algorithms: least angle regression (LARS) and the Dantzig selector (DS).
arxiv-1500-274 | Spatial And Spectral Quality Evaluation Based On Edges Regions Of Satellite Image Fusion | http://arxiv.org/pdf/1207.1922v1.pdf | author:Firouz Abdullah Al-Wassai, N. V. Kalyankar, Ali A. Al-Zaky category:cs.CV published:2012-07-08 summary:The Quality of image fusion is an essential determinant of the value ofprocessing images fusion for many applications. Spatial and spectral qualitiesare the two important indexes that used to evaluate the quality of any fusedimage. However, the jury is still out of fused image's benefits if it comparedwith its original images. In addition, there is a lack of measures forassessing the objective quality of the spatial resolution for the fusionmethods. Therefore, an objective quality of the spatial resolution assessmentfor fusion images is required. Most important details of the image are in edgesregions, but most standards of image estimation do not depend upon specifyingthe edges in the image and measuring their edges. However, they depend upon thegeneral estimation or estimating the uniform region, so this study deals withnew method proposed to estimate the spatial resolution by Contrast StatisticalAnalysis (CSA) depending upon calculating the contrast of the edge, non edgeregions and the rate for the edges regions. Specifying the edges in the imageis made by using Soble operator with different threshold values. In addition,estimating the color distortion added by image fusion based on HistogramAnalysis of the edge brightness values of all RGB-color bands and Lcomponent.
arxiv-1500-275 | Nonparametric Edge Detection in Speckled Imagery | http://arxiv.org/pdf/1207.1915v1.pdf | author:Edwin Girón, Alejandro C. Frery, Francisco Cribari-Neto category:stat.AP cs.CV stat.ML published:2012-07-08 summary:We address the issue of edge detection in Synthetic Aperture Radar imagery.In particular, we propose nonparametric methods for edge detection, andnumerically compare them to an alternative method that has been recentlyproposed in the literature. Our results show that some of the proposed methodsdisplay superior results and are computationally simpler than the existingmethod. An application to real (not simulated) data is presented and discussed.
arxiv-1500-276 | Object Recognition with Multi-Scale Pyramidal Pooling Networks | http://arxiv.org/pdf/1207.1765v1.pdf | author:Jonathan Masci, Ueli Meier, Gabriel Fricout, Jürgen Schmidhuber category:cs.CV cs.NE published:2012-07-07 summary:We present a Multi-Scale Pyramidal Pooling Network, featuring a novelpyramidal pooling layer at multiple scales and a novel encoding layer. Thanksto the former the network does not require all images of a given classificationtask to be of equal size. The encoding layer improves generalisationperformance in comparison to similar neural network architectures, especiallywhen training data is scarce. We evaluate and compare our system toconvolutional neural networks and state-of-the-art computer vision methods onvarious benchmark datasets. We also present results on industrial steel defectclassification, where existing architectures are not applicable because of theconstraint on equally sized input images. The proposed architecture can be seenas a fully supervised hierarchical bag-of-features extension that is trainedonline and can be fine-tuned for any given task.
arxiv-1500-277 | An Innovative Skin Detection Approach Using Color Based Image Retrieval Technique | http://arxiv.org/pdf/1207.1551v1.pdf | author:Shervan Fekri-Ershad, Mohammad Saberi, Farshad Tajeripour category:cs.CV published:2012-07-06 summary:From The late 90th, "Skin Detection" becomes one of the major problems inimage processing. If "Skin Detection" will be done in high accuracy, it can beused in many cases as face recognition, Human Tracking and etc. Until now somany methods were presented for solving this problem. In most of these methods,color space was used to extract feature vector for classifying pixels, but themost of them have not good accuracy in detecting types of skin. The proposedapproach in this paper is based on "Color based image retrieval" (CBIR)technique. In this method, first by means of CBIR method and image tiling andconsidering the relation between pixel and its neighbors, a feature vectorwould be defined and then with using a training step, detecting the skin in thetest stage. The result shows that the presenting approach, in addition to itshigh accuracy in detecting type of skin, has no sensitivity to illuminationintensity and moving face orientation.
arxiv-1500-278 | Multimodal similarity-preserving hashing | http://arxiv.org/pdf/1207.1522v1.pdf | author:Jonathan Masci, Michael M. Bronstein, Alexander A. Bronstein, Jürgen Schmidhuber category:cs.CV cs.NE published:2012-07-06 summary:We introduce an efficient computational framework for hashing data belongingto multiple modalities into a single representation space where they becomemutually comparable. The proposed approach is based on a novel coupled siameseneural network architecture and allows unified treatment of intra- andinter-modality similarity learning. Unlike existing cross-modality similaritylearning approaches, our hashing functions are not limited to binarized linearprojections and can assume arbitrarily complex forms. We show experimentallythat our method significantly outperforms state-of-the-art hashing approacheson multimedia retrieval tasks.
arxiv-1500-279 | Sequential detection of multiple change points in networks: a graphical model approach | http://arxiv.org/pdf/1207.1687v1.pdf | author:Arash Ali Amini, XuanLong Nguyen category:math.ST stat.ML stat.TH published:2012-07-06 summary:We propose a probabilistic formulation that enables sequential detection ofmultiple change points in a network setting. We present a class of sequentialdetection rules for certain functionals of change points (minimum among asubset), and prove their asymptotic optimality properties in terms of expecteddetection delay time. Drawing from graphical model formalism, the sequentialdetection rules can be implemented by a computationally efficientmessage-passing protocol which may scale up linearly in network size and inwaiting time. The effectiveness of our inference algorithm is demonstrated bysimulations.
arxiv-1500-280 | Analysis of Multi-Scale Fractal Dimension to Classify Human Motion | http://arxiv.org/pdf/1207.1649v1.pdf | author:Núbia Rosa da Silva, Odemir Martinez Bruno category:cs.CV published:2012-07-06 summary:In recent years there has been considerable interest in human actionrecognition. Several approaches have been developed in order to enhance theautomatic video analysis. Although some developments have been achieved by thecomputer vision community, the properly classification of human motion is stilla hard and challenging task. The objective of this study is to investigate theuse of 3D multi-scale fractal dimension to recognize motion patterns in videos.In order to develop a robust strategy for human motion classification, weproposed a method where the Fourier transform is used to calculate thederivative in which all data points are deemed. Our results shown thatdifferent accuracy rates can be found for different databases. We believe thatin specific applications our results are the first step to develop an automaticmonitoring system, which can be applied in security systems, trafficmonitoring, biology, physical therapy, cardiovascular disease among manyothers.
arxiv-1500-281 | Mixtures of Shifted Asymmetric Laplace Distributions | http://arxiv.org/pdf/1207.1727v3.pdf | author:Brian C. Franczak, Ryan P. Browne, Paul D. McNicholas category:stat.ME stat.CO stat.ML published:2012-07-06 summary:A mixture of shifted asymmetric Laplace distributions is introduced and usedfor clustering and classification. A variant of the EM algorithm is developedfor parameter estimation by exploiting the relationship with the generalinverse Gaussian distribution. This approach is mathematically elegant andrelatively computationally straightforward. Our novel mixture modellingapproach is demonstrated on both simulated and real data to illustrateclustering and classification applications. In these analyses, our mixture ofshifted asymmetric Laplace distributions performs favourably when compared tothe popular Gaussian approach. This work, which marks an important step in thenon-Gaussian model-based clustering and classification direction, concludeswith discussion as well as suggestions for future work.
arxiv-1500-282 | Robust Online Hamiltonian Learning | http://arxiv.org/pdf/1207.1655v2.pdf | author:Christopher E. Granade, Christopher Ferrie, Nathan Wiebe, D. G. Cory category:quant-ph cs.LG published:2012-07-06 summary:In this work we combine two distinct machine learning methodologies,sequential Monte Carlo and Bayesian experimental design, and apply them to theproblem of inferring the dynamical parameters of a quantum system. We designthe algorithm with practicality in mind by including parameters that controltrade-offs between the requirements on computational and experimentalresources. The algorithm can be implemented online (during experimental datacollection), avoiding the need for storage and post-processing. Mostimportantly, our algorithm is capable of learning Hamiltonian parameters evenwhen the parameters change from experiment-to-experiment, and also whenadditional noise processes are present and unknown. The algorithm alsonumerically estimates the Cramer-Rao lower bound, certifying its ownperformance.
arxiv-1500-283 | An experimental study of exhaustive solutions for the Mastermind puzzle | http://arxiv.org/pdf/1207.1315v1.pdf | author:J. J. Merelo, Antonio M. Mora, Carlos Cotta, Thomas P. Runarsson category:cs.NE math.OC published:2012-07-05 summary:Mastermind is in essence a search problem in which a string of symbols thatis kept secret must be found by sequentially playing strings that use the samealphabet, and using the responses that indicate how close are those otherstrings to the secret one as hints. Although it is commercialized as a game, itis a combinatorial problem of high complexity, with applications on fields thatrange from computer security to genomics. As such a kind of problem, there areno exact solutions; even exhaustive search methods rely on heuristics tochoose, at every step, strings to get the best possible hint. These methodsmostly try to play the move that offers the best reduction in search space sizein the next step; this move is chosen according to an empirical score. However,in this paper we will examine several state of the art exhaustive searchmethods and show that another factor, the presence of the actual solution amongthe candidate moves, or, in other words, the fact that the actual solution hasthe highest score, plays also a very important role. Using that, we willpropose new exhaustive search approaches that obtain results which arecomparable to the classic ones, and besides, are better suited as a basis fornon-exhaustive search strategies such as evolutionary algorithms, since theirbehavior in a series of key indicators is better than the classical algorithms.
arxiv-1500-284 | Learning Bayesian Network Parameters with Prior Knowledge about Context-Specific Qualitative Influences | http://arxiv.org/pdf/1207.1387v1.pdf | author:Ad Feelders, Linda C. van der Gaag category:cs.AI cs.LG stat.ML published:2012-07-04 summary:We present a method for learning the parameters of a Bayesian network withprior knowledge about the signs of influences between variables. Our methodaccommodates not just the standard signs, but provides for context-specificsigns as well. We show how the various signs translate into order constraintson the network parameters and how isotonic regression can be used to computeorder-constrained estimates from the available data. Our experimental resultsshow that taking prior knowledge about the signs of influences into accountleads to an improved fit of the true distribution, especially when only a smallsample of data is available. Moreover, the computed estimates are guaranteed tobe consistent with the specified signs, thereby resulting in a network that ismore likely to be accepted by experts in its domain of application.
arxiv-1500-285 | Learning about individuals from group statistics | http://arxiv.org/pdf/1207.1393v1.pdf | author:Hendrik Kuck, Nando de Freitas category:cs.LG stat.ML published:2012-07-04 summary:We propose a new problem formulation which is similar to, but moreinformative than, the binary multiple-instance learning problem. In thissetting, we are given groups of instances (described by feature vectors) alongwith estimates of the fraction of positively-labeled instances per group. Thetask is to learn an instance level classifier from this information. That is,we are trying to estimate the unknown binary labels of individuals fromknowledge of group statistics. We propose a principled probabilistic model tosolve this problem that accounts for uncertainty in the parameters and in theunknown individual labels. This model is trained with an efficient MCMCalgorithm. Its performance is demonstrated on both synthetic and real-worlddata arising in general object recognition.
arxiv-1500-286 | Maximum Margin Bayesian Networks | http://arxiv.org/pdf/1207.1382v1.pdf | author:Yuhong Guo, Dana Wilkinson, Dale Schuurmans category:cs.LG stat.ML published:2012-07-04 summary:We consider the problem of learning Bayesian network classifiers thatmaximize the marginover a set of classification variables. We find that thisproblem is harder for Bayesian networks than for undirected graphical modelslike maximum margin Markov networks. The main difficulty is that the parametersin a Bayesian network must satisfy additional normalization constraints that anundirected graphical model need not respect. These additional constraintscomplicate the optimization task. Nevertheless, we derive an effective trainingalgorithm that solves the maximum margin training problem for a range ofBayesian network topologies, and converges to an approximate solution forarbitrary network topologies. Experimental results show that the method candemonstrate improved generalization performance over Markov networks when thedirected graphical structure encodes relevant knowledge. In practice, thetraining technique allows one to combine prior knowledge expressed as adirected (causal) model with state of the art discriminative learning methods.
arxiv-1500-287 | Bayes Blocks: An Implementation of the Variational Bayesian Building Blocks Framework | http://arxiv.org/pdf/1207.1380v1.pdf | author:Markus Harva, Tapani Raiko, Antti Honkela, Harri Valpola, Juha Karhunen category:cs.MS cs.LG stat.ML published:2012-07-04 summary:A software library for constructing and learning probabilistic models ispresented. The library offers a set of building blocks from which a largevariety of static and dynamic models can be built. These include hierarchicalmodels for variances of other variables and many nonlinear models. Theunderlying variational Bayesian machinery, providing for fast and robustestimation but being mathematically rather involved, is almost completelyhidden from the user thus making it very easy to use the library. The buildingblocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variablesand computational nodes which can be combined rather freely.
arxiv-1500-288 | On the Detection of Concept Changes in Time-Varying Data Stream by Testing Exchangeability | http://arxiv.org/pdf/1207.1379v1.pdf | author:Shen-Shyang Ho, Harry Wechsler category:cs.LG stat.ML published:2012-07-04 summary:A martingale framework for concept change detection based on testing dataexchangeability was recently proposed (Ho, 2005). In this paper, we describethe proposed change-detection test based on the Doob's Maximal Inequality andshow that it is an approximation of the sequential probability ratio test(SPRT). The relationship between the threshold value used in the proposed testand its size and power is deduced from the approximation. The mean delay timebefore a change is detected is estimated using the average sample number of aSPRT. The performance of the test using various threshold values is examined onfive different data stream scenarios simulated using two synthetic data sets.Finally, experimental results show that the test is effective in detectingchanges in time-varying data streams simulated using three benchmark data sets.
arxiv-1500-289 | Toward Practical N2 Monte Carlo: the Marginal Particle Filter | http://arxiv.org/pdf/1207.1396v1.pdf | author:Mike Klaas, Nando de Freitas, Arnaud Doucet category:stat.CO cs.LG stat.ML published:2012-07-04 summary:Sequential Monte Carlo techniques are useful for state estimation innon-linear, non-Gaussian dynamic models. These methods allow us to approximatethe joint posterior distribution using sequential importance sampling. In thisframework, the dimension of the target distribution grows with each time step,thus it is necessary to introduce some resampling steps to ensure that theestimates provided by the algorithm have a reasonable variance. In manyapplications, we are only interested in the marginal filtering distributionwhich is defined on a space of fixed dimension. We present a Sequential MonteCarlo algorithm called the Marginal Particle Filter which operates directly onthe marginal distribution, hence avoiding having to perform importance samplingon a space of growing dimension. Using this idea, we also derive an improvedversion of the auxiliary particle filter. We show theoretic and empiricalresults which demonstrate a reduction in variance over conventional particlefiltering, and present techniques for reducing the cost of the marginalparticle filter with N particles from O(N2) to O(N logN).
arxiv-1500-290 | Obtaining Calibrated Probabilities from Boosting | http://arxiv.org/pdf/1207.1403v1.pdf | author:Alexandru Niculescu-Mizil, Richard A. Caruana category:cs.LG stat.ML published:2012-07-04 summary:Boosted decision trees typically yield good accuracy, precision, and ROCarea. However, because the outputs from boosting are not well calibratedposterior probabilities, boosting yields poor squared error and cross-entropy.We empirically demonstrate why AdaBoost predicts distorted probabilities andexamine three calibration methods for correcting this distortion: PlattScaling, Isotonic Regression, and Logistic Correction. We also experiment withboosting using log-loss instead of the usual exponential loss. Experiments showthat Logistic Correction and boosting with log-loss work well when boostingweak models such as decision stumps, but yield poor performance when boostingmore complex models such as full decision trees. Platt Scaling and IsotonicRegression, however, significantly improve the probabilities predicted by
arxiv-1500-291 | Belief Updating and Learning in Semi-Qualitative Probabilistic Networks | http://arxiv.org/pdf/1207.1367v1.pdf | author:Cassio Polpo de Campos, Fabio Gagliardi Cozman category:cs.AI stat.ML published:2012-07-04 summary:This paper explores semi-qualitative probabilistic networks (SQPNs) thatcombine numeric and qualitative information. We first show that exactinferences with SQPNs are NPPP-Complete. We then show that existing qualitativerelations in SQPNs (plus probabilistic logic and imprecise assessments) can bedealt effectively through multilinear programming. We then discuss learning: weconsider a maximum likelihood method that generates point estimates given aSQPN and empirical data, and we describe a Bayesian-minded method that employsthe Imprecise Dirichlet Model to generate set-valued estimates.
arxiv-1500-292 | A submodular-supermodular procedure with applications to discriminative structure learning | http://arxiv.org/pdf/1207.1404v1.pdf | author:Mukund Narasimhan, Jeff A. Bilmes category:cs.LG cs.DS stat.ML published:2012-07-04 summary:In this paper, we present an algorithm for minimizing the difference betweentwo submodular functions using a variational framework which is based on (anextension of) the concave-convex procedure [17]. Because several commonly usedmetrics in machine learning, like mutual information and conditional mutualinformation, are submodular, the problem of minimizing the difference of twosubmodular problems arises naturally in many machine learning applications. Twosuch applications are learning discriminatively structured graphical models andfeature selection under computational complexity constraints. A commonly usedmetric for measuring discriminative capacity is the EAR measure which is thedifference between two conditional mutual information terms. Feature selectiontaking complexity considerations into account also fall into this frameworkbecause both the information that a set of features provide and the cost ofcomputing and using the features can be modeled as submodular functions. Thisproblem is NP-hard, and we give a polynomial time heuristic for it. We alsopresent results on synthetic data to show that classifiers based ondiscriminative graphical models using this algorithm can significantlyoutperform classifiers based on generative graphical models.
arxiv-1500-293 | A Conditional Random Field for Discriminatively-trained Finite-state String Edit Distance | http://arxiv.org/pdf/1207.1406v1.pdf | author:Andrew McCallum, Kedar Bellare, Fernando Pereira category:cs.LG cs.AI published:2012-07-04 summary:The need to measure sequence similarity arises in information extraction,object identity, data mining, biological sequence analysis, and other domains.This paper presents discriminative string-edit CRFs, a finitestate conditionalrandom field model for edit sequences between strings. Conditional randomfields have advantages over generative approaches to this problem, such as pairHMMs or the work of Ristad and Yianilos, because as conditionally-trainedmethods, they enable the use of complex, arbitrary actions and features of theinput strings. As in generative models, the training data does not have tospecify the edit sequences between the given string pairs. Unlike generativemodels, however, our model is trained on both positive and negative instancesof string pairs. We present positive experimental results on several data sets.
arxiv-1500-294 | Learning Factor Graphs in Polynomial Time & Sample Complexity | http://arxiv.org/pdf/1207.1366v1.pdf | author:Pieter Abbeel, Daphne Koller, Andrew Y. Ng category:cs.LG stat.ML published:2012-07-04 summary:We study computational and sample complexity of parameter and structurelearning in graphical models. Our main result shows that the class of factorgraphs with bounded factor size and bounded connectivity can be learned inpolynomial time and polynomial number of samples, assuming that the data isgenerated by a network in this class. This result covers both parameterestimation for a known network structure and structure learning. It implies asa corollary that we can learn factor graphs for both Bayesian networks andMarkov networks of bounded degree, in polynomial time and sample complexity.Unlike maximum likelihood estimation, our method does not require inference inthe underlying network, and so applies to networks where inference isintractable. We also show that the error of our learned model degradesgracefully when the generating distribution is not a member of the target classof networks.
arxiv-1500-295 | Learning from Sparse Data by Exploiting Monotonicity Constraints | http://arxiv.org/pdf/1207.1364v1.pdf | author:Eric E. Altendorf, Angelo C. Restificar, Thomas G. Dietterich category:cs.LG stat.ML published:2012-07-04 summary:When training data is sparse, more domain knowledge must be incorporated intothe learning algorithm in order to reduce the effective size of the hypothesisspace. This paper builds on previous work in which knowledge about qualitativemonotonicities was formally represented and incorporated into learningalgorithms (e.g., Clark & Matwin's work with the CN2 rule learning algorithm).We show how to interpret knowledge of qualitative influences, and in particularof monotonicities, as constraints on probability distributions, and toincorporate this knowledge into Bayesian network learning algorithms. We showthat this yields improved accuracy, particularly with very small training sets(e.g. less than 10 examples).
arxiv-1500-296 | Piecewise Training for Undirected Models | http://arxiv.org/pdf/1207.1409v1.pdf | author:Charles Sutton, Andrew McCallum category:cs.LG stat.ML published:2012-07-04 summary:For many large undirected models that arise in real-world applications, exactmaximumlikelihood training is intractable, because it requires computingmarginal distributions of the model. Conditional training is even moredifficult, because the partition function depends not only on the parameters,but also on the observed input, requiring repeated inference over each trainingexample. An appealing idea for such models is to independently train a localundirected classifier over each clique, afterwards combining the learnedweights into a single global model. In this paper, we show that this piecewisemethod can be justified as minimizing a new family of upper bounds on the logpartition function. On three natural-language data sets, piecewise training ismore accurate than pseudolikelihood, and often performs comparably to globaltraining using belief propagation.
arxiv-1500-297 | PAC-Bayesian Majority Vote for Late Classifier Fusion | http://arxiv.org/pdf/1207.1019v1.pdf | author:Emilie Morvant, Amaury Habrard, Stéphane Ayache category:stat.ML cs.CV cs.LG cs.MM published:2012-07-04 summary:A lot of attention has been devoted to multimedia indexing over the past fewyears. In the literature, we often consider two kinds of fusion schemes: Theearly fusion and the late fusion. In this paper we focus on late classifierfusion, where one combines the scores of each modality at the decision level.To tackle this problem, we investigate a recent and elegant well-foundedquadratic program named MinCq coming from the Machine Learning PAC-Bayestheory. MinCq looks for the weighted combination, over a set of real-valuedfunctions seen as voters, leading to the lowest misclassification rate, whilemaking use of the voters' diversity. We provide evidence that this method isnaturally adapted to late fusion procedure. We propose an extension of MinCq byadding an order- preserving pairwise loss for ranking, helping to improve MeanAveraged Precision measure. We confirm the good behavior of the MinCq-basedfusion approaches with experiments on a real image benchmark.
arxiv-1500-298 | Unsupervised spectral learning | http://arxiv.org/pdf/1207.1358v1.pdf | author:Susan Shortreed, Marina Meila category:cs.LG stat.ML published:2012-07-04 summary:In spectral clustering and spectral image segmentation, the data is partionedstarting from a given matrix of pairwise similarities S. the matrix S isconstructed by hand, or learned on a separate training set. In this paper weshow how to achieve spectral clustering in unsupervised mode. Our algorithmstarts with a set of observed pairwise features, which are possible componentsof an unknown, parametric similarity function. This function is learnediteratively, at the same time as the clustering of the data. The algorithmshows promosing results on synthetic and real data.
arxiv-1500-299 | Discovery of non-gaussian linear causal models using ICA | http://arxiv.org/pdf/1207.1413v1.pdf | author:Shohei Shimizu, Aapo Hyvarinen, Yutaka Kano, Patrik O. Hoyer category:cs.LG cs.MS stat.ML published:2012-07-04 summary:In recent years, several methods have been proposed for the discovery ofcausal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000).Such methods make various assumptions on the data generating process tofacilitate its identification from purely observational data. Continuing thisline of research, we show how to discover the complete causal structure ofcontinuous-valued data, under the assumptions that (a) the data generatingprocess is linear, (b) there are no unobserved confounders, and (c) disturbancevariables have non-gaussian distributions of non-zero variances. The solutionrelies on the use of the statistical method known as independent componentanalysis (ICA), and does not require any pre-specified time-ordering of thevariables. We provide a complete Matlab package for performing this LiNGAManalysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate theeffectiveness of the method using artificially generated data.
arxiv-1500-300 | On unified view of nullspace-type conditions for recoveries associated with general sparsity structures | http://arxiv.org/pdf/1207.1119v1.pdf | author:Anatoli Juditsky, Fatma Kilinc Karzan, Arkadi Nemirovski category:math.OC cs.IT math.IT stat.ML published:2012-07-04 summary:We discuss a general notion of "sparsity structure" and associated recoveriesof a sparse signal from its linear image of reduced dimension possiblycorrupted with noise. Our approach allows for uni?ed treatment of (a) the"usual sparsity" and "usual $\ell_1$ recovery," (b) block-sparsity withpossibly overlapping blocks and associated block-$\ell_1$ recovery, and (c)low-rank-oriented recovery by nuclear norm minimization. The proposed recoveryroutines are natural extensions of the usual $\ell_1$ minimization used inCompressed Sensing. Specifically we present nullspace-type sufficientconditions for the recovery to be precise on sparse signals in the noiselesscase. Then we derive error bounds for imperfect (nearly sparse signal, presenceof observation noise, etc.) recovery under these conditions. In all of thesecases, we present efficiently verifiable sufficient conditions for the validityof the associated nullspace properties.
