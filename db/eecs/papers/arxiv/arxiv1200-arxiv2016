arxiv-1207-1429 | Ordering-Based Search: A Simple and Effective Algorithm for Learning Bayesian Networks |  http://arxiv.org/abs/1207.1429  | author:Marc Teyssier, Daphne Koller category:cs.LG cs.AI stat.ML published:2012-07-04 summary:One of the basic tasks for Bayesian networks (BNs) is that of learning anetwork structure from data. The BN-learning problem is NP-hard, so thestandard solution is heuristic search. Many approaches have been proposed forthis task, but only a very small number outperform the baseline of greedyhill-climbing with tabu lists; moreover, many of the proposed algorithms arequite complex and hard to implement. In this paper, we propose a very simpleand easy-to-implement method for addressing this task. Our approach is based onthe well-known fact that the best network (of bounded in-degree) consistentwith a given node ordering can be found very efficiently. We therefore proposea search not over the space of structures, but over the space of orderings,selecting for each ordering the best network consistent with it. This searchspace is much smaller, makes more global search steps, has a lower branchingfactor, and avoids costly acyclicity checks. We present results for thisalgorithm on both synthetic and real data sets, evaluating both the score ofthe network found and in the running time. We show that ordering-based searchoutperforms the standard baseline, and is competitive with recent algorithmsthat are much harder to implement.
arxiv-1207-1421 | A Function Approximation Approach to Estimation of Policy Gradient for POMDP with Structured Policies |  http://arxiv.org/abs/1207.1421  | author:Huizhen Yu category:cs.LG stat.ML published:2012-07-04 summary:We consider the estimation of the policy gradient in partially observableMarkov decision processes (POMDP) with a special class of structured policiesthat are finite-state controllers. We show that the gradient estimation can bedone in the Actor-Critic framework, by making the critic compute a "value"function that does not depend on the states of POMDP. This function is theconditional mean of the true value function that depends on the states. We showthat the critic can be implemented using temporal difference (TD) methods withlinear function approximations, and the analytical results on TD andActor-Critic can be transfered to this case. Although Actor-Critic algorithmshave been used extensively in Markov decision processes (MDP), up to now theyhave not been proposed for POMDP as an alternative to the earlier proposalGPOMDP algorithm, an actor-only method. Furthermore, we show that the same ideaapplies to semi-Markov problems with a subset of finite-state controllers.
arxiv-1207-1420 | Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars |  http://arxiv.org/abs/1207.1420  | author:Luke S. Zettlemoyer, Michael Collins category:cs.CL published:2012-07-04 summary:This paper addresses the problem of mapping natural language sentences tolambda-calculus encodings of their meaning. We describe a learning algorithmthat takes as input a training set of sentences labeled with expressions in thelambda calculus. The algorithm induces a grammar for the problem, along with alog-linear model that represents a distribution over syntactic and semanticanalyses conditioned on the input sentence. We apply the method to the task oflearning natural language interfaces to databases and show that the learnedparsers outperform previous methods in two benchmark database domains.
arxiv-1207-1417 | The DLR Hierarchy of Approximate Inference |  http://arxiv.org/abs/1207.1417  | author:Michal Rosen-Zvi, Michael I. Jordan, Alan Yuille category:cs.LG stat.ML published:2012-07-04 summary:We propose a hierarchy for approximate inference based on the Dobrushin,Lanford, Ruelle (DLR) equations. This hierarchy includes existing algorithms,such as belief propagation, and also motivates novel algorithms such asfactorized neighbors (FN) algorithms and variants of mean field (MF)algorithms. In particular, we show that extrema of the Bethe free energycorrespond to approximate solutions of the DLR equations. In addition, wedemonstrate a close connection between these approximate algorithms and Gibbssampling. Finally, we compare and contrast various of the algorithms in the DLRhierarchy on spin-glass problems. The experiments show that algorithms higherup in the hierarchy give more accurate results when they converge but tend tobe less stable.
arxiv-1207-1414 | Two-Way Latent Grouping Model for User Preference Prediction |  http://arxiv.org/abs/1207.1414  | author:Eerika Savia, Kai Puolamaki, Janne Sinkkonen, Samuel Kaski category:cs.IR cs.LG stat.ML published:2012-07-04 summary:We introduce a novel latent grouping model for predicting the relevance of anew document to a user. The model assumes a latent group structure for bothusers and documents. We compared the model against a state-of-the-art method,the User Rating Profile model, where only users have a latent group structure.We estimate both models by Gibbs sampling. The new method predicts relevancemore accurately for new documents that have few known ratings. The reason isthat generalization over documents then becomes necessary and hence the twowaygrouping is profitable.
arxiv-1207-1119 | On unified view of nullspace-type conditions for recoveries associated with general sparsity structures |  http://arxiv.org/abs/1207.1119  | author:Anatoli Juditsky, Fatma Kilinc Karzan, Arkadi Nemirovski category:math.OC cs.IT math.IT stat.ML published:2012-07-04 summary:We discuss a general notion of "sparsity structure" and associated recoveriesof a sparse signal from its linear image of reduced dimension possiblycorrupted with noise. Our approach allows for uni?ed treatment of (a) the"usual sparsity" and "usual $\ell_1$ recovery," (b) block-sparsity withpossibly overlapping blocks and associated block-$\ell_1$ recovery, and (c)low-rank-oriented recovery by nuclear norm minimization. The proposed recoveryroutines are natural extensions of the usual $\ell_1$ minimization used inCompressed Sensing. Specifically we present nullspace-type sufficientconditions for the recovery to be precise on sparse signals in the noiselesscase. Then we derive error bounds for imperfect (nearly sparse signal, presenceof observation noise, etc.) recovery under these conditions. In all of thesecases, we present efficiently verifiable sufficient conditions for the validityof the associated nullspace properties.
arxiv-1207-1114 | A Fast Projected Fixed-Point Algorithm for Large Graph Matching |  http://arxiv.org/abs/1207.1114  | author:Yao Lu, Kaizhu Huang, Cheng-Lin Liu category:cs.CV published:2012-07-03 summary:We propose a fast approximate algorithm for large graph matching. A newprojected fixed-point method is defined and a new doubly stochastic projectionis adopted to derive the algorithm. Previous graph matching algorithms sufferfrom high computational complexity and therefore do not have good scalabilitywith respect to graph size. For matching two weighted graphs of $n$ nodes, ouralgorithm has time complexity only $O(n^3)$ per iteration and space complexity$O(n^2)$. In addition to its scalability, our algorithm is easy to implement,robust, and able to match undirected weighted attributed graphs of differentsizes. While the convergence rate of previous iterative graph matchingalgorithms is unknown, our algorithm is theoretically guaranteed to converge ata linear rate. Extensive experiments on large synthetic and real graphs (morethan 1,000 nodes) were conducted to evaluate the performance of variousalgorithms. Results show that in most cases our proposed algorithm achievesbetter performance than previous state-of-the-art algorithms in terms of bothspeed and accuracy in large graph matching. In particular, with high accuracy,our algorithm takes only a few seconds (in a PC) to match two graphs of 1,000nodes.
arxiv-1207-0578 | Parameterized Runtime Analyses of Evolutionary Algorithms for the Euclidean Traveling Salesperson Problem |  http://arxiv.org/abs/1207.0578  | author:Andrew M. Sutton, Frank Neumann category:cs.NE cs.DS published:2012-07-03 summary:Parameterized runtime analysis seeks to understand the influence of problemstructure on algorithmic runtime. In this paper, we contribute to thetheoretical understanding of evolutionary algorithms and carry out aparameterized analysis of evolutionary algorithms for the Euclidean travelingsalesperson problem (Euclidean TSP). We investigate the structural properties in TSP instances that influence theoptimization process of evolutionary algorithms and use this information tobound the runtime of simple evolutionary algorithms. Our analysis studies theruntime in dependence of the number of inner points $k$ and shows that $(\mu +\lambda)$ evolutionary algorithms solve the Euclidean TSP in expected time$O((\mu/\lambda) \cdot n^3\gamma(\epsilon) + n\gamma(\epsilon) + (\mu/\lambda)\cdot n^{4k}(2k-1)!)$ where $\gamma$ is a function of the minimum angle$\epsilon$ between any three points. Finally, our analysis provides insights into designing a mutation operatorthat improves the upper bound on expected runtime. We show that a mixedmutation strategy that incorporates both 2-opt moves and permutation jumpsresults in an upper bound of $O((\mu/\lambda) \cdot n^3\gamma(\epsilon) +n\gamma(\epsilon) + (\mu/\lambda) \cdot n^{2k}(k-1)!)$ for the $(\mu+\lambda)$EA.
arxiv-1207-0833 | Relational Data Mining Through Extraction of Representative Exemplars |  http://arxiv.org/abs/1207.0833  | author:Frédéric Blanchard, Michel Herbin category:cs.AI cs.IR stat.ML published:2012-07-03 summary:With the growing interest on Network Analysis, Relational Data Mining isbecoming an emphasized domain of Data Mining. This paper addresses the problemof extracting representative elements from a relational dataset. After definingthe notion of degree of representativeness, computed using the Bordaaggregation procedure, we present the extraction of exemplars which are therepresentative elements of the dataset. We use these concepts to build anetwork on the dataset. We expose the main properties of these notions and wepropose two typical applications of our framework. The first applicationconsists in resuming and structuring a set of binary images and the second inmining co-authoring relation in a research team.
arxiv-1207-0784 | Web-Based Benchmark for Keystroke Dynamics Biometric Systems: A Statistical Analysis |  http://arxiv.org/abs/1207.0784  | author:Romain Giot, Mohamad El-Abed, Christophe Rosenberger category:cs.LG published:2012-07-03 summary:Most keystroke dynamics studies have been evaluated using a specific kind ofdataset in which users type an imposed login and password. Moreover, thesestudies are optimistics since most of them use different acquisition protocols,private datasets, controlled environment, etc. In order to enhance the accuracyof keystroke dynamics' performance, the main contribution of this paper istwofold. First, we provide a new kind of dataset in which users have typed bothan imposed and a chosen pairs of logins and passwords. In addition, thekeystroke dynamics samples are collected in a web-based uncontrolledenvironment (OS, keyboards, browser, etc.). Such kind of dataset is importantsince it provides us more realistic results of keystroke dynamics' performancein comparison to the literature (controlled environment, etc.). Second, wepresent a statistical analysis of well known assertions such as therelationship between performance and password size, impact of fusion schemes onsystem overall performance, and others such as the relationship betweenperformance and entropy. We put into obviousness in this paper some new resultson keystroke dynamics in realistic conditions.
arxiv-1207-0783 | Hybrid Template Update System for Unimodal Biometric Systems |  http://arxiv.org/abs/1207.0783  | author:Romain Giot, Christophe Rosenberger, Bernadette Dorizzi category:cs.LG published:2012-07-03 summary:Semi-supervised template update systems allow to automatically take intoaccount the intra-class variability of the biometric data over time. Suchsystems can be inefficient by including too many impostor's samples or skippingtoo many genuine's samples. In the first case, the biometric reference driftsfrom the real biometric data and attracts more often impostors. In the secondcase, the biometric reference does not evolve quickly enough and alsoprogressively drifts from the real biometric data. We propose a hybrid systemusing several biometric sub-references in order to increase per- formance ofself-update systems by reducing the previously cited errors. The proposition isvalidated for a keystroke- dynamics authentication system (this modalitysuffers of high variability over time) on two consequent datasets from thestate of the art.
arxiv-1207-0771 | Polarimetric SAR Image Smoothing with Stochastic Distances |  http://arxiv.org/abs/1207.0771  | author:Leonardo Torres, Antonio C. Medeiros, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2012-07-03 summary:Polarimetric Synthetic Aperture Radar (PolSAR) images are establishing as animportant source of information in remote sensing applications. The mostcomplete format this type of imaging produces consists of complex-valuedHermitian matrices in every image coordinate and, as such, their visualizationis challenging. They also suffer from speckle noise which reduces thesignal-to-noise ratio. Smoothing techniques have been proposed in theliterature aiming at preserving different features and, analogously,projections from the cone of Hermitian positive matrices to different colorrepresentation spaces are used for enhancing certain characteristics. In thiswork we propose the use of stochastic distances between models that describethis type of data in a Nagao-Matsuyama-type of smoothing technique. Theresulting images are shown to present good visualization properties (noisereduction with preservation of fine details) in all the consideredvisualization spaces.
arxiv-1207-0757 | Generalized Statistical Complexity of SAR Imagery |  http://arxiv.org/abs/1207.0757  | author:Eliana S. de Almeida, Antonio Carlos de Medeiros, Osvaldo A. Rosso, Alejandro C. Frery category:cs.IT cs.GR math.IT stat.AP stat.ML published:2012-07-03 summary:A new generalized Statistical Complexity Measure (SCM) was proposed by Rossoet al in 2010. It is a functional that captures the notions of order/disorderand of distance to an equilibrium distribution. The former is computed by ameasure of entropy, while the latter depends on the definition of a stochasticdivergence. When the scene is illuminated by coherent radiation, image data iscorrupted by speckle noise, as is the case of ultrasound-B, sonar, laser andSynthetic Aperture Radar (SAR) sensors. In the amplitude and intensity formats,this noise is multiplicative and non-Gaussian requiring, thus, specializedtechniques for image processing and understanding. One of the most successfulfamily of models for describing these images is the Multiplicative Model whichleads, among other probability distributions, to the G0 law. This distributionhas been validated in the literature as an expressive and tractable model,deserving the "universal" denomination for its ability to describe most typesof targets. In order to compute the statistical complexity of a site in animage corrupted by speckle noise, we assume that the equilibrium distributionis that of fully developed speckle, namely the Gamma law in intensity format,which appears in areas with little or no texture. We use the Shannon entropyalong with the Hellinger distance to measure the statistical complexity ofintensity SAR images, and we show that it is an expressive feature capable ofidentifying many types of targets.
arxiv-1207-0742 | The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling |  http://arxiv.org/abs/1207.0742  | author:Marc Dymetman, Guillaume Bouchard, Simon Carter category:cs.AI cs.CL cs.LG published:2012-07-03 summary:Most current sampling algorithms for high-dimensional distributions are basedon MCMC techniques and are approximate in the sense that they are valid onlyasymptotically. Rejection sampling, on the other hand, produces valid samples,but is unrealistically slow in high-dimension spaces. The OS* algorithm that wepropose is a unified approach to exact optimization and sampling, based onincremental refinements of a functional upper bound, which combines ideas ofadaptive rejection sampling and of A* optimization search. We show that thechoice of the refinement can be done in a way that ensures tractability inhigh-dimension spaces, and we present first experiments in two differentsettings: inference in high-order HMMs and in large discrete graphical models.
arxiv-1207-1115 | Inferring land use from mobile phone activity |  http://arxiv.org/abs/1207.1115  | author:Jameson L. Toole, Michael Ulm, Dietmar Bauer, Marta C. Gonzalez category:stat.ML cs.LG physics.soc-ph H.2.8 published:2012-07-03 summary:Understanding the spatiotemporal distribution of people within a city iscrucial to many planning applications. Obtaining data to create requiredknowledge, currently involves costly survey methods. At the same timeubiquitous mobile sensors from personal GPS devices to mobile phones arecollecting massive amounts of data on urban systems. The locations,communications, and activities of millions of people are recorded and stored bynew information technologies. This work utilizes novel dynamic data, generatedby mobile phone users, to measure spatiotemporal changes in population. In theprocess, we identify the relationship between land use and dynamic populationover the course of a typical week. A machine learning classification algorithmis used to identify clusters of locations with similar zoned uses and mobilephone activity patterns. It is shown that the mobile phone data is capable ofdelivering useful information on actual land use that supplements zoningregulations.
arxiv-1207-0560 | Algorithms for Approximate Minimization of the Difference Between Submodular Functions, with Applications |  http://arxiv.org/abs/1207.0560  | author:Rishabh Iyer, Jeff Bilmes category:cs.DS cs.LG published:2012-07-03 summary:We extend the work of Narasimhan and Bilmes [30] for minimizing set functionsrepresentable as a difference between submodular functions. Similar to [30],our new algorithms are guaranteed to monotonically reduce the objectivefunction at every step. We empirically and theoretically show that theper-iteration cost of our algorithms is much less than [30], and our algorithmscan be used to efficiently minimize a difference between submodular functionsunder various combinatorial constraints, a problem not previously addressed. Weprovide computational bounds and a hardness result on the mul- tiplicativeinapproximability of minimizing the difference between submodular functions. Weshow, however, that it is possible to give worst-case additive bounds byproviding a polynomial time computable lower-bound on the minima. Finally weshow how a number of machine learning problems can be modeled as minimizing thedifference between submodular functions. We experimentally show the validity ofour algorithms by testing them on the problem of feature selection withsubmodular cost features.
arxiv-1207-0805 | Anatomical Structure Segmentation in Liver MRI Images |  http://arxiv.org/abs/1207.0805  | author:G. Geethu Lakshmi category:cs.CV published:2012-07-03 summary:Segmentation of medical images is a challenging task owing to theircomplexity. A standard segmentation problem within Magnetic Resonance Imaging(MRI) is the task of labeling voxels according to their tissue type. Imagesegmentation provides volumetric quantification of liver area and thus helps inthe diagnosis of disorders, such as Hepatitis, Cirrhosis, Jaundice,Hemochromatosis etc.This work deals with comparison of segmentation by applyingLevel Set Method,Fuzzy Level Information C-Means Clustering Algorithm andGradient Vector Flow Snake Algorithm.The results are compared using theparameters such as Number of pixels correctly classified, and percentage ofarea segmented.
arxiv-1207-0704 | Speckle Reduction using Stochastic Distances |  http://arxiv.org/abs/1207.0704  | author:Leonardo Torres, Tamer Cavalcante, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML published:2012-07-03 summary:This paper presents a new approach for filter design based on stochasticdistances and tests between distributions. A window is defined around eachpixel, samples are compared and only those which pass a goodness-of-fit testare used to compute the filtered value. The technique is applied to intensitySynthetic Aperture Radar (SAR) data, using the Gamma model with varying numberof looks allowing, thus, changes in heterogeneity. Modified Nagao-Matsuyamawindows are used to define the samples. The proposal is compared with the Lee'sfilter which is considered a standard, using a protocol based on simulation.Among the criteria used to quantify the quality of filters, we employ theequivalent number of looks (related to the signal-to-noise ratio), linecontrast, and edge preservation. Moreover, we also assessed the filters by theUniversal Image Quality Index and the Pearson's correlation between edges.
arxiv-1207-0702 | Meme as Building Block for Evolutionary Optimization of Problem Instances |  http://arxiv.org/abs/1207.0702  | author:Liang Feng, Yew Soon Ong, Ah Hwee Tan, Ivor Wai-Hung Tsang category:cs.NE published:2012-07-03 summary:A significantly under-explored area of evolutionary optimization in theliterature is the study of optimization methodologies that can evolve alongwith the problems solved. Particularly, present evolutionary optimizationapproaches generally start their search from scratch or the ground-zero stateof knowledge, independent of how similar the given new problem of interest isto those optimized previously. There has thus been the apparent lack ofautomated knowledge transfers and reuse across problems. Taking the cue, thispaper introduces a novel Memetic Computational Paradigm for search, one thatmodels after how human solves problems, and embarks on a study towardsintelligent evolutionary optimization of problems through the transfers ofstructured knowledge in the form of memes learned from previous problem-solvingexperiences, to enhance future evolutionary searches. In particular, theproposed memetic search paradigm is composed of four culture-inspiredoperators, namely, Meme Learning, Meme Selection, Meme Variation and MemeImitation. The learning operator mines for memes in the form of latentstructures derived from past experiences of problem-solving. The selectionoperator identifies the fit memes that replicate and transmit across problems,while the variation operator introduces innovations into the memes. Theimitation operator, on the other hand, defines how fit memes assimilate intothe search process of newly encountered problems, thus gearing towardsefficient and effective evolutionary optimization. Finally, comprehensivestudies on two widely studied challenging well established NP-hard routingproblem domains, particularly, the capacitated vehicle routing (CVR) andcapacitated arc routing (CAR), confirm the high efficacy of the proposedmemetic computational search paradigm for intelligent evolutionary optimizationof problems.
arxiv-1207-0677 | Local Water Diffusion Phenomenon Clustering From High Angular Resolution Diffusion Imaging (HARDI) |  http://arxiv.org/abs/1207.0677  | author:Romain Giot, Christophe Charrier, Maxime Descoteaux category:cs.LG cs.CV published:2012-07-03 summary:The understanding of neurodegenerative diseases undoubtedly passes throughthe study of human brain white matter fiber tracts. To date, diffusion magneticresonance imaging (dMRI) is the unique technique to obtain information aboutthe neural architecture of the human brain, thus permitting the study of whitematter connections and their integrity. However, a remaining challenge of thedMRI community is to better characterize complex fiber crossing configurations,where diffusion tensor imaging (DTI) is limited but high angular resolutiondiffusion imaging (HARDI) now brings solutions. This paper investigates thedevelopment of both identification and classification process of the localwater diffusion phenomenon based on HARDI data to automatically detect imagingvoxels where there are single and crossing fiber bundle populations. Thetechnique is based on knowledge extraction processes and is validated on a dMRIphantom dataset with ground truth.
arxiv-1207-0658 | On the origin of long-range correlations in texts |  http://arxiv.org/abs/1207.0658  | author:Eduardo G. Altmann, Giampaolo Cristadoro, Mirko Degli Esposti category:cs.CL physics.soc-ph published:2012-07-03 summary:The complexity of human interactions with social and natural phenomena ismirrored in the way we describe our experiences through natural language. Inorder to retain and convey such a high dimensional information, the statisticalproperties of our linguistic output has to be highly correlated in time. Anexample are the robust observations, still largely not understood, ofcorrelations on arbitrary long scales in literary texts. In this paper weexplain how long-range correlations flow from highly structured linguisticlevels down to the building blocks of a text (words, letters, etc..). Bycombining calculations and data analysis we show that correlations take form ofa bursty sequence of events once we approach the semantically relevant topicsof the text. The mechanisms we identify are fairly general and can be equallyapplied to other hierarchical settings.
arxiv-1207-0580 | Improving neural networks by preventing co-adaptation of feature detectors |  http://arxiv.org/abs/1207.0580  | author:Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R. Salakhutdinov category:cs.NE cs.CV cs.LG published:2012-07-03 summary:When a large feedforward neural network is trained on a small training set,it typically performs poorly on held-out test data. This "overfitting" isgreatly reduced by randomly omitting half of the feature detectors on eachtraining case. This prevents complex co-adaptations in which a feature detectoris only helpful in the context of several other specific feature detectors.Instead, each neuron learns to detect a feature that is generally helpful forproducing the correct answer given the combinatorially large variety ofinternal contexts in which it must operate. Random "dropout" gives bigimprovements on many benchmark tasks and sets new records for speech and objectrecognition.
arxiv-1207-0577 | Robust Dequantized Compressive Sensing |  http://arxiv.org/abs/1207.0577  | author:Ji Liu, Stephen J. Wright category:stat.ML cs.LG published:2012-07-03 summary:We consider the reconstruction problem in compressed sensing in which theobservations are recorded in a finite number of bits. They may thus containquantization errors (from being rounded to the nearest representable value) andsaturation errors (from being outside the range of representable values). Ourformulation has an objective of weighted $\ell_2$-$\ell_1$ type, along withconstraints that account explicitly for quantization and saturation errors, andis solved with an augmented Lagrangian method. We prove a consistency resultfor the recovered solution, stronger than those that have appeared to date inthe literature, showing in particular that asymptotic consistency can beobtained without oversampling. We present extensive computational comparisonswith formulations proposed previously, and variants thereof.
arxiv-1207-0396 | Applying Deep Belief Networks to Word Sense Disambiguation |  http://arxiv.org/abs/1207.0396  | author:Peratham Wiriyathammabhum, Boonserm Kijsirikul, Hiroya Takamura, Manabu Okumura category:cs.CL cs.LG published:2012-07-02 summary:In this paper, we applied a novel learning algorithm, namely, Deep BeliefNetworks (DBN) to word sense disambiguation (WSD). DBN is a probabilisticgenerative model composed of multiple layers of hidden units. DBN usesRestricted Boltzmann Machine (RBM) to greedily train layer by layer as apretraining. Then, a separate fine tuning step is employed to improve thediscriminative power. We compared DBN with various state-of-the-art supervisedlearning algorithms in WSD such as Support Vector Machine (SVM), MaximumEntropy model (MaxEnt), Naive Bayes classifier (NB) and Kernel PrincipalComponent Analysis (KPCA). We used all words in the given paragraph,surrounding context words and part-of-speech of surrounding words as ourknowledge sources. We conducted our experiment on the SENSEVAL-2 data set. Weobserved that DBN outperformed all other learning algorithms.
arxiv-1207-0369 | More Effective Crossover Operators for the All-Pairs Shortest Path Problem |  http://arxiv.org/abs/1207.0369  | author:Benjamin Doerr, Daniel Johannsen, Timo Kötzing, Frank Neumann, Madeleine Theile category:cs.NE published:2012-07-02 summary:The all-pairs shortest path problem is the first non-artificial problem forwhich it was shown that adding crossover can significantly speed up amutation-only evolutionary algorithm. Recently, the analysis of this algorithmwas refined and it was shown to have an expected optimization time (w.r.t. thenumber of fitness evaluations) of $\Theta(n^{3.25}(\log n)^{0.25})$. In contrast to this simple algorithm, evolutionary algorithms used inpractice usually employ refined recombination strategies in order to avoid thecreation of infeasible offspring. We study extensions of the basic algorithm bytwo such concepts which are central in recombination, namely \emph{repairmechanisms} and \emph{parent selection}. We show that repairing infeasibleoffspring leads to an improved expected optimization time of$\mathord{O}(n^{3.2}(\log n)^{0.2})$. As a second part of our study we provethat choosing parents that guarantee feasible offspring results in an evenbetter optimization time of $\mathord{O}(n^{3}\log n)$. Both results show that already simple adjustments of the recombinationoperator can asymptotically improve the runtime of evolutionary algorithms.
arxiv-1207-0268 | Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses |  http://arxiv.org/abs/1207.0268  | author:Shivani Agarwal category:cs.LG stat.ML I.2.6 published:2012-07-02 summary:The problem of bipartite ranking, where instances are labeled positive ornegative and the goal is to learn a scoring function that minimizes theprobability of mis-ranking a pair of positive and negative instances (orequivalently, that maximizes the area under the ROC curve), has been widelystudied in recent years. A dominant theoretical and algorithmic framework forthe problem has been to reduce bipartite ranking to pairwise classification; inparticular, it is well known that the bipartite ranking regret can beformulated as a pairwise classification regret, which in turn can be upperbounded using usual regret bounds for classification problems. Recently,Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms ofthe regret associated with balanced versions of the standard (non-pairwise)logistic and exponential losses. In this paper, we show that such(non-pairwise) surrogate regret bounds for bipartite ranking can be obtained interms of a broad class of proper (composite) losses that we term as stronglyproper. Our proof technique is much simpler than that of Kotlowski et al.(2011), and relies on properties of proper (composite) losses as elucidatedrecently by Reid and Williamson (2010, 2011) and others. Our result yieldsexplicit surrogate bounds (with no hidden balancing terms) in terms of avariety of strongly proper losses, including for example logistic, exponential,squared and squared hinge losses as special cases. We also obtain tightersurrogate bounds under certain low-noise conditions via a recent result ofClemencon and Robbiano (2011).
arxiv-1207-0170 | Single parameter galaxy classification: The Principal Curve through the multi-dimensional space of galaxy properties |  http://arxiv.org/abs/1207.0170  | author:M. Taghizadeh-Popp, S. Heinis, A. S. Szalay category:astro-ph.CO cs.CV stat.ML published:2012-07-01 summary:We propose to describe the variety of galaxies from SDSS by using only oneaffine parameter. To this aim, we build the Principal Curve (P-curve) passingthrough the spine of the data point cloud, considering the eigenspace derivedfrom Principal Component Analysis of morphological, physical and photometricgalaxy properties. Thus, galaxies can be labeled, ranked and classified by asingle arc length value of the curve, measured at the unique closest projectionof the data points on the P-curve. We find that the P-curve has a "W" lettershape with 3 turning points, defining 4 branches that represent distinct galaxypopulations. This behavior is controlled mainly by 2 properties, namely u-r andSFR. We further present the variations of several galaxy properties as afunction of arc length. Luminosity functions variate from steep Schechter fitsat low arc length, to double power law and ending in Log-normal fits at higharc length. Galaxy clustering shows increasing autocorrelation power at largescales as arc length increases. PCA analysis allowed to find peculiar galaxypopulations located apart from the main cloud of data points, such as small redgalaxies dominated by a disk, of relatively high stellar mass-to-light ratioand surface mass density. The P-curve allows not only dimensionality reduction,but also provides supporting evidence for relevant physical models andscenarios in extragalactic astronomy: 1) Evidence for the hierarchical mergingscenario in the formation of a selected group of red massive galaxies. Thesegalaxies present a log-normal r-band luminosity function, which might arisefrom multiplicative processes involved in this scenario. 2) Connection betweenthe onset of AGN activity and star formation quenching, which appears in greengalaxies when transitioning from blue to red populations. (Full abstract indownloadable version)
arxiv-1207-0245 | Adversarial Evaluation for Models of Natural Language |  http://arxiv.org/abs/1207.0245  | author:Noah A. Smith category:cs.CL published:2012-07-01 summary:We now have a rich and growing set of modeling tools and algorithms forinducing linguistic structure from text that is less than fully annotated. Inthis paper, we discuss some of the weaknesses of our current methodology. Wepresent a new abstract framework for evaluating natural language processing(NLP) models in general and unsupervised NLP models in particular. The centralidea is to make explicit certain adversarial roles among researchers, so thatthe different roles in an evaluation are more clearly defined and performers ofall roles are offered ways to make measurable contributions to the larger goal.Adopting this approach may help to characterize model successes and failures byencouraging earlier consideration of error analysis. The framework can beinstantiated in a variety of ways, simulating some familiar intrinsic andextrinsic evaluations as well as some new evaluations.
arxiv-1207-0166 | On Multilabel Classification and Ranking with Partial Feedback |  http://arxiv.org/abs/1207.0166  | author:Claudio Gentile, Francesco Orabona category:cs.LG published:2012-06-30 summary:We present a novel multilabel/ranking algorithm working in partialinformation settings. The algorithm is based on 2nd-order descent methods, andrelies on upper-confidence bounds to trade-off exploration and exploitation. Weanalyze this algorithm in a partial adversarial setting, where covariates canbe adversarial, but multilabel probabilities are ruled by (generalized) linearmodels. We show O(T^{1/2} log T) regret bounds, which improve in several wayson the existing results. We test the effectiveness of our upper-confidencescheme by contrasting against full-information baselines on real-worldmultilabel datasets, often obtaining comparable performance.
arxiv-1207-0099 | Density-Difference Estimation |  http://arxiv.org/abs/1207.0099  | author:Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus Christoffel du Plessis, Song Liu, Ichiro Takeuchi category:cs.LG stat.ML published:2012-06-30 summary:We address the problem of estimating the difference between two probabilitydensities. A naive approach is a two-step procedure of first estimating twodensities separately and then computing their difference. However, such atwo-step procedure does not necessarily work well because the first step isperformed without regard to the second step and thus a small error incurred inthe first stage can cause a big error in the second stage. In this paper, wepropose a single-shot procedure for directly estimating the density differencewithout separately estimating two densities. We derive a non-parametricfinite-sample error bound for the proposed single-shot density-differenceestimator and show that it achieves the optimal convergence rate. Theusefulness of the proposed method is also demonstrated experimentally.
arxiv-1207-0151 | Differentiable Pooling for Hierarchical Feature Learning |  http://arxiv.org/abs/1207.0151  | author:Matthew D. Zeiler, Rob Fergus category:cs.CV cs.LG published:2012-06-30 summary:We introduce a parametric form of pooling, based on a Gaussian, which can beoptimized alongside the features in a single global objective function. Bycontrast, existing pooling schemes are based on heuristics (e.g. local maximum)and have no clear link to the cost function of the model. Furthermore, thevariables of the Gaussian explicitly store location information, distinct fromthe appearance captured by the features, thus providing a what/wheredecomposition of the input signal. Although the differentiable pooling schemecan be incorporated in a wide range of hierarchical models, we demonstrate itin the context of a Deconvolutional Network model (Zeiler et al. ICCV 2011). Wealso explore a number of secondary issues within this model and presentdetailed experiments on MNIST digits.
arxiv-1207-0057 | Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders |  http://arxiv.org/abs/1207.0057  | author:Yoshua Bengio, Guillaume Alain, Salah Rifai category:cs.LG stat.ML published:2012-06-30 summary:Recent work suggests that some auto-encoder variants do a good job ofcapturing the local manifold structure of the unknown data generating density.This paper contributes to the mathematical understanding of this phenomenon andhelps define better justified sampling algorithms for deep learning based onauto-encoder variants. We consider an MCMC where each step samples from aGaussian whose mean and covariance matrix depend on the previous state, definesthrough its asymptotic distribution a target density. First, we show that goodchoices (in the sense of consistency) for these mean and covariance functionsare the local expected value and local covariance under that target density.Then we show that an auto-encoder with a contractive penalty capturesestimators of these local moments in its reconstruction function and itsJacobian. A contribution of this work is thus a novel alternative tomaximum-likelihood density estimation, which we call local moment matching. Italso justifies a recently proposed sampling algorithm for the ContractiveAuto-Encoder and extends it to the Denoising Auto-Encoder.
arxiv-1207-0052 | The Complexity of Learning Principles and Parameters Grammars |  http://arxiv.org/abs/1207.0052  | author:Jacob Andreas category:cs.FL cs.CL published:2012-06-30 summary:We investigate models for learning the class of context-free andcontext-sensitive languages (CFLs and CSLs). We begin with a brief discussionof some early hardness results which show that unrestricted language learningis impossible, and unrestricted CFL learning is computationally infeasible; wethen briefly survey the literature on algorithms for learning restrictedsubclasses of the CFLs. Finally, we introduce a new family of subclasses, theprincipled parametric context-free grammars (and a corresponding family ofprincipled parametric context-sensitive grammars), which roughly model the"Principles and Parameters" framework in psycholinguistics. We present threehardness results: first, that the PPCFGs are not efficiently learnable givenequivalence and membership oracles, second, that the PPCFGs are not efficientlylearnable from positive presentations unless P = NP, and third, that the PPCSGsare not efficiently learnable from positive presentations unless integerfactorization is in P.
arxiv-1206-6927 | Consistent Biclustering |  http://arxiv.org/abs/1206.6927  | author:Cheryl J. Flynn, Patrick O. Perry category:stat.ME math.ST stat.ML stat.TH published:2012-06-29 summary:Biclustering, the process of simultaneously clustering the rows and columnsof a data matrix, is a popular and effective tool for finding structure in ahigh-dimensional dataset. Many biclustering procedures appear to work well inpractice, but most do not have associated consistency guarantees. To addressthis shortcoming, we propose a new biclustering procedure based on profilelikelihood. The procedure applies to a broad range of data modalities,including binary, count, and continuous observations. We prove that theprocedure recovers the true row and column classes when the dimensions of thedata matrix tend to infinity, even if the functional form of the datadistribution is misspecified. The procedure requires computing a combinatorialsearch, which can be expensive in practice. Rather than performing this searchdirectly, we propose a new heuristic optimization procedure based on theKernighan-Lin heuristic, which has nice computational properties and performswell in simulations. We demonstrate our procedure with applications tocongressional voting records, and microarray analysis.
arxiv-1206-7051 | Stochastic Variational Inference |  http://arxiv.org/abs/1206.7051  | author:Matt Hoffman, David M. Blei, Chong Wang, John Paisley category:stat.ML cs.AI stat.CO stat.ME published:2012-06-29 summary:We develop stochastic variational inference, a scalable algorithm forapproximating posterior distributions. We develop this technique for a largeclass of probabilistic models and we demonstrate it with two probabilistictopic models, latent Dirichlet allocation and the hierarchical Dirichletprocess topic model. Using stochastic variational inference, we analyze severallarge collections of documents: 300K articles from Nature, 1.8M articles fromThe New York Times, and 3.8M articles from Wikipedia. Stochastic inference caneasily handle data sets of this size and outperforms traditional variationalinference, which can only handle a smaller subset. (We also show that theBayesian nonparametric topic model outperforms its parametric counterpart.)Stochastic variational inference lets us apply complex Bayesian models tomassive data sets.
arxiv-1207-7244 | Visual Vocabulary Learning and Its Application to 3D and Mobile Visual Search |  http://arxiv.org/abs/1207.7244  | author:Liujuan Cao category:cs.CV published:2012-06-29 summary:In this technical report, we review related works and recent trends in visualvocabulary based web image search, object recognition, mobile visual search,and 3D object retrieval. Especial focuses would be also given for the recenttrends in supervised/unsupervised vocabulary optimization, compact descriptorfor visual search, as well as in multi-view based 3D object representation.
arxiv-1206-7112 | A Hybrid Method for Distance Metric Learning |  http://arxiv.org/abs/1206.7112  | author:Yi-Hao Kao, Benjamin Van Roy, Daniel Rubin, Jiajing Xu, Jessica Faruque, Sandy Napel category:cs.LG cs.IR stat.ML published:2012-06-29 summary:We consider the problem of learning a measure of distance among vectors in afeature space and propose a hybrid method that simultaneously learns fromsimilarity ratings assigned to pairs of vectors and class labels assigned toindividual vectors. Our method is based on a generative model in which classlabels can provide information that is not encoded in feature vectors but yetrelates to perceived similarity between objects. Experiments with syntheticdata as well as a real medical image retrieval problem demonstrate thatleveraging class labels through use of our method improves retrievalperformance significantly.
arxiv-1207-3749 | Preliminary Design of Debris Removal Missions by Means of Simplified Models for Low-Thrust, Many-Revolution Transfers |  http://arxiv.org/abs/1207.3749  | author:Federico Zuiani, Massimiliano Vasile category:math.OC cs.NE published:2012-06-29 summary:This paper presents a novel approach for the preliminary design ofLow-Thrust, many-revolution transfers. The main feature of the novel approachis a considerable reduction in the control parameters and a consequent gain incomputational speed. Each spiral is built by using a predefined pattern forthrust direction and switching structure. The pattern is then optimised tominimise propellant consumption and transfer time. The variation of the orbitalelements due to the thrust is computed analytically from a first-order solutionof the perturbed Keplerian motion. The proposed approach allows for a realisticestimation of {\Delta}V and time of flight required to transfer a spacecraftbetween two arbitrary orbits. Eccentricity and plane changes are both accountedfor. The novel approach is applied here to the design of missions for theremoval of space debris by means of an Ion Beam Shepherd Spacecraft. Inparticular, two slightly different variants of the proposed low-thrust controlmodel are used for the different phases of the mission. Thanks to their lowcomputational cost they can be included in a multiobjective optimisationproblem in which the sequence and timing of the removal of five pieces ofdebris are optimised to minimise propellant consumption and mission duration.
arxiv-1206-6722 | Piecewise Linear Topology, Evolutionary Algorithms, and Optimization Problems |  http://arxiv.org/abs/1206.6722  | author:Andrew Clark category:cs.NE math.GN math.OC I.6.1 published:2012-06-28 summary:Schemata theory, Markov chains, and statistical mechanics have been used toexplain how evolutionary algorithms (EAs) work. Incremental success has beenachieved with all of these methods, but each has been stymied by limitationsrelated to its less-than-global view. We show that moving the investigationinto topological space improves our understanding of why EAs work.
arxiv-1206-6883 | Learning Neighborhoods for Metric Learning |  http://arxiv.org/abs/1206.6883  | author:Jun Wang, Adam Woznica, Alexandros Kalousis category:cs.LG published:2012-06-28 summary:Metric learning methods have been shown to perform well on different learningtasks. Many of them rely on target neighborhood relationships that are computedin the original feature space and remain fixed throughout learning. As aresult, the learned metric reflects the original neighborhood relations. Wepropose a novel formulation of the metric learning problem in which, inaddition to the metric, the target neighborhood relations are also learned in atwo-step iterative approach. The new formulation can be seen as ageneralization of many existing metric learning methods. The formulationincludes a target neighbor assignment rule that assigns different numbers ofneighbors to instances according to their quality; `high quality' instances getmore neighbors. We experiment with two of its instantiations that correspond tothe metric learning algorithms LMNN and MCML and compare it to other metriclearning methods on a number of datasets. The experimental results showstate-of-the-art performance and provide evidence that learning theneighborhood relations does improve predictive performance.
arxiv-1206-6735 | Elimination of Spurious Ambiguity in Transition-Based Dependency Parsing |  http://arxiv.org/abs/1206.6735  | author:Shay B. Cohen, Carlos Gómez-Rodríguez, Giorgio Satta category:cs.CL cs.AI published:2012-06-28 summary:We present a novel technique to remove spurious ambiguity from transitionsystems for dependency parsing. Our technique chooses a canonical sequence oftransition operations (computation) for a given dependency tree. Our techniquecan be applied to a large class of bottom-up transition systems, including forinstance Nivre (2004) and Attardi (2006).
arxiv-1206-6679 | Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression |  http://arxiv.org/abs/1206.6679  | author:Tim Salimans, David A. Knowles category:stat.CO cs.CV stat.ML 62F15 published:2012-06-28 summary:We propose a general algorithm for approximating nonstandard Bayesianposterior distributions. The algorithm minimizes the Kullback-Leiblerdivergence of an approximating distribution to the intractable posteriordistribution. Our method can be used to approximate any posterior distribution,provided that it is given in closed form up to the proportionality constant.The approximation can be any distribution in the exponential family or anymixture of such distributions, which means that it can be made arbitrarilyprecise. Several examples illustrate the speed and accuracy of ourapproximation method in practice.
arxiv-1206-6418 | Learning Invariant Representations with Local Transformations |  http://arxiv.org/abs/1206.6418  | author:Kihyuk Sohn, Honglak Lee category:cs.LG cs.CV stat.ML published:2012-06-27 summary:Learning invariant representations is an important problem in machinelearning and pattern recognition. In this paper, we present a novel frameworkof transformation-invariant feature learning by incorporating lineartransformations into the feature learning algorithms. For example, we presentthe transformation-invariant restricted Boltzmann machine that compactlyrepresents data by its weights and their transformations, which achievesinvariance of the feature representation via probabilistic max pooling. Inaddition, we show that our transformation-invariant feature learning frameworkcan also be extended to other unsupervised learning methods, such asautoencoders or sparse coding. We evaluate our method on several imageclassification benchmark datasets, such as MNIST variations, CIFAR-10, andSTL-10, and show competitive or superior classification performance whencompared to the state-of-the-art. Furthermore, our method achievesstate-of-the-art performance on phone classification tasks with the TIMITdataset, which demonstrates wide applicability of our proposed algorithms toother domains.
arxiv-1206-6417 | Learning Task Grouping and Overlap in Multi-task Learning |  http://arxiv.org/abs/1206.6417  | author:Abhishek Kumar, Hal Daume III category:cs.LG stat.ML published:2012-06-27 summary:In the paradigm of multi-task learning, mul- tiple related prediction tasksare learned jointly, sharing information across the tasks. We propose aframework for multi-task learn- ing that enables one to selectively share theinformation across the tasks. We assume that each task parameter vector is alinear combi- nation of a finite number of underlying basis tasks. Thecoefficients of the linear combina- tion are sparse in nature and the overlapin the sparsity patterns of two tasks controls the amount of sharing acrossthese. Our model is based on on the assumption that task pa- rameters within agroup lie in a low dimen- sional subspace but allows the tasks in differ- entgroups to overlap with each other in one or more bases. Experimental results onfour datasets show that our approach outperforms competing methods.
arxiv-1206-6416 | An Infinite Latent Attribute Model for Network Data |  http://arxiv.org/abs/1206.6416  | author:Konstantina Palla, David Knowles, Zoubin Ghahramani category:cs.LG stat.ML published:2012-06-27 summary:Latent variable models for network data extract a summary of the relationalstructure underlying an observed network. The simplest possible modelssubdivide nodes of the network into clusters; the probability of a link betweenany two nodes then depends only on their cluster assignment. Currentlyavailable models can be classified by whether clusters are disjoint or areallowed to overlap. These models can explain a "flat" clustering structure.Hierarchical Bayesian models provide a natural approach to capture more complexdependencies. We propose a model in which objects are characterised by a latentfeature vector. Each feature is itself partitioned into disjoint groups(subclusters), corresponding to a second layer of hierarchy. In experimentalcomparisons, the model achieves significantly improved predictive performanceon social and biological link prediction tasks. The results indicate thatmodels with a single layer hierarchy over-simplify real networks.
arxiv-1206-6415 | The Big Data Bootstrap |  http://arxiv.org/abs/1206.6415  | author:Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael Jordan category:cs.LG stat.ML published:2012-06-27 summary:The bootstrap provides a simple and powerful means of assessing the qualityof estimators. However, in settings involving large datasets, the computationof bootstrap-based quantities can be prohibitively demanding. As analternative, we present the Bag of Little Bootstraps (BLB), a new procedurewhich incorporates features of both the bootstrap and subsampling to obtain arobust, computationally efficient means of assessing estimator quality. BLB iswell suited to modern parallel and distributed computing architectures andretains the generic applicability, statistical efficiency, and favorabletheoretical properties of the bootstrap. We provide the results of an extensiveempirical and theoretical investigation of BLB's behavior, including a study ofits statistical correctness, its large-scale implementation and performance,selection of hyperparameters, and performance on real data.
arxiv-1206-6414 | The Nonparametric Metadata Dependent Relational Model |  http://arxiv.org/abs/1206.6414  | author:Dae Il Kim, Michael Hughes, Erik Sudderth category:cs.LG cs.SI stat.ML published:2012-06-27 summary:We introduce the nonparametric metadata dependent relational (NMDR) model, aBayesian nonparametric stochastic block model for network data. The NMDR allowsthe entities associated with each node to have mixed membership in an unboundedcollection of latent communities. Learned regression models allow thesememberships to depend on, and be predicted from, arbitrary node metadata. Wedevelop efficient MCMC algorithms for learning NMDR models from partiallyobserved node relationships. Retrospective MCMC methods allow our sampler towork directly with the infinite stick-breaking representation of the NMDR,avoiding the need for finite truncations. Our results demonstrate recovery ofuseful latent communities from real-world social and ecological networks, andthe usefulness of metadata in link prediction tasks.
arxiv-1206-6419 | Cross-Domain Multitask Learning with Latent Probit Models |  http://arxiv.org/abs/1206.6419  | author:Shaobo Han, Xuejun Liao, Lawrence Carin category:cs.LG stat.ML published:2012-06-27 summary:Learning multiple tasks across heterogeneous domains is a challenging problemsince the feature space may not be the same for different tasks. We assume thedata in multiple tasks are generated from a latent common domain via sparsedomain transforms and propose a latent probit model (LPM) to jointly learn thedomain transforms, and the shared probit classifier in the common domain. Tolearn meaningful task relatedness and avoid over-fitting in classification, weintroduce sparsity in the domain transforms matrices, as well as in the commonclassifier. We derive theoretical bounds for the estimation error of theclassifier in terms of the sparsity of domain transforms. Anexpectation-maximization algorithm is derived for learning the LPM. Theeffectiveness of the approach is demonstrated on several real datasets.
arxiv-1206-6413 | A Convex Relaxation for Weakly Supervised Classifiers |  http://arxiv.org/abs/1206.6413  | author:Armand Joulin, Francis Bach category:cs.LG stat.ML published:2012-06-27 summary:This paper introduces a general multi-class approach to weakly supervisedclassification. Inferring the labels and learning the parameters of the modelis usually done jointly through a block-coordinate descent algorithm such asexpectation-maximization (EM), which may lead to local minima. To avoid thisproblem, we propose a cost function based on a convex relaxation of thesoft-max loss. We then propose an algorithm specifically designed toefficiently solve the corresponding semidefinite program (SDP). Empirically,our method compares favorably to standard ones on different datasets formultiple instance learning and semi-supervised learning as well as onclustering tasks.
arxiv-1206-6420 | Distributed Parameter Estimation via Pseudo-likelihood |  http://arxiv.org/abs/1206.6420  | author:Qiang Liu, Alexander Ihler category:cs.LG cs.DC stat.ML published:2012-06-27 summary:Estimating statistical models within sensor networks requires distributedalgorithms, in which both data and computation are distributed across the nodesof the network. We propose a general approach for distributed learning based oncombining local estimators defined by pseudo-likelihood components,encompassing a number of combination methods, and provide both theoretical andexperimental analysis. We show that simple linear combination or max-votingmethods, when combined with second-order information, are statisticallycompetitive with more advanced and costly joint optimization. Our algorithmshave many attractive properties including low communication and computationalcost and "any-time" behavior.
arxiv-1206-6412 | A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound |  http://arxiv.org/abs/1206.6412  | author:Ming Ji, Tianbao Yang, Binbin Lin, Rong Jin, Jiawei Han category:cs.LG stat.ML published:2012-06-27 summary:In this work, we develop a simple algorithm for semi-supervised regression.The key idea is to use the top eigenfunctions of integral operator derived fromboth labeled and unlabeled examples as the basis functions and learn theprediction function by a simple linear regression. We show that underappropriate assumptions about the integral operator, this approach is able toachieve an improved regression error bound better than existing bounds ofsupervised learning. We also verify the effectiveness of the proposed algorithmby an empirical study.
arxiv-1206-6230 | Decentralized Data Fusion and Active Sensing with Mobile Sensors for Modeling and Predicting Spatiotemporal Traffic Phenomena |  http://arxiv.org/abs/1206.6230  | author:Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan, Ali Oran, Patrick Jaillet, John M. Dolan, Gaurav S. Sukhatme category:cs.LG cs.AI cs.DC cs.MA cs.RO published:2012-06-27 summary:The problem of modeling and predicting spatiotemporal traffic phenomena overan urban road network is important to many traffic applications such asdetecting and forecasting congestion hotspots. This paper presents adecentralized data fusion and active sensing (D2FAS) algorithm for mobilesensors to actively explore the road network to gather and assimilate the mostinformative data for predicting the traffic phenomenon. We analyze the time andcommunication complexity of D2FAS and demonstrate that it can scale well with alarge number of observations and sensors. We provide a theoretical guarantee onits predictive performance to be equivalent to that of a sophisticatedcentralized sparse approximation for the Gaussian process (GP) model: Thecomputation of such a sparse approximate GP model can thus be parallelized anddistributed among the mobile sensors (in a Google-like MapReduce paradigm),thereby achieving efficient and scalable prediction. We also theoreticallyguarantee its active sensing performance that improves under various practicalenvironmental conditions. Empirical evaluation on real-world urban road networkdata shows that our D2FAS algorithm is significantly more time-efficient andscalable than state-of-the-art centralized algorithms while achievingcomparable predictive performance.
arxiv-1206-6421 | Structured Learning from Partial Annotations |  http://arxiv.org/abs/1206.6421  | author:Xinghua Lou, Fred Hamprecht category:cs.LG stat.ML published:2012-06-27 summary:Structured learning is appropriate when predicting structured outputs such astrees, graphs, or sequences. Most prior work requires the training set toconsist of complete trees, graphs or sequences. Specifying such detailed groundtruth can be tedious or infeasible for large outputs. Our main contribution isa large margin formulation that makes structured learning from only partiallyannotated data possible. The resulting optimization problem is non-convex, yetcan be efficiently solve by concave-convex procedure (CCCP) with novel speedupstrategies. We apply our method to a challenging tracking-by-assignment problemof a variable number of divisible objects. On this benchmark, using only 25% ofa full annotation we achieve a performance comparable to a model learned with afull annotation. Finally, we offer a unifying perspective of previous workusing the hinge, ramp, or max loss for structured learning, followed by anempirical comparison on their practical performance.
arxiv-1206-6411 | On the Difficulty of Nearest Neighbor Search |  http://arxiv.org/abs/1206.6411  | author:Junfeng He, Sanjiv Kumar, Shih-Fu Chang category:cs.LG cs.DB cs.IR stat.ML published:2012-06-27 summary:Fast approximate nearest neighbor (NN) search in large databases is becomingpopular. Several powerful learning-based formulations have been proposedrecently. However, not much attention has been paid to a more fundamentalquestion: how difficult is (approximate) nearest neighbor search in a givendata set? And which data properties affect the difficulty of nearest neighborsearch and how? This paper introduces the first concrete measure calledRelative Contrast that can be used to evaluate the influence of several crucialdata characteristics such as dimensionality, sparsity, and database sizesimultaneously in arbitrary normed metric spaces. Moreover, we present atheoretical analysis to prove how the difficulty measure (relative contrast)determines/affects the complexity of Local Sensitive Hashing, a popularapproximate NN search method. Relative contrast also provides an explanationfor a family of heuristic hashing algorithms with good practical performancebased on PCA. Finally, we show that most of the previous works in measuring NNsearch meaningfulness/difficulty can be derived as special asymptotic cases fordense vectors of the proposed measure.
arxiv-1206-6410 | On the Partition Function and Random Maximum A-Posteriori Perturbations |  http://arxiv.org/abs/1206.6410  | author:Tamir Hazan, Tommi Jaakkola category:cs.LG stat.ML published:2012-06-27 summary:In this paper we relate the partition function to the max-statistics ofrandom variables. In particular, we provide a novel framework for approximatingand bounding the partition function using MAP inference on randomly perturbedmodels. As a result, we can use efficient MAP solvers such as graph-cuts toevaluate the corresponding partition function. We show that our method excelsin the typical "high signal - high coupling" regime that results in raggedenergy landscapes difficult for alternative approaches.
arxiv-1206-6409 | Scaling Up Coordinate Descent Algorithms for Large $\ell_1$ Regularization Problems |  http://arxiv.org/abs/1206.6409  | author:Chad Scherrer, Mahantesh Halappanavar, Ambuj Tewari, David Haglin category:cs.LG cs.DC stat.ML published:2012-06-27 summary:We present a generic framework for parallel coordinate descent (CD)algorithms that includes, as special cases, the original sequential algorithmsCyclic CD and Stochastic CD, as well as the recent parallel Shotgun algorithm.We introduce two novel parallel algorithms that are also specialcases---Thread-Greedy CD and Coloring-Based CD---and give performancemeasurements for an OpenMP implementation of these.
arxiv-1206-6408 | Sequential Nonparametric Regression |  http://arxiv.org/abs/1206.6408  | author:Haijie Gu, John Lafferty category:stat.ME astro-ph.IM cs.LG published:2012-06-27 summary:We present algorithms for nonparametric regression in settings where the dataare obtained sequentially. While traditional estimators select bandwidths thatdepend upon the sample size, for sequential data the effective sample size isdynamically changing. We propose a linear time algorithm that adjusts thebandwidth for each new data point, and show that the estimator achieves theoptimal minimax rate of convergence. We also propose the use of online expertmixing algorithms to adapt to unknown smoothness of the regression function. Weprovide simulations that confirm the theoretical results, and demonstrate theeffectiveness of the methods.
arxiv-1206-6422 | An Online Boosting Algorithm with Theoretical Justifications |  http://arxiv.org/abs/1206.6422  | author:Shang-Tse Chen, Hsuan-Tien Lin, Chi-Jen Lu category:cs.LG stat.ML published:2012-06-27 summary:We study the task of online boosting--combining online weak learners into anonline strong learner. While batch boosting has a sound theoretical foundation,online boosting deserves more study from the theoretical perspective. In thispaper, we carefully compare the differences between online and batch boosting,and propose a novel and reasonable assumption for the online weak learner.Based on the assumption, we design an online boosting algorithm with a strongtheoretical guarantee by adapting from the offline SmoothBoost algorithm thatmatches the assumption closely. We further tackle the task of deciding thenumber of weak learners using established theoretical results for online convexprogramming and predicting with expert advice. Experiments on real-world datasets demonstrate that the proposed algorithm compares favorably with existingonline boosting algorithms.
arxiv-1206-6407 | Large-Scale Feature Learning With Spike-and-Slab Sparse Coding |  http://arxiv.org/abs/1206.6407  | author:Ian Goodfellow, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML published:2012-06-27 summary:We consider the problem of object recognition with a large number of classes.In order to overcome the low amount of labeled examples available in thissetting, we introduce a new feature learning and extraction procedure based ona factor model we call spike-and-slab sparse coding (S3C). Prior work on S3Chas not prioritized the ability to exploit parallel architectures and scale S3Cto the enormous problem sizes needed for object recognition. We present a novelinference procedure for appropriate for use with GPUs which allows us todramatically increase both the training set size and the amount of latentfactors that S3C may be trained with. We demonstrate that this approachimproves upon the supervised learning capabilities of both sparse coding andthe spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10dataset. We use the CIFAR-100 dataset to demonstrate that our method scales tolarge numbers of classes better than previous methods. Finally, we use ourmethod to win the NIPS 2011 Workshop on Challenges In Learning HierarchicalModels? Transfer Learning Challenge.
arxiv-1206-6406 | Bayesian Optimal Active Search and Surveying |  http://arxiv.org/abs/1206.6406  | author:Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong, Jeff Schneider, Richard Mann category:cs.LG stat.ML published:2012-06-27 summary:We consider two active binary-classification problems with atypicalobjectives. In the first, active search, our goal is to actively uncover asmany members of a given class as possible. In the second, active surveying, ourgoal is to actively query points to ultimately predict the proportion of agiven class. Numerous real-world problems can be framed in these terms, and ineither case typical model-based concerns such as generalization error are onlyof secondary importance. We approach these problems via Bayesian decision theory; after choosingnatural utility functions, we derive the optimal policies. We provide threecontributions. In addition to introducing the active surveying problem, weextend previous work on active search in two ways. First, we prove a noveltheoretical result, that less-myopic approximations to the optimal policy canoutperform more-myopic approximations by any arbitrary degree. We then derivebounds that for certain models allow us to reduce (in practice dramatically)the exponential search space required by a na?ve implementation of the optimalpolicy, enabling further lookahead while still ensuring that optimal decisionsare always made.
arxiv-1206-6405 | Bounded Planning in Passive POMDPs |  http://arxiv.org/abs/1206.6405  | author:Roy Fox, Naftali Tishby category:cs.LG cs.AI stat.ML published:2012-06-27 summary:In Passive POMDPs actions do not affect the world state, but still incurcosts. When the agent is bounded by information-processing constraints, it canonly keep an approximation of the belief. We present a variational principlefor the problem of maintaining the information which is most useful forminimizing the cost, and introduce an efficient and simple algorithm forfinding an optimum.
arxiv-1206-6404 | Policy Gradients with Variance Related Risk Criteria |  http://arxiv.org/abs/1206.6404  | author:Dotan Di Castro, Aviv Tamar, Shie Mannor category:cs.LG cs.CY math.OC stat.ML published:2012-06-27 summary:Managing risk in dynamic decision problems is of cardinal importance in manyfields such as finance and process control. The most common approach todefining risk is through various variance related criteria such as the SharpeRatio or the standard deviation adjusted reward. It is known that optimizingmany of the variance related risk criteria is NP-hard. In this paper we devisea framework for local policy gradient style algorithms for reinforcementlearning for variance related criteria. Our starting point is a new formula forthe variance of the cost-to-go in episodic tasks. Using this formula we developpolicy gradient algorithms for criteria that involve both the expected cost andthe variance of the cost. We prove the convergence of these algorithms to localminima and demonstrate their applicability in a portfolio planning problem.
arxiv-1206-6423 | A Joint Model of Language and Perception for Grounded Attribute Learning |  http://arxiv.org/abs/1206.6423  | author:Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, Dieter Fox category:cs.CL cs.LG cs.RO published:2012-06-27 summary:As robots become more ubiquitous and capable, it becomes ever more importantto enable untrained users to easily interact with them. Recently, this has ledto study of the language grounding problem, where the goal is to extractrepresentations of the meanings of natural language tied to perception andactuation in the physical world. In this paper, we present an approach forjoint learning of language and perception models for grounded attributeinduction. Our perception model includes attribute classifiers, for example todetect object color and shape, and the language model is based on aprobabilistic categorial grammar that enables the construction of rich,compositional meaning representations. The approach is evaluated on the task ofinterpreting sentences that describe sets of objects in a physical workspace.We demonstrate accurate task performance and effective latent-variable conceptinduction in physical grounded scenes.
arxiv-1206-6403 | Two Step CCA: A new spectral method for estimating vector models of words |  http://arxiv.org/abs/1206.6403  | author:Paramveer Dhillon, Jordan Rodu, Dean Foster, Lyle Ungar category:cs.CL cs.LG published:2012-06-27 summary:Unlabeled data is often used to learn representations which can be used tosupplement baseline features in a supervised learner. For example, for textapplications where the words lie in a very high dimensional space (the size ofthe vocabulary), one can learn a low rank "dictionary" by aneigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA).In this paper, we present a new spectral method based on CCA to learn aneigenword dictionary. Our improved procedure computes two set of CCAs, thefirst one between the left and right contexts of the given word and the secondone between the projections resulting from this CCA and the word itself. Weprove theoretically that this two-step procedure has lower sample complexitythan the simple single step procedure and also illustrate the empiricalefficacy of our approach and the richness of representations learned by our TwoStep CCA (TSCCA) procedure on the tasks of POS tagging and sentimentclassification.
arxiv-1206-6402 | Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization |  http://arxiv.org/abs/1206.6402  | author:Thomas Desautels, Andreas Krause, Joel Burdick category:cs.LG stat.ML published:2012-06-27 summary:Can one parallelize complex exploration exploitation tradeoffs? As anexample, consider the problem of optimal high-throughput experimental design,where we wish to sequentially design batches of experiments in order tosimultaneously learn a surrogate function mapping stimulus to response andidentify the maximum of the function. We formalize the task as a multi-armedbandit problem, where the unknown payoff function is sampled from a Gaussianprocess (GP), and instead of a single arm, in each round we pull a batch ofseveral arms in parallel. We develop GP-BUCB, a principled algorithm forchoosing batches, based on the GP-UCB algorithm for sequential GP optimization.We prove a surprising result; as compared to the sequential approach, thecumulative regret of the parallel algorithm only increases by a constant factorindependent of the batch size B. Our results provide rigorous theoreticalsupport for exploiting parallelism in Bayesian global optimization. Wedemonstrate the effectiveness of our approach on two real-world applications.
arxiv-1206-6428 | A Binary Classification Framework for Two-Stage Multiple Kernel Learning |  http://arxiv.org/abs/1206.6428  | author:Abhishek Kumar, Alexandru Niculescu-Mizil, Koray Kavukcuoglu, Hal Daume III category:cs.LG stat.ML published:2012-06-27 summary:With the advent of kernel methods, automating the task of specifying asuitable kernel has become increasingly important. In this context, theMultiple Kernel Learning (MKL) problem of finding a combination ofpre-specified base kernels that is suitable for the task at hand has receivedsignificant attention from researchers. In this paper we show that MultipleKernel Learning can be framed as a standard binary classification problem withadditional constraints that ensure the positive definiteness of the learnedkernel. Framing MKL in this way has the distinct advantage that it makes iteasy to leverage the extensive research in binary classification to developbetter performing and more scalable MKL algorithms that are conceptuallysimpler, and, arguably, more accessible to practitioners. Experiments on ninedata sets from different domains show that, despite its simplicity, theproposed technique compares favorably with current leading MKL approaches.
arxiv-1206-6401 | Consistent Multilabel Ranking through Univariate Losses |  http://arxiv.org/abs/1206.6401  | author:Krzysztof Dembczynski, Wojciech Kotlowski, Eyke Huellermeier category:cs.LG stat.ML published:2012-06-27 summary:We consider the problem of rank loss minimization in the setting ofmultilabel classification, which is usually tackled by means of convexsurrogate losses defined on pairs of labels. Very recently, this approach wasput into question by a negative result showing that commonly used pairwisesurrogate losses, such as exponential and logistic losses, are inconsistent. Inthis paper, we show a positive result which is arguably surprising in light ofthe previous one: the simpler univariate variants of exponential and logisticsurrogates (i.e., defined on single labels) are consistent for rank lossminimization. Instead of directly proving convergence, we give a much strongerresult by deriving regret bounds and convergence rates. The proposed lossessuggest efficient and scalable algorithms, which are tested experimentally.
arxiv-1206-6400 | Online Bandit Learning against an Adaptive Adversary: from Regret to Policy Regret |  http://arxiv.org/abs/1206.6400  | author:Raman Arora, Ofer Dekel, Ambuj Tewari category:cs.LG stat.ML published:2012-06-27 summary:Online learning algorithms are designed to learn even when their input isgenerated by an adversary. The widely-accepted formal definition of an onlinealgorithm's ability to learn is the game-theoretic notion of regret. We arguethat the standard definition of regret becomes inadequate if the adversary isallowed to adapt to the online algorithm's actions. We define the alternativenotion of policy regret, which attempts to provide a more meaningful way tomeasure an online algorithm's performance against adaptive adversaries.Focusing on the online bandit setting, we show that no bandit algorithm canguarantee a sublinear policy regret against an adaptive adversary withunbounded memory. On the other hand, if the adversary's memory is bounded, wepresent a general technique that converts any bandit algorithm with a sublinearregret bound into an algorithm with a sublinear policy regret bound. We extendthis result to other variants of regret, such as switching regret, internalregret, and swap regret.
arxiv-1206-6399 | Demand-Driven Clustering in Relational Domains for Predicting Adverse Drug Events |  http://arxiv.org/abs/1206.6399  | author:Jesse Davis, Vitor Santos Costa, Peggy Peissig, Michael Caldwell, Elizabeth Berg, David Page category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Learning from electronic medical records (EMR) is challenging due to theirrelational nature and the uncertain dependence between a patient's past andfuture health status. Statistical relational learning is a natural fit foranalyzing EMRs but is less adept at handling their inherent latent structure,such as connections between related medications or diseases. One way to capturethe latent structure is via a relational clustering of objects. We propose anovel approach that, instead of pre-clustering the objects, performs ademand-driven clustering during learning. We evaluate our algorithm on threereal-world tasks where the goal is to use EMRs to predict whether a patientwill have an adverse reaction to a medication. We find that our approach ismore accurate than performing no clustering, pre-clustering, and usingexpert-constructed medical heterarchies.
arxiv-1206-6397 | Communications Inspired Linear Discriminant Analysis |  http://arxiv.org/abs/1206.6397  | author:Minhua Chen, William Carson, Miguel Rodrigues, Robert Calderbank, Lawrence Carin category:cs.LG stat.ML published:2012-06-27 summary:We study the problem of supervised linear dimensionality reduction, taking aninformation-theoretic viewpoint. The linear projection matrix is designed bymaximizing the mutual information between the projected signal and the classlabel (based on a Shannon entropy measure). By harnessing a recent theoreticalresult on the gradient of mutual information, the above optimization problemcan be solved directly using gradient descent, without requiring simplificationof the objective function. Theoretical analysis and empirical comparison aremade between the proposed method and two closely related methods (LinearDiscriminant Analysis and Information Discriminant Analysis), and comparisonsare also made with a method in which Renyi entropy is used to define the mutualinformation (in this case the gradient may be computed simply, under a specialparameter setting). Relative to these alternative approaches, the proposedmethod achieves promising results on real datasets.
arxiv-1206-6396 | Joint Optimization and Variable Selection of High-dimensional Gaussian Processes |  http://arxiv.org/abs/1206.6396  | author:Bo Chen, Rui Castro, Andreas Krause category:cs.LG stat.ML published:2012-06-27 summary:Maximizing high-dimensional, non-convex functions through noisy observationsis a notoriously hard problem, but one that arises in many applications. Inthis paper, we tackle this challenge by modeling the unknown function as asample from a high-dimensional Gaussian process (GP) distribution. Assumingthat the unknown function only depends on few relevant variables, we show thatit is possible to perform joint variable selection and GP optimization. Weprovide strong performance guarantees for our algorithm, bounding the samplecomplexity of variable selection, and as well as providing cumulative regretbounds. We further provide empirical evidence on the effectiveness of ouralgorithm on several benchmark optimization problems.
arxiv-1206-6395 | Convergence Rates for Differentially Private Statistical Estimation |  http://arxiv.org/abs/1206.6395  | author:Kamalika Chaudhuri, Daniel Hsu category:cs.LG cs.CR stat.ML published:2012-06-27 summary:Differential privacy is a cryptographically-motivated definition of privacywhich has gained significant attention over the past few years. Differentiallyprivate solutions enforce privacy by adding random noise to a function computedover the data, and the challenge in designing such algorithms is to control theadded noise in order to optimize the privacy-accuracy-sample size tradeoff. This work studies differentially-private statistical estimation, and showsupper and lower bounds on the convergence rates of differentially privateapproximations to statistical estimators. Our results reveal a formalconnection between differential privacy and the notion of Gross ErrorSensitivity (GES) in robust statistics, by showing that the convergence rate ofany differentially private approximation to an estimator that is accurate overa large class of distributions has to grow with the GES of the estimator. Wethen provide an upper bound on the convergence rate of a differentially privateapproximation to an estimator with bounded range and bounded GES. We show thatthe bounded range condition is necessary if we wish to ensure a strict form ofdifferential privacy.
arxiv-1206-6394 | Nonparametric Link Prediction in Dynamic Networks |  http://arxiv.org/abs/1206.6394  | author:Purnamrita Sarkar, Deepayan Chakrabarti, Michael Jordan category:cs.LG cs.SI stat.ML published:2012-06-27 summary:We propose a non-parametric link prediction algorithm for a sequence of graphsnapshots over time. The model predicts links based on the features of itsendpoints, as well as those of the local neighborhood around the endpoints.This allows for different types of neighborhoods in a graph, each with its owndynamics (e.g, growing or shrinking communities). We prove the consistency ofour estimator, and give a fast implementation based on locality-sensitivehashing. Experiments with simulated as well as five real-world dynamic graphsshow that we outperform the state of the art, especially when sharpfluctuations or non-linearities are present.
arxiv-1206-6393 | Local Loss Optimization in Operator Models: A New Insight into Spectral Learning |  http://arxiv.org/abs/1206.6393  | author:Borja Balle, Ariadna Quattoni, Xavier Carreras category:cs.LG stat.ML published:2012-06-27 summary:This paper re-visits the spectral method for learning latent variable modelsdefined in terms of observable operators. We give a new perspective on themethod, showing that operators can be recovered by minimizing a loss defined ona finite subset of the domain. A non-convex optimization similar to thespectral method is derived. We also propose a regularized convex relaxation ofthis optimization. We show that in practice the availabilty of a continuousregularization parameter (in contrast with the discrete number of states in theoriginal method) allows a better trade-off between accuracy and modelcomplexity. We also prove that in general, a randomized strategy for choosingthe local loss will succeed with high probability.
arxiv-1206-6429 | Incorporating Domain Knowledge in Matching Problems via Harmonic Analysis |  http://arxiv.org/abs/1206.6429  | author:Deepti Pachauri, Maxwell Collins, Vikas SIngh, Risi Kondor category:cs.LG cs.CV stat.ML published:2012-06-27 summary:Matching one set of objects to another is a ubiquitous task in machinelearning and computer vision that often reduces to some form of the quadraticassignment problem (QAP). The QAP is known to be notoriously hard, both intheory and in practice. Here, we investigate if this difficulty can bemitigated when some additional piece of information is available: (a) that allQAP instances of interest come from the same application, and (b) the correctsolution for a set of such QAP instances is given. We propose a new approach toaccelerate the solution of QAPs based on learning parameters for a modifiedobjective function from prior QAP instances. A key feature of our approach isthat it takes advantage of the algebraic structure of permutations, inconjunction with special methods for optimizing functions over the symmetricgroup Sn in Fourier space. Experiments show that in practical domains the newmethod can outperform existing approaches.
arxiv-1206-6392 | Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription |  http://arxiv.org/abs/1206.6392  | author:Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent category:cs.LG cs.SD stat.ML published:2012-06-27 summary:We investigate the problem of modeling symbolic sequences of polyphonic musicin a completely general piano-roll representation. We introduce a probabilisticmodel based on distribution estimators conditioned on a recurrent neuralnetwork that is able to discover temporal dependencies in high-dimensionalsequences. Our approach outperforms many traditional models of polyphonic musicon a variety of realistic datasets. We show how our musical language model canserve as a symbolic prior to improve the accuracy of polyphonic transcription.
arxiv-1206-6391 | Gaussian Process Quantile Regression using Expectation Propagation |  http://arxiv.org/abs/1206.6391  | author:Alexis Boukouvalas, Remi Barillec, Dan Cornford category:stat.ME cs.LG stat.AP published:2012-06-27 summary:Direct quantile regression involves estimating a given quantile of a responsevariable as a function of input variables. We present a new framework fordirect quantile regression where a Gaussian process model is learned,minimising the expected tilted loss function. The integration required inlearning is not analytically tractable so to speed up the learning we employthe Expectation Propagation algorithm. We describe how this work relates toother quantile regression methods and apply the method on both synthetic andreal data sets. The method is shown to be competitive with state of the artmethods whilst allowing for the leverage of the full Gaussian processprobabilistic framework.
arxiv-1206-6390 | Incorporating Causal Prior Knowledge as Path-Constraints in Bayesian Networks and Maximal Ancestral Graphs |  http://arxiv.org/abs/1206.6390  | author:Giorgos Borboudakis, Ioannis Tsamardinos category:cs.AI cs.CE cs.LG published:2012-06-27 summary:We consider the incorporation of causal knowledge about the presence orabsence of (possibly indirect) causal relations into a causal model. Suchcausal relations correspond to directed paths in a causal model. This type ofknowledge naturally arises from experimental data, among others. Specifically,we consider the formalisms of Causal Bayesian Networks and Maximal AncestralGraphs and their Markov equivalence classes: Partially Directed Acyclic Graphsand Partially Oriented Ancestral Graphs. We introduce sound and completeprocedures which are able to incorporate causal prior knowledge in such models.In simulated experiments, we show that often considering even a few causalfacts leads to a significant number of new inferences. In a case study, we alsoshow how to use real experimental data to infer causal knowledge andincorporate it into a real biological causal network. The code is available atmensxmachina.org.
arxiv-1206-6388 | Canonical Trends: Detecting Trend Setters in Web Data |  http://arxiv.org/abs/1206.6388  | author:Felix Biessmann, Jens-Michalis Papaioannou, Mikio Braun, Andreas Harth category:cs.LG cs.SI stat.ML published:2012-06-27 summary:Much information available on the web is copied, reused or rephrased. Thephenomenon that multiple web sources pick up certain information is oftencalled trend. A central problem in the context of web data mining is to detectthose web sources that are first to publish information which will give rise toa trend. We present a simple and efficient method for finding trends dominatinga pool of web sources and identifying those web sources that publish theinformation relevant to a trend before others. We validate our approach on realdata collected from influential technology news feeds.
arxiv-1206-6387 | Fast classification using sparse decision DAGs |  http://arxiv.org/abs/1206.6387  | author:Djalel Benbouzid, Robert Busa-Fekete, Balazs Kegl category:cs.LG stat.ML published:2012-06-27 summary:In this paper we propose an algorithm that builds sparse decision DAGs(directed acyclic graphs) from a list of base classifiers provided by anexternal learning method such as AdaBoost. The basic idea is to cast the DAGdesign task as a Markov decision process. Each instance can decide to use or toskip each base classifier, based on the current state of the classifier beingbuilt. The result is a sparse decision DAG where the base classifiers areselected in a data-dependent way. The method has a single hyperparameter with aclear semantics of controlling the accuracy/speed trade-off. The algorithm iscompetitive with state-of-the-art cascade detectors on three object-detectionbenchmarks, and it clearly outperforms them when there is a small number ofbase classifiers. Unlike cascades, it is also readily applicable formulti-class classification. Using the multi-class setup, we show on a benchmarkweb page ranking data set that we can significantly improve the decision speedwithout harming the performance of the ranker.
arxiv-1206-6424 | Anytime Marginal MAP Inference |  http://arxiv.org/abs/1206.6424  | author:Denis Maua, Cassio De Campos category:cs.AI stat.ML published:2012-06-27 summary:This paper presents a new anytime algorithm for the marginal MAP problem ingraphical models. The algorithm is described in detail, its complexity andconvergence rate are studied, and relations to previous theoretical results forthe problem are discussed. It is shown that the algorithm runs inpolynomial-time if the underlying graph of the model has bounded tree-width,and that it provides guarantees to the lower and upper bounds obtained within afixed amount of computational resources. Experiments with both real andsynthetic generated models highlight its main characteristics and show that itcompares favorably against Park and Darwiche's systematic search, particularlyin the case of problems with many MAP variables and moderate tree-width.
arxiv-1206-6386 | How To Grade a Test Without Knowing the Answers --- A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing |  http://arxiv.org/abs/1206.6386  | author:Yoram Bachrach, Thore Graepel, Tom Minka, John Guiver category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We propose a new probabilistic graphical model that jointly models thedifficulties of questions, the abilities of participants and the correctanswers to questions in aptitude testing and crowdsourcing settings. We devisean active learning/adaptive testing scheme based on a greedy minimization ofexpected model entropy, which allows a more efficient resource allocation bydynamically choosing the next question to be asked based on the previousresponses. We present experimental results that confirm the ability of ourmodel to infer the required parameters and demonstrate that the adaptivetesting scheme requires fewer questions to obtain the same accuracy as a statictest scenario.
arxiv-1206-6385 | Improved Estimation in Time Varying Models |  http://arxiv.org/abs/1206.6385  | author:Doina Precup, Philip Bachman category:cs.LG stat.ME stat.ML published:2012-06-27 summary:Locally adapted parameterizations of a model (such as locally weightedregression) are expressive but often suffer from high variance. We describe anapproach for reducing the variance, based on the idea of estimatingsimultaneously a transformed space for the model, as well as locally adaptedparameterizations in this new space. We present a new problem formulation thatcaptures this idea and illustrate it in the important context of time varyingmodels. We develop an algorithm for learning a set of bases for approximating atime varying sparse network; each learned basis constitutes an archetypalsparse network structure. We also provide an extension for learning task-drivenbases. We present empirical results on synthetic data sets, as well as on a BCIEEG classification task.
arxiv-1206-6430 | Variational Bayesian Inference with Stochastic Search |  http://arxiv.org/abs/1206.6430  | author:John Paisley, David Blei, Michael Jordan category:cs.LG stat.CO stat.ML published:2012-06-27 summary:Mean-field variational inference is a method for approximate Bayesianposterior inference. It approximates a full posterior distribution with afactorized set of distributions by maximizing a lower bound on the marginallikelihood. This requires the ability to integrate a sum of terms in the logjoint likelihood using this factorized distribution. Often not all integralsare in closed form, which is typically handled by using a lower bound. Wepresent an alternative algorithm based on stochastic optimization that allowsfor direct optimization of the variational lower bound. This method usescontrol variates to reduce the variance of the stochastic search gradient, inwhich existing lower bounds can play an important role. We demonstrate theapproach on two non-conjugate models: logistic regression and an approximationto the HDP.
arxiv-1206-6425 | Sparse Stochastic Inference for Latent Dirichlet allocation |  http://arxiv.org/abs/1206.6425  | author:David Mimno, Matt Hoffman, David Blei category:cs.LG stat.ML published:2012-06-27 summary:We present a hybrid algorithm for Bayesian topic models that combines theefficiency of sparse Gibbs sampling with the scalability of online stochasticinference. We used our algorithm to analyze a corpus of 1.2 million books (33billion words) with thousands of topics. Our approach reduces the bias ofvariational inference and generalizes to many Bayesian hidden-variable models.
arxiv-1206-6431 | Exact Maximum Margin Structure Learning of Bayesian Networks |  http://arxiv.org/abs/1206.6431  | author:Robert Peharz, Franz Pernkopf category:cs.LG stat.ML published:2012-06-27 summary:Recently, there has been much interest in finding globally optimal Bayesiannetwork structures. These techniques were developed for generative scores andcan not be directly extended to discriminative scores, as desired forclassification. In this paper, we propose an exact method for finding networkstructures maximizing the probabilistic soft margin, a successfully applieddiscriminative score. Our method is based on branch-and-bound techniques withina linear programming framework and maintains an any-time solution, togetherwith worst-case sub-optimality bounds. We apply a set of order constraints forenforcing the network structure to be acyclic, which allows a compact problemrepresentation and the use of general-purpose optimization techniques. Inclassification experiments, our methods clearly outperform generatively trainednetwork structures and compete with support vector machines.
arxiv-1206-6432 | Sparse Support Vector Infinite Push |  http://arxiv.org/abs/1206.6432  | author:Alain Rakotomamonjy category:cs.LG cs.CE stat.ML published:2012-06-27 summary:In this paper, we address the problem of embedded feature selection forranking on top of the list problems. We pose this problem as a regularizedempirical risk minimization with $p$-norm push loss function ($p=\infty$) andsparsity inducing regularizers. We leverage the issues related to thischallenging optimization problem by considering an alternating direction methodof multipliers algorithm which is built upon proximal operators of the lossfunction and the regularizer. Our main technical contribution is thus toprovide a numerical scheme for computing the infinite push loss functionproximal operator. Experimental results on toy, DNA microarray and BCI problemsshow how our novel algorithm compares favorably to competitors for ranking ontop while using fewer variables in the scoring function.
arxiv-1206-6433 | Copula Mixture Model for Dependency-seeking Clustering |  http://arxiv.org/abs/1206.6433  | author:Melanie Rey, Volker Roth category:stat.ME cs.LG stat.ML published:2012-06-27 summary:We introduce a copula mixture model to perform dependency-seeking clusteringwhen co-occurring samples from different data sources are available. The modeltakes advantage of the great flexibility offered by the copulas framework toextend mixtures of Canonical Correlation Analysis to multivariate data witharbitrary continuous marginal densities. We formulate our model as anon-parametric Bayesian mixture, while providing efficient MCMC inference.Experiments on synthetic and real data demonstrate that the increasedflexibility of the copula mixture significantly improves the clustering and theinterpretability of the results.
arxiv-1206-6434 | A Generative Process for Sampling Contractive Auto-Encoders |  http://arxiv.org/abs/1206.6434  | author:Salah Rifai, Yoshua Bengio, Yann Dauphin, Pascal Vincent category:cs.LG stat.ML published:2012-06-27 summary:The contractive auto-encoder learns a representation of the input data thatcaptures the local manifold structure around each data point, through theleading singular vectors of the Jacobian of the transformation from input torepresentation. The corresponding singular values specify how much localvariation is plausible in directions associated with the corresponding singularvectors, while remaining in a high-density region of the input space. Thispaper proposes a procedure for generating samples that are consistent with thelocal structure captured by a contractive auto-encoder. The associatedstochastic process defines a distribution from which one can sample, and whichexperimentally appears to converge quickly and mix well between modes, comparedto Restricted Boltzmann Machines and Deep Belief Networks. The intuitionsbehind this procedure can also be used to train the second layer of contractionthat pools lower-level features and learns to be invariant to the localdirections of variation discovered in the first layer. We show that this canhelp learn and represent invariances present in the data and improveclassification error.
arxiv-1206-6384 | Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization |  http://arxiv.org/abs/1206.6384  | author:Haim Avron, Satyen Kale, Shiva Kasiviswanathan, Vikas Sindhwani category:cs.LG stat.ML published:2012-06-27 summary:We describe novel subgradient methods for a broad class of matrixoptimization problems involving nuclear norm regularization. Unlike existingapproaches, our method executes very cheap iterations by combining low-rankstochastic subgradients with efficient incremental SVD updates, made possibleby highly optimized and parallelizable dense linear algebra operations on smallmatrices. Our practical algorithms always maintain a low-rank factorization ofiterates that can be conveniently held in memory and efficiently multiplied togenerate predictions in matrix completion settings. Empirical comparisonsconfirm that our approach is highly competitive with several recently proposedstate-of-the-art solvers for such problems.
arxiv-1206-6383 | Feature Selection via Probabilistic Outputs |  http://arxiv.org/abs/1206.6383  | author:Andrea Danyluk, Nicholas Arnosti category:cs.LG stat.ML published:2012-06-27 summary:This paper investigates two feature-scoring criteria that make use ofestimated class probabilities: one method proposed by \citet{shen} and acomplementary approach proposed below. We develop a theoretical framework toanalyze each criterion and show that both estimate the spread (across allvalues of a given feature) of the probability that an example belongs to thepositive class. Based on our analysis, we predict when each scoring techniquewill be advantageous over the other and give empirical results validating ourpredictions.
arxiv-1206-6435 | Rethinking Collapsed Variational Bayes Inference for LDA |  http://arxiv.org/abs/1206.6435  | author:Issei Sato, Hiroshi Nakagawa category:cs.LG stat.ML published:2012-06-27 summary:We propose a novel interpretation of the collapsed variational Bayesinference with a zero-order Taylor expansion approximation, called CVB0inference, for latent Dirichlet allocation (LDA). We clarify the properties ofthe CVB0 inference by using the alpha-divergence. We show that the CVB0inference is composed of two different divergence projections: alpha=1 and -1.This interpretation will help shed light on CVB0 works.
arxiv-1206-6446 | Agglomerative Bregman Clustering |  http://arxiv.org/abs/1206.6446  | author:Matus Telgarsky, Sanjoy Dasgupta category:cs.LG stat.ML published:2012-06-27 summary:This manuscript develops the theory of agglomerative clustering with Bregmandivergences. Geometric smoothing techniques are developed to deal withdegenerate clusters. To allow for cluster models based on exponential familieswith overcomplete representations, Bregman divergences are developed fornondifferentiable convex functions.
arxiv-1206-6436 | Efficient Structured Prediction with Latent Variables for General Graphical Models |  http://arxiv.org/abs/1206.6436  | author:Alexander Schwing, Tamir Hazan, Marc Pollefeys, Raquel Urtasun category:cs.LG stat.ML published:2012-06-27 summary:In this paper we propose a unified framework for structured prediction withlatent variables which includes hidden conditional random fields and latentstructured support vector machines as special cases. We describe a localentropy approximation for this general formulation using duality, and derive anefficient message passing algorithm that is guaranteed to converge. Wedemonstrate its effectiveness in the tasks of image segmentation as well as 3Dindoor scene understanding from single images, showing that our approach issuperior to latent structured support vector machines and hidden conditionalrandom fields.
arxiv-1206-6437 | Large Scale Variational Bayesian Inference for Structured Scale Mixture Models |  http://arxiv.org/abs/1206.6437  | author:Young Jun Ko, Matthias Seeger category:cs.CV cs.LG stat.ML published:2012-06-27 summary:Natural image statistics exhibit hierarchical dependencies across multiplescales. Representing such prior knowledge in non-factorial latent tree modelscan boost performance of image denoising, inpainting, deconvolution orreconstruction substantially, beyond standard factorial "sparse" methodology.We derive a large scale approximate Bayesian inference algorithm for linearmodels with non-factorial (latent tree-structured) scale mixture priors.Experimental results on a range of denoising and inpainting problemsdemonstrate substantially improved performance compared to MAP estimation or toinference with factorial priors.
arxiv-1206-6438 | Information-Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation |  http://arxiv.org/abs/1206.6438  | author:Yuan Shi, Fei Sha category:cs.LG stat.ML published:2012-06-27 summary:We study the problem of unsupervised domain adaptation, which aims to adaptclassifiers trained on a labeled source domain to an unlabeled target domain.Many existing approaches first learn domain-invariant features and thenconstruct classifiers with them. We propose a novel approach that jointly learnthe both. Specifically, while the method identifies a feature space where datain the source and the target domains are similarly distributed, it also learnsthe feature space discriminatively, optimizing an information-theoretic metricas an proxy to the expected misclassification error on the target domain. Weshow how this optimization can be effectively carried out with simplegradient-based methods and how hyperparameters can be cross-validated withoutdemanding any labeled data from the target domain. Empirical studies onbenchmark tasks of object recognition and sentiment analysis validated ourmodeling assumptions and demonstrated significant improvement of our methodover competing ones in classification accuracies.
arxiv-1206-6447 | Small-sample Brain Mapping: Sparse Recovery on Spatially Correlated Designs with Randomization and Clustering |  http://arxiv.org/abs/1206.6447  | author:Gael Varoquaux, Alexandre Gramfort, Bertrand Thirion category:cs.LG cs.CV stat.AP stat.ML published:2012-06-27 summary:Functional neuroimaging can measure the brain?s response to an externalstimulus. It is used to perform brain mapping: identifying from theseobservations the brain regions involved. This problem can be cast into a linearsupervised learning task where the neuroimaging data are used as predictors forthe stimulus. Brain mapping is then seen as a support recovery problem. Onfunctional MRI (fMRI) data, this problem is particularly challenging as i) thenumber of samples is small due to limited acquisition time and ii) thevariables are strongly correlated. We propose to overcome these difficultiesusing sparse regression models over new variables obtained by clustering of theoriginal variables. The use of randomization techniques, e.g. bootstrapsamples, and clustering of the variables improves the recovery properties ofsparse methods. We demonstrate the benefit of our approach on an extensivesimulation study as well as two fMRI datasets.
arxiv-1206-6439 | Gap Filling in the Plant Kingdom---Trait Prediction Using Hierarchical Probabilistic Matrix Factorization |  http://arxiv.org/abs/1206.6439  | author:Hanhuai Shan, Jens Kattge, Peter Reich, Arindam Banerjee, Franziska Schrodt, Markus Reichstein category:cs.CE cs.LG stat.AP published:2012-06-27 summary:Plant traits are a key to understanding and predicting the adaptation ofecosystems to environmental changes, which motivates the TRY project aiming atconstructing a global database for plant traits and becoming a standardresource for the ecological community. Despite its unprecedented coverage, alarge percentage of missing data substantially constrains joint trait analysis.Meanwhile, the trait data is characterized by the hierarchical phylogeneticstructure of the plant kingdom. While factorization based matrix completiontechniques have been widely used to address the missing data problem,traditional matrix factorization methods are unable to leverage thephylogenetic structure. We propose hierarchical probabilistic matrixfactorization (HPMF), which effectively uses hierarchical phylogeneticinformation for trait prediction. We demonstrate HPMF's high accuracy,effectiveness of incorporating hierarchical structure and ability to capturetrait correlation through experiments.
arxiv-1206-6448 | Online Alternating Direction Method |  http://arxiv.org/abs/1206.6448  | author:Huahua Wang, Arindam Banerjee category:cs.LG stat.ML published:2012-06-27 summary:Online optimization has emerged as powerful tool in large scale optimization.In this paper, we introduce efficient online algorithms based on thealternating directions method (ADM). We introduce a new proof technique for ADMin the batch setting, which yields the O(1/T) convergence rate of ADM and formsthe basis of regret analysis in the online setting. We consider two scenariosin the online setting, based on whether the solution needs to lie in thefeasible set or not. In both settings, we establish regret bounds for both theobjective function as well as constraint violation for general and stronglyconvex functions. Preliminary results are presented to illustrate theperformance of the proposed algorithms.
arxiv-1206-6426 | A Fast and Simple Algorithm for Training Neural Probabilistic Language Models |  http://arxiv.org/abs/1206.6426  | author:Andriy Mnih, Yee Whye Teh category:cs.CL cs.LG published:2012-06-27 summary:In spite of their superior performance, neural probabilistic language models(NPLMs) remain far less widely used than n-gram models due to their notoriouslylong training times, which are measured in weeks even for moderately-sizeddatasets. Training NPLMs is computationally expensive because they areexplicitly normalized, which leads to having to consider all words in thevocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based onnoise-contrastive estimation, a newly introduced procedure for estimatingunnormalized continuous distributions. We investigate the behaviour of thealgorithm on the Penn Treebank corpus and show that it reduces the trainingtimes by more than an order of magnitude without affecting the quality of theresulting models. The algorithm is also more efficient and much more stablethan importance sampling because it requires far fewer noise samples to performwell. We demonstrate the scalability of the proposed approach by training severalneural language models on a 47M-word corpus with a 80K-word vocabulary,obtaining state-of-the-art results on the Microsoft Research SentenceCompletion Challenge dataset.
arxiv-1206-6389 | Poisoning Attacks against Support Vector Machines |  http://arxiv.org/abs/1206.6389  | author:Battista Biggio, Blaine Nelson, Pavel Laskov category:cs.LG cs.CR stat.ML published:2012-06-27 summary:We investigate a family of poisoning attacks against Support Vector Machines(SVM). Such attacks inject specially crafted training data that increases theSVM's test error. Central to the motivation for these attacks is the fact thatmost learning algorithms assume that their training data comes from a naturalor well-behaved distribution. However, this assumption does not generally holdin security-sensitive settings. As we demonstrate, an intelligent adversarycan, to some extent, predict the change of the SVM's decision function due tomalicious input and use this ability to construct malicious data. The proposedattack uses a gradient ascent strategy in which the gradient is computed basedon properties of the SVM's optimal solution. This method can be kernelized andenables the attack to be constructed in the input space even for non-linearkernels. We experimentally demonstrate that our gradient ascent procedurereliably identifies good local maxima of the non-convex validation errorsurface, which significantly increases the classifier's test error.
arxiv-1206-6519 | A Permutation Approach to Testing Interactions in Many Dimensions |  http://arxiv.org/abs/1206.6519  | author:Noah Simon, Robert Tibshirani category:stat.ML stat.CO stat.ME published:2012-06-27 summary:To date, testing interactions in high dimensions has been a challenging task.Existing methods often have issues with sensitivity to modeling assumptions andheavily asymptotic nominal p-values. To help alleviate these issues, we proposea permutation-based method for testing marginal interactions with a binaryresponse. Our method searches for pairwise correlations which differ betweenclasses. In this manuscript, we compare our method on real and simulated datato the standard approach of running many pairwise logistic models. On simulateddata our method finds more significant interactions at a lower false discoveryrate (especially in the presence of main effects). On real genomic data,although there is no gold standard, our method finds apparent signal and tellsa believable story, while logistic regression does not. We also give asymptoticconsistency results under not too restrictive assumptions.
arxiv-1206-6382 | High-Dimensional Covariance Decomposition into Sparse Markov and Independence Domains |  http://arxiv.org/abs/1206.6382  | author:Majid Janzamin, Animashree Anandkumar category:cs.LG stat.ML published:2012-06-27 summary:In this paper, we present a novel framework incorporating a combination ofsparse models in different domains. We posit the observed data as generatedfrom a linear combination of a sparse Gaussian Markov model (with a sparseprecision matrix) and a sparse Gaussian independence model (with a sparsecovariance matrix). We provide efficient methods for decomposition of the datainto two domains, \viz Markov and independence domains. We characterize a setof sufficient conditions for identifiability and model consistency. Ourdecomposition method is based on a simple modification of the popular$\ell_1$-penalized maximum-likelihood estimator ($\ell_1$-MLE). We establishthat our estimator is consistent in both the domains, i.e., it successfullyrecovers the supports of both Markov and independence models, when the numberof samples $n$ scales as $n = \Omega(d^2 \log p)$, where $p$ is the number ofvariables and $d$ is the maximum node degree in the Markov model. Ourconditions for recovery are comparable to those of $\ell_1$-MLE for consistentestimation of a sparse Markov model, and thus, we guarantee successfulhigh-dimensional estimation of a richer class of models under comparableconditions. Our experiments validate these results and also demonstrate thatour models have better inference accuracy under simple algorithms such as loopybelief propagation.
arxiv-1206-6440 | Predicting Preference Flips in Commerce Search |  http://arxiv.org/abs/1206.6440  | author:Or Sheffet, Nina Mishra, Samuel Ieong category:cs.LG stat.ML published:2012-06-27 summary:Traditional approaches to ranking in web search follow the paradigm ofrank-by-score: a learned function gives each query-URL combination an absolutescore and URLs are ranked according to this score. This paradigm ensures thatif the score of one URL is better than another then one will always be rankedhigher than the other. Scoring contradicts prior work in behavioral economicsthat showed that users' preferences between two items depend not only on theitems but also on the presented alternatives. Thus, for the same query, users'preference between items A and B depends on the presence/absence of item C. Wepropose a new model of ranking, the Random Shopper Model, that allows andexplains such behavior. In this model, each feature is viewed as a Markov chainover the items to be ranked, and the goal is to find a weighting of thefeatures that best reflects their importance. We show that our model can belearned under the empirical risk minimization framework, and give an efficientlearning algorithm. Experiments on commerce search logs demonstrate that ouralgorithm outperforms scoring-based approaches including regression andlistwise ranking.
arxiv-1206-6449 | Monte Carlo Bayesian Reinforcement Learning |  http://arxiv.org/abs/1206.6449  | author:Yi Wang, Kok Sung Won, David Hsu, Wee Sun Lee category:cs.LG stat.ML published:2012-06-27 summary:Bayesian reinforcement learning (BRL) encodes prior knowledge of the world ina model and represents uncertainty in model parameters by maintaining aprobability distribution over them. This paper presents Monte Carlo BRL(MC-BRL), a simple and general approach to BRL. MC-BRL samples a priori afinite set of hypotheses for the model parameter values and forms a discretepartially observable Markov decision process (POMDP) whose state space is across product of the state space for the reinforcement learning task and thesampled model parameter space. The POMDP does not require conjugatedistributions for belief representation, as earlier works do, and can be solvedrelatively easily with point-based approximation algorithms. MC-BRL naturallyhandles both fully and partially observable worlds. Theoretical andexperimental results show that the discrete POMDP approximates the underlyingBRL task well with guaranteed performance.
arxiv-1206-6441 | A Topic Model for Melodic Sequences |  http://arxiv.org/abs/1206.6441  | author:Athina Spiliopoulou, Amos Storkey category:cs.LG cs.IR stat.ML published:2012-06-27 summary:We examine the problem of learning a probabilistic model for melody directlyfrom musical sequences belonging to the same genre. This is a challenging taskas one needs to capture not only the rich temporal structure evident in music,but also the complex statistical dependencies among different music components.To address this problem we introduce the Variable-gram Topic Model, whichcouples the latent topic formalism with a systematic model for contextualinformation. We evaluate the model on next-step prediction. Additionally, wepresent a novel way of model evaluation, where we directly compare modelsamples with data sequences using the Maximum Mean Discrepancy of stringkernels, to assess how close is the model distribution to the datadistribution. We show that the model has the highest performance under bothevaluation measures when compared to LDA, the Topic Bigram and relatednon-topic models.
arxiv-1206-6442 | Minimizing The Misclassification Error Rate Using a Surrogate Convex Loss |  http://arxiv.org/abs/1206.6442  | author:Shai Ben-David, David Loker, Nathan Srebro, Karthik Sridharan category:cs.LG stat.ML published:2012-06-27 summary:We carefully study how well minimizing convex surrogate loss functions,corresponds to minimizing the misclassification error rate for the problem ofbinary classification with linear predictors. In particular, we show thatamongst all convex surrogate losses, the hinge loss gives essentially the bestpossible bound, of all convex loss functions, for the misclassification errorrate of the resulting linear predictor in terms of the best possible marginerror rate. We also provide lower bounds for specific convex surrogates thatshow how different commonly used losses qualitatively differ from each other.
arxiv-1206-6444 | Statistical Linear Estimation with Penalized Estimators: an Application to Reinforcement Learning |  http://arxiv.org/abs/1206.6444  | author:Bernardo Avila Pires, Csaba Szepesvari category:cs.LG stat.ML published:2012-06-27 summary:Motivated by value function estimation in reinforcement learning, we studystatistical linear inverse problems, i.e., problems where the coefficients of alinear system to be solved are observed in noise. We consider penalizedestimators, where performance is evaluated using a matrix-weighted two-norm ofthe defect of the estimator measured with respect to the true, unknowncoefficients. Two objective functions are considered depending whether theerror of the defect measured with respect to the noisy coefficients is squaredor unsquared. We propose simple, yet novel and theoretically well-foundeddata-dependent choices for the regularization parameters for both cases thatavoid data-splitting. A distinguishing feature of our analysis is that wederive deterministic error bounds in terms of the error of the coefficients,thus allowing the complete separation of the analysis of the stochasticproperties of these errors. We show that our results lead to new insights andbounds for linear value function estimation in reinforcement learning.
arxiv-1206-6427 | Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients |  http://arxiv.org/abs/1206.6427  | author:Iftekhar Naim, Daniel Gildea category:cs.LG stat.ML published:2012-06-27 summary:The speed of convergence of the Expectation Maximization (EM) algorithm forGaussian mixture model fitting is known to be dependent on the amount ofoverlap among the mixture components. In this paper, we study the impact ofmixing coefficients on the convergence of EM. We show that when the mixturecomponents exhibit some overlap, the convergence of EM becomes slower as thedynamic range among the mixing coefficients increases. We propose adeterministic anti-annealing algorithm, that significantly improves the speedof convergence of EM for such mixtures with unbalanced mixing coefficients. Theproposed algorithm is compared against other standard optimization techniqueslike BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, wepropose a similar deterministic anti-annealing based algorithm for theDirichlet process mixture model and demonstrate its advantages over theconventional variational Bayesian approach.
arxiv-1206-6445 | Deep Lambertian Networks |  http://arxiv.org/abs/1206.6445  | author:Yichuan Tang, Ruslan Salakhutdinov, Geoffrey Hinton category:cs.CV cs.LG stat.ML published:2012-06-27 summary:Visual perception is a challenging problem in part due to illuminationvariations. A possible solution is to first estimate an illumination invariantrepresentation before using it for recognition. The object albedo and surfacenormals are examples of such representations. In this paper, we introduce amultilayer generative model where the latent variables include the albedo,surface normals, and the light source. Combining Deep Belief Nets with theLambertian reflectance assumption, our model can learn good priors over thealbedo from 2D images. Illumination variations can be explained by changingonly the lighting latent variable in our model. By transferring learnedknowledge from similar objects, albedo and surface normals estimation from asingle image is possible in our model. Experiments demonstrate that our modelis able to generalize as well as improve over standard baselines in one-shotface recognition.
arxiv-1206-6380 | Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring |  http://arxiv.org/abs/1206.6380  | author:Sungjin Ahn, Anoop Korattikara, Max Welling category:cs.LG stat.CO stat.ML published:2012-06-27 summary:In this paper we address the following question: Can we approximately samplefrom a Bayesian posterior distribution if we are only allowed to touch a smallmini-batch of data-items for every sample we generate?. An algorithm based onthe Langevin equation with stochastic gradients (SGLD) was previously proposedto solve this, but its mixing rate was slow. By leveraging the Bayesian CentralLimit Theorem, we extend the SGLD algorithm so that at high mixing rates itwill sample from a normal approximation of the posterior, while for slow mixingrates it will mimic the behavior of SGLD with a pre-conditioner matrix. As abonus, the proposed algorithm is reminiscent of Fisher scoring (with stochasticgradients) and as such an efficient optimizer during burn-in.
arxiv-1206-6466 | Utilizing Static Analysis and Code Generation to Accelerate Neural Networks |  http://arxiv.org/abs/1206.6466  | author:Lawrence McAfee, Kunle Olukotun category:cs.NE cs.MS cs.PL published:2012-06-27 summary:As datasets continue to grow, neural network (NN) applications are becomingincreasingly limited by both the amount of available computational power andthe ease of developing high-performance applications. Researchers often musthave expert systems knowledge to make their algorithms run efficiently.Although available computing power increases rapidly each year, algorithmefficiency is not able to keep pace due to the use of general purposecompilers, which are not able to fully optimize specialized applicationdomains. Within the domain of NNs, we have the added knowledge that networkarchitecture remains constant during training, meaning the architecture's datastructure can be statically optimized by a compiler. In this paper, we presentSONNC, a compiler for NNs that utilizes static analysis to generate optimizedparallel code. We show that SONNC's use of static optimizations make it able tooutperform hand-optimized C++ code by up to 7.8X, and MATLAB code by up to 24X.Additionally, we show that use of SONNC significantly reduces code complexitywhen using structurally sparse networks.
arxiv-1206-6465 | Bayesian Efficient Multiple Kernel Learning |  http://arxiv.org/abs/1206.6465  | author:Mehmet Gonen category:cs.LG stat.ML published:2012-06-27 summary:Multiple kernel learning algorithms are proposed to combine kernels in orderto obtain a better similarity measure or to integrate feature representationscoming from different data sources. Most of the previous research on suchmethods is focused on the computational efficiency issue. However, it is stillnot feasible to combine many kernels using existing Bayesian approaches due totheir high time complexity. We propose a fully conjugate Bayesian formulationand derive a deterministic variational approximation, which allows us tocombine hundreds or thousands of kernels very efficiently. We briefly explainhow the proposed method can be extended for multiclass learning andsemi-supervised learning. Experiments with large numbers of kernels onbenchmark data sets show that our inference method is quite fast, requiringless than a minute. On one bioinformatics and three image recognition datasets, our method outperforms previously reported results with bettergeneralization performance.
arxiv-1206-6463 | An Iterative Locally Linear Embedding Algorithm |  http://arxiv.org/abs/1206.6463  | author:Deguang Kong, Chris H. Q. Ding, Heng Huang, Feiping Nie category:cs.LG stat.ML published:2012-06-27 summary:Local Linear embedding (LLE) is a popular dimension reduction method. In thispaper, we first show LLE with nonnegative constraint is equivalent to thewidely used Laplacian embedding. We further propose to iterate the two steps inLLE repeatedly to improve the results. Thirdly, we relax the kNN constraint ofLLE and present a sparse similarity learning algorithm. The final Iterative LLEcombines these three improvements. Extensive experiment results show thatiterative LLE algorithm significantly improve both classification andclustering results.
arxiv-1206-6462 | Learning Object Arrangements in 3D Scenes using Human Context |  http://arxiv.org/abs/1206.6462  | author:Yun Jiang, Marcus Lim, Ashutosh Saxena category:cs.LG cs.CV cs.RO stat.ML published:2012-06-27 summary:We consider the problem of learning object arrangements in a 3D scene. Thekey idea here is to learn how objects relate to human poses based on theiraffordances, ease of use and reachability. In contrast to modelingobject-object relationships, modeling human-object relationships scaleslinearly in the number of objects. We design appropriate density functionsbased on 3D spatial features to capture this. We learn the distribution ofhuman poses in a scene using a variant of the Dirichlet process mixture modelthat allows sharing of the density function parameters across the same objecttypes. Then we can reason about arrangements of the objects in the room basedon these meaningful human poses. In our extensive experiments on 20 differentrooms with a total of 47 objects, our algorithm predicted correct placementswith an average error of 1.6 meters from ground truth. In arranging five realscenes, it received a score of 4.3/5 compared to 3.7 for the best baselinemethod.
arxiv-1206-6461 | On the Sample Complexity of Reinforcement Learning with a Generative Model |  http://arxiv.org/abs/1206.6461  | author:Mohammad Gheshlaghi Azar, Remi Munos, Bert Kappen category:cs.LG stat.ML published:2012-06-27 summary:We consider the problem of learning the optimal action-value function in thediscounted-reward Markov decision processes (MDPs). We prove a new PAC bound onthe sample-complexity of model-based value iteration algorithm in the presenceof the generative model, which indicates that for an MDP with N state-actionpairs and the discount factor \gamma\in[0,1) onlyO(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) samples are required to find an\epsilon-optimal estimation of the action-value function with the probability1-\delta. We also prove a matching lower bound of \Theta(N\log(N/\delta)/((1-\gamma)^3\epsilon^2)) on the sample complexity ofestimating the optimal action-value function by every RL algorithm. To the bestof our knowledge, this is the first matching result on the sample complexity ofestimating the optimal (action-) value function in which the upper boundmatches the lower bound of RL in terms of N, \epsilon, \delta and 1/(1-\gamma).Also, both our lower bound and our upper bound significantly improve on thestate-of-the-art in terms of 1/(1-\gamma).
arxiv-1206-6460 | Output Space Search for Structured Prediction |  http://arxiv.org/abs/1206.6460  | author:Janardhan Rao Doppa, Alan Fern, Prasad Tadepalli category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We consider a framework for structured prediction based on search in thespace of complete structured outputs. Given a structured input, an output isproduced by running a time-bounded search procedure guided by a learned costfunction, and then returning the least cost output uncovered during the search.This framework can be instantiated for a wide range of search spaces and searchprocedures, and easily incorporates arbitrary structured-prediction lossfunctions. In this paper, we make two main technical contributions. First, wedefine the limited-discrepancy search space over structured outputs, which isable to leverage powerful classification learning algorithms to improve thesearch space quality. Second, we give a generic cost function learningapproach, where the key idea is to learn a cost function that attempts to mimicthe behavior of conducting searches guided by the true loss function. Ourexperiments on six benchmark domains demonstrate that using our framework withonly a small amount of search is sufficient for significantly improving onstate-of-the-art structured-prediction performance.
arxiv-1206-6459 | Bayesian Conditional Cointegration |  http://arxiv.org/abs/1206.6459  | author:Chris Bracegirdle, David Barber category:cs.CE cs.LG stat.ME published:2012-06-27 summary:Cointegration is an important topic for time-series, and describes arelationship between two series in which a linear combination is stationary.Classically, the test for cointegration is based on a two stage process inwhich first the linear relation between the series is estimated by OrdinaryLeast Squares. Subsequently a unit root test is performed on the residuals. Awell-known deficiency of this classical approach is that it can lead toerroneous conclusions about the presence of cointegration. As an alternative,we present a framework for estimating whether cointegration exists usingBayesian inference which is empirically superior to the classical approach.Finally, we apply our technique to model segmented cointegration in whichcointegration may exist only for limited time. In contrast to previousapproaches our model makes no restriction on the number of possiblecointegration segments.
arxiv-1206-6458 | Batch Active Learning via Coordinated Matching |  http://arxiv.org/abs/1206.6458  | author:Javad Azimi, Alan Fern, Xiaoli Zhang-Fern, Glencora Borradaile, Brent Heeringa category:cs.LG stat.ML published:2012-06-27 summary:Most prior work on active learning of classifiers has focused on sequentiallyselecting one unlabeled example at a time to be labeled in order to reduce theoverall labeling effort. In many scenarios, however, it is desirable to labelan entire batch of examples at once, for example, when labels can be acquiredin parallel. This motivates us to study batch active learning, whichiteratively selects batches of $k>1$ examples to be labeled. We propose a novelbatch active learning method that leverages the availability of high-qualityand efficient sequential active-learning policies by attempting to approximatetheir behavior when applied for $k$ steps. Specifically, our algorithm firstuses Monte-Carlo simulation to estimate the distribution of unlabeled examplesselected by a sequential policy over $k$ step executions. The algorithm thenattempts to select a set of $k$ examples that best matches this distribution,leading to a combinatorial optimization problem that we term "boundedcoordinated matching". While we show this problem is NP-hard in general, wegive an efficient greedy solution, which inherits approximation bounds fromsupermodular minimization theory. Our experimental results on eight benchmarkdatasets show that the proposed approach is highly effective
arxiv-1206-6457 | Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations |  http://arxiv.org/abs/1206.6457  | author:Nando de Freitas, Alex Smola, Masrour Zoghi category:cs.LG stat.ML published:2012-06-27 summary:This paper analyzes the problem of Gaussian process (GP) bandits withdeterministic observations. The analysis uses a branch and bound algorithm thatis related to the UCB algorithm of (Srinivas et al, 2010). For GPs withGaussian observation noise, with variance strictly greater than zero, Srinivaset al proved that the regret vanishes at the approximate rate of$O(1/\sqrt{t})$, where t is the number of observations. To complement theirresult, we attack the deterministic case and attain a much faster exponentialconvergence rate. Under some regularity assumptions, we show that the regretdecreases asymptotically according to $O(e^{-\frac{\tau t}{(\ln t)^{d/4}}})$with high probability. Here, d is the dimension of the search space and tau isa constant that depends on the behaviour of the objective function near itsglobal maximum.
arxiv-1206-6456 | Lognormal and Gamma Mixed Negative Binomial Regression |  http://arxiv.org/abs/1206.6456  | author:Mingyuan Zhou, Lingbo Li, David Dunson, Lawrence Carin category:stat.AP cs.LG stat.ME published:2012-06-27 summary:In regression analysis of counts, a lack of simple and efficient algorithmsfor posterior computation has made Bayesian approaches appear unattractive andthus underdeveloped. We propose a lognormal and gamma mixed negative binomial(NB) regression model for counts, and present efficient closed-form Bayesianinference; unlike conventional Poisson models, the proposed approach has twofree parameters to include two different kinds of random effects, and allowsthe incorporation of prior information, such as sparsity in the regressioncoefficients. By placing a gamma distribution prior on the NB dispersionparameter r, and connecting a lognormal distribution prior with the logit ofthe NB probability parameter p, efficient Gibbs sampling and variational Bayesinference are both developed. The closed-form updates are obtained byexploiting conditional conjugacy via both a compound Poisson representation anda Polya-Gamma distribution based data augmentation approach. The proposedBayesian inference can be implemented routinely, while being easilygeneralizable to more complex settings involving multivariate dependencestructures. The algorithms are illustrated using real examples.
arxiv-1206-6455 | Regularizers versus Losses for Nonlinear Dimensionality Reduction: A Factored View with New Convex Relaxations |  http://arxiv.org/abs/1206.6455  | author:Yaoliang Yu, James Neufeld, Ryan Kiros, Xinhua Zhang, Dale Schuurmans category:cs.LG stat.ML published:2012-06-27 summary:We demonstrate that almost all non-parametric dimensionality reductionmethods can be expressed by a simple procedure: regularized loss minimizationplus singular value truncation. By distinguishing the role of the loss andregularizer in such a process, we recover a factored perspective that revealssome gaps in the current literature. Beyond identifying a useful new loss formanifold unfolding, a key contribution is to derive new convex regularizersthat combine distance maximization with rank reduction. These regularizers canbe applied to any loss.
arxiv-1206-6454 | Hierarchical Exploration for Accelerating Contextual Bandits |  http://arxiv.org/abs/1206.6454  | author:Yisong Yue, Sue Ann Hong, Carlos Guestrin category:cs.LG stat.ML published:2012-06-27 summary:Contextual bandit learning is an increasingly popular approach to optimizingrecommender systems via user feedback, but can be slow to converge in practicedue to the need for exploring a large feature space. In this paper, we proposea coarse-to-fine hierarchical approach for encoding prior knowledge thatdrastically reduces the amount of exploration required. Intuitively, userpreferences can be reasonably embedded in a coarse low-dimensional featurespace that can be explored efficiently, requiring exploration in thehigh-dimensional space only as necessary. We introduce a bandit algorithm thatexplores within this coarse-to-fine spectrum, and prove performance guaranteesthat depend on how well the coarse space captures the user's preferences. Wedemonstrate substantial improvement over conventional bandit algorithms throughextensive simulation as well as a live user study in the setting ofpersonalized news recommendation.
arxiv-1206-6453 | Adaptive Canonical Correlation Analysis Based On Matrix Manifolds |  http://arxiv.org/abs/1206.6453  | author:Florian Yger, Maxime Berar, Gilles Gasso, Alain Rakotomamonjy category:cs.LG stat.ML published:2012-06-27 summary:In this paper, we formulate the Canonical Correlation Analysis (CCA) problemon matrix manifolds. This framework provides a natural way for dealing withmatrix constraints and tools for building efficient algorithms even in anadaptive setting. Finally, an adaptive CCA algorithm is proposed and applied toa change detection problem in EEG signals.
arxiv-1206-6452 | Smoothness and Structure Learning by Proxy |  http://arxiv.org/abs/1206.6452  | author:Benjamin Yackley, Terran Lane category:cs.LG math.OC stat.ML published:2012-06-27 summary:As data sets grow in size, the ability of learning methods to find structurein them is increasingly hampered by the time needed to search the large spacesof possibilities and generate a score for each that takes all of the observeddata into account. For instance, Bayesian networks, the model chosen in thispaper, have a super-exponentially large search space for a fixed number ofvariables. One possible method to alleviate this problem is to use a proxy,such as a Gaussian Process regressor, in place of the true scoring function,training it on a selection of sampled networks. We prove here that the use ofsuch a proxy is well-founded, as we can bound the smoothness of a commonly-usedscoring function for Bayesian network structure learning. We show here that,compared to an identical search strategy using the network?s exact scores, ourproxy-based search is able to get equivalent or better scores on a number ofdata sets in a fraction of the time.
arxiv-1206-6451 | The Greedy Miser: Learning under Test-time Budgets |  http://arxiv.org/abs/1206.6451  | author:Zhixiang Xu, Kilian Weinberger, Olivier Chapelle category:cs.LG stat.ML published:2012-06-27 summary:As machine learning algorithms enter applications in industrial settings,there is increased interest in controlling their cpu-time during testing. Thecpu-time consists of the running time of the algorithm and the extraction timeof the features. The latter can vary drastically when the feature set isdiverse. In this paper, we propose an algorithm, the Greedy Miser, thatincorporates the feature extraction cost during training to explicitly minimizethe cpu-time during testing. The algorithm is a straightforward extension ofstage-wise regression and is equally suitable for regression or multi-classclassification. Compared to prior work, it is significantly more cost-effectiveand scales to larger data sets.
arxiv-1206-6450 | Conditional Sparse Coding and Grouped Multivariate Regression |  http://arxiv.org/abs/1206.6450  | author:Min Xu, John Lafferty category:cs.LG stat.ML published:2012-06-27 summary:We study the problem of multivariate regression where the data are naturallygrouped, and a regression matrix is to be estimated for each group. We proposean approach in which a dictionary of low rank parameter matrices is estimatedacross groups, and a sparse linear combination of the dictionary elements isestimated to form a model within each group. We refer to the method asconditional sparse coding since it is a coding procedure for the responsevectors Y conditioned on the covariate vectors X. This approach captures theshared information across the groups while adapting to the structure withineach group. It exploits the same intuition behind sparse coding that has beensuccessfully developed in computer vision and computational neuroscience. Wepropose an algorithm for conditional sparse coding, analyze its theoreticalproperties in terms of predictive accuracy, and present the results ofsimulation and brain imaging experiments that compare the new technique toreduced rank regression.
arxiv-1206-6514 | Investigation of Color Constancy for Ubiquitous Wireless LAN/Camera Positioning: An Initial Outcome |  http://arxiv.org/abs/1206.6514  | author:Wan Mohd Yaakob Wan Bejuri, Mohd Murtadha Mohamad, Maimunah Sapri, Mohd Adly Rosly category:cs.CV cs.HC published:2012-06-27 summary:This paper present our color constancy investigation in the hybridization ofWireless LAN and Camera positioning in the mobile phone. Five typical colorconstancy schemes are analyzed in different location environment. The resultscan be used to combine with RF signals from Wireless LAN positioning by usingmodel fitting approach in order to establish absolute positioning output. Thereis no conventional searching algorithm required, thus it is expected to reducethe complexity of computation. Finally we present our preliminary results toillustrate the indoor positioning algorithm performance evaluation for anindoor environment set-up.
arxiv-1206-6398 | Learning Parameterized Skills |  http://arxiv.org/abs/1206.6398  | author:Bruno Da Silva, George Konidaris, Andrew Barto category:cs.LG stat.ML published:2012-06-27 summary:We introduce a method for constructing skills capable of solving tasks drawnfrom a distribution of parameterized reinforcement learning problems. Themethod draws example tasks from a distribution of interest and uses thecorresponding learned policies to estimate the topology of thelower-dimensional piecewise-smooth manifold on which the skill policies lie.This manifold models how policy parameters change as task parameters vary. Themethod identifies the number of charts that compose the manifold and thenapplies non-linear regression in each chart to construct a parameterized skillby predicting policy parameters from task parameters. We evaluate our method onan underactuated simulated robotic arm tasked with learning to accurately throwdarts at a parameterized target location.
arxiv-1206-6475 | A Split-Merge Framework for Comparing Clusterings |  http://arxiv.org/abs/1206.6475  | author:Qiaoliang Xiang, Qi Mao, Kian Ming Chai, Hai Leong Chieu, Ivor Tsang, Zhendong Zhao category:cs.LG stat.ML published:2012-06-27 summary:Clustering evaluation measures are frequently used to evaluate theperformance of algorithms. However, most measures are not properly normalizedand ignore some information in the inherent structure of clusterings. We modelthe relation between two clusterings as a bipartite graph and propose a generalcomponent-based decomposition formula based on the components of the graph.Most existing measures are examples of this formula. In order to satisfyconsistency in the component, we further propose a split-merge framework forcomparing clusterings of different data sets. Our framework gives measures thatare conditionally normalized, and it can make use of data point information,such as feature vectors and pairwise distances. We use an entropy-basedinstance of the framework and a coreference resolution data set to demonstrateempirically the utility of our framework over other measures.
arxiv-1206-6443 | Isoelastic Agents and Wealth Updates in Machine Learning Markets |  http://arxiv.org/abs/1206.6443  | author:Amos Storkey, Jono Millin, Krzysztof Geras category:cs.LG cs.GT stat.ML published:2012-06-27 summary:Recently, prediction markets have shown considerable promise for developingflexible mechanisms for machine learning. In this paper, agents with isoelasticutilities are considered. It is shown that the costs associated withhomogeneous markets of agents with isoelastic utilities produce equilibriumprices corresponding to alpha-mixtures, with a particular form of mixingcomponent relating to each agent's wealth. We also demonstrate that wealthaccumulation for logarithmic and other isoelastic agents (through payoffs onprediction of training targets) can implement both Bayesian model updates andmixture weight updates by imposing different market payoff structures. Aniterative algorithm is given for market equilibrium computation. We demonstratethat inhomogeneous markets of agents with isoelastic utilities outperform stateof the art aggregate classifiers such as random forests, as well as singleclassifiers (neural networks, decision trees) on a number of machine learningbenchmarks, and show that isoelastic combination methods are generally betterthan their logarithmic counterparts.
arxiv-1206-6464 | Estimating the Hessian by Back-propagating Curvature |  http://arxiv.org/abs/1206.6464  | author:James Martens, Ilya Sutskever, Kevin Swersky category:cs.LG stat.ML published:2012-06-27 summary:In this work we develop Curvature Propagation (CP), a general technique forefficiently computing unbiased approximations of the Hessian of any functionthat is computed using a computational graph. At the cost of roughly twogradient evaluations, CP can give a rank-1 approximation of the whole Hessian,and can be repeatedly applied to give increasingly precise unbiased estimatesof any or all of the entries of the Hessian. Of particular interest is thediagonal of the Hessian, for which no general approach is known to exist thatis both efficient and accurate. We show in experiments that CP turns out towork well in practice, giving very accurate estimates of the Hessian of neuralnetworks, for example, with a relatively small amount of work. We also apply CPto Score Matching, where a diagonal of a Hessian plays an integral role in theScore Matching objective, and where it is usually computed exactly usinginefficient algorithms which do not scale to larger and more complex models.
arxiv-1206-6838 | Continuous Time Markov Networks |  http://arxiv.org/abs/1206.6838  | author:Tal El-Hay, Nir Friedman, Daphne Koller, Raz Kupferman category:cs.AI cs.LG published:2012-06-27 summary:A central task in many applications is reasoning about processes that changein a continuous time. The mathematical framework of Continuous Time MarkovProcesses provides the basic foundations for modeling such systems. Recently,Nodelman et al introduced continuous time Bayesian networks (CTBNs), whichallow a compact representation of continuous-time processes over a factoredstate space. In this paper, we introduce continuous time Markov networks(CTMNs), an alternative representation language that represents a differenttype of continuous-time dynamics. In many real life processes, such asbiological and chemical systems, the dynamics of the process can be naturallydescribed as an interplay between two forces - the tendency of each entity tochange its state, and the overall fitness or energy function of the entiresystem. In our model, the first force is described by a continuous-timeproposal process that suggests possible local changes to the state of thesystem at different rates. The second force is represented by a Markov networkthat encodes the fitness, or desirability, of different states; a proposedlocal change is then accepted with a probability that is a function of thechange in the fitness distribution. We show that the fitness distribution isalso the stationary distribution of the Markov process, so that thisrepresentation provides a characterization of a temporal process whosestationary distribution has a compact graphical representation. This allows usto naturally capture a different type of structure in complex dynamicalprocesses, such as evolving biological sequences. We describe the semantics ofthe representation, its basic properties, and how it compares to CTBNs. We alsoprovide algorithms for learning such models from data, and discuss itsapplicability to biological sequence evolution.
arxiv-1206-6467 | Semi-Supervised Collective Classification via Hybrid Label Regularization |  http://arxiv.org/abs/1206.6467  | author:Luke McDowell, David Aha category:cs.LG stat.ML published:2012-06-27 summary:Many classification problems involve data instances that are interlinked witheach other, such as webpages connected by hyperlinks. Techniques for"collective classification" (CC) often increase accuracy for such data graphs,but usually require a fully-labeled training graph. In contrast, we examine howto improve the semi-supervised learning of CC models when given only asparsely-labeled graph, a common situation. We first describe how to use novelcombinations of classifiers to exploit the different characteristics of therelational features vs. the non-relational features. We also extend the ideasof "label regularization" to such hybrid classifiers, enabling them to leveragethe unlabeled data to bias the learning process. We find that these techniques,which are efficient and easy to implement, significantly increase accuracy onthree real datasets. In addition, our results explain conflicting findings fromprior related studies.
arxiv-1206-6833 | Matrix Tile Analysis |  http://arxiv.org/abs/1206.6833  | author:Inmar Givoni, Vincent Cheung, Brendan J. Frey category:cs.LG cs.CE cs.NA stat.ML published:2012-06-27 summary:Many tasks require finding groups of elements in a matrix of numbers, symbolsor class likelihoods. One approach is to use efficient bi- or tri-linearfactorization techniques including PCA, ICA, sparse matrix factorization andplaid analysis. These techniques are not appropriate when addition andmultiplication of matrix elements are not sensibly defined. More directly,methods like bi-clustering can be used to classify matrix elements, but thesemethods make the overly-restrictive assumption that the class of each elementis a function of a row class and a column class. We introduce a generalcomputational problem, `matrix tile analysis' (MTA), which consists ofdecomposing a matrix into a set of non-overlapping tiles, each of which isdefined by a subset of usually nonadjacent rows and columns. MTA does notrequire an algebra for combining tiles, but must search over discretecombinations of tile assignments. Exact MTA is a computationally intractableinteger programming problem, but we describe an approximate iterative techniqueand a computationally efficient sum-product relaxation of the integer program.We compare the effectiveness of these methods to PCA and plaid on hundreds ofrandomly generated tasks. Using double-gene-knockout data, we show that MTAfinds groups of interacting yeast genes that have biologically-relatedfunctions.
arxiv-1206-6832 | Convex Structure Learning for Bayesian Networks: Polynomial Feature Selection and Approximate Ordering |  http://arxiv.org/abs/1206.6832  | author:Yuhong Guo, Dale Schuurmans category:cs.LG stat.ML published:2012-06-27 summary:We present a new approach to learning the structure and parameters of aBayesian network based on regularized estimation in an exponential familyrepresentation. Here we show that, given a fixed variable order, the optimalstructure and parameters can be learned efficiently, even without restrictingthe size of the parent sets. We then consider the problem of optimizing thevariable order for a given set of features. This is still a computationallyhard problem, but we present a convex relaxation that yields an optimal 'soft'ordering in polynomial time. One novel aspect of the approach is that we do notperform a discrete search over DAG structures, nor over variable orders, butinstead solve a continuous relaxation that can then be rounded to obtain avalid network structure. We conduct an experimental comparison against standardstructure search procedures over standard objectives, which cope with localminima, and evaluate the advantages of using convex relaxations that reduce theeffects of local minima.
arxiv-1206-6830 | The AI&M Procedure for Learning from Incomplete Data |  http://arxiv.org/abs/1206.6830  | author:Manfred Jaeger category:stat.ME cs.AI cs.LG published:2012-06-27 summary:We investigate methods for parameter learning from incomplete data that isnot missing at random. Likelihood-based methods then require the optimizationof a profile likelihood that takes all possible missingness mechanisms intoaccount. Optimzing this profile likelihood poses two main difficulties:multiple (local) maxima, and its very high-dimensional parameter space. In thispaper a new method is presented for optimizing the profile likelihood thataddresses the second difficulty: in the proposed AI&M (adjusting imputation andmazimization) procedure the optimization is performed by operations in thespace of data completions, rather than directly in the parameter space of theprofile likelihood. We apply the AI&M method to learning parameters forBayesian networks. The method is compared against conservative inference, whichtakes into account each possible data completion, and against EM. The resultsindicate that likelihood-based inference is still feasible in the case ofunknown missingness mechanisms, and that conservative inference isunnecessarily weak. On the other hand, our results also provide evidence thatthe EM algorithm is still quite effective when the data is not missing atrandom.
arxiv-1206-6468 | Variational Inference in Non-negative Factorial Hidden Markov Models for Efficient Audio Source Separation |  http://arxiv.org/abs/1206.6468  | author:Gautham Mysore, Maneesh Sahani category:cs.LG cs.SD stat.ML published:2012-06-27 summary:The past decade has seen substantial work on the use of non-negative matrixfactorization and its probabilistic counterparts for audio source separation.Although able to capture audio spectral structure well, these models neglectthe non-stationarity and temporal dynamics that are important properties ofaudio. The recently proposed non-negative factorial hidden Markov model(N-FHMM) introduces a temporal dimension and improves source separationperformance. However, the factorial nature of this model makes the complexityof inference exponential in the number of sound sources. Here, we present aBayesian variant of the N-FHMM suited to an efficient variational inferencealgorithm, whose complexity is linear in the number of sound sources. Ouralgorithm performs comparably to exact inference in the original N-FHMM but issignificantly faster. In typical configurations of the N-FHMM, our methodachieves around a 30x increase in speed.
arxiv-1206-6469 | Inferring Latent Structure From Mixed Real and Categorical Relational Data |  http://arxiv.org/abs/1206.6469  | author:Esther Salazar, Matthew Cain, Elise Darling, Stephen Mitroff, Lawrence Carin category:cs.LG stat.ML published:2012-06-27 summary:We consider analysis of relational data (a matrix), in which the rowscorrespond to subjects (e.g., people) and the columns correspond to attributes.The elements of the matrix may be a mix of real and categorical. Each subjectand attribute is characterized by a latent binary feature vector, and aninferred matrix maps each row-column pair of binary feature vectors to anobserved matrix element. The latent binary features of the rows are modeled viaa multivariate Gaussian distribution with low-rank covariance matrix, and theGaussian random variables are mapped to latent binary features via a probitlink. The same type construction is applied jointly to the columns. The modelinfers latent, low-dimensional binary features associated with each row andeach column, as well correlation structure between all rows and between allcolumns.
arxiv-1206-6470 | A Combinatorial Algebraic Approach for the Identifiability of Low-Rank Matrix Completion |  http://arxiv.org/abs/1206.6470  | author:Franz Kiraly, Ryota Tomioka category:cs.LG cs.DM cs.NA stat.ML published:2012-06-27 summary:In this paper, we review the problem of matrix completion and expose itsintimate relations with algebraic geometry, combinatorics and graph theory. Wepresent the first necessary and sufficient combinatorial conditions formatrices of arbitrary rank to be identifiable from a set of matrix entries,yielding theoretical constraints and new algorithms for the problem of matrixcompletion. We conclude by algorithmically evaluating the tightness of thegiven conditions and algorithms for practically relevant matrix sizes, showingthat the algebraic-combinatoric approach can lead to improvements overstate-of-the-art matrix completion methods.
arxiv-1206-6471 | On Causal and Anticausal Learning |  http://arxiv.org/abs/1206.6471  | author:Bernhard Schoelkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, Joris Mooij category:cs.LG stat.ML published:2012-06-27 summary:We consider the problem of function estimation in the case where anunderlying causal model can be inferred. This has implications for popularscenarios such as covariate shift, concept drift, transfer learning andsemi-supervised learning. We argue that causal knowledge may facilitate someapproaches for a given problem, and rule out others. In particular, weformulate a hypothesis for when semi-supervised learning can help, andcorroborate it with empirical results.
arxiv-1206-6472 | An Efficient Approach to Sparse Linear Discriminant Analysis |  http://arxiv.org/abs/1206.6472  | author:Luis Francisco Sanchez Merchante, Yves Grandvalet, Gerrad Govaert category:cs.LG stat.ML published:2012-06-27 summary:We present a novel approach to the formulation and the resolution of sparseLinear Discriminant Analysis (LDA). Our proposal, is based on penalized OptimalScoring. It has an exact equivalence with penalized LDA, contrary to themulti-class approaches based on the regression of class indicator that havebeen proposed so far. Sparsity is obtained thanks to a group-Lasso penalty thatselects the same features in all discriminant directions. Our experimentsdemonstrate that this approach generates extremely parsimonious models withoutcompromising prediction performances. Besides prediction, the resulting sparsediscriminant directions are also amenable to low-dimensional representations ofdata. Our algorithm is highly efficient for medium to large number ofvariables, and is thus particularly well suited to the analysis of geneexpression data.
arxiv-1206-6473 | Compositional Planning Using Optimal Option Models |  http://arxiv.org/abs/1206.6473  | author:David Silver, Kamil Ciosek category:cs.AI cs.LG published:2012-06-27 summary:In this paper we introduce a framework for option model composition. Optionmodels are temporal abstractions that, like macro-operators in classicalplanning, jump directly from a start state to an end state. Prior work hasfocused on constructing option models from primitive actions, by intra-optionmodel learning; or on using option models to construct a value function, byinter-option planning. We present a unified view of intra- and inter-optionmodel learning, based on a major generalisation of the Bellman equation. Ourfundamental operation is the recursive composition of option models into otheroption models. This key idea enables compositional planning over many levels ofabstraction. We illustrate our framework using a dynamic programming algorithmthat simultaneously constructs optimal option models for multiple subgoals, andalso searches over those option models to provide rapid progress towards othersubgoals.
arxiv-1206-6474 | Estimation of Simultaneously Sparse and Low Rank Matrices |  http://arxiv.org/abs/1206.6474  | author:Emile Richard, Pierre-Andre Savalle, Nicolas Vayatis category:cs.DS cs.LG cs.NA stat.ML published:2012-06-27 summary:The paper introduces a penalized matrix estimation procedure aiming atsolutions which are sparse and low-rank at the same time. Such structures arisein the context of social networks or protein interactions where underlyinggraphs have adjacency matrices which are block-diagonal in the appropriatebasis. We introduce a convex mixed penalty which involves $\ell_1$-norm andtrace norm simultaneously. We obtain an oracle inequality which indicates howthe two effects interact according to the nature of the target matrix. We boundgeneralization error in the link prediction problem. We also develop proximaldescent strategies to solve the optimization problem efficiently and evaluateperformance on synthetic and real data sets.
arxiv-1206-6476 | Similarity Learning for Provably Accurate Sparse Linear Classification |  http://arxiv.org/abs/1206.6476  | author:Aurelien Bellet, Amaury Habrard, Marc Sebban category:cs.LG stat.ML published:2012-06-27 summary:In recent years, the crucial importance of metrics in machine learningalgorithms has led to an increasing interest for optimizing distance andsimilarity functions. Most of the state of the art focus on learningMahalanobis distances (requiring to fulfill a constraint of positivesemi-definiteness) for use in a local k-NN algorithm. However, no theoreticallink is established between the learned metrics and their performance inclassification. In this paper, we make use of the formal framework of goodsimilarities introduced by Balcan et al. to design an algorithm for learning anon PSD linear similarity optimized in a nonlinear feature space, which is thenused to build a global linear classifier. We show that our approach has uniformstability and derive a generalization bound on the classification error.Experiments performed on various datasets confirm the effectiveness of ourapproach compared to state-of-the-art methods and provide evidence that (i) itis fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.
arxiv-1206-6477 | Discovering Support and Affiliated Features from Very High Dimensions |  http://arxiv.org/abs/1206.6477  | author:Yiteng Zhai, Mingkui Tan, Ivor Tsang, Yew Soon Ong category:cs.LG stat.ML published:2012-06-27 summary:In this paper, a novel learning paradigm is presented to automaticallyidentify groups of informative and correlated features from very highdimensions. Specifically, we explicitly incorporate correlation measures asconstraints and then propose an efficient embedded feature selection methodusing recently developed cutting plane strategy. The benefits of the proposedalgorithm are two-folds. First, it can identify the optimal discriminative anduncorrelated feature subset to the output labels, denoted here as SupportFeatures, which brings about significant improvements in prediction performanceover other state of the art feature selection methods considered in the paper.Second, during the learning process, the underlying group structures ofcorrelated features associated with each support feature, denoted as AffiliatedFeatures, can also be discovered without any additional cost. These affiliatedfeatures serve to improve the interpretations on the learning tasks. Extensiveempirical studies on both synthetic and very high dimensional real-worlddatasets verify the validity and efficiency of the proposed method.
arxiv-1206-6478 | Maximum Margin Output Coding |  http://arxiv.org/abs/1206.6478  | author:Yi Zhang, Jeff Schneider category:cs.LG stat.ML published:2012-06-27 summary:In this paper we study output coding for multi-label prediction. For amulti-label output coding to be discriminative, it is important that codewordsfor different label vectors are significantly different from each other. In themeantime, unlike in traditional coding theory, codewords in output coding areto be predicted from the input, so it is also critical to have a predictablelabel encoding. To find output codes that are both discriminative and predictable, we firstpropose a max-margin formulation that naturally captures these two properties.We then convert it to a metric learning formulation, but with an exponentiallylarge number of constraints as commonly encountered in structured predictionproblems. Without a label structure for tractable inference, we useovergenerating (i.e., relaxation) techniques combined with the cutting planemethod for optimization. In our empirical study, the proposed output coding scheme outperforms avariety of existing multi-label prediction methods for image, text and musicclassification.
arxiv-1206-6479 | The Landmark Selection Method for Multiple Output Prediction |  http://arxiv.org/abs/1206.6479  | author:Krishnakumar Balasubramanian, Guy Lebanon category:cs.LG stat.ML published:2012-06-27 summary:Conditional modeling x \to y is a central problem in machine learning. Asubstantial research effort is devoted to such modeling when x is highdimensional. We consider, instead, the case of a high dimensional y, where x iseither low dimensional or high dimensional. Our approach is based on selectinga small subset y_L of the dimensions of y, and proceed by modeling (i) x \toy_L and (ii) y_L \to y. Composing these two models, we obtain a conditionalmodel x \to y that possesses convenient statistical properties. Multi-labelclassification and multivariate regression experiments on several datasets showthat this model outperforms the one vs. all approach as well as severalsophisticated multiple output prediction methods.
arxiv-1206-6828 | Advances in exact Bayesian structure discovery in Bayesian networks |  http://arxiv.org/abs/1206.6828  | author:Mikko Koivisto category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We consider a Bayesian method for learning the Bayesian network structurefrom complete data. Recently, Koivisto and Sood (2004) presented an algorithmthat for any single edge computes its marginal posterior probability in O(n2^n) time, where n is the number of attributes; the number of parents perattribute is bounded by a constant. In this paper we show that the posteriorprobabilities for all the n (n - 1) potential edges can be computed in O(n 2^n)total time. This result is achieved by a forward-backward technique and fastMoebius transform algorithms, which are of independent interest. The resultingspeedup by a factor of about n^2 allows us to experimentally study thestatistical power of learning moderate-size networks. We report results from asimulation study that covers data sets with 20 to 10,000 records over 5 to 25discrete attributes
arxiv-1206-6480 | A Dantzig Selector Approach to Temporal Difference Learning |  http://arxiv.org/abs/1206.6480  | author:Matthieu Geist, Bruno Scherrer, Alessandro Lazaric, Mohammad Ghavamzadeh category:cs.LG stat.ML published:2012-06-27 summary:LSTD is a popular algorithm for value function approximation. Whenever thenumber of features is larger than the number of samples, it must be paired withsome form of regularization. In particular, L1-regularization methods tend toperform feature selection by promoting sparsity, and thus, are well-suited forhigh-dimensional problems. However, since LSTD is not a simple regressionalgorithm, but it solves a fixed--point problem, its integration withL1-regularization is not straightforward and might come with some drawbacks(e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce anovel algorithm obtained by integrating LSTD with the Dantzig Selector. Weinvestigate the performance of the proposed algorithm and its relationship withthe existing regularized approaches, and show how it addresses some of theirdrawbacks.
arxiv-1206-6481 | Cross Language Text Classification via Subspace Co-Regularized Multi-View Learning |  http://arxiv.org/abs/1206.6481  | author:Yuhong Guo, Min Xiao category:cs.CL cs.IR cs.LG published:2012-06-27 summary:In many multilingual text classification problems, the documents in differentlanguages often share the same set of categories. To reduce the labeling costof training a classification model for each individual language, it isimportant to transfer the label knowledge gained from one language to anotherlanguage by conducting cross language classification. In this paper we developa novel subspace co-regularized multi-view learning method for cross languagetext classification. This method is built on parallel corpora produced bymachine translation. It jointly minimizes the training error of each classifierin each language while penalizing the distance between the subspacerepresentations of parallel documents. Our empirical study on a large set ofcross language text classification tasks shows the proposed method consistentlyoutperforms a number of inductive methods, domain adaptation methods, andmulti-view learning methods.
arxiv-1206-6482 | Modeling Images using Transformed Indian Buffet Processes |  http://arxiv.org/abs/1206.6482  | author:Ke Zhai, Yuening Hu, Sinead Williamson, Jordan Boyd-Graber category:cs.CV cs.LG stat.ML published:2012-06-27 summary:Latent feature models are attractive for image modeling, since imagesgenerally contain multiple objects. However, many latent feature models ignorethat objects can appear at different locations or require pre-segmentation ofimages. While the transformed Indian buffet process (tIBP) provides a methodfor modeling transformation-invariant features in unsegmented binary images,its current form is inappropriate for real images because of its computationalcost and modeling assumptions. We combine the tIBP with likelihoods appropriatefor real images and develop an efficient inference, using the cross-correlationbetween images and features, that is theoretically and empirically faster thanexisting inference techniques. Our method discovers reasonable components andachieve effective image reconstruction in natural images.
arxiv-1206-6483 | Subgraph Matching Kernels for Attributed Graphs |  http://arxiv.org/abs/1206.6483  | author:Nils Kriege, Petra Mutzel category:cs.LG stat.ML published:2012-06-27 summary:We propose graph kernels based on subgraph matchings, i.e.structure-preserving bijections between subgraphs. While recently proposedkernels based on common subgraphs (Wale et al., 2008; Shervashidze et al.,2009) in general can not be applied to attributed graphs, our approach allowsto rate mappings of subgraphs by a flexible scoring scheme comparing vertex andedge attributes by kernels. We show that subgraph matching kernels generalizeseveral known kernels. To compute the kernel we propose a graph-theoreticalalgorithm inspired by a classical relation between common subgraphs of twographs and cliques in their product graph observed by Levi (1973). Encouragingexperimental results on a classification task of real-world graphs arepresented.
arxiv-1206-6824 | Gene Expression Time Course Clustering with Countably Infinite Hidden Markov Models |  http://arxiv.org/abs/1206.6824  | author:Matthew Beal, Praveen Krishnamurthy category:cs.LG cs.CE stat.ML published:2012-06-27 summary:Most existing approaches to clustering gene expression time course data treatthe different time points as independent dimensions and are invariant topermutations, such as reversal, of the experimental time course. Approachesutilizing HMMs have been shown to be helpful in this regard, but are hamperedby having to choose model architectures with appropriate complexities. Here wepropose for a clustering application an HMM with a countably infinite statespace; inference in this model is possible by recasting it in the hierarchicalDirichlet process (HDP) framework (Teh et al. 2006), and hence we call it theHDP-HMM. We show that the infinite model outperforms model selection methodsover finite models, and traditional time-independent methods, as measured by avariety of external and internal indices for clustering on two large publiclyavailable data sets. Moreover, we show that the infinite models utilize morehidden states and employ richer architectures (e.g. state-to-state transitions)without the damaging effects of overfitting.
arxiv-1206-6484 | Apprenticeship Learning for Model Parameters of Partially Observable Environments |  http://arxiv.org/abs/1206.6484  | author:Takaki Makino, Johane Takeuchi category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We consider apprenticeship learning, i.e., having an agent learn a task byobserving an expert demonstrating the task in a partially observableenvironment when the model of the environment is uncertain. This setting isuseful in applications where the explicit modeling of the environment isdifficult, such as a dialogue system. We show that we can extract informationabout the environment model by inferring action selection process behind thedemonstration, under the assumption that the expert is choosing optimal actionsbased on knowledge of the true model of the target environment. Proposedalgorithms can achieve more accurate estimates of POMDP parameters and betterpolicies from a short demonstration, compared to methods that learns only fromthe reaction from the environment.
arxiv-1206-6485 | Greedy Algorithms for Sparse Reinforcement Learning |  http://arxiv.org/abs/1206.6485  | author:Christopher Painter-Wakefield, Ronald Parr category:cs.LG stat.ML published:2012-06-27 summary:Feature selection and regularization are becoming increasingly prominenttools in the efforts of the reinforcement learning (RL) community to expand thereach and applicability of RL. One approach to the problem of feature selectionis to impose a sparsity-inducing form of regularization on the learning method.Recent work on $L_1$ regularization has adapted techniques from the supervisedlearning literature for use with RL. Another approach that has received renewedattention in the supervised learning community is that of using a simplealgorithm that greedily adds new features. Such algorithms have many of thegood properties of the $L_1$ regularization methods, while also being extremelyefficient and, in some cases, allowing theoretical guarantees on recovery ofthe true form of a sparse target function from sampled data. This paperconsiders variants of orthogonal matching pursuit (OMP) applied toreinforcement learning. The resulting algorithms are analyzed and comparedexperimentally with existing $L_1$ regularized approaches. We demonstrate thatperhaps the most natural scenario in which one might hope to achieve sparserecovery fails; however, one variant, OMP-BRM, provides promising theoreticalguarantees under certain assumptions on the feature dictionary. Anothervariant, OMP-TD, empirically outperforms prior methods both in approximationaccuracy and efficiency on several benchmark problems.
arxiv-1206-6486 | Flexible Modeling of Latent Task Structures in Multitask Learning |  http://arxiv.org/abs/1206.6486  | author:Alexandre Passos, Piyush Rai, Jacques Wainer, Hal Daume III category:cs.LG stat.ML published:2012-06-27 summary:Multitask learning algorithms are typically designed assuming some fixed, apriori known latent structure shared by all the tasks. However, it is usuallyunclear what type of latent task structure is the most appropriate for a givenmultitask learning problem. Ideally, the "right" latent task structure shouldbe learned in a data-driven manner. We present a flexible, nonparametricBayesian model that posits a mixture of factor analyzers structure on thetasks. The nonparametric aspect makes the model expressive enough to subsumemany existing models of latent task structures (e.g, mean-regularized tasks,clustered tasks, low-rank or linear/non-linear subspace assumption on tasks,etc.). Moreover, it can also learn more general task structures, addressing theshortcomings of such models. We present a variational inference algorithm forour model. Experimental results on synthetic and real-world datasets, on bothregression and classification problems, demonstrate the effectiveness of theproposed method.
arxiv-1206-6487 | An Adaptive Algorithm for Finite Stochastic Partial Monitoring |  http://arxiv.org/abs/1206.6487  | author:Gabor Bartok, Navid Zolghadr, Csaba Szepesvari category:cs.LG cs.GT stat.ML published:2012-06-27 summary:We present a new anytime algorithm that achieves near-optimal regret for anyinstance of finite stochastic partial monitoring. In particular, the newalgorithm achieves the minimax regret, within logarithmic factors, for both"easy" and "hard" problems. For easy problems, it additionally achieveslogarithmic individual regret. Most importantly, the algorithm is adaptive inthe sense that if the opponent strategy is in an "easy region" of the strategyspace then the regret grows as if the problem was easy. As an implication, weshow that under some reasonable additional assumptions, the algorithm enjoys anO(\sqrt{T}) regret in Dynamic Pricing, proven to be hard by Bartok et al.(2011).
arxiv-1206-6488 | The Nonparanormal SKEPTIC |  http://arxiv.org/abs/1206.6488  | author:Han Liu, Fang Han, Ming Yuan, John Lafferty, Larry Wasserman category:stat.ME cs.LG stat.ML published:2012-06-27 summary:We propose a semiparametric approach, named nonparanormal skeptic, forestimating high dimensional undirected graphical models. In terms of modeling,we consider the nonparanormal family proposed by Liu et al (2009). In terms ofestimation, we exploit nonparametric rank-based correlation coefficientestimators including the Spearman's rho and Kendall's tau. In high dimensionalsettings, we prove that the nonparanormal skeptic achieves the optimalparametric rate of convergence in both graph and parameter estimation. Thisresult suggests that the nonparanormal graphical models are a safe replacementof the Gaussian graphical models, even when the data are Gaussian.
arxiv-1206-6361 | Learning Markov Network Structure using Brownian Distance Covariance |  http://arxiv.org/abs/1206.6361  | author:Ehsan Khoshgnauz category:stat.ML cs.LG published:2012-06-27 summary:In this paper, we present a simple non-parametric method for learning thestructure of undirected graphs from data that drawn from an underlying unknowndistribution. We propose to use Brownian distance covariance to estimate theconditional independences between the random variables and encodes pairwiseMarkov graph. This framework can be applied in high-dimensional setting, wherethe number of parameters much be larger than the sample size.
arxiv-1206-6878 | Efficient Selection of Disambiguating Actions for Stereo Vision |  http://arxiv.org/abs/1206.6878  | author:Monika Schaeffer, Ron Parr category:cs.CV published:2012-06-27 summary:In many domains that involve the use of sensors, such as robotics or sensornetworks, there are opportunities to use some form of active sensing todisambiguate data from noisy or unreliable sensors. These disambiguatingactions typically take time and expend energy. One way to choose the nextdisambiguating action is to select the action with the greatest expectedentropy reduction, or information gain. In this work, we consider activesensing in aid of stereo vision for robotics. Stereo vision is a powerfulsensing technique for mobile robots, but it can fail in scenes that lack strongtexture. In such cases, a structured light source, such as vertical laser linecan be used for disambiguation. By treating the stereo matching problem as aspecially structured HMM-like graphical model, we demonstrate that for a scanline with n columns and maximum stereo disparity d, the entropy minimizing aimpoint for the laser can be selected in O(nd) time - cost no greater than thestereo algorithm itself. In contrast, a typical HMM formulation would suggestat least O(nd^2) time for the entropy calculation alone.
arxiv-1206-6873 | Variable noise and dimensionality reduction for sparse Gaussian processes |  http://arxiv.org/abs/1206.6873  | author:Edward Snelson, Zoubin Ghahramani category:cs.LG stat.ML published:2012-06-27 summary:The sparse pseudo-input Gaussian process (SPGP) is a new approximation methodfor speeding up GP regression in the case of a large number of data points N.The approximation is controlled by the gradient optimization of a small set ofM `pseudo-inputs', thereby reducing complexity from N^3 to NM^2. One limitationof the SPGP is that this optimization space becomes impractically big for highdimensional data sets. This paper addresses this limitation by performingautomatic dimensionality reduction. A projection of the input space to a lowdimensional space is learned in a supervised manner, alongside thepseudo-inputs, which now live in this reduced space. The paper alsoinvestigates the suitability of the SPGP for modeling data with input-dependentnoise. A further extension of the model is made to make it even more powerfulin this regard - we learn an uncertainty parameter for each pseudo-input. Thecombination of sparsity, reduced dimension, and input-dependent noise makes itpossible to apply GPs to much larger and more complex data sets than waspreviously practical. We demonstrate the benefits of these methods on severalsynthetic and real world problems.
arxiv-1206-6872 | A Self-Supervised Terrain Roughness Estimator for Off-Road Autonomous Driving |  http://arxiv.org/abs/1206.6872  | author:David Stavens, Sebastian Thrun category:cs.CV cs.LG cs.RO published:2012-06-27 summary:We present a machine learning approach for estimating the second derivativeof a drivable surface, its roughness. Robot perception generally focuses on thefirst derivative, obstacle detection. However, the second derivative is alsoimportant due to its direct relation (with speed) to the shock the vehicleexperiences. Knowing the second derivative allows a vehicle to slow down inadvance of rough terrain. Estimating the second derivative is challenging dueto uncertainty. For example, at range, laser readings may be so sparse thatsignificant information about the surface is missing. Also, a high degree ofprecision is required in projecting laser readings. This precision may beunavailable due to latency or error in the pose estimation. We model thesesources of error as a multivariate polynomial. Its coefficients are learnedusing the shock data as ground truth -- the accelerometers are used to trainthe lasers. The resulting classifier operates on individual laser readings froma road surface described by a 3D point cloud. The classifier identifiessections of road where the second derivative is likely to be large. Thus, thevehicle can slow down in advance, reducing the shock it experiences. Thealgorithm is an evolution of one we used in the 2005 DARPA Grand Challenge. Weanalyze it using data from that route.
arxiv-1206-6871 | Ranking by Dependence - A Fair Criteria |  http://arxiv.org/abs/1206.6871  | author:Harald Steck category:cs.LG stat.ML published:2012-06-27 summary:Estimating the dependences between random variables, and ranking themaccordingly, is a prevalent problem in machine learning. Pursuing frequentistand information-theoretic approaches, we first show that the p-value and themutual information can fail even in simplistic situations. We then propose twoconditions for regularizing an estimator of dependence, which leads to a simpleyet effective new measure. We discuss its advantages and compare it towell-established model-selection criteria. Apart from that, we derive a simpleconstraint for regularizing parameter estimates in a graphical model. Thisresults in an analytical approximation for the optimal value of the equivalentsample size, which agrees very well with the more involved Bayesian approach inour experiments.
arxiv-1206-6870 | Incremental Model-based Learners With Formal Learning-Time Guarantees |  http://arxiv.org/abs/1206.6870  | author:Alexander L. Strehl, Lihong Li, Michael L. Littman category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Model-based learning algorithms have been shown to use experience efficientlywhen learning to solve Markov Decision Processes (MDPs) with finite state andaction spaces. However, their high computational cost due to repeatedly solvingan internal model inhibits their use in large-scale problems. We propose amethod based on real-time dynamic programming (RTDP) to speed up twomodel-based algorithms, RMAX and MBIE (model-based interval estimation),resulting in computationally much faster algorithms with little loss comparedto existing bounds. Specifically, our two new learning algorithms, RTDP-RMAXand RTDP-IE, have considerably smaller computational demands than RMAX andMBIE. We develop a general theoretical framework that allows us to prove thatboth are efficient learners in a PAC (probably approximately correct) sense. Wealso present an experimental evaluation of these new algorithms that helpsquantify the tradeoff between computational and experience demands.
arxiv-1206-6868 | Bayesian Random Fields: The Bethe-Laplace Approximation |  http://arxiv.org/abs/1206.6868  | author:Max Welling, Sridevi Parise category:cs.LG stat.ML published:2012-06-27 summary:While learning the maximum likelihood value of parameters of an undirectedgraphical model is hard, modelling the posterior distribution over parametersgiven data is harder. Yet, undirected models are ubiquitous in computer visionand text modelling (e.g. conditional random fields). But where Bayesianapproaches for directed models have been very successful, a proper Bayesiantreatment of undirected models in still in its infant stages. We propose a newmethod for approximating the posterior of the parameters given data based onthe Laplace approximation. This approximation requires the computation of thecovariance matrix over features which we compute using the linear responseapproximation based in turn on loopy belief propagation. We develop the theoryfor conditional and 'unconditional' random fields with or without hiddenvariables. In the conditional setting we introduce a new variant of baggingsuitable for structured domains. Here we run the loopy max-product algorithm ona 'super-graph' composed of graphs for individual models sampled from theposterior and connected by constraints. Experiments on real world data validatethe proposed methods.
arxiv-1206-6865 | A Non-Parametric Bayesian Method for Inferring Hidden Causes |  http://arxiv.org/abs/1206.6865  | author:Frank Wood, Thomas Griffiths, Zoubin Ghahramani category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We present a non-parametric Bayesian approach to structure learning withhidden causes. Previous Bayesian treatments of this problem define a prior overthe number of hidden causes and use algorithms such as reversible jump Markovchain Monte Carlo to move between solutions. In contrast, we assume that thenumber of hidden causes is unbounded, but only a finite number influenceobservable variables. This makes it possible to use a Gibbs sampler toapproximate the distribution over causal structures. We evaluate theperformance of both approaches in discovering hidden causes in simulated data,and use our non-parametric approach to discover hidden causes in a real medicaldataset.
arxiv-1206-6381 | Shortest path distance in random k-nearest neighbor graphs |  http://arxiv.org/abs/1206.6381  | author:Morteza Alamgir, Ulrike von Luxburg category:cs.LG stat.ML published:2012-06-27 summary:Consider a weighted or unweighted k-nearest neighbor graph that has beenbuilt on n data points drawn randomly according to some density p on R^d. Westudy the convergence of the shortest path distance in such graphs as thesample size tends to infinity. We prove that for unweighted kNN graphs, thisdistance converges to an unpleasant distance function on the underlying spacewhose properties are detrimental to machine learning. We also study thebehavior of the shortest path distance in weighted kNN graphs.
arxiv-1206-6864 | Infinite Hidden Relational Models |  http://arxiv.org/abs/1206.6864  | author:Zhao Xu, Volker Tresp, Kai Yu, Hans-Peter Kriegel category:cs.AI cs.DB cs.LG published:2012-06-27 summary:In many cases it makes sense to model a relationship symmetrically, notimplying any particular directionality. Consider the classical example of arecommendation system where the rating of an item by a user shouldsymmetrically be dependent on the attributes of both the user and the item. Theattributes of the (known) relationships are also relevant for predictingattributes of entities and for predicting attributes of new relations. Inrecommendation systems, the exploitation of relational attributes is oftenreferred to as collaborative filtering. Again, in many applications one mightprefer to model the collaborative effect in a symmetrical way. In this paper wepresent a relational model, which is completely symmetrical. The key innovationis that we introduce for each entity (or object) an infinite-dimensional latentvariable as part of a Dirichlet process (DP) model. We discuss inference in themodel, which is based on a DP Gibbs sampler, i.e., the Chinese restaurantprocess. We extend the Chinese restaurant process to be applicable torelational modeling. Our approach is evaluated in three applications. One is arecommendation system based on the MovieLens data set. The second applicationconcerns the prediction of the function of yeast genes/proteins on the data setof KDD Cup 2001 using a multi-relational model. The third application involvesa relational medical domain. The experimental results show that our model givessignificantly improved estimates of attributes describing relationships orentities in complex relational models.
arxiv-1206-6863 | Bayesian Multicategory Support Vector Machines |  http://arxiv.org/abs/1206.6863  | author:Zhihua Zhang, Michael I. Jordan category:cs.LG stat.ML published:2012-06-27 summary:We show that the multi-class support vector machine (MSVM) proposed by Leeet. al. (2004), can be viewed as a MAP estimation procedure under anappropriate probabilistic interpretation of the classifier. We also show thatthis interpretation can be extended to a hierarchical Bayesian architecture andto a fully-Bayesian inference procedure for multi-class classification based ondata augmentation. We present empirical results that show that the advantagesof the Bayesian formalism are obtained without a loss in classificationaccuracy.
arxiv-1206-6862 | On the Number of Samples Needed to Learn the Correct Structure of a Bayesian Network |  http://arxiv.org/abs/1206.6862  | author:Or Zuk, Shiri Margel, Eytan Domany category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Bayesian Networks (BNs) are useful tools giving a natural and compactrepresentation of joint probability distributions. In many applications oneneeds to learn a Bayesian Network (BN) from data. In this context, it isimportant to understand the number of samples needed in order to guarantee asuccessful learning. Previous work have studied BNs sample complexity, yet itmainly focused on the requirement that the learned distribution will be closeto the original distribution which generated the data. In this work, we study adifferent aspect of the learning, namely the number of samples needed in orderto learn the correct structure of the network. We give both asymptotic results,valid in the large sample limit, and experimental results, demonstrating thelearning behavior for feasible sample sizes. We show that structure learning isa more difficult task, compared to approximating the correct distribution, inthe sense that it requires a much larger number of samples, regardless of thecomputational power available for the learner.
arxiv-1206-6860 | Predicting Conditional Quantiles via Reduction to Classification |  http://arxiv.org/abs/1206.6860  | author:John Langford, Roberto Oliveira, Bianca Zadrozny category:cs.LG stat.ML published:2012-06-27 summary:We show how to reduce the process of predicting general order statistics (andthe median in particular) to solving classification. The accompanyingtheoretical statement shows that the regret of the classifier bounds the regretof the quantile regression under a quantile loss. We also test this reductionempirically against existing quantile regression methods on large real-worlddatasets and discover that it provides state-of-the-art performance.
arxiv-1206-6858 | Sequential Document Representations and Simplicial Curves |  http://arxiv.org/abs/1206.6858  | author:Guy Lebanon category:cs.IR cs.LG published:2012-06-27 summary:The popular bag of words assumption represents a document as a histogram ofword occurrences. While computationally efficient, such a representation isunable to maintain any sequential information. We present a continuous anddifferentiable sequential document representation that goes beyond the bag ofwords assumption, and yet is efficient and effective. This representationemploys smooth curves in the multinomial simplex to account for sequentialinformation. We discuss the representation and its geometric properties anddemonstrate its applicability for the task of text classification.
arxiv-1206-6857 | Faster Gaussian Summation: Theory and Experiment |  http://arxiv.org/abs/1206.6857  | author:Dongryeol Lee, Alexander G. Gray category:cs.LG cs.NA stat.ML published:2012-06-27 summary:We provide faster algorithms for the problem of Gaussian summation, whichoccurs in many machine learning methods. We develop two new extensions - anO(Dp) Taylor expansion for the Gaussian kernel with rigorous error bounds and anew error control scheme integrating any arbitrary approximation method -within the best discretealgorithmic framework using adaptive hierarchical datastructures. We rigorously evaluate these techniques empirically in the contextof optimal bandwidth selection in kernel density estimation, revealing thestrengths and weaknesses of current state-of-the-art approaches for the firsttime. Our results demonstrate that the new error control scheme yields improvedperformance, whereas the series expansion approach is only effective in lowdimensions (five or less).
arxiv-1206-6852 | Structured Priors for Structure Learning |  http://arxiv.org/abs/1206.6852  | author:Vikash Mansinghka, Charles Kemp, Thomas Griffiths, Joshua Tenenbaum category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Traditional approaches to Bayes net structure learning typically assumelittle regularity in graph structure other than sparseness. However, in manycases, we expect more systematicity: variables in real-world systems oftengroup into classes that predict the kinds of probabilistic dependencies theyparticipate in. Here we capture this form of prior knowledge in a hierarchicalBayesian framework, and exploit it to enable structure learning and typediscovery from small datasets. Specifically, we present a nonparametricgenerative model for directed acyclic graphs as a prior for Bayes net structurelearning. Our model assumes that variables come in one or more classes and thatthe prior probability of an edge existing between two variables is a functiononly of their classes. We derive an MCMC algorithm for simultaneous inferenceof the number of classes, the class assignments of variables, and the Bayes netstructure over variables. For several realistic, sparse datasets, we show thatthe bias towards systematicity of connections provided by our model yields moreaccurate learned networks than a traditional, uniform prior approach, and thatthe classes found by our model are appropriate.
arxiv-1206-6851 | A compact, hierarchical Q-function decomposition |  http://arxiv.org/abs/1206.6851  | author:Bhaskara Marthi, Stuart Russell, David Andre category:cs.LG cs.AI stat.ML published:2012-06-27 summary:Previous work in hierarchical reinforcement learning has faced a dilemma:either ignore the values of different possible exit states from a subroutine,thereby risking suboptimal behavior, or represent those values explicitlythereby incurring a possibly large representation cost because exit valuesrefer to nonlocal aspects of the world (i.e., all subsequent rewards). Thispaper shows that, in many cases, one can avoid both of these problems. Thesolution is based on recursively decomposing the exit value function in termsof Q-functions at higher levels of the hierarchy. This leads to an intuitivelyappealing runtime architecture in which a parent subroutine passes to its childa value function on the exit states and the child reasons about how its choicesaffect the exit value. We also identify structural conditions on the valuefunction and transition distributions that allow much more conciserepresentations of exit state distributions, leading to further stateabstraction. In essence, the only variables whose exit values need beconsidered are those that the parent cares about and the child affects. Wedemonstrate the utility of our algorithms on a series of increasingly complexenvironments.
arxiv-1206-6847 | Identifying the Relevant Nodes Without Learning the Model |  http://arxiv.org/abs/1206.6847  | author:Jose M. Pena, Roland Nilsson, Johan Björkegren, Jesper Tegnér category:cs.LG cs.AI stat.ML published:2012-06-27 summary:We propose a method to identify all the nodes that are relevant to computeall the conditional probability distributions for a given set of nodes. Ourmethod is simple, effcient, consistent, and does not require learning aBayesian network first. Therefore, our method can be applied tohigh-dimensional databases, e.g. gene expression databases.
arxiv-1206-6846 | Approximate Separability for Weak Interaction in Dynamic Systems |  http://arxiv.org/abs/1206.6846  | author:Avi Pfeffer category:cs.LG cs.AI stat.ML published:2012-06-27 summary:One approach to monitoring a dynamic system relies on decomposition of thesystem into weakly interacting subsystems. An earlier paper introduced a notionof weak interaction called separability, and showed that it leads to exactpropagation of marginals for prediction. This paper addresses two questionsleft open by the earlier paper: can we define a notion of approximateseparability that occurs naturally in practice, and do separability andapproximate separability lead to accurate monitoring? The answer to bothquestions is afirmative. The paper also analyzes the structure of approximatelyseparable decompositions, and provides some explanation as to why these modelsperform well.
arxiv-1206-6845 | Gibbs Sampling for (Coupled) Infinite Mixture Models in the Stick Breaking Representation |  http://arxiv.org/abs/1206.6845  | author:Ian Porteous, Alexander T. Ihler, Padhraic Smyth, Max Welling category:stat.ME cs.LG stat.ML published:2012-06-27 summary:Nonparametric Bayesian approaches to clustering, information retrieval,language modeling and object recognition have recently shown great promise as anew paradigm for unsupervised data analysis. Most contributions have focused onthe Dirichlet process mixture models or extensions thereof for which efficientGibbs samplers exist. In this paper we explore Gibbs samplers for infinitecomplexity mixture models in the stick breaking representation. The advantageof this representation is improved modeling flexibility. For instance, one candesign the prior distribution over cluster sizes or couple multiple infinitemixture models (e.g. over time) at the level of their parameters (i.e. thedependent Dirichlet process model). However, Gibbs samplers for infinitemixture models (as recently introduced in the statistics literature) seem tomix poorly over cluster labels. Among others issues, this can have the adverseeffect that labels for the same cluster in coupled mixture models are mixed up.We introduce additional moves in these samplers to improve mixing over clusterlabels and to bring clusters into correspondence. An application to modeling ofstorm trajectories is used to illustrate these ideas.
arxiv-1206-6842 | Chi-square Tests Driven Method for Learning the Structure of Factored MDPs |  http://arxiv.org/abs/1206.6842  | author:Thomas Degris, Olivier Sigaud, Pierre-Henri Wuillemin category:cs.LG cs.AI stat.ML published:2012-06-27 summary:SDYNA is a general framework designed to address large stochasticreinforcement learning problems. Unlike previous model based methods in FMDPs,it incrementally learns the structure and the parameters of a RL problem usingsupervised learning techniques. Then, it integrates decision-theoric planningalgorithms based on FMDPs to compute its policy. SPITI is an instanciation ofSDYNA that exploits ITI, an incremental decision tree algorithm, to learn thereward function and the Dynamic Bayesian Networks with local structuresrepresenting the transition function of the problem. These representations areused by an incremental version of the Structured Value Iteration algorithm. Inorder to learn the structure, SPITI uses Chi-Square tests to detect theindependence between two probability distributions. Thus, we study the relationbetween the threshold used in the Chi-Square test, the size of the model builtand the relative error of the value function of the induced policy with respectto the optimal value. We show that, on stochastic problems, one can tune thethreshold so as to generate both a compact model and an efficient policy. Then,we show that SPITI, while keeping its model compact, uses the generalizationproperty of its learning method to perform better than a stochastic classicaltabular algorithm in large RL problem with an unknown structure. We alsointroduce a new measure based on Chi-Square to qualify the accuracy of themodel learned by SPITI. We qualitatively show that the generalization propertyin SPITI within the FMDP framework may prevent an exponential growth of thetime required to learn the structure of large stochastic RL problems.
arxiv-1206-6815 | Discriminative Learning via Semidefinite Probabilistic Models |  http://arxiv.org/abs/1206.6815  | author:Koby Crammer, Amir Globerson category:cs.LG stat.ML published:2012-06-27 summary:Discriminative linear models are a popular tool in machine learning. Thesecan be generally divided into two types: The first is linear classifiers, suchas support vector machines, which are well studied and provide state-of-the-artresults. One shortcoming of these models is that their output (known as the'margin') is not calibrated, and cannot be translated naturally into adistribution over the labels. Thus, it is difficult to incorporate such modelsas components of larger systems, unlike probabilistic based approaches. Thesecond type of approach constructs class conditional distributions using anonlinearity (e.g. log-linear models), but is occasionally worse in terms ofclassification error. We propose a supervised learning method which combinesthe best of both approaches. Specifically, our method provides a distributionover the labels, which is a linear function of the model parameters. As aconsequence, differences between probabilities are linear functions, a propertywhich most probabilistic models (e.g. log-linear) do not have. Our model assumes that classes correspond to linear subspaces (rather than tohalf spaces). Using a relaxed projection operator, we construct a measure whichevaluates the degree to which a given vector 'belongs' to a subspace, resultingin a distribution over labels. Interestingly, this view is closely related tosimilar concepts in quantum detection theory. The resulting models can betrained either to maximize the margin or to optimize average likelihoodmeasures. The corresponding optimization problems are semidefinite programswhich can be solved efficiently. We illustrate the performance of our algorithmon real world datasets, and show that it outperforms 2nd order kernel methods.
arxiv-1206-6814 | An Empirical Comparison of Algorithms for Aggregating Expert Predictions |  http://arxiv.org/abs/1206.6814  | author:Varsha Dani, Omid Madani, David M Pennock, Sumit Sanghai, Brian Galebach category:cs.AI cs.LG published:2012-06-27 summary:Predicting the outcomes of future events is a challenging problem for which avariety of solution methods have been explored and attempted. We present anempirical comparison of a variety of online and offline adaptive algorithms foraggregating experts' predictions of the outcomes of five years of US NationalFootball League games (1319 games) using expert probability elicitationsobtained from an Internet contest called ProbabilitySports. We find that it isdifficult to improve over simple averaging of the predictions in terms ofprediction accuracy, but that there is room for improvement in quadratic loss.Somewhat surprisingly, a Bayesian estimation algorithm which estimates thevariance of each expert's prediction exhibits the most consistent superiorperformance over simple averaging among our collection of algorithms.
arxiv-1206-6813 | A concentration theorem for projections |  http://arxiv.org/abs/1206.6813  | author:Sanjoy Dasgupta, Daniel Hsu, Nakul Verma category:cs.LG stat.ML published:2012-06-27 summary:X in R^D has mean zero and finite second moments. We show that there is aprecise sense in which almost all linear projections of X into R^d (for d < D)look like a scale-mixture of spherical Gaussians -- specifically, a mixture ofdistributions N(0, sigma^2 I_d) where the weight of the particular sigmacomponent is P ( X ^2 = sigma^2 D). The extent of this effect depends uponthe ratio of d to D, and upon a particular coefficient of eccentricity of X'sdistribution. We explore this result in a variety of experiments.
arxiv-1206-6262 | Scaling Life-long Off-policy Learning |  http://arxiv.org/abs/1206.6262  | author:Adam White, Joseph Modayil, Richard S. Sutton category:cs.AI cs.LG published:2012-06-27 summary:We pursue a life-long learning approach to artificial intelligence that makesextensive use of reinforcement learning algorithms. We build on our prior workwith general value functions (GVFs) and the Horde architecture. GVFs have beenshown able to represent a wide variety of facts about the world's dynamics thatmay be useful to a long-lived agent (Sutton et al. 2011). We have alsopreviously shown scaling - that thousands of on-policy GVFs can be learnedaccurately in real-time on a mobile robot (Modayil, White & Sutton 2011). Thatwork was limited in that it learned about only one policy at a time, whereasthe greatest potential benefits of life-long learning come from learning aboutmany policies in parallel, as we explore in this paper. Many new challengesarise in this off-policy learning setting. To deal with convergence andefficiency challenges, we utilize the recently introduced GTD({\lambda})algorithm. We show that GTD({\lambda}) with tile coding can simultaneouslylearn hundreds of predictions for five simple target policies while following asingle random behavior policy, assessing accuracy with interspersed on-policytests. To escape the need for the tests, which preclude further scaling, weintroduce and empirically vali- date two online estimators of the off-policyobjective (MSPBE). Finally, we use the more efficient of the two estimators todemonstrate off-policy learning at scale - the learning of value functions forone thousand policies in real time on a physical robot. This abilityconstitutes a significant step towards scaling life-long off-policy learning.
arxiv-1206-6196 | Discrete Elastic Inner Vector Spaces with Application in Time Series and Sequence Mining |  http://arxiv.org/abs/1206.6196  | author:Pierre-François Marteau, Nicolas Bonnel, Gilbas Ménier category:cs.LG cs.DB published:2012-06-27 summary:This paper proposes a framework dedicated to the construction of what we calldiscrete elastic inner product allowing one to embed sets of non-uniformlysampled multivariate time series or sequences of varying lengths into innerproduct space structures. This framework is based on a recursive definitionthat covers the case of multiple embedded time elastic dimensions. We provethat such inner products exist in our general framework and show how a simpleinstance of this inner product class operates on some prospective applications,while generalizing the Euclidean inner product. Classification experimentationson time series and symbolic sequences datasets demonstrate the benefits that wecan expect by embedding time series or sequences into elastic inner spacesrather than into classical Euclidean spaces. These experiments show goodaccuracy when compared to the euclidean distance or even dynamic programmingalgorithms while maintaining a linear algorithmic complexity at exploitationstage, although a quadratic indexing phase beforehand is required.
arxiv-1206-6030 | An Additive Model View to Sparse Gaussian Process Classifier Design |  http://arxiv.org/abs/1206.6030  | author:Sundararajan Sellamanickam, Shirish Shevade category:cs.LG stat.ML published:2012-06-26 summary:We consider the problem of designing a sparse Gaussian process classifier(SGPC) that generalizes well. Viewing SGPC design as constructing an additivemodel like in boosting, we present an efficient and effective SGPC designmethod to perform a stage-wise optimization of a predictive loss function. Weintroduce new methods for two key components viz., site parameter estimationand basis vector selection in any SGPC design. The proposed adaptive samplingbased basis vector selection method aids in achieving improved generalizationperformance at a reduced computational cost. This method can also be used inconjunction with any other site parameter estimation methods. It has similarcomputational and storage complexities as the well-known information vectormachine and is suitable for large datasets. The hyperparameters can bedetermined by optimizing a predictive loss function. The experimental resultsshow better generalization performance of the proposed basis vector selectionmethod on several benchmark datasets, particularly for relatively smaller basisvector set sizes or on difficult datasets.
arxiv-1206-6038 | Predictive Approaches For Gaussian Process Classifier Model Selection |  http://arxiv.org/abs/1206.6038  | author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG stat.ML published:2012-06-26 summary:In this paper we consider the problem of Gaussian process classifier (GPC)model selection with different Leave-One-Out (LOO) Cross Validation (CV) basedoptimization criteria and provide a practical algorithm using LOO predictivedistributions with such criteria to select hyperparameters. Apart from thestandard average negative logarithm of predictive probability (NLP), we alsoconsider smoothed versions of criteria such as F-measure and Weighted ErrorRate (WER), which are useful for handling imbalanced data. Unlike theregression case, LOO predictive distributions for the classifier case areintractable. We use approximate LOO predictive distributions arrived fromExpectation Propagation (EP) approximation. We conduct experiments on severalreal world benchmark datasets. When the NLP criterion is used for optimizingthe hyperparameters, the predictive approaches show better or comparable NLPgeneralization performance with existing GPC approaches. On the other hand,when the F-measure criterion is used, the F-measure generalization performanceimproves significantly on several datasets. Overall, the EP-based predictivealgorithm comes out as an excellent choice for GP classifier model selectionwith different optimization criteria.
arxiv-1206-6141 | Directed Time Series Regression for Control |  http://arxiv.org/abs/1206.6141  | author:Yi-Hao Kao, Benjamin Van Roy category:cs.LG cs.SY stat.ML published:2012-06-26 summary:We propose directed time series regression, a new approach to estimatingparameters of time-series models for use in certainty equivalent modelpredictive control. The approach combines merits of least squares regressionand empirical optimization. Through a computational study involving astochastic version of a well known inverted pendulum balancing problem, wedemonstrate that directed time series regression can generate significantimprovements in controller performance over either of the aforementionedalternatives.
arxiv-1206-6015 | Transductive Classification Methods for Mixed Graphs |  http://arxiv.org/abs/1206.6015  | author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG stat.ML published:2012-06-26 summary:In this paper we provide a principled approach to solve a transductiveclassification problem involving a similar graph (edges tend to connect nodeswith same labels) and a dissimilar graph (edges tend to connect nodes withopposing labels). Most of the existing methods, e.g., InformationRegularization (IR), Weighted vote Relational Neighbor classifier (WvRN) etc,assume that the given graph is only a similar graph. We extend the IR and WvRNmethods to deal with mixed graphs. We evaluate the proposed extensions onseveral benchmark datasets as well as two real world datasets and demonstratethe usefulness of our ideas.
arxiv-1206-5915 | Graph Based Classification Methods Using Inaccurate External Classifier Information |  http://arxiv.org/abs/1206.5915  | author:Sundararajan Sellamanickam, Sathiya Keerthi Selvaraj category:cs.LG published:2012-06-26 summary:In this paper we consider the problem of collectively classifying entitieswhere relational information is available across the entities. In practiceinaccurate class distribution for each entity is often available from another(external) classifier. For example this distribution could come from aclassifier built using content features or a simple dictionary. Given therelational and inaccurate external classifier information, we consider twograph based settings in which the problem of collective classification can besolved. In the first setting the class distribution is used to fix labels to asubset of nodes and the labels for the remaining nodes are obtained like in atransductive setting. In the other setting the class distributions of all nodesare used to define the fitting function part of a graph regularized objectivefunction. We define a generalized objective function that handles both thesettings. Methods like harmonic Gaussian field and local-global consistency(LGC) reported in the literature can be seen as special cases. We extend theLGC and weighted vote relational neighbor classification (WvRN) methods tosupport usage of external classifier information. We also propose an efficientleast squares regularization (LSR) based method and relate it to informationregularization methods. All the methods are evaluated on several benchmark andreal world datasets. Considering together speed, robustness and accuracy,experimental results indicate that the LSR and WvRN-extension methods performbetter than other methods.
arxiv-1206-5882 | Exact Recovery of Sparsely-Used Dictionaries |  http://arxiv.org/abs/1206.5882  | author:Daniel A. Spielman, Huan Wang, John Wright category:cs.LG cs.IT math.IT published:2012-06-26 summary:We consider the problem of learning sparsely used dictionaries with anarbitrary square dictionary and a random, sparse coefficient matrix. We provethat $O (n \log n)$ samples are sufficient to uniquely determine thecoefficient matrix. Based on this proof, we design a polynomial-time algorithm,called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove thatit probably recovers the dictionary and coefficient matrix when the coefficientmatrix is sufficiently sparse. Simulation results show that ER-SpUD reveals thetrue dictionary as well as the coefficients with probability higher than manystate-of-the-art algorithms.
arxiv-1206-5851 | A meta-analysis of state-of-the-art electoral prediction from Twitter data |  http://arxiv.org/abs/1206.5851  | author:Daniel Gayo-Avello category:cs.SI cs.CL cs.CY physics.soc-ph published:2012-06-25 summary:Electoral prediction from Twitter data is an appealing research topic. Itseems relatively straightforward and the prevailing view is overly optimistic.This is problematic because while simple approaches are assumed to be goodenough, core problems are not addressed. Thus, this paper aims to (1) provide abalanced and critical review of the state of the art; (2) cast light on thepresume predictive power of Twitter data; and (3) depict a roadmap to pushforward the field. Hence, a scheme to characterize Twitter prediction methodsis proposed. It covers every aspect from data collection to performanceevaluation, through data processing and vote inference. Using that scheme,prior research is analyzed and organized to explain the main approaches takenup to date but also their weaknesses. This is the first meta-analysis of thewhole body of research regarding electoral prediction from Twitter data. Itreveals that its presumed predictive power regarding electoral prediction hasbeen rather exaggerated: although social media may provide a glimpse onelectoral outcomes current research does not provide strong evidence to supportit can replace traditional polls. Finally, future lines of research along witha set of requirements they must fulfill are provided.
arxiv-1206-5559 | Speeding up the construction of slow adaptive walks |  http://arxiv.org/abs/1206.5559  | author:Susan Khor category:cs.NE published:2012-06-25 summary:An algorithm (bliss) is proposed to speed up the construction of slowadaptive walks. Slow adaptive walks are adaptive walks biased towards closerpoints or smaller move steps. They were previously introduced to explore asearch space, e.g. to detect potential local optima or to assess the ruggednessof a fitness landscape. To avoid the quadratic cost of computing Hammingdistance (HD) for all-pairs of strings in a set in order to find the set ofclosest strings for each string, strings are sorted and clustered by bliss suchthat similar strings are more likely to get paired off for HD computation. Toefficiently arrange the strings by similarity, bliss employs the idea of sharednon-overlapping position specific subsequences between strings which isinspired by an alignment-free protein sequence comparison algorithm. Tests areperformed to evaluate the quality of b-walks, i.e. slow adaptive walksconstructed from the output of bliss, on enumerated search spaces. Finally,b-walks are applied to explore larger search spaces with the help ofWang-Landau sampling.
arxiv-1206-5766 | Learning mixtures of spherical Gaussians: moment methods and spectral decompositions |  http://arxiv.org/abs/1206.5766  | author:Daniel Hsu, Sham M. Kakade category:cs.LG stat.ML published:2012-06-25 summary:This work provides a computationally efficient and statistically consistentmoment-based estimator for mixtures of spherical Gaussians. Under the conditionthat component means are in general position, a simple spectral decompositiontechnique yields consistent parameter estimates from low-order observablemoments, without additional minimum separation assumptions needed by previouscomputationally efficient estimation procedures. Thus computational andinformation-theoretic barriers to efficient estimation in mixture models areprecluded when the mixture components have means in general position andspherical covariances. Some connections are made to estimation problems relatedto independent component analysis.
arxiv-1206-5651 | Optimization of Real, Hermitian Quadratic Forms: Real, Complex Hopfield-Amari Neural Network |  http://arxiv.org/abs/1206.5651  | author:Garimella Ramamurthy, Bondalapati Nischal category:cs.NE published:2012-06-25 summary:In this research paper, the problem of optimization of quadratic formsassociated with the dynamics of Hopfield-Amari neural network is considered. Anelegant (and short) proof of the states at which local/global minima ofquadratic form are attained is provided. A theorem associated with local/globalminimization of quadratic energy function using the Hopfield-Amari neuralnetwork is discussed. The results are generalized to a "Complex Hopfield neuralnetwork" dynamics over the complex hypercube (using a "complex signumfunction"). It is also reasoned through two theorems that there is no loss ofgenerality in assuming the threshold vector to be a zero vector in the case ofreal as well as a "Complex Hopfield neural network". Some structured quadraticforms like Toeplitz form and Complex Toeplitz form are discussed.
arxiv-1206-5754 | Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox |  http://arxiv.org/abs/1206.5754  | author:Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari category:stat.ML cs.AI cs.MS published:2012-06-25 summary:Gaussian processes (GP) are powerful tools for probabilistic modelingpurposes. They can be used to define prior distributions over latent functionsin hierarchical Bayesian models. The prior over functions is defined implicitlyby the mean and covariance function, which determine the smoothness andvariability of the function. The inference can then be conducted directly inthe function space by evaluating or approximating the posterior process.Despite their attractive theoretical properties GPs provide practicalchallenges in their implementation. GPstuff is a versatile collection ofcomputational tools for GP models compatible with Linux and Windows MATLAB andOctave. It includes, among others, various inference methods, sparseapproximations and tools for model assessment. In this work, we review thesetools and demonstrate the use of GPstuff in several models.
arxiv-1206-5771 | The evolution of representation in simple cognitive networks |  http://arxiv.org/abs/1206.5771  | author:Lars Marstaller, Arend Hintze, Christoph Adami category:q-bio.NC cs.NE q-bio.PE published:2012-06-25 summary:Representations are internal models of the environment that can provideguidance to a behaving agent, even in the absence of sensory information. It isnot clear how representations are developed and whether or not they arenecessary or even essential for intelligent behavior. We argue here that theability to represent relevant features of the environment is the expectedconsequence of an adaptive process, give a formal definition of representationbased on information theory, and quantify it with a measure R. To measure how Rchanges over time, we evolve two types of networks---an artificial neuralnetwork and a network of hidden Markov gates---to solve a categorization taskusing a genetic algorithm. We find that the capacity to represent increasesduring evolutionary adaptation, and that agents form representations of theirenvironment during their lifetime. This ability allows the agents to act onsensorial inputs in the context of their acquired representations and enablescomplex and context-dependent behavior. We examine which concepts (features ofthe environment) our networks are representing, how the representations arelogically encoded in the networks, and how they form as an agent behaves tosolve a task. We conclude that R should be able to quantify the representationswithin any cognitive system, and should be predictive of an agent's long-termadaptive success.
arxiv-1206-5580 | A Geometric Algorithm for Scalable Multiple Kernel Learning |  http://arxiv.org/abs/1206.5580  | author:John Moeller, Parasaran Raman, Avishek Saha, Suresh Venkatasubramanian category:cs.LG stat.ML published:2012-06-25 summary:We present a geometric formulation of the Multiple Kernel Learning (MKL)problem. To do so, we reinterpret the problem of learning kernel weights assearching for a kernel that maximizes the minimum (kernel) distance between twoconvex polytopes. This interpretation combined with novel structural insightsfrom our geometric formulation allows us to reduce the MKL problem to a simpleoptimization routine that yields provable convergence as well as qualityguarantees. As a result our method scales efficiently to much larger data setsthan most prior methods can handle. Empirical evaluation on eleven datasetsshows that we are significantly faster and even compare favorably with auniform unweighted combination of kernels.
arxiv-1206-5533 | Practical recommendations for gradient-based training of deep architectures |  http://arxiv.org/abs/1206.5533  | author:Yoshua Bengio category:cs.LG published:2012-06-24 summary:Learning algorithms related to artificial neural networks and in particularfor Deep Learning may seem to involve many bells and whistles, calledhyper-parameters. This chapter is meant as a practical guide withrecommendations for some of the most commonly used hyper-parameters, inparticular in the context of learning algorithms based on back-propagatedgradient and gradient-based optimization. It also discusses how to deal withthe fact that more interesting results can be obtained when allowing one toadjust many hyper-parameters. Overall, it describes elements of the practiceused to successfully and efficiently train and debug large-scale and often deepmulti-layer neural networks. It closes with open questions about the trainingdifficulties observed with deeper architectures.
arxiv-1206-5538 | Representation Learning: A Review and New Perspectives |  http://arxiv.org/abs/1206.5538  | author:Yoshua Bengio, Aaron Courville, Pascal Vincent category:cs.LG published:2012-06-24 summary:The success of machine learning algorithms generally depends on datarepresentation, and we hypothesize that this is because differentrepresentations can entangle and hide more or less the different explanatoryfactors of variation behind the data. Although specific domain knowledge can beused to help design representations, learning with generic priors can also beused, and the quest for AI is motivating the design of more powerfulrepresentation-learning algorithms implementing such priors. This paper reviewsrecent work in the area of unsupervised feature learning and deep learning,covering advances in probabilistic models, auto-encoders, manifold learning,and deep networks. This motivates longer-term unanswered questions about theappropriate objectives for learning good representations, for computingrepresentations (i.e., inference), and the geometrical connections betweenrepresentation learning, density estimation and manifold learning.
arxiv-1206-5360 | Analysis of a Nature Inspired Firefly Algorithm based Back-propagation Neural Network Training |  http://arxiv.org/abs/1206.5360  | author:Sudarshan Nandy, Partha Pratim Sarkar, Achintya Das category:cs.AI cs.NE published:2012-06-23 summary:Optimization algorithms are normally influenced by meta-heuristic approach.In recent years several hybrid methods for optimization are developed to findout a better solution. The proposed work using meta-heuristic Nature Inspiredalgorithm is applied with back-propagation method to train a feed-forwardneural network. Firefly algorithm is a nature inspired meta-heuristicalgorithm, and it is incorporated into back-propagation algorithm to achievefast and improved convergence rate in training feed-forward neural network. Theproposed technique is tested over some standard data set. It is found thatproposed method produces an improved convergence within very few iteration.This performance is also analyzed and compared to genetic algorithm basedback-propagation. It is observed that proposed method consumes less time toconverge and providing improved convergence rate with minimum feed-forwardneural network design.
arxiv-1206-5384 | Keyphrase Based Arabic Summarizer (KPAS) |  http://arxiv.org/abs/1206.5384  | author:Tarek El-Shishtawy, Fatma El-Ghannam category:cs.CL cs.AI published:2012-06-23 summary:This paper describes a computationally inexpensive and efficient genericsummarization algorithm for Arabic texts. The algorithm belongs to extractivesummarization family, which reduces the problem into representative sentencesidentification and extraction sub-problems. Important keyphrases of thedocument to be summarized are identified employing combinations of statisticaland linguistic features. The sentence extraction algorithm exploits keyphrasesas the primary attributes to rank a sentence. The present experimental work,demonstrates different techniques for achieving various summarization goalsincluding: informative richness, coverage of both main and auxiliary topics,and keeping redundancy to a minimum. A scoring scheme is then adopted thatbalances between these summarization goals. To evaluate the resulted Arabicsummaries with well-established systems, aligned English/Arabic texts are usedthrough the experiments.
arxiv-1206-5345 | Dynamic Pricing under Finite Space Demand Uncertainty: A Multi-Armed Bandit with Dependent Arms |  http://arxiv.org/abs/1206.5345  | author:Pouya Tehrani, Yixuan Zhai, Qing Zhao category:cs.LG published:2012-06-23 summary:We consider a dynamic pricing problem under unknown demand models. In thisproblem a seller offers prices to a stream of customers and observes eithersuccess or failure in each sale attempt. The underlying demand model is unknownto the seller and can take one of N possible forms. In this paper, we show thatthis problem can be formulated as a multi-armed bandit with dependent arms. Wepropose a dynamic pricing policy based on the likelihood ratio test. We showthat the proposed policy achieves complete learning, i.e., it offers a boundedregret where regret is defined as the revenue loss with respect to the casewith a known demand model. This is in sharp contrast with the logarithmicgrowing regret in multi-armed bandit with independent arms.
arxiv-1206-5349 | Provable ICA with Unknown Gaussian Noise, and Implications for Gaussian Mixtures and Autoencoders |  http://arxiv.org/abs/1206.5349  | author:Sanjeev Arora, Rong Ge, Ankur Moitra, Sushant Sachdeva category:cs.LG cs.DS published:2012-06-23 summary:We present a new algorithm for Independent Component Analysis (ICA) which hasprovable performance guarantees. In particular, suppose we are given samples ofthe form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ isa random variable whose components are independent and have a fourth momentstrictly less than that of a standard Gaussian random variable and $\eta$ is an$n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: Wegive an algorithm that provable recovers $A$ and $\Sigma$ up to an additive$\epsilon$ and whose running time and sample complexity are polynomial in $n$and $1 / \epsilon$. To accomplish this, we introduce a novel "quasi-whitening"step that may be useful in other contexts in which the covariance of Gaussiannoise is not known in advance. We also give a general framework for finding alllocal optima of a function (given an oracle for approximately finding just one)and this is a crucial step in our algorithm, one that has been overlooked inprevious attempts, and allows us to control the accumulation of error when wefind the columns of $A$ one by one via local search.
arxiv-1206-5065 | A generic framework for video understanding applied to group behavior recognition |  http://arxiv.org/abs/1206.5065  | author:Sofia Zaidenberg, Bernard Boulay, François Bremond category:cs.CV published:2012-06-22 summary:This paper presents an approach to detect and track groups of people invideo-surveillance applications, and to automatically recognize their behavior.This method keeps track of individuals moving together by maintaining a spacialand temporal group coherence. First, people are individually detected andtracked. Second, their trajectories are analyzed over a temporal window andclustered using the Mean-Shift algorithm. A coherence value describes how wella set of people can be described as a group. Furthermore, we propose a formalevent description language. The group events recognition approach issuccessfully validated on 4 camera views from 3 datasets: an airport, a subway,a shopping center corridor and an entrance hall.
arxiv-1206-5102 | Hidden Markov Models with mixtures as emission distributions |  http://arxiv.org/abs/1206.5102  | author:Stevenn Volant, Caroline Bérard, Marie-Laure Martin-Magniette, Stéphane Robin category:stat.ML cs.LG stat.CO published:2012-06-22 summary:In unsupervised classification, Hidden Markov Models (HMM) are used toaccount for a neighborhood structure between observations. The emissiondistributions are often supposed to belong to some parametric family. In thispaper, a semiparametric modeling where the emission distributions are a mixtureof parametric distributions is proposed to get a higher flexibility. We showthat the classical EM algorithm can be adapted to infer the model parameters.For the initialisation step, starting from a large number of components, ahierarchical method to combine them into the hidden states is proposed. Threelikelihood-based criteria to select the components to be combined arediscussed. To estimate the number of hidden states, BIC-like criteria arederived. A simulation study is carried out both to determine the bestcombination between the merging criteria and the model selection criteria andto evaluate the accuracy of classification. The proposed method is alsoillustrated using a biological dataset from the model plant Arabidopsisthaliana. A R package HMMmix is freely available on the CRAN.
arxiv-1206-5157 | Leaf vein segmentation using Odd Gabor filters and morphological operations |  http://arxiv.org/abs/1206.5157  | author:Vini Katyal, Aviral category:cs.CV cs.AI published:2012-06-22 summary:Leaf vein forms the basis of leaf characterization and classification.Different species have different leaf vein patterns. It is seen that leaf veinsegmentation will help in maintaining a record of all the leaves according totheir specific pattern of veins thus provide an effective way to retrieve andstore information regarding various plant species in database as well asprovide an effective means to characterize plants on the basis of leaf veinstructure which is unique for every species. The algorithm proposes a new wayof segmentation of leaf veins with the use of Odd Gabor filters and the use ofmorphological operations for producing a better output. The Odd Gabor filtergives an efficient output and is robust and scalable as compared with theexisting techniques as it detects the fine fiber like veins present in leavesmuch more efficiently.
arxiv-1206-5057 | The Robustness and Super-Robustness of L^p Estimation, when p < 1 |  http://arxiv.org/abs/1206.5057  | author:Qinghuai Gao category:cs.LG math.ST stat.TH published:2012-06-22 summary:In robust statistics, the breakdown point of an estimator is the percentageof outliers with which an estimator still generates reliable estimation. Theupper bound of breakdown point is 50%, which means it is not possible togenerate reliable estimation with more than half outliers. In this paper, it is shown that for majority of experiences, when theoutliers exceed 50%, but if they are distributed randomly enough, it is stillpossible to generate a reliable estimation from minority good observations. Thephenomenal of that the breakdown point is larger than 50% is named as superrobustness. And, in this paper, a robust estimator is called strict robust ifit generates a perfect estimation when all the good observations are perfect. More specifically, the super robustness of the maximum likelihood estimatorof the exponential power distribution, or L^p estimation, where p<1, isinvestigated. This paper starts with proving that L^p (p<1) is a strict robustlocation estimator. Further, it is proved that L^p (p < 1)has the property ofstrict super-robustness on translation, rotation, scaling transformation androbustness on Euclidean transform.
arxiv-1206-5333 | TempEval-3: Evaluating Events, Time Expressions, and Temporal Relations |  http://arxiv.org/abs/1206.5333  | author:Naushad UzZaman, Hector Llorens, James Allen, Leon Derczynski, Marc Verhagen, James Pustejovsky category:cs.CL published:2012-06-22 summary:We describe the TempEval-3 task which is currently in preparation for theSemEval-2013 evaluation exercise. The aim of TempEval is to advance research ontemporal information processing. TempEval-3 follows on from previous TempEvalevents, incorporating: a three-part task structure covering event, temporalexpression and temporal relation extraction; a larger dataset; and singleoverall task quality scores.
arxiv-1206-5036 | Estimating Densities with Non-Parametric Exponential Families |  http://arxiv.org/abs/1206.5036  | author:Lin Yuan, Sergey Kirshner, Robert Givan category:stat.ML cs.LG published:2012-06-22 summary:We propose a novel approach for density estimation with exponential familiesfor the case when the true density may not fall within the chosen family. Ourapproach augments the sufficient statistics with features designed toaccumulate probability mass in the neighborhood of the observed points,resulting in a non-parametric model similar to kernel density estimators. Weshow that under mild conditions, the resulting model uses only the sufficientstatistics if the density is within the chosen exponential family, andasymptotically, it approximates densities outside of the chosen exponentialfamily. Using the proposed approach, we modify the exponential random graphmodel, commonly used for modeling small-size graph distributions, to addressthe well-known issue of model degeneracy.
arxiv-1206-5162 | Fast Variational Inference in the Conjugate Exponential Family |  http://arxiv.org/abs/1206.5162  | author:James Hensman, Magnus Rattray, Neil D. Lawrence category:cs.LG stat.ML published:2012-06-22 summary:We present a general method for deriving collapsed variational inferencealgo- rithms for probabilistic models in the conjugate exponential family. Ourmethod unifies many existing approaches to collapsed variational inference. Ourcollapsed variational inference leads to a new lower bound on the marginallikelihood. We exploit the information geometry of the bound to derive muchfaster optimization methods based on conjugate gradients for these models. Ourapproach is very general and is easily applied to any model where the meanfield update equations have been derived. Empirically we show significantspeed-ups for probabilistic models optimized using our bound.
arxiv-1206-4968 | Convergence of the Continuous Time Trajectories of Isotropic Evolution Strategies on Monotonic C^2-composite Functions |  http://arxiv.org/abs/1206.4968  | author:Youhei Akimoto, Anne Auger, Nikolaus Hansen category:cs.NE math.OC published:2012-06-21 summary:The Information-Geometric Optimization (IGO) has been introduced as a unifiedframework for stochastic search algorithms. Given a parametrized family ofprobability distributions on the search space, the IGO turns an arbitraryoptimization problem on the search space into an optimization problem on theparameter space of the probability distribution family and defines a naturalgradient ascent on this space. From the natural gradients defined over theentire parameter space we obtain continuous time trajectories which are thesolutions of an ordinary differential equation (ODE). Via discretization, theIGO naturally defines an iterated gradient ascent algorithm. Depending on thechosen distribution family, the IGO recovers several known algorithms such asthe pure rank-\mu update CMA-ES. Consequently, the continuous timeIGO-trajectory can be viewed as an idealization of the original algorithm. Inthis paper we study the continuous time trajectories of the IGO given thefamily of isotropic Gaussian distributions. These trajectories are adeterministic continuous time model of the underlying evolution strategy in thelimit for population size to infinity and change rates to zero. On functionsthat are the composite of a monotone and a convex-quadratic function, we provethe global convergence of the solution of the ODE towards the global optimum.We extend this result to composites of monotone and twice continuouslydifferentiable functions and prove local convergence towards local optima.
arxiv-1206-4866 | Portraits of Julius Caesar: a proposal for 3D analysis |  http://arxiv.org/abs/1206.4866  | author:Amelia Carolina Sparavigna category:cs.CV published:2012-06-21 summary:Here I suggest the use of a 3D scanning and rendering to create some virtualcopies of ancient artifacts to study and compare them. In particular, thisapproach could be interesting for some roman marble busts, two of which areportraits of Julius Caesar, and the third is a realistic portrait of a manrecently found at Arles, France. The comparison of some images indicates that athree-dimensional visualization is necessary.
arxiv-1206-4958 | A Pointillism Approach for Natural Language Processing of Social Media |  http://arxiv.org/abs/1206.4958  | author:Peiyou Song, Anhei Shu, Anyu Zhou, Dan Wallach, Jedidiah R. Crandall category:cs.IR cs.CL cs.SI published:2012-06-21 summary:The Chinese language poses challenges for natural language processing basedon the unit of a word even for formal uses of the Chinese language, socialmedia only makes word segmentation in Chinese even more difficult. In thisdocument we propose a pointillism approach to natural language processing.Rather than words that have individual meanings, the basic unit of apointillism approach is trigrams of characters. These grams take on meaning inaggregate when they appear together in a way that is correlated over time. Our results from three kinds of experiments show that when words and topicsdo have a meme-like trend, they can be reconstructed from only trigrams. Forexample, for 4-character idioms that appear at least 99 times in one day in ourdata, the unconstrained precision (that is, precision that allows for deviationfrom a lexicon when the result is just as correct as the lexicon version of theword or phrase) is 0.93. For longer words and phrases collected fromWiktionary, including neologisms, the unconstrained precision is 0.87. Weconsider these results to be very promising, because they suggest that it isfeasible for a machine to reconstruct complex idioms, phrases, and neologismswith good precision without any notion of words. Thus the colorful and baroqueuses of language that typify social media in challenging languages such asChinese may in fact be accessible to machines.
arxiv-1206-4832 | Smoothed Functional Algorithms for Stochastic Optimization using q-Gaussian Distributions |  http://arxiv.org/abs/1206.4832  | author:Debarghya Ghoshdastidar, Ambedkar Dukkipati, Shalabh Bhatnagar category:cs.IT cs.LG math.IT stat.ME G.1.6; I.6.8 published:2012-06-21 summary:Smoothed functional (SF) schemes for gradient estimation are known to beefficient in stochastic optimization algorithms, specially when the objectiveis to improve the performance of a stochastic system. However, the performanceof these methods depends on several parameters, such as the choice of asuitable smoothing kernel. Different kernels have been studied in literature,which include Gaussian, Cauchy and uniform distributions among others. Thispaper studies a new class of kernels based on the q-Gaussian distribution, thathas gained popularity in statistical physics over the last decade. Though theimportance of this family of distributions is attributed to its ability togeneralize the Gaussian distribution, we observe that this class encompassesalmost all existing smoothing kernels. This motivates us to study SF schemesfor gradient estimation using the q-Gaussian distribution. Using the derivedgradient estimates, we propose two-timescale algorithms for optimization of astochastic objective function in a constrained setting with projected gradientsearch approach. We prove the convergence of our algorithms to the set ofstationary points of an associated ODE. We also demonstrate their performancenumerically through simulations on a queuing model.
arxiv-1206-4812 | A biological gradient descent for prediction through a combination of STDP and homeostatic plasticity |  http://arxiv.org/abs/1206.4812  | author:Mathieu Galtier, Gilles Wainrib category:q-bio.NC cs.NE math.DS published:2012-06-21 summary:Identifying, formalizing and combining biological mechanisms which implementknown brain functions, such as prediction, is a main aspect of current researchin theoretical neuroscience. In this letter, the mechanisms of Spike TimingDependent Plasticity (STDP) and homeostatic plasticity, combined in an originalmathematical formalism, are shown to shape recurrent neural networks intopredictors. Following a rigorous mathematical treatment, we prove that theyimplement the online gradient descent of a distance between the networkactivity and its stimuli. The convergence to an equilibrium, where the networkcan spontaneously reproduce or predict its stimuli, does not suffer frombifurcation issues usually encountered in learning in recurrent neuralnetworks.
arxiv-1206-4822 | Feature extraction in protein sequences classification : a new stability measure |  http://arxiv.org/abs/1206.4822  | author:Rabie Saidi, Sabeur Aridhi, Mondher Maddouri, Engelbert Mephu Nguifo category:cs.LG cs.CE q-bio.QM published:2012-06-21 summary:Feature extraction is an unavoidable task, especially in the critical step ofpreprocessing biological sequences. This step consists for example intransforming the biological sequences into vectors of motifs where each motifis a subsequence that can be seen as a property (or attribute) characterizingthe sequence. Hence, we obtain an object-property table where objects aresequences and properties are motifs extracted from sequences. This output canbe used to apply standard machine learning tools to perform data mining taskssuch as classification. Several previous works have described featureextraction methods for bio-sequence classification, but none of them discussedthe robustness of these methods when perturbing the input data. In this work,we introduce the notion of stability of the generated motifs in order to studythe robustness of motif extraction methods. We express this robustness in termsof the ability of the method to reveal any change occurring in the input dataand also its ability to target the interesting motifs. We use these criteria toevaluate and experimentally compare four existing extraction methods forbiological sequences.
arxiv-1206-5277 | Accuracy Bounds for Belief Propagation |  http://arxiv.org/abs/1206.5277  | author:Alexander T. Ihler category:cs.AI cs.LG stat.ML published:2012-06-20 summary:The belief propagation (BP) algorithm is widely applied to performapproximate inference on arbitrary graphical models, in part due to itsexcellent empirical properties and performance. However, little is knowntheoretically about when this algorithm will perform well. Using recentanalysis of convergence and stability properties in BP and new results onapproximations in binary systems, we derive a bound on the error in BP'sestimates for pairwise Markov random fields over discrete valued randomvariables. Our bound is relatively simple to compute, and compares favorablywith a previous method of bounding the accuracy of BP.
arxiv-1206-4522 | BADREX: In situ expansion and coreference of biomedical abbreviations using dynamic regular expressions |  http://arxiv.org/abs/1206.4522  | author:Phil Gooch category:cs.CL published:2012-06-20 summary:BADREX uses dynamically generated regular expressions to annotate termdefinition-term abbreviation pairs, and corefers unpaired acronyms andabbreviations back to their initial definition in the text. Against theMedstract corpus BADREX achieves precision and recall of 98% and 97%, andagainst a much larger corpus, 90% and 85%, respectively. BADREX yields improvedperformance over previous approaches, requires no training data and allowsruntime customisation of its input parameters. BADREX is freely available fromhttps://github.com/philgooch/BADREX-Biomedical-Abbreviation-Expander as aplugin for the General Architecture for Text Engineering (GATE) framework andis licensed under the GPLv3.
arxiv-1206-5274 | On Discarding, Caching, and Recalling Samples in Active Learning |  http://arxiv.org/abs/1206.5274  | author:Ashish Kapoor, Eric J. Horvitz category:cs.LG stat.ML published:2012-06-20 summary:We address challenges of active learning under scarce informational resourcesin non-stationary environments. In real-world settings, data labeled andintegrated into a predictive model may become invalid over time. However, thedata can become informative again with switches in context and such changes mayindicate unmodeled cyclic or other temporal dynamics. We explore principles fordiscarding, caching, and recalling labeled data points in active learning basedon computations of value of information. We review key concepts and study thevalue of the methods via investigations of predictive performance and costs ofacquiring data for simulated and real-world data sets.
arxiv-1206-5270 | Nonparametric Bayes Pachinko Allocation |  http://arxiv.org/abs/1206.5270  | author:Wei Li, David Blei, Andrew McCallum category:cs.IR cs.LG stat.ML published:2012-06-20 summary:Recent advances in topic models have explored complicated structureddistributions to represent topic correlation. For example, the pachinkoallocation model (PAM) captures arbitrary, nested, and possibly sparsecorrelations between topics using a directed acyclic graph (DAG). While PAMprovides more flexibility and greater expressive power than previous modelslike latent Dirichlet allocation (LDA), it is also more difficult to determinethe appropriate topic structure for a specific dataset. In this paper, wepropose a nonparametric Bayesian prior for PAM based on a variant of thehierarchical Dirichlet process (HDP). Although the HDP can capture topiccorrelations defined by nested data structure, it does not automaticallydiscover such correlations from unstructured data. By assuming an HDP-basedprior for PAM, we are able to learn both the number of topics and how thetopics are correlated. We evaluate our model on synthetic and real-world textdatasets, and show that nonparametric PAM achieves performance matching thebest of PAM without manually tuning the number of topics.
arxiv-1206-5267 | Collaborative Filtering and the Missing at Random Assumption |  http://arxiv.org/abs/1206.5267  | author:Benjamin Marlin, Richard S. Zemel, Sam Roweis, Malcolm Slaney category:cs.LG cs.IR stat.ML published:2012-06-20 summary:Rating prediction is an important application, and a popular research topicin collaborative filtering. However, both the validity of learning algorithms,and the validity of standard testing procedures rest on the assumption thatmissing ratings are missing at random (MAR). In this paper we present theresults of a user study in which we collect a random sample of ratings fromcurrent users of an online radio service. An analysis of the rating datacollected in the study shows that the sample of random ratings has markedlydifferent properties than ratings of user-selected songs. When asked to reporton their own rating behaviour, a large number of users indicate they believetheir opinion of a song does affect whether they choose to rate that song, aviolation of the MAR condition. Finally, we present experimental resultsshowing that incorporating an explicit model of the missing data mechanism canlead to significant improvements in prediction performance on the random sampleof ratings.
arxiv-1206-5265 | Consensus ranking under the exponential model |  http://arxiv.org/abs/1206.5265  | author:Marina Meila, Kapil Phadnis, Arthur Patterson, Jeff A. Bilmes category:cs.LG cs.AI stat.ML published:2012-06-20 summary:We analyze the generalized Mallows model, a popular exponential model overrankings. Estimating the central (or consensus) ranking from data is NP-hard.We obtain the following new results: (1) We show that search methods canestimate both the central ranking pi0 and the model parameters theta exactly.The search is n! in the worst case, but is tractable when the true distributionis concentrated around its mode; (2) We show that the generalized Mallows modelis jointly exponential in (pi0; theta), and introduce the conjugate prior forthis model class; (3) The sufficient statistics are the pairwise marginalprobabilities that item i is preferred to item j. Preliminary experimentsconfirm the theoretical predictions and compare the new algorithm and existingheuristics.
arxiv-1206-5264 | Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods |  http://arxiv.org/abs/1206.5264  | author:Gergely Neu, Csaba Szepesvari category:cs.LG stat.ML published:2012-06-20 summary:In this paper we propose a novel gradient algorithm to learn a policy from anexpert's observed behavior assuming that the expert behaves optimally withrespect to some unknown reward function of a Markovian Decision Problem. Thealgorithm's aim is to find a reward function such that the resulting optimalpolicy matches well the expert's observed behavior. The main difficulty is thatthe mapping from the parameters to policies is both nonsmooth and highlyredundant. Resorting to subdifferentials solves the first difficulty, while thesecond one is over- come by computing natural gradients. We tested the proposedmethod in two artificial domains and found it to be more reliable and efficientthan some previous methods.
arxiv-1206-5263 | Reading Dependencies from Polytree-Like Bayesian Networks |  http://arxiv.org/abs/1206.5263  | author:Jose M. Pena category:cs.AI cs.LG stat.ML published:2012-06-20 summary:We present a graphical criterion for reading dependencies from the minimaldirected independence map G of a graphoid p when G is a polytree and psatisfies composition and weak transitivity. We prove that the criterion issound and complete. We argue that assuming composition and weak transitivity isnot too restrictive.
arxiv-1206-5261 | Mixture-of-Parents Maximum Entropy Markov Models |  http://arxiv.org/abs/1206.5261  | author:David S. Rosenberg, Dan Klein, Ben Taskar category:cs.LG cs.AI stat.ML published:2012-06-20 summary:We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), aclass of directed graphical models extending MEMMs. The MoP-MEMM allowstractable incorporation of long-range dependencies between nodes by restrictingthe conditional distribution of each node to be a mixture of distributionsgiven the parents. We show how to efficiently compute the exact marginalposterior node distributions, regardless of the range of the dependencies. Thisenables us to model non-sequential correlations present within text documents,as well as between interconnected documents, such as hyperlinked web pages. Weapply the MoP-MEMM to a named entity recognition task and a web pageclassification task. In each, our model shows significant improvement over thebasic MEMM, and is competitive with other long-range sequence models that useapproximate inference.
arxiv-1206-5256 | Discovering Patterns in Biological Sequences by Optimal Segmentation |  http://arxiv.org/abs/1206.5256  | author:Joseph Bockhorst, Nebojsa Jojic category:cs.CE cs.LG q-bio.QM stat.AP published:2012-06-20 summary:Computational methods for discovering patterns of local correlations insequences are important in computational biology. Here we show how to determinethe optimal partitioning of aligned sequences into non-overlapping segmentssuch that positions in the same segment are strongly correlated while positionsin different segments are not. Our approach involves discovering the hiddenvariables of a Bayesian network that interact with observed sequences so as toform a set of independent mixture models. We introduce a dynamic program toefficiently discover the optimal segmentation, or equivalently the optimal setof hidden variables. We evaluate our approach on two computational biologytasks. One task is related to the design of vaccines against polymorphicpathogens and the other task involves analysis of single nucleotidepolymorphisms (SNPs) in human DNA. We show how common tasks in these problemsnaturally correspond to inference procedures in the learned models. Error ratesof our learned models for the prediction of missing SNPs are up to 1/3 lessthan the error rates of a state-of-the-art SNP prediction method. Source codeis available at www.uwm.edu/~joebock/segmentation.
arxiv-1206-5248 | Statistical Translation, Heat Kernels and Expected Distances |  http://arxiv.org/abs/1206.5248  | author:Joshua Dillon, Yi Mao, Guy Lebanon, Jian Zhang category:cs.LG cs.CV cs.IR stat.ML published:2012-06-20 summary:High dimensional structured data such as text and images is often poorlyunderstood and misrepresented in statistical modeling. The standard histogramrepresentation suffers from high variance and performs poorly in general. Weexplore novel connections between statistical translation, heat kernels onmanifolds and graphs, and expected distances. These connections provide a newframework for unsupervised metric learning for text documents. Experimentsindicate that the resulting distances are generally superior to their morestandard counterparts.
arxiv-1206-5247 | Bayesian structure learning using dynamic programming and MCMC |  http://arxiv.org/abs/1206.5247  | author:Daniel Eaton, Kevin Murphy category:cs.LG stat.ML published:2012-06-20 summary:MCMC methods for sampling from the space of DAGs can mix poorly due to thelocal nature of the proposals that are commonly used. It has been shown thatsampling from the space of node orders yields better results [FK03, EW06].Recently, Koivisto and Sood showed how one can analytically marginalize overorders using dynamic programming (DP) [KS04, Koi06]. Their method computes theexact marginal posterior edge probabilities, thus avoiding the need for MCMC.Unfortunately, there are four drawbacks to the DP technique: it can only usemodular priors, it can only compute posteriors over modular features, it isdifficult to compute a predictive density, and it takes exponential time andspace. We show how to overcome the first three of these problems by using theDP algorithm as a proposal distribution for MCMC in DAG space. We show thatthis hybrid technique converges to the posterior faster than other methods,resulting in more accurate structure learning and higher predictive likelihoodson test data.
arxiv-1206-5245 | A new parameter Learning Method for Bayesian Networks with Qualitative Influences |  http://arxiv.org/abs/1206.5245  | author:Ad Feelders category:cs.AI cs.LG stat.ME published:2012-06-20 summary:We propose a new method for parameter learning in Bayesian networks withqualitative influences. This method extends our previous work from networks ofbinary variables to networks of discrete variables with ordered values. Thespecified qualitative influences correspond to certain order restrictions onthe parameters in the network. These parameters may therefore be estimatedusing constrained maximum likelihood estimation. We propose an alternativemethod, based on the isotonic regression. The constrained maximum likelihoodestimates are fairly complicated to compute, whereas computation of theisotonic regression estimates only requires the repeated application of thePool Adjacent Violators algorithm for linear orders. Therefore, the isotonicregression estimator is to be preferred from the viewpoint of computationalcomplexity. Through experiments on simulated and real data, we show that thenew learning method is competitive in performance to the constrained maximumlikelihood estimator, and that both estimators improve on the standardestimator.
arxiv-1206-5243 | Convergent Propagation Algorithms via Oriented Trees |  http://arxiv.org/abs/1206.5243  | author:Amir Globerson, Tommi S. Jaakkola category:cs.LG stat.ML published:2012-06-20 summary:Inference problems in graphical models are often approximated by casting themas constrained optimization problems. Message passing algorithms, such asbelief propagation, have previously been suggested as methods for solving theseoptimization problems. However, there are few convergence guarantees for suchalgorithms, and the algorithms are therefore not guaranteed to solve thecorresponding optimization problem. Here we present an oriented treedecomposition algorithm that is guaranteed to converge to the global optimum ofthe Tree-Reweighted (TRW) variational problem. Our algorithm performs localupdates in the convex dual of the TRW problem - an unconstrained generalizedgeometric program. Primal updates, also local, correspond to orientedreparametrization operations that leave the distribution intact.
arxiv-1206-5241 | Shift-Invariance Sparse Coding for Audio Classification |  http://arxiv.org/abs/1206.5241  | author:Roger Grosse, Rajat Raina, Helen Kwong, Andrew Y. Ng category:cs.LG stat.ML published:2012-06-20 summary:Sparse coding is an unsupervised learning algorithm that learns a succincthigh-level representation of the inputs given only unlabeled data; itrepresents each input as a sparse linear combination of a set of basisfunctions. Originally applied to modeling the human visual cortex, sparsecoding has also been shown to be useful for self-taught learning, in which thegoal is to solve a supervised classification task given access to additionalunlabeled data drawn from different classes than that in the supervisedlearning problem. Shift-invariant sparse coding (SISC) is an extension ofsparse coding which reconstructs a (usually time-series) input using all of thebasis functions in all possible shifts. In this paper, we present an efficientalgorithm for learning SISC bases. Our method is based on iteratively solvingtwo large convex optimization problems: The first, which computes the linearcoefficients, is an L1-regularized linear least squares problem withpotentially hundreds of thousands of variables. Existing methods typically usea heuristic to select a small subset of the variables to optimize, but wepresent a way to efficiently compute the exact solution. The second, whichsolves for bases, is a constrained linear least squares problem. By optimizingover complex-valued variables in the Fourier domain, we reduce the couplingbetween the different variables, allowing the problem to be solved efficiently.We show that SISC's learned high-level representations of speech and musicprovide useful features for classification tasks within those domains. Whenapplied to classification, under certain conditions the learned featuresoutperform state of the art spectral and cepstral features.
arxiv-1206-5240 | Analysis of Semi-Supervised Learning with the Yarowsky Algorithm |  http://arxiv.org/abs/1206.5240  | author:Gholam Reza Haffari, Anoop Sarkar category:cs.LG stat.ML published:2012-06-20 summary:The Yarowsky algorithm is a rule-based semi-supervised learning algorithmthat has been successfully applied to some problems in computationallinguistics. The algorithm was not mathematically well understood until (Abney2004) which analyzed some specific variants of the algorithm, and also proposedsome new algorithms for bootstrapping. In this paper, we extend Abney's workand show that some of his proposed algorithms actually optimize (an upper-boundon) an objective function based on a new definition of cross-entropy which isbased on a particular instantiation of the Bregman distance between probabilitydistributions. Moreover, we suggest some new algorithms for rule-basedsemi-supervised learning and show connections with harmonic functions andminimum multi-way cuts in graph-based semi-supervised learning.
arxiv-1206-4391 | Gray Image extraction using Fuzzy Logic |  http://arxiv.org/abs/1206.4391  | author:Koushik Mondal, Paramartha Dutta, Siddhartha Bhattacharyya category:cs.CV cs.AI published:2012-06-20 summary:Fuzzy systems concern fundamental methodology to represent and processuncertainty and imprecision in the linguistic information. The fuzzy systemsthat use fuzzy rules to represent the domain knowledge of the problem are knownas Fuzzy Rule Base Systems (FRBS). On the other hand image segmentation andsubsequent extraction from a noise-affected background, with the help ofvarious soft computing methods, are relatively new and quite popular due tovarious reasons. These methods include various Artificial Neural Network (ANN)models (primarily supervised in nature), Genetic Algorithm (GA) basedtechniques, intensity histogram based methods etc. providing an extractionsolution working in unsupervised mode happens to be even more interestingproblem. Literature suggests that effort in this respect appears to be quiterudimentary. In the present article, we propose a fuzzy rule guided noveltechnique that is functional devoid of any external intervention duringexecution. Experimental results suggest that this approach is an efficient onein comparison to different other techniques extensively addressed inliterature. In order to justify the supremacy of performance of our proposedtechnique in respect of its competitors, we take recourse to effective metricslike Mean Squared Error (MSE), Mean Absolute Error (MAE), Peak Signal to NoiseRatio (PSNR).
arxiv-1206-5278 | Fast Nonparametric Conditional Density Estimation |  http://arxiv.org/abs/1206.5278  | author:Michael P. Holmes, Alexander G. Gray, Charles Lee Isbell category:stat.ME cs.LG stat.ML published:2012-06-20 summary:Conditional density estimation generalizes regression by modeling a fulldensity f(yjx) rather than only the expected value E(yjx). This is importantfor many tasks, including handling multi-modality and generating predictionintervals. Though fundamental and widely applicable, nonparametric conditionaldensity estimators have received relatively little attention from statisticiansand little or none from the machine learning community. None of that work hasbeen applied to greater than bivariate data, presumably due to thecomputational difficulty of data-driven bandwidth selection. We describe thedouble kernel conditional density estimator and derive fast dual-tree-basedalgorithms for bandwidth selection using a maximum likelihood criterion. Thesetechniques give speedups of up to 3.8 million in our experiments, and enablethe first applications to previously intractable large multivariate datasets,including a redshift prediction problem from the Sloan Digital Sky Survey.
arxiv-1206-5281 | Learning Selectively Conditioned Forest Structures with Applications to DBNs and Classification |  http://arxiv.org/abs/1206.5281  | author:Brian D. Ziebart, Anind K. Dey, J Andrew Bagnell category:cs.LG stat.ML published:2012-06-20 summary:Dealing with uncertainty in Bayesian Network structures using maximum aposteriori (MAP) estimation or Bayesian Model Averaging (BMA) is oftenintractable due to the superexponential number of possible directed, acyclicgraphs. When the prior is decomposable, two classes of graphs where efficientlearning can take place are tree structures, and fixed-orderings with limitedin-degree. We show how MAP estimates and BMA for selectively conditionedforests (SCF), a combination of these two classes, can be computed efficientlyfor ordered sets of variables. We apply SCFs to temporal data to learn DynamicBayesian Networks having an intra-timestep forest and inter-timestep limitedin-degree structure, improving model accuracy over DBNs without the combinationof structures. We also apply SCFs to Bayes Net classification to learnselective forest augmented Naive Bayes classifiers. We argue that the built-infeature selection of selective augmented Bayes classifiers makes thempreferable to similar non-selective classifiers based on empirical evidence.
arxiv-1206-5282 | A Characterization of Markov Equivalence Classes for Directed Acyclic Graphs with Latent Variables |  http://arxiv.org/abs/1206.5282  | author:Jiji Zhang category:stat.ME cs.LG stat.ML published:2012-06-20 summary:Different directed acyclic graphs (DAGs) may be Markov equivalent in thesense that they entail the same conditional independence relations among theobserved variables. Meek (1995) characterizes Markov equivalence classes forDAGs (with no latent variables) by presenting a set of orientation rules thatcan correctly identify all arrow orientations shared by all DAGs in a Markovequivalence class, given a member of that class. For DAG models with latentvariables, maximal ancestral graphs (MAGs) provide a neat representation thatfacilitates model search. Earlier work (Ali et al. 2005) has identified a setof orientation rules sufficient to construct all arrowheads common to a Markovequivalence class of MAGs. In this paper, we provide extra rules sufficient toconstruct all common tails as well. We end up with a set of orientation rulessound and complete for identifying commonalities across a Markov equivalenceclass of MAGs, which is particularly useful for causal inference.
arxiv-1206-5283 | Bayesian Active Distance Metric Learning |  http://arxiv.org/abs/1206.5283  | author:Liu Yang, Rong Jin, Rahul Sukthankar category:cs.LG stat.ML published:2012-06-20 summary:Distance metric learning is an important component for many tasks, such asstatistical classification and content-based image retrieval. Existingapproaches for learning distance metrics from pairwise constraints typicallysuffer from two major problems. First, most algorithms only offer pointestimation of the distance metric and can therefore be unreliable when thenumber of training examples is small. Second, since these algorithms generallyselect their training examples at random, they can be inefficient if labelingeffort is limited. This paper presents a Bayesian framework for distance metriclearning that estimates a posterior distribution for the distance metric fromlabeled pairwise constraints. We describe an efficient algorithm based on thevariational method for the proposed Bayesian approach. Furthermore, we applythe proposed Bayesian framework to active distance metric learning by selectingthose unlabeled example pairs with the greatest uncertainty in relativedistance. Experiments in classification demonstrate that the proposed frameworkachieves higher classification accuracy and identifies more informativetraining examples than the non-Bayesian approach and state-of-the-art distancemetric learning algorithms.
arxiv-1206-5291 | Improved Dynamic Schedules for Belief Propagation |  http://arxiv.org/abs/1206.5291  | author:Charles Sutton, Andrew McCallum category:cs.LG cs.AI stat.ML published:2012-06-20 summary:Belief propagation and its variants are popular methods for approximateinference, but their running time and even their convergence depend greatly onthe schedule used to send the messages. Recently, dynamic update schedules havebeen shown to converge much faster on hard networks than static schedules,namely the residual BP schedule of Elidan et al. [2006]. But that RBP algorithmwastes message updates: many messages are computed solely to determine theirpriority, and are never actually performed. In this paper, we show thatestimating the residual, rather than calculating it directly, leads tosignificant decreases in the number of messages required for convergence, andin the total running time. The residual is estimated using an upper bound basedon recent work on message errors in BP. On both synthetic and real-worldnetworks, this dramatically decreases the running time of BP, in some cases bya factor of five, without affecting the quality of the solution.
arxiv-1206-4481 | Parsimonious Mahalanobis Kernel for the Classification of High Dimensional Data |  http://arxiv.org/abs/1206.4481  | author:M. Fauvel, A. Villa, J. Chanussot, J. A. Benediktsson category:cs.NA cs.LG published:2012-06-20 summary:The classification of high dimensional data with kernel methods is consideredin this article. Exploit- ing the emptiness property of high dimensionalspaces, a kernel based on the Mahalanobis distance is proposed. The computationof the Mahalanobis distance requires the inversion of a covariance matrix. Inhigh dimensional spaces, the estimated covariance matrix is ill-conditioned andits inversion is unstable or impossible. Using a parsimonious statisticalmodel, namely the High Dimensional Discriminant Analysis model, the specificsignal and noise subspaces are estimated for each considered class making theinverse of the class specific covariance matrix explicit and stable, leading tothe definition of a parsimonious Mahalanobis kernel. A SVM based framework isused for selecting the hyperparameters of the parsimonious Mahalanobis kernelby optimizing the so-called radius-margin bound. Experimental results on threehigh dimensional data sets show that the proposed kernel is suitable forclassifying high dimensional data, providing better classification accuraciesthan the conventional Gaussian kernel.
arxiv-1206-5286 | MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies |  http://arxiv.org/abs/1206.5286  | author:Yair Weiss, Chen Yanover, Talya Meltzer category:cs.AI cs.LG stat.ML published:2012-06-20 summary:Finding the most probable assignment (MAP) in a general graphical model isknown to be NP hard but good approximations have been attained with max-productbelief propagation (BP) and its variants. In particular, it is known that usingBP on a single-cycle graph or tree reweighted BP on an arbitrary graph willgive the MAP solution if the beliefs have no ties. In this paper we extend thesetting under which BP can be used to provably extract the MAP. We defineConvex BP as BP algorithms based on a convex free energy approximation and showthat this class includes ordinary BP with single-cycle, tree reweighted BP andmany other BP variants. We show that when there are no ties, fixed-points ofconvex max-product BP will provably give the MAP solution. We also show thatconvex sum-product BP at sufficiently small temperatures can be used to solvelinear programs that arise from relaxing the MAP problem. Finally, we derive anovel condition that allows us to derive the MAP solution even if some of theconvex BP beliefs have ties. In experiments, we show that our theorems allow usto find the MAP in many real-world instances of graphical models where exactinference using junction-tree is impossible.
arxiv-1206-5290 | Imitation Learning with a Value-Based Prior |  http://arxiv.org/abs/1206.5290  | author:Umar Syed, Robert E. Schapire category:cs.LG cs.AI stat.ML published:2012-06-20 summary:The goal of imitation learning is for an apprentice to learn how to behave ina stochastic environment by observing a mentor demonstrating the correctbehavior. Accurate prior knowledge about the correct behavior can reduce theneed for demonstrations from the mentor. We present a novel approach toencoding prior knowledge about the correct behavior, where we assume that thisprior knowledge takes the form of a Markov Decision Process (MDP) that is usedby the apprentice as a rough and imperfect model of the mentor's behavior.Specifically, taking a Bayesian approach, we treat the value of a policy inthis modeling MDP as the log prior probability of the policy. In other words,we assume a priori that the mentor's behavior is likely to be a high valuepolicy in the modeling MDP, though quite possibly different from the optimalpolicy. We describe an efficient algorithm that, given a modeling MDP and a setof demonstrations by a mentor, provably converges to a stationary point of thelog posterior of the mentor's policy, where the posterior is computed withrespect to the "value based" prior. We also present empirical evidence thatthis prior does in fact speed learning of the mentor's policy, and is animprovement in our experiments over similar previous methods.
arxiv-1206-5293 | On Sensitivity of the MAP Bayesian Network Structure to the Equivalent Sample Size Parameter |  http://arxiv.org/abs/1206.5293  | author:Tomi Silander, Petri Kontkanen, Petri Myllymaki category:cs.LG stat.ML published:2012-06-20 summary:BDeu marginal likelihood score is a popular model selection criterion forselecting a Bayesian network structure based on sample data. Thisnon-informative scoring criterion assigns same score for network structuresthat encode same independence statements. However, before applying the BDeuscore, one must determine a single parameter, the equivalent sample size alpha.Unfortunately no generally accepted rule for determining the alpha parameterhas been suggested. This is disturbing, since in this paper we show through aseries of concrete experiments that the solution of the network structureoptimization problem is highly sensitive to the chosen alpha parameter value.Based on these results, we are able to give explanations for how and why thisphenomenon happens, and discuss ideas for solving this problem.
arxiv-1206-4326 | Joint Reconstruction of Multi-view Compressed Images |  http://arxiv.org/abs/1206.4326  | author:Vijayaraghavan Thirumalai, Pascal Frossard category:cs.MM cs.CV published:2012-06-19 summary:The distributed representation of correlated multi-view images is animportant problem that arise in vision sensor networks. This paper concentrateson the joint reconstruction problem where the distributively compressedcorrelated images are jointly decoded in order to improve the reconstructionquality of all the compressed images. We consider a scenario where the imagescaptured at different viewpoints are encoded independently using common codingsolutions (e.g., JPEG, H.264 intra) with a balanced rate distribution amongdifferent cameras. A central decoder first estimates the underlying correlationmodel from the independently compressed images which will be used for the jointsignal recovery. The joint reconstruction is then cast as a constrained convexoptimization problem that reconstructs total-variation (TV) smooth images thatcomply with the estimated correlation model. At the same time, we addconstraints that force the reconstructed images to be consistent with theircompressed versions. We show by experiments that the proposed jointreconstruction scheme outperforms independent reconstruction in terms of imagequality, for a given target bit rate. In addition, the decoding performance ofour proposed algorithm compares advantageously to state-of-the-art distributedcoding schemes based on disparity learning and on the DISCOVER.
arxiv-1206-4110 | ConeRANK: Ranking as Learning Generalized Inequalities |  http://arxiv.org/abs/1206.4110  | author:Truyen T. Tran, Duc Son Pham category:cs.LG cs.IR published:2012-06-19 summary:We propose a new data mining approach in ranking documents based on theconcept of cone-based generalized inequalities between vectors. A partialordering between two vectors is made with respect to a proper cone and thuslearning the preferences is formulated as learning proper cones. A pairwiselearning-to-rank algorithm (ConeRank) is proposed to learn a non-negativesubspace, formulated as a polyhedral cone, over document-pair differences. Thealgorithm is regularized by controlling the `volume' of the cone. Theexperimental studies on the latest and largest ranking dataset LETOR 4.0 showsthat ConeRank is competitive against other recent ranking approaches.
arxiv-1206-4116 | Dependence Maximizing Temporal Alignment via Squared-Loss Mutual Information |  http://arxiv.org/abs/1206.4116  | author:Makoto Yamada, Leonid Sigal, Michalis Raptis, Masashi Sugiyama category:stat.ML cs.AI published:2012-06-19 summary:The goal of temporal alignment is to establish time correspondence betweentwo sequences, which has many applications in a variety of areas such as speechprocessing, bioinformatics, computer vision, and computer graphics. In thispaper, we propose a novel temporal alignment method called least-squaresdynamic time warping (LSDTW). LSDTW finds an alignment that maximizesstatistical dependency between sequences, measured by a squared-loss variant ofmutual information. The benefit of this novel information-theoretic formulationis that LSDTW can align sequences with different lengths, differentdimensionality, high non-linearity, and non-Gaussianity in a computationallyefficient manner. In addition, model parameters such as an initial alignmentmatrix can be systematically optimized by cross-validation. We demonstrate theusefulness of LSDTW through experiments on synthetic and real-world Kinectaction recognition datasets.
arxiv-1206-4169 | Clustered Bandits |  http://arxiv.org/abs/1206.4169  | author:Loc Bui, Ramesh Johari, Shie Mannor category:cs.LG published:2012-06-19 summary:We consider a multi-armed bandit setting that is inspired by real-worldapplications in e-commerce. In our setting, there are a few types of users,each with a specific response to the different arms. When a user enters thesystem, his type is unknown to the decision maker. The decision maker caneither treat each user separately ignoring the previously observed users, orcan attempt to take advantage of knowing that only few types exist and clusterthe users according to their response to the arms. We devise algorithms thatcombine the usual exploration-exploitation tradeoff with clustering of usersand demonstrate the value of clustering. In the process of developingalgorithms for the clustered setting, we propose and analyze simple algorithmsfor the setup where a decision maker knows that a user belongs to one of fewtypes, but does not know which one.
arxiv-1206-4667 | Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation |  http://arxiv.org/abs/1206.4667  | author:Kendrick Boyd, Vitor Santos Costa, Jesse Davis, David Page category:cs.LG cs.AI cs.IR published:2012-06-18 summary:Precision-recall (PR) curves and the areas under them are widely used tosummarize machine learning results, especially for data sets exhibiting classskew. They are often used analogously to ROC curves and the area under ROCcurves. It is known that PR curves vary as class skew changes. What was notrecognized before this paper is that there is a region of PR space that iscompletely unachievable, and the size of this region depends only on the skew.This paper precisely characterizes the size of that region and discusses itsimplications for empirical evaluation methodology in machine learning.
arxiv-1206-4074 | A Linear Approximation to the chi^2 Kernel with Geometric Convergence |  http://arxiv.org/abs/1206.4074  | author:Fuxin Li, Guy Lebanon, Cristian Sminchisescu category:cs.LG cs.CV stat.ML published:2012-06-18 summary:We propose a new analytical approximation to the $\chi^2$ kernel thatconverges geometrically. The analytical approximation is derived withelementary methods and adapts to the input distribution for optimal convergencerate. Experiments show the new approximation leads to improved performance inimage classification and semantic segmentation tasks using a random Fourierfeature approximation of the $\exp-\chi^2$ kernel. Besides, out-of-coreprincipal component analysis (PCA) methods are introduced to reduce thedimensionality of the approximation and achieve better performance at theexpense of only an additional constant factor to the time complexity. Moreover,when PCA is performed jointly on the training and unlabeled testing data,further performance improvements can be obtained. Experiments conducted on thePASCAL VOC 2010 segmentation and the ImageNet ILSVRC 2010 datasets showstatistically significant improvements over alternative approximation methods.
arxiv-1206-4631 | A Poisson convolution model for characterizing topical content with word frequency and exclusivity |  http://arxiv.org/abs/1206.4631  | author:Edoardo M Airoldi, Jonathan M Bischof category:cs.LG cs.CL cs.IR stat.ME stat.ML published:2012-06-18 summary:An ongoing challenge in the analysis of document collections is how tosummarize content in terms of a set of inferred themes that can be interpretedsubstantively in terms of topics. The current practice of parametrizing thethemes in terms of most frequent words limits interpretability by ignoring thedifferential use of words across topics. We argue that words that are bothcommon and exclusive to a theme are more effective at characterizing topicalcontent. We consider a setting where professional editors have annotateddocuments to a collection of topic categories, organized into a tree, in whichleaf-nodes correspond to the most specific topics. Each document is annotatedto multiple categories, at different levels of the tree. We introduce ahierarchical Poisson convolution model to analyze annotated documents in thissetting. The model leverages the structure among categories defined byprofessional editors to infer a clear semantic description for each topic interms of words that are both frequent and exclusive. We carry out a largerandomized experiment on Amazon Turk to demonstrate that topic summaries basedon the FREX score are more interpretable than currently established frequencybased summaries, and that the proposed model produces more efficient estimatesof exclusivity than with currently models. We also develop a parallelizedHamiltonian Monte Carlo sampler that allows the inference to scale to millionsof documents.
arxiv-1206-3975 | The Ultrasound Visualization Pipeline - A Survey |  http://arxiv.org/abs/1206.3975  | author:Åsmund Birkeland, Veronika Solteszova, Dieter Hönigmann, Odd Helge Gilja, Svein Brekke, Timo Ropinski, Ivan Viola category:cs.GR cs.CV published:2012-06-18 summary:Ultrasound is one of the most frequently used imaging modality in medicine.The high spatial resolution, its interactive nature and non-invasiveness makesit the first choice in many examinations. Image interpretation is one ofultrasound's main challenges. Much training is required to obtain a confidentskill level in ultrasound-based diagnostics. State-of-the-art graphicstechniques is needed to provide meaningful visualizations of ultrasound inreal-time. In this paper we present the process-pipeline for ultrasoundvisualization, including an overview of the tasks performed in the specificsteps. To provide an insight into the trends of ultrasound visualizationresearch, we have selected a set of significant publications and divided theminto a technique-based taxonomy covering the topics pre-processing,segmentation, registration, rendering and augmented reality. For the differenttechnique types we discuss the difference between ultrasound-based techniquesand techniques for other modalities.
arxiv-1206-4686 | Discriminative Probabilistic Prototype Learning |  http://arxiv.org/abs/1206.4686  | author:Edwin Bonilla, Antonio Robles-Kelly category:cs.LG stat.ML published:2012-06-18 summary:In this paper we propose a simple yet powerful method for learningrepresentations in supervised learning scenarios where each original inputdatapoint is described by a set of vectors and their associated outputs may begiven by soft labels indicating, for example, class probabilities. We representan input datapoint as a mixture of probabilities over the corresponding set offeature vectors where each probability indicates how likely each vector is tobelong to an unknown prototype pattern. We propose a probabilistic model thatparameterizes these prototype patterns in terms of hidden variables andtherefore it can be trained with conventional approaches based on likelihoodmaximization. More importantly, both the model parameters and the prototypepatterns can be learned from data in a discriminative way. We show that ourmodel can be seen as a probabilistic generalization of learning vectorquantization (LVQ). We apply our method to the problems of shapeclassification, hyperspectral imaging classification and people's work classcategorization, showing the superior performance of our method compared to thestandard prototype-based classification approach and other competitivebenchmark methods.
arxiv-1206-4685 | Sparse-GEV: Sparse Latent Space Model for Multivariate Extreme Value Time Serie Modeling |  http://arxiv.org/abs/1206.4685  | author:Yan Liu, Taha Bahadori, Hongfei Li category:stat.ME cs.LG stat.AP published:2012-06-18 summary:In many applications of time series models, such as climate analysis andsocial media analysis, we are often interested in extreme events, such asheatwave, wind gust, and burst of topics. These time series data usuallyexhibit a heavy-tailed distribution rather than a Gaussian distribution. Thisposes great challenges to existing approaches due to the significantlydifferent assumptions on the data distributions and the lack of sufficient pastdata on extreme events. In this paper, we propose the Sparse-GEV model, alatent state model based on the theory of extreme value modeling toautomatically learn sparse temporal dependence and make predictions. Our modelis theoretically significant because it is among the first models to learnsparse temporal dependencies among multivariate extreme value time series. Wedemonstrate the superior performance of our algorithm to the state-of-artmethods, including Granger causality, copula approach, and transfer entropy, onone synthetic dataset, one climate dataset and two Twitter datasets.
arxiv-1206-3881 | DANCo: Dimensionality from Angle and Norm Concentration |  http://arxiv.org/abs/1206.3881  | author:Claudio Ceruti, Simone Bassis, Alessandro Rozza, Gabriele Lombardi, Elena Casiraghi, Paola Campadelli category:cs.LG stat.ML published:2012-06-18 summary:In the last decades the estimation of the intrinsic dimensionality of adataset has gained considerable importance. Despite the great deal of researchwork devoted to this task, most of the proposed solutions prove to beunreliable when the intrinsic dimensionality of the input dataset is high andthe manifold where the points lie is nonlinearly embedded in a higherdimensional space. In this paper we propose a novel robust intrinsicdimensionality estimator that exploits the twofold complementary informationconveyed both by the normalized nearest neighbor distances and by the anglescomputed on couples of neighboring points, providing also closed-forms for theKullback-Leibler divergences of the respective distributions. Experimentsperformed on both synthetic and real datasets highlight the robustness and theeffectiveness of the proposed algorithm when compared to state of the artmethodologies.
arxiv-1206-4599 | A Unified Robust Classification Model |  http://arxiv.org/abs/1206.4599  | author:Akiko Takeda, Hiroyuki Mitsugi, Takafumi Kanamori category:cs.LG stat.ML published:2012-06-18 summary:A wide variety of machine learning algorithms such as support vector machine(SVM), minimax probability machine (MPM), and Fisher discriminant analysis(FDA), exist for binary classification. The purpose of this paper is to providea unified classification model that includes the above models through a robustoptimization approach. This unified model has several benefits. One is that theextensions and improvements intended for SVM become applicable to MPM and FDA,and vice versa. Another benefit is to provide theoretical results to abovelearning methods at once by dealing with the unified model. We give astatistical interpretation of the unified classification model and propose anon-convex optimization algorithm that can be applied to non-convex variants ofexisting learning methods.
arxiv-1206-4600 | Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes |  http://arxiv.org/abs/1206.4600  | author:Murat Dundar, Ferit Akova, Alan Qi, Bartek Rajwa category:cs.LG stat.ML published:2012-06-18 summary:We present a framework for online inference in the presence of anonexhaustively defined set of classes that incorporates supervisedclassification with class discovery and modeling. A Dirichlet process prior(DPP) model defined over class distributions ensures that both known andunknown class distributions originate according to a common base distribution.In an attempt to automatically discover potentially interesting classformations, the prior model is coupled with a suitably chosen data model, andsequential Monte Carlo sampling is used to perform online inference. Ourresearch is driven by a biodetection application, where a new class of pathogenmay suddenly appear, and the rapid increase in the number of samplesoriginating from this class indicates the onset of an outbreak.
arxiv-1206-4601 | Convex Multitask Learning with Flexible Task Clusters |  http://arxiv.org/abs/1206.4601  | author:Wenliang Zhong, James Kwok category:cs.LG stat.ML published:2012-06-18 summary:Traditionally, multitask learning (MTL) assumes that all the tasks arerelated. This can lead to negative transfer when tasks are indeed incoherent.Recently, a number of approaches have been proposed that alleviate this problemby discovering the underlying task clusters or relationships. However, they arelimited to modeling these relationships at the task level, which may berestrictive in some applications. In this paper, we propose a novel MTLformulation that captures task relationships at the feature-level. Depending onthe interactions among tasks and features, the proposed method constructdifferent task clusters for different features, without even the need ofpre-specifying the number of clusters. Computationally, the proposedformulation is strongly convex, and can be efficiently solved by acceleratedproximal methods. Experiments are performed on a number of synthetic andreal-world data sets. Under various degrees of task relationships, the accuracyof the proposed method is consistently among the best. Moreover, thefeature-specific task clusters obtained agree with the known/plausible taskstructures of the data.
arxiv-1206-4602 | Quasi-Newton Methods: A New Direction |  http://arxiv.org/abs/1206.4602  | author:Philipp Hennig, Martin Kiefel category:cs.NA cs.LG stat.ML published:2012-06-18 summary:Four decades after their invention, quasi-Newton methods are still state ofthe art in unconstrained numerical optimization. Although not usuallyinterpreted thus, these are learning algorithms that fit a local quadraticapproximation to the objective function. We show that many, including the mostpopular, quasi-Newton methods can be interpreted as approximations of Bayesianlinear regression under varying prior assumptions. This new notion elucidatessome shortcomings of classical algorithms, and lights the way to a novelnonparametric quasi-Newton method, which is able to make more efficient use ofavailable information at computational cost similar to its predecessors.
arxiv-1206-4604 | Learning the Experts for Online Sequence Prediction |  http://arxiv.org/abs/1206.4604  | author:Elad Eban, Aharon Birnbaum, Shai Shalev-Shwartz, Amir Globerson category:cs.LG cs.AI published:2012-06-18 summary:Online sequence prediction is the problem of predicting the next element of asequence given previous elements. This problem has been extensively studied inthe context of individual sequence prediction, where no prior assumptions aremade on the origin of the sequence. Individual sequence prediction algorithmswork quite well for long sequences, where the algorithm has enough time tolearn the temporal structure of the sequence. However, they might give poorpredictions for short sequences. A possible remedy is to rely on the generalmodel of prediction with expert advice, where the learner has access to a setof $r$ experts, each of which makes its own predictions on the sequence. It iswell known that it is possible to predict almost as well as the best expert ifthe sequence length is order of $\log(r)$. But, without firm prior knowledge onthe problem, it is not clear how to choose a small set of {\em good} experts.In this paper we describe and analyze a new algorithm that learns a good set ofexperts using a training set of previously observed sequences. We demonstratethe merits of our approach by applying it on the task of click prediction onthe web.
arxiv-1206-4606 | TrueLabel + Confusions: A Spectrum of Probabilistic Models in Analyzing Multiple Ratings |  http://arxiv.org/abs/1206.4606  | author:Chao Liu, Yi-Min Wang category:cs.LG cs.AI stat.ML published:2012-06-18 summary:This paper revisits the problem of analyzing multiple ratings given bydifferent judges. Different from previous work that focuses on distilling thetrue labels from noisy crowdsourcing ratings, we emphasize gaining diagnosticinsights into our in-house well-trained judges. We generalize the well-knownDawidSkene model (Dawid & Skene, 1979) to a spectrum of probabilistic modelsunder the same "TrueLabel + Confusion" paradigm, and show that our proposedhierarchical Bayesian model, called HybridConfusion, consistently outperformsDawidSkene on both synthetic and real-world data sets.
arxiv-1206-4607 | Distributed Tree Kernels |  http://arxiv.org/abs/1206.4607  | author:Fabio Massimo Zanzotto, Lorenzo Dell'Arciprete category:cs.LG stat.ML published:2012-06-18 summary:In this paper, we propose the distributed tree kernels (DTK) as a novelmethod to reduce time and space complexity of tree kernels. Using a linearcomplexity algorithm to compute vectors for trees, we embed feature spaces oftree fragments in low-dimensional spaces where the kernel computation isdirectly done with dot product. We show that DTKs are faster, correlate withtree kernels, and obtain a statistically similar performance in two naturallanguage processing tasks.
arxiv-1206-4608 | A Hybrid Algorithm for Convex Semidefinite Optimization |  http://arxiv.org/abs/1206.4608  | author:Soeren Laue category:cs.LG cs.DS cs.NA stat.ML published:2012-06-18 summary:We present a hybrid algorithm for optimizing a convex, smooth function overthe cone of positive semidefinite matrices. Our algorithm converges to theglobal optimal solution and can be used to solve general large-scalesemidefinite programs and hence can be readily applied to a variety of machinelearning problems. We show experimental results on three machine learningproblems (matrix completion, metric learning, and sparse PCA) . Our approachoutperforms state-of-the-art algorithms.
arxiv-1206-4609 | On multi-view feature learning |  http://arxiv.org/abs/1206.4609  | author:Roland Memisevic category:cs.CV cs.LG stat.ML published:2012-06-18 summary:Sparse coding is a common approach to learning local features for objectrecognition. Recently, there has been an increasing interest in learningfeatures from spatio-temporal, binocular, or other multi-observation data,where the goal is to encode the relationship between images rather than thecontent of a single image. We provide an analysis of multi-view featurelearning, which shows that hidden variables encode transformations by detectingrotation angles in the eigenspaces shared among multiple image warps. Ouranalysis helps explain recent experimental results showing thattransformation-specific features emerge when training complex cell models onvideos. Our analysis also shows that transformation-invariant features canemerge as a by-product of learning representations of transformations.
arxiv-1206-4610 | Manifold Relevance Determination |  http://arxiv.org/abs/1206.4610  | author:Andreas Damianou, Carl Ek, Michalis Titsias, Neil Lawrence category:cs.LG cs.CV stat.ML published:2012-06-18 summary:In this paper we present a fully Bayesian latent variable model whichexploits conditional nonlinear(in)-dependence structures to learn an efficientlatent representation. The latent space is factorized to represent shared andprivate information from multiple views of the data. In contrast to previousapproaches, we introduce a relaxation to the discrete segmentation and allowfor a "softly" shared latent space. Further, Bayesian techniques allow us toautomatically estimate the dimensionality of the latent spaces. The model iscapable of capturing structure underlying extremely high dimensional spaces.This is illustrated by modelling unprocessed images with tenths of thousands ofpixels. This also allows us to directly generate novel images from the trainedmodel by sampling from the discovered latent spaces. We also demonstrate themodel by prediction of human pose in an ambiguous setting. Our Bayesianframework allows us to perform disambiguation in a principled manner byincluding latent space priors which incorporate the dynamic nature of the data.
arxiv-1206-4611 | A Convex Feature Learning Formulation for Latent Task Structure Discovery |  http://arxiv.org/abs/1206.4611  | author:Pratik Jawanpuria, J. Saketha Nath category:cs.LG stat.ML published:2012-06-18 summary:This paper considers the multi-task learning problem and in the setting wheresome relevant features could be shared across few related tasks. Most of theexisting methods assume the extent to which the given tasks are related orshare a common feature space to be known apriori. In real-world applicationshowever, it is desirable to automatically discover the groups of related tasksthat share a feature space. In this paper we aim at searching the exponentiallylarge space of all possible groups of tasks that may share a feature space. Themain contribution is a convex formulation that employs a graph-basedregularizer and simultaneously discovers few groups of related tasks, havingclose-by task parameters, as well as the feature space shared within eachgroup. The regularizer encodes an important structure among the groups of tasksleading to an efficient algorithm for solving it: if there is no feature spaceunder which a group of tasks has close-by task parameters, then there does notexist such a feature space for any of its supersets. An efficient active setalgorithm that exploits this simplification and performs a clever search in theexponentially large space is presented. The algorithm is guaranteed to solvethe proposed formulation (within some precision) in a time polynomial in thenumber of groups of related tasks discovered. Empirical results on benchmarkdatasets show that the proposed formulation achieves good generalization andoutperforms state-of-the-art multi-task learning algorithms in some cases.
arxiv-1206-4612 | Exact Soft Confidence-Weighted Learning |  http://arxiv.org/abs/1206.4612  | author:Jialei Wang, Peilin Zhao, Steven C. H. Hoi category:cs.LG published:2012-06-18 summary:In this paper, we propose a new Soft Confidence-Weighted (SCW) onlinelearning scheme, which enables the conventional confidence-weighted learningmethod to handle non-separable cases. Unlike the previous confidence-weightedlearning algorithms, the proposed soft confidence-weighted learning methodenjoys all the four salient properties: (i) large margin training, (ii)confidence weighting, (iii) capability to handle non-separable data, and (iv)adaptive margin. Our experimental results show that the proposed SCW algorithmssignificantly outperform the original CW algorithm. When comparing with avariety of state-of-the-art algorithms (including AROW, NAROW and NHERD), wefound that SCW generally achieves better or at least comparable predictiveaccuracy, but enjoys significant advantage of computational efficiency (i.e.,smaller number of updates and lower time cost).
arxiv-1206-4613 | Near-Optimal BRL using Optimistic Local Transitions |  http://arxiv.org/abs/1206.4613  | author:Mauricio Araya, Olivier Buffet, Vincent Thomas category:cs.AI cs.LG stat.ML published:2012-06-18 summary:Model-based Bayesian Reinforcement Learning (BRL) allows a foundformalization of the problem of acting optimally while facing an unknownenvironment, i.e., avoiding the exploration-exploitation dilemma. However,algorithms explicitly addressing BRL suffer from such a combinatorial explosionthat a large body of work relies on heuristic algorithms. This paper introducesBOLT, a simple and (almost) deterministic heuristic algorithm for BRL which isoptimistic about the transition function. We analyze BOLT's sample complexity,and show that under certain parameters, the algorithm is near-optimal in theBayesian sense with high probability. Then, experimental results highlight thekey differences of this method compared to previous work.
arxiv-1206-4614 | Information-theoretic Semi-supervised Metric Learning via Entropy Regularization |  http://arxiv.org/abs/1206.4614  | author:Gang Niu, Bo Dai, Makoto Yamada, Masashi Sugiyama category:cs.LG stat.ML published:2012-06-18 summary:We propose a general information-theoretic approach called Seraph(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metriclearning that does not rely upon the manifold assumption. Given the probabilityparameterized by a Mahalanobis distance, we maximize the entropy of thatprobability on labeled data and minimize it on unlabeled data following entropyregularization, which allows the supervised and unsupervised parts to beintegrated in a natural and meaningful way. Furthermore, Seraph is regularizedby encouraging a low-rank projection induced from the metric. The optimizationof Seraph is solved efficiently and stably by an EM-like scheme with theanalytical E-Step and convex M-Step. Experiments demonstrate that Seraphcompares favorably with many well-known global and local metric learningmethods.
arxiv-1206-4615 | Levy Measure Decompositions for the Beta and Gamma Processes |  http://arxiv.org/abs/1206.4615  | author:Yingjian Wang, Lawrence Carin category:stat.ME cs.LG math.ST stat.TH published:2012-06-18 summary:We develop new representations for the Levy measures of the beta and gammaprocesses. These representations are manifested in terms of an infinite sum ofwell-behaved (proper) beta and gamma distributions. Further, we demonstrate howthese infinite sums may be truncated in practice, and explicitly characterizetruncation errors. We also perform an analysis of the characteristics ofposterior distributions, based on the proposed decompositions. Thedecompositions provide new insights into the beta and gamma processes (andtheir generalizations), and we demonstrate how the proposed representationunifies some properties of the two. This paper is meant to provide a rigorousfoundation for and new perspectives on Levy processes, as these are ofincreasing importance in machine learning.
arxiv-1206-4683 | Marginalized Denoising Autoencoders for Domain Adaptation |  http://arxiv.org/abs/1206.4683  | author:Minmin Chen, Zhixiang Xu, Kilian Weinberger, Fei Sha category:cs.LG published:2012-06-18 summary:Stacked denoising autoencoders (SDAs) have been successfully used to learnnew representations for domain adaptation. Recently, they have attained recordaccuracy on standard benchmark tasks of sentiment analysis across differenttext domains. SDAs learn robust data representations by reconstruction,recovering original features from data that are artificially corrupted withnoise. In this paper, we propose marginalized SDA (mSDA) that addresses twocrucial limitations of SDAs: high computational cost and lack of scalability tohigh-dimensional features. In contrast to SDAs, our approach of mSDAmarginalizes noise and thus does not require stochastic gradient descent orother optimization algorithms to learn parameters ? in fact, they are computedin closed-form. Consequently, mSDA, which can be implemented in only 20 linesof MATLAB^{TM}, significantly speeds up SDAs by two orders of magnitude.Furthermore, the representations learnt by mSDA are as effective as thetraditional SDAs, attaining almost identical accuracies in benchmark tasks.
arxiv-1206-4682 | Copula-based Kernel Dependency Measures |  http://arxiv.org/abs/1206.4682  | author:Barnabas Poczos, Zoubin Ghahramani, Jeff Schneider category:cs.LG math.ST stat.ML stat.TH published:2012-06-18 summary:The paper presents a new copula based method for measuring dependence betweenrandom variables. Our approach extends the Maximum Mean Discrepancy to thecopula of the joint distribution. We prove that this approach has severaladvantageous properties. Similarly to Shannon mutual information, the proposeddependence measure is invariant to any strictly increasing transformation ofthe marginal variables. This is important in many applications, for example infeature selection. The estimator is consistent, robust to outliers, and usesrank statistics only. We derive upper bounds on the convergence rate andpropose independence tests too. We illustrate the theoretical contributionsthrough a series of experiments in feature selection and low-dimensionalembedding of distributions.
arxiv-1206-4681 | LPQP for MAP: Putting LP Solvers to Better Use |  http://arxiv.org/abs/1206.4681  | author:Patrick Pletscher, Sharon Wulff category:cs.LG stat.ML published:2012-06-18 summary:MAP inference for general energy functions remains a challenging problem.While most efforts are channeled towards improving the linear programming (LP)based relaxation, this work is motivated by the quadratic programming (QP)relaxation. We propose a novel MAP relaxation that penalizes theKullback-Leibler divergence between the LP pairwise auxiliary variables, and QPequivalent terms given by the product of the unaries. We develop two efficientalgorithms based on variants of this relaxation. The algorithms minimize thenon-convex objective using belief propagation and dual decomposition asbuilding blocks. Experiments on synthetic and real-world data show that thesolutions returned by our algorithms substantially improve over the LPrelaxation.
arxiv-1206-4680 | Fast Prediction of New Feature Utility |  http://arxiv.org/abs/1206.4680  | author:Hoyt Koepke, Mikhail Bilenko category:cs.LG math.ST stat.TH published:2012-06-18 summary:We study the new feature utility prediction problem: statistically testingwhether adding a new feature to the data representation can improve predictiveaccuracy on a supervised learning task. In many applications, identifying newinformative features is the primary pathway for improving performance. However,evaluating every potential feature by re-training the predictor with it can becostly. The paper describes an efficient, learner-independent technique forestimating new feature utility without re-training based on the currentpredictor's outputs. The method is obtained by deriving a connection betweenloss reduction potential and the new feature's correlation with the lossgradient of the current predictor. This leads to a simple yet powerfulhypothesis testing procedure, for which we prove consistency. Our theoreticalanalysis is accompanied by empirical evaluation on standard benchmarks and alarge-scale industrial dataset.
arxiv-1206-4679 | Factorized Asymptotic Bayesian Hidden Markov Models |  http://arxiv.org/abs/1206.4679  | author:Ryohei Fujimaki, Kohei Hayashi category:cs.LG stat.ML published:2012-06-18 summary:This paper addresses the issue of model selection for hidden Markov models(HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which hasbeen recently developed for model selection on independent hidden variables(i.e., mixture models), for time-dependent hidden variables. As with FAB inmixture models, FAB for HMMs is derived as an iterative lower boundmaximization algorithm of a factorized information criterion (FIC). Itinherits, from FAB for mixture models, several desirable properties forlearning HMMs, such as asymptotic consistency of FIC with marginallog-likelihood, a shrinkage effect for hidden state selection, monotonicincrease of the lower FIC bound through the iterative optimization. Further, itdoes not have a tunable hyper-parameter, and thus its model selection processcan be fully automated. Experimental results shows that FAB outperformsstates-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM interms of model selection accuracy and computational efficiency.
arxiv-1206-4678 | Linear Regression with Limited Observation |  http://arxiv.org/abs/1206.4678  | author:Elad Hazan, Tomer Koren category:cs.LG stat.ML published:2012-06-18 summary:We consider the most common variants of linear regression, including Ridge,Lasso and Support-vector regression, in a setting where the learner is allowedto observe only a fixed number of attributes of each example at training time.We present simple and efficient algorithms for these problems: for Lasso andRidge regression they need the same total number of attributes (up toconstants) as do full-information algorithms, for reaching a certain accuracy.For Support-vector regression, we require exponentially less attributescompared to the state of the art. By that, we resolve an open problem recentlyposed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds tobe justified by superior performance compared to the state of the art.
arxiv-1206-4677 | Semi-Supervised Learning of Class Balance under Class-Prior Change by Distribution Matching |  http://arxiv.org/abs/1206.4677  | author:Marthinus Du Plessis, Masashi Sugiyama category:cs.LG stat.ML published:2012-06-18 summary:In real-world classification problems, the class balance in the trainingdataset does not necessarily reflect that of the test dataset, which can causesignificant estimation bias. If the class ratio of the test dataset is known,instance re-weighting or resampling allows systematical bias correction.However, learning the class ratio of the test dataset is challenging when nolabeled data is available from the test domain. In this paper, we propose toestimate the class ratio in the test dataset by matching probabilitydistributions of training and test input data. We demonstrate the utility ofthe proposed approach through experiments.
arxiv-1206-4676 | Clustering by Low-Rank Doubly Stochastic Matrix Decomposition |  http://arxiv.org/abs/1206.4676  | author:Zhirong Yang, Erkki Oja category:cs.LG cs.CV cs.NA stat.ML published:2012-06-18 summary:Clustering analysis by nonnegative low-rank approximations has achievedremarkable progress in the past decade. However, most approximation approachesin this direction are still restricted to matrix factorization. We propose anew low-rank learning method to improve the clustering performance, which isbeyond matrix factorization. The approximation is based on a two-step bipartiterandom walk through virtual cluster nodes, where the approximation is formed byonly cluster assigning probabilities. Minimizing the approximation errormeasured by Kullback-Leibler divergence is equivalent to maximizing thelikelihood of a discriminative model, which endows our method with a solidprobabilistic interpretation. The optimization is implemented by a relaxedMajorization-Minimization algorithm that is advantageous in finding good localminima. Furthermore, we point out that the regularized algorithm with Dirichletprior only serves as initialization. Experimental results show that the newmethod has strong performance in clustering purity for various datasets,especially for large-scale manifold data.
arxiv-1206-4675 | Finding Botnets Using Minimal Graph Clusterings |  http://arxiv.org/abs/1206.4675  | author:Peter Haider, Tobias Scheffer category:cs.CR cs.DC cs.LG published:2012-06-18 summary:We study the problem of identifying botnets and the IP addresses which theycomprise, based on the observation of a fraction of the global email spamtraffic. Observed mailing campaigns constitute evidence for joint botnetmembership, they are represented by cliques in the graph of all messages. Noevidence against an association of nodes is ever available. We reduce theproblem of identifying botnets to a problem of finding a minimal clustering ofthe graph of messages. We directly model the distribution of clusterings giventhe input graph; this avoids potential errors caused by distributionalassumptions of a generative model. We report on a case study in which weevaluate the model by its ability to predict the spam campaign that a given IPaddress is going to participate in.
arxiv-1206-4674 | Comparison-Based Learning with Rank Nets |  http://arxiv.org/abs/1206.4674  | author:Amin Karbasi, Stratis Ioannidis, laurent Massoulie category:cs.LG cs.DS stat.ML published:2012-06-18 summary:We consider the problem of search through comparisons, where a user ispresented with two candidate objects and reveals which is closer to herintended target. We study adaptive strategies for finding the target, thatrequire knowledge of rank relationships but not actual distances betweenobjects. We propose a new strategy based on rank nets, and show that for targetdistributions with a bounded doubling constant, it finds the target in a numberof comparisons close to the entropy of the target distribution and, hence, ofthe optimum. We extend these results to the case of noisy oracles, and comparethis strategy to prior art over multiple datasets.
arxiv-1206-4673 | Group Sparse Additive Models |  http://arxiv.org/abs/1206.4673  | author:Junming Yin, Xi Chen, Eric Xing category:cs.LG stat.ML published:2012-06-18 summary:We consider the problem of sparse variable selection in nonparametricadditive models, with the prior knowledge of the structure among the covariatesto encourage those variables within a group to be selected jointly. Previousworks either study the group sparsity in the parametric setting (e.g., grouplasso), or address the problem in the non-parametric setting without exploitingthe structural information (e.g., sparse additive models). In this paper, wepresent a new method, called group sparse additive models (GroupSpAM), whichcan handle group sparsity in additive models. We generalize the l1/l2 norm toHilbert spaces as the sparsity-inducing penalty in GroupSpAM. Moreover, wederive a novel thresholding condition for identifying the functional sparsityat the group level, and propose an efficient block coordinate descent algorithmfor constructing the estimate. We demonstrate by simulation that GroupSpAMsubstantially outperforms the competing methods in terms of support recoveryand prediction accuracy in additive models, and also conduct a comparativeexperiment on a real breast cancer dataset.
arxiv-1206-4672 | Efficient Active Algorithms for Hierarchical Clustering |  http://arxiv.org/abs/1206.4672  | author:Akshay Krishnamurthy, Sivaraman Balakrishnan, Min Xu, Aarti Singh category:cs.LG stat.ML published:2012-06-18 summary:Advances in sensing technologies and the growth of the internet have resultedin an explosion in the size of modern datasets, while storage and processingpower continue to lag behind. This motivates the need for algorithms that areefficient, both in terms of the number of measurements needed and running time.To combat the challenges associated with large datasets, we propose a generalframework for active hierarchical clustering that repeatedly runs anoff-the-shelf clustering algorithm on small subsets of the data and comes withguarantees on performance, measurement complexity and runtime complexity. Weinstantiate this framework with a simple spectral clustering algorithm andprovide concrete results on its performance, showing that, under someassumptions, this algorithm recovers all clusters of size ?(log n) using O(nlog^2 n) similarities and runs in O(n log^3 n) time for a dataset of n objects.Through extensive experimentation we also demonstrate that this framework ispractically alluring.
arxiv-1206-4671 | Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling |  http://arxiv.org/abs/1206.4671  | author:Changyou Chen, Nan Ding, Wray Buntine category:cs.LG stat.ML published:2012-06-18 summary:We develop dependent hierarchical normalized random measures and apply themto dynamic topic modeling. The dependency arises via superposition, subsamplingand point transition on the underlying Poisson processes of these measures. Themeasures used include normalised generalised Gamma processes that demonstratepower law properties, unlike Dirichlet processes used previously in dynamictopic modeling. Inference for the model includes adapting a recently developedslice sampler to directly manipulate the underlying Poisson process.Experiments performed on news, blogs, academic and Twitter collectionsdemonstrate the technique gives superior perplexity over a number of previousmodels.
arxiv-1206-4670 | State-Space Inference for Non-Linear Latent Force Models with Application to Satellite Orbit Prediction |  http://arxiv.org/abs/1206.4670  | author:Jouni Hartikainen, Mari Seppanen, Simo Sarkka category:cs.IT astro-ph.EP cs.LG math.IT published:2012-06-18 summary:Latent force models (LFMs) are flexible models that combine mechanisticmodelling principles (i.e., physical models) with non-parametric data-drivencomponents. Several key applications of LFMs need non-linearities, whichresults in analytically intractable inference. In this work we show hownon-linear LFMs can be represented as non-linear white noise driven state-spacemodels and present an efficient non-linear Kalman filtering and smoothing basedmethod for approximate state and parameter inference. We illustrate theperformance of the proposed methodology via two simulated examples, and applyit to a real-world problem of long-term prediction of GPS satellite orbits.
arxiv-1206-4669 | Sparse Additive Functional and Kernel CCA |  http://arxiv.org/abs/1206.4669  | author:Sivaraman Balakrishnan, Kriti Puniyani, John Lafferty category:cs.LG stat.ML published:2012-06-18 summary:Canonical Correlation Analysis (CCA) is a classical tool for findingcorrelations among the components of two random vectors. In recent years, CCAhas been widely applied to the analysis of genomic data, where it is common forresearchers to perform multiple assays on a single set of patient samples.Recent work has proposed sparse variants of CCA to address the highdimensionality of such data. However, classical and sparse CCA are based onlinear models, and are thus limited in their ability to find generalcorrelations. In this paper, we present two approaches to high-dimensionalnonparametric CCA, building on recent developments in high-dimensionalnonparametric regression. We present estimation procedures for both approaches,and analyze their theoretical properties in the high-dimensional setting. Wedemonstrate the effectiveness of these procedures in discovering nonlinearcorrelations via extensive simulations, as well as through experiments withgenomic data.
arxiv-1206-4668 | Approximate Principal Direction Trees |  http://arxiv.org/abs/1206.4668  | author:Mark McCartin-Lim, Andrew McGregor, Rui Wang category:cs.LG cs.DS stat.ML published:2012-06-18 summary:We introduce a new spatial data structure for high dimensional data calledthe \emph{approximate principal direction tree} (APD tree) that adapts to theintrinsic dimension of the data. Our algorithm ensures vector-quantizationaccuracy similar to that of computationally-expensive PCA trees with similartime-complexity to that of lower-accuracy RP trees. APD trees use a small number of power-method iterations to find splittingplanes for recursively partitioning the data. As such they provide a naturaltrade-off between the running-time and accuracy achieved by RP and PCA trees.Our theoretical results establish a) strong performance guarantees regardlessof the convergence rate of the power-method and b) that $O(\log d)$ iterationssuffice to establish the guarantee of PCA trees when the intrinsic dimension is$d$. We demonstrate this trade-off and the efficacy of our data structure onboth the CPU and GPU.
arxiv-1206-4666 | A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices |  http://arxiv.org/abs/1206.4666  | author:Mingjun Zhong, Mark Girolami category:stat.CO cs.LG stat.ME published:2012-06-18 summary:We present a Bayesian scheme for the approximate diagonalisation of severalsquare matrices which are not necessarily symmetric. A Gibbs sampler is derivedto simulate samples of the common eigenvectors and the eigenvalues for thesematrices. Several synthetic examples are used to illustrate the performance ofthe proposed Gibbs sampler and we then provide comparisons to several otherjoint diagonalization algorithms, which shows that the Gibbs sampler achievesthe state-of-the-art performance on the examples considered. As a byproduct,the output of the Gibbs sampler could be used to estimate the log marginallikelihood, however we employ the approximation based on the Bayesianinformation criterion (BIC) which in the synthetic examples consideredcorrectly located the number of common eigenvectors. We then succesfullyapplied the sampler to the source separation problem as well as the commonprincipal component analysis and the common spatial pattern analysis problems.
arxiv-1206-4665 | Nonparametric variational inference |  http://arxiv.org/abs/1206.4665  | author:Samuel Gershman, Matt Hoffman, David Blei category:cs.LG stat.ML published:2012-06-18 summary:Variational methods are widely used for approximate posterior inference.However, their use is typically limited to families of distributions that enjoyparticular conjugacy properties. To circumvent this limitation, we propose afamily of variational approximations inspired by nonparametric kernel densityestimation. The locations of these kernels and their bandwidth are treated asvariational parameters and optimized to improve an approximate lower bound onthe marginal likelihood of the data. Using multiple kernels allows theapproximation to capture multiple modes of the posterior, unlike most othervariational approximations. We demonstrate the efficacy of the nonparametricapproximation with a hierarchical logistic regression model and a nonlinearmatrix factorization model. We obtain predictive performance as good as orbetter than more specialized variational methods and sample-basedapproximations. The method is easy to apply to more general graphical modelsfor which standard variational methods are difficult to derive.
arxiv-1206-4664 | Tighter Variational Representations of f-Divergences via Restriction to Probability Measures |  http://arxiv.org/abs/1206.4664  | author:Avraham Ruderman, Mark Reid, Dario Garcia-Garcia, James Petterson category:cs.LG stat.ML published:2012-06-18 summary:We show that the variational representations for f-divergences currently usedin the literature can be tightened. This has implications to a number ofmethods recently proposed based on this representation. As an exampleapplication we use our tighter representation to derive a general f-divergenceestimator based on two i.i.d. samples and derive the dual program for thisestimator that performs well empirically. We also point out a connectionbetween our estimator and MMD.
arxiv-1206-4663 | The Convexity and Design of Composite Multiclass Losses |  http://arxiv.org/abs/1206.4663  | author:Mark Reid, Robert Williamson, Peng Sun category:cs.LG stat.ML published:2012-06-18 summary:We consider composite loss functions for multiclass prediction comprising aproper (i.e., Fisher-consistent) loss over probability distributions and aninverse link function. We establish conditions for their (strong) convexity andexplore the implications. We also show how the separation of concerns affordedby using this composite representation allows for the design of families oflosses with the same Bayes risk.
arxiv-1206-4662 | Bayesian Watermark Attacks |  http://arxiv.org/abs/1206.4662  | author:Ivo Shterev, David Dunson category:cs.CR cs.LG cs.MM published:2012-06-18 summary:This paper presents an application of statistical machine learning to thefield of watermarking. We propose a new attack model on additivespread-spectrum watermarking systems. The proposed attack is based on Bayesianstatistics. We consider the scenario in which a watermark signal is repeatedlyembedded in specific, possibly chosen based on a secret message bitstream,segments (signals) of the host data. The host signal can represent a patch ofpixels from an image or a video frame. We propose a probabilistic model thatinfers the embedded message bitstream and watermark signal, directly from thewatermarked data, without access to the decoder. We develop an efficient Markovchain Monte Carlo sampler for updating the model parameters from theirconjugate full conditional posteriors. We also provide a variational Bayesiansolution, which further increases the convergence speed of the algorithm.Experiments with synthetic and real image signals demonstrate that the attackmodel is able to correctly infer a large part of the message bitstream andobtain a very accurate estimate of the watermark signal.
arxiv-1206-4661 | Predicting accurate probabilities with a ranking loss |  http://arxiv.org/abs/1206.4661  | author:Aditya Menon, Xiaoqian Jiang, Shankar Vembu, Charles Elkan, Lucila Ohno-Machado category:cs.LG stat.ML published:2012-06-18 summary:In many real-world applications of machine learning classifiers, it isessential to predict the probability of an example belonging to a particularclass. This paper proposes a simple technique for predicting probabilitiesbased on optimizing a ranking loss, followed by isotonic regression. Thissemi-parametric technique offers both good ranking and regression performance,and models a richer set of probability distributions than statisticalworkhorses such as logistic regression. We provide experimental results thatshow the effectiveness of this technique on real-world applications ofprobability prediction.
arxiv-1206-4660 | Learning with Augmented Features for Heterogeneous Domain Adaptation |  http://arxiv.org/abs/1206.4660  | author:Lixin Duan, Dong Xu, Ivor Tsang category:cs.LG published:2012-06-18 summary:We propose a new learning method for heterogeneous domain adaptation (HDA),in which the data from the source domain and the target domain are representedby heterogeneous features with different dimensions. Using two differentprojection matrices, we first transform the data from two domains into a commonsubspace in order to measure the similarity between the data from two domains.We then propose two new feature mapping functions to augment the transformeddata with their original features and zeros. The existing learning methods(e.g., SVM and SVR) can be readily incorporated with our newly proposedaugmented feature representations to effectively utilize the data from bothdomains for HDA. Using the hinge loss function in SVM as an example, weintroduce the detailed objective function in our method called HeterogeneousFeature Augmentation (HFA) for a linear case and also describe itskernelization in order to efficiently cope with the data with very highdimensions. Moreover, we also develop an alternating optimization algorithm toeffectively solve the nontrivial optimization problem in our HFA method.Comprehensive experiments on two benchmark datasets clearly demonstrate thatHFA outperforms the existing HDA methods.
arxiv-1206-4659 | Max-Margin Nonparametric Latent Feature Models for Link Prediction |  http://arxiv.org/abs/1206.4659  | author:Jun Zhu category:cs.LG stat.ML published:2012-06-18 summary:We present a max-margin nonparametric latent feature model, which unites theideas of max-margin learning and Bayesian nonparametrics to discoverdiscriminative latent features for link prediction and automatically infer theunknown latent social dimension. By minimizing a hinge-loss using the linearexpectation operator, we can perform posterior inference efficiently withoutdealing with a highly nonlinear link likelihood function; by using afully-Bayesian formulation, we can avoid tuning regularization constants.Experimental results on real datasets appear to demonstrate the benefitsinherited from max-margin learning and fully-Bayesian nonparametric inference.
arxiv-1206-4658 | Dirichlet Process with Mixed Random Measures: A Nonparametric Topic Model for Labeled Data |  http://arxiv.org/abs/1206.4658  | author:Dongwoo Kim, Suin Kim, Alice Oh category:cs.LG stat.ML published:2012-06-18 summary:We describe a nonparametric topic model for labeled data. The model uses amixture of random measures (MRM) as a base distribution of the Dirichletprocess (DP) of the HDP framework, so we call it the DP-MRM. To model labeleddata, we define a DP distributed random measure for each label, and theresulting model generates an unbounded number of topics for each label. Weapply DP-MRM on single-labeled and multi-labeled corpora of documents andcompare the performance on label prediction with MedLDA, LDA-SVM, andLabeled-LDA. We further enhance the model by incorporating ddCRP and modelingmulti-labeled images for image segmentation and object labeling, comparing theperformance with nCuts and rddCRP.
arxiv-1206-4657 | Projection-free Online Learning |  http://arxiv.org/abs/1206.4657  | author:Elad Hazan, Satyen Kale category:cs.LG cs.DS published:2012-06-18 summary:The computational bottleneck in applying online learning to massive data setsis usually the projection step. We present efficient online learning algorithmsthat eschew projections in favor of much more efficient linear optimizationsteps using the Frank-Wolfe technique. We obtain a range of regret bounds foronline convex optimization, with better bounds for specific cases such asstochastic online smooth convex optimization. Besides the computational advantage, other desirable features of ouralgorithms are that they are parameter-free in the stochastic case and producesparse decisions. We apply our algorithms to computationally intensiveapplications of collaborative filtering, and show the theoretical improvementsto be clearly visible on standard datasets.
arxiv-1206-4656 | Machine Learning that Matters |  http://arxiv.org/abs/1206.4656  | author:Kiri Wagstaff category:cs.LG cs.AI stat.ML published:2012-06-18 summary:Much of current machine learning (ML) research has lost its connection toproblems of import to the larger world of science and society. From thisperspective, there exist glaring limitations in the data sets we investigate,the metrics we employ for evaluation, and the degree to which results arecommunicated back to their originating domains. What changes are needed to howwe conduct research to increase the impact that ML has? We present six ImpactChallenges to explicitly focus the field?s energy and attention, and we discussexisting obstacles that must be addressed. We aim to inspire ongoing discussionand focus on ML that matters.
arxiv-1206-4655 | Modelling transition dynamics in MDPs with RKHS embeddings |  http://arxiv.org/abs/1206.4655  | author:Steffen Grunewalder, Guy Lever, Luca Baldassarre, Massi Pontil, Arthur Gretton category:cs.LG published:2012-06-18 summary:We propose a new, nonparametric approach to learning and representingtransition dynamics in Markov decision processes (MDPs), which can be combinedeasily with dynamic programming methods for policy optimisation and valueestimation. This approach makes use of a recently developed representation ofconditional distributions as \emph{embeddings} in a reproducing kernel Hilbertspace (RKHS). Such representations bypass the need for estimating transitionprobabilities or densities, and apply to any domain on which kernels can bedefined. This avoids the need to calculate intractable integrals, sinceexpectations are represented as RKHS inner products whose computation haslinear complexity in the number of points used to represent the embedding. Weprovide guarantees for the proposed applications in MDPs: in the context of avalue iteration algorithm, we prove convergence to either the optimal policy,or to the closest projection of the optimal policy in our model class (anRKHS), under reasonable assumptions. In experiments, we investigate a learningtask in a typical classical control setting (the under-actuated pendulum), andon a navigation problem where only images from a sensor are observed. Forpolicy optimisation we compare with least-squares policy iteration where aGaussian process is used for value function estimation. For value estimation wealso compare to the NPDP method. Our approach achieves better performance inall experiments.
arxiv-1206-4654 | A Generalized Loop Correction Method for Approximate Inference in Graphical Models |  http://arxiv.org/abs/1206.4654  | author:Siamak Ravanbakhsh, Chun-Nam Yu, Russell Greiner category:cs.AI cs.LG stat.ML published:2012-06-18 summary:Belief Propagation (BP) is one of the most popular methods for inference inprobabilistic graphical models. BP is guaranteed to return the correct answerfor tree structures, but can be incorrect or non-convergent for loopy graphicalmodels. Recently, several new approximate inference algorithms based on cavitydistribution have been proposed. These methods can account for the effect ofloops by incorporating the dependency between BP messages. Alternatively,region-based approximations (that lead to methods such as Generalized BeliefPropagation) improve upon BP by considering interactions within small clustersof variables, thus taking small loops within these clusters into account. Thispaper introduces an approach, Generalized Loop Correction (GLC), that benefitsfrom both of these types of loop correction. We show how GLC relates to thesetwo families of inference methods, then provide empirical evidence that GLCworks effectively in general, and can be significantly more accurate than bothcorrection schemes.
arxiv-1206-4653 | Dimensionality Reduction by Local Discriminative Gaussians |  http://arxiv.org/abs/1206.4653  | author:Nathan Parrish, Maya Gupta category:cs.LG cs.CV stat.ML published:2012-06-18 summary:We present local discriminative Gaussian (LDG) dimensionality reduction, asupervised dimensionality reduction technique for classification. The LDGobjective function is an approximation to the leave-one-out training error of alocal quadratic discriminant analysis classifier, and thus acts locally to eachtraining point in order to find a mapping where similar data can bediscriminated from dissimilar data. While other state-of-the-art lineardimensionality reduction methods require gradient descent or iterative solutionapproaches, LDG is solved with a single eigen-decomposition. Thus, it scalesbetter for datasets with a large number of feature dimensions or trainingexamples. We also adapt LDG to the transfer learning setting, and show that itachieves good performance when the test data distribution differs from that ofthe training data.
arxiv-1206-4652 | The Most Persistent Soft-Clique in a Set of Sampled Graphs |  http://arxiv.org/abs/1206.4652  | author:Novi Quadrianto, Chao Chen, Christoph Lampert category:cs.LG cs.AI published:2012-06-18 summary:When searching for characteristic subpatterns in potentially noisy graphdata, it appears self-evident that having multiple observations would be betterthan having just one. However, it turns out that the inconsistencies introducedwhen different graph instances have different edge sets pose a seriouschallenge. In this work we address this challenge for the problem of findingmaximum weighted cliques. We introduce the concept of most persistent soft-clique. This is subset ofvertices, that 1) is almost fully or at least densely connected, 2) occurs inall or almost all graph instances, and 3) has the maximum weight. We present ameasure of clique-ness, that essentially counts the number of edge missing tomake a subset of vertices into a clique. With this measure, we show that theproblem of finding the most persistent soft-clique problem can be cast eitheras: a) a max-min two person game optimization problem, or b) a min-min softmargin optimization problem. Both formulations lead to the same solution whenusing a partial Lagrangian method to solve the optimization problems. Byexperiments on synthetic data and on real social network data, we show that theproposed method is able to reliably find soft cliques in graph data, even ifthat is distorted by random noise or unreliable observations.
