CS224D-Prefac | Preface | | parentid:CS224D
CS224D-1 | Introduction | | parentid:CS224D
CS224D-1.1 | Reinforcement Learning | | parentid:CS224D-1
CS224D-1.2 | Examples | | parentid:CS224D-1
CS224D-1.3 | Elements of Reinforcement Learning | | parentid:CS224D-1
CS224D-1.4 | An Extended Example: Tic-Tac-Toe | | parentid:CS224D-1
CS224D-1.5 | Summary | | parentid:CS224D-1
CS224D-1.6 | History of Reinforcement Learning | | parentid:CS224D-1
CS224D-1.7 | Bibliographical Remarks | | parentid:CS224D-1
CS224D-2 | Evaluative Feedback | | parentid:CS224D
CS224D-2.1 | An n-armed Bandit Problem | | parentid:CS224D-2
CS224D-2.2 | Action-Value Methods | | parentid:CS224D-2
CS224D-2.3 | Softmax Action Selection | | parentid:CS224D-2
CS224D-2.4 | Evaluation versus Instruction | | parentid:CS224D-2
CS224D-2.5 | Incremental Implementation | | parentid:CS224D-2
CS224D-2.6 | Tracking a Nonstationary Problem | | parentid:CS224D-2
CS224D-2.7 | Optimistic Initial Values | | parentid:CS224D-2
CS224D-2.8 | Reinforcement Comparison | | parentid:CS224D-2
CS224D-2.9 | Pursuit Methods | | parentid:CS224D-2
CS224D-2.10 | Associative Search | | parentid:CS224D-2
CS224D-2.11 | Conclusion | | parentid:CS224D-2
CS224D-2.12 | Bibliographical and Historical Remarks | | parentid:CS224D-2
CS224D-Footnote | Footnotes | | parentid:CS224D
CS224D-3 | The Reinforcement Learning Problem | | parentid:CS224D
CS224D-3.1 | The Agent-Environment Interface | | parentid:CS224D-3
CS224D-3.2 | Goals and Rewards | | parentid:CS224D-3
CS224D-3.3 | Returns | | parentid:CS224D-3
CS224D-3.4 | A Unified Notation for Episodic and Continual Tasks | | parentid:CS224D-3
CS224D-3.5 | The Markov Property | | parentid:CS224D-3
CS224D-3.6 | Markov Decision Processes | | parentid:CS224D-3
CS224D-3.7 | Value Functions | | parentid:CS224D-3
CS224D-3.8 | Optimal Value Functions | | parentid:CS224D-3
CS224D-3/node10.htm | 3/node10.html | | parentid:CS224D-3/node10
CS224D-3/node11.htm | 3/node11.html | | parentid:CS224D-3/node11
CS224D-3/node12.htm | 3/node12.html | | parentid:CS224D-3/node12
CS224D-3/footnode.htm | 3/footnode.html | | parentid:CS224D-3/footnode
CS224D-Part | II: Elementary Solution Methods | | parentid:CS224D
CS224D-4 | Dynamic Programming | | parentid:CS224D
CS224D-4.1 | Policy Evaluation | | parentid:CS224D-4
CS224D-4/node3.htm | 4/node3.html | | parentid:CS224D-4/node3
CS224D-4.3 | Policy Iteration | | parentid:CS224D-4
CS224D-4.4 | Value Iteration | | parentid:CS224D-4
CS224D-4.5 | Asynchronous Dynamic Programming | | parentid:CS224D-4
CS224D-4.6 | Generalized Policy Iteration | | parentid:CS224D-4
CS224D-4.7 | Efficiency of Dynamic Programming | | parentid:CS224D-4
CS224D-4.8 | Summary | | parentid:CS224D-4
CS224D-4.9 | Historical and Bibliographical Remarks | | parentid:CS224D-4
CS224D-Footnote | Footnotes | | parentid:CS224D
CS224D-5 | Monte Carlo Methods | | parentid:CS224D
CS224D-5.1 | Monte Carlo Policy Evaluation | | parentid:CS224D-5
CS224D-5.2 | Monte Carlo Estimation of Action Values | | parentid:CS224D-5
CS224D-5.3 | Monte Carlo Control | | parentid:CS224D-5
CS224D-5.4 | On-Policy Monte Carlo Control | | parentid:CS224D-5
CS224D-5.5 | Evaluating One Policy While Following Another | | parentid:CS224D-5
CS224D-5.6 | Off-Policy Monte Carlo Control | | parentid:CS224D-5
CS224D-5.7 | Incremental Implementation | | parentid:CS224D-5
CS224D-5.8 | Summary | | parentid:CS224D-5
CS224D-5.9 | Historical and Bibliographical Remarks | | parentid:CS224D-5
CS224D-6 | Temporal Difference Learning | | parentid:CS224D
CS224D-6.1 | TD Prediction | | parentid:CS224D-6
CS224D-6.2 | Advantages of TD Prediction Methods | | parentid:CS224D-6
CS224D-6.3 | Optimality of TD(0) | | parentid:CS224D-6
CS224D-6.4 | Sarsa: On-Policy TD Control | | parentid:CS224D-6
CS224D-6.5 | Q-learning: Off-Policy TD Control | | parentid:CS224D-6
CS224D-6.6 | Actor-Critic Methods (*) | | parentid:CS224D-6
CS224D-6.7 | R-Learning for Undiscounted Continual Tasks (*) | | parentid:CS224D-6
CS224D-6.8 | Games, After States, and other Special Cases | | parentid:CS224D-6
CS224D-6.9 | Conclusions | | parentid:CS224D-6
CS224D-6.10 | Historical and Bibliographical Remarks | | parentid:CS224D-6
CS224D-Footnote | Footnotes | | parentid:CS224D
CS224D-Part | III: A Unified View | | parentid:CS224D
CS224D-7 | Eligibility Traces | | parentid:CS224D
CS224D-7.1 | n-step TD Prediction | | parentid:CS224D-7
CS224D-7.2 | The Forward View of TD() | | parentid:CS224D-7
CS224D-7.3 | The Backward View of TD() | | parentid:CS224D-7
CS224D-7.4 | Equivalence of the Forward and Backward Views | | parentid:CS224D-7
CS224D-7.5 | Sarsa() | | parentid:CS224D-7
CS224D-7.6 | Q() | | parentid:CS224D-7
CS224D-7.7 | Eligibility Traces for Actor-Critic Methods (*) | | parentid:CS224D-7
CS224D-7.8 | Replacing Traces | | parentid:CS224D-7
CS224D-7/node10.htm | 7/node10.html | | parentid:CS224D-7/node10
CS224D-7.10 | Variable (*) | | parentid:CS224D-7
CS224D-7.11 | Conclusions | | parentid:CS224D-7
CS224D-7.12 | Bibliographical and Historical Remarks | | parentid:CS224D-7
CS224D-8 | Generalization and Function Approximation | | parentid:CS224D
CS224D-8/node1.htm | 8/node1.html | | parentid:CS224D-8/node1
CS224D-8/node2.htm | 8/node2.html | | parentid:CS224D-8/node2
CS224D-8/node3.htm | 8/node3.html | | parentid:CS224D-8/node3
CS224D-8/node4.htm | 8/node4.html | | parentid:CS224D-8/node4
CS224D-8.3.1 | Coarse Coding | | parentid:CS224D-8
CS224D-8.3.2 | Tile Coding | | parentid:CS224D-8
CS224D-8.3.3 | Radial Basis Functions | | parentid:CS224D-8
CS224D-8.3.4 | Kanerva Coding | | parentid:CS224D-8
CS224D-8.4 | Control with Function Approximation | | parentid:CS224D-8
CS224D-8.5 | Off-Policy Bootstrapping | | parentid:CS224D-8
CS224D-8.6 | Should We Bootstrap? | | parentid:CS224D-8
CS224D-8.7 | Summary | | parentid:CS224D-8
CS224D-8.8 | Bibliographical and Historical Remarks | | parentid:CS224D-8
CS224D-9 | Planning and Learning | | parentid:CS224D
CS224D-9.1 | Models and Planning | | parentid:CS224D-9
CS224D-9.2 | Integrating Planning, Acting, and Learning | | parentid:CS224D-9
CS224D-9.3 | When the Model is Wrong | | parentid:CS224D-9
CS224D-9.4 | Prioritized Sweeping | | parentid:CS224D-9
CS224D-9.5 | Full vs. Sample Backups | | parentid:CS224D-9
CS224D-9.6 | Trajectory Sampling | | parentid:CS224D-9
CS224D-9.7 | Heuristic Search | | parentid:CS224D-9
CS224D-9.8 | Summary | | parentid:CS224D-9
CS224D-9.9 | Historical and Bibliographical Remarks | | parentid:CS224D-9
CS224D-Footnote | Footnotes | | parentid:CS224D
CS224D-10 | Dimensions | | parentid:CS224D
CS224D-10.1 | The Unified View | | parentid:CS224D-10
CS224D-10.2 | Other Frontier Dimensions | | parentid:CS224D-10
CS224D-11 | Case Studies | | parentid:CS224D
CS224D-11.1 | TD-Gammon | | parentid:CS224D-11
CS224D-11.2 | Samuel s Checkers Player | | parentid:CS224D-11
CS224D-11.3 | The Acrobot | | parentid:CS224D-11
CS224D-11.4 | Elevator Dispatching | | parentid:CS224D-11
CS224D-11.5 | Dynamic Channel Allocation | | parentid:CS224D-11
CS224D-11.6 | Job-Shop Scheduling | | parentid:CS224D-11
CS224D-Reference | References | | parentid:CS224D
CS224D-Summary | of Notation | | parentid:CS224D
CS224D- |  | | parentid:CS224D
